==========================================
Method: tova
GPU: 1
Start Time: Sat Dec 13 05:54:09 PM CST 2025
==========================================

========================================
LongBench Task: 2wikimqa
========================================
----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 17:54:14,641 - INFO - Set deterministic seeds to 42
2025-12-13 17:54:14,641 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 17:54:14,641 - INFO - Starting evaluation run...
2025-12-13 17:54:14,641 - INFO - Output directory set to: longbenchresult
2025-12-13 17:54:14,641 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 17:54:14,641 - INFO - KV Press 'tova' setup.
2025-12-13 17:54:14,641 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 17:54:14,641 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.89it/s]
Device set to use cuda:0
2025-12-13 17:54:26,829 - INFO - Model pipeline loaded.
2025-12-13 17:54:26,830 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 17:54:31,615 - INFO - Dataset loaded with 200 entries.
2025-12-13 17:54:31,615 - INFO - Dataset processed with 200 entries.
2025-12-13 17:54:31,634 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:32,  4.39s/it]Running Inference:   1%|          | 2/200 [00:05<07:51,  2.38s/it]Running Inference:   2%|▏         | 3/200 [00:08<09:03,  2.76s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:10,  2.50s/it]Running Inference:   2%|▎         | 5/200 [00:14<10:10,  3.13s/it]Running Inference:   3%|▎         | 6/200 [00:16<08:18,  2.57s/it]Running Inference:   4%|▎         | 7/200 [00:17<06:43,  2.09s/it]Running Inference:   4%|▍         | 8/200 [00:20<07:51,  2.46s/it]Running Inference:   4%|▍         | 9/200 [00:21<06:33,  2.06s/it]Running Inference:   5%|▌         | 10/200 [00:22<05:06,  1.62s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:24<04:56,  1.57s/it]Running Inference:   6%|▌         | 12/200 [00:25<04:52,  1.56s/it]Running Inference:   6%|▋         | 13/200 [00:29<06:45,  2.17s/it]Running Inference:   7%|▋         | 14/200 [00:29<05:13,  1.68s/it]Running Inference:   8%|▊         | 15/200 [00:30<04:30,  1.46s/it]Running Inference:   8%|▊         | 16/200 [00:31<04:04,  1.33s/it]Running Inference:   8%|▊         | 17/200 [00:33<04:16,  1.40s/it]Running Inference:   9%|▉         | 18/200 [00:34<03:52,  1.28s/it]Running Inference:  10%|▉         | 19/200 [00:37<05:15,  1.74s/it]Running Inference:  10%|█         | 20/200 [00:37<03:56,  1.31s/it]Running Inference:  10%|█         | 21/200 [00:39<04:37,  1.55s/it]Running Inference:  11%|█         | 22/200 [00:41<04:58,  1.68s/it]Running Inference:  12%|█▏        | 23/200 [00:44<06:31,  2.21s/it]Running Inference:  12%|█▏        | 24/200 [00:45<05:19,  1.81s/it]Running Inference:  12%|█▎        | 25/200 [00:46<04:37,  1.59s/it]Running Inference:  13%|█▎        | 26/200 [00:48<04:23,  1.51s/it]Running Inference:  14%|█▎        | 27/200 [00:48<03:37,  1.26s/it]Running Inference:  14%|█▍        | 28/200 [00:49<03:06,  1.08s/it]Running Inference:  14%|█▍        | 29/200 [00:52<04:32,  1.59s/it]Running Inference:  15%|█▌        | 30/200 [00:52<03:23,  1.20s/it]Running Inference:  16%|█▌        | 31/200 [00:54<03:34,  1.27s/it]Running Inference:  16%|█▌        | 32/200 [00:57<05:01,  1.80s/it]Running Inference:  16%|█▋        | 33/200 [00:58<04:18,  1.55s/it]Running Inference:  17%|█▋        | 34/200 [01:01<05:32,  2.00s/it]Running Inference:  18%|█▊        | 35/200 [01:01<04:32,  1.65s/it]Running Inference:  18%|█▊        | 36/200 [01:02<03:54,  1.43s/it]Running Inference:  18%|█▊        | 37/200 [01:05<05:16,  1.94s/it]Running Inference:  19%|█▉        | 38/200 [01:07<04:53,  1.81s/it]Running Inference:  20%|█▉        | 39/200 [01:09<04:39,  1.74s/it]Running Inference:  20%|██        | 40/200 [01:11<05:36,  2.11s/it]Running Inference:  20%|██        | 41/200 [01:13<05:08,  1.94s/it]Running Inference:  21%|██        | 42/200 [01:14<04:31,  1.72s/it]Running Inference:  22%|██▏       | 43/200 [01:15<03:51,  1.47s/it]Running Inference:  22%|██▏       | 44/200 [01:16<03:10,  1.22s/it]Running Inference:  22%|██▎       | 45/200 [01:17<02:50,  1.10s/it]Running Inference:  23%|██▎       | 46/200 [01:18<03:22,  1.32s/it]Running Inference:  24%|██▎       | 47/200 [01:20<03:21,  1.32s/it]Running Inference:  24%|██▍       | 48/200 [01:21<02:59,  1.18s/it]Running Inference:  24%|██▍       | 49/200 [01:24<04:26,  1.77s/it]Running Inference:  25%|██▌       | 50/200 [01:27<05:15,  2.10s/it]Running Inference:  26%|██▌       | 51/200 [01:29<05:15,  2.12s/it]Running Inference:  26%|██▌       | 52/200 [01:29<03:57,  1.60s/it]Running Inference:  26%|██▋       | 53/200 [01:32<04:45,  1.94s/it]Running Inference:  27%|██▋       | 54/200 [01:33<04:23,  1.80s/it]Running Inference:  28%|██▊       | 55/200 [01:34<03:40,  1.52s/it]Running Inference:  28%|██▊       | 56/200 [01:36<03:28,  1.45s/it]Running Inference:  28%|██▊       | 57/200 [01:36<03:00,  1.26s/it]Running Inference:  29%|██▉       | 58/200 [01:40<04:20,  1.83s/it]Running Inference:  30%|██▉       | 59/200 [01:40<03:34,  1.52s/it]Running Inference:  30%|███       | 60/200 [01:41<03:13,  1.38s/it]Running Inference:  30%|███       | 61/200 [01:42<02:42,  1.17s/it]Running Inference:  31%|███       | 62/200 [01:45<04:10,  1.82s/it]Running Inference:  32%|███▏      | 63/200 [01:46<03:07,  1.37s/it]Running Inference:  32%|███▏      | 64/200 [01:50<04:54,  2.16s/it]Running Inference:  32%|███▎      | 65/200 [01:50<03:43,  1.66s/it]Running Inference:  33%|███▎      | 66/200 [01:51<03:22,  1.51s/it]Running Inference:  34%|███▎      | 67/200 [01:55<04:29,  2.02s/it]Running Inference:  34%|███▍      | 68/200 [01:57<04:29,  2.04s/it]Running Inference:  34%|███▍      | 69/200 [01:59<04:57,  2.27s/it]Running Inference:  35%|███▌      | 70/200 [02:01<04:06,  1.90s/it]Running Inference:  36%|███▌      | 71/200 [02:01<03:15,  1.51s/it]Running Inference:  36%|███▌      | 72/200 [02:04<04:20,  2.03s/it]Running Inference:  36%|███▋      | 73/200 [02:06<03:43,  1.76s/it]Running Inference:  37%|███▋      | 74/200 [02:08<03:56,  1.88s/it]Running Inference:  38%|███▊      | 75/200 [02:09<03:26,  1.65s/it]Running Inference:  38%|███▊      | 76/200 [02:10<03:08,  1.52s/it]Running Inference:  38%|███▊      | 77/200 [02:11<02:51,  1.39s/it]Running Inference:  39%|███▉      | 78/200 [02:14<04:01,  1.98s/it]Running Inference:  40%|███▉      | 79/200 [02:18<04:38,  2.30s/it]Running Inference:  40%|████      | 80/200 [02:19<04:13,  2.11s/it]Running Inference:  40%|████      | 81/200 [02:22<04:38,  2.34s/it]Running Inference:  41%|████      | 82/200 [02:25<05:02,  2.56s/it]Running Inference:  42%|████▏     | 83/200 [02:27<04:23,  2.25s/it]Running Inference:  42%|████▏     | 84/200 [02:28<03:35,  1.86s/it]Running Inference:  42%|████▎     | 85/200 [02:29<03:07,  1.63s/it]Running Inference:  43%|████▎     | 86/200 [02:33<04:23,  2.31s/it]Running Inference:  44%|████▎     | 87/200 [02:33<03:24,  1.81s/it]Running Inference:  44%|████▍     | 88/200 [02:37<04:20,  2.32s/it]Running Inference:  44%|████▍     | 89/200 [02:37<03:24,  1.84s/it]Running Inference:  45%|████▌     | 90/200 [02:41<04:03,  2.22s/it]Running Inference:  46%|████▌     | 91/200 [02:43<04:20,  2.39s/it]Running Inference:  46%|████▌     | 92/200 [02:44<03:30,  1.95s/it]Running Inference:  46%|████▋     | 93/200 [02:45<03:00,  1.68s/it]Running Inference:  47%|████▋     | 94/200 [02:46<02:18,  1.31s/it]Running Inference:  48%|████▊     | 95/200 [02:47<01:59,  1.14s/it]Running Inference:  48%|████▊     | 96/200 [02:48<01:53,  1.09s/it]Running Inference:  48%|████▊     | 97/200 [02:49<02:16,  1.32s/it]Running Inference:  49%|████▉     | 98/200 [02:50<02:06,  1.24s/it]Running Inference:  50%|████▉     | 99/200 [02:51<01:49,  1.08s/it]Running Inference:  50%|█████     | 100/200 [02:52<01:37,  1.02it/s]Running Inference:  50%|█████     | 101/200 [02:55<02:30,  1.52s/it]Running Inference:  51%|█████     | 102/200 [02:56<02:15,  1.39s/it]Running Inference:  52%|█████▏    | 103/200 [02:59<02:55,  1.81s/it]Running Inference:  52%|█████▏    | 104/200 [03:01<03:26,  2.15s/it]Running Inference:  52%|█████▎    | 105/200 [03:02<02:46,  1.76s/it]Running Inference:  53%|█████▎    | 106/200 [03:03<02:23,  1.52s/it]Running Inference:  54%|█████▎    | 107/200 [03:05<02:26,  1.58s/it]Running Inference:  54%|█████▍    | 108/200 [03:08<03:02,  1.98s/it]Running Inference:  55%|█████▍    | 109/200 [03:09<02:31,  1.66s/it]Running Inference:  55%|█████▌    | 110/200 [03:10<02:27,  1.64s/it]Running Inference:  56%|█████▌    | 111/200 [03:11<02:00,  1.36s/it]Running Inference:  56%|█████▌    | 112/200 [03:12<01:57,  1.34s/it]Running Inference:  56%|█████▋    | 113/200 [03:13<01:49,  1.26s/it]Running Inference:  57%|█████▋    | 114/200 [03:14<01:36,  1.12s/it]Running Inference:  57%|█████▊    | 115/200 [03:16<01:56,  1.37s/it]Running Inference:  58%|█████▊    | 116/200 [03:19<02:28,  1.77s/it]Running Inference:  58%|█████▊    | 117/200 [03:23<03:24,  2.46s/it]Running Inference:  59%|█████▉    | 118/200 [03:24<02:38,  1.93s/it]Running Inference:  60%|█████▉    | 119/200 [03:27<03:08,  2.32s/it]Running Inference:  60%|██████    | 120/200 [03:28<02:30,  1.89s/it]Running Inference:  60%|██████    | 121/200 [03:29<02:03,  1.57s/it]Running Inference:  61%|██████    | 122/200 [03:29<01:39,  1.28s/it]Running Inference:  62%|██████▏   | 123/200 [03:30<01:30,  1.17s/it]Running Inference:  62%|██████▏   | 124/200 [03:33<02:12,  1.74s/it]Running Inference:  62%|██████▎   | 125/200 [03:34<01:47,  1.44s/it]Running Inference:  63%|██████▎   | 126/200 [03:37<02:15,  1.83s/it]Running Inference:  64%|██████▎   | 127/200 [03:37<01:42,  1.40s/it]Running Inference:  64%|██████▍   | 128/200 [03:38<01:29,  1.25s/it]Running Inference:  64%|██████▍   | 129/200 [03:40<01:34,  1.33s/it]Running Inference:  65%|██████▌   | 130/200 [03:40<01:22,  1.18s/it]Running Inference:  66%|██████▌   | 131/200 [03:43<01:58,  1.72s/it]Running Inference:  66%|██████▌   | 132/200 [03:48<02:50,  2.51s/it]Running Inference:  66%|██████▋   | 133/200 [03:49<02:28,  2.22s/it]Running Inference:  67%|██████▋   | 134/200 [03:52<02:43,  2.48s/it]Running Inference:  68%|██████▊   | 135/200 [03:57<03:17,  3.04s/it]Running Inference:  68%|██████▊   | 136/200 [03:57<02:31,  2.36s/it]Running Inference:  68%|██████▊   | 137/200 [03:59<02:05,  1.99s/it]Running Inference:  69%|██████▉   | 138/200 [04:00<01:46,  1.72s/it]Running Inference:  70%|██████▉   | 139/200 [04:02<01:51,  1.84s/it]Running Inference:  70%|███████   | 140/200 [04:03<01:45,  1.77s/it]Running Inference:  70%|███████   | 141/200 [04:07<02:20,  2.38s/it]Running Inference:  71%|███████   | 142/200 [04:08<01:55,  1.99s/it]Running Inference:  72%|███████▏  | 143/200 [04:10<01:51,  1.96s/it]Running Inference:  72%|███████▏  | 144/200 [04:13<02:06,  2.26s/it]Running Inference:  72%|███████▎  | 145/200 [04:15<01:57,  2.14s/it]Running Inference:  73%|███████▎  | 146/200 [04:16<01:37,  1.81s/it]Running Inference:  74%|███████▎  | 147/200 [04:17<01:20,  1.51s/it]Running Inference:  74%|███████▍  | 148/200 [04:18<01:08,  1.32s/it]Running Inference:  74%|███████▍  | 149/200 [04:19<01:03,  1.24s/it]Running Inference:  75%|███████▌  | 150/200 [04:21<01:15,  1.51s/it]Running Inference:  76%|███████▌  | 151/200 [04:21<00:56,  1.14s/it]Running Inference:  76%|███████▌  | 152/200 [04:22<00:46,  1.02it/s]Running Inference:  76%|███████▋  | 153/200 [04:23<00:43,  1.09it/s]Running Inference:  77%|███████▋  | 154/200 [04:25<01:09,  1.50s/it]Running Inference:  78%|███████▊  | 155/200 [04:27<01:09,  1.55s/it]Running Inference:  78%|███████▊  | 156/200 [04:29<01:14,  1.68s/it]Running Inference:  78%|███████▊  | 157/200 [04:30<00:56,  1.32s/it]Running Inference:  79%|███████▉  | 158/200 [04:30<00:50,  1.19s/it]Running Inference:  80%|███████▉  | 159/200 [04:34<01:21,  1.99s/it]Running Inference:  80%|████████  | 160/200 [04:38<01:40,  2.52s/it]Running Inference:  80%|████████  | 161/200 [04:40<01:27,  2.23s/it]Running Inference:  81%|████████  | 162/200 [04:42<01:31,  2.41s/it]Running Inference:  82%|████████▏ | 163/200 [04:44<01:14,  2.02s/it]Running Inference:  82%|████████▏ | 164/200 [04:46<01:20,  2.25s/it]Running Inference:  82%|████████▎ | 165/200 [04:47<01:03,  1.81s/it]Running Inference:  83%|████████▎ | 166/200 [04:49<01:04,  1.90s/it]Running Inference:  84%|████████▎ | 167/200 [04:51<01:03,  1.92s/it]Running Inference:  84%|████████▍ | 168/200 [04:52<00:48,  1.51s/it]Running Inference:  84%|████████▍ | 169/200 [04:52<00:35,  1.16s/it]Running Inference:  85%|████████▌ | 170/200 [04:55<00:51,  1.72s/it]Running Inference:  86%|████████▌ | 171/200 [04:58<01:00,  2.10s/it]Running Inference:  86%|████████▌ | 172/200 [04:59<00:49,  1.77s/it]Running Inference:  86%|████████▋ | 173/200 [05:00<00:41,  1.55s/it]Running Inference:  87%|████████▋ | 174/200 [05:03<00:50,  1.95s/it]Running Inference:  88%|████████▊ | 175/200 [05:07<01:04,  2.58s/it]Running Inference:  88%|████████▊ | 176/200 [05:10<01:05,  2.71s/it]Running Inference:  88%|████████▊ | 177/200 [05:13<01:02,  2.72s/it]Running Inference:  89%|████████▉ | 178/200 [05:15<00:55,  2.52s/it]Running Inference:  90%|████████▉ | 179/200 [05:19<01:01,  2.91s/it]Running Inference:  90%|█████████ | 180/200 [05:20<00:48,  2.45s/it]Running Inference:  90%|█████████ | 181/200 [05:21<00:36,  1.92s/it]Running Inference:  91%|█████████ | 182/200 [05:23<00:33,  1.89s/it]Running Inference:  92%|█████████▏| 183/200 [05:24<00:30,  1.79s/it]Running Inference:  92%|█████████▏| 184/200 [05:25<00:23,  1.48s/it]Running Inference:  92%|█████████▎| 185/200 [05:27<00:26,  1.75s/it]Running Inference:  93%|█████████▎| 186/200 [05:28<00:20,  1.44s/it]Running Inference:  94%|█████████▎| 187/200 [05:29<00:17,  1.34s/it]Running Inference:  94%|█████████▍| 188/200 [05:32<00:21,  1.80s/it]Running Inference:  94%|█████████▍| 189/200 [05:35<00:23,  2.13s/it]Running Inference:  95%|█████████▌| 190/200 [05:35<00:15,  1.60s/it]Running Inference:  96%|█████████▌| 191/200 [05:37<00:14,  1.60s/it]Running Inference:  96%|█████████▌| 192/200 [05:38<00:11,  1.47s/it]Running Inference:  96%|█████████▋| 193/200 [05:42<00:15,  2.17s/it]Running Inference:  97%|█████████▋| 194/200 [05:42<00:09,  1.66s/it]Running Inference:  98%|█████████▊| 195/200 [05:45<00:10,  2.12s/it]Running Inference:  98%|█████████▊| 196/200 [05:47<00:07,  1.83s/it]Running Inference:  98%|█████████▊| 197/200 [05:48<00:04,  1.63s/it]Running Inference:  99%|█████████▉| 198/200 [05:48<00:02,  1.33s/it]Running Inference: 100%|█████████▉| 199/200 [05:49<00:01,  1.18s/it]Running Inference: 100%|██████████| 200/200 [05:50<00:00,  1.01s/it]Running Inference: 100%|██████████| 200/200 [05:50<00:00,  1.75s/it]
2025-12-13 18:00:21,995 - INFO - Inference completed.
2025-12-13 18:00:22,032 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 18:00:22,032 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:00:22,036 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 18:00:22,037 - INFO - Metrics:
27.67
2025-12-13 18:00:22,038 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa, ratio=0.1) on GPU 1

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:00:28,553 - INFO - Set deterministic seeds to 42
2025-12-13 18:00:28,554 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:00:28,554 - INFO - Starting evaluation run...
2025-12-13 18:00:28,554 - INFO - Output directory set to: longbenchresult
2025-12-13 18:00:28,554 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 18:00:28,554 - INFO - KV Press 'tova' setup.
2025-12-13 18:00:28,554 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:00:28,554 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.36it/s]
Device set to use cuda:0
2025-12-13 18:00:40,522 - INFO - Model pipeline loaded.
2025-12-13 18:00:40,522 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:00:47,762 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:00:47,762 - INFO - Dataset processed with 200 entries.
2025-12-13 18:00:47,781 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:36,  4.41s/it]Running Inference:   1%|          | 2/200 [00:06<09:32,  2.89s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:54,  3.02s/it]Running Inference:   2%|▏         | 4/200 [00:11<08:42,  2.67s/it]Running Inference:   2%|▎         | 5/200 [00:15<10:33,  3.25s/it]Running Inference:   3%|▎         | 6/200 [00:17<08:34,  2.65s/it]Running Inference:   4%|▎         | 7/200 [00:18<06:54,  2.15s/it]Running Inference:   4%|▍         | 8/200 [00:21<07:58,  2.49s/it]Running Inference:   4%|▍         | 9/200 [00:22<06:37,  2.08s/it]Running Inference:   5%|▌         | 10/200 [00:23<05:10,  1.63s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:24<04:59,  1.58s/it]Running Inference:   6%|▌         | 12/200 [00:26<04:54,  1.57s/it]Running Inference:   6%|▋         | 13/200 [00:30<06:46,  2.17s/it]Running Inference:   7%|▋         | 14/200 [00:30<05:14,  1.69s/it]Running Inference:   8%|▊         | 15/200 [00:31<04:31,  1.47s/it]Running Inference:   8%|▊         | 16/200 [00:32<04:01,  1.31s/it]Running Inference:   8%|▊         | 17/200 [00:34<04:14,  1.39s/it]Running Inference:   9%|▉         | 18/200 [00:35<03:50,  1.27s/it]Running Inference:  10%|▉         | 19/200 [00:37<05:13,  1.73s/it]Running Inference:  10%|█         | 20/200 [00:38<03:55,  1.31s/it]Running Inference:  10%|█         | 21/200 [00:40<04:36,  1.54s/it]Running Inference:  11%|█         | 22/200 [00:42<04:57,  1.67s/it]Running Inference:  12%|█▏        | 23/200 [00:43<04:40,  1.58s/it]Running Inference:  12%|█▏        | 24/200 [00:44<04:01,  1.37s/it]Running Inference:  12%|█▎        | 25/200 [00:45<03:43,  1.28s/it]Running Inference:  13%|█▎        | 26/200 [00:46<03:45,  1.30s/it]Running Inference:  14%|█▎        | 27/200 [00:47<03:10,  1.10s/it]Running Inference:  14%|█▍        | 28/200 [00:48<02:47,  1.02it/s]Running Inference:  14%|█▍        | 29/200 [00:51<04:18,  1.51s/it]Running Inference:  15%|█▌        | 30/200 [00:51<03:14,  1.14s/it]Running Inference:  16%|█▌        | 31/200 [00:52<03:28,  1.23s/it]Running Inference:  16%|█▌        | 32/200 [00:55<04:56,  1.76s/it]Running Inference:  16%|█▋        | 33/200 [00:56<04:15,  1.53s/it]Running Inference:  17%|█▋        | 34/200 [00:57<03:47,  1.37s/it]Running Inference:  18%|█▊        | 35/200 [00:58<03:19,  1.21s/it]Running Inference:  18%|█▊        | 36/200 [00:59<03:03,  1.12s/it]Running Inference:  18%|█▊        | 37/200 [01:02<04:43,  1.74s/it]Running Inference:  19%|█▉        | 38/200 [01:04<04:30,  1.67s/it]Running Inference:  20%|█▉        | 39/200 [01:07<05:30,  2.05s/it]Running Inference:  20%|██        | 40/200 [01:09<05:38,  2.11s/it]Running Inference:  20%|██        | 41/200 [01:10<05:09,  1.95s/it]Running Inference:  21%|██        | 42/200 [01:11<04:26,  1.69s/it]Running Inference:  22%|██▏       | 43/200 [01:12<03:48,  1.45s/it]Running Inference:  22%|██▏       | 44/200 [01:13<03:08,  1.21s/it]Running Inference:  22%|██▎       | 45/200 [01:14<02:50,  1.10s/it]Running Inference:  23%|██▎       | 46/200 [01:16<03:54,  1.52s/it]Running Inference:  24%|██▎       | 47/200 [01:18<03:44,  1.46s/it]Running Inference:  24%|██▍       | 48/200 [01:19<03:15,  1.28s/it]Running Inference:  24%|██▍       | 49/200 [01:22<04:37,  1.84s/it]Running Inference:  25%|██▌       | 50/200 [01:23<03:56,  1.58s/it]Running Inference:  26%|██▌       | 51/200 [01:25<04:21,  1.75s/it]Running Inference:  26%|██▌       | 52/200 [01:25<03:19,  1.35s/it]Running Inference:  26%|██▋       | 53/200 [01:28<04:18,  1.76s/it]Running Inference:  27%|██▋       | 54/200 [01:29<04:04,  1.67s/it]Running Inference:  28%|██▊       | 55/200 [01:30<03:27,  1.43s/it]Running Inference:  28%|██▊       | 56/200 [01:32<03:19,  1.39s/it]Running Inference:  28%|██▊       | 57/200 [01:32<02:56,  1.23s/it]Running Inference:  29%|██▉       | 58/200 [01:36<04:15,  1.80s/it]Running Inference:  30%|██▉       | 59/200 [01:36<03:31,  1.50s/it]Running Inference:  30%|███       | 60/200 [01:37<03:11,  1.37s/it]Running Inference:  30%|███       | 61/200 [01:38<02:41,  1.16s/it]Running Inference:  31%|███       | 62/200 [01:39<02:34,  1.12s/it]Running Inference:  32%|███▏      | 63/200 [01:39<01:58,  1.16it/s]Running Inference:  32%|███▏      | 64/200 [01:43<04:05,  1.81s/it]Running Inference:  32%|███▎      | 65/200 [01:44<03:10,  1.41s/it]Running Inference:  33%|███▎      | 66/200 [01:45<02:59,  1.34s/it]Running Inference:  34%|███▎      | 67/200 [01:48<04:12,  1.90s/it]Running Inference:  34%|███▍      | 68/200 [01:50<04:18,  1.95s/it]Running Inference:  34%|███▍      | 69/200 [01:53<04:48,  2.20s/it]Running Inference:  35%|███▌      | 70/200 [01:54<04:00,  1.85s/it]Running Inference:  36%|███▌      | 71/200 [01:55<03:10,  1.48s/it]Running Inference:  36%|███▌      | 72/200 [01:58<04:16,  2.00s/it]Running Inference:  36%|███▋      | 73/200 [02:00<04:23,  2.08s/it]Running Inference:  37%|███▋      | 74/200 [02:02<04:24,  2.10s/it]Running Inference:  38%|███▊      | 75/200 [02:04<03:45,  1.81s/it]Running Inference:  38%|███▊      | 76/200 [02:05<03:21,  1.63s/it]Running Inference:  38%|███▊      | 77/200 [02:06<03:00,  1.47s/it]Running Inference:  39%|███▉      | 78/200 [02:07<02:58,  1.46s/it]Running Inference:  40%|███▉      | 79/200 [02:10<03:54,  1.94s/it]Running Inference:  40%|████      | 80/200 [02:14<04:49,  2.41s/it]Running Inference:  40%|████      | 81/200 [02:17<05:02,  2.54s/it]Running Inference:  41%|████      | 82/200 [02:20<05:18,  2.70s/it]Running Inference:  42%|████▏     | 83/200 [02:21<04:34,  2.35s/it]Running Inference:  42%|████▏     | 84/200 [02:22<03:43,  1.93s/it]Running Inference:  42%|████▎     | 85/200 [02:23<03:13,  1.68s/it]Running Inference:  43%|████▎     | 86/200 [02:27<04:26,  2.34s/it]Running Inference:  44%|████▎     | 87/200 [02:28<03:26,  1.82s/it]Running Inference:  44%|████▍     | 88/200 [02:31<04:03,  2.17s/it]Running Inference:  44%|████▍     | 89/200 [02:32<03:31,  1.90s/it]Running Inference:  45%|████▌     | 90/200 [02:35<04:08,  2.26s/it]Running Inference:  46%|████▌     | 91/200 [02:38<04:22,  2.41s/it]Running Inference:  46%|████▌     | 92/200 [02:39<03:31,  1.96s/it]Running Inference:  46%|████▋     | 93/200 [02:40<03:01,  1.69s/it]Running Inference:  47%|████▋     | 94/200 [02:40<02:19,  1.32s/it]Running Inference:  48%|████▊     | 95/200 [02:41<01:59,  1.14s/it]Running Inference:  48%|████▊     | 96/200 [02:42<01:53,  1.09s/it]Running Inference:  48%|████▊     | 97/200 [02:44<02:16,  1.32s/it]Running Inference:  49%|████▉     | 98/200 [02:45<02:05,  1.23s/it]Running Inference:  50%|████▉     | 99/200 [02:46<01:48,  1.08s/it]Running Inference:  50%|█████     | 100/200 [02:46<01:37,  1.02it/s]Running Inference:  50%|█████     | 101/200 [02:49<02:30,  1.52s/it]Running Inference:  51%|█████     | 102/200 [02:50<02:10,  1.33s/it]Running Inference:  52%|█████▏    | 103/200 [02:53<02:51,  1.76s/it]Running Inference:  52%|█████▏    | 104/200 [02:56<03:22,  2.11s/it]Running Inference:  52%|█████▎    | 105/200 [02:57<02:44,  1.73s/it]Running Inference:  53%|█████▎    | 106/200 [02:58<02:21,  1.50s/it]Running Inference:  54%|█████▎    | 107/200 [03:01<03:17,  2.12s/it]Running Inference:  54%|█████▍    | 108/200 [03:04<03:37,  2.36s/it]Running Inference:  55%|█████▍    | 109/200 [03:05<02:55,  1.92s/it]Running Inference:  55%|█████▌    | 110/200 [03:07<02:44,  1.83s/it]Running Inference:  56%|█████▌    | 111/200 [03:07<02:12,  1.48s/it]Running Inference:  56%|█████▌    | 112/200 [03:09<02:05,  1.43s/it]Running Inference:  56%|█████▋    | 113/200 [03:10<01:54,  1.31s/it]Running Inference:  57%|█████▋    | 114/200 [03:10<01:39,  1.16s/it]Running Inference:  57%|█████▊    | 115/200 [03:12<01:59,  1.40s/it]Running Inference:  58%|█████▊    | 116/200 [03:15<02:29,  1.78s/it]Running Inference:  58%|█████▊    | 117/200 [03:19<03:25,  2.47s/it]Running Inference:  59%|█████▉    | 118/200 [03:20<02:38,  1.94s/it]Running Inference:  60%|█████▉    | 119/200 [03:23<03:07,  2.32s/it]Running Inference:  60%|██████    | 120/200 [03:24<02:30,  1.88s/it]Running Inference:  60%|██████    | 121/200 [03:25<02:03,  1.56s/it]Running Inference:  61%|██████    | 122/200 [03:25<01:39,  1.28s/it]Running Inference:  62%|██████▏   | 123/200 [03:26<01:30,  1.17s/it]Running Inference:  62%|██████▏   | 124/200 [03:28<01:35,  1.25s/it]Running Inference:  62%|██████▎   | 125/200 [03:28<01:22,  1.10s/it]Running Inference:  63%|██████▎   | 126/200 [03:29<01:14,  1.01s/it]Running Inference:  64%|██████▎   | 127/200 [03:30<01:00,  1.21it/s]Running Inference:  64%|██████▍   | 128/200 [03:30<01:00,  1.18it/s]Running Inference:  64%|██████▍   | 129/200 [03:32<01:14,  1.05s/it]Running Inference:  65%|██████▌   | 130/200 [03:33<01:08,  1.01it/s]Running Inference:  66%|██████▌   | 131/200 [03:34<01:13,  1.07s/it]Running Inference:  66%|██████▌   | 132/200 [03:38<02:19,  2.05s/it]Running Inference:  66%|██████▋   | 133/200 [03:40<02:09,  1.94s/it]Running Inference:  67%|██████▋   | 134/200 [03:43<02:30,  2.28s/it]Running Inference:  68%|██████▊   | 135/200 [03:48<03:08,  2.89s/it]Running Inference:  68%|██████▊   | 136/200 [03:48<02:23,  2.24s/it]Running Inference:  68%|██████▊   | 137/200 [03:49<02:00,  1.91s/it]Running Inference:  69%|██████▉   | 138/200 [03:50<01:42,  1.66s/it]Running Inference:  70%|██████▉   | 139/200 [03:53<01:49,  1.79s/it]Running Inference:  70%|███████   | 140/200 [03:54<01:44,  1.74s/it]Running Inference:  70%|███████   | 141/200 [03:58<02:18,  2.35s/it]Running Inference:  71%|███████   | 142/200 [03:59<01:54,  1.97s/it]Running Inference:  72%|███████▏  | 143/200 [04:00<01:36,  1.70s/it]Running Inference:  72%|███████▏  | 144/200 [04:03<01:56,  2.08s/it]Running Inference:  72%|███████▎  | 145/200 [04:05<01:50,  2.01s/it]Running Inference:  73%|███████▎  | 146/200 [04:06<01:32,  1.72s/it]Running Inference:  74%|███████▎  | 147/200 [04:07<01:16,  1.44s/it]Running Inference:  74%|███████▍  | 148/200 [04:08<01:06,  1.28s/it]Running Inference:  74%|███████▍  | 149/200 [04:09<01:01,  1.20s/it]Running Inference:  75%|███████▌  | 150/200 [04:11<01:14,  1.48s/it]Running Inference:  76%|███████▌  | 151/200 [04:11<00:55,  1.12s/it]Running Inference:  76%|███████▌  | 152/200 [04:12<00:46,  1.04it/s]Running Inference:  76%|███████▋  | 153/200 [04:12<00:42,  1.10it/s]Running Inference:  77%|███████▋  | 154/200 [04:15<01:08,  1.49s/it]Running Inference:  78%|███████▊  | 155/200 [04:17<01:11,  1.59s/it]Running Inference:  78%|███████▊  | 156/200 [04:19<01:15,  1.71s/it]Running Inference:  78%|███████▊  | 157/200 [04:20<00:57,  1.34s/it]Running Inference:  79%|███████▉  | 158/200 [04:22<01:15,  1.79s/it]Running Inference:  80%|███████▉  | 159/200 [04:26<01:38,  2.40s/it]Running Inference:  80%|████████  | 160/200 [04:30<01:52,  2.80s/it]Running Inference:  80%|████████  | 161/200 [04:32<01:34,  2.43s/it]Running Inference:  81%|████████  | 162/200 [04:35<01:38,  2.58s/it]Running Inference:  82%|████████▏ | 163/200 [04:36<01:19,  2.14s/it]Running Inference:  82%|████████▏ | 164/200 [04:37<01:10,  1.96s/it]Running Inference:  82%|████████▎ | 165/200 [04:38<00:56,  1.62s/it]Running Inference:  83%|████████▎ | 166/200 [04:40<00:59,  1.76s/it]Running Inference:  84%|████████▎ | 167/200 [04:42<00:59,  1.82s/it]Running Inference:  84%|████████▍ | 168/200 [04:43<00:45,  1.44s/it]Running Inference:  84%|████████▍ | 169/200 [04:43<00:34,  1.11s/it]Running Inference:  85%|████████▌ | 170/200 [04:45<00:42,  1.42s/it]Running Inference:  86%|████████▌ | 171/200 [04:48<00:54,  1.88s/it]Running Inference:  86%|████████▌ | 172/200 [04:49<00:45,  1.62s/it]Running Inference:  86%|████████▋ | 173/200 [04:50<00:39,  1.45s/it]Running Inference:  87%|████████▋ | 174/200 [04:53<00:48,  1.87s/it]Running Inference:  88%|████████▊ | 175/200 [04:57<01:02,  2.52s/it]Running Inference:  88%|████████▊ | 176/200 [05:00<01:03,  2.66s/it]Running Inference:  88%|████████▊ | 177/200 [05:02<00:55,  2.43s/it]Running Inference:  89%|████████▉ | 178/200 [05:04<00:50,  2.32s/it]Running Inference:  90%|████████▉ | 179/200 [05:08<00:58,  2.77s/it]Running Inference:  90%|█████████ | 180/200 [05:09<00:46,  2.34s/it]Running Inference:  90%|█████████ | 181/200 [05:10<00:35,  1.87s/it]Running Inference:  91%|█████████ | 182/200 [05:12<00:33,  1.85s/it]Running Inference:  92%|█████████▏| 183/200 [05:13<00:29,  1.76s/it]Running Inference:  92%|█████████▏| 184/200 [05:14<00:23,  1.46s/it]Running Inference:  92%|█████████▎| 185/200 [05:16<00:25,  1.73s/it]Running Inference:  93%|█████████▎| 186/200 [05:17<00:19,  1.42s/it]Running Inference:  94%|█████████▎| 187/200 [05:18<00:17,  1.33s/it]Running Inference:  94%|█████████▍| 188/200 [05:19<00:14,  1.19s/it]Running Inference:  94%|█████████▍| 189/200 [05:22<00:18,  1.70s/it]Running Inference:  95%|█████████▌| 190/200 [05:22<00:12,  1.29s/it]Running Inference:  96%|█████████▌| 191/200 [05:24<00:12,  1.39s/it]Running Inference:  96%|█████████▌| 192/200 [05:25<00:10,  1.32s/it]Running Inference:  96%|█████████▋| 193/200 [05:28<00:13,  1.94s/it]Running Inference:  97%|█████████▋| 194/200 [05:29<00:09,  1.51s/it]Running Inference:  98%|█████████▊| 195/200 [05:32<00:10,  2.01s/it]Running Inference:  98%|█████████▊| 196/200 [05:33<00:06,  1.75s/it]Running Inference:  98%|█████████▊| 197/200 [05:34<00:04,  1.57s/it]Running Inference:  99%|█████████▉| 198/200 [05:35<00:02,  1.29s/it]Running Inference: 100%|█████████▉| 199/200 [05:36<00:01,  1.15s/it]Running Inference: 100%|██████████| 200/200 [05:36<00:00,  1.01it/s]Running Inference: 100%|██████████| 200/200 [05:36<00:00,  1.68s/it]
2025-12-13 18:06:24,688 - INFO - Inference completed.
2025-12-13 18:06:24,696 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 18:06:24,696 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:06:24,701 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 18:06:24,701 - INFO - Metrics:
27.88
2025-12-13 18:06:24,702 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa, ratio=0.2) on GPU 1

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:06:31,217 - INFO - Set deterministic seeds to 42
2025-12-13 18:06:31,218 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:06:31,218 - INFO - Starting evaluation run...
2025-12-13 18:06:31,218 - INFO - Output directory set to: longbenchresult
2025-12-13 18:06:31,218 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 18:06:31,218 - INFO - KV Press 'tova' setup.
2025-12-13 18:06:31,218 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:06:31,218 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.72it/s]
Device set to use cuda:0
2025-12-13 18:06:45,711 - INFO - Model pipeline loaded.
2025-12-13 18:06:45,711 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:06:49,962 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:06:49,962 - INFO - Dataset processed with 200 entries.
2025-12-13 18:06:49,981 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:06,  4.25s/it]Running Inference:   1%|          | 2/200 [00:07<11:23,  3.45s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:48,  3.29s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:14,  2.83s/it]Running Inference:   2%|▎         | 5/200 [00:16<10:45,  3.31s/it]Running Inference:   3%|▎         | 6/200 [00:18<08:47,  2.72s/it]Running Inference:   4%|▎         | 7/200 [00:19<07:03,  2.19s/it]Running Inference:   4%|▍         | 8/200 [00:22<07:59,  2.50s/it]Running Inference:   4%|▍         | 9/200 [00:23<06:44,  2.12s/it]Running Inference:   5%|▌         | 10/200 [00:24<05:15,  1.66s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:25<05:02,  1.60s/it]Running Inference:   6%|▌         | 12/200 [00:27<04:55,  1.57s/it]Running Inference:   6%|▋         | 13/200 [00:30<06:41,  2.15s/it]Running Inference:   7%|▋         | 14/200 [00:31<05:09,  1.67s/it]Running Inference:   8%|▊         | 15/200 [00:32<04:28,  1.45s/it]Running Inference:   8%|▊         | 16/200 [00:33<03:57,  1.29s/it]Running Inference:   8%|▊         | 17/200 [00:34<04:11,  1.37s/it]Running Inference:   9%|▉         | 18/200 [00:35<03:48,  1.26s/it]Running Inference:  10%|▉         | 19/200 [00:38<05:09,  1.71s/it]Running Inference:  10%|█         | 20/200 [00:38<03:51,  1.29s/it]Running Inference:  10%|█         | 21/200 [00:40<04:33,  1.53s/it]Running Inference:  11%|█         | 22/200 [00:43<05:38,  1.90s/it]Running Inference:  12%|█▏        | 23/200 [00:45<05:08,  1.74s/it]Running Inference:  12%|█▏        | 24/200 [00:45<04:20,  1.48s/it]Running Inference:  12%|█▎        | 25/200 [00:46<03:56,  1.35s/it]Running Inference:  13%|█▎        | 26/200 [00:48<03:53,  1.34s/it]Running Inference:  14%|█▎        | 27/200 [00:48<03:15,  1.13s/it]Running Inference:  14%|█▍        | 28/200 [00:49<02:50,  1.01it/s]Running Inference:  14%|█▍        | 29/200 [00:52<04:15,  1.50s/it]Running Inference:  15%|█▌        | 30/200 [00:52<03:12,  1.13s/it]Running Inference:  16%|█▌        | 31/200 [00:53<03:25,  1.22s/it]Running Inference:  16%|█▌        | 32/200 [00:56<04:49,  1.73s/it]Running Inference:  16%|█▋        | 33/200 [00:57<04:09,  1.49s/it]Running Inference:  17%|█▋        | 34/200 [00:58<03:42,  1.34s/it]Running Inference:  18%|█▊        | 35/200 [00:59<03:15,  1.18s/it]Running Inference:  18%|█▊        | 36/200 [01:00<03:00,  1.10s/it]Running Inference:  18%|█▊        | 37/200 [01:03<04:34,  1.68s/it]Running Inference:  19%|█▉        | 38/200 [01:05<04:23,  1.63s/it]Running Inference:  20%|█▉        | 39/200 [01:07<05:20,  1.99s/it]Running Inference:  20%|██        | 40/200 [01:09<05:18,  1.99s/it]Running Inference:  20%|██        | 41/200 [01:12<05:57,  2.25s/it]Running Inference:  21%|██        | 42/200 [01:14<05:10,  1.96s/it]Running Inference:  22%|██▏       | 43/200 [01:14<04:18,  1.65s/it]Running Inference:  22%|██▏       | 44/200 [01:15<03:29,  1.34s/it]Running Inference:  22%|██▎       | 45/200 [01:16<03:11,  1.24s/it]Running Inference:  23%|██▎       | 46/200 [01:18<04:05,  1.60s/it]Running Inference:  24%|██▎       | 47/200 [01:19<03:35,  1.41s/it]Running Inference:  24%|██▍       | 48/200 [01:20<03:08,  1.24s/it]Running Inference:  24%|██▍       | 49/200 [01:23<04:28,  1.78s/it]Running Inference:  25%|██▌       | 50/200 [01:24<03:49,  1.53s/it]Running Inference:  26%|██▌       | 51/200 [01:26<04:16,  1.72s/it]Running Inference:  26%|██▌       | 52/200 [01:27<03:15,  1.32s/it]Running Inference:  26%|██▋       | 53/200 [01:29<04:11,  1.71s/it]Running Inference:  27%|██▋       | 54/200 [01:31<03:58,  1.63s/it]Running Inference:  28%|██▊       | 55/200 [01:32<03:22,  1.40s/it]Running Inference:  28%|██▊       | 56/200 [01:33<03:15,  1.36s/it]Running Inference:  28%|██▊       | 57/200 [01:34<02:51,  1.20s/it]Running Inference:  29%|██▉       | 58/200 [01:37<04:08,  1.75s/it]Running Inference:  30%|██▉       | 59/200 [01:38<03:26,  1.46s/it]Running Inference:  30%|███       | 60/200 [01:39<03:07,  1.34s/it]Running Inference:  30%|███       | 61/200 [01:39<02:37,  1.14s/it]Running Inference:  31%|███       | 62/200 [01:40<02:31,  1.10s/it]Running Inference:  32%|███▏      | 63/200 [01:41<01:56,  1.18it/s]Running Inference:  32%|███▏      | 64/200 [01:45<04:00,  1.77s/it]Running Inference:  32%|███▎      | 65/200 [01:45<03:06,  1.38s/it]Running Inference:  33%|███▎      | 66/200 [01:46<02:56,  1.32s/it]Running Inference:  34%|███▎      | 67/200 [01:49<04:06,  1.85s/it]Running Inference:  34%|███▍      | 68/200 [01:51<04:12,  1.91s/it]Running Inference:  34%|███▍      | 69/200 [01:54<04:41,  2.15s/it]Running Inference:  35%|███▌      | 70/200 [01:55<03:54,  1.81s/it]Running Inference:  36%|███▌      | 71/200 [01:56<03:06,  1.45s/it]Running Inference:  36%|███▌      | 72/200 [01:59<04:10,  1.95s/it]Running Inference:  36%|███▋      | 73/200 [02:00<03:36,  1.70s/it]Running Inference:  37%|███▋      | 74/200 [02:02<03:50,  1.83s/it]Running Inference:  38%|███▊      | 75/200 [02:03<03:22,  1.62s/it]Running Inference:  38%|███▊      | 76/200 [02:04<03:04,  1.49s/it]Running Inference:  38%|███▊      | 77/200 [02:05<02:48,  1.37s/it]Running Inference:  39%|███▉      | 78/200 [02:09<03:55,  1.93s/it]Running Inference:  40%|███▉      | 79/200 [02:12<04:30,  2.24s/it]Running Inference:  40%|████      | 80/200 [02:15<05:11,  2.60s/it]Running Inference:  40%|████      | 81/200 [02:18<05:14,  2.64s/it]Running Inference:  41%|████      | 82/200 [02:21<05:23,  2.74s/it]Running Inference:  42%|████▏     | 83/200 [02:22<04:38,  2.38s/it]Running Inference:  42%|████▏     | 84/200 [02:23<03:45,  1.95s/it]Running Inference:  42%|████▎     | 85/200 [02:25<03:31,  1.84s/it]Running Inference:  43%|████▎     | 86/200 [02:30<05:16,  2.77s/it]Running Inference:  44%|████▎     | 87/200 [02:30<04:00,  2.13s/it]Running Inference:  44%|████▍     | 88/200 [02:33<04:23,  2.36s/it]Running Inference:  44%|████▍     | 89/200 [02:35<03:45,  2.03s/it]Running Inference:  45%|████▌     | 90/200 [02:38<04:16,  2.33s/it]Running Inference:  46%|████▌     | 91/200 [02:40<04:21,  2.40s/it]Running Inference:  46%|████▌     | 92/200 [02:41<03:30,  1.95s/it]Running Inference:  46%|████▋     | 93/200 [02:42<03:00,  1.68s/it]Running Inference:  47%|████▋     | 94/200 [02:43<02:18,  1.31s/it]Running Inference:  48%|████▊     | 95/200 [02:43<01:59,  1.13s/it]Running Inference:  48%|████▊     | 96/200 [02:44<01:52,  1.08s/it]Running Inference:  48%|████▊     | 97/200 [02:46<02:14,  1.30s/it]Running Inference:  49%|████▉     | 98/200 [02:47<02:04,  1.22s/it]Running Inference:  50%|████▉     | 99/200 [02:48<01:48,  1.07s/it]Running Inference:  50%|█████     | 100/200 [02:49<01:37,  1.03it/s]Running Inference:  50%|█████     | 101/200 [02:51<02:28,  1.50s/it]Running Inference:  51%|█████     | 102/200 [02:52<02:08,  1.32s/it]Running Inference:  52%|█████▏    | 103/200 [02:55<02:47,  1.73s/it]Running Inference:  52%|█████▏    | 104/200 [02:58<03:17,  2.06s/it]Running Inference:  52%|█████▎    | 105/200 [02:59<02:40,  1.69s/it]Running Inference:  53%|█████▎    | 106/200 [03:00<02:18,  1.47s/it]Running Inference:  54%|█████▎    | 107/200 [03:01<02:23,  1.54s/it]Running Inference:  54%|█████▍    | 108/200 [03:04<02:57,  1.93s/it]Running Inference:  55%|█████▍    | 109/200 [03:05<02:27,  1.62s/it]Running Inference:  55%|█████▌    | 110/200 [03:07<02:24,  1.61s/it]Running Inference:  56%|█████▌    | 111/200 [03:07<01:58,  1.33s/it]Running Inference:  56%|█████▌    | 112/200 [03:09<01:55,  1.31s/it]Running Inference:  56%|█████▋    | 113/200 [03:10<01:47,  1.23s/it]Running Inference:  57%|█████▋    | 114/200 [03:10<01:34,  1.10s/it]Running Inference:  57%|█████▊    | 115/200 [03:12<01:55,  1.36s/it]Running Inference:  58%|█████▊    | 116/200 [03:15<02:26,  1.74s/it]Running Inference:  58%|█████▊    | 117/200 [03:19<03:20,  2.41s/it]Running Inference:  59%|█████▉    | 118/200 [03:20<02:35,  1.89s/it]Running Inference:  60%|█████▉    | 119/200 [03:23<03:03,  2.27s/it]Running Inference:  60%|██████    | 120/200 [03:24<02:27,  1.84s/it]Running Inference:  60%|██████    | 121/200 [03:24<02:01,  1.54s/it]Running Inference:  61%|██████    | 122/200 [03:25<01:37,  1.26s/it]Running Inference:  62%|██████▏   | 123/200 [03:26<01:28,  1.15s/it]Running Inference:  62%|██████▏   | 124/200 [03:29<02:08,  1.69s/it]Running Inference:  62%|██████▎   | 125/200 [03:30<01:45,  1.41s/it]Running Inference:  63%|██████▎   | 126/200 [03:30<01:30,  1.22s/it]Running Inference:  64%|██████▎   | 127/200 [03:31<01:10,  1.03it/s]Running Inference:  64%|██████▍   | 128/200 [03:32<01:07,  1.06it/s]Running Inference:  64%|██████▍   | 129/200 [03:33<01:18,  1.11s/it]Running Inference:  65%|██████▌   | 130/200 [03:34<01:11,  1.03s/it]Running Inference:  66%|██████▌   | 131/200 [03:35<01:18,  1.13s/it]Running Inference:  66%|██████▌   | 132/200 [03:38<01:47,  1.58s/it]Running Inference:  66%|██████▋   | 133/200 [03:40<01:47,  1.60s/it]Running Inference:  67%|██████▋   | 134/200 [03:43<02:13,  2.02s/it]Running Inference:  68%|██████▊   | 135/200 [03:47<02:54,  2.69s/it]Running Inference:  68%|██████▊   | 136/200 [03:48<02:14,  2.10s/it]Running Inference:  68%|██████▊   | 137/200 [03:49<01:53,  1.80s/it]Running Inference:  69%|██████▉   | 138/200 [03:50<01:37,  1.58s/it]Running Inference:  70%|██████▉   | 139/200 [03:52<01:45,  1.73s/it]Running Inference:  70%|███████   | 140/200 [03:53<01:41,  1.69s/it]Running Inference:  70%|███████   | 141/200 [03:57<02:15,  2.29s/it]Running Inference:  71%|███████   | 142/200 [03:58<01:51,  1.92s/it]Running Inference:  72%|███████▏  | 143/200 [03:59<01:34,  1.66s/it]Running Inference:  72%|███████▏  | 144/200 [04:02<01:53,  2.02s/it]Running Inference:  72%|███████▎  | 145/200 [04:04<01:48,  1.97s/it]Running Inference:  73%|███████▎  | 146/200 [04:05<01:31,  1.69s/it]Running Inference:  74%|███████▎  | 147/200 [04:08<01:46,  2.02s/it]Running Inference:  74%|███████▍  | 148/200 [04:09<01:25,  1.64s/it]Running Inference:  74%|███████▍  | 149/200 [04:10<01:13,  1.45s/it]Running Inference:  75%|███████▌  | 150/200 [04:12<01:23,  1.67s/it]Running Inference:  76%|███████▌  | 151/200 [04:12<01:01,  1.25s/it]Running Inference:  76%|███████▌  | 152/200 [04:13<00:50,  1.05s/it]Running Inference:  76%|███████▋  | 153/200 [04:13<00:45,  1.04it/s]Running Inference:  77%|███████▋  | 154/200 [04:16<01:09,  1.50s/it]Running Inference:  78%|███████▊  | 155/200 [04:18<01:11,  1.58s/it]Running Inference:  78%|███████▊  | 156/200 [04:20<01:14,  1.70s/it]Running Inference:  78%|███████▊  | 157/200 [04:20<00:56,  1.32s/it]Running Inference:  79%|███████▉  | 158/200 [04:23<01:13,  1.75s/it]Running Inference:  80%|███████▉  | 159/200 [04:25<01:19,  1.94s/it]Running Inference:  80%|████████  | 160/200 [04:29<01:37,  2.44s/it]Running Inference:  80%|████████  | 161/200 [04:32<01:38,  2.53s/it]Running Inference:  81%|████████  | 162/200 [04:34<01:38,  2.59s/it]Running Inference:  82%|████████▏ | 163/200 [04:36<01:18,  2.13s/it]Running Inference:  82%|████████▏ | 164/200 [04:38<01:22,  2.29s/it]Running Inference:  82%|████████▎ | 165/200 [04:39<01:04,  1.84s/it]Running Inference:  83%|████████▎ | 166/200 [04:41<01:05,  1.91s/it]Running Inference:  84%|████████▎ | 167/200 [04:43<01:03,  1.92s/it]Running Inference:  84%|████████▍ | 168/200 [04:44<00:48,  1.51s/it]Running Inference:  84%|████████▍ | 169/200 [04:44<00:35,  1.16s/it]Running Inference:  85%|████████▌ | 170/200 [04:47<00:50,  1.69s/it]Running Inference:  86%|████████▌ | 171/200 [04:50<00:59,  2.04s/it]Running Inference:  86%|████████▌ | 172/200 [04:51<00:48,  1.73s/it]Running Inference:  86%|████████▋ | 173/200 [04:52<00:41,  1.52s/it]Running Inference:  87%|████████▋ | 174/200 [04:55<00:49,  1.89s/it]Running Inference:  88%|████████▊ | 175/200 [04:58<01:02,  2.51s/it]Running Inference:  88%|████████▊ | 176/200 [05:01<01:02,  2.62s/it]Running Inference:  88%|████████▊ | 177/200 [05:04<01:00,  2.62s/it]Running Inference:  89%|████████▉ | 178/200 [05:06<00:53,  2.45s/it]Running Inference:  90%|████████▉ | 179/200 [05:10<00:59,  2.83s/it]Running Inference:  90%|█████████ | 180/200 [05:11<00:47,  2.39s/it]Running Inference:  90%|█████████ | 181/200 [05:12<00:35,  1.89s/it]Running Inference:  91%|█████████ | 182/200 [05:14<00:33,  1.86s/it]Running Inference:  92%|█████████▏| 183/200 [05:15<00:29,  1.76s/it]Running Inference:  92%|█████████▏| 184/200 [05:16<00:23,  1.46s/it]Running Inference:  92%|█████████▎| 185/200 [05:18<00:25,  1.73s/it]Running Inference:  93%|█████████▎| 186/200 [05:19<00:19,  1.41s/it]Running Inference:  94%|█████████▎| 187/200 [05:20<00:17,  1.32s/it]Running Inference:  94%|█████████▍| 188/200 [05:21<00:14,  1.18s/it]Running Inference:  94%|█████████▍| 189/200 [05:22<00:11,  1.08s/it]Running Inference:  95%|█████████▌| 190/200 [05:22<00:08,  1.16it/s]Running Inference:  96%|█████████▌| 191/200 [05:24<00:09,  1.08s/it]Running Inference:  96%|█████████▌| 192/200 [05:25<00:08,  1.10s/it]Running Inference:  96%|█████████▋| 193/200 [05:29<00:13,  1.88s/it]Running Inference:  97%|█████████▋| 194/200 [05:29<00:08,  1.46s/it]Running Inference:  98%|█████████▊| 195/200 [05:32<00:09,  1.95s/it]Running Inference:  98%|█████████▊| 196/200 [05:33<00:06,  1.66s/it]Running Inference:  98%|█████████▊| 197/200 [05:34<00:04,  1.51s/it]Running Inference:  99%|█████████▉| 198/200 [05:35<00:02,  1.24s/it]Running Inference: 100%|█████████▉| 199/200 [05:36<00:01,  1.11s/it]Running Inference: 100%|██████████| 200/200 [05:36<00:00,  1.04it/s]Running Inference: 100%|██████████| 200/200 [05:36<00:00,  1.68s/it]
2025-12-13 18:12:26,766 - INFO - Inference completed.
2025-12-13 18:12:26,773 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-13 18:12:26,774 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:12:26,778 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-13 18:12:26,778 - INFO - Metrics:
26.89
2025-12-13 18:12:26,779 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa, ratio=0.3) on GPU 1

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:12:33,192 - INFO - Set deterministic seeds to 42
2025-12-13 18:12:33,192 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:12:33,192 - INFO - Starting evaluation run...
2025-12-13 18:12:33,192 - INFO - Output directory set to: longbenchresult
2025-12-13 18:12:33,193 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-13 18:12:33,193 - INFO - KV Press 'tova' setup.
2025-12-13 18:12:33,193 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:12:33,193 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.57it/s]
Device set to use cuda:0
2025-12-13 18:12:45,718 - INFO - Model pipeline loaded.
2025-12-13 18:12:45,718 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:12:52,990 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:12:52,990 - INFO - Dataset processed with 200 entries.
2025-12-13 18:12:53,010 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:17,  4.31s/it]Running Inference:   1%|          | 2/200 [00:07<11:33,  3.50s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:56,  3.33s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:19,  2.85s/it]Running Inference:   2%|▎         | 5/200 [00:16<10:34,  3.25s/it]Running Inference:   3%|▎         | 6/200 [00:17<08:35,  2.66s/it]Running Inference:   4%|▎         | 7/200 [00:19<06:54,  2.15s/it]Running Inference:   4%|▍         | 8/200 [00:22<07:56,  2.48s/it]Running Inference:   4%|▍         | 9/200 [00:23<06:36,  2.07s/it]Running Inference:   5%|▌         | 10/200 [00:24<05:07,  1.62s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:25<04:53,  1.55s/it]Running Inference:   6%|▌         | 12/200 [00:26<04:50,  1.54s/it]Running Inference:   6%|▋         | 13/200 [00:30<06:40,  2.14s/it]Running Inference:   7%|▋         | 14/200 [00:31<05:09,  1.66s/it]Running Inference:   8%|▊         | 15/200 [00:31<04:27,  1.44s/it]Running Inference:   8%|▊         | 16/200 [00:32<03:57,  1.29s/it]Running Inference:   8%|▊         | 17/200 [00:34<04:11,  1.37s/it]Running Inference:   9%|▉         | 18/200 [00:35<03:48,  1.25s/it]Running Inference:  10%|▉         | 19/200 [00:38<05:10,  1.71s/it]Running Inference:  10%|█         | 20/200 [00:38<03:52,  1.29s/it]Running Inference:  10%|█         | 21/200 [00:40<04:33,  1.53s/it]Running Inference:  11%|█         | 22/200 [00:41<03:52,  1.31s/it]Running Inference:  12%|█▏        | 23/200 [00:42<03:55,  1.33s/it]Running Inference:  12%|█▏        | 24/200 [00:45<05:12,  1.78s/it]Running Inference:  12%|█▎        | 25/200 [00:46<04:33,  1.56s/it]Running Inference:  13%|█▎        | 26/200 [00:47<04:19,  1.49s/it]Running Inference:  14%|█▎        | 27/200 [00:48<03:34,  1.24s/it]Running Inference:  14%|█▍        | 28/200 [00:49<03:03,  1.07s/it]Running Inference:  14%|█▍        | 29/200 [00:52<04:27,  1.57s/it]Running Inference:  15%|█▌        | 30/200 [00:52<03:20,  1.18s/it]Running Inference:  16%|█▌        | 31/200 [00:53<03:14,  1.15s/it]Running Inference:  16%|█▌        | 32/200 [00:56<04:45,  1.70s/it]Running Inference:  16%|█▋        | 33/200 [00:57<04:08,  1.49s/it]Running Inference:  17%|█▋        | 34/200 [00:58<03:42,  1.34s/it]Running Inference:  18%|█▊        | 35/200 [00:59<03:15,  1.18s/it]Running Inference:  18%|█▊        | 36/200 [01:00<03:00,  1.10s/it]Running Inference:  18%|█▊        | 37/200 [01:03<04:36,  1.70s/it]Running Inference:  19%|█▉        | 38/200 [01:04<04:25,  1.64s/it]Running Inference:  20%|█▉        | 39/200 [01:07<05:24,  2.02s/it]Running Inference:  20%|██        | 40/200 [01:09<05:33,  2.08s/it]Running Inference:  20%|██        | 41/200 [01:10<04:40,  1.76s/it]Running Inference:  21%|██        | 42/200 [01:11<03:59,  1.51s/it]Running Inference:  22%|██▏       | 43/200 [01:12<03:28,  1.33s/it]Running Inference:  22%|██▏       | 44/200 [01:13<02:54,  1.12s/it]Running Inference:  22%|██▎       | 45/200 [01:14<02:49,  1.09s/it]Running Inference:  23%|██▎       | 46/200 [01:16<03:49,  1.49s/it]Running Inference:  24%|██▎       | 47/200 [01:17<03:24,  1.34s/it]Running Inference:  24%|██▍       | 48/200 [01:18<03:01,  1.19s/it]Running Inference:  24%|██▍       | 49/200 [01:21<04:25,  1.76s/it]Running Inference:  25%|██▌       | 50/200 [01:22<03:38,  1.45s/it]Running Inference:  26%|██▌       | 51/200 [01:24<04:09,  1.68s/it]Running Inference:  26%|██▌       | 52/200 [01:25<03:11,  1.29s/it]Running Inference:  26%|██▋       | 53/200 [01:25<02:52,  1.17s/it]Running Inference:  27%|██▋       | 54/200 [01:27<03:03,  1.26s/it]Running Inference:  28%|██▊       | 55/200 [01:28<02:44,  1.14s/it]Running Inference:  28%|██▊       | 56/200 [01:29<02:49,  1.18s/it]Running Inference:  28%|██▊       | 57/200 [01:30<02:31,  1.06s/it]Running Inference:  29%|██▉       | 58/200 [01:31<02:42,  1.15s/it]Running Inference:  30%|██▉       | 59/200 [01:32<02:26,  1.04s/it]Running Inference:  30%|███       | 60/200 [01:33<02:25,  1.04s/it]Running Inference:  30%|███       | 61/200 [01:34<02:09,  1.07it/s]Running Inference:  31%|███       | 62/200 [01:35<02:12,  1.04it/s]Running Inference:  32%|███▏      | 63/200 [01:35<01:42,  1.33it/s]Running Inference:  32%|███▏      | 64/200 [01:39<03:53,  1.72s/it]Running Inference:  32%|███▎      | 65/200 [01:39<03:01,  1.34s/it]Running Inference:  33%|███▎      | 66/200 [01:41<02:54,  1.31s/it]Running Inference:  34%|███▎      | 67/200 [01:44<04:07,  1.86s/it]Running Inference:  34%|███▍      | 68/200 [01:46<04:14,  1.93s/it]Running Inference:  34%|███▍      | 69/200 [01:49<04:47,  2.20s/it]Running Inference:  35%|███▌      | 70/200 [01:50<03:59,  1.84s/it]Running Inference:  36%|███▌      | 71/200 [01:50<03:10,  1.47s/it]Running Inference:  36%|███▌      | 72/200 [01:53<04:15,  1.99s/it]Running Inference:  36%|███▋      | 73/200 [01:54<03:34,  1.69s/it]Running Inference:  37%|███▋      | 74/200 [01:57<03:50,  1.83s/it]Running Inference:  38%|███▊      | 75/200 [01:58<03:28,  1.67s/it]Running Inference:  38%|███▊      | 76/200 [01:59<03:09,  1.53s/it]Running Inference:  38%|███▊      | 77/200 [02:00<02:51,  1.40s/it]Running Inference:  39%|███▉      | 78/200 [02:02<02:52,  1.41s/it]Running Inference:  40%|███▉      | 79/200 [02:03<02:38,  1.31s/it]Running Inference:  40%|████      | 80/200 [02:06<03:56,  1.97s/it]Running Inference:  40%|████      | 81/200 [02:09<04:24,  2.22s/it]Running Inference:  41%|████      | 82/200 [02:12<04:52,  2.48s/it]Running Inference:  42%|████▏     | 83/200 [02:14<04:16,  2.19s/it]Running Inference:  42%|████▏     | 84/200 [02:15<03:30,  1.81s/it]Running Inference:  42%|████▎     | 85/200 [02:16<03:28,  1.82s/it]Running Inference:  43%|████▎     | 86/200 [02:20<04:35,  2.42s/it]Running Inference:  44%|████▎     | 87/200 [02:21<03:37,  1.92s/it]Running Inference:  44%|████▍     | 88/200 [02:24<04:10,  2.23s/it]Running Inference:  44%|████▍     | 89/200 [02:27<04:28,  2.41s/it]Running Inference:  45%|████▌     | 90/200 [02:30<04:46,  2.60s/it]Running Inference:  46%|████▌     | 91/200 [02:33<04:48,  2.64s/it]Running Inference:  46%|████▌     | 92/200 [02:33<03:48,  2.12s/it]Running Inference:  46%|████▋     | 93/200 [02:35<03:12,  1.80s/it]Running Inference:  47%|████▋     | 94/200 [02:35<02:27,  1.39s/it]Running Inference:  48%|████▊     | 95/200 [02:36<02:05,  1.19s/it]Running Inference:  48%|████▊     | 96/200 [02:37<01:56,  1.12s/it]Running Inference:  48%|████▊     | 97/200 [02:40<03:02,  1.77s/it]Running Inference:  49%|████▉     | 98/200 [02:41<02:37,  1.55s/it]Running Inference:  50%|████▉     | 99/200 [02:42<02:10,  1.29s/it]Running Inference:  50%|█████     | 100/200 [02:42<01:52,  1.13s/it]Running Inference:  50%|█████     | 101/200 [02:45<02:39,  1.61s/it]Running Inference:  51%|█████     | 102/200 [02:48<03:12,  1.96s/it]Running Inference:  52%|█████▏    | 103/200 [02:51<03:33,  2.20s/it]Running Inference:  52%|█████▏    | 104/200 [02:54<03:50,  2.40s/it]Running Inference:  52%|█████▎    | 105/200 [02:55<03:15,  2.06s/it]Running Inference:  53%|█████▎    | 106/200 [02:56<02:42,  1.73s/it]Running Inference:  54%|█████▎    | 107/200 [02:57<02:39,  1.72s/it]Running Inference:  54%|█████▍    | 108/200 [03:00<03:10,  2.07s/it]Running Inference:  55%|█████▍    | 109/200 [03:01<02:36,  1.72s/it]Running Inference:  55%|█████▌    | 110/200 [03:03<02:31,  1.68s/it]Running Inference:  56%|█████▌    | 111/200 [03:03<02:02,  1.38s/it]Running Inference:  56%|█████▌    | 112/200 [03:05<01:58,  1.35s/it]Running Inference:  56%|█████▋    | 113/200 [03:06<01:49,  1.26s/it]Running Inference:  57%|█████▋    | 114/200 [03:09<02:25,  1.69s/it]Running Inference:  57%|█████▊    | 115/200 [03:10<02:30,  1.77s/it]Running Inference:  58%|█████▊    | 116/200 [03:13<02:50,  2.04s/it]Running Inference:  58%|█████▊    | 117/200 [03:15<02:57,  2.14s/it]Running Inference:  59%|█████▉    | 118/200 [03:16<02:19,  1.70s/it]Running Inference:  60%|█████▉    | 119/200 [03:19<02:53,  2.15s/it]Running Inference:  60%|██████    | 120/200 [03:20<02:20,  1.76s/it]Running Inference:  60%|██████    | 121/200 [03:21<01:56,  1.48s/it]Running Inference:  61%|██████    | 122/200 [03:22<01:34,  1.21s/it]Running Inference:  62%|██████▏   | 123/200 [03:23<01:26,  1.12s/it]Running Inference:  62%|██████▏   | 124/200 [03:26<02:08,  1.69s/it]Running Inference:  62%|██████▎   | 125/200 [03:26<01:45,  1.40s/it]Running Inference:  63%|██████▎   | 126/200 [03:27<01:30,  1.22s/it]Running Inference:  64%|██████▎   | 127/200 [03:27<01:10,  1.03it/s]Running Inference:  64%|██████▍   | 128/200 [03:29<01:19,  1.11s/it]Running Inference:  64%|██████▍   | 129/200 [03:30<01:27,  1.23s/it]Running Inference:  65%|██████▌   | 130/200 [03:31<01:17,  1.11s/it]Running Inference:  66%|██████▌   | 131/200 [03:32<01:10,  1.01s/it]Running Inference:  66%|██████▌   | 132/200 [03:34<01:35,  1.41s/it]Running Inference:  66%|██████▋   | 133/200 [03:36<01:39,  1.49s/it]Running Inference:  67%|██████▋   | 134/200 [03:37<01:29,  1.35s/it]Running Inference:  68%|██████▊   | 135/200 [03:41<02:25,  2.23s/it]Running Inference:  68%|██████▊   | 136/200 [03:42<01:54,  1.79s/it]Running Inference:  68%|██████▊   | 137/200 [03:43<01:38,  1.56s/it]Running Inference:  69%|██████▉   | 138/200 [03:44<01:26,  1.39s/it]Running Inference:  70%|██████▉   | 139/200 [03:46<01:40,  1.65s/it]Running Inference:  70%|███████   | 140/200 [03:48<01:37,  1.63s/it]Running Inference:  70%|███████   | 141/200 [03:52<02:13,  2.27s/it]Running Inference:  71%|███████   | 142/200 [03:53<01:50,  1.91s/it]Running Inference:  72%|███████▏  | 143/200 [03:56<02:03,  2.16s/it]Running Inference:  72%|███████▏  | 144/200 [03:58<02:13,  2.39s/it]Running Inference:  72%|███████▎  | 145/200 [04:00<02:02,  2.22s/it]Running Inference:  73%|███████▎  | 146/200 [04:01<01:40,  1.86s/it]Running Inference:  74%|███████▎  | 147/200 [04:03<01:30,  1.70s/it]Running Inference:  74%|███████▍  | 148/200 [04:03<01:13,  1.42s/it]Running Inference:  74%|███████▍  | 149/200 [04:04<01:06,  1.30s/it]Running Inference:  75%|███████▌  | 150/200 [04:07<01:20,  1.61s/it]Running Inference:  76%|███████▌  | 151/200 [04:07<00:59,  1.21s/it]Running Inference:  76%|███████▌  | 152/200 [04:08<00:49,  1.02s/it]Running Inference:  76%|███████▋  | 153/200 [04:08<00:44,  1.06it/s]Running Inference:  77%|███████▋  | 154/200 [04:11<01:09,  1.51s/it]Running Inference:  78%|███████▊  | 155/200 [04:14<01:25,  1.90s/it]Running Inference:  78%|███████▊  | 156/200 [04:15<01:17,  1.76s/it]Running Inference:  78%|███████▊  | 157/200 [04:16<00:58,  1.37s/it]Running Inference:  79%|███████▉  | 158/200 [04:19<01:15,  1.80s/it]Running Inference:  80%|███████▉  | 159/200 [04:21<01:21,  1.98s/it]Running Inference:  80%|████████  | 160/200 [04:24<01:33,  2.34s/it]Running Inference:  80%|████████  | 161/200 [04:26<01:22,  2.12s/it]Running Inference:  81%|████████  | 162/200 [04:27<01:11,  1.88s/it]Running Inference:  82%|████████▏ | 163/200 [04:28<01:01,  1.66s/it]Running Inference:  82%|████████▏ | 164/200 [04:31<01:11,  1.98s/it]Running Inference:  82%|████████▎ | 165/200 [04:32<00:56,  1.62s/it]Running Inference:  83%|████████▎ | 166/200 [04:34<01:00,  1.77s/it]Running Inference:  84%|████████▎ | 167/200 [04:36<01:00,  1.83s/it]Running Inference:  84%|████████▍ | 168/200 [04:36<00:46,  1.44s/it]Running Inference:  84%|████████▍ | 169/200 [04:37<00:34,  1.12s/it]Running Inference:  85%|████████▌ | 170/200 [04:39<00:42,  1.42s/it]Running Inference:  86%|████████▌ | 171/200 [04:42<00:54,  1.87s/it]Running Inference:  86%|████████▌ | 172/200 [04:43<00:45,  1.61s/it]Running Inference:  86%|████████▋ | 173/200 [04:44<00:38,  1.44s/it]Running Inference:  87%|████████▋ | 174/200 [04:45<00:33,  1.28s/it]Running Inference:  88%|████████▊ | 175/200 [04:49<00:52,  2.10s/it]Running Inference:  88%|████████▊ | 176/200 [04:52<00:56,  2.35s/it]Running Inference:  88%|████████▊ | 177/200 [04:54<00:56,  2.45s/it]Running Inference:  89%|████████▉ | 178/200 [04:56<00:51,  2.33s/it]Running Inference:  90%|████████▉ | 179/200 [05:00<00:58,  2.76s/it]Running Inference:  90%|█████████ | 180/200 [05:02<00:46,  2.34s/it]Running Inference:  90%|█████████ | 181/200 [05:02<00:35,  1.86s/it]Running Inference:  91%|█████████ | 182/200 [05:04<00:33,  1.84s/it]Running Inference:  92%|█████████▏| 183/200 [05:06<00:29,  1.74s/it]Running Inference:  92%|█████████▏| 184/200 [05:08<00:32,  2.03s/it]Running Inference:  92%|█████████▎| 185/200 [05:11<00:31,  2.13s/it]Running Inference:  93%|█████████▎| 186/200 [05:11<00:23,  1.70s/it]Running Inference:  94%|█████████▎| 187/200 [05:13<00:19,  1.52s/it]Running Inference:  94%|█████████▍| 188/200 [05:13<00:15,  1.32s/it]Running Inference:  94%|█████████▍| 189/200 [05:15<00:14,  1.29s/it]Running Inference:  95%|█████████▌| 190/200 [05:15<00:10,  1.01s/it]Running Inference:  96%|█████████▌| 191/200 [05:19<00:16,  1.82s/it]Running Inference:  96%|█████████▌| 192/200 [05:20<00:13,  1.65s/it]Running Inference:  96%|█████████▋| 193/200 [05:22<00:11,  1.68s/it]Running Inference:  97%|█████████▋| 194/200 [05:22<00:07,  1.32s/it]Running Inference:  98%|█████████▊| 195/200 [05:25<00:09,  1.87s/it]Running Inference:  98%|█████████▊| 196/200 [05:26<00:06,  1.61s/it]Running Inference:  98%|█████████▊| 197/200 [05:27<00:04,  1.47s/it]Running Inference:  99%|█████████▉| 198/200 [05:28<00:02,  1.21s/it]Running Inference: 100%|█████████▉| 199/200 [05:29<00:01,  1.09s/it]Running Inference: 100%|██████████| 200/200 [05:29<00:00,  1.05it/s]Running Inference: 100%|██████████| 200/200 [05:29<00:00,  1.65s/it]
2025-12-13 18:18:22,997 - INFO - Inference completed.
2025-12-13 18:18:23,005 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-13 18:18:23,005 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:18:23,009 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-13 18:18:23,009 - INFO - Metrics:
29.58
2025-12-13 18:18:23,011 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa, ratio=0.5) on GPU 1


========================================
LongBench Task: hotpotqa
========================================
----------------------------------------
Task: hotpotqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:18:29,561 - INFO - Set deterministic seeds to 42
2025-12-13 18:18:29,561 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:18:29,561 - INFO - Starting evaluation run...
2025-12-13 18:18:29,561 - INFO - Output directory set to: longbenchresult
2025-12-13 18:18:29,561 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 18:18:29,561 - INFO - KV Press 'tova' setup.
2025-12-13 18:18:29,561 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:18:29,561 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.40it/s]
Device set to use cuda:0
2025-12-13 18:18:41,198 - INFO - Model pipeline loaded.
2025-12-13 18:18:41,198 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 938.62 examples/s]Generating test split: 200 examples [00:00, 934.26 examples/s]
2025-12-13 18:18:45,974 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:18:45,974 - INFO - Dataset processed with 200 entries.
2025-12-13 18:18:46,008 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:07,  3.36s/it]Running Inference:   1%|          | 2/200 [00:03<05:48,  1.76s/it]Running Inference:   2%|▏         | 3/200 [00:06<06:56,  2.11s/it]Running Inference:   2%|▏         | 4/200 [00:08<07:08,  2.19s/it]Running Inference:   2%|▎         | 5/200 [00:11<07:58,  2.45s/it]Running Inference:   3%|▎         | 6/200 [00:13<06:45,  2.09s/it]Running Inference:   4%|▎         | 7/200 [00:15<06:39,  2.07s/it]Running Inference:   4%|▍         | 8/200 [00:16<05:46,  1.80s/it]Running Inference:   4%|▍         | 9/200 [00:18<05:42,  1.79s/it]Running Inference:   5%|▌         | 10/200 [00:19<04:49,  1.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<04:47,  1.52s/it]Running Inference:   6%|▌         | 12/200 [00:21<04:30,  1.44s/it]Running Inference:   6%|▋         | 13/200 [00:23<04:59,  1.60s/it]Running Inference:   7%|▋         | 14/200 [00:26<05:32,  1.79s/it]Running Inference:   8%|▊         | 15/200 [00:28<05:58,  1.94s/it]Running Inference:   8%|▊         | 16/200 [00:30<06:15,  2.04s/it]Running Inference:   8%|▊         | 17/200 [00:34<08:13,  2.70s/it]Running Inference:   9%|▉         | 18/200 [00:37<07:43,  2.55s/it]Running Inference:  10%|▉         | 19/200 [00:37<06:01,  2.00s/it]Running Inference:  10%|█         | 20/200 [00:40<07:03,  2.35s/it]Running Inference:  10%|█         | 21/200 [00:41<05:34,  1.87s/it]Running Inference:  11%|█         | 22/200 [00:42<04:22,  1.47s/it]Running Inference:  12%|█▏        | 23/200 [00:43<04:30,  1.53s/it]Running Inference:  12%|█▏        | 24/200 [00:48<06:46,  2.31s/it]Running Inference:  12%|█▎        | 25/200 [00:51<07:40,  2.63s/it]Running Inference:  13%|█▎        | 26/200 [00:52<05:55,  2.04s/it]Running Inference:  14%|█▎        | 27/200 [00:55<07:15,  2.52s/it]Running Inference:  14%|█▍        | 28/200 [00:58<07:12,  2.52s/it]Running Inference:  14%|█▍        | 29/200 [01:02<08:39,  3.04s/it]Running Inference:  15%|█▌        | 30/200 [01:04<08:04,  2.85s/it]Running Inference:  16%|█▌        | 31/200 [01:06<06:43,  2.39s/it]Running Inference:  16%|█▌        | 32/200 [01:10<08:16,  2.96s/it]Running Inference:  16%|█▋        | 33/200 [01:12<07:46,  2.79s/it]Running Inference:  17%|█▋        | 34/200 [01:15<07:13,  2.61s/it]Running Inference:  18%|█▊        | 35/200 [01:17<06:54,  2.51s/it]Running Inference:  18%|█▊        | 36/200 [01:19<06:47,  2.48s/it]Running Inference:  18%|█▊        | 37/200 [01:22<06:39,  2.45s/it]Running Inference:  19%|█▉        | 38/200 [01:24<06:26,  2.39s/it]Running Inference:  20%|█▉        | 39/200 [01:27<06:46,  2.53s/it]Running Inference:  20%|██        | 40/200 [01:29<06:40,  2.50s/it]Running Inference:  20%|██        | 41/200 [01:31<06:16,  2.37s/it]Running Inference:  21%|██        | 42/200 [01:33<06:03,  2.30s/it]Running Inference:  22%|██▏       | 43/200 [01:35<05:11,  1.98s/it]Running Inference:  22%|██▏       | 44/200 [01:37<05:41,  2.19s/it]Running Inference:  22%|██▎       | 45/200 [01:41<06:31,  2.52s/it]Running Inference:  23%|██▎       | 46/200 [01:43<06:27,  2.52s/it]Running Inference:  24%|██▎       | 47/200 [01:47<07:45,  3.04s/it]Running Inference:  24%|██▍       | 48/200 [01:49<06:29,  2.56s/it]Running Inference:  24%|██▍       | 49/200 [01:51<06:07,  2.43s/it]Running Inference:  25%|██▌       | 50/200 [01:53<05:32,  2.22s/it]Running Inference:  26%|██▌       | 51/200 [01:56<06:33,  2.64s/it]Running Inference:  26%|██▌       | 52/200 [01:58<06:09,  2.50s/it]Running Inference:  26%|██▋       | 53/200 [01:59<05:00,  2.04s/it]Running Inference:  27%|██▋       | 54/200 [02:02<05:01,  2.07s/it]Running Inference:  28%|██▊       | 55/200 [02:04<05:09,  2.13s/it]Running Inference:  28%|██▊       | 56/200 [02:06<05:13,  2.18s/it]Running Inference:  28%|██▊       | 57/200 [02:10<06:12,  2.61s/it]Running Inference:  29%|██▉       | 58/200 [02:11<05:29,  2.32s/it]Running Inference:  30%|██▉       | 59/200 [02:13<04:43,  2.01s/it]Running Inference:  30%|███       | 60/200 [02:15<04:47,  2.05s/it]Running Inference:  30%|███       | 61/200 [02:17<04:41,  2.03s/it]Running Inference:  31%|███       | 62/200 [02:19<04:48,  2.09s/it]Running Inference:  32%|███▏      | 63/200 [02:23<05:47,  2.54s/it]Running Inference:  32%|███▏      | 64/200 [02:27<06:55,  3.05s/it]Running Inference:  32%|███▎      | 65/200 [02:29<06:16,  2.79s/it]Running Inference:  33%|███▎      | 66/200 [02:31<05:53,  2.64s/it]Running Inference:  34%|███▎      | 67/200 [02:35<06:22,  2.88s/it]Running Inference:  34%|███▍      | 68/200 [02:37<05:39,  2.57s/it]Running Inference:  34%|███▍      | 69/200 [02:38<04:46,  2.19s/it]Running Inference:  35%|███▌      | 70/200 [02:40<04:54,  2.26s/it]Running Inference:  36%|███▌      | 71/200 [02:43<05:13,  2.43s/it]Running Inference:  36%|███▌      | 72/200 [02:45<05:00,  2.35s/it]Running Inference:  36%|███▋      | 73/200 [02:48<04:53,  2.31s/it]Running Inference:  37%|███▋      | 74/200 [02:51<05:43,  2.72s/it]Running Inference:  38%|███▊      | 75/200 [02:53<05:01,  2.41s/it]Running Inference:  38%|███▊      | 76/200 [02:57<05:55,  2.86s/it]Running Inference:  38%|███▊      | 77/200 [02:58<04:38,  2.27s/it]Running Inference:  39%|███▉      | 78/200 [03:00<04:24,  2.17s/it]Running Inference:  40%|███▉      | 79/200 [03:01<04:02,  2.01s/it]Running Inference:  40%|████      | 80/200 [03:02<03:18,  1.66s/it]Running Inference:  40%|████      | 81/200 [03:06<04:50,  2.44s/it]Running Inference:  41%|████      | 82/200 [03:08<04:26,  2.26s/it]Running Inference:  42%|████▏     | 83/200 [03:11<04:59,  2.56s/it]Running Inference:  42%|████▏     | 84/200 [03:15<05:34,  2.89s/it]Running Inference:  42%|████▎     | 85/200 [03:17<05:11,  2.71s/it]Running Inference:  43%|████▎     | 86/200 [03:20<04:59,  2.63s/it]Running Inference:  44%|████▎     | 87/200 [03:22<04:38,  2.46s/it]Running Inference:  44%|████▍     | 88/200 [03:23<04:02,  2.17s/it]Running Inference:  44%|████▍     | 89/200 [03:26<04:05,  2.21s/it]Running Inference:  45%|████▌     | 90/200 [03:28<03:52,  2.12s/it]Running Inference:  46%|████▌     | 91/200 [03:29<03:10,  1.75s/it]Running Inference:  46%|████▌     | 92/200 [03:33<04:36,  2.56s/it]Running Inference:  46%|████▋     | 93/200 [03:35<04:05,  2.29s/it]Running Inference:  47%|████▋     | 94/200 [03:37<04:12,  2.38s/it]Running Inference:  48%|████▊     | 95/200 [03:39<03:47,  2.17s/it]Running Inference:  48%|████▊     | 96/200 [03:41<03:45,  2.17s/it]Running Inference:  48%|████▊     | 97/200 [03:43<03:33,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [03:45<03:42,  2.18s/it]Running Inference:  50%|████▉     | 99/200 [03:47<03:33,  2.11s/it]Running Inference:  50%|█████     | 100/200 [03:50<03:36,  2.17s/it]Running Inference:  50%|█████     | 101/200 [03:52<03:45,  2.28s/it]Running Inference:  51%|█████     | 102/200 [03:54<03:37,  2.22s/it]Running Inference:  52%|█████▏    | 103/200 [03:55<02:54,  1.80s/it]Running Inference:  52%|█████▏    | 104/200 [03:57<02:45,  1.73s/it]Running Inference:  52%|█████▎    | 105/200 [03:58<02:36,  1.65s/it]Running Inference:  53%|█████▎    | 106/200 [04:00<02:44,  1.75s/it]Running Inference:  54%|█████▎    | 107/200 [04:03<03:13,  2.08s/it]Running Inference:  54%|█████▍    | 108/200 [04:04<02:33,  1.67s/it]Running Inference:  55%|█████▍    | 109/200 [04:05<02:27,  1.62s/it]Running Inference:  55%|█████▌    | 110/200 [04:06<02:14,  1.50s/it]Running Inference:  56%|█████▌    | 111/200 [04:09<02:37,  1.77s/it]Running Inference:  56%|█████▌    | 112/200 [04:13<03:38,  2.48s/it]Running Inference:  56%|█████▋    | 113/200 [04:15<03:34,  2.47s/it]Running Inference:  57%|█████▋    | 114/200 [04:18<03:26,  2.40s/it]Running Inference:  57%|█████▊    | 115/200 [04:20<03:21,  2.37s/it]Running Inference:  58%|█████▊    | 116/200 [04:24<03:58,  2.83s/it]Running Inference:  58%|█████▊    | 117/200 [04:26<03:46,  2.73s/it]Running Inference:  59%|█████▉    | 118/200 [04:29<03:32,  2.59s/it]Running Inference:  60%|█████▉    | 119/200 [04:29<02:49,  2.10s/it]Running Inference:  60%|██████    | 120/200 [04:33<03:14,  2.43s/it]Running Inference:  60%|██████    | 121/200 [04:35<03:09,  2.39s/it]Running Inference:  61%|██████    | 122/200 [04:37<03:00,  2.32s/it]Running Inference:  62%|██████▏   | 123/200 [04:39<02:55,  2.28s/it]Running Inference:  62%|██████▏   | 124/200 [04:41<02:46,  2.19s/it]Running Inference:  62%|██████▎   | 125/200 [04:43<02:41,  2.16s/it]Running Inference:  63%|██████▎   | 126/200 [04:46<02:41,  2.19s/it]Running Inference:  64%|██████▎   | 127/200 [04:46<02:08,  1.76s/it]Running Inference:  64%|██████▍   | 128/200 [04:51<03:02,  2.54s/it]Running Inference:  64%|██████▍   | 129/200 [04:53<02:52,  2.42s/it]Running Inference:  65%|██████▌   | 130/200 [04:55<02:37,  2.25s/it]Running Inference:  66%|██████▌   | 131/200 [04:59<03:09,  2.74s/it]Running Inference:  66%|██████▌   | 132/200 [05:00<02:38,  2.33s/it]Running Inference:  66%|██████▋   | 133/200 [05:03<02:55,  2.62s/it]Running Inference:  67%|██████▋   | 134/200 [05:06<02:47,  2.53s/it]Running Inference:  68%|██████▊   | 135/200 [05:08<02:36,  2.41s/it]Running Inference:  68%|██████▊   | 136/200 [05:10<02:27,  2.30s/it]Running Inference:  68%|██████▊   | 137/200 [05:11<02:06,  2.01s/it]Running Inference:  69%|██████▉   | 138/200 [05:13<02:08,  2.08s/it]Running Inference:  70%|██████▉   | 139/200 [05:18<02:45,  2.72s/it]Running Inference:  70%|███████   | 140/200 [05:22<03:04,  3.07s/it]Running Inference:  70%|███████   | 141/200 [05:24<02:46,  2.82s/it]Running Inference:  71%|███████   | 142/200 [05:28<03:06,  3.21s/it]Running Inference:  72%|███████▏  | 143/200 [05:29<02:19,  2.45s/it]Running Inference:  72%|███████▏  | 144/200 [05:29<01:50,  1.97s/it]Running Inference:  72%|███████▎  | 145/200 [05:30<01:21,  1.48s/it]Running Inference:  73%|███████▎  | 146/200 [05:34<02:04,  2.31s/it]Running Inference:  74%|███████▎  | 147/200 [05:36<02:00,  2.27s/it]Running Inference:  74%|███████▍  | 148/200 [05:38<01:52,  2.16s/it]Running Inference:  74%|███████▍  | 149/200 [05:39<01:26,  1.69s/it]Running Inference:  75%|███████▌  | 150/200 [05:41<01:30,  1.80s/it]Running Inference:  76%|███████▌  | 151/200 [05:42<01:26,  1.76s/it]Running Inference:  76%|███████▌  | 152/200 [05:44<01:29,  1.86s/it]Running Inference:  76%|███████▋  | 153/200 [05:46<01:29,  1.91s/it]Running Inference:  77%|███████▋  | 154/200 [05:49<01:33,  2.04s/it]Running Inference:  78%|███████▊  | 155/200 [05:51<01:40,  2.23s/it]Running Inference:  78%|███████▊  | 156/200 [05:53<01:23,  1.90s/it]Running Inference:  78%|███████▊  | 157/200 [05:54<01:15,  1.77s/it]Running Inference:  79%|███████▉  | 158/200 [05:57<01:27,  2.07s/it]Running Inference:  80%|███████▉  | 159/200 [05:58<01:17,  1.89s/it]Running Inference:  80%|████████  | 160/200 [06:02<01:42,  2.56s/it]Running Inference:  80%|████████  | 161/200 [06:06<01:48,  2.78s/it]Running Inference:  81%|████████  | 162/200 [06:06<01:20,  2.12s/it]Running Inference:  82%|████████▏ | 163/200 [06:08<01:13,  1.99s/it]Running Inference:  82%|████████▏ | 164/200 [06:10<01:13,  2.05s/it]Running Inference:  82%|████████▎ | 165/200 [06:13<01:16,  2.17s/it]Running Inference:  83%|████████▎ | 166/200 [06:15<01:12,  2.14s/it]Running Inference:  84%|████████▎ | 167/200 [06:17<01:09,  2.11s/it]Running Inference:  84%|████████▍ | 168/200 [06:21<01:25,  2.68s/it]Running Inference:  84%|████████▍ | 169/200 [06:23<01:22,  2.66s/it]Running Inference:  85%|████████▌ | 170/200 [06:26<01:15,  2.51s/it]Running Inference:  86%|████████▌ | 171/200 [06:28<01:11,  2.47s/it]Running Inference:  86%|████████▌ | 172/200 [06:30<01:06,  2.39s/it]Running Inference:  86%|████████▋ | 173/200 [06:32<01:02,  2.31s/it]Running Inference:  87%|████████▋ | 174/200 [06:36<01:11,  2.73s/it]Running Inference:  88%|████████▊ | 175/200 [06:38<01:04,  2.58s/it]Running Inference:  88%|████████▊ | 176/200 [06:40<00:58,  2.46s/it]Running Inference:  88%|████████▊ | 177/200 [06:43<00:55,  2.42s/it]Running Inference:  89%|████████▉ | 178/200 [06:45<00:52,  2.39s/it]Running Inference:  90%|████████▉ | 179/200 [06:47<00:49,  2.37s/it]Running Inference:  90%|█████████ | 180/200 [06:51<00:54,  2.72s/it]Running Inference:  90%|█████████ | 181/200 [06:53<00:49,  2.58s/it]Running Inference:  91%|█████████ | 182/200 [06:55<00:42,  2.37s/it]Running Inference:  92%|█████████▏| 183/200 [06:57<00:36,  2.12s/it]Running Inference:  92%|█████████▏| 184/200 [07:01<00:42,  2.68s/it]Running Inference:  92%|█████████▎| 185/200 [07:02<00:35,  2.35s/it]Running Inference:  93%|█████████▎| 186/200 [07:04<00:31,  2.28s/it]Running Inference:  94%|█████████▎| 187/200 [07:06<00:28,  2.15s/it]Running Inference:  94%|█████████▍| 188/200 [07:07<00:20,  1.73s/it]Running Inference:  94%|█████████▍| 189/200 [07:09<00:20,  1.88s/it]Running Inference:  95%|█████████▌| 190/200 [07:10<00:16,  1.64s/it]Running Inference:  96%|█████████▌| 191/200 [07:12<00:16,  1.79s/it]Running Inference:  96%|█████████▌| 192/200 [07:14<00:13,  1.66s/it]Running Inference:  96%|█████████▋| 193/200 [07:18<00:17,  2.48s/it]Running Inference:  97%|█████████▋| 194/200 [07:20<00:13,  2.19s/it]Running Inference:  98%|█████████▊| 195/200 [07:23<00:12,  2.43s/it]Running Inference:  98%|█████████▊| 196/200 [07:25<00:09,  2.37s/it]Running Inference:  98%|█████████▊| 197/200 [07:26<00:06,  2.09s/it]Running Inference:  99%|█████████▉| 198/200 [07:29<00:04,  2.29s/it]Running Inference: 100%|█████████▉| 199/200 [07:31<00:02,  2.33s/it]Running Inference: 100%|██████████| 200/200 [07:32<00:00,  1.78s/it]Running Inference: 100%|██████████| 200/200 [07:32<00:00,  2.26s/it]
2025-12-13 18:26:18,454 - INFO - Inference completed.
2025-12-13 18:26:18,462 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 18:26:18,462 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:26:18,465 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 18:26:18,466 - INFO - Metrics:
42.35
2025-12-13 18:26:18,467 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa, ratio=0.1) on GPU 1

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:26:24,876 - INFO - Set deterministic seeds to 42
2025-12-13 18:26:24,876 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:26:24,876 - INFO - Starting evaluation run...
2025-12-13 18:26:24,876 - INFO - Output directory set to: longbenchresult
2025-12-13 18:26:24,876 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 18:26:24,876 - INFO - KV Press 'tova' setup.
2025-12-13 18:26:24,876 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:26:24,876 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.72it/s]
Device set to use cuda:0
2025-12-13 18:26:37,785 - INFO - Model pipeline loaded.
2025-12-13 18:26:37,785 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:26:42,633 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:26:42,633 - INFO - Dataset processed with 200 entries.
2025-12-13 18:26:42,668 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:07,  3.36s/it]Running Inference:   1%|          | 2/200 [00:04<05:51,  1.77s/it]Running Inference:   2%|▏         | 3/200 [00:08<09:11,  2.80s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:29,  2.60s/it]Running Inference:   2%|▎         | 5/200 [00:12<08:17,  2.55s/it]Running Inference:   3%|▎         | 6/200 [00:13<06:44,  2.08s/it]Running Inference:   4%|▎         | 7/200 [00:16<06:38,  2.06s/it]Running Inference:   4%|▍         | 8/200 [00:17<05:45,  1.80s/it]Running Inference:   4%|▍         | 9/200 [00:19<05:42,  1.79s/it]Running Inference:   5%|▌         | 10/200 [00:19<04:49,  1.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:21<04:47,  1.52s/it]Running Inference:   6%|▌         | 12/200 [00:22<04:30,  1.44s/it]Running Inference:   6%|▋         | 13/200 [00:24<04:58,  1.60s/it]Running Inference:   7%|▋         | 14/200 [00:26<05:31,  1.78s/it]Running Inference:   8%|▊         | 15/200 [00:29<05:58,  1.94s/it]Running Inference:   8%|▊         | 16/200 [00:31<06:15,  2.04s/it]Running Inference:   8%|▊         | 17/200 [00:35<08:13,  2.70s/it]Running Inference:   9%|▉         | 18/200 [00:37<07:43,  2.55s/it]Running Inference:  10%|▉         | 19/200 [00:38<06:01,  2.00s/it]Running Inference:  10%|█         | 20/200 [00:41<07:05,  2.36s/it]Running Inference:  10%|█         | 21/200 [00:42<05:36,  1.88s/it]Running Inference:  11%|█         | 22/200 [00:43<04:22,  1.47s/it]Running Inference:  12%|█▏        | 23/200 [00:44<04:30,  1.53s/it]Running Inference:  12%|█▏        | 24/200 [00:48<06:47,  2.31s/it]Running Inference:  12%|█▎        | 25/200 [00:50<05:57,  2.04s/it]Running Inference:  13%|█▎        | 26/200 [00:50<04:45,  1.64s/it]Running Inference:  14%|█▎        | 27/200 [00:54<06:28,  2.24s/it]Running Inference:  14%|█▍        | 28/200 [00:58<07:52,  2.75s/it]Running Inference:  14%|█▍        | 29/200 [01:02<09:07,  3.20s/it]Running Inference:  15%|█▌        | 30/200 [01:05<08:23,  2.96s/it]Running Inference:  16%|█▌        | 31/200 [01:06<06:56,  2.47s/it]Running Inference:  16%|█▌        | 32/200 [01:10<08:26,  3.01s/it]Running Inference:  16%|█▋        | 33/200 [01:13<07:52,  2.83s/it]Running Inference:  17%|█▋        | 34/200 [01:15<07:17,  2.64s/it]Running Inference:  18%|█▊        | 35/200 [01:17<06:57,  2.53s/it]Running Inference:  18%|█▊        | 36/200 [01:20<06:48,  2.49s/it]Running Inference:  18%|█▊        | 37/200 [01:22<06:40,  2.46s/it]Running Inference:  19%|█▉        | 38/200 [01:24<06:27,  2.39s/it]Running Inference:  20%|█▉        | 39/200 [01:27<06:43,  2.51s/it]Running Inference:  20%|██        | 40/200 [01:29<06:37,  2.49s/it]Running Inference:  20%|██        | 41/200 [01:31<06:14,  2.36s/it]Running Inference:  21%|██        | 42/200 [01:34<06:02,  2.29s/it]Running Inference:  22%|██▏       | 43/200 [01:35<05:10,  1.98s/it]Running Inference:  22%|██▏       | 44/200 [01:38<05:40,  2.19s/it]Running Inference:  22%|██▎       | 45/200 [01:42<07:07,  2.76s/it]Running Inference:  23%|██▎       | 46/200 [01:44<06:52,  2.68s/it]Running Inference:  24%|██▎       | 47/200 [01:48<08:02,  3.15s/it]Running Inference:  24%|██▍       | 48/200 [01:50<06:41,  2.64s/it]Running Inference:  24%|██▍       | 49/200 [01:52<06:15,  2.49s/it]Running Inference:  25%|██▌       | 50/200 [01:54<05:38,  2.26s/it]Running Inference:  26%|██▌       | 51/200 [01:57<06:38,  2.68s/it]Running Inference:  26%|██▌       | 52/200 [01:59<06:13,  2.52s/it]Running Inference:  26%|██▋       | 53/200 [02:00<05:02,  2.06s/it]Running Inference:  27%|██▋       | 54/200 [02:03<05:03,  2.08s/it]Running Inference:  28%|██▊       | 55/200 [02:05<05:10,  2.14s/it]Running Inference:  28%|██▊       | 56/200 [02:07<05:14,  2.18s/it]Running Inference:  28%|██▊       | 57/200 [02:11<06:13,  2.62s/it]Running Inference:  29%|██▉       | 58/200 [02:12<05:28,  2.31s/it]Running Inference:  30%|██▉       | 59/200 [02:14<04:42,  2.00s/it]Running Inference:  30%|███       | 60/200 [02:16<04:46,  2.05s/it]Running Inference:  30%|███       | 61/200 [02:18<04:55,  2.13s/it]Running Inference:  31%|███       | 62/200 [02:20<04:58,  2.16s/it]Running Inference:  32%|███▏      | 63/200 [02:24<05:54,  2.59s/it]Running Inference:  32%|███▏      | 64/200 [02:28<07:00,  3.09s/it]Running Inference:  32%|███▎      | 65/200 [02:30<06:19,  2.81s/it]Running Inference:  33%|███▎      | 66/200 [02:33<05:55,  2.65s/it]Running Inference:  34%|███▎      | 67/200 [02:36<06:24,  2.89s/it]Running Inference:  34%|███▍      | 68/200 [02:38<05:41,  2.58s/it]Running Inference:  34%|███▍      | 69/200 [02:39<04:48,  2.20s/it]Running Inference:  35%|███▌      | 70/200 [02:42<04:55,  2.27s/it]Running Inference:  36%|███▌      | 71/200 [02:45<05:47,  2.70s/it]Running Inference:  36%|███▌      | 72/200 [02:48<05:23,  2.53s/it]Running Inference:  36%|███▋      | 73/200 [02:50<05:09,  2.44s/it]Running Inference:  37%|███▋      | 74/200 [02:53<05:55,  2.82s/it]Running Inference:  38%|███▊      | 75/200 [02:55<05:09,  2.47s/it]Running Inference:  38%|███▊      | 76/200 [02:59<06:01,  2.91s/it]Running Inference:  38%|███▊      | 77/200 [03:00<04:42,  2.30s/it]Running Inference:  39%|███▉      | 78/200 [03:02<04:27,  2.19s/it]Running Inference:  40%|███▉      | 79/200 [03:04<04:04,  2.02s/it]Running Inference:  40%|████      | 80/200 [03:04<03:20,  1.67s/it]Running Inference:  40%|████      | 81/200 [03:09<04:52,  2.45s/it]Running Inference:  41%|████      | 82/200 [03:10<04:26,  2.26s/it]Running Inference:  42%|████▏     | 83/200 [03:14<04:59,  2.56s/it]Running Inference:  42%|████▏     | 84/200 [03:17<05:35,  2.89s/it]Running Inference:  42%|████▎     | 85/200 [03:20<05:11,  2.71s/it]Running Inference:  43%|████▎     | 86/200 [03:22<04:51,  2.55s/it]Running Inference:  44%|████▎     | 87/200 [03:24<04:35,  2.43s/it]Running Inference:  44%|████▍     | 88/200 [03:25<04:00,  2.15s/it]Running Inference:  44%|████▍     | 89/200 [03:28<04:04,  2.20s/it]Running Inference:  45%|████▌     | 90/200 [03:30<03:51,  2.11s/it]Running Inference:  46%|████▌     | 91/200 [03:31<03:10,  1.74s/it]Running Inference:  46%|████▌     | 92/200 [03:35<04:34,  2.54s/it]Running Inference:  46%|████▋     | 93/200 [03:37<04:03,  2.28s/it]Running Inference:  47%|████▋     | 94/200 [03:39<04:09,  2.36s/it]Running Inference:  48%|████▊     | 95/200 [03:41<03:45,  2.15s/it]Running Inference:  48%|████▊     | 96/200 [03:43<03:44,  2.16s/it]Running Inference:  48%|████▊     | 97/200 [03:45<03:32,  2.06s/it]Running Inference:  49%|████▉     | 98/200 [03:49<04:34,  2.69s/it]Running Inference:  50%|████▉     | 99/200 [03:51<04:08,  2.46s/it]Running Inference:  50%|█████     | 100/200 [03:53<04:00,  2.41s/it]Running Inference:  50%|█████     | 101/200 [03:56<04:02,  2.44s/it]Running Inference:  51%|█████     | 102/200 [03:58<03:49,  2.34s/it]Running Inference:  52%|█████▏    | 103/200 [03:59<03:02,  1.88s/it]Running Inference:  52%|█████▏    | 104/200 [04:00<02:51,  1.78s/it]Running Inference:  52%|█████▎    | 105/200 [04:02<02:40,  1.69s/it]Running Inference:  53%|█████▎    | 106/200 [04:04<02:46,  1.77s/it]Running Inference:  54%|█████▎    | 107/200 [04:07<03:16,  2.11s/it]Running Inference:  54%|█████▍    | 108/200 [04:07<02:35,  1.69s/it]Running Inference:  55%|█████▍    | 109/200 [04:09<02:28,  1.63s/it]Running Inference:  55%|█████▌    | 110/200 [04:10<02:15,  1.51s/it]Running Inference:  56%|█████▌    | 111/200 [04:12<02:38,  1.78s/it]Running Inference:  56%|█████▌    | 112/200 [04:17<03:40,  2.51s/it]Running Inference:  56%|█████▋    | 113/200 [04:19<03:36,  2.49s/it]Running Inference:  57%|█████▋    | 114/200 [04:21<03:27,  2.41s/it]Running Inference:  57%|█████▊    | 115/200 [04:24<03:22,  2.38s/it]Running Inference:  58%|█████▊    | 116/200 [04:28<03:58,  2.84s/it]Running Inference:  58%|█████▊    | 117/200 [04:30<03:44,  2.71s/it]Running Inference:  59%|█████▉    | 118/200 [04:34<04:19,  3.17s/it]Running Inference:  60%|█████▉    | 119/200 [04:35<03:24,  2.52s/it]Running Inference:  60%|██████    | 120/200 [04:39<03:55,  2.94s/it]Running Inference:  60%|██████    | 121/200 [04:41<03:37,  2.75s/it]Running Inference:  61%|██████    | 122/200 [04:44<03:20,  2.57s/it]Running Inference:  62%|██████▏   | 123/200 [04:46<03:09,  2.46s/it]Running Inference:  62%|██████▏   | 124/200 [04:48<02:55,  2.31s/it]Running Inference:  62%|██████▎   | 125/200 [04:50<02:50,  2.27s/it]Running Inference:  63%|██████▎   | 126/200 [04:52<02:48,  2.27s/it]Running Inference:  64%|██████▎   | 127/200 [04:53<02:12,  1.82s/it]Running Inference:  64%|██████▍   | 128/200 [04:55<02:26,  2.03s/it]Running Inference:  64%|██████▍   | 129/200 [04:58<02:26,  2.07s/it]Running Inference:  65%|██████▌   | 130/200 [04:59<02:20,  2.00s/it]Running Inference:  66%|██████▌   | 131/200 [05:03<02:57,  2.57s/it]Running Inference:  66%|██████▌   | 132/200 [05:05<02:29,  2.21s/it]Running Inference:  66%|██████▋   | 133/200 [05:08<02:50,  2.54s/it]Running Inference:  67%|██████▋   | 134/200 [05:10<02:43,  2.47s/it]Running Inference:  68%|██████▊   | 135/200 [05:13<02:34,  2.37s/it]Running Inference:  68%|██████▊   | 136/200 [05:15<02:25,  2.27s/it]Running Inference:  68%|██████▊   | 137/200 [05:16<02:05,  1.99s/it]Running Inference:  69%|██████▉   | 138/200 [05:18<02:08,  2.07s/it]Running Inference:  70%|██████▉   | 139/200 [05:22<02:45,  2.71s/it]Running Inference:  70%|███████   | 140/200 [05:26<03:03,  3.07s/it]Running Inference:  70%|███████   | 141/200 [05:28<02:46,  2.82s/it]Running Inference:  71%|███████   | 142/200 [05:33<03:06,  3.21s/it]Running Inference:  72%|███████▏  | 143/200 [05:33<02:19,  2.45s/it]Running Inference:  72%|███████▏  | 144/200 [05:34<01:50,  1.97s/it]Running Inference:  72%|███████▎  | 145/200 [05:34<01:21,  1.48s/it]Running Inference:  73%|███████▎  | 146/200 [05:39<02:04,  2.31s/it]Running Inference:  74%|███████▎  | 147/200 [05:41<01:59,  2.26s/it]Running Inference:  74%|███████▍  | 148/200 [05:43<01:52,  2.16s/it]Running Inference:  74%|███████▍  | 149/200 [05:43<01:26,  1.69s/it]Running Inference:  75%|███████▌  | 150/200 [05:45<01:30,  1.80s/it]Running Inference:  76%|███████▌  | 151/200 [05:47<01:26,  1.76s/it]Running Inference:  76%|███████▌  | 152/200 [05:49<01:29,  1.86s/it]Running Inference:  76%|███████▋  | 153/200 [05:51<01:29,  1.91s/it]Running Inference:  77%|███████▋  | 154/200 [05:54<01:33,  2.04s/it]Running Inference:  78%|███████▊  | 155/200 [05:58<02:00,  2.67s/it]Running Inference:  78%|███████▊  | 156/200 [05:59<01:37,  2.21s/it]Running Inference:  78%|███████▊  | 157/200 [06:00<01:25,  1.98s/it]Running Inference:  79%|███████▉  | 158/200 [06:03<01:33,  2.23s/it]Running Inference:  80%|███████▉  | 159/200 [06:05<01:22,  2.00s/it]Running Inference:  80%|████████  | 160/200 [06:09<01:45,  2.64s/it]Running Inference:  80%|████████  | 161/200 [06:12<01:50,  2.83s/it]Running Inference:  81%|████████  | 162/200 [06:13<01:22,  2.16s/it]Running Inference:  82%|████████▏ | 163/200 [06:14<01:14,  2.02s/it]Running Inference:  82%|████████▏ | 164/200 [06:16<01:14,  2.07s/it]Running Inference:  82%|████████▎ | 165/200 [06:19<01:16,  2.18s/it]Running Inference:  83%|████████▎ | 166/200 [06:21<01:13,  2.15s/it]Running Inference:  84%|████████▎ | 167/200 [06:23<01:09,  2.12s/it]Running Inference:  84%|████████▍ | 168/200 [06:27<01:25,  2.69s/it]Running Inference:  84%|████████▍ | 169/200 [06:29<01:19,  2.56s/it]Running Inference:  85%|████████▌ | 170/200 [06:31<01:12,  2.42s/it]Running Inference:  86%|████████▌ | 171/200 [06:34<01:09,  2.41s/it]Running Inference:  86%|████████▌ | 172/200 [06:36<01:05,  2.35s/it]Running Inference:  86%|████████▋ | 173/200 [06:38<01:01,  2.28s/it]Running Inference:  87%|████████▋ | 174/200 [06:42<01:10,  2.72s/it]Running Inference:  88%|████████▊ | 175/200 [06:44<01:04,  2.57s/it]Running Inference:  88%|████████▊ | 176/200 [06:46<00:58,  2.45s/it]Running Inference:  88%|████████▊ | 177/200 [06:49<00:55,  2.42s/it]Running Inference:  89%|████████▉ | 178/200 [06:51<00:52,  2.39s/it]Running Inference:  90%|████████▉ | 179/200 [06:53<00:49,  2.37s/it]Running Inference:  90%|█████████ | 180/200 [06:57<00:54,  2.72s/it]Running Inference:  90%|█████████ | 181/200 [06:59<00:49,  2.58s/it]Running Inference:  91%|█████████ | 182/200 [07:01<00:42,  2.37s/it]Running Inference:  92%|█████████▏| 183/200 [07:02<00:36,  2.12s/it]Running Inference:  92%|█████████▏| 184/200 [07:06<00:42,  2.69s/it]Running Inference:  92%|█████████▎| 185/200 [07:08<00:35,  2.35s/it]Running Inference:  93%|█████████▎| 186/200 [07:10<00:32,  2.29s/it]Running Inference:  94%|█████████▎| 187/200 [07:12<00:28,  2.16s/it]Running Inference:  94%|█████████▍| 188/200 [07:13<00:20,  1.73s/it]Running Inference:  94%|█████████▍| 189/200 [07:15<00:20,  1.88s/it]Running Inference:  95%|█████████▌| 190/200 [07:16<00:16,  1.64s/it]Running Inference:  96%|█████████▌| 191/200 [07:18<00:16,  1.80s/it]Running Inference:  96%|█████████▌| 192/200 [07:20<00:13,  1.67s/it]Running Inference:  96%|█████████▋| 193/200 [07:24<00:17,  2.51s/it]Running Inference:  97%|█████████▋| 194/200 [07:26<00:13,  2.21s/it]Running Inference:  98%|█████████▊| 195/200 [07:29<00:12,  2.45s/it]Running Inference:  98%|█████████▊| 196/200 [07:31<00:09,  2.39s/it]Running Inference:  98%|█████████▊| 197/200 [07:32<00:06,  2.10s/it]Running Inference:  99%|█████████▉| 198/200 [07:35<00:04,  2.29s/it]Running Inference: 100%|█████████▉| 199/200 [07:37<00:02,  2.34s/it]Running Inference: 100%|██████████| 200/200 [07:38<00:00,  1.78s/it]Running Inference: 100%|██████████| 200/200 [07:38<00:00,  2.29s/it]
2025-12-13 18:34:21,111 - INFO - Inference completed.
2025-12-13 18:34:21,120 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 18:34:21,120 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:34:21,124 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 18:34:21,124 - INFO - Metrics:
42.42
2025-12-13 18:34:21,125 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa, ratio=0.2) on GPU 1

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:34:27,530 - INFO - Set deterministic seeds to 42
2025-12-13 18:34:27,531 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:34:27,531 - INFO - Starting evaluation run...
2025-12-13 18:34:27,531 - INFO - Output directory set to: longbenchresult
2025-12-13 18:34:27,531 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 18:34:27,531 - INFO - KV Press 'tova' setup.
2025-12-13 18:34:27,531 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:34:27,531 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.48it/s]
Device set to use cuda:0
2025-12-13 18:34:46,864 - INFO - Model pipeline loaded.
2025-12-13 18:34:46,864 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:34:51,953 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:34:51,953 - INFO - Dataset processed with 200 entries.
2025-12-13 18:34:51,988 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:12,  3.38s/it]Running Inference:   1%|          | 2/200 [00:04<05:53,  1.79s/it]Running Inference:   2%|▏         | 3/200 [00:08<09:18,  2.83s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:33,  2.62s/it]Running Inference:   2%|▎         | 5/200 [00:11<06:55,  2.13s/it]Running Inference:   3%|▎         | 6/200 [00:12<05:50,  1.81s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:02,  1.88s/it]Running Inference:   4%|▍         | 8/200 [00:16<05:21,  1.68s/it]Running Inference:   4%|▍         | 9/200 [00:17<05:25,  1.71s/it]Running Inference:   5%|▌         | 10/200 [00:18<04:43,  1.49s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<04:43,  1.50s/it]Running Inference:   6%|▌         | 12/200 [00:21<04:27,  1.42s/it]Running Inference:   6%|▋         | 13/200 [00:23<04:57,  1.59s/it]Running Inference:   7%|▋         | 14/200 [00:25<05:30,  1.78s/it]Running Inference:   8%|▊         | 15/200 [00:28<05:57,  1.93s/it]Running Inference:   8%|▊         | 16/200 [00:30<06:15,  2.04s/it]Running Inference:   8%|▊         | 17/200 [00:34<08:16,  2.71s/it]Running Inference:   9%|▉         | 18/200 [00:36<07:45,  2.56s/it]Running Inference:  10%|▉         | 19/200 [00:37<06:02,  2.00s/it]Running Inference:  10%|█         | 20/200 [00:40<07:08,  2.38s/it]Running Inference:  10%|█         | 21/200 [00:41<05:38,  1.89s/it]Running Inference:  11%|█         | 22/200 [00:42<04:23,  1.48s/it]Running Inference:  12%|█▏        | 23/200 [00:43<04:32,  1.54s/it]Running Inference:  12%|█▏        | 24/200 [00:48<06:51,  2.34s/it]Running Inference:  12%|█▎        | 25/200 [00:49<06:00,  2.06s/it]Running Inference:  13%|█▎        | 26/200 [00:50<04:48,  1.66s/it]Running Inference:  14%|█▎        | 27/200 [00:53<06:32,  2.27s/it]Running Inference:  14%|█▍        | 28/200 [00:56<06:44,  2.35s/it]Running Inference:  14%|█▍        | 29/200 [01:00<08:22,  2.94s/it]Running Inference:  15%|█▌        | 30/200 [01:03<07:52,  2.78s/it]Running Inference:  16%|█▌        | 31/200 [01:04<06:35,  2.34s/it]Running Inference:  16%|█▌        | 32/200 [01:08<08:14,  2.94s/it]Running Inference:  16%|█▋        | 33/200 [01:11<07:45,  2.79s/it]Running Inference:  17%|█▋        | 34/200 [01:13<07:12,  2.61s/it]Running Inference:  18%|█▊        | 35/200 [01:15<06:53,  2.51s/it]Running Inference:  18%|█▊        | 36/200 [01:18<06:46,  2.48s/it]Running Inference:  18%|█▊        | 37/200 [01:20<06:39,  2.45s/it]Running Inference:  19%|█▉        | 38/200 [01:22<06:26,  2.39s/it]Running Inference:  20%|█▉        | 39/200 [01:25<06:22,  2.38s/it]Running Inference:  20%|██        | 40/200 [01:27<06:23,  2.40s/it]Running Inference:  20%|██        | 41/200 [01:29<06:05,  2.30s/it]Running Inference:  21%|██        | 42/200 [01:31<05:56,  2.25s/it]Running Inference:  22%|██▏       | 43/200 [01:32<05:06,  1.95s/it]Running Inference:  22%|██▏       | 44/200 [01:35<05:38,  2.17s/it]Running Inference:  22%|██▎       | 45/200 [01:39<07:07,  2.76s/it]Running Inference:  23%|██▎       | 46/200 [01:42<06:53,  2.69s/it]Running Inference:  24%|██▎       | 47/200 [01:46<08:05,  3.17s/it]Running Inference:  24%|██▍       | 48/200 [01:48<06:43,  2.65s/it]Running Inference:  24%|██▍       | 49/200 [01:50<06:17,  2.50s/it]Running Inference:  25%|██▌       | 50/200 [01:51<05:39,  2.27s/it]Running Inference:  26%|██▌       | 51/200 [01:55<06:42,  2.70s/it]Running Inference:  26%|██▌       | 52/200 [01:57<06:16,  2.54s/it]Running Inference:  26%|██▋       | 53/200 [01:58<05:04,  2.07s/it]Running Inference:  27%|██▋       | 54/200 [02:00<05:04,  2.09s/it]Running Inference:  28%|██▊       | 55/200 [02:03<05:11,  2.15s/it]Running Inference:  28%|██▊       | 56/200 [02:05<05:16,  2.20s/it]Running Inference:  28%|██▊       | 57/200 [02:09<06:17,  2.64s/it]Running Inference:  29%|██▉       | 58/200 [02:10<05:30,  2.33s/it]Running Inference:  30%|██▉       | 59/200 [02:12<04:44,  2.02s/it]Running Inference:  30%|███       | 60/200 [02:14<04:48,  2.06s/it]Running Inference:  30%|███       | 61/200 [02:16<04:56,  2.13s/it]Running Inference:  31%|███       | 62/200 [02:18<04:59,  2.17s/it]Running Inference:  32%|███▏      | 63/200 [02:22<05:57,  2.61s/it]Running Inference:  32%|███▏      | 64/200 [02:26<07:04,  3.12s/it]Running Inference:  32%|███▎      | 65/200 [02:28<06:22,  2.84s/it]Running Inference:  33%|███▎      | 66/200 [02:31<05:57,  2.67s/it]Running Inference:  34%|███▎      | 67/200 [02:34<06:28,  2.92s/it]Running Inference:  34%|███▍      | 68/200 [02:36<05:44,  2.61s/it]Running Inference:  34%|███▍      | 69/200 [02:39<06:08,  2.81s/it]Running Inference:  35%|███▌      | 70/200 [02:43<06:39,  3.08s/it]Running Inference:  36%|███▌      | 71/200 [02:47<07:02,  3.27s/it]Running Inference:  36%|███▌      | 72/200 [02:49<06:15,  2.93s/it]Running Inference:  36%|███▋      | 73/200 [02:51<05:45,  2.72s/it]Running Inference:  37%|███▋      | 74/200 [02:55<06:21,  3.03s/it]Running Inference:  38%|███▊      | 75/200 [02:57<05:27,  2.62s/it]Running Inference:  38%|███▊      | 76/200 [03:01<06:16,  3.03s/it]Running Inference:  38%|███▊      | 77/200 [03:01<04:53,  2.38s/it]Running Inference:  39%|███▉      | 78/200 [03:03<04:35,  2.25s/it]Running Inference:  40%|███▉      | 79/200 [03:05<04:09,  2.06s/it]Running Inference:  40%|████      | 80/200 [03:06<03:25,  1.71s/it]Running Inference:  40%|████      | 81/200 [03:10<04:57,  2.50s/it]Running Inference:  41%|████      | 82/200 [03:12<04:30,  2.29s/it]Running Inference:  42%|████▏     | 83/200 [03:15<05:03,  2.60s/it]Running Inference:  42%|████▏     | 84/200 [03:19<05:39,  2.93s/it]Running Inference:  42%|████▎     | 85/200 [03:21<05:15,  2.74s/it]Running Inference:  43%|████▎     | 86/200 [03:24<05:02,  2.65s/it]Running Inference:  44%|████▎     | 87/200 [03:27<05:02,  2.68s/it]Running Inference:  44%|████▍     | 88/200 [03:28<04:19,  2.31s/it]Running Inference:  44%|████▍     | 89/200 [03:30<04:17,  2.32s/it]Running Inference:  45%|████▌     | 90/200 [03:32<04:00,  2.19s/it]Running Inference:  46%|████▌     | 91/200 [03:33<03:16,  1.80s/it]Running Inference:  46%|████▌     | 92/200 [03:36<03:54,  2.17s/it]Running Inference:  46%|████▋     | 93/200 [03:38<03:36,  2.02s/it]Running Inference:  47%|████▋     | 94/200 [03:40<03:52,  2.19s/it]Running Inference:  48%|████▊     | 95/200 [03:42<03:33,  2.03s/it]Running Inference:  48%|████▊     | 96/200 [03:44<03:36,  2.08s/it]Running Inference:  48%|████▊     | 97/200 [03:46<03:27,  2.01s/it]Running Inference:  49%|████▉     | 98/200 [03:50<04:32,  2.68s/it]Running Inference:  50%|████▉     | 99/200 [03:52<04:07,  2.45s/it]Running Inference:  50%|█████     | 100/200 [03:55<04:00,  2.40s/it]Running Inference:  50%|█████     | 101/200 [03:57<04:01,  2.44s/it]Running Inference:  51%|█████     | 102/200 [03:59<03:49,  2.34s/it]Running Inference:  52%|█████▏    | 103/200 [04:00<03:03,  1.89s/it]Running Inference:  52%|█████▏    | 104/200 [04:02<02:51,  1.79s/it]Running Inference:  52%|█████▎    | 105/200 [04:03<02:40,  1.69s/it]Running Inference:  53%|█████▎    | 106/200 [04:05<02:46,  1.78s/it]Running Inference:  54%|█████▎    | 107/200 [04:08<03:18,  2.13s/it]Running Inference:  54%|█████▍    | 108/200 [04:09<02:36,  1.70s/it]Running Inference:  55%|█████▍    | 109/200 [04:10<02:29,  1.65s/it]Running Inference:  55%|█████▌    | 110/200 [04:11<02:16,  1.52s/it]Running Inference:  56%|█████▌    | 111/200 [04:14<02:38,  1.79s/it]Running Inference:  56%|█████▌    | 112/200 [04:18<03:41,  2.51s/it]Running Inference:  56%|█████▋    | 113/200 [04:20<03:36,  2.49s/it]Running Inference:  57%|█████▋    | 114/200 [04:23<03:27,  2.41s/it]Running Inference:  57%|█████▊    | 115/200 [04:25<03:22,  2.38s/it]Running Inference:  58%|█████▊    | 116/200 [04:29<04:00,  2.86s/it]Running Inference:  58%|█████▊    | 117/200 [04:31<03:46,  2.72s/it]Running Inference:  59%|█████▉    | 118/200 [04:34<03:32,  2.60s/it]Running Inference:  60%|█████▉    | 119/200 [04:35<02:49,  2.10s/it]Running Inference:  60%|██████    | 120/200 [04:38<03:19,  2.49s/it]Running Inference:  60%|██████    | 121/200 [04:40<03:12,  2.44s/it]Running Inference:  61%|██████    | 122/200 [04:43<03:03,  2.36s/it]Running Inference:  62%|██████▏   | 123/200 [04:45<02:57,  2.31s/it]Running Inference:  62%|██████▏   | 124/200 [04:47<02:47,  2.21s/it]Running Inference:  62%|██████▎   | 125/200 [04:49<02:42,  2.17s/it]Running Inference:  63%|██████▎   | 126/200 [04:51<02:42,  2.20s/it]Running Inference:  64%|██████▎   | 127/200 [04:52<02:08,  1.77s/it]Running Inference:  64%|██████▍   | 128/200 [04:54<02:24,  2.00s/it]Running Inference:  64%|██████▍   | 129/200 [04:57<02:25,  2.05s/it]Running Inference:  65%|██████▌   | 130/200 [04:58<02:19,  1.99s/it]Running Inference:  66%|██████▌   | 131/200 [05:02<02:58,  2.59s/it]Running Inference:  66%|██████▌   | 132/200 [05:04<02:31,  2.23s/it]Running Inference:  66%|██████▋   | 133/200 [05:07<02:52,  2.58s/it]Running Inference:  67%|██████▋   | 134/200 [05:09<02:45,  2.50s/it]Running Inference:  68%|██████▊   | 135/200 [05:12<02:35,  2.40s/it]Running Inference:  68%|██████▊   | 136/200 [05:14<02:26,  2.29s/it]Running Inference:  68%|██████▊   | 137/200 [05:15<02:06,  2.00s/it]Running Inference:  69%|██████▉   | 138/200 [05:17<02:08,  2.08s/it]Running Inference:  70%|██████▉   | 139/200 [05:22<02:47,  2.75s/it]Running Inference:  70%|███████   | 140/200 [05:26<03:06,  3.11s/it]Running Inference:  70%|███████   | 141/200 [05:28<02:48,  2.85s/it]Running Inference:  71%|███████   | 142/200 [05:32<03:08,  3.24s/it]Running Inference:  72%|███████▏  | 143/200 [05:33<02:21,  2.48s/it]Running Inference:  72%|███████▏  | 144/200 [05:34<01:53,  2.02s/it]Running Inference:  72%|███████▎  | 145/200 [05:34<01:23,  1.52s/it]Running Inference:  73%|███████▎  | 146/200 [05:37<01:41,  1.88s/it]Running Inference:  74%|███████▎  | 147/200 [05:39<01:44,  1.97s/it]Running Inference:  74%|███████▍  | 148/200 [05:41<01:41,  1.95s/it]Running Inference:  74%|███████▍  | 149/200 [05:41<01:18,  1.55s/it]Running Inference:  75%|███████▌  | 150/200 [05:43<01:25,  1.70s/it]Running Inference:  76%|███████▌  | 151/200 [05:45<01:23,  1.70s/it]Running Inference:  76%|███████▌  | 152/200 [05:47<01:26,  1.81s/it]Running Inference:  76%|███████▋  | 153/200 [05:49<01:28,  1.88s/it]Running Inference:  77%|███████▋  | 154/200 [05:52<01:32,  2.02s/it]Running Inference:  78%|███████▊  | 155/200 [05:54<01:40,  2.23s/it]Running Inference:  78%|███████▊  | 156/200 [05:55<01:23,  1.91s/it]Running Inference:  78%|███████▊  | 157/200 [05:57<01:16,  1.77s/it]Running Inference:  79%|███████▉  | 158/200 [05:58<01:01,  1.46s/it]Running Inference:  80%|███████▉  | 159/200 [05:59<01:00,  1.47s/it]Running Inference:  80%|████████  | 160/200 [06:03<01:31,  2.28s/it]Running Inference:  80%|████████  | 161/200 [06:07<01:41,  2.59s/it]Running Inference:  81%|████████  | 162/200 [06:07<01:15,  1.99s/it]Running Inference:  82%|████████▏ | 163/200 [06:09<01:10,  1.90s/it]Running Inference:  82%|████████▏ | 164/200 [06:11<01:11,  1.98s/it]Running Inference:  82%|████████▎ | 165/200 [06:13<01:14,  2.13s/it]Running Inference:  83%|████████▎ | 166/200 [06:16<01:11,  2.11s/it]Running Inference:  84%|████████▎ | 167/200 [06:18<01:09,  2.09s/it]Running Inference:  84%|████████▍ | 168/200 [06:21<01:15,  2.36s/it]Running Inference:  84%|████████▍ | 169/200 [06:23<01:15,  2.42s/it]Running Inference:  85%|████████▌ | 170/200 [06:25<01:09,  2.33s/it]Running Inference:  86%|████████▌ | 171/200 [06:28<01:07,  2.34s/it]Running Inference:  86%|████████▌ | 172/200 [06:30<01:04,  2.30s/it]Running Inference:  86%|████████▋ | 173/200 [06:32<01:00,  2.25s/it]Running Inference:  87%|████████▋ | 174/200 [06:36<01:10,  2.70s/it]Running Inference:  88%|████████▊ | 175/200 [06:38<01:04,  2.57s/it]Running Inference:  88%|████████▊ | 176/200 [06:40<00:58,  2.44s/it]Running Inference:  88%|████████▊ | 177/200 [06:42<00:55,  2.41s/it]Running Inference:  89%|████████▉ | 178/200 [06:47<01:05,  2.98s/it]Running Inference:  90%|████████▉ | 179/200 [06:49<00:58,  2.78s/it]Running Inference:  90%|█████████ | 180/200 [06:53<01:00,  3.03s/it]Running Inference:  90%|█████████ | 181/200 [06:55<00:53,  2.80s/it]Running Inference:  91%|█████████ | 182/200 [06:57<00:45,  2.52s/it]Running Inference:  92%|█████████▏| 183/200 [06:58<00:38,  2.24s/it]Running Inference:  92%|█████████▏| 184/200 [07:02<00:44,  2.79s/it]Running Inference:  92%|█████████▎| 185/200 [07:04<00:36,  2.42s/it]Running Inference:  93%|█████████▎| 186/200 [07:06<00:32,  2.34s/it]Running Inference:  94%|█████████▎| 187/200 [07:08<00:28,  2.19s/it]Running Inference:  94%|█████████▍| 188/200 [07:09<00:21,  1.75s/it]Running Inference:  94%|█████████▍| 189/200 [07:11<00:20,  1.90s/it]Running Inference:  95%|█████████▌| 190/200 [07:12<00:16,  1.65s/it]Running Inference:  96%|█████████▌| 191/200 [07:14<00:16,  1.80s/it]Running Inference:  96%|█████████▌| 192/200 [07:16<00:13,  1.67s/it]Running Inference:  96%|█████████▋| 193/200 [07:20<00:17,  2.50s/it]Running Inference:  97%|█████████▋| 194/200 [07:22<00:13,  2.20s/it]Running Inference:  98%|█████████▊| 195/200 [07:25<00:12,  2.46s/it]Running Inference:  98%|█████████▊| 196/200 [07:27<00:09,  2.39s/it]Running Inference:  98%|█████████▊| 197/200 [07:28<00:06,  2.10s/it]Running Inference:  99%|█████████▉| 198/200 [07:31<00:04,  2.29s/it]Running Inference: 100%|█████████▉| 199/200 [07:33<00:02,  2.34s/it]Running Inference: 100%|██████████| 200/200 [07:34<00:00,  1.78s/it]Running Inference: 100%|██████████| 200/200 [07:34<00:00,  2.27s/it]
2025-12-13 18:42:26,467 - INFO - Inference completed.
2025-12-13 18:42:26,475 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-13 18:42:26,475 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:42:26,479 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-13 18:42:26,479 - INFO - Metrics:
41.34
2025-12-13 18:42:26,481 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa, ratio=0.3) on GPU 1

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:42:32,873 - INFO - Set deterministic seeds to 42
2025-12-13 18:42:32,873 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:42:32,873 - INFO - Starting evaluation run...
2025-12-13 18:42:32,873 - INFO - Output directory set to: longbenchresult
2025-12-13 18:42:32,873 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-13 18:42:32,873 - INFO - KV Press 'tova' setup.
2025-12-13 18:42:32,873 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:42:32,873 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.94it/s]
Device set to use cuda:0
2025-12-13 18:42:50,805 - INFO - Model pipeline loaded.
2025-12-13 18:42:50,805 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:42:57,202 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:42:57,202 - INFO - Dataset processed with 200 entries.
2025-12-13 18:42:57,241 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:19,  3.42s/it]Running Inference:   1%|          | 2/200 [00:04<05:56,  1.80s/it]Running Inference:   2%|▏         | 3/200 [00:08<09:11,  2.80s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:29,  2.60s/it]Running Inference:   2%|▎         | 5/200 [00:11<06:32,  2.01s/it]Running Inference:   3%|▎         | 6/200 [00:12<05:34,  1.73s/it]Running Inference:   4%|▎         | 7/200 [00:14<05:51,  1.82s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:13,  1.63s/it]Running Inference:   4%|▍         | 9/200 [00:17<05:19,  1.67s/it]Running Inference:   5%|▌         | 10/200 [00:18<04:38,  1.46s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<04:38,  1.48s/it]Running Inference:   6%|▌         | 12/200 [00:21<04:24,  1.40s/it]Running Inference:   6%|▋         | 13/200 [00:23<04:54,  1.57s/it]Running Inference:   7%|▋         | 14/200 [00:25<05:28,  1.76s/it]Running Inference:   8%|▊         | 15/200 [00:29<07:42,  2.50s/it]Running Inference:   8%|▊         | 16/200 [00:31<07:27,  2.43s/it]Running Inference:   8%|▊         | 17/200 [00:36<09:01,  2.96s/it]Running Inference:   9%|▉         | 18/200 [00:38<08:15,  2.72s/it]Running Inference:  10%|▉         | 19/200 [00:38<06:23,  2.12s/it]Running Inference:  10%|█         | 20/200 [00:42<07:18,  2.44s/it]Running Inference:  10%|█         | 21/200 [00:42<05:45,  1.93s/it]Running Inference:  11%|█         | 22/200 [00:45<06:17,  2.12s/it]Running Inference:  12%|█▏        | 23/200 [00:47<05:50,  1.98s/it]Running Inference:  12%|█▏        | 24/200 [00:51<07:41,  2.62s/it]Running Inference:  12%|█▎        | 25/200 [00:52<06:35,  2.26s/it]Running Inference:  13%|█▎        | 26/200 [00:53<05:09,  1.78s/it]Running Inference:  14%|█▎        | 27/200 [00:56<06:43,  2.33s/it]Running Inference:  14%|█▍        | 28/200 [01:00<08:01,  2.80s/it]Running Inference:  14%|█▍        | 29/200 [01:05<09:11,  3.23s/it]Running Inference:  15%|█▌        | 30/200 [01:09<10:04,  3.56s/it]Running Inference:  16%|█▌        | 31/200 [01:10<08:06,  2.88s/it]Running Inference:  16%|█▌        | 32/200 [01:14<09:13,  3.29s/it]Running Inference:  16%|█▋        | 33/200 [01:17<08:24,  3.02s/it]Running Inference:  17%|█▋        | 34/200 [01:19<07:39,  2.77s/it]Running Inference:  18%|█▊        | 35/200 [01:21<07:11,  2.62s/it]Running Inference:  18%|█▊        | 36/200 [01:24<06:58,  2.55s/it]Running Inference:  18%|█▊        | 37/200 [01:26<06:46,  2.49s/it]Running Inference:  19%|█▉        | 38/200 [01:28<06:30,  2.41s/it]Running Inference:  20%|█▉        | 39/200 [01:31<06:46,  2.52s/it]Running Inference:  20%|██        | 40/200 [01:33<06:39,  2.50s/it]Running Inference:  20%|██        | 41/200 [01:37<07:09,  2.70s/it]Running Inference:  21%|██        | 42/200 [01:39<06:39,  2.53s/it]Running Inference:  22%|██▏       | 43/200 [01:40<05:36,  2.14s/it]Running Inference:  22%|██▏       | 44/200 [01:44<07:06,  2.73s/it]Running Inference:  22%|██▎       | 45/200 [01:47<07:25,  2.88s/it]Running Inference:  23%|██▎       | 46/200 [01:50<07:04,  2.76s/it]Running Inference:  24%|██▎       | 47/200 [01:54<08:09,  3.20s/it]Running Inference:  24%|██▍       | 48/200 [01:55<06:45,  2.67s/it]Running Inference:  24%|██▍       | 49/200 [01:58<06:17,  2.50s/it]Running Inference:  25%|██▌       | 50/200 [01:59<05:39,  2.26s/it]Running Inference:  26%|██▌       | 51/200 [02:01<05:11,  2.09s/it]Running Inference:  26%|██▌       | 52/200 [02:03<05:12,  2.11s/it]Running Inference:  26%|██▋       | 53/200 [02:04<04:20,  1.77s/it]Running Inference:  27%|██▋       | 54/200 [02:06<04:32,  1.87s/it]Running Inference:  28%|██▊       | 55/200 [02:08<04:47,  1.98s/it]Running Inference:  28%|██▊       | 56/200 [02:12<05:51,  2.44s/it]Running Inference:  28%|██▊       | 57/200 [02:16<06:38,  2.79s/it]Running Inference:  29%|██▉       | 58/200 [02:17<05:45,  2.43s/it]Running Inference:  30%|██▉       | 59/200 [02:18<04:53,  2.08s/it]Running Inference:  30%|███       | 60/200 [02:21<04:54,  2.10s/it]Running Inference:  30%|███       | 61/200 [02:23<04:46,  2.06s/it]Running Inference:  31%|███       | 62/200 [02:25<04:51,  2.11s/it]Running Inference:  32%|███▏      | 63/200 [02:28<05:48,  2.54s/it]Running Inference:  32%|███▏      | 64/200 [02:33<06:54,  3.05s/it]Running Inference:  32%|███▎      | 65/200 [02:35<06:15,  2.78s/it]Running Inference:  33%|███▎      | 66/200 [02:37<05:51,  2.62s/it]Running Inference:  34%|███▎      | 67/200 [02:38<04:59,  2.25s/it]Running Inference:  34%|███▍      | 68/200 [02:40<04:41,  2.13s/it]Running Inference:  34%|███▍      | 69/200 [02:43<05:22,  2.46s/it]Running Inference:  35%|███▌      | 70/200 [02:47<06:03,  2.80s/it]Running Inference:  36%|███▌      | 71/200 [02:51<06:46,  3.15s/it]Running Inference:  36%|███▌      | 72/200 [02:53<06:04,  2.85s/it]Running Inference:  36%|███▋      | 73/200 [02:55<05:37,  2.66s/it]Running Inference:  37%|███▋      | 74/200 [02:59<06:13,  2.96s/it]Running Inference:  38%|███▊      | 75/200 [03:01<05:21,  2.57s/it]Running Inference:  38%|███▊      | 76/200 [03:04<05:37,  2.72s/it]Running Inference:  38%|███▊      | 77/200 [03:05<04:25,  2.16s/it]Running Inference:  39%|███▉      | 78/200 [03:07<04:15,  2.09s/it]Running Inference:  40%|███▉      | 79/200 [03:08<03:56,  1.95s/it]Running Inference:  40%|████      | 80/200 [03:09<03:15,  1.63s/it]Running Inference:  40%|████      | 81/200 [03:13<04:47,  2.42s/it]Running Inference:  41%|████      | 82/200 [03:15<04:23,  2.23s/it]Running Inference:  42%|████▏     | 83/200 [03:18<04:56,  2.53s/it]Running Inference:  42%|████▏     | 84/200 [03:22<05:31,  2.86s/it]Running Inference:  42%|████▎     | 85/200 [03:24<05:08,  2.69s/it]Running Inference:  43%|████▎     | 86/200 [03:27<04:57,  2.61s/it]Running Inference:  44%|████▎     | 87/200 [03:31<05:39,  3.01s/it]Running Inference:  44%|████▍     | 88/200 [03:32<04:44,  2.54s/it]Running Inference:  44%|████▍     | 89/200 [03:34<04:34,  2.47s/it]Running Inference:  45%|████▌     | 90/200 [03:36<04:12,  2.29s/it]Running Inference:  46%|████▌     | 91/200 [03:37<03:24,  1.87s/it]Running Inference:  46%|████▌     | 92/200 [03:40<03:59,  2.21s/it]Running Inference:  46%|████▋     | 93/200 [03:42<03:38,  2.05s/it]Running Inference:  47%|████▋     | 94/200 [03:45<04:25,  2.51s/it]Running Inference:  48%|████▊     | 95/200 [03:47<03:56,  2.26s/it]Running Inference:  48%|████▊     | 96/200 [03:49<03:51,  2.23s/it]Running Inference:  48%|████▊     | 97/200 [03:51<03:37,  2.11s/it]Running Inference:  49%|████▉     | 98/200 [03:55<04:37,  2.72s/it]Running Inference:  50%|████▉     | 99/200 [03:57<04:09,  2.47s/it]Running Inference:  50%|█████     | 100/200 [03:59<04:01,  2.41s/it]Running Inference:  50%|█████     | 101/200 [04:02<04:02,  2.45s/it]Running Inference:  51%|█████     | 102/200 [04:04<03:49,  2.34s/it]Running Inference:  52%|█████▏    | 103/200 [04:05<03:02,  1.88s/it]Running Inference:  52%|█████▏    | 104/200 [04:06<02:51,  1.78s/it]Running Inference:  52%|█████▎    | 105/200 [04:08<02:39,  1.68s/it]Running Inference:  53%|█████▎    | 106/200 [04:10<02:46,  1.77s/it]Running Inference:  54%|█████▎    | 107/200 [04:13<03:15,  2.10s/it]Running Inference:  54%|█████▍    | 108/200 [04:13<02:34,  1.68s/it]Running Inference:  55%|█████▍    | 109/200 [04:15<02:27,  1.62s/it]Running Inference:  55%|█████▌    | 110/200 [04:16<02:14,  1.50s/it]Running Inference:  56%|█████▌    | 111/200 [04:18<02:34,  1.74s/it]Running Inference:  56%|█████▌    | 112/200 [04:22<03:35,  2.45s/it]Running Inference:  56%|█████▋    | 113/200 [04:25<03:32,  2.44s/it]Running Inference:  57%|█████▋    | 114/200 [04:27<03:24,  2.37s/it]Running Inference:  57%|█████▊    | 115/200 [04:29<03:19,  2.35s/it]Running Inference:  58%|█████▊    | 116/200 [04:32<03:26,  2.46s/it]Running Inference:  58%|█████▊    | 117/200 [04:34<03:22,  2.44s/it]Running Inference:  59%|█████▉    | 118/200 [04:37<03:15,  2.39s/it]Running Inference:  60%|█████▉    | 119/200 [04:38<02:38,  1.95s/it]Running Inference:  60%|██████    | 120/200 [04:41<03:09,  2.37s/it]Running Inference:  60%|██████    | 121/200 [04:43<03:05,  2.35s/it]Running Inference:  61%|██████    | 122/200 [04:45<02:58,  2.29s/it]Running Inference:  62%|██████▏   | 123/200 [04:48<02:53,  2.25s/it]Running Inference:  62%|██████▏   | 124/200 [04:50<02:44,  2.17s/it]Running Inference:  62%|██████▎   | 125/200 [04:52<02:40,  2.14s/it]Running Inference:  63%|██████▎   | 126/200 [04:54<02:40,  2.17s/it]Running Inference:  64%|██████▎   | 127/200 [04:55<02:07,  1.75s/it]Running Inference:  64%|██████▍   | 128/200 [04:57<02:24,  2.00s/it]Running Inference:  64%|██████▍   | 129/200 [04:59<02:25,  2.05s/it]Running Inference:  65%|██████▌   | 130/200 [05:01<02:18,  1.98s/it]Running Inference:  66%|██████▌   | 131/200 [05:05<02:55,  2.55s/it]Running Inference:  66%|██████▌   | 132/200 [05:07<02:30,  2.21s/it]Running Inference:  66%|██████▋   | 133/200 [05:08<02:11,  1.96s/it]Running Inference:  67%|██████▋   | 134/200 [05:10<02:16,  2.07s/it]Running Inference:  68%|██████▊   | 135/200 [05:12<02:15,  2.08s/it]Running Inference:  68%|██████▊   | 136/200 [05:14<02:12,  2.07s/it]Running Inference:  68%|██████▊   | 137/200 [05:16<01:59,  1.89s/it]Running Inference:  69%|██████▉   | 138/200 [05:18<02:03,  1.99s/it]Running Inference:  70%|██████▉   | 139/200 [05:22<02:41,  2.65s/it]Running Inference:  70%|███████   | 140/200 [05:26<03:01,  3.02s/it]Running Inference:  70%|███████   | 141/200 [05:28<02:44,  2.78s/it]Running Inference:  71%|███████   | 142/200 [05:32<02:48,  2.91s/it]Running Inference:  72%|███████▏  | 143/200 [05:32<02:07,  2.24s/it]Running Inference:  72%|███████▏  | 144/200 [05:33<01:41,  1.82s/it]Running Inference:  72%|███████▎  | 145/200 [05:33<01:15,  1.38s/it]Running Inference:  73%|███████▎  | 146/200 [05:38<02:00,  2.22s/it]Running Inference:  74%|███████▎  | 147/200 [05:40<01:56,  2.20s/it]Running Inference:  74%|███████▍  | 148/200 [05:42<01:49,  2.11s/it]Running Inference:  74%|███████▍  | 149/200 [05:42<01:24,  1.66s/it]Running Inference:  75%|███████▌  | 150/200 [05:44<01:28,  1.77s/it]Running Inference:  76%|███████▌  | 151/200 [05:46<01:30,  1.84s/it]Running Inference:  76%|███████▌  | 152/200 [05:48<01:31,  1.91s/it]Running Inference:  76%|███████▋  | 153/200 [05:50<01:31,  1.94s/it]Running Inference:  77%|███████▋  | 154/200 [05:53<01:34,  2.06s/it]Running Inference:  78%|███████▊  | 155/200 [05:57<02:00,  2.67s/it]Running Inference:  78%|███████▊  | 156/200 [05:58<01:37,  2.21s/it]Running Inference:  78%|███████▊  | 157/200 [05:59<01:25,  1.98s/it]Running Inference:  79%|███████▉  | 158/200 [06:00<01:07,  1.60s/it]Running Inference:  80%|███████▉  | 159/200 [06:02<01:04,  1.56s/it]Running Inference:  80%|████████  | 160/200 [06:06<01:33,  2.33s/it]Running Inference:  80%|████████  | 161/200 [06:09<01:43,  2.65s/it]Running Inference:  81%|████████  | 162/200 [06:10<01:16,  2.03s/it]Running Inference:  82%|████████▏ | 163/200 [06:11<01:11,  1.93s/it]Running Inference:  82%|████████▏ | 164/200 [06:14<01:11,  2.00s/it]Running Inference:  82%|████████▎ | 165/200 [06:18<01:32,  2.63s/it]Running Inference:  83%|████████▎ | 166/200 [06:20<01:23,  2.46s/it]Running Inference:  84%|████████▎ | 167/200 [06:22<01:16,  2.33s/it]Running Inference:  84%|████████▍ | 168/200 [06:26<01:30,  2.83s/it]Running Inference:  84%|████████▍ | 169/200 [06:30<01:40,  3.25s/it]Running Inference:  85%|████████▌ | 170/200 [06:32<01:26,  2.90s/it]Running Inference:  86%|████████▌ | 171/200 [06:35<01:20,  2.79s/it]Running Inference:  86%|████████▌ | 172/200 [06:37<01:12,  2.60s/it]Running Inference:  86%|████████▋ | 173/200 [06:39<01:06,  2.46s/it]Running Inference:  87%|████████▋ | 174/200 [06:43<01:13,  2.83s/it]Running Inference:  88%|████████▊ | 175/200 [06:45<01:06,  2.65s/it]Running Inference:  88%|████████▊ | 176/200 [06:47<01:00,  2.51s/it]Running Inference:  88%|████████▊ | 177/200 [06:49<00:56,  2.46s/it]Running Inference:  89%|████████▉ | 178/200 [06:54<01:05,  2.99s/it]Running Inference:  90%|████████▉ | 179/200 [06:56<00:58,  2.78s/it]Running Inference:  90%|█████████ | 180/200 [06:59<01:00,  3.01s/it]Running Inference:  90%|█████████ | 181/200 [07:02<00:53,  2.80s/it]Running Inference:  91%|█████████ | 182/200 [07:04<00:45,  2.52s/it]Running Inference:  92%|█████████▏| 183/200 [07:05<00:37,  2.23s/it]Running Inference:  92%|█████████▏| 184/200 [07:09<00:44,  2.75s/it]Running Inference:  92%|█████████▎| 185/200 [07:11<00:35,  2.40s/it]Running Inference:  93%|█████████▎| 186/200 [07:13<00:32,  2.31s/it]Running Inference:  94%|█████████▎| 187/200 [07:15<00:28,  2.17s/it]Running Inference:  94%|█████████▍| 188/200 [07:15<00:20,  1.74s/it]Running Inference:  94%|█████████▍| 189/200 [07:20<00:27,  2.49s/it]Running Inference:  95%|█████████▌| 190/200 [07:21<00:20,  2.07s/it]Running Inference:  96%|█████████▌| 191/200 [07:23<00:18,  2.09s/it]Running Inference:  96%|█████████▌| 192/200 [07:24<00:14,  1.87s/it]Running Inference:  96%|█████████▋| 193/200 [07:29<00:18,  2.61s/it]Running Inference:  97%|█████████▋| 194/200 [07:32<00:17,  2.88s/it]Running Inference:  98%|█████████▊| 195/200 [07:33<00:11,  2.36s/it]Running Inference:  98%|█████████▊| 196/200 [07:35<00:09,  2.32s/it]Running Inference:  98%|█████████▊| 197/200 [07:37<00:06,  2.05s/it]Running Inference:  99%|█████████▉| 198/200 [07:40<00:04,  2.27s/it]Running Inference: 100%|█████████▉| 199/200 [07:42<00:02,  2.31s/it]Running Inference: 100%|██████████| 200/200 [07:43<00:00,  1.78s/it]Running Inference: 100%|██████████| 200/200 [07:43<00:00,  2.32s/it]
2025-12-13 18:50:40,315 - INFO - Inference completed.
2025-12-13 18:50:40,323 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-13 18:50:40,323 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:50:40,327 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-13 18:50:40,327 - INFO - Metrics:
40.64
2025-12-13 18:50:40,328 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa, ratio=0.5) on GPU 1


========================================
LongBench Task: multifieldqa_en
========================================
----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:50:46,661 - INFO - Set deterministic seeds to 42
2025-12-13 18:50:46,661 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:50:46,661 - INFO - Starting evaluation run...
2025-12-13 18:50:46,661 - INFO - Output directory set to: longbenchresult
2025-12-13 18:50:46,661 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 18:50:46,661 - INFO - KV Press 'tova' setup.
2025-12-13 18:50:46,661 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:50:46,661 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.29it/s]
Device set to use cuda:0
2025-12-13 18:50:59,029 - INFO - Model pipeline loaded.
2025-12-13 18:50:59,029 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 150 examples [00:00, 3563.68 examples/s]
2025-12-13 18:51:03,937 - INFO - Dataset loaded with 150 entries.
2025-12-13 18:51:03,937 - INFO - Dataset processed with 150 entries.
2025-12-13 18:51:03,949 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:03<05:43,  3.09s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:04,  1.68s/it]Running Inference:   3%|▎         | 3/112 [00:05<02:50,  1.56s/it]Running Inference:   4%|▎         | 4/112 [00:07<03:38,  2.02s/it]Running Inference:   4%|▍         | 5/112 [00:12<04:55,  2.76s/it]Running Inference:   5%|▌         | 6/112 [00:13<04:10,  2.36s/it]Running Inference:   6%|▋         | 7/112 [00:17<05:06,  2.92s/it]Running Inference:   7%|▋         | 8/112 [00:19<04:26,  2.57s/it]Running Inference:   8%|▊         | 9/112 [00:20<03:34,  2.09s/it]Running Inference:   9%|▉         | 10/112 [00:25<05:05,  3.00s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:27<04:25,  2.63s/it]Running Inference:  11%|█         | 12/112 [00:29<04:00,  2.41s/it]Running Inference:  12%|█▏        | 13/112 [00:30<03:26,  2.09s/it]Running Inference:  12%|█▎        | 14/112 [00:32<03:14,  1.99s/it]Running Inference:  13%|█▎        | 15/112 [00:33<02:36,  1.61s/it]Running Inference:  14%|█▍        | 16/112 [00:33<02:11,  1.37s/it]Running Inference:  15%|█▌        | 17/112 [00:34<01:43,  1.09s/it]Running Inference:  16%|█▌        | 18/112 [00:35<01:53,  1.21s/it]Running Inference:  17%|█▋        | 19/112 [00:40<03:29,  2.25s/it]Running Inference:  18%|█▊        | 20/112 [00:42<03:19,  2.17s/it]Running Inference:  19%|█▉        | 21/112 [00:44<03:12,  2.12s/it]Running Inference:  20%|█▉        | 22/112 [00:46<03:07,  2.09s/it]Running Inference:  21%|██        | 23/112 [00:48<02:50,  1.92s/it]Running Inference:  21%|██▏       | 24/112 [00:49<02:35,  1.76s/it]Running Inference:  22%|██▏       | 25/112 [00:50<02:13,  1.54s/it]Running Inference:  23%|██▎       | 26/112 [00:55<03:32,  2.47s/it]Running Inference:  24%|██▍       | 27/112 [00:57<03:25,  2.41s/it]Running Inference:  25%|██▌       | 28/112 [00:58<02:59,  2.13s/it]Running Inference:  26%|██▌       | 29/112 [01:00<02:36,  1.88s/it]Running Inference:  27%|██▋       | 30/112 [01:03<03:01,  2.21s/it]Running Inference:  28%|██▊       | 31/112 [01:04<02:31,  1.87s/it]Running Inference:  29%|██▊       | 32/112 [01:06<02:38,  1.98s/it]Running Inference:  29%|██▉       | 33/112 [01:08<02:44,  2.08s/it]Running Inference:  30%|███       | 34/112 [01:10<02:44,  2.11s/it]Running Inference:  31%|███▏      | 35/112 [01:14<03:17,  2.57s/it]Running Inference:  32%|███▏      | 36/112 [01:15<02:30,  1.98s/it]Running Inference:  33%|███▎      | 37/112 [01:15<01:54,  1.52s/it]Running Inference:  34%|███▍      | 38/112 [01:16<01:40,  1.36s/it]Running Inference:  35%|███▍      | 39/112 [01:21<02:49,  2.33s/it]Running Inference:  36%|███▌      | 40/112 [01:22<02:24,  2.00s/it]Running Inference:  37%|███▋      | 41/112 [01:23<02:04,  1.75s/it]Running Inference:  38%|███▊      | 42/112 [01:27<02:38,  2.26s/it]Running Inference:  38%|███▊      | 43/112 [01:30<03:00,  2.62s/it]Running Inference:  39%|███▉      | 44/112 [01:34<03:17,  2.90s/it]Running Inference:  40%|████      | 45/112 [01:38<03:41,  3.31s/it]Running Inference:  41%|████      | 46/112 [01:39<02:57,  2.68s/it]Running Inference:  42%|████▏     | 47/112 [01:40<02:23,  2.21s/it]Running Inference:  43%|████▎     | 48/112 [01:42<02:15,  2.11s/it]Running Inference:  44%|████▍     | 49/112 [01:46<02:39,  2.54s/it]Running Inference:  45%|████▍     | 50/112 [01:47<02:17,  2.21s/it]Running Inference:  46%|████▌     | 51/112 [01:48<01:52,  1.85s/it]Running Inference:  46%|████▋     | 52/112 [01:50<01:54,  1.91s/it]Running Inference:  47%|████▋     | 53/112 [01:52<01:59,  2.03s/it]Running Inference:  48%|████▊     | 54/112 [01:54<01:53,  1.96s/it]Running Inference:  49%|████▉     | 55/112 [01:59<02:41,  2.83s/it]Running Inference:  50%|█████     | 56/112 [02:00<02:03,  2.21s/it]Running Inference:  51%|█████     | 57/112 [02:02<01:54,  2.09s/it]Running Inference:  52%|█████▏    | 58/112 [02:05<02:13,  2.47s/it]Running Inference:  53%|█████▎    | 59/112 [02:09<02:40,  3.03s/it]Running Inference:  54%|█████▎    | 60/112 [02:10<02:03,  2.37s/it]Running Inference:  54%|█████▍    | 61/112 [02:13<02:05,  2.46s/it]Running Inference:  55%|█████▌    | 62/112 [02:15<01:53,  2.26s/it]Running Inference:  56%|█████▋    | 63/112 [02:16<01:35,  1.94s/it]Running Inference:  57%|█████▋    | 64/112 [02:19<01:48,  2.26s/it]Running Inference:  58%|█████▊    | 65/112 [02:21<01:40,  2.14s/it]Running Inference:  59%|█████▉    | 66/112 [02:25<02:15,  2.96s/it]Running Inference:  60%|█████▉    | 67/112 [02:28<02:05,  2.79s/it]Running Inference:  61%|██████    | 68/112 [02:29<01:43,  2.35s/it]Running Inference:  62%|██████▏   | 69/112 [02:30<01:24,  1.97s/it]Running Inference:  62%|██████▎   | 70/112 [02:31<01:07,  1.61s/it]Running Inference:  63%|██████▎   | 71/112 [02:32<00:56,  1.38s/it]Running Inference:  64%|██████▍   | 72/112 [02:33<00:51,  1.28s/it]Running Inference:  65%|██████▌   | 73/112 [02:35<00:59,  1.52s/it]Running Inference:  66%|██████▌   | 74/112 [02:39<01:20,  2.12s/it]Running Inference:  67%|██████▋   | 75/112 [02:44<01:57,  3.16s/it]Running Inference:  68%|██████▊   | 76/112 [02:45<01:25,  2.36s/it]Running Inference:  69%|██████▉   | 77/112 [02:46<01:09,  1.99s/it]Running Inference:  70%|██████▉   | 78/112 [02:50<01:28,  2.60s/it]Running Inference:  71%|███████   | 79/112 [02:51<01:13,  2.21s/it]Running Inference:  71%|███████▏  | 80/112 [02:53<01:04,  2.03s/it]Running Inference:  72%|███████▏  | 81/112 [02:54<00:57,  1.87s/it]Running Inference:  73%|███████▎  | 82/112 [02:57<01:07,  2.25s/it]Running Inference:  74%|███████▍  | 83/112 [03:05<01:49,  3.79s/it]Running Inference:  75%|███████▌  | 84/112 [03:10<01:56,  4.17s/it]Running Inference:  76%|███████▌  | 85/112 [03:10<01:21,  3.03s/it]Running Inference:  77%|███████▋  | 86/112 [03:11<01:04,  2.47s/it]Running Inference:  78%|███████▊  | 87/112 [03:13<00:52,  2.09s/it]Running Inference:  79%|███████▊  | 88/112 [03:15<00:55,  2.30s/it]Running Inference:  79%|███████▉  | 89/112 [03:21<01:19,  3.45s/it]Running Inference:  80%|████████  | 90/112 [03:23<01:05,  3.00s/it]Running Inference:  81%|████████▏ | 91/112 [03:28<01:12,  3.47s/it]Running Inference:  82%|████████▏ | 92/112 [03:30<01:01,  3.09s/it]Running Inference:  83%|████████▎ | 93/112 [03:34<01:03,  3.36s/it]Running Inference:  84%|████████▍ | 94/112 [03:35<00:48,  2.72s/it]Running Inference:  85%|████████▍ | 95/112 [03:37<00:42,  2.48s/it]Running Inference:  86%|████████▌ | 96/112 [03:38<00:32,  2.02s/it]Running Inference:  87%|████████▋ | 97/112 [03:41<00:33,  2.22s/it]Running Inference:  88%|████████▊ | 98/112 [03:46<00:41,  2.99s/it]Running Inference:  88%|████████▊ | 99/112 [03:48<00:34,  2.68s/it]Running Inference:  89%|████████▉ | 100/112 [03:55<00:48,  4.04s/it]Running Inference:  90%|█████████ | 101/112 [03:57<00:36,  3.34s/it]Running Inference:  91%|█████████ | 102/112 [04:01<00:37,  3.71s/it]Running Inference:  92%|█████████▏| 103/112 [04:03<00:27,  3.01s/it]Running Inference:  93%|█████████▎| 104/112 [04:04<00:19,  2.44s/it]Running Inference:  94%|█████████▍| 105/112 [04:05<00:14,  2.01s/it]Running Inference:  95%|█████████▍| 106/112 [04:08<00:14,  2.41s/it]Running Inference:  96%|█████████▌| 107/112 [04:11<00:12,  2.50s/it]Running Inference:  96%|█████████▋| 108/112 [04:18<00:16,  4.08s/it]Running Inference:  97%|█████████▋| 109/112 [04:20<00:10,  3.44s/it]Running Inference:  98%|█████████▊| 110/112 [04:22<00:05,  2.85s/it]Running Inference:  99%|█████████▉| 111/112 [04:26<00:03,  3.13s/it]Running Inference: 100%|██████████| 112/112 [04:30<00:00,  3.43s/it]Running Inference: 100%|██████████| 112/112 [04:30<00:00,  2.41s/it]
2025-12-13 18:55:34,263 - INFO - Inference completed.
2025-12-13 18:55:34,271 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 18:55:34,271 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:55:34,276 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 18:55:34,276 - INFO - Metrics:
33.31
2025-12-13 18:55:34,277 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_en, ratio=0.1) on GPU 1

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:55:40,740 - INFO - Set deterministic seeds to 42
2025-12-13 18:55:40,740 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:55:40,740 - INFO - Starting evaluation run...
2025-12-13 18:55:40,740 - INFO - Output directory set to: longbenchresult
2025-12-13 18:55:40,740 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 18:55:40,740 - INFO - KV Press 'tova' setup.
2025-12-13 18:55:40,740 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:55:40,740 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.68it/s]
Device set to use cuda:0
2025-12-13 18:55:53,751 - INFO - Model pipeline loaded.
2025-12-13 18:55:53,751 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 18:56:33,251 - INFO - Dataset loaded with 150 entries.
2025-12-13 18:56:33,251 - INFO - Dataset processed with 150 entries.
2025-12-13 18:56:33,263 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:03<05:42,  3.09s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:05,  1.68s/it]Running Inference:   3%|▎         | 3/112 [00:05<02:51,  1.57s/it]Running Inference:   4%|▎         | 4/112 [00:07<03:37,  2.01s/it]Running Inference:   4%|▍         | 5/112 [00:12<04:55,  2.76s/it]Running Inference:   5%|▌         | 6/112 [00:13<04:09,  2.35s/it]Running Inference:   6%|▋         | 7/112 [00:16<04:41,  2.68s/it]Running Inference:   7%|▋         | 8/112 [00:18<04:09,  2.40s/it]Running Inference:   8%|▊         | 9/112 [00:19<03:22,  1.97s/it]Running Inference:   9%|▉         | 10/112 [00:24<04:35,  2.70s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:25<04:04,  2.42s/it]Running Inference:  11%|█         | 12/112 [00:27<03:50,  2.30s/it]Running Inference:  12%|█▏        | 13/112 [00:29<03:19,  2.02s/it]Running Inference:  12%|█▎        | 14/112 [00:31<03:10,  1.94s/it]Running Inference:  13%|█▎        | 15/112 [00:31<02:27,  1.52s/it]Running Inference:  14%|█▍        | 16/112 [00:32<02:04,  1.30s/it]Running Inference:  15%|█▌        | 17/112 [00:32<01:39,  1.05s/it]Running Inference:  16%|█▌        | 18/112 [00:34<01:50,  1.18s/it]Running Inference:  17%|█▋        | 19/112 [00:38<03:27,  2.23s/it]Running Inference:  18%|█▊        | 20/112 [00:40<03:17,  2.14s/it]Running Inference:  19%|█▉        | 21/112 [00:43<03:14,  2.14s/it]Running Inference:  20%|█▉        | 22/112 [00:45<03:08,  2.10s/it]Running Inference:  21%|██        | 23/112 [00:46<02:51,  1.93s/it]Running Inference:  21%|██▏       | 24/112 [00:47<02:35,  1.77s/it]Running Inference:  22%|██▏       | 25/112 [00:49<02:14,  1.55s/it]Running Inference:  23%|██▎       | 26/112 [00:53<03:31,  2.46s/it]Running Inference:  24%|██▍       | 27/112 [00:55<03:26,  2.43s/it]Running Inference:  25%|██▌       | 28/112 [00:57<02:57,  2.12s/it]Running Inference:  26%|██▌       | 29/112 [00:58<02:35,  1.87s/it]Running Inference:  27%|██▋       | 30/112 [01:01<03:00,  2.20s/it]Running Inference:  28%|██▊       | 31/112 [01:02<02:31,  1.87s/it]Running Inference:  29%|██▊       | 32/112 [01:04<02:32,  1.91s/it]Running Inference:  29%|██▉       | 33/112 [01:07<02:40,  2.04s/it]Running Inference:  30%|███       | 34/112 [01:09<02:45,  2.12s/it]Running Inference:  31%|███▏      | 35/112 [01:12<03:11,  2.49s/it]Running Inference:  32%|███▏      | 36/112 [01:13<02:25,  1.92s/it]Running Inference:  33%|███▎      | 37/112 [01:13<01:51,  1.49s/it]Running Inference:  34%|███▍      | 38/112 [01:14<01:38,  1.34s/it]Running Inference:  35%|███▍      | 39/112 [01:19<02:49,  2.32s/it]Running Inference:  36%|███▌      | 40/112 [01:20<02:24,  2.00s/it]Running Inference:  37%|███▋      | 41/112 [01:21<02:04,  1.75s/it]Running Inference:  38%|███▊      | 42/112 [01:28<03:40,  3.15s/it]Running Inference:  38%|███▊      | 43/112 [01:30<03:14,  2.81s/it]Running Inference:  39%|███▉      | 44/112 [01:33<03:26,  3.04s/it]Running Inference:  40%|████      | 45/112 [01:38<03:49,  3.42s/it]Running Inference:  41%|████      | 46/112 [01:39<03:02,  2.76s/it]Running Inference:  42%|████▏     | 47/112 [01:40<02:26,  2.26s/it]Running Inference:  43%|████▎     | 48/112 [01:42<02:17,  2.15s/it]Running Inference:  44%|████▍     | 49/112 [01:45<02:41,  2.57s/it]Running Inference:  45%|████▍     | 50/112 [01:47<02:18,  2.24s/it]Running Inference:  46%|████▌     | 51/112 [01:48<01:53,  1.87s/it]Running Inference:  46%|████▋     | 52/112 [01:50<01:56,  1.94s/it]Running Inference:  47%|████▋     | 53/112 [01:52<02:00,  2.05s/it]Running Inference:  48%|████▊     | 54/112 [01:54<01:54,  1.98s/it]Running Inference:  49%|████▉     | 55/112 [01:59<02:42,  2.85s/it]Running Inference:  50%|█████     | 56/112 [02:00<02:04,  2.22s/it]Running Inference:  51%|█████     | 57/112 [02:02<01:57,  2.13s/it]Running Inference:  52%|█████▏    | 58/112 [02:05<02:08,  2.39s/it]Running Inference:  53%|█████▎    | 59/112 [02:09<02:38,  2.99s/it]Running Inference:  54%|█████▎    | 60/112 [02:10<02:01,  2.34s/it]Running Inference:  54%|█████▍    | 61/112 [02:12<02:00,  2.36s/it]Running Inference:  55%|█████▌    | 62/112 [02:14<01:43,  2.07s/it]Running Inference:  56%|█████▋    | 63/112 [02:15<01:28,  1.81s/it]Running Inference:  57%|█████▋    | 64/112 [02:19<01:54,  2.38s/it]Running Inference:  58%|█████▊    | 65/112 [02:20<01:38,  2.10s/it]Running Inference:  59%|█████▉    | 66/112 [02:23<01:53,  2.48s/it]Running Inference:  60%|█████▉    | 67/112 [02:26<01:49,  2.43s/it]Running Inference:  61%|██████    | 68/112 [02:27<01:38,  2.23s/it]Running Inference:  62%|██████▏   | 69/112 [02:28<01:21,  1.88s/it]Running Inference:  62%|██████▎   | 70/112 [02:29<01:05,  1.55s/it]Running Inference:  63%|██████▎   | 71/112 [02:30<00:54,  1.34s/it]Running Inference:  64%|██████▍   | 72/112 [02:31<00:49,  1.25s/it]Running Inference:  65%|██████▌   | 73/112 [02:33<00:58,  1.51s/it]Running Inference:  66%|██████▌   | 74/112 [02:37<01:23,  2.20s/it]Running Inference:  67%|██████▋   | 75/112 [02:43<01:59,  3.23s/it]Running Inference:  68%|██████▊   | 76/112 [02:43<01:26,  2.41s/it]Running Inference:  69%|██████▉   | 77/112 [02:44<01:10,  2.02s/it]Running Inference:  70%|██████▉   | 78/112 [02:50<01:44,  3.07s/it]Running Inference:  71%|███████   | 79/112 [02:51<01:24,  2.55s/it]Running Inference:  71%|███████▏  | 80/112 [02:53<01:12,  2.26s/it]Running Inference:  72%|███████▏  | 81/112 [02:54<01:03,  2.03s/it]Running Inference:  73%|███████▎  | 82/112 [02:57<01:11,  2.37s/it]Running Inference:  74%|███████▍  | 83/112 [03:07<02:13,  4.59s/it]Running Inference:  75%|███████▌  | 84/112 [03:12<02:12,  4.74s/it]Running Inference:  76%|███████▌  | 85/112 [03:13<01:32,  3.43s/it]Running Inference:  77%|███████▋  | 86/112 [03:13<01:08,  2.64s/it]Running Inference:  78%|███████▊  | 87/112 [03:15<00:55,  2.20s/it]Running Inference:  79%|███████▊  | 88/112 [03:16<00:49,  2.08s/it]Running Inference:  79%|███████▉  | 89/112 [03:22<01:11,  3.12s/it]Running Inference:  80%|████████  | 90/112 [03:24<01:00,  2.76s/it]Running Inference:  81%|████████▏ | 91/112 [03:28<01:09,  3.31s/it]Running Inference:  82%|████████▏ | 92/112 [03:31<00:59,  2.99s/it]Running Inference:  83%|████████▎ | 93/112 [03:35<01:02,  3.29s/it]Running Inference:  84%|████████▍ | 94/112 [03:36<00:48,  2.67s/it]Running Inference:  85%|████████▍ | 95/112 [03:38<00:40,  2.39s/it]Running Inference:  86%|████████▌ | 96/112 [03:39<00:31,  1.96s/it]Running Inference:  87%|████████▋ | 97/112 [03:44<00:45,  3.01s/it]Running Inference:  88%|████████▊ | 98/112 [03:49<00:49,  3.55s/it]Running Inference:  88%|████████▊ | 99/112 [03:51<00:39,  3.07s/it]Running Inference:  89%|████████▉ | 100/112 [03:58<00:51,  4.29s/it]Running Inference:  90%|█████████ | 101/112 [04:00<00:38,  3.52s/it]Running Inference:  91%|█████████ | 102/112 [04:04<00:38,  3.84s/it]Running Inference:  92%|█████████▏| 103/112 [04:06<00:27,  3.10s/it]Running Inference:  93%|█████████▎| 104/112 [04:07<00:19,  2.47s/it]Running Inference:  94%|█████████▍| 105/112 [04:08<00:14,  2.04s/it]Running Inference:  95%|█████████▍| 106/112 [04:11<00:14,  2.49s/it]Running Inference:  96%|█████████▌| 107/112 [04:14<00:12,  2.58s/it]Running Inference:  96%|█████████▋| 108/112 [04:22<00:16,  4.16s/it]Running Inference:  97%|█████████▋| 109/112 [04:24<00:10,  3.51s/it]Running Inference:  98%|█████████▊| 110/112 [04:25<00:05,  2.93s/it]Running Inference:  99%|█████████▉| 111/112 [04:28<00:02,  2.82s/it]Running Inference: 100%|██████████| 112/112 [04:32<00:00,  3.22s/it]Running Inference: 100%|██████████| 112/112 [04:32<00:00,  2.43s/it]
2025-12-13 19:01:05,969 - INFO - Inference completed.
2025-12-13 19:01:05,977 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 19:01:05,977 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:01:05,981 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 19:01:05,981 - INFO - Metrics:
32.91
2025-12-13 19:01:05,983 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_en, ratio=0.2) on GPU 1

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:01:12,453 - INFO - Set deterministic seeds to 42
2025-12-13 19:01:12,453 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:01:12,453 - INFO - Starting evaluation run...
2025-12-13 19:01:12,453 - INFO - Output directory set to: longbenchresult
2025-12-13 19:01:12,453 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 19:01:12,453 - INFO - KV Press 'tova' setup.
2025-12-13 19:01:12,453 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:01:12,453 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.26it/s]
Device set to use cuda:0
2025-12-13 19:01:25,777 - INFO - Model pipeline loaded.
2025-12-13 19:01:25,777 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:01:34,214 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:01:34,214 - INFO - Dataset processed with 150 entries.
2025-12-13 19:01:34,226 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:03<05:50,  3.15s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:06,  1.69s/it]Running Inference:   3%|▎         | 3/112 [00:05<02:46,  1.53s/it]Running Inference:   4%|▎         | 4/112 [00:07<03:37,  2.01s/it]Running Inference:   4%|▍         | 5/112 [00:11<04:53,  2.74s/it]Running Inference:   5%|▌         | 6/112 [00:13<04:07,  2.34s/it]Running Inference:   6%|▋         | 7/112 [00:16<04:29,  2.57s/it]Running Inference:   7%|▋         | 8/112 [00:18<04:00,  2.32s/it]Running Inference:   8%|▊         | 9/112 [00:19<03:16,  1.91s/it]Running Inference:   9%|▉         | 10/112 [00:24<04:50,  2.85s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:26<04:14,  2.52s/it]Running Inference:  11%|█         | 12/112 [00:27<03:50,  2.31s/it]Running Inference:  12%|█▏        | 13/112 [00:29<03:19,  2.02s/it]Running Inference:  12%|█▎        | 14/112 [00:30<03:10,  1.94s/it]Running Inference:  13%|█▎        | 15/112 [00:31<02:27,  1.52s/it]Running Inference:  14%|█▍        | 16/112 [00:32<02:04,  1.30s/it]Running Inference:  15%|█▌        | 17/112 [00:32<01:39,  1.04s/it]Running Inference:  16%|█▌        | 18/112 [00:37<03:15,  2.08s/it]Running Inference:  17%|█▋        | 19/112 [00:41<04:24,  2.84s/it]Running Inference:  18%|█▊        | 20/112 [00:43<03:56,  2.58s/it]Running Inference:  19%|█▉        | 21/112 [00:47<04:16,  2.82s/it]Running Inference:  20%|█▉        | 22/112 [00:49<03:51,  2.57s/it]Running Inference:  21%|██        | 23/112 [00:50<03:20,  2.26s/it]Running Inference:  21%|██▏       | 24/112 [00:52<02:54,  1.99s/it]Running Inference:  22%|██▏       | 25/112 [00:53<02:27,  1.70s/it]Running Inference:  23%|██▎       | 26/112 [00:55<02:46,  1.93s/it]Running Inference:  24%|██▍       | 27/112 [00:58<02:56,  2.07s/it]Running Inference:  25%|██▌       | 28/112 [00:59<02:36,  1.87s/it]Running Inference:  26%|██▌       | 29/112 [01:00<02:20,  1.70s/it]Running Inference:  27%|██▋       | 30/112 [01:03<02:43,  2.00s/it]Running Inference:  28%|██▊       | 31/112 [01:04<02:19,  1.72s/it]Running Inference:  29%|██▊       | 32/112 [01:07<02:48,  2.11s/it]Running Inference:  29%|██▉       | 33/112 [01:09<02:51,  2.17s/it]Running Inference:  30%|███       | 34/112 [01:11<02:38,  2.04s/it]Running Inference:  31%|███▏      | 35/112 [01:15<03:10,  2.48s/it]Running Inference:  32%|███▏      | 36/112 [01:15<02:25,  1.91s/it]Running Inference:  33%|███▎      | 37/112 [01:16<01:51,  1.49s/it]Running Inference:  34%|███▍      | 38/112 [01:17<01:38,  1.33s/it]Running Inference:  35%|███▍      | 39/112 [01:19<02:03,  1.70s/it]Running Inference:  36%|███▌      | 40/112 [01:20<01:52,  1.56s/it]Running Inference:  37%|███▋      | 41/112 [01:22<01:42,  1.44s/it]Running Inference:  38%|███▊      | 42/112 [01:25<02:19,  1.99s/it]Running Inference:  38%|███▊      | 43/112 [01:27<02:17,  2.00s/it]Running Inference:  39%|███▉      | 44/112 [01:30<02:43,  2.40s/it]Running Inference:  40%|████      | 45/112 [01:34<03:18,  2.96s/it]Running Inference:  41%|████      | 46/112 [01:36<02:40,  2.44s/it]Running Inference:  42%|████▏     | 47/112 [01:37<02:22,  2.19s/it]Running Inference:  43%|████▎     | 48/112 [01:39<02:14,  2.10s/it]Running Inference:  44%|████▍     | 49/112 [01:43<02:39,  2.52s/it]Running Inference:  45%|████▍     | 50/112 [01:44<02:18,  2.23s/it]Running Inference:  46%|████▌     | 51/112 [01:45<01:53,  1.86s/it]Running Inference:  46%|████▋     | 52/112 [01:47<01:55,  1.93s/it]Running Inference:  47%|████▋     | 53/112 [01:49<01:57,  2.00s/it]Running Inference:  48%|████▊     | 54/112 [01:51<01:52,  1.94s/it]Running Inference:  49%|████▉     | 55/112 [01:56<02:40,  2.81s/it]Running Inference:  50%|█████     | 56/112 [01:57<02:02,  2.19s/it]Running Inference:  51%|█████     | 57/112 [02:00<02:14,  2.45s/it]Running Inference:  52%|█████▏    | 58/112 [02:03<02:27,  2.72s/it]Running Inference:  53%|█████▎    | 59/112 [02:05<02:15,  2.55s/it]Running Inference:  54%|█████▎    | 60/112 [02:06<01:45,  2.03s/it]Running Inference:  54%|█████▍    | 61/112 [02:08<01:45,  2.08s/it]Running Inference:  55%|█████▌    | 62/112 [02:10<01:33,  1.87s/it]Running Inference:  56%|█████▋    | 63/112 [02:11<01:21,  1.67s/it]Running Inference:  57%|█████▋    | 64/112 [02:15<01:48,  2.26s/it]Running Inference:  58%|█████▊    | 65/112 [02:16<01:33,  1.99s/it]Running Inference:  59%|█████▉    | 66/112 [02:19<01:51,  2.42s/it]Running Inference:  60%|█████▉    | 67/112 [02:22<01:45,  2.33s/it]Running Inference:  61%|██████    | 68/112 [02:23<01:25,  1.95s/it]Running Inference:  62%|██████▏   | 69/112 [02:24<01:12,  1.69s/it]Running Inference:  62%|██████▎   | 70/112 [02:24<00:59,  1.41s/it]Running Inference:  63%|██████▎   | 71/112 [02:25<00:50,  1.24s/it]Running Inference:  64%|██████▍   | 72/112 [02:33<02:06,  3.15s/it]Running Inference:  65%|██████▌   | 73/112 [02:35<01:50,  2.84s/it]Running Inference:  66%|██████▌   | 74/112 [02:39<01:57,  3.08s/it]Running Inference:  67%|██████▋   | 75/112 [02:44<02:19,  3.77s/it]Running Inference:  68%|██████▊   | 76/112 [02:45<01:40,  2.79s/it]Running Inference:  69%|██████▉   | 77/112 [02:46<01:20,  2.29s/it]Running Inference:  70%|██████▉   | 78/112 [02:51<01:50,  3.24s/it]Running Inference:  71%|███████   | 79/112 [02:52<01:27,  2.67s/it]Running Inference:  71%|███████▏  | 80/112 [02:54<01:14,  2.34s/it]Running Inference:  72%|███████▏  | 81/112 [02:55<01:02,  2.01s/it]Running Inference:  73%|███████▎  | 82/112 [02:58<01:10,  2.35s/it]Running Inference:  74%|███████▍  | 83/112 [03:05<01:45,  3.64s/it]Running Inference:  75%|███████▌  | 84/112 [03:10<01:53,  4.06s/it]Running Inference:  76%|███████▌  | 85/112 [03:10<01:19,  2.95s/it]Running Inference:  77%|███████▋  | 86/112 [03:14<01:25,  3.28s/it]Running Inference:  78%|███████▊  | 87/112 [03:16<01:06,  2.65s/it]Running Inference:  79%|███████▊  | 88/112 [03:17<00:57,  2.39s/it]Running Inference:  79%|███████▉  | 89/112 [03:23<01:18,  3.40s/it]Running Inference:  80%|████████  | 90/112 [03:25<01:05,  2.96s/it]Running Inference:  81%|████████▏ | 91/112 [03:30<01:12,  3.44s/it]Running Inference:  82%|████████▏ | 92/112 [03:32<01:01,  3.07s/it]Running Inference:  83%|████████▎ | 93/112 [03:36<01:03,  3.34s/it]Running Inference:  84%|████████▍ | 94/112 [03:37<00:48,  2.70s/it]Running Inference:  85%|████████▍ | 95/112 [03:39<00:40,  2.36s/it]Running Inference:  86%|████████▌ | 96/112 [03:40<00:30,  1.94s/it]Running Inference:  87%|████████▋ | 97/112 [03:45<00:44,  2.98s/it]Running Inference:  88%|████████▊ | 98/112 [03:50<00:49,  3.52s/it]Running Inference:  88%|████████▊ | 99/112 [03:52<00:39,  3.05s/it]Running Inference:  89%|████████▉ | 100/112 [03:59<00:51,  4.25s/it]Running Inference:  90%|█████████ | 101/112 [04:00<00:38,  3.46s/it]Running Inference:  91%|█████████ | 102/112 [04:05<00:37,  3.79s/it]Running Inference:  92%|█████████▏| 103/112 [04:06<00:27,  3.06s/it]Running Inference:  93%|█████████▎| 104/112 [04:07<00:19,  2.44s/it]Running Inference:  94%|█████████▍| 105/112 [04:08<00:14,  2.01s/it]Running Inference:  95%|█████████▍| 106/112 [04:12<00:14,  2.48s/it]Running Inference:  96%|█████████▌| 107/112 [04:15<00:12,  2.54s/it]Running Inference:  96%|█████████▋| 108/112 [04:22<00:16,  4.11s/it]Running Inference:  97%|█████████▋| 109/112 [04:24<00:10,  3.37s/it]Running Inference:  98%|█████████▊| 110/112 [04:25<00:05,  2.74s/it]Running Inference:  99%|█████████▉| 111/112 [04:29<00:03,  3.07s/it]Running Inference: 100%|██████████| 112/112 [04:33<00:00,  3.39s/it]Running Inference: 100%|██████████| 112/112 [04:33<00:00,  2.44s/it]
2025-12-13 19:06:07,997 - INFO - Inference completed.
2025-12-13 19:06:08,004 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-13 19:06:08,004 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:06:08,009 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-13 19:06:08,009 - INFO - Metrics:
32.38
2025-12-13 19:06:08,011 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_en, ratio=0.3) on GPU 1

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:06:14,354 - INFO - Set deterministic seeds to 42
2025-12-13 19:06:14,354 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:06:14,354 - INFO - Starting evaluation run...
2025-12-13 19:06:14,354 - INFO - Output directory set to: longbenchresult
2025-12-13 19:06:14,354 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-13 19:06:14,354 - INFO - KV Press 'tova' setup.
2025-12-13 19:06:14,354 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:06:14,354 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.66it/s]
Device set to use cuda:0
2025-12-13 19:06:31,152 - INFO - Model pipeline loaded.
2025-12-13 19:06:31,152 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:06:38,352 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:06:38,352 - INFO - Dataset processed with 150 entries.
2025-12-13 19:06:38,364 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<05:06,  2.76s/it]Running Inference:   2%|▏         | 2/112 [00:03<02:48,  1.53s/it]Running Inference:   3%|▎         | 3/112 [00:04<02:39,  1.46s/it]Running Inference:   4%|▎         | 4/112 [00:06<02:54,  1.62s/it]Running Inference:   4%|▍         | 5/112 [00:09<03:45,  2.11s/it]Running Inference:   5%|▌         | 6/112 [00:11<03:23,  1.92s/it]Running Inference:   6%|▋         | 7/112 [00:13<03:32,  2.02s/it]Running Inference:   7%|▋         | 8/112 [00:15<03:22,  1.94s/it]Running Inference:   8%|▊         | 9/112 [00:16<02:48,  1.64s/it]Running Inference:   9%|▉         | 10/112 [00:20<04:12,  2.48s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:22<03:56,  2.34s/it]Running Inference:  11%|█         | 12/112 [00:24<03:44,  2.24s/it]Running Inference:  12%|█▏        | 13/112 [00:25<03:15,  1.97s/it]Running Inference:  12%|█▎        | 14/112 [00:27<03:06,  1.90s/it]Running Inference:  13%|█▎        | 15/112 [00:28<02:33,  1.58s/it]Running Inference:  14%|█▍        | 16/112 [00:29<02:08,  1.34s/it]Running Inference:  15%|█▌        | 17/112 [00:29<01:41,  1.07s/it]Running Inference:  16%|█▌        | 18/112 [00:31<01:53,  1.21s/it]Running Inference:  17%|█▋        | 19/112 [00:35<03:27,  2.23s/it]Running Inference:  18%|█▊        | 20/112 [00:37<03:17,  2.15s/it]Running Inference:  19%|█▉        | 21/112 [00:40<03:27,  2.28s/it]Running Inference:  20%|█▉        | 22/112 [00:42<03:17,  2.19s/it]Running Inference:  21%|██        | 23/112 [00:43<02:58,  2.00s/it]Running Inference:  21%|██▏       | 24/112 [00:45<02:37,  1.79s/it]Running Inference:  22%|██▏       | 25/112 [00:46<02:17,  1.58s/it]Running Inference:  23%|██▎       | 26/112 [00:48<02:42,  1.89s/it]Running Inference:  24%|██▍       | 27/112 [00:51<03:01,  2.14s/it]Running Inference:  25%|██▌       | 28/112 [00:53<02:40,  1.91s/it]Running Inference:  26%|██▌       | 29/112 [00:54<02:21,  1.70s/it]Running Inference:  27%|██▋       | 30/112 [00:57<02:48,  2.05s/it]Running Inference:  28%|██▊       | 31/112 [00:58<02:22,  1.76s/it]Running Inference:  29%|██▊       | 32/112 [01:00<02:24,  1.80s/it]Running Inference:  29%|██▉       | 33/112 [01:02<02:34,  1.96s/it]Running Inference:  30%|███       | 34/112 [01:04<02:24,  1.85s/it]Running Inference:  31%|███▏      | 35/112 [01:08<03:23,  2.64s/it]Running Inference:  32%|███▏      | 36/112 [01:09<02:33,  2.02s/it]Running Inference:  33%|███▎      | 37/112 [01:09<01:57,  1.56s/it]Running Inference:  34%|███▍      | 38/112 [01:10<01:42,  1.38s/it]Running Inference:  35%|███▍      | 39/112 [01:15<02:50,  2.34s/it]Running Inference:  36%|███▌      | 40/112 [01:17<02:43,  2.27s/it]Running Inference:  37%|███▋      | 41/112 [01:18<02:17,  1.94s/it]Running Inference:  38%|███▊      | 42/112 [01:21<02:39,  2.28s/it]Running Inference:  38%|███▊      | 43/112 [01:23<02:41,  2.34s/it]Running Inference:  39%|███▉      | 44/112 [01:26<02:42,  2.38s/it]Running Inference:  40%|████      | 45/112 [01:30<03:17,  2.94s/it]Running Inference:  41%|████      | 46/112 [01:31<02:40,  2.43s/it]Running Inference:  42%|████▏     | 47/112 [01:32<02:10,  2.01s/it]Running Inference:  43%|████▎     | 48/112 [01:34<02:06,  1.97s/it]Running Inference:  44%|████▍     | 49/112 [01:36<01:58,  1.89s/it]Running Inference:  45%|████▍     | 50/112 [01:37<01:49,  1.76s/it]Running Inference:  46%|████▌     | 51/112 [01:38<01:33,  1.53s/it]Running Inference:  46%|████▋     | 52/112 [01:41<01:48,  1.81s/it]Running Inference:  47%|████▋     | 53/112 [01:43<01:56,  1.98s/it]Running Inference:  48%|████▊     | 54/112 [01:45<01:47,  1.85s/it]Running Inference:  49%|████▉     | 55/112 [01:49<02:22,  2.51s/it]Running Inference:  50%|█████     | 56/112 [01:50<01:50,  1.98s/it]Running Inference:  51%|█████     | 57/112 [01:52<01:53,  2.06s/it]Running Inference:  52%|█████▏    | 58/112 [01:55<02:15,  2.51s/it]Running Inference:  53%|█████▎    | 59/112 [01:58<02:05,  2.38s/it]Running Inference:  54%|█████▎    | 60/112 [02:00<02:01,  2.33s/it]Running Inference:  54%|█████▍    | 61/112 [02:02<01:59,  2.35s/it]Running Inference:  55%|█████▌    | 62/112 [02:04<01:42,  2.06s/it]Running Inference:  56%|█████▋    | 63/112 [02:05<01:26,  1.77s/it]Running Inference:  57%|█████▋    | 64/112 [02:08<01:43,  2.15s/it]Running Inference:  58%|█████▊    | 65/112 [02:09<01:31,  1.94s/it]Running Inference:  59%|█████▉    | 66/112 [02:13<01:49,  2.39s/it]Running Inference:  60%|█████▉    | 67/112 [02:15<01:44,  2.31s/it]Running Inference:  61%|██████    | 68/112 [02:18<01:53,  2.59s/it]Running Inference:  62%|██████▏   | 69/112 [02:19<01:31,  2.13s/it]Running Inference:  62%|██████▎   | 70/112 [02:20<01:12,  1.72s/it]Running Inference:  63%|██████▎   | 71/112 [02:21<00:59,  1.45s/it]Running Inference:  64%|██████▍   | 72/112 [02:25<01:31,  2.28s/it]Running Inference:  65%|██████▌   | 73/112 [02:27<01:28,  2.26s/it]Running Inference:  66%|██████▌   | 74/112 [02:29<01:25,  2.26s/it]Running Inference:  67%|██████▋   | 75/112 [02:33<01:37,  2.63s/it]Running Inference:  68%|██████▊   | 76/112 [02:33<01:11,  1.99s/it]Running Inference:  69%|██████▉   | 77/112 [02:34<01:00,  1.73s/it]Running Inference:  70%|██████▉   | 78/112 [02:38<01:23,  2.45s/it]Running Inference:  71%|███████   | 79/112 [02:40<01:09,  2.11s/it]Running Inference:  71%|███████▏  | 80/112 [02:41<01:00,  1.90s/it]Running Inference:  72%|███████▏  | 81/112 [02:42<00:52,  1.70s/it]Running Inference:  73%|███████▎  | 82/112 [02:45<01:02,  2.09s/it]Running Inference:  74%|███████▍  | 83/112 [02:52<01:40,  3.46s/it]Running Inference:  75%|███████▌  | 84/112 [02:57<01:49,  3.91s/it]Running Inference:  76%|███████▌  | 85/112 [02:57<01:16,  2.84s/it]Running Inference:  77%|███████▋  | 86/112 [03:01<01:23,  3.20s/it]Running Inference:  78%|███████▊  | 87/112 [03:03<01:05,  2.62s/it]Running Inference:  79%|███████▊  | 88/112 [03:04<00:56,  2.37s/it]Running Inference:  79%|███████▉  | 89/112 [03:11<01:26,  3.74s/it]Running Inference:  80%|████████  | 90/112 [03:13<01:10,  3.19s/it]Running Inference:  81%|████████▏ | 91/112 [03:17<01:12,  3.48s/it]Running Inference:  82%|████████▏ | 92/112 [03:20<01:01,  3.10s/it]Running Inference:  83%|████████▎ | 93/112 [03:24<01:03,  3.36s/it]Running Inference:  84%|████████▍ | 94/112 [03:25<00:48,  2.72s/it]Running Inference:  85%|████████▍ | 95/112 [03:29<00:55,  3.26s/it]Running Inference:  86%|████████▌ | 96/112 [03:30<00:41,  2.56s/it]Running Inference:  87%|████████▋ | 97/112 [03:36<00:51,  3.42s/it]Running Inference:  88%|████████▊ | 98/112 [03:38<00:41,  2.94s/it]Running Inference:  88%|████████▊ | 99/112 [03:40<00:34,  2.67s/it]Running Inference:  89%|████████▉ | 100/112 [03:47<00:47,  3.96s/it]Running Inference:  90%|█████████ | 101/112 [03:48<00:35,  3.25s/it]Running Inference:  91%|█████████ | 102/112 [03:50<00:28,  2.82s/it]Running Inference:  92%|█████████▏| 103/112 [03:51<00:21,  2.41s/it]Running Inference:  93%|█████████▎| 104/112 [03:52<00:15,  1.99s/it]Running Inference:  94%|█████████▍| 105/112 [03:53<00:11,  1.69s/it]Running Inference:  95%|█████████▍| 106/112 [03:57<00:13,  2.30s/it]Running Inference:  96%|█████████▌| 107/112 [04:00<00:12,  2.48s/it]Running Inference:  96%|█████████▋| 108/112 [04:08<00:16,  4.05s/it]Running Inference:  97%|█████████▋| 109/112 [04:10<00:10,  3.55s/it]Running Inference:  98%|█████████▊| 110/112 [04:11<00:05,  2.86s/it]Running Inference:  99%|█████████▉| 111/112 [04:14<00:02,  2.72s/it]Running Inference: 100%|██████████| 112/112 [04:18<00:00,  3.14s/it]Running Inference: 100%|██████████| 112/112 [04:18<00:00,  2.31s/it]
2025-12-13 19:10:56,814 - INFO - Inference completed.
2025-12-13 19:10:56,822 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-13 19:10:56,822 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:10:56,826 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-13 19:10:56,826 - INFO - Metrics:
30.23
2025-12-13 19:10:56,828 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_en, ratio=0.5) on GPU 1


========================================
LongBench Task: musique
========================================
----------------------------------------
Task: musique | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:11:03,216 - INFO - Set deterministic seeds to 42
2025-12-13 19:11:03,216 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:11:03,217 - INFO - Starting evaluation run...
2025-12-13 19:11:03,217 - INFO - Output directory set to: longbenchresult
2025-12-13 19:11:03,217 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 19:11:03,217 - INFO - KV Press 'tova' setup.
2025-12-13 19:11:03,217 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:11:03,217 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.14it/s]
Device set to use cuda:0
2025-12-13 19:11:18,383 - INFO - Model pipeline loaded.
2025-12-13 19:11:18,383 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 879.71 examples/s]Generating test split: 200 examples [00:00, 876.59 examples/s]
2025-12-13 19:11:33,965 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:11:33,965 - INFO - Dataset processed with 200 entries.
2025-12-13 19:11:34,007 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:24,  3.44s/it]Running Inference:   1%|          | 2/200 [00:06<10:39,  3.23s/it]Running Inference:   2%|▏         | 3/200 [00:08<09:16,  2.82s/it]Running Inference:   2%|▏         | 4/200 [00:13<11:17,  3.46s/it]Running Inference:   2%|▎         | 5/200 [00:17<11:50,  3.65s/it]Running Inference:   3%|▎         | 6/200 [00:21<12:34,  3.89s/it]Running Inference:   4%|▎         | 7/200 [00:26<13:06,  4.07s/it]Running Inference:   4%|▍         | 8/200 [00:28<11:08,  3.48s/it]Running Inference:   4%|▍         | 9/200 [00:30<09:46,  3.07s/it]Running Inference:   5%|▌         | 10/200 [00:32<08:58,  2.83s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:34<08:05,  2.57s/it]Running Inference:   6%|▌         | 12/200 [00:39<09:44,  3.11s/it]Running Inference:   6%|▋         | 13/200 [00:41<09:16,  2.98s/it]Running Inference:   7%|▋         | 14/200 [00:44<09:03,  2.92s/it]Running Inference:   8%|▊         | 15/200 [00:48<10:25,  3.38s/it]Running Inference:   8%|▊         | 16/200 [00:53<11:10,  3.65s/it]Running Inference:   8%|▊         | 17/200 [00:57<11:15,  3.69s/it]Running Inference:   9%|▉         | 18/200 [01:01<11:41,  3.85s/it]Running Inference:  10%|▉         | 19/200 [01:05<12:04,  4.00s/it]Running Inference:  10%|█         | 20/200 [01:07<10:21,  3.46s/it]Running Inference:  10%|█         | 21/200 [01:10<09:16,  3.11s/it]Running Inference:  11%|█         | 22/200 [01:14<10:11,  3.43s/it]Running Inference:  12%|█▏        | 23/200 [01:16<09:19,  3.16s/it]Running Inference:  12%|█▏        | 24/200 [01:18<07:51,  2.68s/it]Running Inference:  12%|█▎        | 25/200 [01:20<07:29,  2.57s/it]Running Inference:  13%|█▎        | 26/200 [01:23<07:25,  2.56s/it]Running Inference:  14%|█▎        | 27/200 [01:25<07:04,  2.45s/it]Running Inference:  14%|█▍        | 28/200 [01:29<08:41,  3.03s/it]Running Inference:  14%|█▍        | 29/200 [01:32<08:37,  3.03s/it]Running Inference:  15%|█▌        | 30/200 [01:35<07:52,  2.78s/it]Running Inference:  16%|█▌        | 31/200 [01:37<07:25,  2.64s/it]Running Inference:  16%|█▌        | 32/200 [01:41<08:21,  2.99s/it]Running Inference:  16%|█▋        | 33/200 [01:45<09:25,  3.39s/it]Running Inference:  17%|█▋        | 34/200 [01:47<08:35,  3.11s/it]Running Inference:  18%|█▊        | 35/200 [01:50<08:03,  2.93s/it]Running Inference:  18%|█▊        | 36/200 [01:52<07:36,  2.78s/it]Running Inference:  18%|█▊        | 37/200 [01:55<07:49,  2.88s/it]Running Inference:  19%|█▉        | 38/200 [02:00<08:47,  3.26s/it]Running Inference:  20%|█▉        | 39/200 [02:04<09:38,  3.59s/it]Running Inference:  20%|██        | 40/200 [02:08<10:11,  3.82s/it]Running Inference:  20%|██        | 41/200 [02:11<08:48,  3.32s/it]Running Inference:  21%|██        | 42/200 [02:13<08:18,  3.15s/it]Running Inference:  22%|██▏       | 43/200 [02:16<07:40,  2.93s/it]Running Inference:  22%|██▏       | 44/200 [02:20<08:44,  3.36s/it]Running Inference:  22%|██▎       | 45/200 [02:24<09:20,  3.61s/it]Running Inference:  23%|██▎       | 46/200 [02:29<09:49,  3.83s/it]Running Inference:  24%|██▎       | 47/200 [02:31<08:43,  3.42s/it]Running Inference:  24%|██▍       | 48/200 [02:35<09:19,  3.68s/it]Running Inference:  24%|██▍       | 49/200 [02:40<09:46,  3.88s/it]Running Inference:  25%|██▌       | 50/200 [02:43<09:30,  3.81s/it]Running Inference:  26%|██▌       | 51/200 [02:47<09:25,  3.79s/it]Running Inference:  26%|██▌       | 52/200 [02:51<09:48,  3.98s/it]Running Inference:  26%|██▋       | 53/200 [02:54<08:28,  3.46s/it]Running Inference:  27%|██▋       | 54/200 [02:56<07:31,  3.09s/it]Running Inference:  28%|██▊       | 55/200 [02:59<07:08,  2.95s/it]Running Inference:  28%|██▊       | 56/200 [03:01<06:45,  2.81s/it]Running Inference:  28%|██▊       | 57/200 [03:04<06:28,  2.72s/it]Running Inference:  29%|██▉       | 58/200 [03:05<05:48,  2.46s/it]Running Inference:  30%|██▉       | 59/200 [03:08<05:37,  2.39s/it]Running Inference:  30%|███       | 60/200 [03:12<06:56,  2.98s/it]Running Inference:  30%|███       | 61/200 [03:16<07:55,  3.42s/it]Running Inference:  31%|███       | 62/200 [03:19<07:00,  3.05s/it]Running Inference:  32%|███▏      | 63/200 [03:23<07:51,  3.44s/it]Running Inference:  32%|███▏      | 64/200 [03:27<08:27,  3.73s/it]Running Inference:  32%|███▎      | 65/200 [03:31<08:31,  3.79s/it]Running Inference:  33%|███▎      | 66/200 [03:36<08:52,  3.97s/it]Running Inference:  34%|███▎      | 67/200 [03:38<07:42,  3.48s/it]Running Inference:  34%|███▍      | 68/200 [03:42<08:11,  3.73s/it]Running Inference:  34%|███▍      | 69/200 [03:45<07:10,  3.29s/it]Running Inference:  35%|███▌      | 70/200 [03:49<07:47,  3.60s/it]Running Inference:  36%|███▌      | 71/200 [03:53<08:14,  3.84s/it]Running Inference:  36%|███▌      | 72/200 [03:56<07:18,  3.43s/it]Running Inference:  36%|███▋      | 73/200 [04:00<07:37,  3.60s/it]Running Inference:  37%|███▋      | 74/200 [04:04<08:03,  3.84s/it]Running Inference:  38%|███▊      | 75/200 [04:09<08:24,  4.03s/it]Running Inference:  38%|███▊      | 76/200 [04:13<08:15,  3.99s/it]Running Inference:  38%|███▊      | 77/200 [04:14<06:45,  3.30s/it]Running Inference:  39%|███▉      | 78/200 [04:17<06:04,  2.99s/it]Running Inference:  40%|███▉      | 79/200 [04:19<05:51,  2.90s/it]Running Inference:  40%|████      | 80/200 [04:21<05:23,  2.70s/it]Running Inference:  40%|████      | 81/200 [04:24<05:13,  2.63s/it]Running Inference:  41%|████      | 82/200 [04:28<06:15,  3.18s/it]Running Inference:  42%|████▏     | 83/200 [04:31<05:58,  3.07s/it]Running Inference:  42%|████▏     | 84/200 [04:35<06:22,  3.29s/it]Running Inference:  42%|████▎     | 85/200 [04:38<06:12,  3.24s/it]Running Inference:  43%|████▎     | 86/200 [04:40<05:33,  2.93s/it]Running Inference:  44%|████▎     | 87/200 [04:43<05:08,  2.73s/it]Running Inference:  44%|████▍     | 88/200 [04:46<05:15,  2.82s/it]Running Inference:  44%|████▍     | 89/200 [04:48<04:53,  2.65s/it]Running Inference:  45%|████▌     | 90/200 [04:52<05:48,  3.16s/it]Running Inference:  46%|████▌     | 91/200 [04:56<06:03,  3.34s/it]Running Inference:  46%|████▌     | 92/200 [05:00<06:11,  3.44s/it]Running Inference:  46%|████▋     | 93/200 [05:02<05:27,  3.06s/it]Running Inference:  47%|████▋     | 94/200 [05:06<06:03,  3.43s/it]Running Inference:  48%|████▊     | 95/200 [05:09<05:44,  3.28s/it]Running Inference:  48%|████▊     | 96/200 [05:13<05:47,  3.34s/it]Running Inference:  48%|████▊     | 97/200 [05:15<05:15,  3.07s/it]Running Inference:  49%|████▉     | 98/200 [05:19<05:51,  3.44s/it]Running Inference:  50%|████▉     | 99/200 [05:21<05:05,  3.02s/it]Running Inference:  50%|█████     | 100/200 [05:25<05:34,  3.34s/it]Running Inference:  50%|█████     | 101/200 [05:28<04:58,  3.01s/it]Running Inference:  51%|█████     | 102/200 [05:30<04:33,  2.79s/it]Running Inference:  52%|█████▏    | 103/200 [05:34<05:13,  3.23s/it]Running Inference:  52%|█████▏    | 104/200 [05:39<05:46,  3.61s/it]Running Inference:  52%|█████▎    | 105/200 [05:43<06:06,  3.86s/it]Running Inference:  53%|█████▎    | 106/200 [05:47<06:15,  4.00s/it]Running Inference:  54%|█████▎    | 107/200 [05:52<06:16,  4.05s/it]Running Inference:  54%|█████▍    | 108/200 [05:54<05:22,  3.51s/it]Running Inference:  55%|█████▍    | 109/200 [05:56<04:49,  3.19s/it]Running Inference:  55%|█████▌    | 110/200 [06:00<04:46,  3.19s/it]Running Inference:  56%|█████▌    | 111/200 [06:04<05:07,  3.45s/it]Running Inference:  56%|█████▌    | 112/200 [06:06<04:27,  3.04s/it]Running Inference:  56%|█████▋    | 113/200 [06:10<04:58,  3.43s/it]Running Inference:  57%|█████▋    | 114/200 [06:13<04:34,  3.20s/it]Running Inference:  57%|█████▊    | 115/200 [06:15<04:12,  2.98s/it]Running Inference:  58%|█████▊    | 116/200 [06:17<03:51,  2.75s/it]Running Inference:  58%|█████▊    | 117/200 [06:22<04:27,  3.22s/it]Running Inference:  59%|█████▉    | 118/200 [06:24<03:58,  2.90s/it]Running Inference:  60%|█████▉    | 119/200 [06:26<03:36,  2.67s/it]Running Inference:  60%|██████    | 120/200 [06:28<03:24,  2.56s/it]Running Inference:  60%|██████    | 121/200 [06:31<03:16,  2.48s/it]Running Inference:  61%|██████    | 122/200 [06:33<03:08,  2.41s/it]Running Inference:  62%|██████▏   | 123/200 [06:36<03:27,  2.70s/it]Running Inference:  62%|██████▏   | 124/200 [06:39<03:36,  2.85s/it]Running Inference:  62%|██████▎   | 125/200 [06:43<03:51,  3.08s/it]Running Inference:  63%|██████▎   | 126/200 [06:47<04:16,  3.47s/it]Running Inference:  64%|██████▎   | 127/200 [06:50<03:52,  3.18s/it]Running Inference:  64%|██████▍   | 128/200 [06:54<04:13,  3.52s/it]Running Inference:  64%|██████▍   | 129/200 [06:57<03:51,  3.25s/it]Running Inference:  65%|██████▌   | 130/200 [07:01<04:10,  3.58s/it]Running Inference:  66%|██████▌   | 131/200 [07:03<03:40,  3.19s/it]Running Inference:  66%|██████▌   | 132/200 [07:06<03:21,  2.96s/it]Running Inference:  66%|██████▋   | 133/200 [07:08<03:02,  2.72s/it]Running Inference:  67%|██████▋   | 134/200 [07:09<02:30,  2.28s/it]Running Inference:  68%|██████▊   | 135/200 [07:12<02:41,  2.49s/it]Running Inference:  68%|██████▊   | 136/200 [07:15<02:35,  2.44s/it]Running Inference:  68%|██████▊   | 137/200 [07:19<03:10,  3.02s/it]Running Inference:  69%|██████▉   | 138/200 [07:23<03:30,  3.39s/it]Running Inference:  70%|██████▉   | 139/200 [07:26<03:15,  3.21s/it]Running Inference:  70%|███████   | 140/200 [07:29<03:03,  3.07s/it]Running Inference:  70%|███████   | 141/200 [07:31<02:47,  2.83s/it]Running Inference:  71%|███████   | 142/200 [07:34<02:38,  2.74s/it]Running Inference:  72%|███████▏  | 143/200 [07:36<02:35,  2.73s/it]Running Inference:  72%|███████▏  | 144/200 [07:38<02:14,  2.40s/it]Running Inference:  72%|███████▎  | 145/200 [07:42<02:42,  2.95s/it]Running Inference:  73%|███████▎  | 146/200 [07:44<02:28,  2.76s/it]Running Inference:  74%|███████▎  | 147/200 [07:49<02:52,  3.25s/it]Running Inference:  74%|███████▍  | 148/200 [07:53<03:06,  3.59s/it]Running Inference:  74%|███████▍  | 149/200 [07:57<03:03,  3.59s/it]Running Inference:  75%|███████▌  | 150/200 [07:59<02:41,  3.24s/it]Running Inference:  76%|███████▌  | 151/200 [08:01<02:23,  2.93s/it]Running Inference:  76%|███████▌  | 152/200 [08:05<02:23,  2.99s/it]Running Inference:  76%|███████▋  | 153/200 [08:09<02:40,  3.41s/it]Running Inference:  77%|███████▋  | 154/200 [08:11<02:21,  3.08s/it]Running Inference:  78%|███████▊  | 155/200 [08:14<02:18,  3.07s/it]Running Inference:  78%|███████▊  | 156/200 [08:18<02:24,  3.29s/it]Running Inference:  78%|███████▊  | 157/200 [08:21<02:11,  3.06s/it]Running Inference:  79%|███████▉  | 158/200 [08:23<01:58,  2.83s/it]Running Inference:  80%|███████▉  | 159/200 [08:27<02:14,  3.28s/it]Running Inference:  80%|████████  | 160/200 [08:31<02:13,  3.33s/it]Running Inference:  80%|████████  | 161/200 [08:33<01:56,  2.99s/it]Running Inference:  81%|████████  | 162/200 [08:37<02:09,  3.41s/it]Running Inference:  82%|████████▏ | 163/200 [08:42<02:16,  3.68s/it]Running Inference:  82%|████████▏ | 164/200 [08:44<01:59,  3.31s/it]Running Inference:  82%|████████▎ | 165/200 [08:48<02:06,  3.61s/it]Running Inference:  83%|████████▎ | 166/200 [08:51<01:50,  3.26s/it]Running Inference:  84%|████████▎ | 167/200 [08:53<01:39,  3.03s/it]Running Inference:  84%|████████▍ | 168/200 [08:58<01:49,  3.43s/it]Running Inference:  84%|████████▍ | 169/200 [09:02<01:55,  3.71s/it]Running Inference:  85%|████████▌ | 170/200 [09:04<01:38,  3.29s/it]Running Inference:  86%|████████▌ | 171/200 [09:07<01:32,  3.19s/it]Running Inference:  86%|████████▌ | 172/200 [09:12<01:38,  3.53s/it]Running Inference:  86%|████████▋ | 173/200 [09:15<01:35,  3.55s/it]Running Inference:  87%|████████▋ | 174/200 [09:17<01:21,  3.13s/it]Running Inference:  88%|████████▊ | 175/200 [09:22<01:27,  3.50s/it]Running Inference:  88%|████████▊ | 176/200 [09:26<01:30,  3.78s/it]Running Inference:  88%|████████▊ | 177/200 [09:30<01:30,  3.93s/it]Running Inference:  89%|████████▉ | 178/200 [09:35<01:29,  4.06s/it]Running Inference:  90%|████████▉ | 179/200 [09:37<01:13,  3.51s/it]Running Inference:  90%|█████████ | 180/200 [09:40<01:08,  3.41s/it]Running Inference:  90%|█████████ | 181/200 [09:42<00:58,  3.07s/it]Running Inference:  91%|█████████ | 182/200 [09:45<00:51,  2.87s/it]Running Inference:  92%|█████████▏| 183/200 [09:47<00:45,  2.69s/it]Running Inference:  92%|█████████▏| 184/200 [09:49<00:41,  2.57s/it]Running Inference:  92%|█████████▎| 185/200 [09:52<00:37,  2.47s/it]Running Inference:  93%|█████████▎| 186/200 [09:56<00:42,  3.03s/it]Running Inference:  94%|█████████▎| 187/200 [09:58<00:36,  2.81s/it]Running Inference:  94%|█████████▍| 188/200 [10:02<00:37,  3.13s/it]Running Inference:  94%|█████████▍| 189/200 [10:07<00:38,  3.52s/it]Running Inference:  95%|█████████▌| 190/200 [10:09<00:30,  3.06s/it]Running Inference:  96%|█████████▌| 191/200 [10:11<00:25,  2.80s/it]Running Inference:  96%|█████████▌| 192/200 [10:15<00:26,  3.27s/it]Running Inference:  96%|█████████▋| 193/200 [10:19<00:25,  3.59s/it]Running Inference:  97%|█████████▋| 194/200 [10:24<00:22,  3.81s/it]Running Inference:  98%|█████████▊| 195/200 [10:26<00:17,  3.40s/it]Running Inference:  98%|█████████▊| 196/200 [10:28<00:11,  2.96s/it]Running Inference:  98%|█████████▊| 197/200 [10:31<00:09,  3.01s/it]Running Inference:  99%|█████████▉| 198/200 [10:35<00:06,  3.12s/it]Running Inference: 100%|█████████▉| 199/200 [10:38<00:03,  3.24s/it]Running Inference: 100%|██████████| 200/200 [10:42<00:00,  3.27s/it]Running Inference: 100%|██████████| 200/200 [10:42<00:00,  3.21s/it]
2025-12-13 19:22:16,067 - INFO - Inference completed.
2025-12-13 19:22:16,076 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 19:22:16,076 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:22:16,083 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 19:22:16,083 - INFO - Metrics:
17.9
2025-12-13 19:22:16,084 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=musique, ratio=0.1) on GPU 1

----------------------------------------
Task: musique | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:22:22,602 - INFO - Set deterministic seeds to 42
2025-12-13 19:22:22,602 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:22:22,602 - INFO - Starting evaluation run...
2025-12-13 19:22:22,602 - INFO - Output directory set to: longbenchresult
2025-12-13 19:22:22,602 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 19:22:22,602 - INFO - KV Press 'tova' setup.
2025-12-13 19:22:22,602 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:22:22,602 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.08it/s]
Device set to use cuda:0
2025-12-13 19:22:35,220 - INFO - Model pipeline loaded.
2025-12-13 19:22:35,220 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:22:41,319 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:22:41,319 - INFO - Dataset processed with 200 entries.
2025-12-13 19:22:41,363 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:16,  3.40s/it]Running Inference:   1%|          | 2/200 [00:07<13:13,  4.01s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:39,  3.25s/it]Running Inference:   2%|▏         | 4/200 [00:14<12:08,  3.71s/it]Running Inference:   2%|▎         | 5/200 [00:18<12:46,  3.93s/it]Running Inference:   3%|▎         | 6/200 [00:23<13:05,  4.05s/it]Running Inference:   4%|▎         | 7/200 [00:27<13:23,  4.16s/it]Running Inference:   4%|▍         | 8/200 [00:29<11:19,  3.54s/it]Running Inference:   4%|▍         | 9/200 [00:31<09:54,  3.11s/it]Running Inference:   5%|▌         | 10/200 [00:34<09:03,  2.86s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:36<08:08,  2.58s/it]Running Inference:   6%|▌         | 12/200 [00:40<09:45,  3.11s/it]Running Inference:   6%|▋         | 13/200 [00:43<09:17,  2.98s/it]Running Inference:   7%|▋         | 14/200 [00:46<09:03,  2.92s/it]Running Inference:   8%|▊         | 15/200 [00:49<09:55,  3.22s/it]Running Inference:   8%|▊         | 16/200 [00:54<10:49,  3.53s/it]Running Inference:   8%|▊         | 17/200 [00:57<11:00,  3.61s/it]Running Inference:   9%|▉         | 18/200 [01:02<11:29,  3.79s/it]Running Inference:  10%|▉         | 19/200 [01:06<11:54,  3.95s/it]Running Inference:  10%|█         | 20/200 [01:08<10:15,  3.42s/it]Running Inference:  10%|█         | 21/200 [01:10<09:11,  3.08s/it]Running Inference:  11%|█         | 22/200 [01:15<10:08,  3.42s/it]Running Inference:  12%|█▏        | 23/200 [01:17<09:14,  3.13s/it]Running Inference:  12%|█▏        | 24/200 [01:19<07:48,  2.66s/it]Running Inference:  12%|█▎        | 25/200 [01:21<07:35,  2.60s/it]Running Inference:  13%|█▎        | 26/200 [01:24<07:29,  2.58s/it]Running Inference:  14%|█▎        | 27/200 [01:26<07:06,  2.47s/it]Running Inference:  14%|█▍        | 28/200 [01:29<07:39,  2.67s/it]Running Inference:  14%|█▍        | 29/200 [01:32<07:54,  2.78s/it]Running Inference:  15%|█▌        | 30/200 [01:34<07:22,  2.60s/it]Running Inference:  16%|█▌        | 31/200 [01:37<07:04,  2.51s/it]Running Inference:  16%|█▌        | 32/200 [01:40<08:05,  2.89s/it]Running Inference:  16%|█▋        | 33/200 [01:45<09:17,  3.34s/it]Running Inference:  17%|█▋        | 34/200 [01:49<09:56,  3.60s/it]Running Inference:  18%|█▊        | 35/200 [01:51<08:59,  3.27s/it]Running Inference:  18%|█▊        | 36/200 [01:54<08:15,  3.02s/it]Running Inference:  18%|█▊        | 37/200 [01:57<08:17,  3.05s/it]Running Inference:  19%|█▉        | 38/200 [02:01<09:09,  3.39s/it]Running Inference:  20%|█▉        | 39/200 [02:06<09:53,  3.69s/it]Running Inference:  20%|██        | 40/200 [02:10<10:21,  3.88s/it]Running Inference:  20%|██        | 41/200 [02:14<10:24,  3.93s/it]Running Inference:  21%|██        | 42/200 [02:16<09:10,  3.48s/it]Running Inference:  22%|██▏       | 43/200 [02:19<08:16,  3.16s/it]Running Inference:  22%|██▏       | 44/200 [02:23<09:08,  3.52s/it]Running Inference:  22%|██▎       | 45/200 [02:27<09:35,  3.71s/it]Running Inference:  23%|██▎       | 46/200 [02:30<08:24,  3.28s/it]Running Inference:  24%|██▎       | 47/200 [02:32<07:44,  3.04s/it]Running Inference:  24%|██▍       | 48/200 [02:36<08:37,  3.41s/it]Running Inference:  24%|██▍       | 49/200 [02:41<09:16,  3.69s/it]Running Inference:  25%|██▌       | 50/200 [02:44<09:09,  3.66s/it]Running Inference:  26%|██▌       | 51/200 [02:48<09:12,  3.71s/it]Running Inference:  26%|██▌       | 52/200 [02:52<09:38,  3.91s/it]Running Inference:  26%|██▋       | 53/200 [02:55<08:20,  3.40s/it]Running Inference:  27%|██▋       | 54/200 [02:57<07:26,  3.06s/it]Running Inference:  28%|██▊       | 55/200 [03:00<07:03,  2.92s/it]Running Inference:  28%|██▊       | 56/200 [03:02<06:41,  2.79s/it]Running Inference:  28%|██▊       | 57/200 [03:05<06:33,  2.75s/it]Running Inference:  29%|██▉       | 58/200 [03:07<05:52,  2.48s/it]Running Inference:  30%|██▉       | 59/200 [03:09<05:39,  2.41s/it]Running Inference:  30%|███       | 60/200 [03:13<06:57,  2.98s/it]Running Inference:  30%|███       | 61/200 [03:18<07:55,  3.42s/it]Running Inference:  31%|███       | 62/200 [03:20<07:00,  3.04s/it]Running Inference:  32%|███▏      | 63/200 [03:24<07:50,  3.43s/it]Running Inference:  32%|███▏      | 64/200 [03:28<08:25,  3.72s/it]Running Inference:  32%|███▎      | 65/200 [03:32<08:29,  3.78s/it]Running Inference:  33%|███▎      | 66/200 [03:37<08:50,  3.96s/it]Running Inference:  34%|███▎      | 67/200 [03:39<07:40,  3.46s/it]Running Inference:  34%|███▍      | 68/200 [03:43<08:09,  3.70s/it]Running Inference:  34%|███▍      | 69/200 [03:46<07:08,  3.27s/it]Running Inference:  35%|███▌      | 70/200 [03:50<07:45,  3.58s/it]Running Inference:  36%|███▌      | 71/200 [03:54<08:11,  3.81s/it]Running Inference:  36%|███▌      | 72/200 [03:57<07:16,  3.41s/it]Running Inference:  36%|███▋      | 73/200 [04:01<07:35,  3.58s/it]Running Inference:  37%|███▋      | 74/200 [04:03<06:47,  3.24s/it]Running Inference:  38%|███▊      | 75/200 [04:08<07:30,  3.60s/it]Running Inference:  38%|███▊      | 76/200 [04:11<07:37,  3.69s/it]Running Inference:  38%|███▊      | 77/200 [04:13<06:19,  3.08s/it]Running Inference:  39%|███▉      | 78/200 [04:15<05:46,  2.84s/it]Running Inference:  40%|███▉      | 79/200 [04:18<05:38,  2.80s/it]Running Inference:  40%|████      | 80/200 [04:20<05:14,  2.62s/it]Running Inference:  40%|████      | 81/200 [04:23<05:06,  2.58s/it]Running Inference:  41%|████      | 82/200 [04:27<06:09,  3.13s/it]Running Inference:  42%|████▏     | 83/200 [04:30<05:54,  3.03s/it]Running Inference:  42%|████▏     | 84/200 [04:34<06:23,  3.30s/it]Running Inference:  42%|████▎     | 85/200 [04:37<06:12,  3.24s/it]Running Inference:  43%|████▎     | 86/200 [04:39<05:33,  2.93s/it]Running Inference:  44%|████▎     | 87/200 [04:41<05:08,  2.73s/it]Running Inference:  44%|████▍     | 88/200 [04:44<05:13,  2.80s/it]Running Inference:  44%|████▍     | 89/200 [04:47<04:52,  2.63s/it]Running Inference:  45%|████▌     | 90/200 [04:51<05:46,  3.15s/it]Running Inference:  46%|████▌     | 91/200 [04:55<06:00,  3.31s/it]Running Inference:  46%|████▌     | 92/200 [04:58<06:06,  3.40s/it]Running Inference:  46%|████▋     | 93/200 [05:00<05:23,  3.03s/it]Running Inference:  47%|████▋     | 94/200 [05:05<06:00,  3.40s/it]Running Inference:  48%|████▊     | 95/200 [05:08<05:41,  3.25s/it]Running Inference:  48%|████▊     | 96/200 [05:11<05:45,  3.32s/it]Running Inference:  48%|████▊     | 97/200 [05:14<05:13,  3.05s/it]Running Inference:  49%|████▉     | 98/200 [05:18<05:49,  3.42s/it]Running Inference:  50%|████▉     | 99/200 [05:20<05:03,  3.01s/it]Running Inference:  50%|█████     | 100/200 [05:24<05:33,  3.33s/it]Running Inference:  50%|█████     | 101/200 [05:26<04:57,  3.00s/it]Running Inference:  51%|█████     | 102/200 [05:28<04:32,  2.78s/it]Running Inference:  52%|█████▏    | 103/200 [05:31<04:33,  2.82s/it]Running Inference:  52%|█████▏    | 104/200 [05:36<05:18,  3.31s/it]Running Inference:  52%|█████▎    | 105/200 [05:40<05:45,  3.64s/it]Running Inference:  53%|█████▎    | 106/200 [05:45<06:01,  3.84s/it]Running Inference:  54%|█████▎    | 107/200 [05:49<06:06,  3.94s/it]Running Inference:  54%|█████▍    | 108/200 [05:51<05:14,  3.42s/it]Running Inference:  55%|█████▍    | 109/200 [05:53<04:44,  3.13s/it]Running Inference:  55%|█████▌    | 110/200 [05:57<04:42,  3.14s/it]Running Inference:  56%|█████▌    | 111/200 [06:01<05:03,  3.42s/it]Running Inference:  56%|█████▌    | 112/200 [06:03<04:25,  3.02s/it]Running Inference:  56%|█████▋    | 113/200 [06:06<04:17,  2.96s/it]Running Inference:  57%|█████▋    | 114/200 [06:08<04:06,  2.87s/it]Running Inference:  57%|█████▊    | 115/200 [06:10<03:48,  2.69s/it]Running Inference:  58%|█████▊    | 116/200 [06:13<03:34,  2.55s/it]Running Inference:  58%|█████▊    | 117/200 [06:17<04:14,  3.07s/it]Running Inference:  59%|█████▉    | 118/200 [06:19<03:48,  2.79s/it]Running Inference:  60%|█████▉    | 119/200 [06:21<03:29,  2.59s/it]Running Inference:  60%|██████    | 120/200 [06:23<03:19,  2.50s/it]Running Inference:  60%|██████    | 121/200 [06:26<03:12,  2.44s/it]Running Inference:  61%|██████    | 122/200 [06:28<03:04,  2.37s/it]Running Inference:  62%|██████▏   | 123/200 [06:31<03:24,  2.65s/it]Running Inference:  62%|██████▏   | 124/200 [06:35<03:33,  2.81s/it]Running Inference:  62%|██████▎   | 125/200 [06:38<03:48,  3.05s/it]Running Inference:  63%|██████▎   | 126/200 [06:42<04:14,  3.44s/it]Running Inference:  64%|██████▎   | 127/200 [06:45<03:47,  3.12s/it]Running Inference:  64%|██████▍   | 128/200 [06:49<04:09,  3.46s/it]Running Inference:  64%|██████▍   | 129/200 [06:52<03:48,  3.21s/it]Running Inference:  65%|██████▌   | 130/200 [06:56<04:07,  3.54s/it]Running Inference:  66%|██████▌   | 131/200 [06:58<03:38,  3.16s/it]Running Inference:  66%|██████▌   | 132/200 [07:01<03:19,  2.93s/it]Running Inference:  66%|██████▋   | 133/200 [07:03<03:01,  2.71s/it]Running Inference:  67%|██████▋   | 134/200 [07:04<02:29,  2.26s/it]Running Inference:  68%|██████▊   | 135/200 [07:06<02:28,  2.29s/it]Running Inference:  68%|██████▊   | 136/200 [07:09<02:26,  2.30s/it]Running Inference:  68%|██████▊   | 137/200 [07:13<03:03,  2.91s/it]Running Inference:  69%|██████▉   | 138/200 [07:17<03:25,  3.31s/it]Running Inference:  70%|██████▉   | 139/200 [07:20<03:12,  3.15s/it]Running Inference:  70%|███████   | 140/200 [07:23<03:01,  3.03s/it]Running Inference:  70%|███████   | 141/200 [07:25<02:45,  2.80s/it]Running Inference:  71%|███████   | 142/200 [07:28<02:40,  2.77s/it]Running Inference:  72%|███████▏  | 143/200 [07:31<02:36,  2.74s/it]Running Inference:  72%|███████▏  | 144/200 [07:32<02:14,  2.40s/it]Running Inference:  72%|███████▎  | 145/200 [07:36<02:42,  2.95s/it]Running Inference:  73%|███████▎  | 146/200 [07:39<02:28,  2.76s/it]Running Inference:  74%|███████▎  | 147/200 [07:43<02:51,  3.24s/it]Running Inference:  74%|███████▍  | 148/200 [07:47<03:06,  3.58s/it]Running Inference:  74%|███████▍  | 149/200 [07:51<03:02,  3.58s/it]Running Inference:  75%|███████▌  | 150/200 [07:53<02:41,  3.23s/it]Running Inference:  76%|███████▌  | 151/200 [07:56<02:23,  2.92s/it]Running Inference:  76%|███████▌  | 152/200 [07:59<02:23,  2.98s/it]Running Inference:  76%|███████▋  | 153/200 [08:03<02:39,  3.40s/it]Running Inference:  77%|███████▋  | 154/200 [08:05<02:21,  3.07s/it]Running Inference:  78%|███████▊  | 155/200 [08:08<02:17,  3.06s/it]Running Inference:  78%|███████▊  | 156/200 [08:11<02:08,  2.91s/it]Running Inference:  78%|███████▊  | 157/200 [08:13<01:57,  2.73s/it]Running Inference:  79%|███████▉  | 158/200 [08:16<01:49,  2.60s/it]Running Inference:  80%|███████▉  | 159/200 [08:20<02:07,  3.12s/it]Running Inference:  80%|████████  | 160/200 [08:23<02:08,  3.21s/it]Running Inference:  80%|████████  | 161/200 [08:26<01:53,  2.91s/it]Running Inference:  81%|████████  | 162/200 [08:30<02:06,  3.34s/it]Running Inference:  82%|████████▏ | 163/200 [08:34<02:14,  3.62s/it]Running Inference:  82%|████████▏ | 164/200 [08:37<01:57,  3.26s/it]Running Inference:  82%|████████▎ | 165/200 [08:41<02:04,  3.56s/it]Running Inference:  83%|████████▎ | 166/200 [08:43<01:49,  3.23s/it]Running Inference:  84%|████████▎ | 167/200 [08:46<01:39,  3.00s/it]Running Inference:  84%|████████▍ | 168/200 [08:50<01:49,  3.41s/it]Running Inference:  84%|████████▍ | 169/200 [08:55<01:54,  3.69s/it]Running Inference:  85%|████████▌ | 170/200 [08:57<01:38,  3.28s/it]Running Inference:  86%|████████▌ | 171/200 [09:01<01:43,  3.58s/it]Running Inference:  86%|████████▌ | 172/200 [09:05<01:46,  3.80s/it]Running Inference:  86%|████████▋ | 173/200 [09:09<01:40,  3.74s/it]Running Inference:  87%|████████▋ | 174/200 [09:11<01:24,  3.26s/it]Running Inference:  88%|████████▊ | 175/200 [09:15<01:28,  3.54s/it]Running Inference:  88%|████████▊ | 176/200 [09:20<01:31,  3.80s/it]Running Inference:  88%|████████▊ | 177/200 [09:24<01:30,  3.95s/it]Running Inference:  89%|████████▉ | 178/200 [09:28<01:29,  4.07s/it]Running Inference:  90%|████████▉ | 179/200 [09:31<01:13,  3.51s/it]Running Inference:  90%|█████████ | 180/200 [09:33<01:02,  3.15s/it]Running Inference:  90%|█████████ | 181/200 [09:35<00:54,  2.89s/it]Running Inference:  91%|█████████ | 182/200 [09:38<00:49,  2.74s/it]Running Inference:  92%|█████████▏| 183/200 [09:40<00:43,  2.59s/it]Running Inference:  92%|█████████▏| 184/200 [09:42<00:40,  2.50s/it]Running Inference:  92%|█████████▎| 185/200 [09:44<00:36,  2.42s/it]Running Inference:  93%|█████████▎| 186/200 [09:49<00:41,  2.99s/it]Running Inference:  94%|█████████▎| 187/200 [09:51<00:36,  2.78s/it]Running Inference:  94%|█████████▍| 188/200 [09:55<00:37,  3.09s/it]Running Inference:  94%|█████████▍| 189/200 [09:59<00:38,  3.48s/it]Running Inference:  95%|█████████▌| 190/200 [10:01<00:30,  3.02s/it]Running Inference:  96%|█████████▌| 191/200 [10:03<00:24,  2.77s/it]Running Inference:  96%|█████████▌| 192/200 [10:08<00:25,  3.24s/it]Running Inference:  96%|█████████▋| 193/200 [10:12<00:24,  3.57s/it]Running Inference:  97%|█████████▋| 194/200 [10:16<00:22,  3.79s/it]Running Inference:  98%|█████████▊| 195/200 [10:19<00:16,  3.39s/it]Running Inference:  98%|█████████▊| 196/200 [10:21<00:11,  2.95s/it]Running Inference:  98%|█████████▊| 197/200 [10:24<00:09,  3.00s/it]Running Inference:  99%|█████████▉| 198/200 [10:27<00:06,  3.06s/it]Running Inference: 100%|█████████▉| 199/200 [10:31<00:03,  3.20s/it]Running Inference: 100%|██████████| 200/200 [10:34<00:00,  3.24s/it]Running Inference: 100%|██████████| 200/200 [10:34<00:00,  3.17s/it]
2025-12-13 19:33:15,727 - INFO - Inference completed.
2025-12-13 19:33:15,736 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 19:33:15,736 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:33:15,743 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 19:33:15,743 - INFO - Metrics:
19.1
2025-12-13 19:33:15,744 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=musique, ratio=0.2) on GPU 1

----------------------------------------
Task: musique | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:33:22,194 - INFO - Set deterministic seeds to 42
2025-12-13 19:33:22,194 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:33:22,194 - INFO - Starting evaluation run...
2025-12-13 19:33:22,195 - INFO - Output directory set to: longbenchresult
2025-12-13 19:33:22,195 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 19:33:22,195 - INFO - KV Press 'tova' setup.
2025-12-13 19:33:22,195 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:33:22,195 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.16it/s]
Device set to use cuda:0
2025-12-13 19:33:36,177 - INFO - Model pipeline loaded.
2025-12-13 19:33:36,177 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:33:42,794 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:33:42,794 - INFO - Dataset processed with 200 entries.
2025-12-13 19:33:42,838 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:20,  3.42s/it]Running Inference:   1%|          | 2/200 [00:07<13:14,  4.01s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:40,  3.25s/it]Running Inference:   2%|▏         | 4/200 [00:14<12:07,  3.71s/it]Running Inference:   2%|▎         | 5/200 [00:18<12:42,  3.91s/it]Running Inference:   3%|▎         | 6/200 [00:23<13:01,  4.03s/it]Running Inference:   4%|▎         | 7/200 [00:27<13:18,  4.14s/it]Running Inference:   4%|▍         | 8/200 [00:29<11:16,  3.52s/it]Running Inference:   4%|▍         | 9/200 [00:31<09:51,  3.10s/it]Running Inference:   5%|▌         | 10/200 [00:36<10:59,  3.47s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:38<09:28,  3.01s/it]Running Inference:   6%|▌         | 12/200 [00:42<10:37,  3.39s/it]Running Inference:   6%|▋         | 13/200 [00:45<09:53,  3.17s/it]Running Inference:   7%|▋         | 14/200 [00:47<09:27,  3.05s/it]Running Inference:   8%|▊         | 15/200 [00:50<08:59,  2.92s/it]Running Inference:   8%|▊         | 16/200 [00:54<10:08,  3.31s/it]Running Inference:   8%|▊         | 17/200 [00:58<10:31,  3.45s/it]Running Inference:   9%|▉         | 18/200 [01:02<11:07,  3.67s/it]Running Inference:  10%|▉         | 19/200 [01:06<11:37,  3.85s/it]Running Inference:  10%|█         | 20/200 [01:09<10:02,  3.35s/it]Running Inference:  10%|█         | 21/200 [01:11<09:29,  3.18s/it]Running Inference:  11%|█         | 22/200 [01:15<10:17,  3.47s/it]Running Inference:  12%|█▏        | 23/200 [01:18<09:23,  3.18s/it]Running Inference:  12%|█▏        | 24/200 [01:20<07:53,  2.69s/it]Running Inference:  12%|█▎        | 25/200 [01:22<07:39,  2.62s/it]Running Inference:  13%|█▎        | 26/200 [01:25<07:31,  2.60s/it]Running Inference:  14%|█▎        | 27/200 [01:27<07:08,  2.48s/it]Running Inference:  14%|█▍        | 28/200 [01:30<08:05,  2.82s/it]Running Inference:  14%|█▍        | 29/200 [01:33<08:12,  2.88s/it]Running Inference:  15%|█▌        | 30/200 [01:36<07:34,  2.67s/it]Running Inference:  16%|█▌        | 31/200 [01:38<07:12,  2.56s/it]Running Inference:  16%|█▌        | 32/200 [01:42<08:09,  2.91s/it]Running Inference:  16%|█▋        | 33/200 [01:46<09:14,  3.32s/it]Running Inference:  17%|█▋        | 34/200 [01:50<09:52,  3.57s/it]Running Inference:  18%|█▊        | 35/200 [01:53<08:57,  3.26s/it]Running Inference:  18%|█▊        | 36/200 [01:55<08:13,  3.01s/it]Running Inference:  18%|█▊        | 37/200 [01:58<08:13,  3.03s/it]Running Inference:  19%|█▉        | 38/200 [02:02<09:01,  3.34s/it]Running Inference:  20%|█▉        | 39/200 [02:06<09:45,  3.63s/it]Running Inference:  20%|██        | 40/200 [02:11<10:12,  3.83s/it]Running Inference:  20%|██        | 41/200 [02:13<08:49,  3.33s/it]Running Inference:  21%|██        | 42/200 [02:15<08:03,  3.06s/it]Running Inference:  22%|██▏       | 43/200 [02:18<07:30,  2.87s/it]Running Inference:  22%|██▏       | 44/200 [02:22<08:33,  3.29s/it]Running Inference:  22%|██▎       | 45/200 [02:26<09:04,  3.51s/it]Running Inference:  23%|██▎       | 46/200 [02:30<09:35,  3.74s/it]Running Inference:  24%|██▎       | 47/200 [02:33<08:33,  3.35s/it]Running Inference:  24%|██▍       | 48/200 [02:37<09:07,  3.60s/it]Running Inference:  24%|██▍       | 49/200 [02:41<09:33,  3.80s/it]Running Inference:  25%|██▌       | 50/200 [02:45<09:18,  3.73s/it]Running Inference:  26%|██▌       | 51/200 [02:48<09:06,  3.66s/it]Running Inference:  26%|██▌       | 52/200 [02:53<09:30,  3.85s/it]Running Inference:  26%|██▋       | 53/200 [02:55<08:14,  3.36s/it]Running Inference:  27%|██▋       | 54/200 [02:57<07:21,  3.02s/it]Running Inference:  28%|██▊       | 55/200 [03:00<06:59,  2.89s/it]Running Inference:  28%|██▊       | 56/200 [03:02<06:37,  2.76s/it]Running Inference:  28%|██▊       | 57/200 [03:05<06:22,  2.68s/it]Running Inference:  29%|██▉       | 58/200 [03:06<05:44,  2.42s/it]Running Inference:  30%|██▉       | 59/200 [03:09<05:33,  2.37s/it]Running Inference:  30%|███       | 60/200 [03:13<06:49,  2.92s/it]Running Inference:  30%|███       | 61/200 [03:17<07:45,  3.35s/it]Running Inference:  31%|███       | 62/200 [03:19<06:53,  2.99s/it]Running Inference:  32%|███▏      | 63/200 [03:22<06:45,  2.96s/it]Running Inference:  32%|███▏      | 64/200 [03:27<07:36,  3.36s/it]Running Inference:  32%|███▎      | 65/200 [03:30<07:52,  3.50s/it]Running Inference:  33%|███▎      | 66/200 [03:35<08:20,  3.74s/it]Running Inference:  34%|███▎      | 67/200 [03:37<07:19,  3.31s/it]Running Inference:  34%|███▍      | 68/200 [03:41<07:51,  3.57s/it]Running Inference:  34%|███▍      | 69/200 [03:43<06:55,  3.17s/it]Running Inference:  35%|███▌      | 70/200 [03:48<07:33,  3.49s/it]Running Inference:  36%|███▌      | 71/200 [03:52<07:59,  3.72s/it]Running Inference:  36%|███▌      | 72/200 [03:54<07:07,  3.34s/it]Running Inference:  36%|███▋      | 73/200 [03:57<06:36,  3.12s/it]Running Inference:  37%|███▋      | 74/200 [04:01<07:17,  3.47s/it]Running Inference:  38%|███▊      | 75/200 [04:06<07:47,  3.74s/it]Running Inference:  38%|███▊      | 76/200 [04:09<07:22,  3.57s/it]Running Inference:  38%|███▊      | 77/200 [04:10<06:08,  3.00s/it]Running Inference:  39%|███▉      | 78/200 [04:13<05:38,  2.77s/it]Running Inference:  40%|███▉      | 79/200 [04:16<05:51,  2.91s/it]Running Inference:  40%|████      | 80/200 [04:18<05:23,  2.70s/it]Running Inference:  40%|████      | 81/200 [04:21<05:11,  2.62s/it]Running Inference:  41%|████      | 82/200 [04:24<05:31,  2.81s/it]Running Inference:  42%|████▏     | 83/200 [04:26<05:09,  2.65s/it]Running Inference:  42%|████▏     | 84/200 [04:30<05:45,  2.98s/it]Running Inference:  42%|████▎     | 85/200 [04:33<05:43,  2.99s/it]Running Inference:  43%|████▎     | 86/200 [04:35<05:13,  2.75s/it]Running Inference:  44%|████▎     | 87/200 [04:37<04:53,  2.60s/it]Running Inference:  44%|████▍     | 88/200 [04:40<05:02,  2.70s/it]Running Inference:  44%|████▍     | 89/200 [04:42<04:44,  2.56s/it]Running Inference:  45%|████▌     | 90/200 [04:47<05:37,  3.07s/it]Running Inference:  46%|████▌     | 91/200 [04:50<05:53,  3.25s/it]Running Inference:  46%|████▌     | 92/200 [04:54<05:59,  3.32s/it]Running Inference:  46%|████▋     | 93/200 [04:56<05:18,  2.97s/it]Running Inference:  47%|████▋     | 94/200 [05:00<05:53,  3.34s/it]Running Inference:  48%|████▊     | 95/200 [05:04<06:17,  3.59s/it]Running Inference:  48%|████▊     | 96/200 [05:08<06:07,  3.53s/it]Running Inference:  48%|████▊     | 97/200 [05:10<05:28,  3.19s/it]Running Inference:  49%|████▉     | 98/200 [05:13<04:59,  2.93s/it]Running Inference:  50%|████▉     | 99/200 [05:15<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [05:19<05:06,  3.06s/it]Running Inference:  50%|█████     | 101/200 [05:21<04:37,  2.80s/it]Running Inference:  51%|█████     | 102/200 [05:23<04:18,  2.64s/it]Running Inference:  52%|█████▏    | 103/200 [05:26<04:22,  2.70s/it]Running Inference:  52%|█████▏    | 104/200 [05:30<05:07,  3.21s/it]Running Inference:  52%|█████▎    | 105/200 [05:35<05:36,  3.54s/it]Running Inference:  53%|█████▎    | 106/200 [05:39<05:51,  3.74s/it]Running Inference:  54%|█████▎    | 107/200 [05:43<05:57,  3.84s/it]Running Inference:  54%|█████▍    | 108/200 [05:45<05:08,  3.35s/it]Running Inference:  55%|█████▍    | 109/200 [05:47<04:39,  3.07s/it]Running Inference:  55%|█████▌    | 110/200 [05:51<04:37,  3.09s/it]Running Inference:  56%|█████▌    | 111/200 [05:55<04:58,  3.36s/it]Running Inference:  56%|█████▌    | 112/200 [05:57<04:21,  2.97s/it]Running Inference:  56%|█████▋    | 113/200 [05:59<04:03,  2.80s/it]Running Inference:  57%|█████▋    | 114/200 [06:02<03:57,  2.76s/it]Running Inference:  57%|█████▊    | 115/200 [06:04<03:42,  2.62s/it]Running Inference:  58%|█████▊    | 116/200 [06:06<03:30,  2.50s/it]Running Inference:  58%|█████▊    | 117/200 [06:10<04:09,  3.00s/it]Running Inference:  59%|█████▉    | 118/200 [06:13<03:45,  2.75s/it]Running Inference:  60%|█████▉    | 119/200 [06:15<03:27,  2.56s/it]Running Inference:  60%|██████    | 120/200 [06:17<03:15,  2.45s/it]Running Inference:  60%|██████    | 121/200 [06:19<03:09,  2.40s/it]Running Inference:  61%|██████    | 122/200 [06:21<03:02,  2.34s/it]Running Inference:  62%|██████▏   | 123/200 [06:25<03:25,  2.67s/it]Running Inference:  62%|██████▏   | 124/200 [06:28<03:29,  2.76s/it]Running Inference:  62%|██████▎   | 125/200 [06:31<03:44,  2.99s/it]Running Inference:  63%|██████▎   | 126/200 [06:36<04:08,  3.36s/it]Running Inference:  64%|██████▎   | 127/200 [06:38<03:43,  3.06s/it]Running Inference:  64%|██████▍   | 128/200 [06:41<03:32,  2.95s/it]Running Inference:  64%|██████▍   | 129/200 [06:43<03:22,  2.85s/it]Running Inference:  65%|██████▌   | 130/200 [06:47<03:48,  3.26s/it]Running Inference:  66%|██████▌   | 131/200 [06:50<03:24,  2.96s/it]Running Inference:  66%|██████▌   | 132/200 [06:52<03:09,  2.79s/it]Running Inference:  66%|██████▋   | 133/200 [06:54<02:54,  2.60s/it]Running Inference:  67%|██████▋   | 134/200 [06:55<02:24,  2.19s/it]Running Inference:  68%|██████▊   | 135/200 [06:58<02:25,  2.24s/it]Running Inference:  68%|██████▊   | 136/200 [07:00<02:24,  2.26s/it]Running Inference:  68%|██████▊   | 137/200 [07:04<02:59,  2.85s/it]Running Inference:  69%|██████▉   | 138/200 [07:09<03:21,  3.24s/it]Running Inference:  70%|██████▉   | 139/200 [07:11<03:08,  3.09s/it]Running Inference:  70%|███████   | 140/200 [07:14<03:00,  3.01s/it]Running Inference:  70%|███████   | 141/200 [07:16<02:44,  2.79s/it]Running Inference:  71%|███████   | 142/200 [07:19<02:34,  2.66s/it]Running Inference:  72%|███████▏  | 143/200 [07:23<02:58,  3.13s/it]Running Inference:  72%|███████▏  | 144/200 [07:25<02:29,  2.66s/it]Running Inference:  72%|███████▎  | 145/200 [07:29<02:50,  3.11s/it]Running Inference:  73%|███████▎  | 146/200 [07:31<02:34,  2.86s/it]Running Inference:  74%|███████▎  | 147/200 [07:35<02:53,  3.28s/it]Running Inference:  74%|███████▍  | 148/200 [07:39<03:05,  3.57s/it]Running Inference:  74%|███████▍  | 149/200 [07:44<03:12,  3.78s/it]Running Inference:  75%|███████▌  | 150/200 [07:46<02:48,  3.36s/it]Running Inference:  76%|███████▌  | 151/200 [07:48<02:27,  3.02s/it]Running Inference:  76%|███████▌  | 152/200 [07:51<02:25,  3.03s/it]Running Inference:  76%|███████▋  | 153/200 [07:56<02:40,  3.41s/it]Running Inference:  77%|███████▋  | 154/200 [07:58<02:21,  3.07s/it]Running Inference:  78%|███████▊  | 155/200 [08:01<02:16,  3.04s/it]Running Inference:  78%|███████▊  | 156/200 [08:05<02:22,  3.25s/it]Running Inference:  78%|███████▊  | 157/200 [08:07<02:07,  2.96s/it]Running Inference:  79%|███████▉  | 158/200 [08:09<01:56,  2.76s/it]Running Inference:  80%|███████▉  | 159/200 [08:13<02:11,  3.20s/it]Running Inference:  80%|████████  | 160/200 [08:17<02:09,  3.25s/it]Running Inference:  80%|████████  | 161/200 [08:19<01:54,  2.94s/it]Running Inference:  81%|████████  | 162/200 [08:23<02:06,  3.33s/it]Running Inference:  82%|████████▏ | 163/200 [08:27<02:12,  3.59s/it]Running Inference:  82%|████████▏ | 164/200 [08:30<01:56,  3.23s/it]Running Inference:  82%|████████▎ | 165/200 [08:34<02:03,  3.51s/it]Running Inference:  83%|████████▎ | 166/200 [08:36<01:48,  3.19s/it]Running Inference:  84%|████████▎ | 167/200 [08:39<01:38,  2.97s/it]Running Inference:  84%|████████▍ | 168/200 [08:43<01:47,  3.36s/it]Running Inference:  84%|████████▍ | 169/200 [08:47<01:52,  3.62s/it]Running Inference:  85%|████████▌ | 170/200 [08:50<01:36,  3.23s/it]Running Inference:  86%|████████▌ | 171/200 [08:53<01:34,  3.25s/it]Running Inference:  86%|████████▌ | 172/200 [08:57<01:39,  3.54s/it]Running Inference:  86%|████████▋ | 173/200 [09:01<01:35,  3.54s/it]Running Inference:  87%|████████▋ | 174/200 [09:03<01:21,  3.12s/it]Running Inference:  88%|████████▊ | 175/200 [09:07<01:26,  3.45s/it]Running Inference:  88%|████████▊ | 176/200 [09:12<01:29,  3.71s/it]Running Inference:  88%|████████▊ | 177/200 [09:16<01:28,  3.85s/it]Running Inference:  89%|████████▉ | 178/200 [09:20<01:27,  3.98s/it]Running Inference:  90%|████████▉ | 179/200 [09:22<01:12,  3.45s/it]Running Inference:  90%|█████████ | 180/200 [09:24<00:58,  2.93s/it]Running Inference:  90%|█████████ | 181/200 [09:26<00:51,  2.73s/it]Running Inference:  91%|█████████ | 182/200 [09:29<00:47,  2.63s/it]Running Inference:  92%|█████████▏| 183/200 [09:31<00:42,  2.51s/it]Running Inference:  92%|█████████▏| 184/200 [09:33<00:39,  2.44s/it]Running Inference:  92%|█████████▎| 185/200 [09:35<00:35,  2.37s/it]Running Inference:  93%|█████████▎| 186/200 [09:40<00:41,  2.93s/it]Running Inference:  94%|█████████▎| 187/200 [09:42<00:35,  2.74s/it]Running Inference:  94%|█████████▍| 188/200 [09:46<00:36,  3.03s/it]Running Inference:  94%|█████████▍| 189/200 [09:50<00:37,  3.41s/it]Running Inference:  95%|█████████▌| 190/200 [09:52<00:29,  2.97s/it]Running Inference:  96%|█████████▌| 191/200 [09:54<00:24,  2.74s/it]Running Inference:  96%|█████████▌| 192/200 [09:58<00:25,  3.19s/it]Running Inference:  96%|█████████▋| 193/200 [10:02<00:24,  3.50s/it]Running Inference:  97%|█████████▋| 194/200 [10:07<00:22,  3.72s/it]Running Inference:  98%|█████████▊| 195/200 [10:09<00:16,  3.33s/it]Running Inference:  98%|█████████▊| 196/200 [10:11<00:11,  2.91s/it]Running Inference:  98%|█████████▊| 197/200 [10:14<00:08,  2.95s/it]Running Inference:  99%|█████████▉| 198/200 [10:18<00:06,  3.24s/it]Running Inference: 100%|█████████▉| 199/200 [10:21<00:03,  3.30s/it]Running Inference: 100%|██████████| 200/200 [10:25<00:00,  3.29s/it]Running Inference: 100%|██████████| 200/200 [10:25<00:00,  3.13s/it]
2025-12-13 19:44:08,041 - INFO - Inference completed.
2025-12-13 19:44:08,049 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-13 19:44:08,049 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:44:08,056 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-13 19:44:08,056 - INFO - Metrics:
19.47
2025-12-13 19:44:08,057 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=musique, ratio=0.3) on GPU 1

----------------------------------------
Task: musique | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:44:14,434 - INFO - Set deterministic seeds to 42
2025-12-13 19:44:14,435 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:44:14,435 - INFO - Starting evaluation run...
2025-12-13 19:44:14,435 - INFO - Output directory set to: longbenchresult
2025-12-13 19:44:14,435 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-13 19:44:14,435 - INFO - KV Press 'tova' setup.
2025-12-13 19:44:14,435 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:44:14,435 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.45it/s]
Device set to use cuda:0
2025-12-13 19:44:29,338 - INFO - Model pipeline loaded.
2025-12-13 19:44:29,338 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:44:36,078 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:44:36,078 - INFO - Dataset processed with 200 entries.
2025-12-13 19:44:36,121 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:11,  3.37s/it]Running Inference:   1%|          | 2/200 [00:07<13:05,  3.97s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:33,  3.22s/it]Running Inference:   2%|▏         | 4/200 [00:14<12:01,  3.68s/it]Running Inference:   2%|▎         | 5/200 [00:18<12:38,  3.89s/it]Running Inference:   3%|▎         | 6/200 [00:22<12:57,  4.01s/it]Running Inference:   4%|▎         | 7/200 [00:27<13:15,  4.12s/it]Running Inference:   4%|▍         | 8/200 [00:29<11:13,  3.51s/it]Running Inference:   4%|▍         | 9/200 [00:31<09:49,  3.09s/it]Running Inference:   5%|▌         | 10/200 [00:35<10:57,  3.46s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:37<09:26,  3.00s/it]Running Inference:   6%|▌         | 12/200 [00:42<10:36,  3.39s/it]Running Inference:   6%|▋         | 13/200 [00:44<09:05,  2.92s/it]Running Inference:   7%|▋         | 14/200 [00:47<09:11,  2.96s/it]Running Inference:   8%|▊         | 15/200 [00:49<08:47,  2.85s/it]Running Inference:   8%|▊         | 16/200 [00:53<09:59,  3.26s/it]Running Inference:   8%|▊         | 17/200 [00:56<09:05,  2.98s/it]Running Inference:   9%|▉         | 18/200 [01:00<10:07,  3.34s/it]Running Inference:  10%|▉         | 19/200 [01:04<10:55,  3.62s/it]Running Inference:  10%|█         | 20/200 [01:06<09:33,  3.19s/it]Running Inference:  10%|█         | 21/200 [01:09<08:53,  2.98s/it]Running Inference:  11%|█         | 22/200 [01:13<09:51,  3.32s/it]Running Inference:  12%|█▏        | 23/200 [01:15<08:53,  3.01s/it]Running Inference:  12%|█▏        | 24/200 [01:17<07:33,  2.57s/it]Running Inference:  12%|█▎        | 25/200 [01:19<07:15,  2.49s/it]Running Inference:  13%|█▎        | 26/200 [01:23<08:49,  3.05s/it]Running Inference:  14%|█▎        | 27/200 [01:26<08:02,  2.79s/it]Running Inference:  14%|█▍        | 28/200 [01:29<08:16,  2.89s/it]Running Inference:  14%|█▍        | 29/200 [01:32<08:19,  2.92s/it]Running Inference:  15%|█▌        | 30/200 [01:34<07:39,  2.70s/it]Running Inference:  16%|█▌        | 31/200 [01:36<07:11,  2.55s/it]Running Inference:  16%|█▌        | 32/200 [01:40<08:08,  2.91s/it]Running Inference:  16%|█▋        | 33/200 [01:44<09:13,  3.32s/it]Running Inference:  17%|█▋        | 34/200 [01:48<09:51,  3.56s/it]Running Inference:  18%|█▊        | 35/200 [01:51<08:54,  3.24s/it]Running Inference:  18%|█▊        | 36/200 [01:53<08:11,  3.00s/it]Running Inference:  18%|█▊        | 37/200 [01:56<07:34,  2.79s/it]Running Inference:  19%|█▉        | 38/200 [02:00<08:34,  3.18s/it]Running Inference:  20%|█▉        | 39/200 [02:04<09:25,  3.52s/it]Running Inference:  20%|██        | 40/200 [02:08<09:59,  3.75s/it]Running Inference:  20%|██        | 41/200 [02:10<08:39,  3.27s/it]Running Inference:  21%|██        | 42/200 [02:13<08:12,  3.12s/it]Running Inference:  22%|██▏       | 43/200 [02:16<07:35,  2.90s/it]Running Inference:  22%|██▏       | 44/200 [02:20<08:38,  3.33s/it]Running Inference:  22%|██▎       | 45/200 [02:24<09:12,  3.56s/it]Running Inference:  23%|██▎       | 46/200 [02:28<09:40,  3.77s/it]Running Inference:  24%|██▎       | 47/200 [02:31<08:36,  3.38s/it]Running Inference:  24%|██▍       | 48/200 [02:35<09:11,  3.63s/it]Running Inference:  24%|██▍       | 49/200 [02:39<09:38,  3.83s/it]Running Inference:  25%|██▌       | 50/200 [02:43<09:28,  3.79s/it]Running Inference:  26%|██▌       | 51/200 [02:47<09:21,  3.77s/it]Running Inference:  26%|██▌       | 52/200 [02:50<09:16,  3.76s/it]Running Inference:  26%|██▋       | 53/200 [02:53<08:08,  3.32s/it]Running Inference:  27%|██▋       | 54/200 [02:55<07:16,  2.99s/it]Running Inference:  28%|██▊       | 55/200 [02:57<06:57,  2.88s/it]Running Inference:  28%|██▊       | 56/200 [03:02<07:56,  3.31s/it]Running Inference:  28%|██▊       | 57/200 [03:04<07:15,  3.05s/it]Running Inference:  29%|██▉       | 58/200 [03:06<06:18,  2.67s/it]Running Inference:  30%|██▉       | 59/200 [03:08<05:57,  2.54s/it]Running Inference:  30%|███       | 60/200 [03:12<07:07,  3.05s/it]Running Inference:  30%|███       | 61/200 [03:17<07:59,  3.45s/it]Running Inference:  31%|███       | 62/200 [03:19<07:03,  3.07s/it]Running Inference:  32%|███▏      | 63/200 [03:22<06:45,  2.96s/it]Running Inference:  32%|███▏      | 64/200 [03:26<07:38,  3.37s/it]Running Inference:  32%|███▎      | 65/200 [03:30<07:55,  3.52s/it]Running Inference:  33%|███▎      | 66/200 [03:34<08:24,  3.76s/it]Running Inference:  34%|███▎      | 67/200 [03:37<07:22,  3.32s/it]Running Inference:  34%|███▍      | 68/200 [03:39<06:52,  3.13s/it]Running Inference:  34%|███▍      | 69/200 [03:42<06:15,  2.86s/it]Running Inference:  35%|███▌      | 70/200 [03:44<06:15,  2.89s/it]Running Inference:  36%|███▌      | 71/200 [03:49<07:07,  3.31s/it]Running Inference:  36%|███▌      | 72/200 [03:51<06:31,  3.06s/it]Running Inference:  36%|███▋      | 73/200 [03:53<05:58,  2.83s/it]Running Inference:  37%|███▋      | 74/200 [03:58<06:53,  3.28s/it]Running Inference:  38%|███▊      | 75/200 [04:02<07:32,  3.62s/it]Running Inference:  38%|███▊      | 76/200 [04:05<06:49,  3.31s/it]Running Inference:  38%|███▊      | 77/200 [04:06<05:46,  2.81s/it]Running Inference:  39%|███▉      | 78/200 [04:09<05:22,  2.65s/it]Running Inference:  40%|███▉      | 79/200 [04:12<05:24,  2.68s/it]Running Inference:  40%|████      | 80/200 [04:14<05:04,  2.54s/it]Running Inference:  40%|████      | 81/200 [04:16<04:58,  2.51s/it]Running Inference:  41%|████      | 82/200 [04:21<06:02,  3.07s/it]Running Inference:  42%|████▏     | 83/200 [04:23<05:30,  2.83s/it]Running Inference:  42%|████▏     | 84/200 [04:25<05:06,  2.64s/it]Running Inference:  42%|████▎     | 85/200 [04:28<05:16,  2.76s/it]Running Inference:  43%|████▎     | 86/200 [04:31<05:12,  2.74s/it]Running Inference:  44%|████▎     | 87/200 [04:33<04:53,  2.59s/it]Running Inference:  44%|████▍     | 88/200 [04:36<05:02,  2.70s/it]Running Inference:  44%|████▍     | 89/200 [04:38<04:44,  2.56s/it]Running Inference:  45%|████▌     | 90/200 [04:42<05:38,  3.08s/it]Running Inference:  46%|████▌     | 91/200 [04:47<06:11,  3.40s/it]Running Inference:  46%|████▌     | 92/200 [04:50<06:12,  3.45s/it]Running Inference:  46%|████▋     | 93/200 [04:52<05:27,  3.06s/it]Running Inference:  47%|████▋     | 94/200 [04:57<06:01,  3.41s/it]Running Inference:  48%|████▊     | 95/200 [05:01<06:24,  3.66s/it]Running Inference:  48%|████▊     | 96/200 [05:04<06:13,  3.59s/it]Running Inference:  48%|████▊     | 97/200 [05:07<05:32,  3.23s/it]Running Inference:  49%|████▉     | 98/200 [05:09<05:02,  2.97s/it]Running Inference:  50%|████▉     | 99/200 [05:11<04:31,  2.69s/it]Running Inference:  50%|█████     | 100/200 [05:13<04:16,  2.57s/it]Running Inference:  50%|█████     | 101/200 [05:17<05:00,  3.04s/it]Running Inference:  51%|█████     | 102/200 [05:20<04:34,  2.80s/it]Running Inference:  52%|█████▏    | 103/200 [05:23<04:34,  2.83s/it]Running Inference:  52%|█████▏    | 104/200 [05:25<04:22,  2.73s/it]Running Inference:  52%|█████▎    | 105/200 [05:29<05:06,  3.22s/it]Running Inference:  53%|█████▎    | 106/200 [05:34<05:32,  3.54s/it]Running Inference:  54%|█████▎    | 107/200 [05:38<05:45,  3.71s/it]Running Inference:  54%|█████▍    | 108/200 [05:40<05:00,  3.26s/it]Running Inference:  55%|█████▍    | 109/200 [05:42<04:34,  3.01s/it]Running Inference:  55%|█████▌    | 110/200 [05:47<05:05,  3.40s/it]Running Inference:  56%|█████▌    | 111/200 [05:51<05:18,  3.58s/it]Running Inference:  56%|█████▌    | 112/200 [05:53<04:35,  3.13s/it]Running Inference:  56%|█████▋    | 113/200 [05:56<04:33,  3.14s/it]Running Inference:  57%|█████▋    | 114/200 [06:00<04:58,  3.47s/it]Running Inference:  57%|█████▊    | 115/200 [06:03<04:24,  3.11s/it]Running Inference:  58%|█████▊    | 116/200 [06:05<04:02,  2.88s/it]Running Inference:  58%|█████▊    | 117/200 [06:09<04:32,  3.28s/it]Running Inference:  59%|█████▉    | 118/200 [06:11<04:01,  2.94s/it]Running Inference:  60%|█████▉    | 119/200 [06:13<03:40,  2.72s/it]Running Inference:  60%|██████    | 120/200 [06:16<03:24,  2.56s/it]Running Inference:  60%|██████    | 121/200 [06:18<03:15,  2.48s/it]Running Inference:  61%|██████    | 122/200 [06:20<03:06,  2.40s/it]Running Inference:  62%|██████▏   | 123/200 [06:24<03:49,  2.99s/it]Running Inference:  62%|██████▏   | 124/200 [06:29<04:16,  3.37s/it]Running Inference:  62%|██████▎   | 125/200 [06:32<04:17,  3.43s/it]Running Inference:  63%|██████▎   | 126/200 [06:37<04:32,  3.69s/it]Running Inference:  64%|██████▎   | 127/200 [06:39<04:00,  3.29s/it]Running Inference:  64%|██████▍   | 128/200 [06:42<03:42,  3.09s/it]Running Inference:  64%|██████▍   | 129/200 [06:44<03:30,  2.96s/it]Running Inference:  65%|██████▌   | 130/200 [06:47<03:32,  3.04s/it]Running Inference:  66%|██████▌   | 131/200 [06:50<03:13,  2.81s/it]Running Inference:  66%|██████▌   | 132/200 [06:52<03:03,  2.70s/it]Running Inference:  66%|██████▋   | 133/200 [06:54<02:49,  2.54s/it]Running Inference:  67%|██████▋   | 134/200 [06:56<02:21,  2.14s/it]Running Inference:  68%|██████▊   | 135/200 [06:58<02:23,  2.20s/it]Running Inference:  68%|██████▊   | 136/200 [07:00<02:22,  2.23s/it]Running Inference:  68%|██████▊   | 137/200 [07:04<02:59,  2.85s/it]Running Inference:  69%|██████▉   | 138/200 [07:09<03:21,  3.25s/it]Running Inference:  70%|██████▉   | 139/200 [07:13<03:37,  3.56s/it]Running Inference:  70%|███████   | 140/200 [07:16<03:18,  3.31s/it]Running Inference:  70%|███████   | 141/200 [07:18<02:58,  3.03s/it]Running Inference:  71%|███████   | 142/200 [07:20<02:40,  2.76s/it]Running Inference:  72%|███████▏  | 143/200 [07:24<03:03,  3.21s/it]Running Inference:  72%|███████▏  | 144/200 [07:27<02:55,  3.14s/it]Running Inference:  72%|███████▎  | 145/200 [07:32<03:10,  3.46s/it]Running Inference:  73%|███████▎  | 146/200 [07:34<02:47,  3.11s/it]Running Inference:  74%|███████▎  | 147/200 [07:38<03:03,  3.47s/it]Running Inference:  74%|███████▍  | 148/200 [07:43<03:13,  3.72s/it]Running Inference:  74%|███████▍  | 149/200 [07:47<03:18,  3.89s/it]Running Inference:  75%|███████▌  | 150/200 [07:49<02:51,  3.43s/it]Running Inference:  76%|███████▌  | 151/200 [07:51<02:30,  3.06s/it]Running Inference:  76%|███████▌  | 152/200 [07:54<02:16,  2.84s/it]Running Inference:  76%|███████▋  | 153/200 [07:58<02:34,  3.29s/it]Running Inference:  77%|███████▋  | 154/200 [08:00<02:17,  2.99s/it]Running Inference:  78%|███████▊  | 155/200 [08:03<02:13,  2.97s/it]Running Inference:  78%|███████▊  | 156/200 [08:06<02:04,  2.82s/it]Running Inference:  78%|███████▊  | 157/200 [08:08<01:54,  2.67s/it]Running Inference:  79%|███████▉  | 158/200 [08:10<01:47,  2.56s/it]Running Inference:  80%|███████▉  | 159/200 [08:15<02:05,  3.07s/it]Running Inference:  80%|████████  | 160/200 [08:18<02:05,  3.15s/it]Running Inference:  80%|████████  | 161/200 [08:20<01:51,  2.87s/it]Running Inference:  81%|████████  | 162/200 [08:24<02:05,  3.30s/it]Running Inference:  82%|████████▏ | 163/200 [08:29<02:12,  3.58s/it]Running Inference:  82%|████████▏ | 164/200 [08:31<01:56,  3.22s/it]Running Inference:  82%|████████▎ | 165/200 [08:35<02:03,  3.53s/it]Running Inference:  83%|████████▎ | 166/200 [08:38<01:48,  3.20s/it]Running Inference:  84%|████████▎ | 167/200 [08:40<01:38,  2.98s/it]Running Inference:  84%|████████▍ | 168/200 [08:45<01:48,  3.38s/it]Running Inference:  84%|████████▍ | 169/200 [08:49<01:53,  3.66s/it]Running Inference:  85%|████████▌ | 170/200 [08:51<01:37,  3.26s/it]Running Inference:  86%|████████▌ | 171/200 [08:54<01:31,  3.17s/it]Running Inference:  86%|████████▌ | 172/200 [08:58<01:37,  3.50s/it]Running Inference:  86%|████████▋ | 173/200 [09:02<01:34,  3.52s/it]Running Inference:  87%|████████▋ | 174/200 [09:04<01:20,  3.11s/it]Running Inference:  88%|████████▊ | 175/200 [09:08<01:26,  3.46s/it]Running Inference:  88%|████████▊ | 176/200 [09:13<01:29,  3.73s/it]Running Inference:  88%|████████▊ | 177/200 [09:17<01:29,  3.88s/it]Running Inference:  89%|████████▉ | 178/200 [09:19<01:14,  3.41s/it]Running Inference:  90%|████████▉ | 179/200 [09:21<01:03,  3.05s/it]Running Inference:  90%|█████████ | 180/200 [09:23<00:52,  2.62s/it]Running Inference:  90%|█████████ | 181/200 [09:25<00:47,  2.51s/it]Running Inference:  91%|█████████ | 182/200 [09:28<00:44,  2.47s/it]Running Inference:  92%|█████████▏| 183/200 [09:30<00:40,  2.40s/it]Running Inference:  92%|█████████▏| 184/200 [09:34<00:47,  2.97s/it]Running Inference:  92%|█████████▎| 185/200 [09:37<00:41,  2.74s/it]Running Inference:  93%|█████████▎| 186/200 [09:39<00:36,  2.58s/it]Running Inference:  94%|█████████▎| 187/200 [09:41<00:32,  2.49s/it]Running Inference:  94%|█████████▍| 188/200 [09:45<00:34,  2.87s/it]Running Inference:  94%|█████████▍| 189/200 [09:49<00:36,  3.32s/it]Running Inference:  95%|█████████▌| 190/200 [09:51<00:29,  2.91s/it]Running Inference:  96%|█████████▌| 191/200 [09:55<00:29,  3.32s/it]Running Inference:  96%|█████████▌| 192/200 [09:58<00:26,  3.26s/it]Running Inference:  96%|█████████▋| 193/200 [10:03<00:24,  3.57s/it]Running Inference:  97%|█████████▋| 194/200 [10:07<00:22,  3.78s/it]Running Inference:  98%|█████████▊| 195/200 [10:09<00:16,  3.37s/it]Running Inference:  98%|█████████▊| 196/200 [10:11<00:11,  2.94s/it]Running Inference:  98%|█████████▊| 197/200 [10:14<00:08,  2.98s/it]Running Inference:  99%|█████████▉| 198/200 [10:18<00:06,  3.08s/it]Running Inference: 100%|█████████▉| 199/200 [10:21<00:03,  3.08s/it]Running Inference: 100%|██████████| 200/200 [10:23<00:00,  2.82s/it]Running Inference: 100%|██████████| 200/200 [10:23<00:00,  3.12s/it]
2025-12-13 19:54:59,709 - INFO - Inference completed.
2025-12-13 19:54:59,717 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-13 19:54:59,717 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:54:59,724 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-13 19:54:59,724 - INFO - Metrics:
18.68
2025-12-13 19:54:59,725 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=musique, ratio=0.5) on GPU 1


========================================
LongBench Task: narrativeqa
========================================
----------------------------------------
Task: narrativeqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:55:06,188 - INFO - Set deterministic seeds to 42
2025-12-13 19:55:06,188 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:55:06,188 - INFO - Starting evaluation run...
2025-12-13 19:55:06,188 - INFO - Output directory set to: longbenchresult
2025-12-13 19:55:06,188 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 19:55:06,188 - INFO - KV Press 'tova' setup.
2025-12-13 19:55:06,188 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:55:06,188 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.43it/s]
Device set to use cuda:0
2025-12-13 19:55:19,639 - INFO - Model pipeline loaded.
2025-12-13 19:55:19,639 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 3391.34 examples/s]
2025-12-13 19:55:49,366 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:55:49,366 - INFO - Dataset processed with 200 entries.
2025-12-13 19:55:49,390 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:14<04:34, 14.42s/it]Running Inference:  10%|█         | 2/20 [00:25<03:44, 12.45s/it]Running Inference:  15%|█▌        | 3/20 [00:47<04:47, 16.93s/it]Running Inference:  20%|██        | 4/20 [01:07<04:51, 18.19s/it]Running Inference:  25%|██▌       | 5/20 [01:41<05:54, 23.64s/it]Running Inference:  30%|███       | 6/20 [01:56<04:49, 20.70s/it]Running Inference:  35%|███▌      | 7/20 [02:16<04:29, 20.70s/it]Running Inference:  40%|████      | 8/20 [02:34<03:56, 19.69s/it]Running Inference:  45%|████▌     | 9/20 [02:40<02:49, 15.44s/it]Running Inference:  50%|█████     | 10/20 [02:58<02:42, 16.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [03:03<01:55, 12.84s/it]Running Inference:  60%|██████    | 12/20 [03:20<01:52, 14.06s/it]Running Inference:  65%|██████▌   | 13/20 [03:31<01:32, 13.26s/it]Running Inference:  70%|███████   | 14/20 [03:37<01:06, 11.02s/it]Running Inference:  75%|███████▌  | 15/20 [03:45<00:50, 10.16s/it]Running Inference:  80%|████████  | 16/20 [03:49<00:32,  8.11s/it]Running Inference:  85%|████████▌ | 17/20 [04:22<00:46, 15.67s/it]Running Inference:  90%|█████████ | 18/20 [04:27<00:25, 12.53s/it]Running Inference:  95%|█████████▌| 19/20 [04:31<00:10, 10.02s/it]Running Inference: 100%|██████████| 20/20 [04:41<00:00,  9.79s/it]Running Inference: 100%|██████████| 20/20 [04:41<00:00, 14.06s/it]
2025-12-13 20:00:30,624 - INFO - Inference completed.
2025-12-13 20:00:30,632 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 20:00:30,632 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:00:30,636 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 20:00:30,637 - INFO - Metrics:
18.39
2025-12-13 20:00:30,638 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=narrativeqa, ratio=0.1) on GPU 1

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:00:37,039 - INFO - Set deterministic seeds to 42
2025-12-13 20:00:37,039 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:00:37,039 - INFO - Starting evaluation run...
2025-12-13 20:00:37,039 - INFO - Output directory set to: longbenchresult
2025-12-13 20:00:37,039 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 20:00:37,039 - INFO - KV Press 'tova' setup.
2025-12-13 20:00:37,039 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:00:37,039 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.08it/s]
Device set to use cuda:0
2025-12-13 20:00:49,660 - INFO - Model pipeline loaded.
2025-12-13 20:00:49,660 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:00:54,483 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:00:54,483 - INFO - Dataset processed with 200 entries.
2025-12-13 20:00:54,507 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:20<06:28, 20.45s/it]Running Inference:  10%|█         | 2/20 [00:31<04:28, 14.92s/it]Running Inference:  15%|█▌        | 3/20 [00:53<05:08, 18.16s/it]Running Inference:  20%|██        | 4/20 [01:19<05:42, 21.42s/it]Running Inference:  25%|██▌       | 5/20 [01:47<05:54, 23.61s/it]Running Inference:  30%|███       | 6/20 [02:02<04:49, 20.65s/it]Running Inference:  35%|███▌      | 7/20 [02:27<04:47, 22.08s/it]Running Inference:  40%|████      | 8/20 [02:38<03:44, 18.69s/it]Running Inference:  45%|████▌     | 9/20 [02:44<02:42, 14.73s/it]Running Inference:  50%|█████     | 10/20 [03:02<02:36, 15.67s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [03:07<01:51, 12.39s/it]Running Inference:  60%|██████    | 12/20 [03:24<01:49, 13.64s/it]Running Inference:  65%|██████▌   | 13/20 [03:29<01:19, 11.29s/it]Running Inference:  70%|███████   | 14/20 [03:35<00:57,  9.57s/it]Running Inference:  75%|███████▌  | 15/20 [03:49<00:54, 10.84s/it]Running Inference:  80%|████████  | 16/20 [03:58<00:41, 10.38s/it]Running Inference:  85%|████████▌ | 17/20 [04:18<00:39, 13.16s/it]Running Inference:  90%|█████████ | 18/20 [04:23<00:21, 10.81s/it]Running Inference:  95%|█████████▌| 19/20 [04:27<00:08,  8.69s/it]Running Inference: 100%|██████████| 20/20 [04:36<00:00,  8.84s/it]Running Inference: 100%|██████████| 20/20 [04:36<00:00, 13.83s/it]
2025-12-13 20:05:31,058 - INFO - Inference completed.
2025-12-13 20:05:31,066 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 20:05:31,066 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:05:31,070 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 20:05:31,070 - INFO - Metrics:
17.53
2025-12-13 20:05:31,072 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=narrativeqa, ratio=0.2) on GPU 1

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:05:37,548 - INFO - Set deterministic seeds to 42
2025-12-13 20:05:37,548 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:05:37,548 - INFO - Starting evaluation run...
2025-12-13 20:05:37,548 - INFO - Output directory set to: longbenchresult
2025-12-13 20:05:37,549 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 20:05:37,549 - INFO - KV Press 'tova' setup.
2025-12-13 20:05:37,549 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:05:37,549 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.52it/s]
Device set to use cuda:0
2025-12-13 20:05:50,397 - INFO - Model pipeline loaded.
2025-12-13 20:05:50,397 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:05:56,114 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:05:56,114 - INFO - Dataset processed with 200 entries.
2025-12-13 20:05:56,140 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:13<04:20, 13.71s/it]Running Inference:  10%|█         | 2/20 [00:24<03:37, 12.07s/it]Running Inference:  15%|█▌        | 3/20 [00:50<05:14, 18.51s/it]Running Inference:  20%|██        | 4/20 [01:23<06:26, 24.14s/it]Running Inference:  25%|██▌       | 5/20 [01:55<06:43, 26.90s/it]Running Inference:  30%|███       | 6/20 [02:10<05:20, 22.88s/it]Running Inference:  35%|███▌      | 7/20 [02:34<05:03, 23.34s/it]Running Inference:  40%|████      | 8/20 [02:57<04:37, 23.09s/it]Running Inference:  45%|████▌     | 9/20 [03:03<03:16, 17.89s/it]Running Inference:  50%|█████     | 10/20 [03:21<02:58, 17.82s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [03:26<02:05, 13.91s/it]Running Inference:  60%|██████    | 12/20 [03:42<01:56, 14.57s/it]Running Inference:  65%|██████▌   | 13/20 [03:53<01:35, 13.58s/it]Running Inference:  70%|███████   | 14/20 [04:11<01:28, 14.73s/it]Running Inference:  75%|███████▌  | 15/20 [04:18<01:03, 12.63s/it]Running Inference:  80%|████████  | 16/20 [04:28<00:46, 11.61s/it]Running Inference:  85%|████████▌ | 17/20 [04:47<00:41, 13.89s/it]Running Inference:  90%|█████████ | 18/20 [04:52<00:22, 11.12s/it]Running Inference:  95%|█████████▌| 19/20 [04:56<00:09,  9.09s/it]Running Inference: 100%|██████████| 20/20 [05:05<00:00,  9.10s/it]Running Inference: 100%|██████████| 20/20 [05:05<00:00, 15.28s/it]
2025-12-13 20:11:01,723 - INFO - Inference completed.
2025-12-13 20:11:01,731 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-13 20:11:01,731 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:11:01,736 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-13 20:11:01,736 - INFO - Metrics:
17.17
2025-12-13 20:11:01,737 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=narrativeqa, ratio=0.3) on GPU 1

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:11:08,215 - INFO - Set deterministic seeds to 42
2025-12-13 20:11:08,215 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:11:08,215 - INFO - Starting evaluation run...
2025-12-13 20:11:08,215 - INFO - Output directory set to: longbenchresult
2025-12-13 20:11:08,215 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-13 20:11:08,215 - INFO - KV Press 'tova' setup.
2025-12-13 20:11:08,215 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:11:08,215 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.33it/s]
Device set to use cuda:0
2025-12-13 20:11:21,293 - INFO - Model pipeline loaded.
2025-12-13 20:11:21,294 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:11:32,582 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:11:32,582 - INFO - Dataset processed with 200 entries.
2025-12-13 20:11:32,606 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:19<06:17, 19.86s/it]Running Inference:  10%|█         | 2/20 [00:30<04:19, 14.42s/it]Running Inference:  15%|█▌        | 3/20 [00:44<04:04, 14.40s/it]Running Inference:  20%|██        | 4/20 [01:01<04:07, 15.46s/it]Running Inference:  25%|██▌       | 5/20 [01:32<05:15, 21.03s/it]Running Inference:  30%|███       | 6/20 [01:47<04:23, 18.82s/it]Running Inference:  35%|███▌      | 7/20 [02:06<04:07, 19.05s/it]Running Inference:  40%|████      | 8/20 [02:17<03:16, 16.35s/it]Running Inference:  45%|████▌     | 9/20 [02:24<02:26, 13.36s/it]Running Inference:  50%|█████     | 10/20 [02:41<02:25, 14.57s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [02:46<01:45, 11.72s/it]Running Inference:  60%|██████    | 12/20 [03:02<01:43, 12.96s/it]Running Inference:  65%|██████▌   | 13/20 [03:08<01:15, 10.82s/it]Running Inference:  70%|███████   | 14/20 [03:14<00:55,  9.25s/it]Running Inference:  75%|███████▌  | 15/20 [03:27<00:52, 10.47s/it]Running Inference:  80%|████████  | 16/20 [03:36<00:40, 10.10s/it]Running Inference:  85%|████████▌ | 17/20 [03:55<00:37, 12.65s/it]Running Inference:  90%|█████████ | 18/20 [04:00<00:20, 10.48s/it]Running Inference:  95%|█████████▌| 19/20 [04:05<00:08,  8.89s/it]Running Inference: 100%|██████████| 20/20 [04:33<00:00, 14.43s/it]Running Inference: 100%|██████████| 20/20 [04:33<00:00, 13.66s/it]
2025-12-13 20:16:05,807 - INFO - Inference completed.
2025-12-13 20:16:05,815 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-13 20:16:05,815 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:16:05,820 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-13 20:16:05,820 - INFO - Metrics:
15.97
2025-12-13 20:16:05,821 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=narrativeqa, ratio=0.5) on GPU 1


========================================
LongBench Task: qasper
========================================
----------------------------------------
Task: qasper | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:16:12,388 - INFO - Set deterministic seeds to 42
2025-12-13 20:16:12,389 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:16:12,389 - INFO - Starting evaluation run...
2025-12-13 20:16:12,389 - INFO - Output directory set to: longbenchresult
2025-12-13 20:16:12,389 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 20:16:12,389 - INFO - KV Press 'tova' setup.
2025-12-13 20:16:12,389 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:16:12,389 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.36it/s]
Device set to use cuda:0
2025-12-13 20:16:24,012 - INFO - Model pipeline loaded.
2025-12-13 20:16:24,012 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 3873.36 examples/s]
2025-12-13 20:16:32,676 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:16:32,676 - INFO - Dataset processed with 200 entries.
2025-12-13 20:16:32,691 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:03<08:14,  3.36s/it]Running Inference:   1%|▏         | 2/148 [00:03<03:55,  1.61s/it]Running Inference:   2%|▏         | 3/148 [00:04<02:46,  1.14s/it]Running Inference:   3%|▎         | 4/148 [00:06<03:39,  1.52s/it]Running Inference:   3%|▎         | 5/148 [00:07<03:26,  1.44s/it]Running Inference:   4%|▍         | 6/148 [00:09<03:35,  1.52s/it]Running Inference:   5%|▍         | 7/148 [00:10<03:36,  1.53s/it]Running Inference:   5%|▌         | 8/148 [00:12<03:13,  1.38s/it]Running Inference:   6%|▌         | 9/148 [00:14<03:47,  1.64s/it]Running Inference:   7%|▋         | 10/148 [00:15<03:38,  1.58s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:17<03:37,  1.59s/it]Running Inference:   8%|▊         | 12/148 [00:18<03:39,  1.62s/it]Running Inference:   9%|▉         | 13/148 [00:20<03:42,  1.64s/it]Running Inference:   9%|▉         | 14/148 [00:21<03:24,  1.52s/it]Running Inference:  10%|█         | 15/148 [00:22<02:49,  1.27s/it]Running Inference:  11%|█         | 16/148 [00:26<04:31,  2.06s/it]Running Inference:  11%|█▏        | 17/148 [00:27<04:00,  1.83s/it]Running Inference:  12%|█▏        | 18/148 [00:29<04:07,  1.90s/it]Running Inference:  13%|█▎        | 19/148 [00:31<03:40,  1.71s/it]Running Inference:  14%|█▎        | 20/148 [00:32<03:14,  1.52s/it]Running Inference:  14%|█▍        | 21/148 [00:33<03:11,  1.51s/it]Running Inference:  15%|█▍        | 22/148 [00:35<03:23,  1.62s/it]Running Inference:  16%|█▌        | 23/148 [00:36<02:52,  1.38s/it]Running Inference:  16%|█▌        | 24/148 [00:37<02:45,  1.34s/it]Running Inference:  17%|█▋        | 25/148 [00:39<03:08,  1.53s/it]Running Inference:  18%|█▊        | 26/148 [00:44<05:07,  2.52s/it]Running Inference:  18%|█▊        | 27/148 [00:45<04:21,  2.16s/it]Running Inference:  19%|█▉        | 28/148 [00:46<03:32,  1.77s/it]Running Inference:  20%|█▉        | 29/148 [00:48<03:43,  1.88s/it]Running Inference:  20%|██        | 30/148 [00:49<02:55,  1.49s/it]Running Inference:  21%|██        | 31/148 [00:50<02:40,  1.37s/it]Running Inference:  22%|██▏       | 32/148 [00:51<02:37,  1.36s/it]Running Inference:  22%|██▏       | 33/148 [00:52<02:24,  1.26s/it]Running Inference:  23%|██▎       | 34/148 [00:54<02:30,  1.32s/it]Running Inference:  24%|██▎       | 35/148 [00:55<02:27,  1.31s/it]Running Inference:  24%|██▍       | 36/148 [01:02<05:34,  2.98s/it]Running Inference:  25%|██▌       | 37/148 [01:03<04:26,  2.40s/it]Running Inference:  26%|██▌       | 38/148 [01:04<03:30,  1.92s/it]Running Inference:  26%|██▋       | 39/148 [01:04<02:39,  1.47s/it]Running Inference:  27%|██▋       | 40/148 [01:05<02:09,  1.20s/it]Running Inference:  28%|██▊       | 41/148 [01:06<02:26,  1.37s/it]Running Inference:  28%|██▊       | 42/148 [01:07<02:00,  1.14s/it]Running Inference:  29%|██▉       | 43/148 [01:09<02:32,  1.45s/it]Running Inference:  30%|██▉       | 44/148 [01:11<02:45,  1.59s/it]Running Inference:  30%|███       | 45/148 [01:13<02:40,  1.56s/it]Running Inference:  31%|███       | 46/148 [01:15<03:03,  1.80s/it]Running Inference:  32%|███▏      | 47/148 [01:17<02:57,  1.76s/it]Running Inference:  32%|███▏      | 48/148 [01:17<02:26,  1.46s/it]Running Inference:  33%|███▎      | 49/148 [01:27<06:14,  3.78s/it]Running Inference:  34%|███▍      | 50/148 [01:28<04:52,  2.99s/it]Running Inference:  34%|███▍      | 51/148 [01:29<04:09,  2.57s/it]Running Inference:  35%|███▌      | 52/148 [01:31<03:26,  2.15s/it]Running Inference:  36%|███▌      | 53/148 [01:32<02:53,  1.83s/it]Running Inference:  36%|███▋      | 54/148 [01:33<02:28,  1.58s/it]Running Inference:  37%|███▋      | 55/148 [01:33<02:03,  1.32s/it]Running Inference:  38%|███▊      | 56/148 [01:35<01:58,  1.29s/it]Running Inference:  39%|███▊      | 57/148 [01:36<01:49,  1.20s/it]Running Inference:  39%|███▉      | 58/148 [01:37<01:59,  1.33s/it]Running Inference:  40%|███▉      | 59/148 [01:38<01:55,  1.29s/it]Running Inference:  41%|████      | 60/148 [01:39<01:42,  1.17s/it]Running Inference:  41%|████      | 61/148 [01:40<01:41,  1.17s/it]Running Inference:  42%|████▏     | 62/148 [01:41<01:34,  1.10s/it]Running Inference:  43%|████▎     | 63/148 [01:42<01:32,  1.09s/it]Running Inference:  43%|████▎     | 64/148 [01:45<02:19,  1.67s/it]Running Inference:  44%|████▍     | 65/148 [01:48<02:28,  1.79s/it]Running Inference:  45%|████▍     | 66/148 [01:48<01:59,  1.46s/it]Running Inference:  45%|████▌     | 67/148 [01:50<02:04,  1.54s/it]Running Inference:  46%|████▌     | 68/148 [01:51<01:43,  1.29s/it]Running Inference:  47%|████▋     | 69/148 [01:52<01:38,  1.25s/it]Running Inference:  47%|████▋     | 70/148 [01:54<01:56,  1.49s/it]Running Inference:  48%|████▊     | 71/148 [01:55<01:55,  1.50s/it]Running Inference:  49%|████▊     | 72/148 [01:58<02:10,  1.71s/it]Running Inference:  49%|████▉     | 73/148 [01:59<02:07,  1.71s/it]Running Inference:  50%|█████     | 74/148 [02:01<02:02,  1.65s/it]Running Inference:  51%|█████     | 75/148 [02:03<02:10,  1.79s/it]Running Inference:  51%|█████▏    | 76/148 [02:05<02:11,  1.83s/it]Running Inference:  52%|█████▏    | 77/148 [02:06<02:00,  1.70s/it]Running Inference:  53%|█████▎    | 78/148 [02:07<01:47,  1.53s/it]Running Inference:  53%|█████▎    | 79/148 [02:08<01:29,  1.30s/it]Running Inference:  54%|█████▍    | 80/148 [02:09<01:24,  1.25s/it]Running Inference:  55%|█████▍    | 81/148 [02:10<01:17,  1.15s/it]Running Inference:  55%|█████▌    | 82/148 [02:12<01:19,  1.20s/it]Running Inference:  56%|█████▌    | 83/148 [02:12<01:10,  1.09s/it]Running Inference:  57%|█████▋    | 84/148 [02:13<00:57,  1.12it/s]Running Inference:  57%|█████▋    | 85/148 [02:14<00:56,  1.12it/s]Running Inference:  58%|█████▊    | 86/148 [02:15<00:58,  1.06it/s]Running Inference:  59%|█████▉    | 87/148 [02:16<01:09,  1.13s/it]Running Inference:  59%|█████▉    | 88/148 [02:19<01:29,  1.49s/it]Running Inference:  60%|██████    | 89/148 [02:20<01:27,  1.49s/it]Running Inference:  61%|██████    | 90/148 [02:21<01:14,  1.29s/it]Running Inference:  61%|██████▏   | 91/148 [02:22<01:12,  1.27s/it]Running Inference:  62%|██████▏   | 92/148 [02:23<01:02,  1.11s/it]Running Inference:  63%|██████▎   | 93/148 [02:24<01:06,  1.20s/it]Running Inference:  64%|██████▎   | 94/148 [02:26<01:12,  1.34s/it]Running Inference:  64%|██████▍   | 95/148 [02:27<01:04,  1.21s/it]Running Inference:  65%|██████▍   | 96/148 [02:28<00:58,  1.12s/it]Running Inference:  66%|██████▌   | 97/148 [02:29<00:50,  1.01it/s]Running Inference:  66%|██████▌   | 98/148 [02:31<01:06,  1.33s/it]Running Inference:  67%|██████▋   | 99/148 [02:31<00:57,  1.17s/it]Running Inference:  68%|██████▊   | 100/148 [02:32<00:52,  1.09s/it]Running Inference:  68%|██████▊   | 101/148 [02:34<00:56,  1.20s/it]Running Inference:  69%|██████▉   | 102/148 [02:38<01:32,  2.01s/it]Running Inference:  70%|██████▉   | 103/148 [02:39<01:17,  1.72s/it]Running Inference:  70%|███████   | 104/148 [02:40<01:05,  1.48s/it]Running Inference:  71%|███████   | 105/148 [02:42<01:08,  1.60s/it]Running Inference:  72%|███████▏  | 106/148 [02:45<01:25,  2.05s/it]Running Inference:  72%|███████▏  | 107/148 [02:45<01:05,  1.59s/it]Running Inference:  73%|███████▎  | 108/148 [02:46<00:53,  1.34s/it]Running Inference:  74%|███████▎  | 109/148 [02:47<00:50,  1.31s/it]Running Inference:  74%|███████▍  | 110/148 [02:48<00:43,  1.15s/it]Running Inference:  75%|███████▌  | 111/148 [02:49<00:39,  1.06s/it]Running Inference:  76%|███████▌  | 112/148 [02:49<00:32,  1.11it/s]Running Inference:  76%|███████▋  | 113/148 [02:51<00:40,  1.15s/it]Running Inference:  77%|███████▋  | 114/148 [02:56<01:12,  2.15s/it]Running Inference:  78%|███████▊  | 115/148 [02:56<00:52,  1.60s/it]Running Inference:  78%|███████▊  | 116/148 [02:57<00:48,  1.51s/it]Running Inference:  79%|███████▉  | 117/148 [02:58<00:43,  1.42s/it]Running Inference:  80%|███████▉  | 118/148 [02:59<00:34,  1.15s/it]Running Inference:  80%|████████  | 119/148 [03:01<00:40,  1.39s/it]Running Inference:  81%|████████  | 120/148 [03:02<00:41,  1.48s/it]Running Inference:  82%|████████▏ | 121/148 [03:04<00:40,  1.51s/it]Running Inference:  82%|████████▏ | 122/148 [03:05<00:32,  1.23s/it]Running Inference:  83%|████████▎ | 123/148 [03:12<01:13,  2.93s/it]Running Inference:  84%|████████▍ | 124/148 [03:13<01:00,  2.52s/it]Running Inference:  84%|████████▍ | 125/148 [03:19<01:18,  3.39s/it]Running Inference:  85%|████████▌ | 126/148 [03:21<01:11,  3.25s/it]Running Inference:  86%|████████▌ | 127/148 [03:22<00:53,  2.55s/it]Running Inference:  86%|████████▋ | 128/148 [03:23<00:40,  2.03s/it]Running Inference:  87%|████████▋ | 129/148 [03:24<00:31,  1.63s/it]Running Inference:  88%|████████▊ | 130/148 [03:30<00:55,  3.08s/it]Running Inference:  89%|████████▊ | 131/148 [03:33<00:50,  3.00s/it]Running Inference:  89%|████████▉ | 132/148 [03:35<00:43,  2.73s/it]Running Inference:  90%|████████▉ | 133/148 [03:37<00:35,  2.36s/it]Running Inference:  91%|█████████ | 134/148 [03:40<00:35,  2.53s/it]Running Inference:  91%|█████████ | 135/148 [03:41<00:26,  2.03s/it]Running Inference:  92%|█████████▏| 136/148 [03:41<00:20,  1.67s/it]Running Inference:  93%|█████████▎| 137/148 [03:43<00:17,  1.63s/it]Running Inference:  93%|█████████▎| 138/148 [03:44<00:15,  1.58s/it]Running Inference:  94%|█████████▍| 139/148 [03:45<00:12,  1.34s/it]Running Inference:  95%|█████████▍| 140/148 [03:46<00:09,  1.18s/it]Running Inference:  95%|█████████▌| 141/148 [03:48<00:10,  1.57s/it]Running Inference:  96%|█████████▌| 142/148 [03:49<00:07,  1.27s/it]Running Inference:  97%|█████████▋| 143/148 [03:51<00:06,  1.39s/it]Running Inference:  97%|█████████▋| 144/148 [03:57<00:11,  2.94s/it]Running Inference:  98%|█████████▊| 145/148 [03:58<00:06,  2.24s/it]Running Inference:  99%|█████████▊| 146/148 [04:00<00:04,  2.25s/it]Running Inference:  99%|█████████▉| 147/148 [04:02<00:02,  2.01s/it]Running Inference: 100%|██████████| 148/148 [04:02<00:00,  1.67s/it]Running Inference: 100%|██████████| 148/148 [04:02<00:00,  1.64s/it]
2025-12-13 20:20:35,657 - INFO - Inference completed.
2025-12-13 20:20:35,666 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 20:20:35,666 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:20:35,675 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 20:20:35,675 - INFO - Metrics:
28.91
2025-12-13 20:20:35,677 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qasper, ratio=0.1) on GPU 1

----------------------------------------
Task: qasper | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:20:42,113 - INFO - Set deterministic seeds to 42
2025-12-13 20:20:42,114 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:20:42,114 - INFO - Starting evaluation run...
2025-12-13 20:20:42,114 - INFO - Output directory set to: longbenchresult
2025-12-13 20:20:42,114 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 20:20:42,114 - INFO - KV Press 'tova' setup.
2025-12-13 20:20:42,114 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:20:42,114 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.59it/s]
Device set to use cuda:0
2025-12-13 20:21:24,447 - INFO - Model pipeline loaded.
2025-12-13 20:21:24,447 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:21:30,782 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:21:30,782 - INFO - Dataset processed with 200 entries.
2025-12-13 20:21:30,796 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:03<08:36,  3.51s/it]Running Inference:   1%|▏         | 2/148 [00:03<04:04,  1.67s/it]Running Inference:   2%|▏         | 3/148 [00:04<02:51,  1.18s/it]Running Inference:   3%|▎         | 4/148 [00:07<04:08,  1.73s/it]Running Inference:   3%|▎         | 5/148 [00:08<03:45,  1.58s/it]Running Inference:   4%|▍         | 6/148 [00:10<03:50,  1.63s/it]Running Inference:   5%|▍         | 7/148 [00:11<03:16,  1.39s/it]Running Inference:   5%|▌         | 8/148 [00:12<03:01,  1.30s/it]Running Inference:   6%|▌         | 9/148 [00:14<03:42,  1.60s/it]Running Inference:   7%|▋         | 10/148 [00:15<03:33,  1.55s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:17<03:35,  1.57s/it]Running Inference:   8%|▊         | 12/148 [00:19<03:39,  1.62s/it]Running Inference:   9%|▉         | 13/148 [00:20<03:43,  1.66s/it]Running Inference:   9%|▉         | 14/148 [00:22<03:24,  1.52s/it]Running Inference:  10%|█         | 15/148 [00:22<02:48,  1.27s/it]Running Inference:  11%|█         | 16/148 [00:31<07:40,  3.49s/it]Running Inference:  11%|█▏        | 17/148 [00:32<06:12,  2.84s/it]Running Inference:  12%|█▏        | 18/148 [00:34<05:40,  2.62s/it]Running Inference:  13%|█▎        | 19/148 [00:36<04:41,  2.18s/it]Running Inference:  14%|█▎        | 20/148 [00:37<03:57,  1.86s/it]Running Inference:  14%|█▍        | 21/148 [00:38<03:42,  1.75s/it]Running Inference:  15%|█▍        | 22/148 [00:40<03:45,  1.79s/it]Running Inference:  16%|█▌        | 23/148 [00:41<02:55,  1.40s/it]Running Inference:  16%|█▌        | 24/148 [00:42<02:48,  1.36s/it]Running Inference:  17%|█▋        | 25/148 [00:44<03:12,  1.56s/it]Running Inference:  18%|█▊        | 26/148 [00:49<05:05,  2.50s/it]Running Inference:  18%|█▊        | 27/148 [00:50<04:21,  2.16s/it]Running Inference:  19%|█▉        | 28/148 [00:51<03:32,  1.77s/it]Running Inference:  20%|█▉        | 29/148 [00:52<03:21,  1.70s/it]Running Inference:  20%|██        | 30/148 [00:53<02:40,  1.36s/it]Running Inference:  21%|██        | 31/148 [00:54<02:30,  1.29s/it]Running Inference:  22%|██▏       | 32/148 [00:56<02:45,  1.42s/it]Running Inference:  22%|██▏       | 33/148 [00:57<02:29,  1.30s/it]Running Inference:  23%|██▎       | 34/148 [00:58<02:34,  1.35s/it]Running Inference:  24%|██▎       | 35/148 [01:00<02:42,  1.44s/it]Running Inference:  24%|██▍       | 36/148 [01:07<05:55,  3.17s/it]Running Inference:  25%|██▌       | 37/148 [01:08<04:41,  2.53s/it]Running Inference:  26%|██▌       | 38/148 [01:09<03:41,  2.01s/it]Running Inference:  26%|██▋       | 39/148 [01:09<02:47,  1.53s/it]Running Inference:  27%|██▋       | 40/148 [01:10<02:14,  1.25s/it]Running Inference:  28%|██▊       | 41/148 [01:12<02:30,  1.41s/it]Running Inference:  28%|██▊       | 42/148 [01:12<02:05,  1.19s/it]Running Inference:  29%|██▉       | 43/148 [01:15<02:37,  1.50s/it]Running Inference:  30%|██▉       | 44/148 [01:17<02:51,  1.65s/it]Running Inference:  30%|███       | 45/148 [01:18<02:45,  1.60s/it]Running Inference:  31%|███       | 46/148 [01:19<02:36,  1.53s/it]Running Inference:  32%|███▏      | 47/148 [01:21<02:40,  1.59s/it]Running Inference:  32%|███▏      | 48/148 [01:22<02:14,  1.34s/it]Running Inference:  33%|███▎      | 49/148 [01:31<06:16,  3.80s/it]Running Inference:  34%|███▍      | 50/148 [01:33<04:54,  3.00s/it]Running Inference:  34%|███▍      | 51/148 [01:34<04:12,  2.60s/it]Running Inference:  35%|███▌      | 52/148 [01:35<03:28,  2.18s/it]Running Inference:  36%|███▌      | 53/148 [01:36<02:50,  1.80s/it]Running Inference:  36%|███▋      | 54/148 [01:37<02:27,  1.57s/it]Running Inference:  37%|███▋      | 55/148 [01:38<02:02,  1.31s/it]Running Inference:  38%|███▊      | 56/148 [01:39<01:57,  1.27s/it]Running Inference:  39%|███▊      | 57/148 [01:40<01:48,  1.20s/it]Running Inference:  39%|███▉      | 58/148 [01:42<01:54,  1.27s/it]Running Inference:  40%|███▉      | 59/148 [01:43<01:51,  1.26s/it]Running Inference:  41%|████      | 60/148 [01:44<01:40,  1.15s/it]Running Inference:  41%|████      | 61/148 [01:45<01:40,  1.16s/it]Running Inference:  42%|████▏     | 62/148 [01:46<01:34,  1.10s/it]Running Inference:  43%|████▎     | 63/148 [01:47<01:32,  1.09s/it]Running Inference:  43%|████▎     | 64/148 [01:50<02:13,  1.59s/it]Running Inference:  44%|████▍     | 65/148 [01:52<02:24,  1.74s/it]Running Inference:  45%|████▍     | 66/148 [01:53<01:57,  1.43s/it]Running Inference:  45%|████▌     | 67/148 [01:54<02:03,  1.53s/it]Running Inference:  46%|████▌     | 68/148 [01:55<01:42,  1.28s/it]Running Inference:  47%|████▋     | 69/148 [01:56<01:44,  1.33s/it]Running Inference:  47%|████▋     | 70/148 [01:59<01:59,  1.53s/it]Running Inference:  48%|████▊     | 71/148 [02:00<01:58,  1.54s/it]Running Inference:  49%|████▊     | 72/148 [02:02<02:02,  1.61s/it]Running Inference:  49%|████▉     | 73/148 [02:04<02:02,  1.64s/it]Running Inference:  50%|█████     | 74/148 [02:05<01:57,  1.58s/it]Running Inference:  51%|█████     | 75/148 [02:07<02:09,  1.78s/it]Running Inference:  51%|█████▏    | 76/148 [02:09<02:06,  1.76s/it]Running Inference:  52%|█████▏    | 77/148 [02:10<01:57,  1.66s/it]Running Inference:  53%|█████▎    | 78/148 [02:12<01:56,  1.66s/it]Running Inference:  53%|█████▎    | 79/148 [02:13<01:36,  1.40s/it]Running Inference:  54%|█████▍    | 80/148 [02:14<01:35,  1.41s/it]Running Inference:  55%|█████▍    | 81/148 [02:15<01:25,  1.27s/it]Running Inference:  55%|█████▌    | 82/148 [02:17<01:25,  1.29s/it]Running Inference:  56%|█████▌    | 83/148 [02:17<01:15,  1.16s/it]Running Inference:  57%|█████▋    | 84/148 [02:18<01:00,  1.06it/s]Running Inference:  57%|█████▋    | 85/148 [02:19<00:58,  1.07it/s]Running Inference:  58%|█████▊    | 86/148 [02:20<01:01,  1.01it/s]Running Inference:  59%|█████▉    | 87/148 [02:22<01:12,  1.19s/it]Running Inference:  59%|█████▉    | 88/148 [02:24<01:32,  1.55s/it]Running Inference:  60%|██████    | 89/148 [02:25<01:30,  1.54s/it]Running Inference:  61%|██████    | 90/148 [02:26<01:16,  1.32s/it]Running Inference:  61%|██████▏   | 91/148 [02:27<01:10,  1.24s/it]Running Inference:  62%|██████▏   | 92/148 [02:28<01:01,  1.09s/it]Running Inference:  63%|██████▎   | 93/148 [02:29<01:05,  1.19s/it]Running Inference:  64%|██████▎   | 94/148 [02:31<01:12,  1.33s/it]Running Inference:  64%|██████▍   | 95/148 [02:32<01:04,  1.22s/it]Running Inference:  65%|██████▍   | 96/148 [02:33<00:57,  1.11s/it]Running Inference:  66%|██████▌   | 97/148 [02:34<00:50,  1.02it/s]Running Inference:  66%|██████▌   | 98/148 [02:36<01:06,  1.34s/it]Running Inference:  67%|██████▋   | 99/148 [02:37<01:00,  1.23s/it]Running Inference:  68%|██████▊   | 100/148 [02:38<00:57,  1.19s/it]Running Inference:  68%|██████▊   | 101/148 [02:39<01:00,  1.28s/it]Running Inference:  69%|██████▉   | 102/148 [02:42<01:19,  1.74s/it]Running Inference:  70%|██████▉   | 103/148 [02:43<01:12,  1.61s/it]Running Inference:  70%|███████   | 104/148 [02:44<01:00,  1.38s/it]Running Inference:  71%|███████   | 105/148 [02:47<01:10,  1.63s/it]Running Inference:  72%|███████▏  | 106/148 [02:50<01:26,  2.07s/it]Running Inference:  72%|███████▏  | 107/148 [02:50<01:05,  1.60s/it]Running Inference:  73%|███████▎  | 108/148 [02:51<00:54,  1.35s/it]Running Inference:  74%|███████▎  | 109/148 [02:52<00:51,  1.32s/it]Running Inference:  74%|███████▍  | 110/148 [02:53<00:44,  1.16s/it]Running Inference:  75%|███████▌  | 111/148 [02:54<00:39,  1.08s/it]Running Inference:  76%|███████▌  | 112/148 [02:54<00:32,  1.10it/s]Running Inference:  76%|███████▋  | 113/148 [02:56<00:40,  1.17s/it]Running Inference:  77%|███████▋  | 114/148 [03:01<01:13,  2.16s/it]Running Inference:  78%|███████▊  | 115/148 [03:01<00:53,  1.62s/it]Running Inference:  78%|███████▊  | 116/148 [03:02<00:48,  1.53s/it]Running Inference:  79%|███████▉  | 117/148 [03:03<00:42,  1.36s/it]Running Inference:  80%|███████▉  | 118/148 [03:04<00:33,  1.11s/it]Running Inference:  80%|████████  | 119/148 [03:06<00:39,  1.37s/it]Running Inference:  81%|████████  | 120/148 [03:07<00:41,  1.48s/it]Running Inference:  82%|████████▏ | 121/148 [03:09<00:40,  1.51s/it]Running Inference:  82%|████████▏ | 122/148 [03:10<00:32,  1.24s/it]Running Inference:  83%|████████▎ | 123/148 [03:12<00:35,  1.44s/it]Running Inference:  84%|████████▍ | 124/148 [03:13<00:34,  1.45s/it]Running Inference:  84%|████████▍ | 125/148 [03:19<01:01,  2.68s/it]Running Inference:  85%|████████▌ | 126/148 [03:22<01:02,  2.86s/it]Running Inference:  86%|████████▌ | 127/148 [03:23<00:47,  2.28s/it]Running Inference:  86%|████████▋ | 128/148 [03:26<00:49,  2.50s/it]Running Inference:  87%|████████▋ | 129/148 [03:27<00:37,  1.97s/it]Running Inference:  88%|████████▊ | 130/148 [03:27<00:29,  1.62s/it]Running Inference:  89%|████████▊ | 131/148 [03:30<00:33,  1.99s/it]Running Inference:  89%|████████▉ | 132/148 [03:32<00:32,  2.03s/it]Running Inference:  90%|████████▉ | 133/148 [03:34<00:28,  1.88s/it]Running Inference:  91%|█████████ | 134/148 [03:37<00:30,  2.16s/it]Running Inference:  91%|█████████ | 135/148 [03:38<00:23,  1.78s/it]Running Inference:  92%|█████████▏| 136/148 [03:38<00:17,  1.50s/it]Running Inference:  93%|█████████▎| 137/148 [03:40<00:16,  1.51s/it]Running Inference:  93%|█████████▎| 138/148 [03:41<00:15,  1.50s/it]Running Inference:  94%|█████████▍| 139/148 [03:42<00:11,  1.29s/it]Running Inference:  95%|█████████▍| 140/148 [03:43<00:09,  1.15s/it]Running Inference:  95%|█████████▌| 141/148 [03:45<00:10,  1.49s/it]Running Inference:  96%|█████████▌| 142/148 [03:46<00:07,  1.22s/it]Running Inference:  97%|█████████▋| 143/148 [03:48<00:06,  1.37s/it]Running Inference:  97%|█████████▋| 144/148 [03:54<00:11,  2.98s/it]Running Inference:  98%|█████████▊| 145/148 [03:55<00:06,  2.27s/it]Running Inference:  99%|█████████▊| 146/148 [03:57<00:04,  2.21s/it]Running Inference:  99%|█████████▉| 147/148 [03:58<00:01,  1.98s/it]Running Inference: 100%|██████████| 148/148 [03:59<00:00,  1.66s/it]Running Inference: 100%|██████████| 148/148 [03:59<00:00,  1.62s/it]
2025-12-13 20:25:30,703 - INFO - Inference completed.
2025-12-13 20:25:30,713 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 20:25:30,713 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:25:30,722 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 20:25:30,722 - INFO - Metrics:
29.78
2025-12-13 20:25:30,723 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qasper, ratio=0.2) on GPU 1

----------------------------------------
Task: qasper | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:25:37,089 - INFO - Set deterministic seeds to 42
2025-12-13 20:25:37,089 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:25:37,089 - INFO - Starting evaluation run...
2025-12-13 20:25:37,089 - INFO - Output directory set to: longbenchresult
2025-12-13 20:25:37,089 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 20:25:37,089 - INFO - KV Press 'tova' setup.
2025-12-13 20:25:37,089 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:25:37,089 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.09it/s]
Device set to use cuda:0
2025-12-13 20:25:50,024 - INFO - Model pipeline loaded.
2025-12-13 20:25:50,024 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:25:55,812 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:25:55,812 - INFO - Dataset processed with 200 entries.
2025-12-13 20:25:55,825 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:03<08:10,  3.34s/it]Running Inference:   1%|▏         | 2/148 [00:03<03:53,  1.60s/it]Running Inference:   2%|▏         | 3/148 [00:04<02:45,  1.14s/it]Running Inference:   3%|▎         | 4/148 [00:06<03:21,  1.40s/it]Running Inference:   3%|▎         | 5/148 [00:07<03:13,  1.35s/it]Running Inference:   4%|▍         | 6/148 [00:09<03:28,  1.46s/it]Running Inference:   5%|▍         | 7/148 [00:10<03:31,  1.50s/it]Running Inference:   5%|▌         | 8/148 [00:11<03:10,  1.36s/it]Running Inference:   6%|▌         | 9/148 [00:13<03:47,  1.64s/it]Running Inference:   7%|▋         | 10/148 [00:15<03:35,  1.56s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:16<03:37,  1.59s/it]Running Inference:   8%|▊         | 12/148 [00:18<03:26,  1.52s/it]Running Inference:   9%|▉         | 13/148 [00:20<03:32,  1.58s/it]Running Inference:   9%|▉         | 14/148 [00:21<03:16,  1.47s/it]Running Inference:  10%|█         | 15/148 [00:21<02:43,  1.23s/it]Running Inference:  11%|█         | 16/148 [00:26<04:42,  2.14s/it]Running Inference:  11%|█▏        | 17/148 [00:27<04:08,  1.89s/it]Running Inference:  12%|█▏        | 18/148 [00:29<04:03,  1.87s/it]Running Inference:  13%|█▎        | 19/148 [00:30<03:32,  1.65s/it]Running Inference:  14%|█▎        | 20/148 [00:31<03:06,  1.45s/it]Running Inference:  14%|█▍        | 21/148 [00:32<03:06,  1.46s/it]Running Inference:  15%|█▍        | 22/148 [00:34<03:18,  1.57s/it]Running Inference:  16%|█▌        | 23/148 [00:35<02:35,  1.25s/it]Running Inference:  16%|█▌        | 24/148 [00:36<02:33,  1.24s/it]Running Inference:  17%|█▋        | 25/148 [00:38<03:00,  1.47s/it]Running Inference:  18%|█▊        | 26/148 [00:42<04:41,  2.31s/it]Running Inference:  18%|█▊        | 27/148 [00:44<04:04,  2.02s/it]Running Inference:  19%|█▉        | 28/148 [00:45<03:26,  1.72s/it]Running Inference:  20%|█▉        | 29/148 [00:46<03:28,  1.75s/it]Running Inference:  20%|██        | 30/148 [00:47<02:44,  1.40s/it]Running Inference:  21%|██        | 31/148 [00:48<02:33,  1.31s/it]Running Inference:  22%|██▏       | 32/148 [00:50<02:45,  1.43s/it]Running Inference:  22%|██▏       | 33/148 [00:51<02:31,  1.31s/it]Running Inference:  23%|██▎       | 34/148 [00:52<02:34,  1.35s/it]Running Inference:  24%|██▎       | 35/148 [00:54<02:30,  1.33s/it]Running Inference:  24%|██▍       | 36/148 [00:55<02:46,  1.49s/it]Running Inference:  25%|██▌       | 37/148 [00:56<02:30,  1.35s/it]Running Inference:  26%|██▌       | 38/148 [00:57<02:10,  1.18s/it]Running Inference:  26%|██▋       | 39/148 [00:58<01:43,  1.05it/s]Running Inference:  27%|██▋       | 40/148 [00:58<01:30,  1.19it/s]Running Inference:  28%|██▊       | 41/148 [01:00<01:59,  1.12s/it]Running Inference:  28%|██▊       | 42/148 [01:01<01:43,  1.02it/s]Running Inference:  29%|██▉       | 43/148 [01:02<02:04,  1.19s/it]Running Inference:  30%|██▉       | 44/148 [01:04<02:26,  1.41s/it]Running Inference:  30%|███       | 45/148 [01:06<02:26,  1.42s/it]Running Inference:  31%|███       | 46/148 [01:07<02:19,  1.37s/it]Running Inference:  32%|███▏      | 47/148 [01:09<02:36,  1.55s/it]Running Inference:  32%|███▏      | 48/148 [01:10<02:11,  1.31s/it]Running Inference:  33%|███▎      | 49/148 [01:13<03:12,  1.95s/it]Running Inference:  34%|███▍      | 50/148 [01:14<02:46,  1.70s/it]Running Inference:  34%|███▍      | 51/148 [01:16<02:43,  1.68s/it]Running Inference:  35%|███▌      | 52/148 [01:17<02:27,  1.53s/it]Running Inference:  36%|███▌      | 53/148 [01:18<02:07,  1.34s/it]Running Inference:  36%|███▋      | 54/148 [01:19<01:50,  1.17s/it]Running Inference:  37%|███▋      | 55/148 [01:19<01:36,  1.03s/it]Running Inference:  38%|███▊      | 56/148 [01:21<01:38,  1.07s/it]Running Inference:  39%|███▊      | 57/148 [01:22<01:34,  1.04s/it]Running Inference:  39%|███▉      | 58/148 [01:23<01:46,  1.19s/it]Running Inference:  40%|███▉      | 59/148 [01:24<01:46,  1.20s/it]Running Inference:  41%|████      | 60/148 [01:25<01:39,  1.13s/it]Running Inference:  41%|████      | 61/148 [01:27<01:55,  1.33s/it]Running Inference:  42%|████▏     | 62/148 [01:28<01:44,  1.22s/it]Running Inference:  43%|████▎     | 63/148 [01:30<01:48,  1.28s/it]Running Inference:  43%|████▎     | 64/148 [01:31<02:04,  1.48s/it]Running Inference:  44%|████▍     | 65/148 [01:34<02:17,  1.66s/it]Running Inference:  45%|████▍     | 66/148 [01:34<01:52,  1.37s/it]Running Inference:  45%|████▌     | 67/148 [01:36<01:59,  1.47s/it]Running Inference:  46%|████▌     | 68/148 [01:37<01:39,  1.25s/it]Running Inference:  47%|████▋     | 69/148 [01:38<01:42,  1.29s/it]Running Inference:  47%|████▋     | 70/148 [01:40<02:02,  1.58s/it]Running Inference:  48%|████▊     | 71/148 [01:47<04:05,  3.19s/it]Running Inference:  49%|████▊     | 72/148 [01:50<03:52,  3.06s/it]Running Inference:  49%|████▉     | 73/148 [01:52<03:18,  2.64s/it]Running Inference:  50%|█████     | 74/148 [01:53<02:52,  2.34s/it]Running Inference:  51%|█████     | 75/148 [01:55<02:44,  2.26s/it]Running Inference:  51%|█████▏    | 76/148 [01:58<02:40,  2.23s/it]Running Inference:  52%|█████▏    | 77/148 [01:59<02:18,  1.95s/it]Running Inference:  53%|█████▎    | 78/148 [02:01<02:10,  1.86s/it]Running Inference:  53%|█████▎    | 79/148 [02:01<01:45,  1.53s/it]Running Inference:  54%|█████▍    | 80/148 [02:03<01:50,  1.62s/it]Running Inference:  55%|█████▍    | 81/148 [02:04<01:34,  1.42s/it]Running Inference:  55%|█████▌    | 82/148 [02:05<01:31,  1.39s/it]Running Inference:  56%|█████▌    | 83/148 [02:06<01:19,  1.22s/it]Running Inference:  57%|█████▋    | 84/148 [02:07<01:03,  1.01it/s]Running Inference:  57%|█████▋    | 85/148 [02:08<01:00,  1.04it/s]Running Inference:  58%|█████▊    | 86/148 [02:09<01:02,  1.01s/it]Running Inference:  59%|█████▉    | 87/148 [02:10<01:11,  1.18s/it]Running Inference:  59%|█████▉    | 88/148 [02:13<01:31,  1.53s/it]Running Inference:  60%|██████    | 89/148 [02:14<01:31,  1.56s/it]Running Inference:  61%|██████    | 90/148 [02:15<01:17,  1.33s/it]Running Inference:  61%|██████▏   | 91/148 [02:16<01:13,  1.29s/it]Running Inference:  62%|██████▏   | 92/148 [02:17<01:03,  1.13s/it]Running Inference:  63%|██████▎   | 93/148 [02:18<01:04,  1.17s/it]Running Inference:  64%|██████▎   | 94/148 [02:20<01:10,  1.31s/it]Running Inference:  64%|██████▍   | 95/148 [02:21<01:03,  1.20s/it]Running Inference:  65%|██████▍   | 96/148 [02:22<00:56,  1.09s/it]Running Inference:  66%|██████▌   | 97/148 [02:22<00:49,  1.03it/s]Running Inference:  66%|██████▌   | 98/148 [02:25<01:09,  1.40s/it]Running Inference:  67%|██████▋   | 99/148 [02:31<02:24,  2.94s/it]Running Inference:  68%|██████▊   | 100/148 [02:32<01:55,  2.41s/it]Running Inference:  68%|██████▊   | 101/148 [02:34<01:39,  2.11s/it]Running Inference:  69%|██████▉   | 102/148 [02:36<01:40,  2.19s/it]Running Inference:  70%|██████▉   | 103/148 [02:38<01:28,  1.97s/it]Running Inference:  70%|███████   | 104/148 [02:39<01:11,  1.63s/it]Running Inference:  71%|███████   | 105/148 [02:41<01:20,  1.86s/it]Running Inference:  72%|███████▏  | 106/148 [02:44<01:33,  2.22s/it]Running Inference:  72%|███████▏  | 107/148 [02:45<01:10,  1.71s/it]Running Inference:  73%|███████▎  | 108/148 [02:45<00:57,  1.43s/it]Running Inference:  74%|███████▎  | 109/148 [02:47<00:54,  1.39s/it]Running Inference:  74%|███████▍  | 110/148 [02:47<00:45,  1.21s/it]Running Inference:  75%|███████▌  | 111/148 [02:48<00:41,  1.11s/it]Running Inference:  76%|███████▌  | 112/148 [02:49<00:33,  1.07it/s]Running Inference:  76%|███████▋  | 113/148 [02:51<00:41,  1.19s/it]Running Inference:  77%|███████▋  | 114/148 [02:55<01:13,  2.17s/it]Running Inference:  78%|███████▊  | 115/148 [02:55<00:53,  1.62s/it]Running Inference:  78%|███████▊  | 116/148 [02:57<00:48,  1.52s/it]Running Inference:  79%|███████▉  | 117/148 [02:58<00:44,  1.43s/it]Running Inference:  80%|███████▉  | 118/148 [02:58<00:34,  1.16s/it]Running Inference:  80%|████████  | 119/148 [03:00<00:40,  1.40s/it]Running Inference:  81%|████████  | 120/148 [03:02<00:42,  1.51s/it]Running Inference:  82%|████████▏ | 121/148 [03:04<00:41,  1.54s/it]Running Inference:  82%|████████▏ | 122/148 [03:04<00:32,  1.26s/it]Running Inference:  83%|████████▎ | 123/148 [03:06<00:36,  1.44s/it]Running Inference:  84%|████████▍ | 124/148 [03:08<00:34,  1.44s/it]Running Inference:  84%|████████▍ | 125/148 [03:15<01:13,  3.20s/it]Running Inference:  85%|████████▌ | 126/148 [03:18<01:09,  3.15s/it]Running Inference:  86%|████████▌ | 127/148 [03:19<00:52,  2.48s/it]Running Inference:  86%|████████▋ | 128/148 [03:20<00:39,  1.98s/it]Running Inference:  87%|████████▋ | 129/148 [03:20<00:30,  1.60s/it]Running Inference:  88%|████████▊ | 130/148 [03:21<00:24,  1.35s/it]Running Inference:  89%|████████▊ | 131/148 [03:24<00:30,  1.81s/it]Running Inference:  89%|████████▉ | 132/148 [03:26<00:28,  1.75s/it]Running Inference:  90%|████████▉ | 133/148 [03:27<00:25,  1.68s/it]Running Inference:  91%|█████████ | 134/148 [03:30<00:27,  2.00s/it]Running Inference:  91%|█████████ | 135/148 [03:31<00:21,  1.66s/it]Running Inference:  92%|█████████▏| 136/148 [03:32<00:16,  1.41s/it]Running Inference:  93%|█████████▎| 137/148 [03:33<00:15,  1.44s/it]Running Inference:  93%|█████████▎| 138/148 [03:35<00:14,  1.47s/it]Running Inference:  94%|█████████▍| 139/148 [03:35<00:11,  1.26s/it]Running Inference:  95%|█████████▍| 140/148 [03:36<00:08,  1.12s/it]Running Inference:  95%|█████████▌| 141/148 [03:40<00:12,  1.77s/it]Running Inference:  96%|█████████▌| 142/148 [03:40<00:08,  1.41s/it]Running Inference:  97%|█████████▋| 143/148 [03:42<00:07,  1.50s/it]Running Inference:  97%|█████████▋| 144/148 [03:48<00:12,  3.03s/it]Running Inference:  98%|█████████▊| 145/148 [03:49<00:06,  2.30s/it]Running Inference:  99%|█████████▊| 146/148 [03:51<00:04,  2.24s/it]Running Inference:  99%|█████████▉| 147/148 [03:52<00:01,  1.95s/it]Running Inference: 100%|██████████| 148/148 [03:53<00:00,  1.65s/it]Running Inference: 100%|██████████| 148/148 [03:53<00:00,  1.58s/it]
2025-12-13 20:29:49,657 - INFO - Inference completed.
2025-12-13 20:29:49,666 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-13 20:29:49,666 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:29:49,675 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-13 20:29:49,675 - INFO - Metrics:
28.85
2025-12-13 20:29:49,676 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qasper, ratio=0.3) on GPU 1

----------------------------------------
Task: qasper | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:29:56,039 - INFO - Set deterministic seeds to 42
2025-12-13 20:29:56,039 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:29:56,039 - INFO - Starting evaluation run...
2025-12-13 20:29:56,039 - INFO - Output directory set to: longbenchresult
2025-12-13 20:29:56,039 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-13 20:29:56,039 - INFO - KV Press 'tova' setup.
2025-12-13 20:29:56,039 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:29:56,039 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.67it/s]
Device set to use cuda:0
2025-12-13 20:30:09,056 - INFO - Model pipeline loaded.
2025-12-13 20:30:09,056 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:30:13,026 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:30:13,026 - INFO - Dataset processed with 200 entries.
2025-12-13 20:30:13,040 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:04<10:00,  4.08s/it]Running Inference:   1%|▏         | 2/148 [00:04<04:38,  1.91s/it]Running Inference:   2%|▏         | 3/148 [00:05<03:09,  1.31s/it]Running Inference:   3%|▎         | 4/148 [00:06<03:15,  1.36s/it]Running Inference:   3%|▎         | 5/148 [00:07<03:09,  1.33s/it]Running Inference:   4%|▍         | 6/148 [00:09<03:31,  1.49s/it]Running Inference:   5%|▍         | 7/148 [00:10<03:04,  1.31s/it]Running Inference:   5%|▌         | 8/148 [00:11<03:05,  1.33s/it]Running Inference:   6%|▌         | 9/148 [00:13<03:18,  1.43s/it]Running Inference:   7%|▋         | 10/148 [00:14<03:15,  1.42s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:15<02:55,  1.28s/it]Running Inference:   8%|▊         | 12/148 [00:17<02:52,  1.27s/it]Running Inference:   9%|▉         | 13/148 [00:19<03:16,  1.45s/it]Running Inference:   9%|▉         | 14/148 [00:20<03:05,  1.38s/it]Running Inference:  10%|█         | 15/148 [00:20<02:35,  1.17s/it]Running Inference:  11%|█         | 16/148 [00:29<07:23,  3.36s/it]Running Inference:  11%|█▏        | 17/148 [00:30<06:05,  2.79s/it]Running Inference:  12%|█▏        | 18/148 [00:32<05:24,  2.50s/it]Running Inference:  13%|█▎        | 19/148 [00:33<04:26,  2.06s/it]Running Inference:  14%|█▎        | 20/148 [00:34<03:34,  1.68s/it]Running Inference:  14%|█▍        | 21/148 [00:35<03:22,  1.60s/it]Running Inference:  15%|█▍        | 22/148 [00:37<03:04,  1.47s/it]Running Inference:  16%|█▌        | 23/148 [00:37<02:26,  1.17s/it]Running Inference:  16%|█▌        | 24/148 [00:38<02:23,  1.16s/it]Running Inference:  17%|█▋        | 25/148 [00:40<02:53,  1.41s/it]Running Inference:  18%|█▊        | 26/148 [00:45<04:50,  2.38s/it]Running Inference:  18%|█▊        | 27/148 [00:46<04:05,  2.03s/it]Running Inference:  19%|█▉        | 28/148 [00:47<03:23,  1.70s/it]Running Inference:  20%|█▉        | 29/148 [00:55<06:53,  3.47s/it]Running Inference:  20%|██        | 30/148 [00:55<05:06,  2.60s/it]Running Inference:  21%|██        | 31/148 [00:56<04:11,  2.15s/it]Running Inference:  22%|██▏       | 32/148 [00:58<03:49,  1.98s/it]Running Inference:  22%|██▏       | 33/148 [00:59<03:15,  1.70s/it]Running Inference:  23%|██▎       | 34/148 [01:00<03:05,  1.62s/it]Running Inference:  24%|██▎       | 35/148 [01:02<02:51,  1.51s/it]Running Inference:  24%|██▍       | 36/148 [01:04<03:15,  1.75s/it]Running Inference:  25%|██▌       | 37/148 [01:05<02:49,  1.53s/it]Running Inference:  26%|██▌       | 38/148 [01:06<02:21,  1.28s/it]Running Inference:  26%|██▋       | 39/148 [01:06<01:51,  1.02s/it]Running Inference:  27%|██▋       | 40/148 [01:07<01:36,  1.12it/s]Running Inference:  28%|██▊       | 41/148 [01:13<04:48,  2.69s/it]Running Inference:  28%|██▊       | 42/148 [01:14<03:39,  2.07s/it]Running Inference:  29%|██▉       | 43/148 [01:16<03:28,  1.99s/it]Running Inference:  30%|██▉       | 44/148 [01:18<03:23,  1.96s/it]Running Inference:  30%|███       | 45/148 [01:19<03:08,  1.83s/it]Running Inference:  31%|███       | 46/148 [01:20<02:33,  1.51s/it]Running Inference:  32%|███▏      | 47/148 [01:22<02:44,  1.63s/it]Running Inference:  32%|███▏      | 48/148 [01:23<02:17,  1.37s/it]Running Inference:  33%|███▎      | 49/148 [01:26<03:03,  1.85s/it]Running Inference:  34%|███▍      | 50/148 [01:27<02:43,  1.67s/it]Running Inference:  34%|███▍      | 51/148 [01:29<02:40,  1.66s/it]Running Inference:  35%|███▌      | 52/148 [01:30<02:25,  1.51s/it]Running Inference:  36%|███▌      | 53/148 [01:31<02:05,  1.32s/it]Running Inference:  36%|███▋      | 54/148 [01:31<01:50,  1.18s/it]Running Inference:  37%|███▋      | 55/148 [01:32<01:36,  1.04s/it]Running Inference:  38%|███▊      | 56/148 [01:34<01:47,  1.17s/it]Running Inference:  39%|███▊      | 57/148 [01:34<01:33,  1.03s/it]Running Inference:  39%|███▉      | 58/148 [01:36<01:42,  1.14s/it]Running Inference:  40%|███▉      | 59/148 [01:37<01:43,  1.16s/it]Running Inference:  41%|████      | 60/148 [01:38<01:34,  1.08s/it]Running Inference:  41%|████      | 61/148 [01:39<01:39,  1.14s/it]Running Inference:  42%|████▏     | 62/148 [01:40<01:34,  1.09s/it]Running Inference:  43%|████▎     | 63/148 [01:41<01:33,  1.10s/it]Running Inference:  43%|████▎     | 64/148 [01:43<01:46,  1.27s/it]Running Inference:  44%|████▍     | 65/148 [01:45<02:05,  1.51s/it]Running Inference:  45%|████▍     | 66/148 [01:46<01:43,  1.27s/it]Running Inference:  45%|████▌     | 67/148 [01:47<01:53,  1.40s/it]Running Inference:  46%|████▌     | 68/148 [01:48<01:35,  1.19s/it]Running Inference:  47%|████▋     | 69/148 [01:50<01:41,  1.28s/it]Running Inference:  47%|████▋     | 70/148 [01:51<01:45,  1.35s/it]Running Inference:  48%|████▊     | 71/148 [01:58<03:53,  3.03s/it]Running Inference:  49%|████▊     | 72/148 [02:01<03:40,  2.91s/it]Running Inference:  49%|████▉     | 73/148 [02:02<03:12,  2.56s/it]Running Inference:  50%|█████     | 74/148 [02:04<02:48,  2.27s/it]Running Inference:  51%|█████     | 75/148 [02:06<02:36,  2.15s/it]Running Inference:  51%|█████▏    | 76/148 [02:08<02:26,  2.03s/it]Running Inference:  52%|█████▏    | 77/148 [02:09<02:16,  1.93s/it]Running Inference:  53%|█████▎    | 78/148 [02:11<02:09,  1.84s/it]Running Inference:  53%|█████▎    | 79/148 [02:12<01:44,  1.51s/it]Running Inference:  54%|█████▍    | 80/148 [02:13<01:43,  1.53s/it]Running Inference:  55%|█████▍    | 81/148 [02:14<01:34,  1.41s/it]Running Inference:  55%|█████▌    | 82/148 [02:16<01:29,  1.35s/it]Running Inference:  56%|█████▌    | 83/148 [02:16<01:17,  1.19s/it]Running Inference:  57%|█████▋    | 84/148 [02:17<01:01,  1.04it/s]Running Inference:  57%|█████▋    | 85/148 [02:18<01:00,  1.04it/s]Running Inference:  58%|█████▊    | 86/148 [02:19<01:02,  1.00s/it]Running Inference:  59%|█████▉    | 87/148 [02:21<01:13,  1.20s/it]Running Inference:  59%|█████▉    | 88/148 [02:23<01:27,  1.46s/it]Running Inference:  60%|██████    | 89/148 [02:24<01:28,  1.51s/it]Running Inference:  61%|██████    | 90/148 [02:25<01:15,  1.30s/it]Running Inference:  61%|██████▏   | 91/148 [02:26<01:14,  1.31s/it]Running Inference:  62%|██████▏   | 92/148 [02:27<01:03,  1.13s/it]Running Inference:  63%|██████▎   | 93/148 [02:28<01:02,  1.14s/it]Running Inference:  64%|██████▎   | 94/148 [02:30<01:08,  1.27s/it]Running Inference:  64%|██████▍   | 95/148 [02:31<01:04,  1.22s/it]Running Inference:  65%|██████▍   | 96/148 [02:32<00:57,  1.10s/it]Running Inference:  66%|██████▌   | 97/148 [02:33<00:49,  1.02it/s]Running Inference:  66%|██████▌   | 98/148 [02:35<01:07,  1.35s/it]Running Inference:  67%|██████▋   | 99/148 [02:36<01:11,  1.46s/it]Running Inference:  68%|██████▊   | 100/148 [02:38<01:05,  1.37s/it]Running Inference:  68%|██████▊   | 101/148 [02:39<01:02,  1.32s/it]Running Inference:  69%|██████▉   | 102/148 [02:41<01:16,  1.66s/it]Running Inference:  70%|██████▉   | 103/148 [02:43<01:10,  1.58s/it]Running Inference:  70%|███████   | 104/148 [02:44<01:11,  1.62s/it]Running Inference:  71%|███████   | 105/148 [02:46<01:13,  1.71s/it]Running Inference:  72%|███████▏  | 106/148 [02:49<01:29,  2.12s/it]Running Inference:  72%|███████▏  | 107/148 [02:50<01:07,  1.64s/it]Running Inference:  73%|███████▎  | 108/148 [02:51<00:54,  1.37s/it]Running Inference:  74%|███████▎  | 109/148 [02:52<00:51,  1.33s/it]Running Inference:  74%|███████▍  | 110/148 [02:53<00:44,  1.17s/it]Running Inference:  75%|███████▌  | 111/148 [02:54<00:39,  1.08s/it]Running Inference:  76%|███████▌  | 112/148 [02:54<00:32,  1.10it/s]Running Inference:  76%|███████▋  | 113/148 [02:55<00:36,  1.06s/it]Running Inference:  77%|███████▋  | 114/148 [03:00<01:10,  2.06s/it]Running Inference:  78%|███████▊  | 115/148 [03:00<00:50,  1.54s/it]Running Inference:  78%|███████▊  | 116/148 [03:01<00:45,  1.43s/it]Running Inference:  79%|███████▉  | 117/148 [03:02<00:41,  1.34s/it]Running Inference:  80%|███████▉  | 118/148 [03:03<00:32,  1.09s/it]Running Inference:  80%|████████  | 119/148 [03:05<00:38,  1.34s/it]Running Inference:  81%|████████  | 120/148 [03:06<00:38,  1.39s/it]Running Inference:  82%|████████▏ | 121/148 [03:08<00:39,  1.45s/it]Running Inference:  82%|████████▏ | 122/148 [03:09<00:31,  1.22s/it]Running Inference:  83%|████████▎ | 123/148 [03:11<00:35,  1.41s/it]Running Inference:  84%|████████▍ | 124/148 [03:12<00:33,  1.42s/it]Running Inference:  84%|████████▍ | 125/148 [03:19<01:09,  3.02s/it]Running Inference:  85%|████████▌ | 126/148 [03:21<00:59,  2.71s/it]Running Inference:  86%|████████▌ | 127/148 [03:22<00:45,  2.18s/it]Running Inference:  86%|████████▋ | 128/148 [03:22<00:35,  1.76s/it]Running Inference:  87%|████████▋ | 129/148 [03:23<00:26,  1.37s/it]Running Inference:  88%|████████▊ | 130/148 [03:29<00:52,  2.90s/it]Running Inference:  89%|████████▊ | 131/148 [03:32<00:49,  2.89s/it]Running Inference:  89%|████████▉ | 132/148 [03:34<00:40,  2.54s/it]Running Inference:  90%|████████▉ | 133/148 [03:35<00:33,  2.22s/it]Running Inference:  91%|█████████ | 134/148 [03:38<00:32,  2.33s/it]Running Inference:  91%|█████████ | 135/148 [03:39<00:24,  1.88s/it]Running Inference:  92%|█████████▏| 136/148 [03:40<00:18,  1.56s/it]Running Inference:  93%|█████████▎| 137/148 [03:42<00:18,  1.70s/it]Running Inference:  93%|█████████▎| 138/148 [03:43<00:16,  1.64s/it]Running Inference:  94%|█████████▍| 139/148 [03:44<00:12,  1.38s/it]Running Inference:  95%|█████████▍| 140/148 [03:45<00:09,  1.22s/it]Running Inference:  95%|█████████▌| 141/148 [03:47<00:10,  1.52s/it]Running Inference:  96%|█████████▌| 142/148 [03:48<00:07,  1.31s/it]Running Inference:  97%|█████████▋| 143/148 [03:50<00:07,  1.54s/it]Running Inference:  97%|█████████▋| 144/148 [03:52<00:06,  1.65s/it]Running Inference:  98%|█████████▊| 145/148 [03:52<00:04,  1.33s/it]Running Inference:  99%|█████████▊| 146/148 [03:55<00:03,  1.57s/it]Running Inference:  99%|█████████▉| 147/148 [04:01<00:03,  3.11s/it]Running Inference: 100%|██████████| 148/148 [04:02<00:00,  2.43s/it]Running Inference: 100%|██████████| 148/148 [04:02<00:00,  1.64s/it]
2025-12-13 20:34:15,670 - INFO - Inference completed.
2025-12-13 20:34:15,679 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-13 20:34:15,679 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:34:15,688 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-13 20:34:15,688 - INFO - Metrics:
27.37
2025-12-13 20:34:15,690 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qasper, ratio=0.5) on GPU 1


========================================
LongBench Task: triviaqa
========================================
----------------------------------------
Task: triviaqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:34:22,037 - INFO - Set deterministic seeds to 42
2025-12-13 20:34:22,037 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:34:22,037 - INFO - Starting evaluation run...
2025-12-13 20:34:22,037 - INFO - Output directory set to: longbenchresult
2025-12-13 20:34:22,038 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 20:34:22,038 - INFO - KV Press 'tova' setup.
2025-12-13 20:34:22,038 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:34:22,038 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.47it/s]
Device set to use cuda:0
2025-12-13 20:34:35,738 - INFO - Model pipeline loaded.
2025-12-13 20:34:35,738 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 1449.80 examples/s]Generating test split: 200 examples [00:00, 1444.25 examples/s]
2025-12-13 20:34:44,025 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:34:44,026 - INFO - Dataset processed with 200 entries.
2025-12-13 20:34:44,055 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<10:07,  3.05s/it]Running Inference:   1%|          | 2/200 [00:04<07:37,  2.31s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:43,  1.74s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:41,  2.05s/it]Running Inference:   2%|▎         | 5/200 [00:10<07:09,  2.20s/it]Running Inference:   3%|▎         | 6/200 [00:11<05:36,  1.73s/it]Running Inference:   4%|▎         | 7/200 [00:12<04:06,  1.28s/it]Running Inference:   4%|▍         | 8/200 [00:12<03:17,  1.03s/it]Running Inference:   4%|▍         | 9/200 [00:13<03:36,  1.13s/it]Running Inference:   5%|▌         | 10/200 [00:15<03:59,  1.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:16<03:51,  1.23s/it]Running Inference:   6%|▌         | 12/200 [00:19<05:35,  1.79s/it]Running Inference:   6%|▋         | 13/200 [00:20<05:00,  1.61s/it]Running Inference:   7%|▋         | 14/200 [00:24<06:48,  2.20s/it]Running Inference:   8%|▊         | 15/200 [00:26<06:59,  2.27s/it]Running Inference:   8%|▊         | 16/200 [00:29<07:38,  2.49s/it]Running Inference:   8%|▊         | 17/200 [00:32<07:39,  2.51s/it]Running Inference:   9%|▉         | 18/200 [00:33<05:58,  1.97s/it]Running Inference:  10%|▉         | 19/200 [00:37<08:13,  2.73s/it]Running Inference:  10%|█         | 20/200 [00:39<07:05,  2.36s/it]Running Inference:  10%|█         | 21/200 [00:39<05:24,  1.81s/it]Running Inference:  11%|█         | 22/200 [00:42<06:38,  2.24s/it]Running Inference:  12%|█▏        | 23/200 [00:44<05:44,  1.95s/it]Running Inference:  12%|█▏        | 24/200 [00:46<05:46,  1.97s/it]Running Inference:  12%|█▎        | 25/200 [00:47<05:29,  1.88s/it]Running Inference:  13%|█▎        | 26/200 [00:49<05:01,  1.73s/it]Running Inference:  14%|█▎        | 27/200 [00:50<04:09,  1.44s/it]Running Inference:  14%|█▍        | 28/200 [00:50<03:29,  1.22s/it]Running Inference:  14%|█▍        | 29/200 [00:54<05:50,  2.05s/it]Running Inference:  15%|█▌        | 30/200 [00:55<04:27,  1.57s/it]Running Inference:  16%|█▌        | 31/200 [00:56<04:21,  1.55s/it]Running Inference:  16%|█▌        | 32/200 [00:58<04:47,  1.71s/it]Running Inference:  16%|█▋        | 33/200 [00:59<04:12,  1.51s/it]Running Inference:  17%|█▋        | 34/200 [01:01<04:07,  1.49s/it]Running Inference:  18%|█▊        | 35/200 [01:02<03:55,  1.42s/it]Running Inference:  18%|█▊        | 36/200 [01:06<06:13,  2.28s/it]Running Inference:  18%|█▊        | 37/200 [01:07<05:18,  1.95s/it]Running Inference:  19%|█▉        | 38/200 [01:12<07:36,  2.82s/it]Running Inference:  20%|█▉        | 39/200 [01:13<05:39,  2.11s/it]Running Inference:  20%|██        | 40/200 [01:16<06:32,  2.45s/it]Running Inference:  20%|██        | 41/200 [01:18<05:43,  2.16s/it]Running Inference:  21%|██        | 42/200 [01:20<06:04,  2.31s/it]Running Inference:  22%|██▏       | 43/200 [01:21<04:34,  1.75s/it]Running Inference:  22%|██▏       | 44/200 [01:22<04:34,  1.76s/it]Running Inference:  22%|██▎       | 45/200 [01:23<03:31,  1.36s/it]Running Inference:  23%|██▎       | 46/200 [01:24<03:15,  1.27s/it]Running Inference:  24%|██▎       | 47/200 [01:25<03:05,  1.21s/it]Running Inference:  24%|██▍       | 48/200 [01:30<06:11,  2.44s/it]Running Inference:  24%|██▍       | 49/200 [01:32<05:55,  2.36s/it]Running Inference:  25%|██▌       | 50/200 [01:35<06:13,  2.49s/it]Running Inference:  26%|██▌       | 51/200 [01:36<04:51,  1.96s/it]Running Inference:  26%|██▌       | 52/200 [01:38<05:02,  2.04s/it]Running Inference:  26%|██▋       | 53/200 [01:39<03:50,  1.57s/it]Running Inference:  27%|██▋       | 54/200 [01:39<03:04,  1.26s/it]Running Inference:  28%|██▊       | 55/200 [01:40<02:58,  1.23s/it]Running Inference:  28%|██▊       | 56/200 [01:42<03:14,  1.35s/it]Running Inference:  28%|██▊       | 57/200 [01:45<04:08,  1.74s/it]Running Inference:  29%|██▉       | 58/200 [01:46<04:08,  1.75s/it]Running Inference:  30%|██▉       | 59/200 [01:51<05:57,  2.54s/it]Running Inference:  30%|███       | 60/200 [01:52<05:15,  2.25s/it]Running Inference:  30%|███       | 61/200 [01:57<07:01,  3.03s/it]Running Inference:  31%|███       | 62/200 [01:59<06:02,  2.63s/it]Running Inference:  32%|███▏      | 63/200 [01:59<04:31,  1.98s/it]Running Inference:  32%|███▏      | 64/200 [02:00<03:45,  1.66s/it]Running Inference:  32%|███▎      | 65/200 [02:03<04:13,  1.87s/it]Running Inference:  33%|███▎      | 66/200 [02:05<04:27,  2.00s/it]Running Inference:  34%|███▎      | 67/200 [02:07<04:24,  1.99s/it]Running Inference:  34%|███▍      | 68/200 [02:08<03:42,  1.68s/it]Running Inference:  34%|███▍      | 69/200 [02:10<03:50,  1.76s/it]Running Inference:  35%|███▌      | 70/200 [02:14<05:27,  2.52s/it]Running Inference:  36%|███▌      | 71/200 [02:17<05:40,  2.64s/it]Running Inference:  36%|███▌      | 72/200 [02:21<06:44,  3.16s/it]Running Inference:  36%|███▋      | 73/200 [02:23<05:36,  2.65s/it]Running Inference:  37%|███▋      | 74/200 [02:26<05:38,  2.68s/it]Running Inference:  38%|███▊      | 75/200 [02:26<04:10,  2.01s/it]Running Inference:  38%|███▊      | 76/200 [02:30<05:20,  2.58s/it]Running Inference:  38%|███▊      | 77/200 [02:33<05:18,  2.59s/it]Running Inference:  39%|███▉      | 78/200 [02:33<03:53,  1.91s/it]Running Inference:  40%|███▉      | 79/200 [02:35<03:56,  1.95s/it]Running Inference:  40%|████      | 80/200 [02:37<04:02,  2.02s/it]Running Inference:  40%|████      | 81/200 [02:40<04:31,  2.28s/it]Running Inference:  41%|████      | 82/200 [02:41<03:25,  1.74s/it]Running Inference:  42%|████▏     | 83/200 [02:42<03:15,  1.67s/it]Running Inference:  42%|████▏     | 84/200 [02:45<04:01,  2.09s/it]Running Inference:  42%|████▎     | 85/200 [02:47<03:40,  1.92s/it]Running Inference:  43%|████▎     | 86/200 [02:52<05:42,  3.01s/it]Running Inference:  44%|████▎     | 87/200 [02:55<05:23,  2.87s/it]Running Inference:  44%|████▍     | 88/200 [02:57<04:46,  2.56s/it]Running Inference:  44%|████▍     | 89/200 [02:58<03:57,  2.14s/it]Running Inference:  45%|████▌     | 90/200 [02:59<03:43,  2.03s/it]Running Inference:  46%|████▌     | 91/200 [03:03<04:27,  2.45s/it]Running Inference:  46%|████▌     | 92/200 [03:04<03:28,  1.93s/it]Running Inference:  46%|████▋     | 93/200 [03:07<04:00,  2.25s/it]Running Inference:  47%|████▋     | 94/200 [03:08<03:37,  2.05s/it]Running Inference:  48%|████▊     | 95/200 [03:11<04:03,  2.32s/it]Running Inference:  48%|████▊     | 96/200 [03:13<04:02,  2.33s/it]Running Inference:  48%|████▊     | 97/200 [03:17<04:32,  2.65s/it]Running Inference:  49%|████▉     | 98/200 [03:19<04:23,  2.58s/it]Running Inference:  50%|████▉     | 99/200 [03:23<04:44,  2.82s/it]Running Inference:  50%|█████     | 100/200 [03:26<04:54,  2.94s/it]Running Inference:  50%|█████     | 101/200 [03:28<04:33,  2.76s/it]Running Inference:  51%|█████     | 102/200 [03:31<04:18,  2.64s/it]Running Inference:  52%|█████▏    | 103/200 [03:33<03:59,  2.47s/it]Running Inference:  52%|█████▏    | 104/200 [03:34<03:31,  2.21s/it]Running Inference:  52%|█████▎    | 105/200 [03:37<03:35,  2.26s/it]Running Inference:  53%|█████▎    | 106/200 [03:40<03:56,  2.52s/it]Running Inference:  54%|█████▎    | 107/200 [03:44<04:29,  2.89s/it]Running Inference:  54%|█████▍    | 108/200 [03:45<03:38,  2.38s/it]Running Inference:  55%|█████▍    | 109/200 [03:46<02:57,  1.95s/it]Running Inference:  55%|█████▌    | 110/200 [03:46<02:17,  1.53s/it]Running Inference:  56%|█████▌    | 111/200 [03:50<03:05,  2.08s/it]Running Inference:  56%|█████▌    | 112/200 [03:51<02:51,  1.95s/it]Running Inference:  56%|█████▋    | 113/200 [03:53<02:56,  2.02s/it]Running Inference:  57%|█████▋    | 114/200 [03:58<04:05,  2.85s/it]Running Inference:  57%|█████▊    | 115/200 [04:00<03:35,  2.54s/it]Running Inference:  58%|█████▊    | 116/200 [04:00<02:39,  1.90s/it]Running Inference:  58%|█████▊    | 117/200 [04:04<03:18,  2.40s/it]Running Inference:  59%|█████▉    | 118/200 [04:05<02:38,  1.93s/it]Running Inference:  60%|█████▉    | 119/200 [04:06<02:25,  1.80s/it]Running Inference:  60%|██████    | 120/200 [04:08<02:24,  1.81s/it]Running Inference:  60%|██████    | 121/200 [04:09<01:48,  1.37s/it]Running Inference:  61%|██████    | 122/200 [04:10<01:48,  1.39s/it]Running Inference:  62%|██████▏   | 123/200 [04:10<01:25,  1.11s/it]Running Inference:  62%|██████▏   | 124/200 [04:11<01:16,  1.00s/it]Running Inference:  62%|██████▎   | 125/200 [04:13<01:43,  1.38s/it]Running Inference:  63%|██████▎   | 126/200 [04:16<02:11,  1.77s/it]Running Inference:  64%|██████▎   | 127/200 [04:20<03:04,  2.53s/it]Running Inference:  64%|██████▍   | 128/200 [04:21<02:23,  1.99s/it]Running Inference:  64%|██████▍   | 129/200 [04:22<02:01,  1.71s/it]Running Inference:  65%|██████▌   | 130/200 [04:23<01:39,  1.42s/it]Running Inference:  66%|██████▌   | 131/200 [04:24<01:39,  1.44s/it]Running Inference:  66%|██████▌   | 132/200 [04:28<02:23,  2.11s/it]Running Inference:  66%|██████▋   | 133/200 [04:30<02:14,  2.01s/it]Running Inference:  67%|██████▋   | 134/200 [04:32<02:11,  1.99s/it]Running Inference:  68%|██████▊   | 135/200 [04:33<01:56,  1.79s/it]Running Inference:  68%|██████▊   | 136/200 [04:36<02:19,  2.18s/it]Running Inference:  68%|██████▊   | 137/200 [04:38<02:05,  2.00s/it]Running Inference:  69%|██████▉   | 138/200 [04:41<02:23,  2.31s/it]Running Inference:  70%|██████▉   | 139/200 [04:44<02:27,  2.42s/it]Running Inference:  70%|███████   | 140/200 [04:46<02:19,  2.33s/it]Running Inference:  70%|███████   | 141/200 [04:48<02:21,  2.39s/it]Running Inference:  71%|███████   | 142/200 [04:51<02:26,  2.52s/it]Running Inference:  72%|███████▏  | 143/200 [04:52<01:57,  2.06s/it]Running Inference:  72%|███████▏  | 144/200 [04:53<01:46,  1.89s/it]Running Inference:  72%|███████▎  | 145/200 [04:55<01:35,  1.74s/it]Running Inference:  73%|███████▎  | 146/200 [04:57<01:47,  1.99s/it]Running Inference:  74%|███████▎  | 147/200 [04:58<01:22,  1.56s/it]Running Inference:  74%|███████▍  | 148/200 [05:02<01:55,  2.21s/it]Running Inference:  74%|███████▍  | 149/200 [05:06<02:19,  2.73s/it]Running Inference:  75%|███████▌  | 150/200 [05:07<01:49,  2.19s/it]Running Inference:  76%|███████▌  | 151/200 [05:08<01:40,  2.06s/it]Running Inference:  76%|███████▌  | 152/200 [05:09<01:23,  1.74s/it]Running Inference:  76%|███████▋  | 153/200 [05:12<01:34,  2.00s/it]Running Inference:  77%|███████▋  | 154/200 [05:16<02:03,  2.69s/it]Running Inference:  78%|███████▊  | 155/200 [05:18<01:50,  2.45s/it]Running Inference:  78%|███████▊  | 156/200 [05:21<01:47,  2.44s/it]Running Inference:  78%|███████▊  | 157/200 [05:25<02:12,  3.09s/it]Running Inference:  79%|███████▉  | 158/200 [05:27<01:54,  2.72s/it]Running Inference:  80%|███████▉  | 159/200 [05:28<01:35,  2.32s/it]Running Inference:  80%|████████  | 160/200 [05:29<01:17,  1.95s/it]Running Inference:  80%|████████  | 161/200 [05:31<01:14,  1.91s/it]Running Inference:  81%|████████  | 162/200 [05:33<01:11,  1.87s/it]Running Inference:  82%|████████▏ | 163/200 [05:35<01:12,  1.96s/it]Running Inference:  82%|████████▏ | 164/200 [05:36<00:56,  1.56s/it]Running Inference:  82%|████████▎ | 165/200 [05:40<01:18,  2.25s/it]Running Inference:  83%|████████▎ | 166/200 [05:42<01:13,  2.16s/it]Running Inference:  84%|████████▎ | 167/200 [05:43<01:02,  1.90s/it]Running Inference:  84%|████████▍ | 168/200 [05:46<01:14,  2.32s/it]Running Inference:  84%|████████▍ | 169/200 [05:47<00:59,  1.92s/it]Running Inference:  85%|████████▌ | 170/200 [05:48<00:46,  1.55s/it]Running Inference:  86%|████████▌ | 171/200 [05:49<00:39,  1.35s/it]Running Inference:  86%|████████▌ | 172/200 [05:51<00:41,  1.49s/it]Running Inference:  86%|████████▋ | 173/200 [05:52<00:41,  1.53s/it]Running Inference:  87%|████████▋ | 174/200 [05:54<00:43,  1.65s/it]Running Inference:  88%|████████▊ | 175/200 [05:55<00:36,  1.46s/it]Running Inference:  88%|████████▊ | 176/200 [05:56<00:32,  1.37s/it]Running Inference:  88%|████████▊ | 177/200 [06:00<00:49,  2.15s/it]Running Inference:  89%|████████▉ | 178/200 [06:03<00:51,  2.32s/it]Running Inference:  90%|████████▉ | 179/200 [06:06<00:50,  2.40s/it]Running Inference:  90%|█████████ | 180/200 [06:07<00:41,  2.07s/it]Running Inference:  90%|█████████ | 181/200 [06:08<00:31,  1.68s/it]Running Inference:  91%|█████████ | 182/200 [06:09<00:25,  1.44s/it]Running Inference:  92%|█████████▏| 183/200 [06:13<00:41,  2.44s/it]Running Inference:  92%|█████████▏| 184/200 [06:18<00:51,  3.23s/it]Running Inference:  92%|█████████▎| 185/200 [06:19<00:38,  2.54s/it]Running Inference:  93%|█████████▎| 186/200 [06:21<00:31,  2.26s/it]Running Inference:  94%|█████████▎| 187/200 [06:23<00:29,  2.25s/it]Running Inference:  94%|█████████▍| 188/200 [06:26<00:27,  2.27s/it]Running Inference:  94%|█████████▍| 189/200 [06:26<00:19,  1.73s/it]Running Inference:  95%|█████████▌| 190/200 [06:28<00:17,  1.76s/it]Running Inference:  96%|█████████▌| 191/200 [06:29<00:15,  1.73s/it]Running Inference:  96%|█████████▌| 192/200 [06:32<00:15,  1.88s/it]Running Inference:  96%|█████████▋| 193/200 [06:33<00:12,  1.80s/it]Running Inference:  97%|█████████▋| 194/200 [06:36<00:12,  2.04s/it]Running Inference:  98%|█████████▊| 195/200 [06:39<00:11,  2.23s/it]Running Inference:  98%|█████████▊| 196/200 [06:39<00:07,  1.77s/it]Running Inference:  98%|█████████▊| 197/200 [06:41<00:05,  1.69s/it]Running Inference:  99%|█████████▉| 198/200 [06:44<00:04,  2.29s/it]Running Inference: 100%|█████████▉| 199/200 [06:45<00:01,  1.77s/it]Running Inference: 100%|██████████| 200/200 [06:46<00:00,  1.56s/it]Running Inference: 100%|██████████| 200/200 [06:46<00:00,  2.03s/it]
2025-12-13 20:41:30,702 - INFO - Inference completed.
2025-12-13 20:41:30,726 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 20:41:30,726 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:41:30,762 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 20:41:30,762 - INFO - Metrics:
54.86
2025-12-13 20:41:30,764 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=triviaqa, ratio=0.1) on GPU 1

----------------------------------------
Task: triviaqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:41:37,178 - INFO - Set deterministic seeds to 42
2025-12-13 20:41:37,178 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:41:37,178 - INFO - Starting evaluation run...
2025-12-13 20:41:37,178 - INFO - Output directory set to: longbenchresult
2025-12-13 20:41:37,178 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 20:41:37,178 - INFO - KV Press 'tova' setup.
2025-12-13 20:41:37,179 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:41:37,179 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.93it/s]
Device set to use cuda:0
2025-12-13 20:41:52,342 - INFO - Model pipeline loaded.
2025-12-13 20:41:52,342 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 20:41:56,959 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:41:56,959 - INFO - Dataset processed with 200 entries.
2025-12-13 20:41:56,990 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<09:59,  3.01s/it]Running Inference:   1%|          | 2/200 [00:04<07:33,  2.29s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:41,  1.74s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:39,  2.04s/it]Running Inference:   2%|▎         | 5/200 [00:10<07:11,  2.21s/it]Running Inference:   3%|▎         | 6/200 [00:11<05:37,  1.74s/it]Running Inference:   4%|▎         | 7/200 [00:12<04:07,  1.28s/it]Running Inference:   4%|▍         | 8/200 [00:12<03:17,  1.03s/it]Running Inference:   4%|▍         | 9/200 [00:13<03:26,  1.08s/it]Running Inference:   5%|▌         | 10/200 [00:15<03:52,  1.22s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:16<03:47,  1.20s/it]Running Inference:   6%|▌         | 12/200 [00:19<05:32,  1.77s/it]Running Inference:   6%|▋         | 13/200 [00:20<04:57,  1.59s/it]Running Inference:   7%|▋         | 14/200 [00:24<06:45,  2.18s/it]Running Inference:   8%|▊         | 15/200 [00:26<06:57,  2.26s/it]Running Inference:   8%|▊         | 16/200 [00:29<07:37,  2.48s/it]Running Inference:   8%|▊         | 17/200 [00:32<07:40,  2.51s/it]Running Inference:   9%|▉         | 18/200 [00:32<05:59,  1.97s/it]Running Inference:  10%|▉         | 19/200 [00:37<08:13,  2.72s/it]Running Inference:  10%|█         | 20/200 [00:38<07:05,  2.36s/it]Running Inference:  10%|█         | 21/200 [00:39<05:24,  1.81s/it]Running Inference:  11%|█         | 22/200 [00:42<06:40,  2.25s/it]Running Inference:  12%|█▏        | 23/200 [00:44<05:46,  1.96s/it]Running Inference:  12%|█▏        | 24/200 [00:46<05:47,  1.97s/it]Running Inference:  12%|█▎        | 25/200 [00:47<05:30,  1.89s/it]Running Inference:  13%|█▎        | 26/200 [00:49<05:01,  1.73s/it]Running Inference:  14%|█▎        | 27/200 [00:49<04:09,  1.44s/it]Running Inference:  14%|█▍        | 28/200 [00:50<03:29,  1.22s/it]Running Inference:  14%|█▍        | 29/200 [00:54<05:52,  2.06s/it]Running Inference:  15%|█▌        | 30/200 [00:55<04:28,  1.58s/it]Running Inference:  16%|█▌        | 31/200 [00:56<04:22,  1.55s/it]Running Inference:  16%|█▌        | 32/200 [00:58<04:48,  1.72s/it]Running Inference:  16%|█▋        | 33/200 [00:59<04:12,  1.51s/it]Running Inference:  17%|█▋        | 34/200 [01:01<04:07,  1.49s/it]Running Inference:  18%|█▊        | 35/200 [01:02<03:55,  1.42s/it]Running Inference:  18%|█▊        | 36/200 [01:06<06:14,  2.28s/it]Running Inference:  18%|█▊        | 37/200 [01:07<05:19,  1.96s/it]Running Inference:  19%|█▉        | 38/200 [01:12<07:37,  2.83s/it]Running Inference:  20%|█▉        | 39/200 [01:13<05:46,  2.15s/it]Running Inference:  20%|██        | 40/200 [01:16<06:36,  2.48s/it]Running Inference:  20%|██        | 41/200 [01:18<05:45,  2.17s/it]Running Inference:  21%|██        | 42/200 [01:20<06:08,  2.33s/it]Running Inference:  22%|██▏       | 43/200 [01:21<04:37,  1.76s/it]Running Inference:  22%|██▏       | 44/200 [01:22<04:35,  1.77s/it]Running Inference:  22%|██▎       | 45/200 [01:23<03:32,  1.37s/it]Running Inference:  23%|██▎       | 46/200 [01:24<03:16,  1.28s/it]Running Inference:  24%|██▎       | 47/200 [01:25<03:05,  1.21s/it]Running Inference:  24%|██▍       | 48/200 [01:30<06:11,  2.44s/it]Running Inference:  24%|██▍       | 49/200 [01:32<05:55,  2.35s/it]Running Inference:  25%|██▌       | 50/200 [01:35<06:13,  2.49s/it]Running Inference:  26%|██▌       | 51/200 [01:36<04:51,  1.96s/it]Running Inference:  26%|██▌       | 52/200 [01:38<05:02,  2.04s/it]Running Inference:  26%|██▋       | 53/200 [01:39<03:49,  1.56s/it]Running Inference:  27%|██▋       | 54/200 [01:40<03:26,  1.42s/it]Running Inference:  28%|██▊       | 55/200 [01:41<03:14,  1.34s/it]Running Inference:  28%|██▊       | 56/200 [01:43<03:24,  1.42s/it]Running Inference:  28%|██▊       | 57/200 [01:45<04:15,  1.79s/it]Running Inference:  29%|██▉       | 58/200 [01:47<04:12,  1.78s/it]Running Inference:  30%|██▉       | 59/200 [01:51<06:01,  2.56s/it]Running Inference:  30%|███       | 60/200 [01:53<05:17,  2.27s/it]Running Inference:  30%|███       | 61/200 [01:58<07:02,  3.04s/it]Running Inference:  31%|███       | 62/200 [01:59<06:03,  2.64s/it]Running Inference:  32%|███▏      | 63/200 [02:00<04:31,  1.99s/it]Running Inference:  32%|███▏      | 64/200 [02:01<03:46,  1.67s/it]Running Inference:  32%|███▎      | 65/200 [02:03<04:13,  1.88s/it]Running Inference:  33%|███▎      | 66/200 [02:05<04:27,  2.00s/it]Running Inference:  34%|███▎      | 67/200 [02:07<04:24,  1.99s/it]Running Inference:  34%|███▍      | 68/200 [02:08<03:42,  1.68s/it]Running Inference:  34%|███▍      | 69/200 [02:10<03:50,  1.76s/it]Running Inference:  35%|███▌      | 70/200 [02:15<05:28,  2.53s/it]Running Inference:  36%|███▌      | 71/200 [02:18<05:40,  2.64s/it]Running Inference:  36%|███▌      | 72/200 [02:22<06:44,  3.16s/it]Running Inference:  36%|███▋      | 73/200 [02:23<05:36,  2.65s/it]Running Inference:  37%|███▋      | 74/200 [02:26<05:38,  2.68s/it]Running Inference:  38%|███▊      | 75/200 [02:27<04:12,  2.02s/it]Running Inference:  38%|███▊      | 76/200 [02:29<04:07,  2.00s/it]Running Inference:  38%|███▊      | 77/200 [02:31<04:28,  2.18s/it]Running Inference:  39%|███▉      | 78/200 [02:32<03:18,  1.63s/it]Running Inference:  40%|███▉      | 79/200 [02:34<03:31,  1.75s/it]Running Inference:  40%|████      | 80/200 [02:36<03:44,  1.87s/it]Running Inference:  40%|████      | 81/200 [02:39<04:19,  2.18s/it]Running Inference:  41%|████      | 82/200 [02:39<03:16,  1.67s/it]Running Inference:  42%|████▏     | 83/200 [02:41<03:09,  1.62s/it]Running Inference:  42%|████▏     | 84/200 [02:44<03:57,  2.05s/it]Running Inference:  42%|████▎     | 85/200 [02:45<03:37,  1.89s/it]Running Inference:  43%|████▎     | 86/200 [02:51<05:40,  2.99s/it]Running Inference:  44%|████▎     | 87/200 [02:53<05:21,  2.85s/it]Running Inference:  44%|████▍     | 88/200 [02:55<04:45,  2.55s/it]Running Inference:  44%|████▍     | 89/200 [02:56<03:56,  2.13s/it]Running Inference:  45%|████▌     | 90/200 [02:58<03:42,  2.02s/it]Running Inference:  46%|████▌     | 91/200 [03:03<05:14,  2.88s/it]Running Inference:  46%|████▌     | 92/200 [03:04<04:01,  2.24s/it]Running Inference:  46%|████▋     | 93/200 [03:07<04:23,  2.46s/it]Running Inference:  47%|████▋     | 94/200 [03:08<03:53,  2.20s/it]Running Inference:  48%|████▊     | 95/200 [03:11<04:15,  2.43s/it]Running Inference:  48%|████▊     | 96/200 [03:14<04:10,  2.41s/it]Running Inference:  48%|████▊     | 97/200 [03:15<03:39,  2.13s/it]Running Inference:  49%|████▉     | 98/200 [03:17<03:46,  2.22s/it]Running Inference:  50%|████▉     | 99/200 [03:21<04:19,  2.57s/it]Running Inference:  50%|█████     | 100/200 [03:24<04:36,  2.77s/it]Running Inference:  50%|█████     | 101/200 [03:26<04:18,  2.61s/it]Running Inference:  51%|█████     | 102/200 [03:29<04:08,  2.53s/it]Running Inference:  52%|█████▏    | 103/200 [03:31<03:52,  2.40s/it]Running Inference:  52%|█████▏    | 104/200 [03:32<03:15,  2.03s/it]Running Inference:  52%|█████▎    | 105/200 [03:34<03:23,  2.14s/it]Running Inference:  53%|█████▎    | 106/200 [03:37<03:46,  2.41s/it]Running Inference:  54%|█████▎    | 107/200 [03:40<03:57,  2.56s/it]Running Inference:  54%|█████▍    | 108/200 [03:41<03:17,  2.14s/it]Running Inference:  55%|█████▍    | 109/200 [03:42<02:41,  1.78s/it]Running Inference:  55%|█████▌    | 110/200 [03:43<02:07,  1.42s/it]Running Inference:  56%|█████▌    | 111/200 [03:46<02:58,  2.00s/it]Running Inference:  56%|█████▌    | 112/200 [03:48<02:46,  1.90s/it]Running Inference:  56%|█████▋    | 113/200 [03:50<02:44,  1.89s/it]Running Inference:  57%|█████▋    | 114/200 [03:55<03:56,  2.75s/it]Running Inference:  57%|█████▊    | 115/200 [03:56<03:29,  2.46s/it]Running Inference:  58%|█████▊    | 116/200 [03:57<02:35,  1.85s/it]Running Inference:  58%|█████▊    | 117/200 [04:00<03:16,  2.36s/it]Running Inference:  59%|█████▉    | 118/200 [04:01<02:36,  1.91s/it]Running Inference:  60%|█████▉    | 119/200 [04:03<02:24,  1.78s/it]Running Inference:  60%|██████    | 120/200 [04:04<02:23,  1.79s/it]Running Inference:  60%|██████    | 121/200 [04:05<01:47,  1.36s/it]Running Inference:  61%|██████    | 122/200 [04:06<01:47,  1.38s/it]Running Inference:  62%|██████▏   | 123/200 [04:07<01:25,  1.11s/it]Running Inference:  62%|██████▏   | 124/200 [04:07<01:15,  1.00it/s]Running Inference:  62%|██████▎   | 125/200 [04:10<01:44,  1.39s/it]Running Inference:  63%|██████▎   | 126/200 [04:12<02:11,  1.78s/it]Running Inference:  64%|██████▎   | 127/200 [04:17<03:05,  2.53s/it]Running Inference:  64%|██████▍   | 128/200 [04:17<02:23,  1.99s/it]Running Inference:  64%|██████▍   | 129/200 [04:19<02:01,  1.71s/it]Running Inference:  65%|██████▌   | 130/200 [04:19<01:39,  1.42s/it]Running Inference:  66%|██████▌   | 131/200 [04:21<01:39,  1.44s/it]Running Inference:  66%|██████▌   | 132/200 [04:22<01:42,  1.50s/it]Running Inference:  66%|██████▋   | 133/200 [04:24<01:44,  1.56s/it]Running Inference:  67%|██████▋   | 134/200 [04:26<01:50,  1.67s/it]Running Inference:  68%|██████▊   | 135/200 [04:27<01:41,  1.56s/it]Running Inference:  68%|██████▊   | 136/200 [04:30<02:08,  2.01s/it]Running Inference:  68%|██████▊   | 137/200 [04:32<01:58,  1.88s/it]Running Inference:  69%|██████▉   | 138/200 [04:35<02:18,  2.23s/it]Running Inference:  70%|██████▉   | 139/200 [04:38<02:23,  2.36s/it]Running Inference:  70%|███████   | 140/200 [04:40<02:16,  2.28s/it]Running Inference:  70%|███████   | 141/200 [04:42<02:19,  2.36s/it]Running Inference:  71%|███████   | 142/200 [04:45<02:24,  2.50s/it]Running Inference:  72%|███████▏  | 143/200 [04:46<01:56,  2.04s/it]Running Inference:  72%|███████▏  | 144/200 [04:48<01:45,  1.88s/it]Running Inference:  72%|███████▎  | 145/200 [04:49<01:34,  1.72s/it]Running Inference:  73%|███████▎  | 146/200 [04:52<01:46,  1.98s/it]Running Inference:  74%|███████▎  | 147/200 [04:52<01:22,  1.55s/it]Running Inference:  74%|███████▍  | 148/200 [04:56<01:54,  2.20s/it]Running Inference:  74%|███████▍  | 149/200 [05:00<02:18,  2.72s/it]Running Inference:  75%|███████▌  | 150/200 [05:01<01:48,  2.18s/it]Running Inference:  76%|███████▌  | 151/200 [05:02<01:40,  2.05s/it]Running Inference:  76%|███████▌  | 152/200 [05:03<01:22,  1.73s/it]Running Inference:  76%|███████▋  | 153/200 [05:06<01:33,  1.99s/it]Running Inference:  77%|███████▋  | 154/200 [05:10<02:03,  2.68s/it]Running Inference:  78%|███████▊  | 155/200 [05:12<01:50,  2.45s/it]Running Inference:  78%|███████▊  | 156/200 [05:15<01:47,  2.44s/it]Running Inference:  78%|███████▊  | 157/200 [05:19<02:12,  3.08s/it]Running Inference:  79%|███████▉  | 158/200 [05:21<01:53,  2.71s/it]Running Inference:  80%|███████▉  | 159/200 [05:22<01:34,  2.32s/it]Running Inference:  80%|████████  | 160/200 [05:24<01:17,  1.94s/it]Running Inference:  80%|████████  | 161/200 [05:25<01:14,  1.90s/it]Running Inference:  81%|████████  | 162/200 [05:27<01:10,  1.87s/it]Running Inference:  82%|████████▏ | 163/200 [05:29<01:12,  1.96s/it]Running Inference:  82%|████████▏ | 164/200 [05:30<00:55,  1.56s/it]Running Inference:  82%|████████▎ | 165/200 [05:34<01:18,  2.25s/it]Running Inference:  83%|████████▎ | 166/200 [05:36<01:13,  2.16s/it]Running Inference:  84%|████████▎ | 167/200 [05:37<01:02,  1.90s/it]Running Inference:  84%|████████▍ | 168/200 [05:40<01:13,  2.31s/it]Running Inference:  84%|████████▍ | 169/200 [05:41<00:59,  1.92s/it]Running Inference:  85%|████████▌ | 170/200 [05:42<00:46,  1.55s/it]Running Inference:  86%|████████▌ | 171/200 [05:43<00:39,  1.35s/it]Running Inference:  86%|████████▌ | 172/200 [05:45<00:41,  1.49s/it]Running Inference:  86%|████████▋ | 173/200 [05:46<00:41,  1.52s/it]Running Inference:  87%|████████▋ | 174/200 [05:48<00:42,  1.65s/it]Running Inference:  88%|████████▊ | 175/200 [05:49<00:36,  1.45s/it]Running Inference:  88%|████████▊ | 176/200 [05:50<00:32,  1.36s/it]Running Inference:  88%|████████▊ | 177/200 [05:54<00:49,  2.15s/it]Running Inference:  89%|████████▉ | 178/200 [05:57<00:50,  2.31s/it]Running Inference:  90%|████████▉ | 179/200 [06:00<00:50,  2.40s/it]Running Inference:  90%|█████████ | 180/200 [06:01<00:41,  2.07s/it]Running Inference:  90%|█████████ | 181/200 [06:02<00:31,  1.68s/it]Running Inference:  91%|█████████ | 182/200 [06:03<00:25,  1.44s/it]Running Inference:  92%|█████████▏| 183/200 [06:06<00:33,  1.98s/it]Running Inference:  92%|█████████▏| 184/200 [06:11<00:46,  2.91s/it]Running Inference:  92%|█████████▎| 185/200 [06:12<00:34,  2.31s/it]Running Inference:  93%|█████████▎| 186/200 [06:13<00:29,  2.10s/it]Running Inference:  94%|█████████▎| 187/200 [06:16<00:27,  2.11s/it]Running Inference:  94%|█████████▍| 188/200 [06:18<00:26,  2.17s/it]Running Inference:  94%|█████████▍| 189/200 [06:18<00:18,  1.67s/it]Running Inference:  95%|█████████▌| 190/200 [06:20<00:17,  1.71s/it]Running Inference:  96%|█████████▌| 191/200 [06:22<00:15,  1.70s/it]Running Inference:  96%|█████████▌| 192/200 [06:24<00:14,  1.86s/it]Running Inference:  96%|█████████▋| 193/200 [06:26<00:12,  1.78s/it]Running Inference:  97%|█████████▋| 194/200 [06:28<00:12,  2.03s/it]Running Inference:  98%|█████████▊| 195/200 [06:31<00:11,  2.22s/it]Running Inference:  98%|█████████▊| 196/200 [06:32<00:07,  1.76s/it]Running Inference:  98%|█████████▊| 197/200 [06:33<00:05,  1.69s/it]Running Inference:  99%|█████████▉| 198/200 [06:37<00:04,  2.29s/it]Running Inference: 100%|█████████▉| 199/200 [06:37<00:01,  1.77s/it]Running Inference: 100%|██████████| 200/200 [06:38<00:00,  1.56s/it]Running Inference: 100%|██████████| 200/200 [06:38<00:00,  1.99s/it]
2025-12-13 20:48:35,990 - INFO - Inference completed.
2025-12-13 20:48:36,014 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 20:48:36,014 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:48:36,052 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 20:48:36,052 - INFO - Metrics:
56.06
2025-12-13 20:48:36,053 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=triviaqa, ratio=0.2) on GPU 1

----------------------------------------
Task: triviaqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:48:42,495 - INFO - Set deterministic seeds to 42
2025-12-13 20:48:42,495 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:48:42,495 - INFO - Starting evaluation run...
2025-12-13 20:48:42,496 - INFO - Output directory set to: longbenchresult
2025-12-13 20:48:42,496 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 20:48:42,496 - INFO - KV Press 'tova' setup.
2025-12-13 20:48:42,496 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:48:42,496 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.57it/s]
Device set to use cuda:0
2025-12-13 20:49:00,612 - INFO - Model pipeline loaded.
2025-12-13 20:49:00,612 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 20:49:25,748 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:49:25,748 - INFO - Dataset processed with 200 entries.
2025-12-13 20:49:25,779 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<10:06,  3.05s/it]Running Inference:   1%|          | 2/200 [00:04<07:36,  2.30s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:42,  1.74s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:39,  2.04s/it]Running Inference:   2%|▎         | 5/200 [00:10<07:12,  2.22s/it]Running Inference:   3%|▎         | 6/200 [00:11<05:37,  1.74s/it]Running Inference:   4%|▎         | 7/200 [00:12<04:07,  1.28s/it]Running Inference:   4%|▍         | 8/200 [00:12<03:17,  1.03s/it]Running Inference:   4%|▍         | 9/200 [00:13<03:26,  1.08s/it]Running Inference:   5%|▌         | 10/200 [00:15<03:52,  1.22s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:16<03:46,  1.20s/it]Running Inference:   6%|▌         | 12/200 [00:19<05:30,  1.76s/it]Running Inference:   6%|▋         | 13/200 [00:20<04:56,  1.58s/it]Running Inference:   7%|▋         | 14/200 [00:24<06:44,  2.17s/it]Running Inference:   8%|▊         | 15/200 [00:26<06:56,  2.25s/it]Running Inference:   8%|▊         | 16/200 [00:29<07:35,  2.48s/it]Running Inference:   8%|▊         | 17/200 [00:32<07:40,  2.52s/it]Running Inference:   9%|▉         | 18/200 [00:32<05:59,  1.97s/it]Running Inference:  10%|▉         | 19/200 [00:37<08:12,  2.72s/it]Running Inference:  10%|█         | 20/200 [00:38<07:04,  2.36s/it]Running Inference:  10%|█         | 21/200 [00:39<05:24,  1.81s/it]Running Inference:  11%|█         | 22/200 [00:42<06:41,  2.25s/it]Running Inference:  12%|█▏        | 23/200 [00:44<05:46,  1.96s/it]Running Inference:  12%|█▏        | 24/200 [00:46<05:47,  1.97s/it]Running Inference:  12%|█▎        | 25/200 [00:47<05:30,  1.89s/it]Running Inference:  13%|█▎        | 26/200 [00:49<05:01,  1.73s/it]Running Inference:  14%|█▎        | 27/200 [00:49<04:09,  1.44s/it]Running Inference:  14%|█▍        | 28/200 [00:50<03:30,  1.22s/it]Running Inference:  14%|█▍        | 29/200 [00:54<05:54,  2.07s/it]Running Inference:  15%|█▌        | 30/200 [00:55<04:30,  1.59s/it]Running Inference:  16%|█▌        | 31/200 [00:56<04:23,  1.56s/it]Running Inference:  16%|█▌        | 32/200 [00:58<04:48,  1.72s/it]Running Inference:  16%|█▋        | 33/200 [00:59<04:12,  1.51s/it]Running Inference:  17%|█▋        | 34/200 [01:01<04:07,  1.49s/it]Running Inference:  18%|█▊        | 35/200 [01:02<03:54,  1.42s/it]Running Inference:  18%|█▊        | 36/200 [01:06<06:15,  2.29s/it]Running Inference:  18%|█▊        | 37/200 [01:07<05:19,  1.96s/it]Running Inference:  19%|█▉        | 38/200 [01:12<07:37,  2.82s/it]Running Inference:  20%|█▉        | 39/200 [01:13<05:40,  2.11s/it]Running Inference:  20%|██        | 40/200 [01:16<06:32,  2.45s/it]Running Inference:  20%|██        | 41/200 [01:17<05:42,  2.15s/it]Running Inference:  21%|██        | 42/200 [01:20<06:07,  2.32s/it]Running Inference:  22%|██▏       | 43/200 [01:21<04:36,  1.76s/it]Running Inference:  22%|██▏       | 44/200 [01:22<04:35,  1.76s/it]Running Inference:  22%|██▎       | 45/200 [01:23<03:31,  1.37s/it]Running Inference:  23%|██▎       | 46/200 [01:24<03:16,  1.27s/it]Running Inference:  24%|██▎       | 47/200 [01:25<03:05,  1.21s/it]Running Inference:  24%|██▍       | 48/200 [01:30<06:09,  2.43s/it]Running Inference:  24%|██▍       | 49/200 [01:32<05:54,  2.35s/it]Running Inference:  25%|██▌       | 50/200 [01:35<06:12,  2.49s/it]Running Inference:  26%|██▌       | 51/200 [01:36<04:50,  1.95s/it]Running Inference:  26%|██▌       | 52/200 [01:38<05:01,  2.04s/it]Running Inference:  26%|██▋       | 53/200 [01:39<03:49,  1.56s/it]Running Inference:  27%|██▋       | 54/200 [01:39<03:04,  1.26s/it]Running Inference:  28%|██▊       | 55/200 [01:40<02:58,  1.23s/it]Running Inference:  28%|██▊       | 56/200 [01:42<03:13,  1.34s/it]Running Inference:  28%|██▊       | 57/200 [01:45<04:07,  1.73s/it]Running Inference:  29%|██▉       | 58/200 [01:46<04:07,  1.74s/it]Running Inference:  30%|██▉       | 59/200 [01:51<05:59,  2.55s/it]Running Inference:  30%|███       | 60/200 [01:52<05:16,  2.26s/it]Running Inference:  30%|███       | 61/200 [01:57<07:01,  3.04s/it]Running Inference:  31%|███       | 62/200 [01:59<06:02,  2.63s/it]Running Inference:  32%|███▏      | 63/200 [01:59<04:31,  1.98s/it]Running Inference:  32%|███▏      | 64/200 [02:00<03:45,  1.66s/it]Running Inference:  32%|███▎      | 65/200 [02:03<04:12,  1.87s/it]Running Inference:  33%|███▎      | 66/200 [02:05<04:27,  1.99s/it]Running Inference:  34%|███▎      | 67/200 [02:07<04:24,  1.99s/it]Running Inference:  34%|███▍      | 68/200 [02:08<03:42,  1.68s/it]Running Inference:  34%|███▍      | 69/200 [02:10<03:50,  1.76s/it]Running Inference:  35%|███▌      | 70/200 [02:14<05:28,  2.53s/it]Running Inference:  36%|███▌      | 71/200 [02:17<05:40,  2.64s/it]Running Inference:  36%|███▌      | 72/200 [02:21<06:45,  3.16s/it]Running Inference:  36%|███▋      | 73/200 [02:23<05:36,  2.65s/it]Running Inference:  37%|███▋      | 74/200 [02:26<05:37,  2.68s/it]Running Inference:  38%|███▊      | 75/200 [02:26<04:10,  2.01s/it]Running Inference:  38%|███▊      | 76/200 [02:30<05:21,  2.59s/it]Running Inference:  38%|███▊      | 77/200 [02:33<05:19,  2.59s/it]Running Inference:  39%|███▉      | 78/200 [02:33<03:54,  1.92s/it]Running Inference:  40%|███▉      | 79/200 [02:35<03:55,  1.95s/it]Running Inference:  40%|████      | 80/200 [02:37<04:01,  2.01s/it]Running Inference:  40%|████      | 81/200 [02:40<04:30,  2.27s/it]Running Inference:  41%|████      | 82/200 [02:40<03:24,  1.73s/it]Running Inference:  42%|████▏     | 83/200 [02:42<03:14,  1.66s/it]Running Inference:  42%|████▏     | 84/200 [02:45<04:00,  2.07s/it]Running Inference:  42%|████▎     | 85/200 [02:46<03:39,  1.91s/it]Running Inference:  43%|████▎     | 86/200 [02:52<05:41,  3.00s/it]Running Inference:  44%|████▎     | 87/200 [02:55<05:22,  2.85s/it]Running Inference:  44%|████▍     | 88/200 [02:56<04:45,  2.55s/it]Running Inference:  44%|████▍     | 89/200 [02:57<03:56,  2.13s/it]Running Inference:  45%|████▌     | 90/200 [02:59<03:42,  2.02s/it]Running Inference:  46%|████▌     | 91/200 [03:04<05:14,  2.89s/it]Running Inference:  46%|████▌     | 92/200 [03:05<04:02,  2.24s/it]Running Inference:  46%|████▋     | 93/200 [03:08<04:23,  2.46s/it]Running Inference:  47%|████▋     | 94/200 [03:09<03:52,  2.20s/it]Running Inference:  48%|████▊     | 95/200 [03:12<04:14,  2.42s/it]Running Inference:  48%|████▊     | 96/200 [03:15<04:09,  2.40s/it]Running Inference:  48%|████▊     | 97/200 [03:16<03:37,  2.12s/it]Running Inference:  49%|████▉     | 98/200 [03:19<03:45,  2.21s/it]Running Inference:  50%|████▉     | 99/200 [03:22<04:19,  2.57s/it]Running Inference:  50%|█████     | 100/200 [03:25<04:38,  2.79s/it]Running Inference:  50%|█████     | 101/200 [03:28<04:19,  2.62s/it]Running Inference:  51%|█████     | 102/200 [03:30<04:08,  2.54s/it]Running Inference:  52%|█████▏    | 103/200 [03:32<03:52,  2.40s/it]Running Inference:  52%|█████▏    | 104/200 [03:33<03:10,  1.98s/it]Running Inference:  52%|█████▎    | 105/200 [03:35<03:19,  2.10s/it]Running Inference:  53%|█████▎    | 106/200 [03:38<03:45,  2.40s/it]Running Inference:  54%|█████▎    | 107/200 [03:42<04:12,  2.71s/it]Running Inference:  54%|█████▍    | 108/200 [03:43<03:27,  2.25s/it]Running Inference:  55%|█████▍    | 109/200 [03:44<02:48,  1.86s/it]Running Inference:  55%|█████▌    | 110/200 [03:45<02:12,  1.47s/it]Running Inference:  56%|█████▌    | 111/200 [03:48<03:02,  2.05s/it]Running Inference:  56%|█████▌    | 112/200 [03:50<02:50,  1.93s/it]Running Inference:  56%|█████▋    | 113/200 [03:52<02:55,  2.02s/it]Running Inference:  57%|█████▋    | 114/200 [03:57<04:04,  2.84s/it]Running Inference:  57%|█████▊    | 115/200 [03:58<03:34,  2.53s/it]Running Inference:  58%|█████▊    | 116/200 [03:59<02:39,  1.89s/it]Running Inference:  58%|█████▊    | 117/200 [04:02<03:19,  2.41s/it]Running Inference:  59%|█████▉    | 118/200 [04:03<02:39,  1.94s/it]Running Inference:  60%|█████▉    | 119/200 [04:05<02:26,  1.80s/it]Running Inference:  60%|██████    | 120/200 [04:07<02:24,  1.81s/it]Running Inference:  60%|██████    | 121/200 [04:07<01:48,  1.37s/it]Running Inference:  61%|██████    | 122/200 [04:08<01:48,  1.39s/it]Running Inference:  62%|██████▏   | 123/200 [04:09<01:25,  1.11s/it]Running Inference:  62%|██████▏   | 124/200 [04:10<01:15,  1.00it/s]Running Inference:  62%|██████▎   | 125/200 [04:12<01:44,  1.39s/it]Running Inference:  63%|██████▎   | 126/200 [04:15<02:13,  1.80s/it]Running Inference:  64%|██████▎   | 127/200 [04:19<03:06,  2.56s/it]Running Inference:  64%|██████▍   | 128/200 [04:20<02:24,  2.01s/it]Running Inference:  64%|██████▍   | 129/200 [04:21<02:02,  1.72s/it]Running Inference:  65%|██████▌   | 130/200 [04:22<01:39,  1.43s/it]Running Inference:  66%|██████▌   | 131/200 [04:23<01:39,  1.44s/it]Running Inference:  66%|██████▌   | 132/200 [04:27<02:24,  2.12s/it]Running Inference:  66%|██████▋   | 133/200 [04:28<02:14,  2.00s/it]Running Inference:  67%|██████▋   | 134/200 [04:30<02:10,  1.98s/it]Running Inference:  68%|██████▊   | 135/200 [04:32<01:55,  1.77s/it]Running Inference:  68%|██████▊   | 136/200 [04:35<02:18,  2.16s/it]Running Inference:  68%|██████▊   | 137/200 [04:36<02:04,  1.98s/it]Running Inference:  69%|██████▉   | 138/200 [04:39<02:22,  2.30s/it]Running Inference:  70%|██████▉   | 139/200 [04:42<02:26,  2.40s/it]Running Inference:  70%|███████   | 140/200 [04:44<02:18,  2.31s/it]Running Inference:  70%|███████   | 141/200 [04:47<02:20,  2.38s/it]Running Inference:  71%|███████   | 142/200 [04:49<02:25,  2.51s/it]Running Inference:  72%|███████▏  | 143/200 [04:50<01:57,  2.05s/it]Running Inference:  72%|███████▏  | 144/200 [04:52<01:45,  1.89s/it]Running Inference:  72%|███████▎  | 145/200 [04:53<01:34,  1.72s/it]Running Inference:  73%|███████▎  | 146/200 [04:56<01:46,  1.97s/it]Running Inference:  74%|███████▎  | 147/200 [04:56<01:22,  1.55s/it]Running Inference:  74%|███████▍  | 148/200 [05:00<01:54,  2.20s/it]Running Inference:  74%|███████▍  | 149/200 [05:04<02:18,  2.71s/it]Running Inference:  75%|███████▌  | 150/200 [05:05<01:48,  2.17s/it]Running Inference:  76%|███████▌  | 151/200 [05:07<01:40,  2.04s/it]Running Inference:  76%|███████▌  | 152/200 [05:08<01:22,  1.72s/it]Running Inference:  76%|███████▋  | 153/200 [05:10<01:33,  1.99s/it]Running Inference:  77%|███████▋  | 154/200 [05:13<01:36,  2.10s/it]Running Inference:  78%|███████▊  | 155/200 [05:14<01:31,  2.03s/it]Running Inference:  78%|███████▊  | 156/200 [05:17<01:34,  2.15s/it]Running Inference:  78%|███████▊  | 157/200 [05:21<02:03,  2.88s/it]Running Inference:  79%|███████▉  | 158/200 [05:23<01:47,  2.57s/it]Running Inference:  80%|███████▉  | 159/200 [05:25<01:30,  2.22s/it]Running Inference:  80%|████████  | 160/200 [05:26<01:14,  1.87s/it]Running Inference:  80%|████████  | 161/200 [05:28<01:12,  1.85s/it]Running Inference:  81%|████████  | 162/200 [05:29<01:09,  1.83s/it]Running Inference:  82%|████████▏ | 163/200 [05:31<01:11,  1.93s/it]Running Inference:  82%|████████▏ | 164/200 [05:32<00:55,  1.54s/it]Running Inference:  82%|████████▎ | 165/200 [05:36<01:18,  2.24s/it]Running Inference:  83%|████████▎ | 166/200 [05:38<01:13,  2.15s/it]Running Inference:  84%|████████▎ | 167/200 [05:39<01:02,  1.90s/it]Running Inference:  84%|████████▍ | 168/200 [05:42<01:13,  2.31s/it]Running Inference:  84%|████████▍ | 169/200 [05:43<00:59,  1.91s/it]Running Inference:  85%|████████▌ | 170/200 [05:44<00:46,  1.55s/it]Running Inference:  86%|████████▌ | 171/200 [05:45<00:39,  1.35s/it]Running Inference:  86%|████████▌ | 172/200 [05:47<00:41,  1.49s/it]Running Inference:  86%|████████▋ | 173/200 [05:48<00:41,  1.52s/it]Running Inference:  87%|████████▋ | 174/200 [05:50<00:42,  1.65s/it]Running Inference:  88%|████████▊ | 175/200 [05:51<00:36,  1.45s/it]Running Inference:  88%|████████▊ | 176/200 [05:53<00:32,  1.36s/it]Running Inference:  88%|████████▊ | 177/200 [05:55<00:36,  1.60s/it]Running Inference:  89%|████████▉ | 178/200 [05:57<00:42,  1.93s/it]Running Inference:  90%|████████▉ | 179/200 [06:00<00:45,  2.15s/it]Running Inference:  90%|█████████ | 180/200 [06:01<00:37,  1.89s/it]Running Inference:  90%|█████████ | 181/200 [06:02<00:29,  1.55s/it]Running Inference:  91%|█████████ | 182/200 [06:03<00:24,  1.35s/it]Running Inference:  92%|█████████▏| 183/200 [06:08<00:40,  2.36s/it]Running Inference:  92%|█████████▏| 184/200 [06:13<00:50,  3.17s/it]Running Inference:  92%|█████████▎| 185/200 [06:14<00:37,  2.49s/it]Running Inference:  93%|█████████▎| 186/200 [06:15<00:31,  2.22s/it]Running Inference:  94%|█████████▎| 187/200 [06:18<00:28,  2.23s/it]Running Inference:  94%|█████████▍| 188/200 [06:20<00:27,  2.25s/it]Running Inference:  94%|█████████▍| 189/200 [06:20<00:18,  1.73s/it]Running Inference:  95%|█████████▌| 190/200 [06:22<00:17,  1.75s/it]Running Inference:  96%|█████████▌| 191/200 [06:24<00:15,  1.73s/it]Running Inference:  96%|█████████▌| 192/200 [06:26<00:15,  1.88s/it]Running Inference:  96%|█████████▋| 193/200 [06:28<00:12,  1.79s/it]Running Inference:  97%|█████████▋| 194/200 [06:30<00:12,  2.03s/it]Running Inference:  98%|█████████▊| 195/200 [06:33<00:11,  2.22s/it]Running Inference:  98%|█████████▊| 196/200 [06:34<00:07,  1.76s/it]Running Inference:  98%|█████████▊| 197/200 [06:35<00:05,  1.69s/it]Running Inference:  99%|█████████▉| 198/200 [06:39<00:04,  2.30s/it]Running Inference: 100%|█████████▉| 199/200 [06:39<00:01,  1.78s/it]Running Inference: 100%|██████████| 200/200 [06:40<00:00,  1.56s/it]Running Inference: 100%|██████████| 200/200 [06:40<00:00,  2.00s/it]
2025-12-13 20:56:06,736 - INFO - Inference completed.
2025-12-13 20:56:06,759 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-13 20:56:06,759 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:56:06,795 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-13 20:56:06,795 - INFO - Metrics:
56.06
2025-12-13 20:56:06,796 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=triviaqa, ratio=0.3) on GPU 1

----------------------------------------
Task: triviaqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:56:13,215 - INFO - Set deterministic seeds to 42
2025-12-13 20:56:13,215 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:56:13,215 - INFO - Starting evaluation run...
2025-12-13 20:56:13,215 - INFO - Output directory set to: longbenchresult
2025-12-13 20:56:13,215 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-13 20:56:13,215 - INFO - KV Press 'tova' setup.
2025-12-13 20:56:13,215 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:56:13,215 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.86it/s]
Device set to use cuda:0
2025-12-13 20:57:12,082 - INFO - Model pipeline loaded.
2025-12-13 20:57:12,083 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 20:57:24,546 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:57:24,546 - INFO - Dataset processed with 200 entries.
2025-12-13 20:57:24,577 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<10:05,  3.04s/it]Running Inference:   1%|          | 2/200 [00:04<07:32,  2.29s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:41,  1.73s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:36,  2.02s/it]Running Inference:   2%|▎         | 5/200 [00:10<07:05,  2.18s/it]Running Inference:   3%|▎         | 6/200 [00:11<05:32,  1.71s/it]Running Inference:   4%|▎         | 7/200 [00:11<04:04,  1.27s/it]Running Inference:   4%|▍         | 8/200 [00:12<03:16,  1.02s/it]Running Inference:   4%|▍         | 9/200 [00:13<03:35,  1.13s/it]Running Inference:   5%|▌         | 10/200 [00:15<03:57,  1.25s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:16<03:51,  1.23s/it]Running Inference:   6%|▌         | 12/200 [00:19<05:31,  1.77s/it]Running Inference:   6%|▋         | 13/200 [00:20<04:56,  1.59s/it]Running Inference:   7%|▋         | 14/200 [00:24<06:41,  2.16s/it]Running Inference:   8%|▊         | 15/200 [00:26<06:53,  2.23s/it]Running Inference:   8%|▊         | 16/200 [00:29<07:32,  2.46s/it]Running Inference:   8%|▊         | 17/200 [00:32<07:36,  2.50s/it]Running Inference:   9%|▉         | 18/200 [00:32<05:56,  1.96s/it]Running Inference:  10%|▉         | 19/200 [00:37<08:36,  2.85s/it]Running Inference:  10%|█         | 20/200 [00:39<07:20,  2.45s/it]Running Inference:  10%|█         | 21/200 [00:39<05:34,  1.87s/it]Running Inference:  11%|█         | 22/200 [00:43<06:45,  2.28s/it]Running Inference:  12%|█▏        | 23/200 [00:44<05:48,  1.97s/it]Running Inference:  12%|█▏        | 24/200 [00:46<05:47,  1.98s/it]Running Inference:  12%|█▎        | 25/200 [00:47<05:29,  1.88s/it]Running Inference:  13%|█▎        | 26/200 [00:49<05:00,  1.72s/it]Running Inference:  14%|█▎        | 27/200 [00:50<04:36,  1.60s/it]Running Inference:  14%|█▍        | 28/200 [00:51<03:47,  1.32s/it]Running Inference:  14%|█▍        | 29/200 [00:55<06:01,  2.12s/it]Running Inference:  15%|█▌        | 30/200 [00:55<04:35,  1.62s/it]Running Inference:  16%|█▌        | 31/200 [00:57<04:26,  1.58s/it]Running Inference:  16%|█▌        | 32/200 [00:59<04:49,  1.72s/it]Running Inference:  16%|█▋        | 33/200 [01:00<04:16,  1.54s/it]Running Inference:  17%|█▋        | 34/200 [01:01<04:10,  1.51s/it]Running Inference:  18%|█▊        | 35/200 [01:03<03:55,  1.43s/it]Running Inference:  18%|█▊        | 36/200 [01:07<06:12,  2.27s/it]Running Inference:  18%|█▊        | 37/200 [01:08<05:17,  1.95s/it]Running Inference:  19%|█▉        | 38/200 [01:11<06:23,  2.37s/it]Running Inference:  20%|█▉        | 39/200 [01:12<04:48,  1.79s/it]Running Inference:  20%|██        | 40/200 [01:15<05:55,  2.22s/it]Running Inference:  20%|██        | 41/200 [01:16<05:16,  1.99s/it]Running Inference:  21%|██        | 42/200 [01:19<05:45,  2.19s/it]Running Inference:  22%|██▏       | 43/200 [01:20<04:20,  1.66s/it]Running Inference:  22%|██▏       | 44/200 [01:21<04:24,  1.69s/it]Running Inference:  22%|██▎       | 45/200 [01:22<03:23,  1.32s/it]Running Inference:  23%|██▎       | 46/200 [01:23<03:10,  1.24s/it]Running Inference:  24%|██▎       | 47/200 [01:24<03:00,  1.18s/it]Running Inference:  24%|██▍       | 48/200 [01:29<06:01,  2.38s/it]Running Inference:  24%|██▍       | 49/200 [01:31<05:47,  2.30s/it]Running Inference:  25%|██▌       | 50/200 [01:34<06:06,  2.45s/it]Running Inference:  26%|██▌       | 51/200 [01:35<04:46,  1.92s/it]Running Inference:  26%|██▌       | 52/200 [01:37<04:58,  2.01s/it]Running Inference:  26%|██▋       | 53/200 [01:37<03:52,  1.58s/it]Running Inference:  27%|██▋       | 54/200 [01:38<03:05,  1.27s/it]Running Inference:  28%|██▊       | 55/200 [01:39<02:59,  1.23s/it]Running Inference:  28%|██▊       | 56/200 [01:41<03:12,  1.34s/it]Running Inference:  28%|██▊       | 57/200 [01:43<04:06,  1.72s/it]Running Inference:  29%|██▉       | 58/200 [01:45<04:05,  1.73s/it]Running Inference:  30%|██▉       | 59/200 [01:47<04:25,  1.89s/it]Running Inference:  30%|███       | 60/200 [01:49<04:10,  1.79s/it]Running Inference:  30%|███       | 61/200 [01:54<06:12,  2.68s/it]Running Inference:  31%|███       | 62/200 [01:55<05:27,  2.38s/it]Running Inference:  32%|███▏      | 63/200 [01:56<04:06,  1.80s/it]Running Inference:  32%|███▏      | 64/200 [01:57<03:28,  1.53s/it]Running Inference:  32%|███▎      | 65/200 [01:59<03:59,  1.77s/it]Running Inference:  33%|███▎      | 66/200 [02:01<04:17,  1.92s/it]Running Inference:  34%|███▎      | 67/200 [02:03<04:16,  1.93s/it]Running Inference:  34%|███▍      | 68/200 [02:04<03:36,  1.64s/it]Running Inference:  34%|███▍      | 69/200 [02:06<03:45,  1.72s/it]Running Inference:  35%|███▌      | 70/200 [02:10<05:16,  2.44s/it]Running Inference:  36%|███▌      | 71/200 [02:13<05:31,  2.57s/it]Running Inference:  36%|███▌      | 72/200 [02:17<06:36,  3.09s/it]Running Inference:  36%|███▋      | 73/200 [02:19<05:30,  2.60s/it]Running Inference:  37%|███▋      | 74/200 [02:22<05:31,  2.63s/it]Running Inference:  38%|███▊      | 75/200 [02:22<04:05,  1.96s/it]Running Inference:  38%|███▊      | 76/200 [02:26<05:14,  2.54s/it]Running Inference:  38%|███▊      | 77/200 [02:28<05:13,  2.55s/it]Running Inference:  39%|███▉      | 78/200 [02:29<03:50,  1.89s/it]Running Inference:  40%|███▉      | 79/200 [02:31<03:52,  1.92s/it]Running Inference:  40%|████      | 80/200 [02:33<03:58,  1.98s/it]Running Inference:  40%|████      | 81/200 [02:36<04:26,  2.24s/it]Running Inference:  41%|████      | 82/200 [02:36<03:21,  1.71s/it]Running Inference:  42%|████▏     | 83/200 [02:38<03:12,  1.65s/it]Running Inference:  42%|████▏     | 84/200 [02:41<03:57,  2.05s/it]Running Inference:  42%|████▎     | 85/200 [02:42<03:37,  1.89s/it]Running Inference:  43%|████▎     | 86/200 [02:48<05:35,  2.94s/it]Running Inference:  44%|████▎     | 87/200 [02:50<05:17,  2.81s/it]Running Inference:  44%|████▍     | 88/200 [02:52<04:41,  2.51s/it]Running Inference:  44%|████▍     | 89/200 [02:53<03:52,  2.10s/it]Running Inference:  45%|████▌     | 90/200 [02:55<03:39,  2.00s/it]Running Inference:  46%|████▌     | 91/200 [03:00<05:09,  2.84s/it]Running Inference:  46%|████▌     | 92/200 [03:00<03:57,  2.20s/it]Running Inference:  46%|████▋     | 93/200 [03:03<04:19,  2.42s/it]Running Inference:  47%|████▋     | 94/200 [03:05<03:49,  2.16s/it]Running Inference:  48%|████▊     | 95/200 [03:08<04:10,  2.39s/it]Running Inference:  48%|████▊     | 96/200 [03:10<04:06,  2.37s/it]Running Inference:  48%|████▊     | 97/200 [03:13<04:34,  2.67s/it]Running Inference:  49%|████▉     | 98/200 [03:16<04:23,  2.59s/it]Running Inference:  50%|████▉     | 99/200 [03:19<04:44,  2.82s/it]Running Inference:  50%|█████     | 100/200 [03:22<04:53,  2.94s/it]Running Inference:  50%|█████     | 101/200 [03:25<04:28,  2.72s/it]Running Inference:  51%|█████     | 102/200 [03:27<04:14,  2.60s/it]Running Inference:  52%|█████▏    | 103/200 [03:29<03:56,  2.44s/it]Running Inference:  52%|█████▏    | 104/200 [03:30<03:12,  2.00s/it]Running Inference:  52%|█████▎    | 105/200 [03:32<03:20,  2.11s/it]Running Inference:  53%|█████▎    | 106/200 [03:35<03:46,  2.41s/it]Running Inference:  54%|█████▎    | 107/200 [03:39<04:09,  2.69s/it]Running Inference:  54%|█████▍    | 108/200 [03:40<03:25,  2.23s/it]Running Inference:  55%|█████▍    | 109/200 [03:41<02:47,  1.84s/it]Running Inference:  55%|█████▌    | 110/200 [03:41<02:10,  1.46s/it]Running Inference:  56%|█████▌    | 111/200 [03:45<03:00,  2.02s/it]Running Inference:  56%|█████▌    | 112/200 [03:46<02:47,  1.90s/it]Running Inference:  56%|█████▋    | 113/200 [03:49<02:52,  1.98s/it]Running Inference:  57%|█████▋    | 114/200 [03:53<04:00,  2.80s/it]Running Inference:  57%|█████▊    | 115/200 [03:55<03:31,  2.49s/it]Running Inference:  58%|█████▊    | 116/200 [03:55<02:36,  1.87s/it]Running Inference:  58%|█████▊    | 117/200 [03:59<03:16,  2.37s/it]Running Inference:  59%|█████▉    | 118/200 [04:00<02:36,  1.91s/it]Running Inference:  60%|█████▉    | 119/200 [04:01<02:24,  1.78s/it]Running Inference:  60%|██████    | 120/200 [04:03<02:23,  1.79s/it]Running Inference:  60%|██████    | 121/200 [04:03<01:47,  1.36s/it]Running Inference:  61%|██████    | 122/200 [04:05<01:47,  1.38s/it]Running Inference:  62%|██████▏   | 123/200 [04:05<01:25,  1.10s/it]Running Inference:  62%|██████▏   | 124/200 [04:06<01:15,  1.01it/s]Running Inference:  62%|██████▎   | 125/200 [04:08<01:42,  1.37s/it]Running Inference:  63%|██████▎   | 126/200 [04:11<02:10,  1.76s/it]Running Inference:  64%|██████▎   | 127/200 [04:13<02:19,  1.91s/it]Running Inference:  64%|██████▍   | 128/200 [04:14<01:51,  1.55s/it]Running Inference:  64%|██████▍   | 129/200 [04:15<01:39,  1.40s/it]Running Inference:  65%|██████▌   | 130/200 [04:16<01:24,  1.20s/it]Running Inference:  66%|██████▌   | 131/200 [04:17<01:28,  1.28s/it]Running Inference:  66%|██████▌   | 132/200 [04:19<01:43,  1.53s/it]Running Inference:  66%|██████▋   | 133/200 [04:20<01:29,  1.34s/it]Running Inference:  67%|██████▋   | 134/200 [04:22<01:39,  1.51s/it]Running Inference:  68%|██████▊   | 135/200 [04:23<01:33,  1.44s/it]Running Inference:  68%|██████▊   | 136/200 [04:26<02:02,  1.92s/it]Running Inference:  68%|██████▊   | 137/200 [04:28<01:54,  1.81s/it]Running Inference:  69%|██████▉   | 138/200 [04:32<02:30,  2.42s/it]Running Inference:  70%|██████▉   | 139/200 [04:34<02:31,  2.49s/it]Running Inference:  70%|███████   | 140/200 [04:37<02:21,  2.37s/it]Running Inference:  70%|███████   | 141/200 [04:39<02:22,  2.41s/it]Running Inference:  71%|███████   | 142/200 [04:42<02:26,  2.53s/it]Running Inference:  72%|███████▏  | 143/200 [04:43<01:57,  2.07s/it]Running Inference:  72%|███████▏  | 144/200 [04:44<01:45,  1.89s/it]Running Inference:  72%|███████▎  | 145/200 [04:46<01:34,  1.72s/it]Running Inference:  73%|███████▎  | 146/200 [04:48<01:45,  1.96s/it]Running Inference:  74%|███████▎  | 147/200 [04:49<01:21,  1.54s/it]Running Inference:  74%|███████▍  | 148/200 [04:52<01:53,  2.17s/it]Running Inference:  74%|███████▍  | 149/200 [04:56<02:16,  2.68s/it]Running Inference:  75%|███████▌  | 150/200 [04:57<01:47,  2.15s/it]Running Inference:  76%|███████▌  | 151/200 [04:59<01:39,  2.02s/it]Running Inference:  76%|███████▌  | 152/200 [05:00<01:21,  1.71s/it]Running Inference:  76%|███████▋  | 153/200 [05:02<01:32,  1.97s/it]Running Inference:  77%|███████▋  | 154/200 [05:05<01:35,  2.07s/it]Running Inference:  78%|███████▊  | 155/200 [05:07<01:30,  2.01s/it]Running Inference:  78%|███████▊  | 156/200 [05:09<01:33,  2.13s/it]Running Inference:  78%|███████▊  | 157/200 [05:14<02:02,  2.85s/it]Running Inference:  79%|███████▉  | 158/200 [05:15<01:46,  2.54s/it]Running Inference:  80%|███████▉  | 159/200 [05:17<01:29,  2.19s/it]Running Inference:  80%|████████  | 160/200 [05:18<01:14,  1.85s/it]Running Inference:  80%|████████  | 161/200 [05:20<01:11,  1.83s/it]Running Inference:  81%|████████  | 162/200 [05:21<01:08,  1.80s/it]Running Inference:  82%|████████▏ | 163/200 [05:24<01:10,  1.90s/it]Running Inference:  82%|████████▏ | 164/200 [05:24<00:54,  1.51s/it]Running Inference:  82%|████████▎ | 165/200 [05:28<01:17,  2.21s/it]Running Inference:  83%|████████▎ | 166/200 [05:30<01:11,  2.12s/it]Running Inference:  84%|████████▎ | 167/200 [05:31<01:01,  1.87s/it]Running Inference:  84%|████████▍ | 168/200 [05:36<01:24,  2.63s/it]Running Inference:  84%|████████▍ | 169/200 [05:39<01:24,  2.74s/it]Running Inference:  85%|████████▌ | 170/200 [05:39<01:03,  2.11s/it]Running Inference:  86%|████████▌ | 171/200 [05:40<00:50,  1.73s/it]Running Inference:  86%|████████▌ | 172/200 [05:42<00:49,  1.75s/it]Running Inference:  86%|████████▋ | 173/200 [05:43<00:45,  1.70s/it]Running Inference:  87%|████████▋ | 174/200 [05:45<00:45,  1.77s/it]Running Inference:  88%|████████▊ | 175/200 [05:46<00:38,  1.54s/it]Running Inference:  88%|████████▊ | 176/200 [05:47<00:33,  1.42s/it]Running Inference:  88%|████████▊ | 177/200 [05:50<00:37,  1.63s/it]Running Inference:  89%|████████▉ | 178/200 [05:52<00:42,  1.95s/it]Running Inference:  90%|████████▉ | 179/200 [05:53<00:32,  1.56s/it]Running Inference:  90%|█████████ | 180/200 [05:54<00:29,  1.48s/it]Running Inference:  90%|█████████ | 181/200 [05:55<00:23,  1.26s/it]Running Inference:  91%|█████████ | 182/200 [05:56<00:20,  1.14s/it]Running Inference:  92%|█████████▏| 183/200 [06:00<00:37,  2.20s/it]Running Inference:  92%|█████████▏| 184/200 [06:05<00:48,  3.03s/it]Running Inference:  92%|█████████▎| 185/200 [06:06<00:35,  2.39s/it]Running Inference:  93%|█████████▎| 186/200 [06:08<00:30,  2.15s/it]Running Inference:  94%|█████████▎| 187/200 [06:10<00:27,  2.15s/it]Running Inference:  94%|█████████▍| 188/200 [06:12<00:26,  2.20s/it]Running Inference:  94%|█████████▍| 189/200 [06:13<00:18,  1.69s/it]Running Inference:  95%|█████████▌| 190/200 [06:15<00:17,  1.72s/it]Running Inference:  96%|█████████▌| 191/200 [06:17<00:15,  1.75s/it]Running Inference:  96%|█████████▌| 192/200 [06:19<00:15,  1.88s/it]Running Inference:  96%|█████████▋| 193/200 [06:20<00:12,  1.79s/it]Running Inference:  97%|█████████▋| 194/200 [06:23<00:12,  2.03s/it]Running Inference:  98%|█████████▊| 195/200 [06:26<00:11,  2.21s/it]Running Inference:  98%|█████████▊| 196/200 [06:26<00:07,  1.75s/it]Running Inference:  98%|█████████▊| 197/200 [06:28<00:05,  1.68s/it]Running Inference:  99%|█████████▉| 198/200 [06:29<00:03,  1.70s/it]Running Inference: 100%|█████████▉| 199/200 [06:30<00:01,  1.36s/it]Running Inference: 100%|██████████| 200/200 [06:31<00:00,  1.27s/it]Running Inference: 100%|██████████| 200/200 [06:31<00:00,  1.96s/it]
2025-12-13 21:03:56,180 - INFO - Inference completed.
2025-12-13 21:03:56,203 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-13 21:03:56,204 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:03:56,239 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-13 21:03:56,240 - INFO - Metrics:
56.17
2025-12-13 21:03:56,241 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=triviaqa, ratio=0.5) on GPU 1


========================================
LongBench Task: gov_report
========================================
----------------------------------------
Task: gov_report | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:04:02,780 - INFO - Set deterministic seeds to 42
2025-12-13 21:04:02,780 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:04:02,780 - INFO - Starting evaluation run...
2025-12-13 21:04:02,781 - INFO - Output directory set to: longbenchresult
2025-12-13 21:04:02,781 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-13 21:04:02,781 - INFO - KV Press 'tova' setup.
2025-12-13 21:04:02,781 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:04:02,781 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.79it/s]
Device set to use cuda:0
2025-12-13 21:04:15,265 - INFO - Model pipeline loaded.
2025-12-13 21:04:15,266 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 1347.91 examples/s]Generating test split: 200 examples [00:00, 1336.27 examples/s]
2025-12-13 21:04:19,514 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:04:19,514 - INFO - Dataset processed with 200 entries.
2025-12-13 21:04:19,548 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:19<1:04:38, 19.49s/it]Running Inference:   1%|          | 2/200 [00:42<1:11:32, 21.68s/it]Running Inference:   2%|▏         | 3/200 [01:07<1:16:13, 23.22s/it]Running Inference:   2%|▏         | 4/200 [01:30<1:15:05, 22.99s/it]Running Inference:   2%|▎         | 5/200 [01:53<1:14:54, 23.05s/it]Running Inference:   3%|▎         | 6/200 [02:18<1:16:39, 23.71s/it]Running Inference:   4%|▎         | 7/200 [02:42<1:16:04, 23.65s/it]Running Inference:   4%|▍         | 8/200 [03:05<1:15:12, 23.50s/it]Running Inference:   4%|▍         | 9/200 [03:28<1:14:34, 23.43s/it]Running Inference:   5%|▌         | 10/200 [03:51<1:14:01, 23.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:14<1:13:17, 23.27s/it]Running Inference:   6%|▌         | 12/200 [04:37<1:12:39, 23.19s/it]Running Inference:   6%|▋         | 13/200 [04:40<53:02, 17.02s/it]  Running Inference:   7%|▋         | 14/200 [05:03<58:42, 18.94s/it]Running Inference:   8%|▊         | 15/200 [05:26<1:01:57, 20.09s/it]Running Inference:   8%|▊         | 16/200 [05:50<1:04:53, 21.16s/it]Running Inference:   8%|▊         | 17/200 [06:14<1:07:22, 22.09s/it]Running Inference:   9%|▉         | 18/200 [06:34<1:05:19, 21.53s/it]Running Inference:  10%|▉         | 19/200 [07:01<1:09:13, 22.95s/it]Running Inference:  10%|█         | 20/200 [07:24<1:08:59, 23.00s/it]Running Inference:  10%|█         | 21/200 [07:47<1:08:59, 23.13s/it]Running Inference:  11%|█         | 22/200 [08:11<1:09:15, 23.34s/it]Running Inference:  12%|█▏        | 23/200 [08:35<1:09:17, 23.49s/it]Running Inference:  12%|█▏        | 24/200 [09:01<1:10:58, 24.20s/it]Running Inference:  12%|█▎        | 25/200 [09:23<1:09:17, 23.76s/it]Running Inference:  13%|█▎        | 26/200 [09:47<1:08:22, 23.57s/it]Running Inference:  14%|█▎        | 27/200 [10:10<1:07:42, 23.48s/it]Running Inference:  14%|█▍        | 28/200 [10:33<1:06:39, 23.25s/it]Running Inference:  14%|█▍        | 29/200 [10:59<1:09:00, 24.22s/it]Running Inference:  15%|█▌        | 30/200 [11:23<1:08:04, 24.03s/it]Running Inference:  16%|█▌        | 31/200 [11:46<1:07:01, 23.80s/it]Running Inference:  16%|█▌        | 32/200 [12:10<1:06:52, 23.89s/it]Running Inference:  16%|█▋        | 33/200 [12:33<1:05:21, 23.48s/it]Running Inference:  17%|█▋        | 34/200 [12:56<1:05:15, 23.59s/it]Running Inference:  18%|█▊        | 35/200 [13:20<1:04:35, 23.49s/it]Running Inference:  18%|█▊        | 36/200 [13:44<1:04:48, 23.71s/it]Running Inference:  18%|█▊        | 37/200 [14:07<1:03:47, 23.48s/it]Running Inference:  19%|█▉        | 38/200 [14:31<1:03:37, 23.56s/it]Running Inference:  20%|█▉        | 39/200 [14:54<1:03:02, 23.49s/it]Running Inference:  20%|██        | 40/200 [14:59<47:35, 17.85s/it]  Running Inference:  20%|██        | 41/200 [15:22<51:52, 19.58s/it]Running Inference:  21%|██        | 42/200 [15:46<54:39, 20.76s/it]Running Inference:  22%|██▏       | 43/200 [16:09<56:38, 21.65s/it]Running Inference:  22%|██▏       | 44/200 [16:32<57:09, 21.98s/it]Running Inference:  22%|██▎       | 45/200 [16:55<57:26, 22.23s/it]Running Inference:  23%|██▎       | 46/200 [17:18<57:35, 22.44s/it]Running Inference:  24%|██▎       | 47/200 [17:21<42:04, 16.50s/it]Running Inference:  24%|██▍       | 48/200 [17:44<46:52, 18.50s/it]Running Inference:  24%|██▍       | 49/200 [18:07<50:06, 19.91s/it]Running Inference:  25%|██▌       | 50/200 [18:08<35:52, 14.35s/it]Running Inference:  26%|██▌       | 51/200 [18:31<41:47, 16.83s/it]Running Inference:  26%|██▌       | 52/200 [18:54<46:27, 18.83s/it]Running Inference:  26%|██▋       | 53/200 [19:22<52:33, 21.45s/it]Running Inference:  27%|██▋       | 54/200 [19:45<53:03, 21.80s/it]Running Inference:  28%|██▊       | 55/200 [20:08<53:34, 22.17s/it]Running Inference:  28%|██▊       | 56/200 [20:31<54:04, 22.53s/it]Running Inference:  28%|██▊       | 57/200 [20:36<40:52, 17.15s/it]Running Inference:  29%|██▉       | 58/200 [20:59<44:53, 18.97s/it]Running Inference:  30%|██▉       | 59/200 [21:22<47:36, 20.26s/it]Running Inference:  30%|███       | 60/200 [21:47<50:46, 21.76s/it]Running Inference:  30%|███       | 61/200 [22:11<51:31, 22.24s/it]Running Inference:  31%|███       | 62/200 [22:34<51:41, 22.47s/it]Running Inference:  32%|███▏      | 63/200 [22:56<51:26, 22.53s/it]Running Inference:  32%|███▏      | 64/200 [23:19<51:19, 22.64s/it]Running Inference:  32%|███▎      | 65/200 [23:22<37:28, 16.65s/it]Running Inference:  33%|███▎      | 66/200 [23:45<41:41, 18.67s/it]Running Inference:  34%|███▎      | 67/200 [24:09<44:44, 20.19s/it]Running Inference:  34%|███▍      | 68/200 [24:32<46:20, 21.07s/it]Running Inference:  34%|███▍      | 69/200 [24:55<47:22, 21.69s/it]Running Inference:  35%|███▌      | 70/200 [25:19<48:35, 22.42s/it]Running Inference:  36%|███▌      | 71/200 [25:43<49:01, 22.80s/it]Running Inference:  36%|███▌      | 72/200 [26:07<49:06, 23.02s/it]Running Inference:  36%|███▋      | 73/200 [26:31<49:46, 23.52s/it]Running Inference:  37%|███▋      | 74/200 [26:55<49:13, 23.44s/it]Running Inference:  38%|███▊      | 75/200 [27:18<48:42, 23.38s/it]Running Inference:  38%|███▊      | 76/200 [27:42<48:39, 23.55s/it]Running Inference:  38%|███▊      | 77/200 [28:05<48:19, 23.58s/it]Running Inference:  39%|███▉      | 78/200 [28:28<47:29, 23.36s/it]Running Inference:  40%|███▉      | 79/200 [28:51<46:56, 23.28s/it]Running Inference:  40%|████      | 80/200 [29:15<47:01, 23.51s/it]Running Inference:  40%|████      | 81/200 [29:39<46:38, 23.51s/it]Running Inference:  41%|████      | 82/200 [30:03<46:22, 23.58s/it]Running Inference:  42%|████▏     | 83/200 [30:25<45:25, 23.30s/it]Running Inference:  42%|████▏     | 84/200 [30:49<45:11, 23.38s/it]Running Inference:  42%|████▎     | 85/200 [31:13<45:02, 23.50s/it]Running Inference:  43%|████▎     | 86/200 [31:33<43:05, 22.68s/it]Running Inference:  44%|████▎     | 87/200 [31:57<43:02, 22.85s/it]Running Inference:  44%|████▍     | 88/200 [32:23<44:28, 23.82s/it]Running Inference:  44%|████▍     | 89/200 [32:46<43:38, 23.59s/it]Running Inference:  45%|████▌     | 90/200 [33:09<43:03, 23.49s/it]Running Inference:  46%|████▌     | 91/200 [33:33<42:42, 23.51s/it]Running Inference:  46%|████▌     | 92/200 [33:55<41:57, 23.31s/it]Running Inference:  46%|████▋     | 93/200 [34:19<41:36, 23.33s/it]Running Inference:  47%|████▋     | 94/200 [34:26<32:32, 18.42s/it]Running Inference:  48%|████▊     | 95/200 [34:50<35:22, 20.21s/it]Running Inference:  48%|████▊     | 96/200 [35:14<36:48, 21.24s/it]Running Inference:  48%|████▊     | 97/200 [35:37<37:15, 21.71s/it]Running Inference:  49%|████▉     | 98/200 [36:00<37:34, 22.10s/it]Running Inference:  50%|████▉     | 99/200 [36:23<37:40, 22.38s/it]Running Inference:  50%|█████     | 100/200 [36:47<38:02, 22.82s/it]Running Inference:  50%|█████     | 101/200 [37:09<37:40, 22.83s/it]Running Inference:  51%|█████     | 102/200 [37:33<37:49, 23.16s/it]Running Inference:  52%|█████▏    | 103/200 [37:56<37:16, 23.05s/it]Running Inference:  52%|█████▏    | 104/200 [38:20<37:09, 23.22s/it]Running Inference:  52%|█████▎    | 105/200 [38:43<36:41, 23.17s/it]Running Inference:  53%|█████▎    | 106/200 [39:06<36:06, 23.05s/it]Running Inference:  54%|█████▎    | 107/200 [39:30<36:17, 23.41s/it]Running Inference:  54%|█████▍    | 108/200 [39:53<35:35, 23.21s/it]Running Inference:  55%|█████▍    | 109/200 [39:55<25:45, 16.98s/it]Running Inference:  55%|█████▌    | 110/200 [40:18<28:20, 18.89s/it]Running Inference:  56%|█████▌    | 111/200 [40:42<30:21, 20.46s/it]Running Inference:  56%|█████▌    | 112/200 [40:59<28:17, 19.29s/it]Running Inference:  56%|█████▋    | 113/200 [41:23<29:48, 20.56s/it]Running Inference:  57%|█████▋    | 114/200 [41:45<30:29, 21.27s/it]Running Inference:  57%|█████▊    | 115/200 [42:13<32:58, 23.27s/it]Running Inference:  58%|█████▊    | 116/200 [42:36<32:23, 23.14s/it]Running Inference:  58%|█████▊    | 117/200 [42:59<31:49, 23.01s/it]Running Inference:  59%|█████▉    | 118/200 [43:20<30:43, 22.48s/it]Running Inference:  60%|█████▉    | 119/200 [43:45<31:28, 23.31s/it]Running Inference:  60%|██████    | 120/200 [44:09<31:01, 23.27s/it]Running Inference:  60%|██████    | 121/200 [44:32<30:44, 23.35s/it]Running Inference:  61%|██████    | 122/200 [44:55<30:03, 23.12s/it]Running Inference:  62%|██████▏   | 123/200 [45:18<29:32, 23.02s/it]Running Inference:  62%|██████▏   | 124/200 [45:41<29:10, 23.03s/it]Running Inference:  62%|██████▎   | 125/200 [46:04<28:54, 23.12s/it]Running Inference:  63%|██████▎   | 126/200 [46:26<28:18, 22.95s/it]Running Inference:  64%|██████▎   | 127/200 [46:49<27:53, 22.92s/it]Running Inference:  64%|██████▍   | 128/200 [47:12<27:19, 22.76s/it]Running Inference:  64%|██████▍   | 129/200 [47:34<26:51, 22.70s/it]Running Inference:  65%|██████▌   | 130/200 [47:57<26:30, 22.72s/it]Running Inference:  66%|██████▌   | 131/200 [48:20<26:05, 22.69s/it]Running Inference:  66%|██████▌   | 132/200 [48:44<26:08, 23.07s/it]Running Inference:  66%|██████▋   | 133/200 [49:06<25:36, 22.94s/it]Running Inference:  67%|██████▋   | 134/200 [49:25<23:43, 21.57s/it]Running Inference:  68%|██████▊   | 135/200 [49:28<17:23, 16.05s/it]Running Inference:  68%|██████▊   | 136/200 [49:53<19:55, 18.68s/it]Running Inference:  68%|██████▊   | 137/200 [50:14<20:25, 19.46s/it]Running Inference:  69%|██████▉   | 138/200 [50:39<22:00, 21.29s/it]Running Inference:  70%|██████▉   | 139/200 [51:03<22:18, 21.95s/it]Running Inference:  70%|███████   | 140/200 [51:26<22:13, 22.23s/it]Running Inference:  70%|███████   | 141/200 [51:49<22:01, 22.40s/it]Running Inference:  71%|███████   | 142/200 [52:12<21:54, 22.67s/it]Running Inference:  72%|███████▏  | 143/200 [52:26<19:01, 20.03s/it]Running Inference:  72%|███████▏  | 144/200 [52:48<19:27, 20.84s/it]Running Inference:  72%|███████▎  | 145/200 [53:13<20:05, 21.93s/it]Running Inference:  73%|███████▎  | 146/200 [53:36<19:58, 22.19s/it]Running Inference:  74%|███████▎  | 147/200 [54:03<20:59, 23.77s/it]Running Inference:  74%|███████▍  | 148/200 [54:27<20:28, 23.63s/it]Running Inference:  74%|███████▍  | 149/200 [54:50<20:06, 23.66s/it]Running Inference:  75%|███████▌  | 150/200 [55:14<19:38, 23.58s/it]Running Inference:  76%|███████▌  | 151/200 [55:37<19:11, 23.50s/it]Running Inference:  76%|███████▌  | 152/200 [55:59<18:32, 23.18s/it]Running Inference:  76%|███████▋  | 153/200 [56:23<18:09, 23.17s/it]Running Inference:  77%|███████▋  | 154/200 [56:45<17:40, 23.05s/it]Running Inference:  78%|███████▊  | 155/200 [57:09<17:20, 23.12s/it]Running Inference:  78%|███████▊  | 156/200 [57:32<17:03, 23.25s/it]Running Inference:  78%|███████▊  | 157/200 [57:55<16:30, 23.03s/it]Running Inference:  79%|███████▉  | 158/200 [58:19<16:23, 23.41s/it]Running Inference:  80%|███████▉  | 159/200 [58:44<16:17, 23.84s/it]Running Inference:  80%|████████  | 160/200 [59:08<15:53, 23.85s/it]Running Inference:  80%|████████  | 161/200 [59:22<13:34, 20.89s/it]Running Inference:  81%|████████  | 162/200 [59:44<13:32, 21.38s/it]Running Inference:  82%|████████▏ | 163/200 [1:00:07<13:25, 21.78s/it]Running Inference:  82%|████████▏ | 164/200 [1:00:32<13:37, 22.70s/it]Running Inference:  82%|████████▎ | 165/200 [1:00:54<13:11, 22.61s/it]Running Inference:  83%|████████▎ | 166/200 [1:01:17<12:46, 22.56s/it]Running Inference:  84%|████████▎ | 167/200 [1:01:40<12:33, 22.84s/it]Running Inference:  84%|████████▍ | 168/200 [1:02:03<12:10, 22.83s/it]Running Inference:  84%|████████▍ | 169/200 [1:02:26<11:46, 22.78s/it]Running Inference:  85%|████████▌ | 170/200 [1:02:49<11:25, 22.85s/it]Running Inference:  86%|████████▌ | 171/200 [1:03:14<11:21, 23.52s/it]Running Inference:  86%|████████▌ | 172/200 [1:03:34<10:33, 22.61s/it]Running Inference:  86%|████████▋ | 173/200 [1:03:57<10:16, 22.83s/it]Running Inference:  87%|████████▋ | 174/200 [1:04:21<09:55, 22.90s/it]Running Inference:  88%|████████▊ | 175/200 [1:04:43<09:30, 22.81s/it]Running Inference:  88%|████████▊ | 176/200 [1:05:20<10:45, 26.92s/it]Running Inference:  88%|████████▊ | 177/200 [1:05:54<11:08, 29.04s/it]Running Inference:  89%|████████▉ | 178/200 [1:06:13<09:32, 26.02s/it]Running Inference:  90%|████████▉ | 179/200 [1:06:35<08:44, 24.97s/it]Running Inference:  90%|█████████ | 180/200 [1:06:59<08:12, 24.63s/it]Running Inference:  90%|█████████ | 181/200 [1:07:22<07:39, 24.20s/it]Running Inference:  91%|█████████ | 182/200 [1:07:43<06:58, 23.23s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:15<07:21, 25.97s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:36<06:28, 24.29s/it]Running Inference:  92%|█████████▎| 185/200 [1:08:59<06:00, 24.06s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:22<05:31, 23.65s/it]Running Inference:  94%|█████████▎| 187/200 [1:09:45<05:03, 23.37s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:08<04:41, 23.45s/it]Running Inference:  94%|█████████▍| 189/200 [1:10:31<04:15, 23.27s/it]Running Inference:  95%|█████████▌| 190/200 [1:10:54<03:51, 23.12s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:17<03:28, 23.17s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:45<03:15, 24.48s/it]Running Inference:  96%|█████████▋| 193/200 [1:12:05<02:42, 23.21s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:29<02:20, 23.35s/it]Running Inference:  98%|█████████▊| 195/200 [1:12:51<01:55, 23.02s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:06<01:22, 20.57s/it]Running Inference:  98%|█████████▊| 197/200 [1:13:30<01:04, 21.54s/it]Running Inference:  99%|█████████▉| 198/200 [1:13:56<00:45, 22.86s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:18<00:22, 22.71s/it]Running Inference: 100%|██████████| 200/200 [1:14:41<00:00, 22.76s/it]Running Inference: 100%|██████████| 200/200 [1:14:41<00:00, 22.41s/it]
2025-12-13 22:19:00,902 - INFO - Inference completed.
2025-12-13 22:19:00,936 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-13 22:19:00,936 - INFO - Calculating metrics for dataset: longbench
2025-12-13 22:19:19,104 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-13 22:19:19,104 - INFO - Metrics:
13.79
2025-12-13 22:19:19,105 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report, ratio=0.1) on GPU 1

----------------------------------------
Task: gov_report | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 22:19:26,037 - INFO - Set deterministic seeds to 42
2025-12-13 22:19:26,038 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 22:19:26,038 - INFO - Starting evaluation run...
2025-12-13 22:19:26,038 - INFO - Output directory set to: longbenchresult
2025-12-13 22:19:26,038 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-13 22:19:26,038 - INFO - KV Press 'tova' setup.
2025-12-13 22:19:26,038 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 22:19:26,038 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.49it/s]
Device set to use cuda:0
2025-12-13 22:19:41,894 - INFO - Model pipeline loaded.
2025-12-13 22:19:41,894 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-13 22:19:47,105 - INFO - Dataset loaded with 200 entries.
2025-12-13 22:19:47,105 - INFO - Dataset processed with 200 entries.
2025-12-13 22:19:47,137 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:19:56, 24.10s/it]Running Inference:   1%|          | 2/200 [00:41<1:07:23, 20.42s/it]Running Inference:   2%|▏         | 3/200 [01:06<1:13:31, 22.39s/it]Running Inference:   2%|▏         | 4/200 [01:29<1:13:27, 22.49s/it]Running Inference:   2%|▎         | 5/200 [01:52<1:13:54, 22.74s/it]Running Inference:   3%|▎         | 6/200 [02:17<1:15:38, 23.39s/it]Running Inference:   4%|▎         | 7/200 [02:40<1:15:25, 23.45s/it]Running Inference:   4%|▍         | 8/200 [03:03<1:14:50, 23.39s/it]Running Inference:   4%|▍         | 9/200 [03:27<1:14:24, 23.37s/it]Running Inference:   5%|▌         | 10/200 [03:38<1:01:52, 19.54s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:46<50:15, 15.95s/it]  Running Inference:   6%|▌         | 12/200 [04:09<56:46, 18.12s/it]Running Inference:   6%|▋         | 13/200 [04:33<1:02:00, 19.90s/it]Running Inference:   7%|▋         | 14/200 [04:56<1:05:00, 20.97s/it]Running Inference:   8%|▊         | 15/200 [05:15<1:02:29, 20.27s/it]Running Inference:   8%|▊         | 16/200 [05:38<1:05:19, 21.30s/it]Running Inference:   8%|▊         | 17/200 [06:03<1:07:42, 22.20s/it]Running Inference:   9%|▉         | 18/200 [06:23<1:05:15, 21.51s/it]Running Inference:  10%|▉         | 19/200 [06:49<1:08:51, 22.82s/it]Running Inference:  10%|█         | 20/200 [07:12<1:08:48, 22.93s/it]Running Inference:  10%|█         | 21/200 [07:35<1:08:58, 23.12s/it]Running Inference:  11%|█         | 22/200 [07:59<1:09:14, 23.34s/it]Running Inference:  12%|█▏        | 23/200 [08:23<1:09:22, 23.52s/it]Running Inference:  12%|█▏        | 24/200 [08:49<1:10:45, 24.12s/it]Running Inference:  12%|█▎        | 25/200 [09:11<1:09:15, 23.74s/it]Running Inference:  13%|█▎        | 26/200 [09:35<1:08:25, 23.59s/it]Running Inference:  14%|█▎        | 27/200 [09:58<1:07:40, 23.47s/it]Running Inference:  14%|█▍        | 28/200 [10:21<1:06:42, 23.27s/it]Running Inference:  14%|█▍        | 29/200 [10:47<1:08:48, 24.14s/it]Running Inference:  15%|█▌        | 30/200 [11:11<1:07:59, 24.00s/it]Running Inference:  16%|█▌        | 31/200 [11:34<1:07:04, 23.81s/it]Running Inference:  16%|█▌        | 32/200 [11:58<1:07:01, 23.94s/it]Running Inference:  16%|█▋        | 33/200 [12:21<1:05:32, 23.55s/it]Running Inference:  17%|█▋        | 34/200 [12:45<1:05:29, 23.67s/it]Running Inference:  18%|█▊        | 35/200 [13:08<1:04:50, 23.58s/it]Running Inference:  18%|█▊        | 36/200 [13:32<1:05:02, 23.79s/it]Running Inference:  18%|█▊        | 37/200 [13:56<1:04:04, 23.59s/it]Running Inference:  19%|█▉        | 38/200 [14:19<1:03:53, 23.67s/it]Running Inference:  20%|█▉        | 39/200 [14:43<1:03:10, 23.55s/it]Running Inference:  20%|██        | 40/200 [14:47<47:31, 17.82s/it]  Running Inference:  20%|██        | 41/200 [15:11<51:55, 19.59s/it]Running Inference:  21%|██        | 42/200 [15:34<54:46, 20.80s/it]Running Inference:  22%|██▏       | 43/200 [15:58<56:48, 21.71s/it]Running Inference:  22%|██▏       | 44/200 [16:21<57:23, 22.07s/it]Running Inference:  22%|██▎       | 45/200 [16:44<57:39, 22.32s/it]Running Inference:  23%|██▎       | 46/200 [17:07<57:49, 22.53s/it]Running Inference:  24%|██▎       | 47/200 [17:10<42:12, 16.55s/it]Running Inference:  24%|██▍       | 48/200 [17:33<47:01, 18.56s/it]Running Inference:  24%|██▍       | 49/200 [17:56<50:15, 19.97s/it]Running Inference:  25%|██▌       | 50/200 [18:19<52:18, 20.92s/it]Running Inference:  26%|██▌       | 51/200 [18:38<50:01, 20.14s/it]Running Inference:  26%|██▌       | 52/200 [19:01<52:08, 21.14s/it]Running Inference:  26%|██▋       | 53/200 [19:28<56:13, 22.95s/it]Running Inference:  27%|██▋       | 54/200 [19:51<55:40, 22.88s/it]Running Inference:  28%|██▊       | 55/200 [20:14<55:25, 22.93s/it]Running Inference:  28%|██▊       | 56/200 [20:38<55:27, 23.11s/it]Running Inference:  28%|██▊       | 57/200 [20:42<41:43, 17.51s/it]Running Inference:  29%|██▉       | 58/200 [21:01<42:38, 18.02s/it]Running Inference:  30%|██▉       | 59/200 [21:25<46:07, 19.62s/it]Running Inference:  30%|███       | 60/200 [21:50<49:31, 21.22s/it]Running Inference:  30%|███       | 61/200 [22:13<50:39, 21.87s/it]Running Inference:  31%|███       | 62/200 [22:27<45:12, 19.66s/it]Running Inference:  32%|███▏      | 63/200 [22:50<46:57, 20.56s/it]Running Inference:  32%|███▏      | 64/200 [23:13<48:14, 21.29s/it]Running Inference:  32%|███▎      | 65/200 [23:16<35:20, 15.71s/it]Running Inference:  33%|███▎      | 66/200 [23:39<39:58, 17.90s/it]Running Inference:  34%|███▎      | 67/200 [24:03<43:34, 19.66s/it]Running Inference:  34%|███▍      | 68/200 [24:26<45:37, 20.74s/it]Running Inference:  34%|███▍      | 69/200 [24:49<46:57, 21.51s/it]Running Inference:  35%|███▌      | 70/200 [25:13<48:18, 22.30s/it]Running Inference:  36%|███▌      | 71/200 [25:37<48:54, 22.75s/it]Running Inference:  36%|███▌      | 72/200 [25:58<47:04, 22.07s/it]Running Inference:  36%|███▋      | 73/200 [26:22<48:16, 22.81s/it]Running Inference:  37%|███▋      | 74/200 [26:45<48:06, 22.91s/it]Running Inference:  38%|███▊      | 75/200 [27:08<47:39, 22.87s/it]Running Inference:  38%|███▊      | 76/200 [27:32<47:58, 23.22s/it]Running Inference:  38%|███▊      | 77/200 [27:56<47:53, 23.36s/it]Running Inference:  39%|███▉      | 78/200 [28:19<47:12, 23.22s/it]Running Inference:  40%|███▉      | 79/200 [28:42<46:46, 23.19s/it]Running Inference:  40%|████      | 80/200 [29:06<46:53, 23.44s/it]Running Inference:  40%|████      | 81/200 [29:29<46:30, 23.45s/it]Running Inference:  41%|████      | 82/200 [29:53<46:16, 23.53s/it]Running Inference:  42%|████▏     | 83/200 [30:16<45:21, 23.26s/it]Running Inference:  42%|████▏     | 84/200 [30:39<45:09, 23.36s/it]Running Inference:  42%|████▎     | 85/200 [31:03<45:01, 23.49s/it]Running Inference:  43%|████▎     | 86/200 [31:26<44:22, 23.35s/it]Running Inference:  44%|████▎     | 87/200 [31:49<43:54, 23.32s/it]Running Inference:  44%|████▍     | 88/200 [32:15<44:52, 24.04s/it]Running Inference:  44%|████▍     | 89/200 [32:38<43:56, 23.76s/it]Running Inference:  45%|████▌     | 90/200 [33:01<43:20, 23.64s/it]Running Inference:  46%|████▌     | 91/200 [33:25<43:00, 23.67s/it]Running Inference:  46%|████▌     | 92/200 [33:48<42:09, 23.42s/it]Running Inference:  46%|████▋     | 93/200 [34:11<41:46, 23.42s/it]Running Inference:  47%|████▋     | 94/200 [34:34<40:58, 23.19s/it]Running Inference:  48%|████▊     | 95/200 [34:58<41:10, 23.53s/it]Running Inference:  48%|████▊     | 96/200 [35:22<40:49, 23.56s/it]Running Inference:  48%|████▊     | 97/200 [35:45<40:03, 23.34s/it]Running Inference:  49%|████▉     | 98/200 [36:05<37:58, 22.34s/it]Running Inference:  50%|████▉     | 99/200 [36:28<37:58, 22.56s/it]Running Inference:  50%|█████     | 100/200 [36:52<38:16, 22.97s/it]Running Inference:  50%|█████     | 101/200 [37:15<37:50, 22.93s/it]Running Inference:  51%|█████     | 102/200 [37:39<37:57, 23.24s/it]Running Inference:  52%|█████▏    | 103/200 [38:01<37:22, 23.12s/it]Running Inference:  52%|█████▏    | 104/200 [38:25<37:13, 23.27s/it]Running Inference:  52%|█████▎    | 105/200 [38:48<36:43, 23.20s/it]Running Inference:  53%|█████▎    | 106/200 [39:11<36:09, 23.08s/it]Running Inference:  54%|█████▎    | 107/200 [39:35<36:18, 23.42s/it]Running Inference:  54%|█████▍    | 108/200 [39:58<35:36, 23.22s/it]Running Inference:  55%|█████▍    | 109/200 [40:00<25:47, 17.01s/it]Running Inference:  55%|█████▌    | 110/200 [40:24<28:23, 18.93s/it]Running Inference:  56%|█████▌    | 111/200 [40:48<30:24, 20.50s/it]Running Inference:  56%|█████▌    | 112/200 [41:11<31:07, 21.22s/it]Running Inference:  56%|█████▋    | 113/200 [41:21<26:04, 17.98s/it]Running Inference:  57%|█████▋    | 114/200 [41:45<28:15, 19.71s/it]Running Inference:  57%|█████▊    | 115/200 [42:13<31:14, 22.05s/it]Running Inference:  58%|█████▊    | 116/200 [42:35<31:13, 22.30s/it]Running Inference:  58%|█████▊    | 117/200 [42:58<31:02, 22.44s/it]Running Inference:  59%|█████▉    | 118/200 [43:21<30:51, 22.57s/it]Running Inference:  60%|█████▉    | 119/200 [43:46<31:27, 23.30s/it]Running Inference:  60%|██████    | 120/200 [43:57<26:13, 19.67s/it]Running Inference:  60%|██████    | 121/200 [44:21<27:32, 20.91s/it]Running Inference:  61%|██████    | 122/200 [44:44<27:56, 21.50s/it]Running Inference:  62%|██████▏   | 123/200 [45:07<28:11, 21.97s/it]Running Inference:  62%|██████▏   | 124/200 [45:30<28:21, 22.39s/it]Running Inference:  62%|██████▎   | 125/200 [45:54<28:25, 22.75s/it]Running Inference:  63%|██████▎   | 126/200 [46:17<28:05, 22.77s/it]Running Inference:  64%|██████▎   | 127/200 [46:37<26:44, 21.98s/it]Running Inference:  64%|██████▍   | 128/200 [47:00<26:37, 22.19s/it]Running Inference:  64%|██████▍   | 129/200 [47:23<26:29, 22.39s/it]Running Inference:  65%|██████▌   | 130/200 [47:43<25:31, 21.88s/it]Running Inference:  66%|██████▌   | 131/200 [48:06<25:30, 22.18s/it]Running Inference:  66%|██████▌   | 132/200 [48:30<25:50, 22.81s/it]Running Inference:  66%|██████▋   | 133/200 [48:53<25:30, 22.84s/it]Running Inference:  67%|██████▋   | 134/200 [49:12<23:54, 21.74s/it]Running Inference:  68%|██████▊   | 135/200 [49:15<17:23, 16.05s/it]Running Inference:  68%|██████▊   | 136/200 [49:40<19:54, 18.66s/it]Running Inference:  68%|██████▊   | 137/200 [50:04<21:24, 20.39s/it]Running Inference:  69%|██████▉   | 138/200 [50:30<22:40, 21.94s/it]Running Inference:  70%|██████▉   | 139/200 [50:54<22:51, 22.48s/it]Running Inference:  70%|███████   | 140/200 [51:17<22:41, 22.69s/it]Running Inference:  70%|███████   | 141/200 [51:40<22:25, 22.81s/it]Running Inference:  71%|███████   | 142/200 [52:04<22:16, 23.04s/it]Running Inference:  72%|███████▏  | 143/200 [52:27<22:06, 23.28s/it]Running Inference:  72%|███████▏  | 144/200 [52:50<21:40, 23.22s/it]Running Inference:  72%|███████▎  | 145/200 [53:15<21:38, 23.60s/it]Running Inference:  73%|███████▎  | 146/200 [53:38<21:06, 23.45s/it]Running Inference:  74%|███████▎  | 147/200 [54:05<21:43, 24.59s/it]Running Inference:  74%|███████▍  | 148/200 [54:29<21:03, 24.29s/it]Running Inference:  74%|███████▍  | 149/200 [54:53<20:34, 24.21s/it]Running Inference:  75%|███████▌  | 150/200 [55:17<20:02, 24.05s/it]Running Inference:  76%|███████▌  | 151/200 [55:40<19:32, 23.92s/it]Running Inference:  76%|███████▌  | 152/200 [56:01<18:18, 22.89s/it]Running Inference:  76%|███████▋  | 153/200 [56:24<18:04, 23.07s/it]Running Inference:  77%|███████▋  | 154/200 [56:47<17:41, 23.07s/it]Running Inference:  78%|███████▊  | 155/200 [57:11<17:25, 23.23s/it]Running Inference:  78%|███████▊  | 156/200 [57:35<17:10, 23.42s/it]Running Inference:  78%|███████▊  | 157/200 [57:58<16:38, 23.23s/it]Running Inference:  79%|███████▉  | 158/200 [58:22<16:30, 23.59s/it]Running Inference:  80%|███████▉  | 159/200 [58:47<16:22, 23.95s/it]Running Inference:  80%|████████  | 160/200 [59:11<16:00, 24.02s/it]Running Inference:  80%|████████  | 161/200 [59:35<15:39, 24.10s/it]Running Inference:  81%|████████  | 162/200 [59:58<15:01, 23.72s/it]Running Inference:  82%|████████▏ | 163/200 [1:00:21<14:29, 23.51s/it]Running Inference:  82%|████████▏ | 164/200 [1:00:45<14:05, 23.49s/it]Running Inference:  82%|████████▎ | 165/200 [1:01:07<13:33, 23.25s/it]Running Inference:  83%|████████▎ | 166/200 [1:01:30<13:05, 23.09s/it]Running Inference:  84%|████████▎ | 167/200 [1:01:54<12:52, 23.39s/it]Running Inference:  84%|████████▍ | 168/200 [1:02:14<11:51, 22.24s/it]Running Inference:  84%|████████▍ | 169/200 [1:02:36<11:35, 22.44s/it]Running Inference:  85%|████████▌ | 170/200 [1:03:00<11:20, 22.70s/it]Running Inference:  86%|████████▌ | 171/200 [1:03:25<11:18, 23.40s/it]Running Inference:  86%|████████▌ | 172/200 [1:03:48<10:52, 23.29s/it]Running Inference:  86%|████████▋ | 173/200 [1:04:12<10:31, 23.40s/it]Running Inference:  87%|████████▋ | 174/200 [1:04:35<10:08, 23.40s/it]Running Inference:  88%|████████▊ | 175/200 [1:04:58<09:41, 23.25s/it]Running Inference:  88%|████████▊ | 176/200 [1:05:34<10:49, 27.06s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:07<11:06, 29.00s/it]Running Inference:  89%|████████▉ | 178/200 [1:06:31<10:05, 27.52s/it]Running Inference:  90%|████████▉ | 179/200 [1:06:54<09:08, 26.12s/it]Running Inference:  90%|█████████ | 180/200 [1:07:18<08:30, 25.54s/it]Running Inference:  90%|█████████ | 181/200 [1:07:42<07:53, 24.94s/it]Running Inference:  91%|█████████ | 182/200 [1:08:06<07:26, 24.83s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:36<07:24, 26.17s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:59<06:42, 25.18s/it]Running Inference:  92%|█████████▎| 185/200 [1:09:21<06:05, 24.39s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:44<05:35, 23.97s/it]Running Inference:  94%|█████████▎| 187/200 [1:10:07<05:08, 23.70s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:31<04:45, 23.78s/it]Running Inference:  94%|█████████▍| 189/200 [1:10:54<04:19, 23.60s/it]Running Inference:  95%|█████████▌| 190/200 [1:11:17<03:54, 23.44s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:41<03:31, 23.49s/it]Running Inference:  96%|█████████▌| 192/200 [1:12:08<03:17, 24.64s/it]Running Inference:  96%|█████████▋| 193/200 [1:12:32<02:49, 24.21s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:56<02:24, 24.16s/it]Running Inference:  98%|█████████▊| 195/200 [1:13:18<01:58, 23.69s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:42<01:34, 23.57s/it]Running Inference:  98%|█████████▊| 197/200 [1:14:06<01:11, 23.74s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:31<00:48, 24.36s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:54<00:23, 23.86s/it]Running Inference: 100%|██████████| 200/200 [1:15:17<00:00, 23.66s/it]Running Inference: 100%|██████████| 200/200 [1:15:17<00:00, 22.59s/it]
2025-12-13 23:35:05,035 - INFO - Inference completed.
2025-12-13 23:35:05,068 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-13 23:35:05,068 - INFO - Calculating metrics for dataset: longbench
2025-12-13 23:35:22,804 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-13 23:35:22,804 - INFO - Metrics:
13.9
2025-12-13 23:35:22,805 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report, ratio=0.2) on GPU 1

----------------------------------------
Task: gov_report | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 23:35:29,714 - INFO - Set deterministic seeds to 42
2025-12-13 23:35:29,714 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 23:35:29,714 - INFO - Starting evaluation run...
2025-12-13 23:35:29,714 - INFO - Output directory set to: longbenchresult
2025-12-13 23:35:29,715 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-13 23:35:29,715 - INFO - KV Press 'tova' setup.
2025-12-13 23:35:29,715 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 23:35:29,715 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.27it/s]
Device set to use cuda:0
2025-12-13 23:35:43,469 - INFO - Model pipeline loaded.
2025-12-13 23:35:43,469 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-13 23:35:48,994 - INFO - Dataset loaded with 200 entries.
2025-12-13 23:35:48,994 - INFO - Dataset processed with 200 entries.
2025-12-13 23:35:49,028 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:21:23, 24.54s/it]Running Inference:   1%|          | 2/200 [00:48<1:19:19, 24.04s/it]Running Inference:   2%|▏         | 3/200 [01:13<1:20:31, 24.52s/it]Running Inference:   2%|▏         | 4/200 [01:36<1:18:15, 23.95s/it]Running Inference:   2%|▎         | 5/200 [02:00<1:17:31, 23.85s/it]Running Inference:   3%|▎         | 6/200 [02:25<1:18:27, 24.27s/it]Running Inference:   4%|▎         | 7/200 [02:49<1:17:44, 24.17s/it]Running Inference:   4%|▍         | 8/200 [02:54<58:18, 18.22s/it]  Running Inference:   4%|▍         | 9/200 [03:18<1:03:31, 19.96s/it]Running Inference:   5%|▌         | 10/200 [03:33<58:38, 18.52s/it] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:41<48:09, 15.29s/it]Running Inference:   6%|▌         | 12/200 [04:05<55:45, 17.80s/it]Running Inference:   6%|▋         | 13/200 [04:08<41:21, 13.27s/it]Running Inference:   7%|▋         | 14/200 [04:31<51:07, 16.49s/it]Running Inference:   8%|▊         | 15/200 [04:55<57:37, 18.69s/it]Running Inference:   8%|▊         | 16/200 [05:19<1:02:19, 20.32s/it]Running Inference:   8%|▊         | 17/200 [05:44<1:06:02, 21.66s/it]Running Inference:   9%|▉         | 18/200 [06:06<1:06:14, 21.84s/it]Running Inference:  10%|▉         | 19/200 [06:32<1:09:38, 23.08s/it]Running Inference:  10%|█         | 20/200 [06:56<1:09:45, 23.25s/it]Running Inference:  10%|█         | 21/200 [07:20<1:10:01, 23.47s/it]Running Inference:  11%|█         | 22/200 [07:44<1:10:21, 23.71s/it]Running Inference:  12%|█▏        | 23/200 [08:08<1:09:57, 23.72s/it]Running Inference:  12%|█▏        | 24/200 [08:34<1:11:16, 24.30s/it]Running Inference:  12%|█▎        | 25/200 [08:57<1:09:58, 23.99s/it]Running Inference:  13%|█▎        | 26/200 [09:21<1:09:19, 23.90s/it]Running Inference:  14%|█▎        | 27/200 [09:44<1:08:41, 23.82s/it]Running Inference:  14%|█▍        | 28/200 [10:07<1:07:46, 23.64s/it]Running Inference:  14%|█▍        | 29/200 [10:34<1:09:36, 24.42s/it]Running Inference:  15%|█▌        | 30/200 [10:58<1:08:56, 24.33s/it]Running Inference:  16%|█▌        | 31/200 [11:22<1:08:06, 24.18s/it]Running Inference:  16%|█▌        | 32/200 [11:31<55:03, 19.66s/it]  Running Inference:  16%|█▋        | 33/200 [11:54<57:34, 20.68s/it]Running Inference:  17%|█▋        | 34/200 [12:18<1:00:18, 21.80s/it]Running Inference:  18%|█▊        | 35/200 [12:42<1:01:36, 22.40s/it]Running Inference:  18%|█▊        | 36/200 [13:07<1:03:09, 23.10s/it]Running Inference:  18%|█▊        | 37/200 [13:30<1:03:09, 23.25s/it]Running Inference:  19%|█▉        | 38/200 [13:55<1:03:34, 23.55s/it]Running Inference:  20%|█▉        | 39/200 [14:14<59:43, 22.26s/it]  Running Inference:  20%|██        | 40/200 [14:38<1:00:46, 22.79s/it]Running Inference:  20%|██        | 41/200 [15:02<1:01:28, 23.20s/it]Running Inference:  21%|██        | 42/200 [15:26<1:01:46, 23.46s/it]Running Inference:  22%|██▏       | 43/200 [15:50<1:01:59, 23.69s/it]Running Inference:  22%|██▏       | 44/200 [16:14<1:01:18, 23.58s/it]Running Inference:  22%|██▎       | 45/200 [16:33<57:46, 22.37s/it]  Running Inference:  23%|██▎       | 46/200 [16:57<58:16, 22.70s/it]Running Inference:  24%|██▎       | 47/200 [16:59<42:31, 16.68s/it]Running Inference:  24%|██▍       | 48/200 [17:23<47:36, 18.79s/it]Running Inference:  24%|██▍       | 49/200 [17:43<48:22, 19.22s/it]Running Inference:  25%|██▌       | 50/200 [18:07<51:19, 20.53s/it]Running Inference:  26%|██▌       | 51/200 [18:25<49:17, 19.85s/it]Running Inference:  26%|██▌       | 52/200 [18:49<51:59, 21.08s/it]Running Inference:  26%|██▋       | 53/200 [19:16<56:07, 22.91s/it]Running Inference:  27%|██▋       | 54/200 [19:39<55:55, 22.99s/it]Running Inference:  28%|██▊       | 55/200 [20:03<55:55, 23.14s/it]Running Inference:  28%|██▊       | 56/200 [20:27<56:07, 23.38s/it]Running Inference:  28%|██▊       | 57/200 [20:32<42:21, 17.77s/it]Running Inference:  29%|██▉       | 58/200 [20:55<46:19, 19.57s/it]Running Inference:  30%|██▉       | 59/200 [21:19<48:59, 20.85s/it]Running Inference:  30%|███       | 60/200 [21:44<51:42, 22.16s/it]Running Inference:  30%|███       | 61/200 [22:08<52:28, 22.65s/it]Running Inference:  31%|███       | 62/200 [22:32<52:41, 22.91s/it]Running Inference:  32%|███▏      | 63/200 [22:55<52:28, 22.98s/it]Running Inference:  32%|███▏      | 64/200 [23:18<52:23, 23.11s/it]Running Inference:  32%|███▎      | 65/200 [23:42<52:20, 23.26s/it]Running Inference:  33%|███▎      | 66/200 [24:06<52:29, 23.50s/it]Running Inference:  34%|███▎      | 67/200 [24:30<52:34, 23.72s/it]Running Inference:  34%|███▍      | 68/200 [24:54<52:07, 23.69s/it]Running Inference:  34%|███▍      | 69/200 [25:18<51:47, 23.72s/it]Running Inference:  35%|███▌      | 70/200 [25:42<51:57, 23.98s/it]Running Inference:  36%|███▌      | 71/200 [26:06<51:42, 24.05s/it]Running Inference:  36%|███▌      | 72/200 [26:30<51:14, 24.02s/it]Running Inference:  36%|███▋      | 73/200 [26:55<51:28, 24.32s/it]Running Inference:  37%|███▋      | 74/200 [27:19<50:38, 24.11s/it]Running Inference:  38%|███▊      | 75/200 [27:43<49:59, 23.99s/it]Running Inference:  38%|███▊      | 76/200 [27:48<37:44, 18.26s/it]Running Inference:  38%|███▊      | 77/200 [28:12<41:04, 20.04s/it]Running Inference:  39%|███▉      | 78/200 [28:35<42:45, 21.03s/it]Running Inference:  40%|███▉      | 79/200 [28:59<43:59, 21.81s/it]Running Inference:  40%|████      | 80/200 [29:23<45:15, 22.63s/it]Running Inference:  40%|████      | 81/200 [29:47<45:40, 23.03s/it]Running Inference:  41%|████      | 82/200 [30:11<45:58, 23.37s/it]Running Inference:  42%|████▏     | 83/200 [30:14<33:19, 17.09s/it]Running Inference:  42%|████▏     | 84/200 [30:38<37:05, 19.18s/it]Running Inference:  42%|████▎     | 85/200 [31:02<39:41, 20.70s/it]Running Inference:  43%|████▎     | 86/200 [31:20<37:39, 19.82s/it]Running Inference:  44%|████▎     | 87/200 [31:44<39:32, 21.00s/it]Running Inference:  44%|████▍     | 88/200 [32:10<41:56, 22.47s/it]Running Inference:  44%|████▍     | 89/200 [32:33<42:11, 22.80s/it]Running Inference:  45%|████▌     | 90/200 [32:57<42:19, 23.08s/it]Running Inference:  46%|████▌     | 91/200 [33:21<42:29, 23.39s/it]Running Inference:  46%|████▌     | 92/200 [33:44<42:04, 23.38s/it]Running Inference:  46%|████▋     | 93/200 [34:08<42:00, 23.55s/it]Running Inference:  47%|████▋     | 94/200 [34:31<41:24, 23.43s/it]Running Inference:  48%|████▊     | 95/200 [34:56<41:45, 23.86s/it]Running Inference:  48%|████▊     | 96/200 [35:20<41:29, 23.93s/it]Running Inference:  48%|████▊     | 97/200 [35:44<40:48, 23.78s/it]Running Inference:  49%|████▉     | 98/200 [36:07<40:17, 23.70s/it]Running Inference:  50%|████▉     | 99/200 [36:31<39:47, 23.64s/it]Running Inference:  50%|█████     | 100/200 [36:55<39:42, 23.83s/it]Running Inference:  50%|█████     | 101/200 [37:18<39:01, 23.65s/it]Running Inference:  51%|█████     | 102/200 [37:43<38:58, 23.86s/it]Running Inference:  52%|█████▏    | 103/200 [38:06<38:15, 23.66s/it]Running Inference:  52%|█████▏    | 104/200 [38:30<38:01, 23.76s/it]Running Inference:  52%|█████▎    | 105/200 [38:32<27:18, 17.25s/it]Running Inference:  53%|█████▎    | 106/200 [38:55<29:48, 19.03s/it]Running Inference:  54%|█████▎    | 107/200 [39:20<32:05, 20.70s/it]Running Inference:  54%|█████▍    | 108/200 [39:43<32:51, 21.43s/it]Running Inference:  55%|█████▍    | 109/200 [40:06<33:18, 21.97s/it]Running Inference:  55%|█████▌    | 110/200 [40:30<33:45, 22.51s/it]Running Inference:  56%|█████▌    | 111/200 [40:54<34:17, 23.12s/it]Running Inference:  56%|█████▌    | 112/200 [41:18<33:58, 23.16s/it]Running Inference:  56%|█████▋    | 113/200 [41:42<33:54, 23.39s/it]Running Inference:  57%|█████▋    | 114/200 [42:03<32:39, 22.78s/it]Running Inference:  57%|█████▊    | 115/200 [42:30<34:16, 24.19s/it]Running Inference:  58%|█████▊    | 116/200 [42:54<33:28, 23.91s/it]Running Inference:  58%|█████▊    | 117/200 [43:17<32:45, 23.67s/it]Running Inference:  59%|█████▉    | 118/200 [43:40<32:10, 23.54s/it]Running Inference:  60%|█████▉    | 119/200 [44:05<32:25, 24.02s/it]Running Inference:  60%|██████    | 120/200 [44:29<31:50, 23.89s/it]Running Inference:  60%|██████    | 121/200 [44:53<31:32, 23.96s/it]Running Inference:  61%|██████    | 122/200 [45:16<30:50, 23.73s/it]Running Inference:  62%|██████▏   | 123/200 [45:39<30:19, 23.63s/it]Running Inference:  62%|██████▏   | 124/200 [46:03<29:56, 23.64s/it]Running Inference:  62%|██████▎   | 125/200 [46:27<29:39, 23.72s/it]Running Inference:  63%|██████▎   | 126/200 [46:50<29:02, 23.55s/it]Running Inference:  64%|██████▎   | 127/200 [47:12<28:08, 23.13s/it]Running Inference:  64%|██████▍   | 128/200 [47:35<27:42, 23.09s/it]Running Inference:  64%|██████▍   | 129/200 [47:59<27:21, 23.11s/it]Running Inference:  65%|██████▌   | 130/200 [48:22<27:03, 23.19s/it]Running Inference:  66%|██████▌   | 131/200 [48:45<26:41, 23.21s/it]Running Inference:  66%|██████▌   | 132/200 [49:10<26:46, 23.63s/it]Running Inference:  66%|██████▋   | 133/200 [49:33<26:15, 23.51s/it]Running Inference:  67%|██████▋   | 134/200 [49:51<24:03, 21.87s/it]Running Inference:  68%|██████▊   | 135/200 [49:54<17:29, 16.15s/it]Running Inference:  68%|██████▊   | 136/200 [50:19<20:03, 18.81s/it]Running Inference:  68%|██████▊   | 137/200 [50:44<21:37, 20.59s/it]Running Inference:  69%|██████▉   | 138/200 [51:09<22:48, 22.08s/it]Running Inference:  70%|██████▉   | 139/200 [51:33<23:02, 22.67s/it]Running Inference:  70%|███████   | 140/200 [51:57<22:55, 22.92s/it]Running Inference:  70%|███████   | 141/200 [52:20<22:41, 23.07s/it]Running Inference:  71%|███████   | 142/200 [52:44<22:32, 23.32s/it]Running Inference:  72%|███████▏  | 143/200 [53:08<22:18, 23.48s/it]Running Inference:  72%|███████▏  | 144/200 [53:31<21:53, 23.46s/it]Running Inference:  72%|███████▎  | 145/200 [53:56<21:52, 23.86s/it]Running Inference:  73%|███████▎  | 146/200 [54:20<21:21, 23.74s/it]Running Inference:  74%|███████▎  | 147/200 [54:47<21:52, 24.77s/it]Running Inference:  74%|███████▍  | 148/200 [55:11<21:14, 24.51s/it]Running Inference:  74%|███████▍  | 149/200 [55:35<20:47, 24.47s/it]Running Inference:  75%|███████▌  | 150/200 [55:59<20:16, 24.33s/it]Running Inference:  76%|███████▌  | 151/200 [56:23<19:46, 24.21s/it]Running Inference:  76%|███████▌  | 152/200 [56:46<19:05, 23.85s/it]Running Inference:  76%|███████▋  | 153/200 [57:10<18:39, 23.83s/it]Running Inference:  77%|███████▋  | 154/200 [57:33<18:09, 23.69s/it]Running Inference:  78%|███████▊  | 155/200 [57:57<17:50, 23.79s/it]Running Inference:  78%|███████▊  | 156/200 [58:21<17:34, 23.96s/it]Running Inference:  78%|███████▊  | 157/200 [58:45<16:59, 23.71s/it]Running Inference:  79%|███████▉  | 158/200 [59:09<16:49, 24.04s/it]Running Inference:  80%|███████▉  | 159/200 [59:35<16:39, 24.37s/it]Running Inference:  80%|████████  | 160/200 [59:59<16:16, 24.42s/it]Running Inference:  80%|████████  | 161/200 [1:00:24<15:55, 24.49s/it]Running Inference:  81%|████████  | 162/200 [1:00:47<15:15, 24.10s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:10<14:43, 23.88s/it]Running Inference:  82%|████████▏ | 164/200 [1:01:27<12:57, 21.61s/it]Running Inference:  82%|████████▎ | 165/200 [1:01:50<12:51, 22.04s/it]Running Inference:  83%|████████▎ | 166/200 [1:02:13<12:40, 22.36s/it]Running Inference:  84%|████████▎ | 167/200 [1:02:32<11:46, 21.40s/it]Running Inference:  84%|████████▍ | 168/200 [1:02:49<10:43, 20.12s/it]Running Inference:  84%|████████▍ | 169/200 [1:03:12<10:53, 21.07s/it]Running Inference:  85%|████████▌ | 170/200 [1:03:36<10:55, 21.85s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:01<11:03, 22.86s/it]Running Inference:  86%|████████▌ | 172/200 [1:04:25<10:44, 23.02s/it]Running Inference:  86%|████████▋ | 173/200 [1:04:49<10:29, 23.32s/it]Running Inference:  87%|████████▋ | 174/200 [1:05:12<10:10, 23.47s/it]Running Inference:  88%|████████▊ | 175/200 [1:05:36<09:45, 23.41s/it]Running Inference:  88%|████████▊ | 176/200 [1:06:11<10:50, 27.09s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:45<11:06, 28.97s/it]Running Inference:  89%|████████▉ | 178/200 [1:07:09<10:07, 27.61s/it]Running Inference:  90%|████████▉ | 179/200 [1:07:32<09:12, 26.31s/it]Running Inference:  90%|█████████ | 180/200 [1:07:57<08:35, 25.79s/it]Running Inference:  90%|█████████ | 181/200 [1:08:21<07:59, 25.22s/it]Running Inference:  91%|█████████ | 182/200 [1:08:46<07:32, 25.13s/it]Running Inference:  92%|█████████▏| 183/200 [1:09:18<07:40, 27.11s/it]Running Inference:  92%|█████████▏| 184/200 [1:09:41<06:55, 25.95s/it]Running Inference:  92%|█████████▎| 185/200 [1:10:05<06:21, 25.42s/it]Running Inference:  93%|█████████▎| 186/200 [1:10:28<05:47, 24.82s/it]Running Inference:  94%|█████████▎| 187/200 [1:10:52<05:17, 24.40s/it]Running Inference:  94%|█████████▍| 188/200 [1:11:16<04:52, 24.37s/it]Running Inference:  94%|█████████▍| 189/200 [1:11:40<04:25, 24.11s/it]Running Inference:  95%|█████████▌| 190/200 [1:12:03<03:59, 23.91s/it]Running Inference:  96%|█████████▌| 191/200 [1:12:27<03:35, 23.93s/it]Running Inference:  96%|█████████▌| 192/200 [1:12:54<03:19, 24.95s/it]Running Inference:  96%|█████████▋| 193/200 [1:13:18<02:51, 24.55s/it]Running Inference:  97%|█████████▋| 194/200 [1:13:42<02:27, 24.51s/it]Running Inference:  98%|█████████▊| 195/200 [1:14:05<02:00, 24.04s/it]Running Inference:  98%|█████████▊| 196/200 [1:14:29<01:35, 23.94s/it]Running Inference:  98%|█████████▊| 197/200 [1:14:54<01:12, 24.12s/it]Running Inference:  99%|█████████▉| 198/200 [1:15:20<00:49, 24.66s/it]Running Inference: 100%|█████████▉| 199/200 [1:15:43<00:24, 24.19s/it]Running Inference: 100%|██████████| 200/200 [1:16:06<00:00, 24.03s/it]Running Inference: 100%|██████████| 200/200 [1:16:06<00:00, 22.83s/it]
2025-12-14 00:51:55,838 - INFO - Inference completed.
2025-12-14 00:51:55,872 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 00:51:55,872 - INFO - Calculating metrics for dataset: longbench
2025-12-14 00:52:13,615 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 00:52:13,615 - INFO - Metrics:
13.86
2025-12-14 00:52:13,617 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report, ratio=0.3) on GPU 1

----------------------------------------
Task: gov_report | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 00:52:20,632 - INFO - Set deterministic seeds to 42
2025-12-14 00:52:20,633 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 00:52:20,633 - INFO - Starting evaluation run...
2025-12-14 00:52:20,633 - INFO - Output directory set to: longbenchresult
2025-12-14 00:52:20,633 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 00:52:20,633 - INFO - KV Press 'tova' setup.
2025-12-14 00:52:20,633 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 00:52:20,633 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.69it/s]
Device set to use cuda:0
2025-12-14 00:53:09,754 - INFO - Model pipeline loaded.
2025-12-14 00:53:09,754 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-14 00:53:16,509 - INFO - Dataset loaded with 200 entries.
2025-12-14 00:53:16,509 - INFO - Dataset processed with 200 entries.
2025-12-14 00:53:16,545 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:19:45, 24.05s/it]Running Inference:   1%|          | 2/200 [00:46<1:16:02, 23.04s/it]Running Inference:   2%|▏         | 3/200 [01:11<1:18:08, 23.80s/it]Running Inference:   2%|▏         | 4/200 [01:33<1:16:15, 23.34s/it]Running Inference:   2%|▎         | 5/200 [01:52<1:10:19, 21.64s/it]Running Inference:   3%|▎         | 6/200 [02:17<1:13:19, 22.68s/it]Running Inference:   4%|▎         | 7/200 [02:40<1:13:57, 22.99s/it]Running Inference:   4%|▍         | 8/200 [03:00<1:10:43, 22.10s/it]Running Inference:   4%|▍         | 9/200 [03:25<1:12:45, 22.86s/it]Running Inference:   5%|▌         | 10/200 [03:49<1:14:01, 23.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:14<1:14:24, 23.62s/it]Running Inference:   6%|▌         | 12/200 [04:30<1:07:29, 21.54s/it]Running Inference:   6%|▋         | 13/200 [04:56<1:10:31, 22.63s/it]Running Inference:   7%|▋         | 14/200 [05:20<1:11:58, 23.22s/it]Running Inference:   8%|▊         | 15/200 [05:45<1:12:47, 23.61s/it]Running Inference:   8%|▊         | 16/200 [06:09<1:13:31, 23.97s/it]Running Inference:   8%|▊         | 17/200 [06:30<1:09:43, 22.86s/it]Running Inference:   9%|▉         | 18/200 [06:55<1:11:09, 23.46s/it]Running Inference:  10%|▉         | 19/200 [07:21<1:13:28, 24.35s/it]Running Inference:  10%|█         | 20/200 [07:45<1:13:02, 24.35s/it]Running Inference:  10%|█         | 21/200 [08:10<1:12:54, 24.44s/it]Running Inference:  11%|█         | 22/200 [08:35<1:12:56, 24.59s/it]Running Inference:  12%|█▏        | 23/200 [09:00<1:13:01, 24.75s/it]Running Inference:  12%|█▏        | 24/200 [09:27<1:14:07, 25.27s/it]Running Inference:  12%|█▎        | 25/200 [09:51<1:12:34, 24.88s/it]Running Inference:  13%|█▎        | 26/200 [10:15<1:11:43, 24.73s/it]Running Inference:  14%|█▎        | 27/200 [10:37<1:09:20, 24.05s/it]Running Inference:  14%|█▍        | 28/200 [11:01<1:08:57, 24.06s/it]Running Inference:  14%|█▍        | 29/200 [11:28<1:10:43, 24.82s/it]Running Inference:  15%|█▌        | 30/200 [11:53<1:10:18, 24.81s/it]Running Inference:  16%|█▌        | 31/200 [12:17<1:09:38, 24.73s/it]Running Inference:  16%|█▌        | 32/200 [12:43<1:09:51, 24.95s/it]Running Inference:  16%|█▋        | 33/200 [13:07<1:08:33, 24.63s/it]Running Inference:  17%|█▋        | 34/200 [13:32<1:08:36, 24.80s/it]Running Inference:  18%|█▊        | 35/200 [13:57<1:08:02, 24.74s/it]Running Inference:  18%|█▊        | 36/200 [14:22<1:08:15, 24.97s/it]Running Inference:  18%|█▊        | 37/200 [14:46<1:07:22, 24.80s/it]Running Inference:  19%|█▉        | 38/200 [15:11<1:07:08, 24.87s/it]Running Inference:  20%|█▉        | 39/200 [15:36<1:06:24, 24.75s/it]Running Inference:  20%|██        | 40/200 [15:41<50:06, 18.79s/it]  Running Inference:  20%|██        | 41/200 [16:06<54:43, 20.65s/it]Running Inference:  21%|██        | 42/200 [16:31<57:40, 21.90s/it]Running Inference:  22%|██▏       | 43/200 [16:56<59:49, 22.86s/it]Running Inference:  22%|██▏       | 44/200 [17:20<1:00:27, 23.25s/it]Running Inference:  22%|██▎       | 45/200 [17:44<1:00:48, 23.54s/it]Running Inference:  23%|██▎       | 46/200 [18:08<1:01:00, 23.77s/it]Running Inference:  24%|██▎       | 47/200 [18:11<44:26, 17.43s/it]  Running Inference:  24%|██▍       | 48/200 [18:36<49:32, 19.55s/it]Running Inference:  24%|██▍       | 49/200 [19:00<52:58, 21.05s/it]Running Inference:  25%|██▌       | 50/200 [19:24<55:09, 22.06s/it]Running Inference:  26%|██▌       | 51/200 [19:44<52:42, 21.23s/it]Running Inference:  26%|██▌       | 52/200 [20:09<54:58, 22.28s/it]Running Inference:  26%|██▋       | 53/200 [20:36<58:15, 23.78s/it]Running Inference:  27%|██▋       | 54/200 [21:00<57:58, 23.82s/it]Running Inference:  28%|██▊       | 55/200 [21:24<57:56, 23.97s/it]Running Inference:  28%|██▊       | 56/200 [21:49<58:04, 24.20s/it]Running Inference:  28%|██▊       | 57/200 [21:54<44:06, 18.50s/it]Running Inference:  29%|██▉       | 58/200 [22:19<48:08, 20.34s/it]Running Inference:  30%|██▉       | 59/200 [22:43<50:50, 21.64s/it]Running Inference:  30%|███       | 60/200 [23:09<53:33, 22.95s/it]Running Inference:  30%|███       | 61/200 [23:33<53:48, 23.22s/it]Running Inference:  31%|███       | 62/200 [23:57<54:11, 23.56s/it]Running Inference:  32%|███▏      | 63/200 [24:21<54:03, 23.67s/it]Running Inference:  32%|███▏      | 64/200 [24:46<54:01, 23.83s/it]Running Inference:  32%|███▎      | 65/200 [25:09<53:09, 23.63s/it]Running Inference:  33%|███▎      | 66/200 [25:34<53:31, 23.96s/it]Running Inference:  34%|███▎      | 67/200 [25:59<53:47, 24.27s/it]Running Inference:  34%|███▍      | 68/200 [26:23<53:28, 24.31s/it]Running Inference:  34%|███▍      | 69/200 [26:47<53:13, 24.37s/it]Running Inference:  35%|███▌      | 70/200 [27:13<53:27, 24.67s/it]Running Inference:  36%|███▌      | 71/200 [27:38<53:15, 24.77s/it]Running Inference:  36%|███▌      | 72/200 [28:03<52:51, 24.78s/it]Running Inference:  36%|███▋      | 73/200 [28:28<53:04, 25.08s/it]Running Inference:  37%|███▋      | 74/200 [28:53<52:14, 24.88s/it]Running Inference:  38%|███▊      | 75/200 [29:17<51:35, 24.77s/it]Running Inference:  38%|███▊      | 76/200 [29:22<38:56, 18.84s/it]Running Inference:  38%|███▊      | 77/200 [29:47<42:23, 20.68s/it]Running Inference:  39%|███▉      | 78/200 [30:11<44:08, 21.71s/it]Running Inference:  40%|███▉      | 79/200 [30:32<43:14, 21.44s/it]Running Inference:  40%|████      | 80/200 [30:58<45:13, 22.61s/it]Running Inference:  40%|████      | 81/200 [31:22<46:04, 23.23s/it]Running Inference:  41%|████      | 82/200 [31:47<46:42, 23.75s/it]Running Inference:  42%|████▏     | 83/200 [32:11<46:27, 23.82s/it]Running Inference:  42%|████▏     | 84/200 [32:36<46:38, 24.12s/it]Running Inference:  42%|████▎     | 85/200 [33:01<46:45, 24.40s/it]Running Inference:  43%|████▎     | 86/200 [33:25<46:18, 24.38s/it]Running Inference:  44%|████▎     | 87/200 [33:50<45:59, 24.42s/it]Running Inference:  44%|████▍     | 88/200 [34:16<46:45, 25.05s/it]Running Inference:  44%|████▍     | 89/200 [34:41<45:58, 24.85s/it]Running Inference:  45%|████▌     | 90/200 [35:05<45:25, 24.77s/it]Running Inference:  46%|████▌     | 91/200 [35:30<45:07, 24.84s/it]Running Inference:  46%|████▌     | 92/200 [35:55<44:22, 24.65s/it]Running Inference:  46%|████▋     | 93/200 [36:15<41:56, 23.52s/it]Running Inference:  47%|████▋     | 94/200 [36:39<41:45, 23.63s/it]Running Inference:  48%|████▊     | 95/200 [37:05<42:28, 24.27s/it]Running Inference:  48%|████▊     | 96/200 [37:30<42:28, 24.50s/it]Running Inference:  48%|████▊     | 97/200 [37:54<41:52, 24.39s/it]Running Inference:  49%|████▉     | 98/200 [38:19<41:23, 24.35s/it]Running Inference:  50%|████▉     | 99/200 [38:43<40:59, 24.35s/it]Running Inference:  50%|█████     | 100/200 [39:08<41:01, 24.61s/it]Running Inference:  50%|█████     | 101/200 [39:32<40:23, 24.48s/it]Running Inference:  51%|█████     | 102/200 [39:58<40:21, 24.71s/it]Running Inference:  52%|█████▏    | 103/200 [40:22<39:39, 24.53s/it]Running Inference:  52%|█████▏    | 104/200 [40:47<39:30, 24.69s/it]Running Inference:  52%|█████▎    | 105/200 [41:11<38:54, 24.57s/it]Running Inference:  53%|█████▎    | 106/200 [41:35<38:14, 24.41s/it]Running Inference:  54%|█████▎    | 107/200 [42:01<38:23, 24.77s/it]Running Inference:  54%|█████▍    | 108/200 [42:25<37:37, 24.54s/it]Running Inference:  55%|█████▍    | 109/200 [42:49<37:03, 24.43s/it]Running Inference:  55%|█████▌    | 110/200 [43:14<36:50, 24.56s/it]Running Inference:  56%|█████▌    | 111/200 [43:39<36:38, 24.70s/it]Running Inference:  56%|█████▌    | 112/200 [44:03<36:00, 24.55s/it]Running Inference:  56%|█████▋    | 113/200 [44:28<35:43, 24.64s/it]Running Inference:  57%|█████▋    | 114/200 [44:53<35:31, 24.79s/it]Running Inference:  57%|█████▊    | 115/200 [45:17<34:54, 24.64s/it]Running Inference:  58%|█████▊    | 116/200 [45:41<34:18, 24.50s/it]Running Inference:  58%|█████▊    | 117/200 [46:06<33:45, 24.40s/it]Running Inference:  59%|█████▉    | 118/200 [46:30<33:15, 24.34s/it]Running Inference:  60%|█████▉    | 119/200 [46:56<33:33, 24.86s/it]Running Inference:  60%|██████    | 120/200 [47:20<33:00, 24.76s/it]Running Inference:  60%|██████    | 121/200 [47:45<32:43, 24.85s/it]Running Inference:  61%|██████    | 122/200 [48:10<32:02, 24.65s/it]Running Inference:  62%|██████▏   | 123/200 [48:34<31:32, 24.57s/it]Running Inference:  62%|██████▏   | 124/200 [48:59<31:08, 24.58s/it]Running Inference:  62%|██████▎   | 125/200 [49:24<30:50, 24.68s/it]Running Inference:  63%|██████▎   | 126/200 [49:48<30:14, 24.52s/it]Running Inference:  64%|██████▎   | 127/200 [50:04<26:45, 22.00s/it]Running Inference:  64%|██████▍   | 128/200 [50:28<27:06, 22.58s/it]Running Inference:  64%|██████▍   | 129/200 [50:52<27:17, 23.06s/it]Running Inference:  65%|██████▌   | 130/200 [51:16<27:21, 23.45s/it]Running Inference:  66%|██████▌   | 131/200 [51:41<27:16, 23.72s/it]Running Inference:  66%|██████▌   | 132/200 [52:06<27:30, 24.27s/it]Running Inference:  66%|██████▋   | 133/200 [52:30<27:04, 24.25s/it]Running Inference:  67%|██████▋   | 134/200 [52:53<25:59, 23.63s/it]Running Inference:  68%|██████▊   | 135/200 [53:17<25:48, 23.83s/it]Running Inference:  68%|██████▊   | 136/200 [53:43<26:06, 24.47s/it]Running Inference:  68%|██████▊   | 137/200 [54:09<26:05, 24.84s/it]Running Inference:  69%|██████▉   | 138/200 [54:35<26:10, 25.33s/it]Running Inference:  70%|██████▉   | 139/200 [55:00<25:41, 25.28s/it]Running Inference:  70%|███████   | 140/200 [55:25<25:01, 25.02s/it]Running Inference:  70%|███████   | 141/200 [55:49<24:24, 24.83s/it]Running Inference:  71%|███████   | 142/200 [56:14<24:00, 24.83s/it]Running Inference:  72%|███████▏  | 143/200 [56:39<23:38, 24.89s/it]Running Inference:  72%|███████▏  | 144/200 [57:03<23:05, 24.75s/it]Running Inference:  72%|███████▎  | 145/200 [57:29<22:57, 25.04s/it]Running Inference:  73%|███████▎  | 146/200 [57:53<22:22, 24.85s/it]Running Inference:  74%|███████▎  | 147/200 [58:12<20:25, 23.11s/it]Running Inference:  74%|███████▍  | 148/200 [58:37<20:28, 23.62s/it]Running Inference:  74%|███████▍  | 149/200 [59:03<20:30, 24.12s/it]Running Inference:  75%|███████▌  | 150/200 [59:27<20:18, 24.38s/it]Running Inference:  76%|███████▌  | 151/200 [59:52<20:01, 24.53s/it]Running Inference:  76%|███████▌  | 152/200 [1:00:16<19:28, 24.35s/it]Running Inference:  76%|███████▋  | 153/200 [1:00:41<19:09, 24.46s/it]Running Inference:  77%|███████▋  | 154/200 [1:01:05<18:44, 24.44s/it]Running Inference:  78%|███████▊  | 155/200 [1:01:30<18:25, 24.57s/it]Running Inference:  78%|███████▊  | 156/200 [1:01:55<18:08, 24.75s/it]Running Inference:  78%|███████▊  | 157/200 [1:02:19<17:35, 24.54s/it]Running Inference:  79%|███████▉  | 158/200 [1:02:45<17:24, 24.86s/it]Running Inference:  80%|███████▉  | 159/200 [1:03:11<17:12, 25.18s/it]Running Inference:  80%|████████  | 160/200 [1:03:36<16:50, 25.25s/it]Running Inference:  80%|████████  | 161/200 [1:04:02<16:28, 25.34s/it]Running Inference:  81%|████████  | 162/200 [1:04:26<15:47, 24.94s/it]Running Inference:  82%|████████▏ | 163/200 [1:04:50<15:15, 24.73s/it]Running Inference:  82%|████████▏ | 164/200 [1:05:16<15:03, 25.09s/it]Running Inference:  82%|████████▎ | 165/200 [1:05:40<14:25, 24.72s/it]Running Inference:  83%|████████▎ | 166/200 [1:06:04<13:51, 24.47s/it]Running Inference:  84%|████████▎ | 167/200 [1:06:29<13:35, 24.71s/it]Running Inference:  84%|████████▍ | 168/200 [1:06:49<12:20, 23.14s/it]Running Inference:  84%|████████▍ | 169/200 [1:07:13<12:06, 23.44s/it]Running Inference:  85%|████████▌ | 170/200 [1:07:37<11:52, 23.75s/it]Running Inference:  86%|████████▌ | 171/200 [1:08:03<11:49, 24.46s/it]Running Inference:  86%|████████▌ | 172/200 [1:08:28<11:23, 24.40s/it]Running Inference:  86%|████████▋ | 173/200 [1:08:52<11:02, 24.53s/it]Running Inference:  87%|████████▋ | 174/200 [1:09:17<10:38, 24.56s/it]Running Inference:  88%|████████▊ | 175/200 [1:09:41<10:10, 24.42s/it]Running Inference:  88%|████████▊ | 176/200 [1:10:16<11:00, 27.50s/it]Running Inference:  88%|████████▊ | 177/200 [1:10:48<11:06, 28.99s/it]Running Inference:  89%|████████▉ | 178/200 [1:11:14<10:13, 27.87s/it]Running Inference:  90%|████████▉ | 179/200 [1:11:38<09:21, 26.72s/it]Running Inference:  90%|█████████ | 180/200 [1:12:03<08:46, 26.33s/it]Running Inference:  90%|█████████ | 181/200 [1:12:28<08:11, 25.87s/it]Running Inference:  91%|█████████ | 182/200 [1:12:42<06:42, 22.36s/it]Running Inference:  92%|█████████▏| 183/200 [1:13:13<07:05, 25.00s/it]Running Inference:  92%|█████████▏| 184/200 [1:13:37<06:35, 24.71s/it]Running Inference:  92%|█████████▎| 185/200 [1:14:02<06:11, 24.80s/it]Running Inference:  93%|█████████▎| 186/200 [1:14:26<05:44, 24.64s/it]Running Inference:  94%|█████████▎| 187/200 [1:14:44<04:51, 22.42s/it]Running Inference:  94%|█████████▍| 188/200 [1:15:09<04:38, 23.24s/it]Running Inference:  94%|█████████▍| 189/200 [1:15:33<04:19, 23.59s/it]Running Inference:  95%|█████████▌| 190/200 [1:15:58<03:57, 23.80s/it]Running Inference:  96%|█████████▌| 191/200 [1:16:22<03:36, 24.10s/it]Running Inference:  96%|█████████▌| 192/200 [1:16:50<03:20, 25.10s/it]Running Inference:  96%|█████████▋| 193/200 [1:17:14<02:54, 24.92s/it]Running Inference:  97%|█████████▋| 194/200 [1:17:40<02:30, 25.03s/it]Running Inference:  98%|█████████▊| 195/200 [1:18:03<02:03, 24.66s/it]Running Inference:  98%|█████████▊| 196/200 [1:18:21<01:30, 22.55s/it]Running Inference:  98%|█████████▊| 197/200 [1:18:46<01:10, 23.40s/it]Running Inference:  99%|█████████▉| 198/200 [1:19:13<00:48, 24.34s/it]Running Inference: 100%|█████████▉| 199/200 [1:19:37<00:24, 24.24s/it]Running Inference: 100%|██████████| 200/200 [1:20:01<00:00, 24.30s/it]Running Inference: 100%|██████████| 200/200 [1:20:01<00:00, 24.01s/it]
2025-12-14 02:13:18,463 - INFO - Inference completed.
2025-12-14 02:13:18,503 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 02:13:18,503 - INFO - Calculating metrics for dataset: longbench
2025-12-14 02:13:37,085 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 02:13:37,085 - INFO - Metrics:
13.29
2025-12-14 02:13:37,087 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report, ratio=0.5) on GPU 1


========================================
LongBench Task: multi_news
========================================
----------------------------------------
Task: multi_news | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 02:13:44,512 - INFO - Set deterministic seeds to 42
2025-12-14 02:13:44,513 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 02:13:44,513 - INFO - Starting evaluation run...
2025-12-14 02:13:44,513 - INFO - Output directory set to: longbenchresult
2025-12-14 02:13:44,513 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 02:13:44,513 - INFO - KV Press 'tova' setup.
2025-12-14 02:13:44,513 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 02:13:44,513 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.83it/s]
Device set to use cuda:0
2025-12-14 02:13:56,011 - INFO - Model pipeline loaded.
2025-12-14 02:13:56,011 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 6958.15 examples/s]
2025-12-14 02:13:59,837 - INFO - Dataset loaded with 200 entries.
2025-12-14 02:13:59,837 - INFO - Dataset processed with 200 entries.
2025-12-14 02:13:59,845 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:22<1:15:58, 22.91s/it]Running Inference:   1%|          | 2/200 [00:45<1:15:24, 22.85s/it]Running Inference:   2%|▏         | 3/200 [01:08<1:15:08, 22.89s/it]Running Inference:   2%|▏         | 4/200 [01:31<1:14:47, 22.90s/it]Running Inference:   2%|▎         | 5/200 [01:54<1:14:24, 22.90s/it]Running Inference:   3%|▎         | 6/200 [02:17<1:14:01, 22.89s/it]Running Inference:   4%|▎         | 7/200 [02:40<1:13:55, 22.98s/it]Running Inference:   4%|▍         | 8/200 [03:03<1:13:35, 22.99s/it]Running Inference:   4%|▍         | 9/200 [03:26<1:13:08, 22.98s/it]Running Inference:   5%|▌         | 10/200 [03:49<1:12:36, 22.93s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:12<1:12:05, 22.89s/it]Running Inference:   6%|▌         | 12/200 [04:31<1:08:01, 21.71s/it]Running Inference:   6%|▋         | 13/200 [04:54<1:08:48, 22.07s/it]Running Inference:   7%|▋         | 14/200 [05:16<1:09:06, 22.29s/it]Running Inference:   8%|▊         | 15/200 [05:39<1:09:31, 22.55s/it]Running Inference:   8%|▊         | 16/200 [06:02<1:09:04, 22.53s/it]Running Inference:   8%|▊         | 17/200 [06:25<1:08:56, 22.60s/it]Running Inference:   9%|▉         | 18/200 [06:48<1:09:26, 22.89s/it]Running Inference:  10%|▉         | 19/200 [07:11<1:09:02, 22.89s/it]Running Inference:  10%|█         | 20/200 [07:30<1:04:49, 21.61s/it]Running Inference:  10%|█         | 21/200 [07:53<1:05:30, 21.96s/it]Running Inference:  11%|█         | 22/200 [08:15<1:05:58, 22.24s/it]Running Inference:  12%|█▏        | 23/200 [08:38<1:06:06, 22.41s/it]Running Inference:  12%|█▏        | 24/200 [09:01<1:06:19, 22.61s/it]Running Inference:  12%|█▎        | 25/200 [09:23<1:05:23, 22.42s/it]Running Inference:  13%|█▎        | 26/200 [09:46<1:05:26, 22.57s/it]Running Inference:  14%|█▎        | 27/200 [10:10<1:06:29, 23.06s/it]Running Inference:  14%|█▍        | 28/200 [10:34<1:06:15, 23.12s/it]Running Inference:  14%|█▍        | 29/200 [10:56<1:05:36, 23.02s/it]Running Inference:  15%|█▌        | 30/200 [11:19<1:05:03, 22.96s/it]Running Inference:  16%|█▌        | 31/200 [11:42<1:04:44, 22.99s/it]Running Inference:  16%|█▌        | 32/200 [12:05<1:04:13, 22.94s/it]Running Inference:  16%|█▋        | 33/200 [12:28<1:03:44, 22.90s/it]Running Inference:  17%|█▋        | 34/200 [12:51<1:03:26, 22.93s/it]Running Inference:  18%|█▊        | 35/200 [13:14<1:02:59, 22.91s/it]Running Inference:  18%|█▊        | 36/200 [13:37<1:02:36, 22.90s/it]Running Inference:  18%|█▊        | 37/200 [14:00<1:02:28, 23.00s/it]Running Inference:  19%|█▉        | 38/200 [14:23<1:02:28, 23.14s/it]Running Inference:  20%|█▉        | 39/200 [14:46<1:01:19, 22.86s/it]Running Inference:  20%|██        | 40/200 [15:03<56:39, 21.24s/it]  Running Inference:  20%|██        | 41/200 [15:26<57:39, 21.76s/it]Running Inference:  21%|██        | 42/200 [15:49<58:04, 22.06s/it]Running Inference:  22%|██▏       | 43/200 [16:12<58:19, 22.29s/it]Running Inference:  22%|██▏       | 44/200 [16:15<43:08, 16.59s/it]Running Inference:  22%|██▎       | 45/200 [16:38<48:05, 18.61s/it]Running Inference:  23%|██▎       | 46/200 [17:01<51:07, 19.92s/it]Running Inference:  24%|██▎       | 47/200 [17:25<53:26, 20.96s/it]Running Inference:  24%|██▍       | 48/200 [17:48<54:47, 21.63s/it]Running Inference:  24%|██▍       | 49/200 [18:11<55:25, 22.02s/it]Running Inference:  25%|██▌       | 50/200 [18:34<55:54, 22.36s/it]Running Inference:  26%|██▌       | 51/200 [18:57<56:04, 22.58s/it]Running Inference:  26%|██▌       | 52/200 [19:20<56:07, 22.75s/it]Running Inference:  26%|██▋       | 53/200 [19:43<55:52, 22.81s/it]Running Inference:  27%|██▋       | 54/200 [20:06<55:39, 22.87s/it]Running Inference:  28%|██▊       | 55/200 [20:29<55:18, 22.89s/it]Running Inference:  28%|██▊       | 56/200 [20:52<55:06, 22.96s/it]Running Inference:  28%|██▊       | 57/200 [21:15<54:53, 23.03s/it]Running Inference:  29%|██▉       | 58/200 [21:38<54:28, 23.02s/it]Running Inference:  30%|██▉       | 59/200 [22:01<54:06, 23.03s/it]Running Inference:  30%|███       | 60/200 [22:24<53:40, 23.00s/it]Running Inference:  30%|███       | 61/200 [22:47<53:21, 23.03s/it]Running Inference:  31%|███       | 62/200 [23:10<52:52, 22.99s/it]Running Inference:  32%|███▏      | 63/200 [23:33<52:26, 22.96s/it]Running Inference:  32%|███▏      | 64/200 [23:56<52:01, 22.95s/it]Running Inference:  32%|███▎      | 65/200 [24:19<51:54, 23.07s/it]Running Inference:  33%|███▎      | 66/200 [24:42<51:26, 23.03s/it]Running Inference:  34%|███▎      | 67/200 [25:06<51:21, 23.17s/it]Running Inference:  34%|███▍      | 68/200 [25:29<50:55, 23.15s/it]Running Inference:  34%|███▍      | 69/200 [25:52<50:22, 23.07s/it]Running Inference:  35%|███▌      | 70/200 [26:13<48:59, 22.61s/it]Running Inference:  36%|███▌      | 71/200 [26:36<48:50, 22.71s/it]Running Inference:  36%|███▌      | 72/200 [27:00<48:45, 22.85s/it]Running Inference:  36%|███▋      | 73/200 [27:22<48:22, 22.85s/it]Running Inference:  37%|███▋      | 74/200 [27:45<48:02, 22.88s/it]Running Inference:  38%|███▊      | 75/200 [28:08<47:41, 22.89s/it]Running Inference:  38%|███▊      | 76/200 [28:31<47:17, 22.89s/it]Running Inference:  38%|███▊      | 77/200 [28:54<46:56, 22.90s/it]Running Inference:  39%|███▉      | 78/200 [29:17<46:36, 22.92s/it]Running Inference:  40%|███▉      | 79/200 [29:40<46:12, 22.91s/it]Running Inference:  40%|████      | 80/200 [30:03<45:54, 22.95s/it]Running Inference:  40%|████      | 81/200 [30:26<45:28, 22.93s/it]Running Inference:  41%|████      | 82/200 [30:46<43:42, 22.23s/it]Running Inference:  42%|████▏     | 83/200 [31:10<43:48, 22.47s/it]Running Inference:  42%|████▏     | 84/200 [31:33<43:51, 22.68s/it]Running Inference:  42%|████▎     | 85/200 [31:56<43:34, 22.74s/it]Running Inference:  43%|████▎     | 86/200 [32:19<43:28, 22.88s/it]Running Inference:  44%|████▎     | 87/200 [32:42<43:04, 22.87s/it]Running Inference:  44%|████▍     | 88/200 [33:05<42:52, 22.97s/it]Running Inference:  44%|████▍     | 89/200 [33:28<42:35, 23.03s/it]Running Inference:  45%|████▌     | 90/200 [33:51<42:15, 23.05s/it]Running Inference:  46%|████▌     | 91/200 [34:14<41:49, 23.02s/it]Running Inference:  46%|████▌     | 92/200 [34:37<41:22, 22.99s/it]Running Inference:  46%|████▋     | 93/200 [35:00<40:55, 22.94s/it]Running Inference:  47%|████▋     | 94/200 [35:23<40:36, 22.99s/it]Running Inference:  48%|████▊     | 95/200 [35:46<40:19, 23.04s/it]Running Inference:  48%|████▊     | 96/200 [36:09<39:56, 23.05s/it]Running Inference:  48%|████▊     | 97/200 [36:32<39:34, 23.06s/it]Running Inference:  49%|████▉     | 98/200 [36:55<39:12, 23.07s/it]Running Inference:  50%|████▉     | 99/200 [37:20<39:29, 23.46s/it]Running Inference:  50%|█████     | 100/200 [37:43<38:52, 23.33s/it]Running Inference:  50%|█████     | 101/200 [38:06<38:21, 23.25s/it]Running Inference:  51%|█████     | 102/200 [38:29<37:48, 23.15s/it]Running Inference:  52%|█████▏    | 103/200 [38:52<37:33, 23.23s/it]Running Inference:  52%|█████▏    | 104/200 [39:15<37:14, 23.27s/it]Running Inference:  52%|█████▎    | 105/200 [39:39<36:45, 23.22s/it]Running Inference:  53%|█████▎    | 106/200 [40:02<36:15, 23.14s/it]Running Inference:  54%|█████▎    | 107/200 [40:24<35:46, 23.08s/it]Running Inference:  54%|█████▍    | 108/200 [40:47<35:22, 23.07s/it]Running Inference:  55%|█████▍    | 109/200 [41:10<34:54, 23.02s/it]Running Inference:  55%|█████▌    | 110/200 [41:33<34:27, 22.98s/it]Running Inference:  56%|█████▌    | 111/200 [41:56<34:05, 22.99s/it]Running Inference:  56%|█████▌    | 112/200 [42:19<33:39, 22.95s/it]Running Inference:  56%|█████▋    | 113/200 [42:42<33:21, 23.01s/it]Running Inference:  57%|█████▋    | 114/200 [43:05<32:54, 22.96s/it]Running Inference:  57%|█████▊    | 115/200 [43:28<32:28, 22.92s/it]Running Inference:  58%|█████▊    | 116/200 [43:51<32:02, 22.89s/it]Running Inference:  58%|█████▊    | 117/200 [44:14<31:39, 22.89s/it]Running Inference:  59%|█████▉    | 118/200 [44:37<31:18, 22.91s/it]Running Inference:  60%|█████▉    | 119/200 [45:00<31:04, 23.02s/it]Running Inference:  60%|██████    | 120/200 [45:23<30:38, 22.99s/it]Running Inference:  60%|██████    | 121/200 [45:46<30:14, 22.97s/it]Running Inference:  61%|██████    | 122/200 [46:09<29:48, 22.93s/it]Running Inference:  62%|██████▏   | 123/200 [46:32<29:27, 22.96s/it]Running Inference:  62%|██████▏   | 124/200 [46:54<29:00, 22.90s/it]Running Inference:  62%|██████▎   | 125/200 [47:17<28:37, 22.90s/it]Running Inference:  63%|██████▎   | 126/200 [47:40<28:11, 22.86s/it]Running Inference:  64%|██████▎   | 127/200 [48:03<27:51, 22.90s/it]Running Inference:  64%|██████▍   | 128/200 [48:27<27:49, 23.18s/it]Running Inference:  64%|██████▍   | 129/200 [48:50<27:18, 23.08s/it]Running Inference:  65%|██████▌   | 130/200 [49:13<26:50, 23.00s/it]Running Inference:  66%|██████▌   | 131/200 [49:31<24:48, 21.57s/it]Running Inference:  66%|██████▌   | 132/200 [49:54<24:56, 22.01s/it]Running Inference:  66%|██████▋   | 133/200 [50:17<24:53, 22.29s/it]Running Inference:  67%|██████▋   | 134/200 [50:40<24:46, 22.52s/it]Running Inference:  68%|██████▊   | 135/200 [51:03<24:43, 22.82s/it]Running Inference:  68%|██████▊   | 136/200 [51:26<24:27, 22.93s/it]Running Inference:  68%|██████▊   | 137/200 [51:50<24:07, 22.98s/it]Running Inference:  69%|██████▉   | 138/200 [52:13<23:43, 22.96s/it]Running Inference:  70%|██████▉   | 139/200 [52:16<17:20, 17.06s/it]Running Inference:  70%|███████   | 140/200 [52:39<18:51, 18.86s/it]Running Inference:  70%|███████   | 141/200 [53:02<19:47, 20.12s/it]Running Inference:  71%|███████   | 142/200 [53:25<20:15, 20.95s/it]Running Inference:  72%|███████▏  | 143/200 [53:46<20:01, 21.08s/it]Running Inference:  72%|███████▏  | 144/200 [54:09<20:10, 21.62s/it]Running Inference:  72%|███████▎  | 145/200 [54:32<20:10, 22.02s/it]Running Inference:  73%|███████▎  | 146/200 [54:55<20:02, 22.26s/it]Running Inference:  74%|███████▎  | 147/200 [55:18<19:49, 22.44s/it]Running Inference:  74%|███████▍  | 148/200 [55:39<19:06, 22.05s/it]Running Inference:  74%|███████▍  | 149/200 [56:02<18:56, 22.29s/it]Running Inference:  75%|███████▌  | 150/200 [56:25<18:50, 22.61s/it]Running Inference:  76%|███████▌  | 151/200 [56:48<18:32, 22.71s/it]Running Inference:  76%|███████▌  | 152/200 [57:11<18:15, 22.81s/it]Running Inference:  76%|███████▋  | 153/200 [57:34<17:58, 22.94s/it]Running Inference:  77%|███████▋  | 154/200 [57:57<17:37, 22.98s/it]Running Inference:  78%|███████▊  | 155/200 [58:21<17:18, 23.08s/it]Running Inference:  78%|███████▊  | 156/200 [58:21<11:59, 16.36s/it]Running Inference:  78%|███████▊  | 157/200 [58:45<13:12, 18.42s/it]Running Inference:  79%|███████▉  | 158/200 [59:08<13:54, 19.86s/it]Running Inference:  80%|███████▉  | 159/200 [59:31<14:10, 20.76s/it]Running Inference:  80%|████████  | 160/200 [59:54<14:19, 21.49s/it]Running Inference:  80%|████████  | 161/200 [1:00:17<14:14, 21.92s/it]Running Inference:  81%|████████  | 162/200 [1:00:40<14:03, 22.20s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:03<13:49, 22.41s/it]Running Inference:  82%|████████▏ | 164/200 [1:01:26<13:35, 22.65s/it]Running Inference:  82%|████████▎ | 165/200 [1:01:49<13:14, 22.70s/it]Running Inference:  83%|████████▎ | 166/200 [1:02:08<12:14, 21.61s/it]Running Inference:  84%|████████▎ | 167/200 [1:02:31<12:06, 22.03s/it]Running Inference:  84%|████████▍ | 168/200 [1:02:52<11:35, 21.74s/it]Running Inference:  84%|████████▍ | 169/200 [1:03:15<11:25, 22.13s/it]Running Inference:  85%|████████▌ | 170/200 [1:03:38<11:12, 22.42s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:01<10:54, 22.58s/it]Running Inference:  86%|████████▌ | 172/200 [1:04:24<10:36, 22.72s/it]Running Inference:  86%|████████▋ | 173/200 [1:04:47<10:15, 22.78s/it]Running Inference:  87%|████████▋ | 174/200 [1:05:10<09:54, 22.86s/it]Running Inference:  88%|████████▊ | 175/200 [1:05:26<08:40, 20.81s/it]Running Inference:  88%|████████▊ | 176/200 [1:05:49<08:34, 21.46s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:12<08:24, 21.95s/it]Running Inference:  89%|████████▉ | 178/200 [1:06:35<08:12, 22.40s/it]Running Inference:  90%|████████▉ | 179/200 [1:06:38<05:46, 16.48s/it]Running Inference:  90%|█████████ | 180/200 [1:07:01<06:08, 18.43s/it]Running Inference:  90%|█████████ | 181/200 [1:07:24<06:16, 19.79s/it]Running Inference:  91%|█████████ | 182/200 [1:07:47<06:13, 20.73s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:10<06:03, 21.38s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:34<05:54, 22.18s/it]Running Inference:  92%|█████████▎| 185/200 [1:08:57<05:36, 22.43s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:20<05:16, 22.57s/it]Running Inference:  94%|█████████▎| 187/200 [1:09:43<04:54, 22.68s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:06<04:33, 22.77s/it]Running Inference:  94%|█████████▍| 189/200 [1:10:28<04:10, 22.78s/it]Running Inference:  95%|█████████▌| 190/200 [1:10:51<03:48, 22.82s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:15<03:26, 22.94s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:16<02:11, 16.39s/it]Running Inference:  96%|█████████▋| 193/200 [1:11:39<02:08, 18.41s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:02<01:58, 19.76s/it]Running Inference:  98%|█████████▊| 195/200 [1:12:25<01:43, 20.74s/it]Running Inference:  98%|█████████▊| 196/200 [1:12:34<01:08, 17.16s/it]Running Inference:  98%|█████████▊| 197/200 [1:12:56<00:56, 18.88s/it]Running Inference:  99%|█████████▉| 198/200 [1:13:19<00:40, 20.08s/it]Running Inference: 100%|█████████▉| 199/200 [1:13:42<00:20, 20.91s/it]Running Inference: 100%|██████████| 200/200 [1:14:05<00:00, 21.58s/it]Running Inference: 100%|██████████| 200/200 [1:14:05<00:00, 22.23s/it]
2025-12-14 03:28:05,656 - INFO - Inference completed.
2025-12-14 03:28:05,679 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 03:28:05,679 - INFO - Calculating metrics for dataset: longbench
2025-12-14 03:28:12,753 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 03:28:12,753 - INFO - Metrics:
12.69
2025-12-14 03:28:12,755 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news, ratio=0.1) on GPU 1

----------------------------------------
Task: multi_news | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 03:28:19,452 - INFO - Set deterministic seeds to 42
2025-12-14 03:28:19,452 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 03:28:19,452 - INFO - Starting evaluation run...
2025-12-14 03:28:19,452 - INFO - Output directory set to: longbenchresult
2025-12-14 03:28:19,452 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 03:28:19,452 - INFO - KV Press 'tova' setup.
2025-12-14 03:28:19,452 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 03:28:19,452 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 96.74it/s]
Device set to use cuda:0
2025-12-14 03:28:31,472 - INFO - Model pipeline loaded.
2025-12-14 03:28:31,472 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 03:28:34,894 - INFO - Dataset loaded with 200 entries.
2025-12-14 03:28:34,894 - INFO - Dataset processed with 200 entries.
2025-12-14 03:28:34,904 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:07, 24.16s/it]Running Inference:   1%|          | 2/200 [00:47<1:17:46, 23.57s/it]Running Inference:   2%|▏         | 3/200 [01:10<1:16:55, 23.43s/it]Running Inference:   2%|▏         | 4/200 [01:33<1:16:17, 23.36s/it]Running Inference:   2%|▎         | 5/200 [01:57<1:15:45, 23.31s/it]Running Inference:   3%|▎         | 6/200 [02:20<1:15:14, 23.27s/it]Running Inference:   4%|▎         | 7/200 [02:43<1:15:05, 23.34s/it]Running Inference:   4%|▍         | 8/200 [03:07<1:14:40, 23.34s/it]Running Inference:   4%|▍         | 9/200 [03:30<1:14:11, 23.31s/it]Running Inference:   5%|▌         | 10/200 [03:45<1:05:54, 20.81s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:08<1:07:47, 21.52s/it]Running Inference:   6%|▌         | 12/200 [04:31<1:09:07, 22.06s/it]Running Inference:   6%|▋         | 13/200 [04:55<1:09:54, 22.43s/it]Running Inference:   7%|▋         | 14/200 [05:12<1:04:40, 20.86s/it]Running Inference:   8%|▊         | 15/200 [05:35<1:06:44, 21.64s/it]Running Inference:   8%|▊         | 16/200 [05:59<1:07:55, 22.15s/it]Running Inference:   8%|▊         | 17/200 [06:22<1:08:31, 22.47s/it]Running Inference:   9%|▉         | 18/200 [06:46<1:09:28, 22.90s/it]Running Inference:  10%|▉         | 19/200 [07:09<1:09:21, 22.99s/it]Running Inference:  10%|█         | 20/200 [07:28<1:05:06, 21.70s/it]Running Inference:  10%|█         | 21/200 [07:51<1:05:59, 22.12s/it]Running Inference:  11%|█         | 22/200 [08:14<1:06:36, 22.45s/it]Running Inference:  12%|█▏        | 23/200 [08:35<1:04:30, 21.87s/it]Running Inference:  12%|█▏        | 24/200 [08:58<1:05:31, 22.34s/it]Running Inference:  12%|█▎        | 25/200 [09:21<1:05:50, 22.57s/it]Running Inference:  13%|█▎        | 26/200 [09:44<1:06:02, 22.77s/it]Running Inference:  14%|█▎        | 27/200 [10:09<1:07:10, 23.30s/it]Running Inference:  14%|█▍        | 28/200 [10:33<1:07:03, 23.39s/it]Running Inference:  14%|█▍        | 29/200 [10:56<1:06:28, 23.33s/it]Running Inference:  15%|█▌        | 30/200 [11:19<1:05:58, 23.29s/it]Running Inference:  16%|█▌        | 31/200 [11:42<1:05:44, 23.34s/it]Running Inference:  16%|█▌        | 32/200 [12:06<1:05:13, 23.30s/it]Running Inference:  16%|█▋        | 33/200 [12:29<1:04:44, 23.26s/it]Running Inference:  17%|█▋        | 34/200 [12:52<1:04:25, 23.29s/it]Running Inference:  18%|█▊        | 35/200 [13:15<1:03:59, 23.27s/it]Running Inference:  18%|█▊        | 36/200 [13:39<1:03:37, 23.28s/it]Running Inference:  18%|█▊        | 37/200 [14:02<1:03:30, 23.38s/it]Running Inference:  19%|█▉        | 38/200 [14:26<1:03:28, 23.51s/it]Running Inference:  20%|█▉        | 39/200 [14:49<1:02:50, 23.42s/it]Running Inference:  20%|██        | 40/200 [15:12<1:02:15, 23.34s/it]Running Inference:  20%|██        | 41/200 [15:36<1:01:48, 23.32s/it]Running Inference:  21%|██        | 42/200 [15:59<1:01:15, 23.26s/it]Running Inference:  22%|██▏       | 43/200 [16:22<1:00:47, 23.23s/it]Running Inference:  22%|██▏       | 44/200 [16:25<44:53, 17.26s/it]  Running Inference:  22%|██▎       | 45/200 [16:49<49:18, 19.08s/it]Running Inference:  23%|██▎       | 46/200 [17:07<48:39, 18.96s/it]Running Inference:  24%|██▎       | 47/200 [17:31<51:53, 20.35s/it]Running Inference:  24%|██▍       | 48/200 [17:54<53:54, 21.28s/it]Running Inference:  24%|██▍       | 49/200 [18:14<52:14, 20.76s/it]Running Inference:  25%|██▌       | 50/200 [18:37<53:52, 21.55s/it]Running Inference:  26%|██▌       | 51/200 [19:01<54:49, 22.08s/it]Running Inference:  26%|██▌       | 52/200 [19:19<51:39, 20.94s/it]Running Inference:  26%|██▋       | 53/200 [19:38<50:19, 20.54s/it]Running Inference:  27%|██▋       | 54/200 [20:02<52:01, 21.38s/it]Running Inference:  28%|██▊       | 55/200 [20:25<52:57, 21.92s/it]Running Inference:  28%|██▊       | 56/200 [20:48<53:38, 22.35s/it]Running Inference:  28%|██▊       | 57/200 [21:12<54:00, 22.66s/it]Running Inference:  29%|██▉       | 58/200 [21:35<54:01, 22.83s/it]Running Inference:  30%|██▉       | 59/200 [21:58<53:56, 22.95s/it]Running Inference:  30%|███       | 60/200 [22:20<53:02, 22.73s/it]Running Inference:  30%|███       | 61/200 [22:44<53:04, 22.91s/it]Running Inference:  31%|███       | 62/200 [23:07<52:50, 22.98s/it]Running Inference:  32%|███▏      | 63/200 [23:30<52:40, 23.07s/it]Running Inference:  32%|███▏      | 64/200 [23:53<52:23, 23.12s/it]Running Inference:  32%|███▎      | 65/200 [24:17<52:24, 23.29s/it]Running Inference:  33%|███▎      | 66/200 [24:40<51:59, 23.28s/it]Running Inference:  34%|███▎      | 67/200 [25:04<51:56, 23.43s/it]Running Inference:  34%|███▍      | 68/200 [25:28<51:32, 23.43s/it]Running Inference:  34%|███▍      | 69/200 [25:42<45:05, 20.65s/it]Running Inference:  35%|███▌      | 70/200 [26:05<46:25, 21.43s/it]Running Inference:  36%|███▌      | 71/200 [26:28<47:14, 21.97s/it]Running Inference:  36%|███▌      | 72/200 [26:52<47:51, 22.43s/it]Running Inference:  36%|███▋      | 73/200 [27:15<47:56, 22.65s/it]Running Inference:  37%|███▋      | 74/200 [27:38<47:57, 22.84s/it]Running Inference:  38%|███▊      | 75/200 [28:01<47:49, 22.96s/it]Running Inference:  38%|███▊      | 76/200 [28:14<41:03, 19.87s/it]Running Inference:  38%|███▊      | 77/200 [28:37<42:48, 20.88s/it]Running Inference:  39%|███▉      | 78/200 [29:01<43:55, 21.60s/it]Running Inference:  40%|███▉      | 79/200 [29:24<44:29, 22.07s/it]Running Inference:  40%|████      | 80/200 [29:47<44:54, 22.45s/it]Running Inference:  40%|████      | 81/200 [30:10<44:57, 22.66s/it]Running Inference:  41%|████      | 82/200 [30:33<44:53, 22.83s/it]Running Inference:  42%|████▏     | 83/200 [30:57<44:47, 22.97s/it]Running Inference:  42%|████▏     | 84/200 [31:20<44:41, 23.12s/it]Running Inference:  42%|████▎     | 85/200 [31:44<44:26, 23.19s/it]Running Inference:  43%|████▎     | 86/200 [32:07<44:16, 23.30s/it]Running Inference:  44%|████▎     | 87/200 [32:30<43:47, 23.25s/it]Running Inference:  44%|████▍     | 88/200 [32:54<43:31, 23.32s/it]Running Inference:  44%|████▍     | 89/200 [33:17<43:16, 23.39s/it]Running Inference:  45%|████▌     | 90/200 [33:41<42:55, 23.41s/it]Running Inference:  46%|████▌     | 91/200 [34:04<42:29, 23.39s/it]Running Inference:  46%|████▌     | 92/200 [34:27<42:02, 23.36s/it]Running Inference:  46%|████▋     | 93/200 [34:51<41:33, 23.30s/it]Running Inference:  47%|████▋     | 94/200 [35:14<41:11, 23.32s/it]Running Inference:  48%|████▊     | 95/200 [35:37<40:50, 23.34s/it]Running Inference:  48%|████▊     | 96/200 [36:01<40:25, 23.32s/it]Running Inference:  48%|████▊     | 97/200 [36:24<40:03, 23.33s/it]Running Inference:  49%|████▉     | 98/200 [36:47<39:25, 23.19s/it]Running Inference:  50%|████▉     | 99/200 [37:11<39:48, 23.65s/it]Running Inference:  50%|█████     | 100/200 [37:35<39:16, 23.57s/it]Running Inference:  50%|█████     | 101/200 [37:58<38:48, 23.52s/it]Running Inference:  51%|█████     | 102/200 [38:22<38:19, 23.47s/it]Running Inference:  52%|█████▏    | 103/200 [38:45<38:05, 23.56s/it]Running Inference:  52%|█████▏    | 104/200 [39:09<37:47, 23.62s/it]Running Inference:  52%|█████▎    | 105/200 [39:33<37:17, 23.55s/it]Running Inference:  53%|█████▎    | 106/200 [39:56<36:45, 23.46s/it]Running Inference:  54%|█████▎    | 107/200 [40:19<36:17, 23.41s/it]Running Inference:  54%|█████▍    | 108/200 [40:42<35:30, 23.15s/it]Running Inference:  55%|█████▍    | 109/200 [41:05<35:09, 23.18s/it]Running Inference:  55%|█████▌    | 110/200 [41:28<34:46, 23.18s/it]Running Inference:  56%|█████▌    | 111/200 [41:51<34:27, 23.23s/it]Running Inference:  56%|█████▌    | 112/200 [42:15<34:04, 23.23s/it]Running Inference:  56%|█████▋    | 113/200 [42:38<33:47, 23.31s/it]Running Inference:  57%|█████▋    | 114/200 [42:54<30:08, 21.03s/it]Running Inference:  57%|█████▊    | 115/200 [43:17<30:43, 21.69s/it]Running Inference:  58%|█████▊    | 116/200 [43:40<31:00, 22.14s/it]Running Inference:  58%|█████▊    | 117/200 [44:03<31:03, 22.46s/it]Running Inference:  59%|█████▉    | 118/200 [44:27<31:01, 22.70s/it]Running Inference:  60%|█████▉    | 119/200 [44:50<30:41, 22.74s/it]Running Inference:  60%|██████    | 120/200 [45:13<30:30, 22.88s/it]Running Inference:  60%|██████    | 121/200 [45:35<29:55, 22.72s/it]Running Inference:  61%|██████    | 122/200 [45:58<29:42, 22.86s/it]Running Inference:  62%|██████▏   | 123/200 [46:22<29:31, 23.01s/it]Running Inference:  62%|██████▏   | 124/200 [46:38<26:35, 20.99s/it]Running Inference:  62%|██████▎   | 125/200 [47:01<27:05, 21.67s/it]Running Inference:  63%|██████▎   | 126/200 [47:24<27:16, 22.11s/it]Running Inference:  64%|██████▎   | 127/200 [47:48<27:20, 22.47s/it]Running Inference:  64%|██████▍   | 128/200 [48:12<27:33, 22.97s/it]Running Inference:  64%|██████▍   | 129/200 [48:35<27:14, 23.03s/it]Running Inference:  65%|██████▌   | 130/200 [48:58<26:54, 23.07s/it]Running Inference:  66%|██████▌   | 131/200 [49:21<26:26, 22.99s/it]Running Inference:  66%|██████▌   | 132/200 [49:44<26:09, 23.07s/it]Running Inference:  66%|██████▋   | 133/200 [50:07<25:48, 23.12s/it]Running Inference:  67%|██████▋   | 134/200 [50:31<25:30, 23.18s/it]Running Inference:  68%|██████▊   | 135/200 [50:55<25:19, 23.38s/it]Running Inference:  68%|██████▊   | 136/200 [51:18<24:58, 23.42s/it]Running Inference:  68%|██████▊   | 137/200 [51:42<24:35, 23.42s/it]Running Inference:  69%|██████▉   | 138/200 [52:05<24:07, 23.35s/it]Running Inference:  70%|██████▉   | 139/200 [52:08<17:38, 17.35s/it]Running Inference:  70%|███████   | 140/200 [52:31<19:07, 19.13s/it]Running Inference:  70%|███████   | 141/200 [52:55<20:03, 20.40s/it]Running Inference:  71%|███████   | 142/200 [53:18<20:32, 21.25s/it]Running Inference:  72%|███████▏  | 143/200 [53:40<20:25, 21.51s/it]Running Inference:  72%|███████▏  | 144/200 [54:03<20:31, 22.00s/it]Running Inference:  72%|███████▎  | 145/200 [54:26<20:30, 22.37s/it]Running Inference:  73%|███████▎  | 146/200 [54:50<20:21, 22.62s/it]Running Inference:  74%|███████▎  | 147/200 [55:13<20:08, 22.80s/it]Running Inference:  74%|███████▍  | 148/200 [55:36<19:50, 22.90s/it]Running Inference:  74%|███████▍  | 149/200 [55:59<19:31, 22.97s/it]Running Inference:  75%|███████▌  | 150/200 [56:23<19:18, 23.17s/it]Running Inference:  76%|███████▌  | 151/200 [56:46<18:56, 23.19s/it]Running Inference:  76%|███████▌  | 152/200 [57:09<18:35, 23.23s/it]Running Inference:  76%|███████▋  | 153/200 [57:33<18:16, 23.32s/it]Running Inference:  77%|███████▋  | 154/200 [57:56<17:52, 23.32s/it]Running Inference:  78%|███████▊  | 155/200 [58:20<17:32, 23.39s/it]Running Inference:  78%|███████▊  | 156/200 [58:20<12:09, 16.59s/it]Running Inference:  78%|███████▊  | 157/200 [58:44<13:22, 18.67s/it]Running Inference:  79%|███████▉  | 158/200 [59:07<14:04, 20.11s/it]Running Inference:  80%|███████▉  | 159/200 [59:31<14:22, 21.03s/it]Running Inference:  80%|████████  | 160/200 [59:54<14:31, 21.79s/it]Running Inference:  80%|████████  | 161/200 [1:00:17<14:26, 22.23s/it]Running Inference:  81%|████████  | 162/200 [1:00:41<14:17, 22.56s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:04<14:01, 22.75s/it]Running Inference:  82%|████████▏ | 164/200 [1:01:27<13:47, 22.97s/it]Running Inference:  82%|████████▎ | 165/200 [1:01:51<13:26, 23.04s/it]Running Inference:  83%|████████▎ | 166/200 [1:02:14<13:05, 23.10s/it]Running Inference:  84%|████████▎ | 167/200 [1:02:37<12:44, 23.16s/it]Running Inference:  84%|████████▍ | 168/200 [1:03:00<12:21, 23.18s/it]Running Inference:  84%|████████▍ | 169/200 [1:03:18<11:07, 21.55s/it]Running Inference:  85%|████████▌ | 170/200 [1:03:42<11:03, 22.13s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:05<10:52, 22.49s/it]Running Inference:  86%|████████▌ | 172/200 [1:04:28<10:37, 22.75s/it]Running Inference:  86%|████████▋ | 173/200 [1:04:51<10:17, 22.88s/it]Running Inference:  87%|████████▋ | 174/200 [1:05:15<09:58, 23.00s/it]Running Inference:  88%|████████▊ | 175/200 [1:05:38<09:37, 23.09s/it]Running Inference:  88%|████████▊ | 176/200 [1:06:01<09:15, 23.16s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:25<08:54, 23.22s/it]Running Inference:  89%|████████▉ | 178/200 [1:06:48<08:34, 23.38s/it]Running Inference:  90%|████████▉ | 179/200 [1:06:51<06:00, 17.18s/it]Running Inference:  90%|█████████ | 180/200 [1:07:14<06:19, 19.00s/it]Running Inference:  90%|█████████ | 181/200 [1:07:38<06:25, 20.27s/it]Running Inference:  91%|█████████ | 182/200 [1:08:01<06:20, 21.15s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:24<06:10, 21.77s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:49<06:01, 22.57s/it]Running Inference:  92%|█████████▎| 185/200 [1:09:12<05:42, 22.81s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:35<05:21, 22.93s/it]Running Inference:  94%|█████████▎| 187/200 [1:09:58<04:59, 23.03s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:22<04:37, 23.12s/it]Running Inference:  94%|█████████▍| 189/200 [1:10:45<04:15, 23.19s/it]Running Inference:  95%|█████████▌| 190/200 [1:11:08<03:52, 23.21s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:32<03:29, 23.32s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:55<03:06, 23.37s/it]Running Inference:  96%|█████████▋| 193/200 [1:12:19<02:43, 23.41s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:42<02:20, 23.36s/it]Running Inference:  98%|█████████▊| 195/200 [1:13:06<01:56, 23.37s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:14<01:16, 19.04s/it]Running Inference:  98%|█████████▊| 197/200 [1:13:38<01:00, 20.30s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:01<00:42, 21.17s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:24<00:21, 21.79s/it]Running Inference: 100%|██████████| 200/200 [1:14:48<00:00, 22.30s/it]Running Inference: 100%|██████████| 200/200 [1:14:48<00:00, 22.44s/it]
2025-12-14 04:43:23,041 - INFO - Inference completed.
2025-12-14 04:43:23,062 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 04:43:23,062 - INFO - Calculating metrics for dataset: longbench
2025-12-14 04:43:30,272 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 04:43:30,272 - INFO - Metrics:
13.31
2025-12-14 04:43:30,274 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news, ratio=0.2) on GPU 1

----------------------------------------
Task: multi_news | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 04:43:36,968 - INFO - Set deterministic seeds to 42
2025-12-14 04:43:36,968 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 04:43:36,968 - INFO - Starting evaluation run...
2025-12-14 04:43:36,968 - INFO - Output directory set to: longbenchresult
2025-12-14 04:43:36,968 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 04:43:36,968 - INFO - KV Press 'tova' setup.
2025-12-14 04:43:36,968 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 04:43:36,968 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.66it/s]
Device set to use cuda:0
2025-12-14 04:43:48,082 - INFO - Model pipeline loaded.
2025-12-14 04:43:48,082 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 04:43:51,351 - INFO - Dataset loaded with 200 entries.
2025-12-14 04:43:51,351 - INFO - Dataset processed with 200 entries.
2025-12-14 04:43:51,361 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:18:12, 23.58s/it]Running Inference:   1%|          | 2/200 [00:46<1:15:51, 22.99s/it]Running Inference:   2%|▏         | 3/200 [01:08<1:15:00, 22.85s/it]Running Inference:   2%|▏         | 4/200 [01:31<1:14:22, 22.77s/it]Running Inference:   2%|▎         | 5/200 [01:54<1:13:50, 22.72s/it]Running Inference:   3%|▎         | 6/200 [02:16<1:13:23, 22.70s/it]Running Inference:   4%|▎         | 7/200 [02:39<1:13:14, 22.77s/it]Running Inference:   4%|▍         | 8/200 [03:02<1:12:52, 22.77s/it]Running Inference:   4%|▍         | 9/200 [03:25<1:12:24, 22.75s/it]Running Inference:   5%|▌         | 10/200 [03:44<1:08:58, 21.78s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:07<1:09:21, 22.02s/it]Running Inference:   6%|▌         | 12/200 [04:30<1:09:39, 22.23s/it]Running Inference:   6%|▋         | 13/200 [04:52<1:09:40, 22.36s/it]Running Inference:   7%|▋         | 14/200 [05:15<1:09:31, 22.43s/it]Running Inference:   8%|▊         | 15/200 [05:38<1:09:35, 22.57s/it]Running Inference:   8%|▊         | 16/200 [06:00<1:08:57, 22.49s/it]Running Inference:   8%|▊         | 17/200 [06:23<1:08:43, 22.53s/it]Running Inference:   9%|▉         | 18/200 [06:46<1:09:05, 22.78s/it]Running Inference:  10%|▉         | 19/200 [07:09<1:08:36, 22.75s/it]Running Inference:  10%|█         | 20/200 [07:31<1:08:04, 22.69s/it]Running Inference:  10%|█         | 21/200 [07:54<1:07:32, 22.64s/it]Running Inference:  11%|█         | 22/200 [08:16<1:07:09, 22.64s/it]Running Inference:  12%|█▏        | 23/200 [08:39<1:06:45, 22.63s/it]Running Inference:  12%|█▏        | 24/200 [09:02<1:06:34, 22.70s/it]Running Inference:  12%|█▎        | 25/200 [09:21<1:02:55, 21.57s/it]Running Inference:  13%|█▎        | 26/200 [09:43<1:03:29, 21.90s/it]Running Inference:  14%|█▎        | 27/200 [10:07<1:04:54, 22.51s/it]Running Inference:  14%|█▍        | 28/200 [10:30<1:05:00, 22.68s/it]Running Inference:  14%|█▍        | 29/200 [10:53<1:04:32, 22.65s/it]Running Inference:  15%|█▌        | 30/200 [11:16<1:04:09, 22.64s/it]Running Inference:  16%|█▌        | 31/200 [11:39<1:03:56, 22.70s/it]Running Inference:  16%|█▌        | 32/200 [12:01<1:03:30, 22.68s/it]Running Inference:  16%|█▋        | 33/200 [12:24<1:03:03, 22.65s/it]Running Inference:  17%|█▋        | 34/200 [12:46<1:02:46, 22.69s/it]Running Inference:  18%|█▊        | 35/200 [13:09<1:02:22, 22.68s/it]Running Inference:  18%|█▊        | 36/200 [13:32<1:02:01, 22.69s/it]Running Inference:  18%|█▊        | 37/200 [13:55<1:01:55, 22.79s/it]Running Inference:  19%|█▉        | 38/200 [14:18<1:01:55, 22.93s/it]Running Inference:  20%|█▉        | 39/200 [14:41<1:01:18, 22.85s/it]Running Inference:  20%|██        | 40/200 [15:03<1:00:44, 22.78s/it]Running Inference:  20%|██        | 41/200 [15:26<1:00:18, 22.76s/it]Running Inference:  21%|██        | 42/200 [15:49<59:47, 22.70s/it]  Running Inference:  22%|██▏       | 43/200 [16:11<59:25, 22.71s/it]Running Inference:  22%|██▏       | 44/200 [16:15<44:07, 16.97s/it]Running Inference:  22%|██▎       | 45/200 [16:38<48:18, 18.70s/it]Running Inference:  23%|██▎       | 46/200 [17:00<51:00, 19.87s/it]Running Inference:  24%|██▎       | 47/200 [17:23<53:06, 20.82s/it]Running Inference:  24%|██▍       | 48/200 [17:46<54:20, 21.45s/it]Running Inference:  24%|██▍       | 49/200 [18:06<52:21, 20.81s/it]Running Inference:  25%|██▌       | 50/200 [18:28<53:31, 21.41s/it]Running Inference:  26%|██▌       | 51/200 [18:51<54:09, 21.81s/it]Running Inference:  26%|██▌       | 52/200 [19:14<54:31, 22.10s/it]Running Inference:  26%|██▋       | 53/200 [19:33<51:33, 21.04s/it]Running Inference:  27%|██▋       | 54/200 [19:55<52:23, 21.53s/it]Running Inference:  28%|██▊       | 55/200 [20:18<52:46, 21.84s/it]Running Inference:  28%|██▊       | 56/200 [20:41<53:05, 22.12s/it]Running Inference:  28%|██▊       | 57/200 [21:03<53:14, 22.34s/it]Running Inference:  29%|██▉       | 58/200 [21:26<53:04, 22.42s/it]Running Inference:  30%|██▉       | 59/200 [21:49<52:53, 22.50s/it]Running Inference:  30%|███       | 60/200 [22:11<52:32, 22.52s/it]Running Inference:  30%|███       | 61/200 [22:34<52:18, 22.58s/it]Running Inference:  31%|███       | 62/200 [22:57<52:01, 22.62s/it]Running Inference:  32%|███▏      | 63/200 [23:19<51:39, 22.62s/it]Running Inference:  32%|███▏      | 64/200 [23:42<51:17, 22.63s/it]Running Inference:  32%|███▎      | 65/200 [24:05<51:14, 22.77s/it]Running Inference:  33%|███▎      | 66/200 [24:28<50:47, 22.74s/it]Running Inference:  34%|███▎      | 67/200 [24:51<50:44, 22.89s/it]Running Inference:  34%|███▍      | 68/200 [25:14<50:18, 22.87s/it]Running Inference:  34%|███▍      | 69/200 [25:36<49:48, 22.81s/it]Running Inference:  35%|███▌      | 70/200 [25:59<49:19, 22.76s/it]Running Inference:  36%|███▌      | 71/200 [26:22<48:54, 22.75s/it]Running Inference:  36%|███▌      | 72/200 [26:45<48:37, 22.79s/it]Running Inference:  36%|███▋      | 73/200 [27:07<48:06, 22.73s/it]Running Inference:  37%|███▋      | 74/200 [27:30<47:42, 22.72s/it]Running Inference:  38%|███▊      | 75/200 [27:53<47:17, 22.70s/it]Running Inference:  38%|███▊      | 76/200 [28:15<46:48, 22.65s/it]Running Inference:  38%|███▊      | 77/200 [28:38<46:26, 22.65s/it]Running Inference:  39%|███▉      | 78/200 [29:01<46:04, 22.66s/it]Running Inference:  40%|███▉      | 79/200 [29:23<45:37, 22.63s/it]Running Inference:  40%|████      | 80/200 [29:46<45:19, 22.66s/it]Running Inference:  40%|████      | 81/200 [30:08<44:52, 22.63s/it]Running Inference:  41%|████      | 82/200 [30:28<42:40, 21.70s/it]Running Inference:  42%|████▏     | 83/200 [30:51<42:53, 22.00s/it]Running Inference:  42%|████▏     | 84/200 [31:13<43:01, 22.25s/it]Running Inference:  42%|████▎     | 85/200 [31:36<42:54, 22.38s/it]Running Inference:  43%|████▎     | 86/200 [31:59<42:50, 22.54s/it]Running Inference:  44%|████▎     | 87/200 [32:22<42:27, 22.54s/it]Running Inference:  44%|████▍     | 88/200 [32:44<42:02, 22.53s/it]Running Inference:  44%|████▍     | 89/200 [33:07<41:54, 22.65s/it]Running Inference:  45%|████▌     | 90/200 [33:30<41:38, 22.72s/it]Running Inference:  46%|████▌     | 91/200 [33:53<41:15, 22.71s/it]Running Inference:  46%|████▌     | 92/200 [34:15<40:52, 22.71s/it]Running Inference:  46%|████▋     | 93/200 [34:38<40:23, 22.65s/it]Running Inference:  47%|████▋     | 94/200 [35:01<40:04, 22.69s/it]Running Inference:  48%|████▊     | 95/200 [35:23<39:46, 22.73s/it]Running Inference:  48%|████▊     | 96/200 [35:46<39:21, 22.71s/it]Running Inference:  48%|████▊     | 97/200 [36:09<39:00, 22.72s/it]Running Inference:  49%|████▉     | 98/200 [36:32<38:39, 22.74s/it]Running Inference:  50%|████▉     | 99/200 [36:56<38:58, 23.15s/it]Running Inference:  50%|█████     | 100/200 [37:18<38:22, 23.03s/it]Running Inference:  50%|█████     | 101/200 [37:41<37:51, 22.95s/it]Running Inference:  51%|█████     | 102/200 [38:04<37:18, 22.85s/it]Running Inference:  52%|█████▏    | 103/200 [38:27<37:05, 22.94s/it]Running Inference:  52%|█████▏    | 104/200 [38:30<26:59, 16.87s/it]Running Inference:  52%|█████▎    | 105/200 [38:52<29:30, 18.64s/it]Running Inference:  53%|█████▎    | 106/200 [39:15<31:05, 19.84s/it]Running Inference:  54%|█████▎    | 107/200 [39:38<32:04, 20.69s/it]Running Inference:  54%|█████▍    | 108/200 [40:01<32:40, 21.31s/it]Running Inference:  55%|█████▍    | 109/200 [40:23<32:58, 21.74s/it]Running Inference:  55%|█████▌    | 110/200 [40:46<32:57, 21.97s/it]Running Inference:  56%|█████▌    | 111/200 [41:08<32:50, 22.15s/it]Running Inference:  56%|█████▌    | 112/200 [41:31<32:38, 22.25s/it]Running Inference:  56%|█████▋    | 113/200 [41:54<32:28, 22.40s/it]Running Inference:  57%|█████▋    | 114/200 [42:16<32:06, 22.40s/it]Running Inference:  57%|█████▊    | 115/200 [42:38<31:46, 22.43s/it]Running Inference:  58%|█████▊    | 116/200 [43:01<31:25, 22.44s/it]Running Inference:  58%|█████▊    | 117/200 [43:23<31:02, 22.43s/it]Running Inference:  59%|█████▉    | 118/200 [43:46<30:41, 22.46s/it]Running Inference:  60%|█████▉    | 119/200 [44:09<30:28, 22.57s/it]Running Inference:  60%|██████    | 120/200 [44:31<30:03, 22.55s/it]Running Inference:  60%|██████    | 121/200 [44:54<29:40, 22.54s/it]Running Inference:  61%|██████    | 122/200 [45:16<29:15, 22.51s/it]Running Inference:  62%|██████▏   | 123/200 [45:39<28:55, 22.54s/it]Running Inference:  62%|██████▏   | 124/200 [46:01<28:29, 22.50s/it]Running Inference:  62%|██████▎   | 125/200 [46:24<28:08, 22.51s/it]Running Inference:  63%|██████▎   | 126/200 [46:46<27:43, 22.48s/it]Running Inference:  64%|██████▎   | 127/200 [47:09<27:22, 22.50s/it]Running Inference:  64%|██████▍   | 128/200 [47:32<27:19, 22.77s/it]Running Inference:  64%|██████▍   | 129/200 [47:54<26:48, 22.66s/it]Running Inference:  65%|██████▌   | 130/200 [48:17<26:20, 22.58s/it]Running Inference:  66%|██████▌   | 131/200 [48:41<26:28, 23.02s/it]Running Inference:  66%|██████▌   | 132/200 [49:03<25:55, 22.88s/it]Running Inference:  66%|██████▋   | 133/200 [49:26<25:24, 22.76s/it]Running Inference:  67%|██████▋   | 134/200 [49:49<24:58, 22.71s/it]Running Inference:  68%|██████▊   | 135/200 [50:12<24:44, 22.84s/it]Running Inference:  68%|██████▊   | 136/200 [50:34<24:20, 22.82s/it]Running Inference:  68%|██████▊   | 137/200 [50:57<23:54, 22.77s/it]Running Inference:  69%|██████▉   | 138/200 [51:20<23:26, 22.69s/it]Running Inference:  70%|██████▉   | 139/200 [51:23<17:08, 16.86s/it]Running Inference:  70%|███████   | 140/200 [51:45<18:34, 18.58s/it]Running Inference:  70%|███████   | 141/200 [52:08<19:28, 19.80s/it]Running Inference:  71%|███████   | 142/200 [52:31<19:55, 20.62s/it]Running Inference:  72%|███████▏  | 143/200 [52:53<20:07, 21.19s/it]Running Inference:  72%|███████▏  | 144/200 [53:16<20:07, 21.56s/it]Running Inference:  72%|███████▎  | 145/200 [53:38<20:01, 21.84s/it]Running Inference:  73%|███████▎  | 146/200 [54:01<19:49, 22.03s/it]Running Inference:  74%|███████▎  | 147/200 [54:23<19:35, 22.18s/it]Running Inference:  74%|███████▍  | 148/200 [54:45<19:16, 22.24s/it]Running Inference:  74%|███████▍  | 149/200 [55:08<18:56, 22.29s/it]Running Inference:  75%|███████▌  | 150/200 [55:31<18:43, 22.48s/it]Running Inference:  76%|███████▌  | 151/200 [55:53<18:22, 22.49s/it]Running Inference:  76%|███████▌  | 152/200 [56:16<18:01, 22.54s/it]Running Inference:  76%|███████▋  | 153/200 [56:39<17:43, 22.62s/it]Running Inference:  77%|███████▋  | 154/200 [57:01<17:20, 22.61s/it]Running Inference:  78%|███████▊  | 155/200 [57:24<17:00, 22.68s/it]Running Inference:  78%|███████▊  | 156/200 [57:47<16:35, 22.62s/it]Running Inference:  78%|███████▊  | 157/200 [58:09<16:15, 22.68s/it]Running Inference:  79%|███████▉  | 158/200 [58:32<15:53, 22.70s/it]Running Inference:  80%|███████▉  | 159/200 [58:55<15:28, 22.65s/it]Running Inference:  80%|████████  | 160/200 [59:18<15:08, 22.70s/it]Running Inference:  80%|████████  | 161/200 [59:40<14:43, 22.65s/it]Running Inference:  81%|████████  | 162/200 [59:52<12:16, 19.38s/it]Running Inference:  82%|████████▏ | 163/200 [1:00:14<12:31, 20.32s/it]Running Inference:  82%|████████▏ | 164/200 [1:00:37<12:37, 21.04s/it]Running Inference:  82%|████████▎ | 165/200 [1:01:00<12:31, 21.46s/it]Running Inference:  83%|████████▎ | 166/200 [1:01:22<12:20, 21.78s/it]Running Inference:  84%|████████▎ | 167/200 [1:01:45<12:06, 22.01s/it]Running Inference:  84%|████████▍ | 168/200 [1:02:07<11:49, 22.16s/it]Running Inference:  84%|████████▍ | 169/200 [1:02:20<10:00, 19.39s/it]Running Inference:  85%|████████▌ | 170/200 [1:02:43<10:11, 20.39s/it]Running Inference:  86%|████████▌ | 171/200 [1:03:05<10:10, 21.05s/it]Running Inference:  86%|████████▌ | 172/200 [1:03:23<09:20, 20.01s/it]Running Inference:  86%|████████▋ | 173/200 [1:03:45<09:19, 20.73s/it]Running Inference:  87%|████████▋ | 174/200 [1:04:08<09:13, 21.28s/it]Running Inference:  88%|████████▊ | 175/200 [1:04:18<07:26, 17.85s/it]Running Inference:  88%|████████▊ | 176/200 [1:04:40<07:42, 19.26s/it]Running Inference:  88%|████████▊ | 177/200 [1:05:03<07:46, 20.28s/it]Running Inference:  89%|████████▉ | 178/200 [1:05:26<07:44, 21.10s/it]Running Inference:  90%|████████▉ | 179/200 [1:05:29<05:26, 15.55s/it]Running Inference:  90%|█████████ | 180/200 [1:05:51<05:52, 17.64s/it]Running Inference:  90%|█████████ | 181/200 [1:06:14<06:03, 19.11s/it]Running Inference:  91%|█████████ | 182/200 [1:06:36<06:02, 20.13s/it]Running Inference:  92%|█████████▏| 183/200 [1:06:59<05:54, 20.83s/it]Running Inference:  92%|█████████▏| 184/200 [1:07:22<05:47, 21.69s/it]Running Inference:  92%|█████████▎| 185/200 [1:07:45<05:29, 21.97s/it]Running Inference:  93%|█████████▎| 186/200 [1:08:07<05:09, 22.12s/it]Running Inference:  94%|█████████▎| 187/200 [1:08:30<04:49, 22.24s/it]Running Inference:  94%|█████████▍| 188/200 [1:08:53<04:28, 22.34s/it]Running Inference:  94%|█████████▍| 189/200 [1:09:15<04:06, 22.43s/it]Running Inference:  95%|█████████▌| 190/200 [1:09:38<03:44, 22.47s/it]Running Inference:  96%|█████████▌| 191/200 [1:10:01<03:23, 22.58s/it]Running Inference:  96%|█████████▌| 192/200 [1:10:02<02:09, 16.13s/it]Running Inference:  96%|█████████▋| 193/200 [1:10:25<02:07, 18.15s/it]Running Inference:  97%|█████████▋| 194/200 [1:10:47<01:56, 19.49s/it]Running Inference:  98%|█████████▊| 195/200 [1:11:10<01:42, 20.45s/it]Running Inference:  98%|█████████▊| 196/200 [1:11:33<01:24, 21.19s/it]Running Inference:  98%|█████████▊| 197/200 [1:11:55<01:04, 21.59s/it]Running Inference:  99%|█████████▉| 198/200 [1:12:18<00:43, 21.87s/it]Running Inference: 100%|█████████▉| 199/200 [1:12:40<00:22, 22.05s/it]Running Inference: 100%|██████████| 200/200 [1:13:03<00:00, 22.27s/it]Running Inference: 100%|██████████| 200/200 [1:13:03<00:00, 21.92s/it]
2025-12-14 05:56:54,935 - INFO - Inference completed.
2025-12-14 05:56:54,957 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 05:56:54,957 - INFO - Calculating metrics for dataset: longbench
2025-12-14 05:57:02,402 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 05:57:02,402 - INFO - Metrics:
12.74
2025-12-14 05:57:02,404 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news, ratio=0.3) on GPU 1

----------------------------------------
Task: multi_news | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 05:57:09,079 - INFO - Set deterministic seeds to 42
2025-12-14 05:57:09,079 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 05:57:09,079 - INFO - Starting evaluation run...
2025-12-14 05:57:09,079 - INFO - Output directory set to: longbenchresult
2025-12-14 05:57:09,079 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 05:57:09,079 - INFO - KV Press 'tova' setup.
2025-12-14 05:57:09,079 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 05:57:09,079 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.48it/s]
Device set to use cuda:0
2025-12-14 05:57:19,837 - INFO - Model pipeline loaded.
2025-12-14 05:57:19,837 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 05:57:23,058 - INFO - Dataset loaded with 200 entries.
2025-12-14 05:57:23,058 - INFO - Dataset processed with 200 entries.
2025-12-14 05:57:23,068 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:19:42, 24.03s/it]Running Inference:   1%|          | 2/200 [00:47<1:17:26, 23.47s/it]Running Inference:   2%|▏         | 3/200 [01:10<1:16:34, 23.32s/it]Running Inference:   2%|▏         | 4/200 [01:33<1:15:57, 23.25s/it]Running Inference:   2%|▎         | 5/200 [01:56<1:15:25, 23.21s/it]Running Inference:   3%|▎         | 6/200 [02:19<1:14:55, 23.17s/it]Running Inference:   4%|▎         | 7/200 [02:43<1:14:44, 23.24s/it]Running Inference:   4%|▍         | 8/200 [03:06<1:14:21, 23.24s/it]Running Inference:   4%|▍         | 9/200 [03:29<1:13:51, 23.20s/it]Running Inference:   5%|▌         | 10/200 [03:52<1:13:19, 23.15s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:15<1:12:46, 23.10s/it]Running Inference:   6%|▌         | 12/200 [04:38<1:12:27, 23.13s/it]Running Inference:   6%|▋         | 13/200 [05:01<1:12:05, 23.13s/it]Running Inference:   7%|▋         | 14/200 [05:22<1:09:08, 22.30s/it]Running Inference:   8%|▊         | 15/200 [05:45<1:09:44, 22.62s/it]Running Inference:   8%|▊         | 16/200 [06:08<1:09:51, 22.78s/it]Running Inference:   8%|▊         | 17/200 [06:31<1:09:45, 22.87s/it]Running Inference:   9%|▉         | 18/200 [06:55<1:10:09, 23.13s/it]Running Inference:  10%|▉         | 19/200 [07:18<1:09:44, 23.12s/it]Running Inference:  10%|█         | 20/200 [07:41<1:09:13, 23.07s/it]Running Inference:  10%|█         | 21/200 [08:04<1:08:54, 23.10s/it]Running Inference:  11%|█         | 22/200 [08:27<1:08:31, 23.10s/it]Running Inference:  12%|█▏        | 23/200 [08:50<1:08:06, 23.08s/it]Running Inference:  12%|█▏        | 24/200 [09:14<1:07:53, 23.14s/it]Running Inference:  12%|█▎        | 25/200 [09:37<1:07:26, 23.12s/it]Running Inference:  13%|█▎        | 26/200 [10:00<1:07:02, 23.12s/it]Running Inference:  14%|█▎        | 27/200 [10:24<1:07:46, 23.51s/it]Running Inference:  14%|█▍        | 28/200 [10:48<1:07:21, 23.50s/it]Running Inference:  14%|█▍        | 29/200 [11:11<1:06:32, 23.35s/it]Running Inference:  15%|█▌        | 30/200 [11:34<1:05:54, 23.26s/it]Running Inference:  16%|█▌        | 31/200 [11:57<1:05:29, 23.25s/it]Running Inference:  16%|█▌        | 32/200 [12:20<1:04:56, 23.19s/it]Running Inference:  16%|█▋        | 33/200 [12:43<1:04:24, 23.14s/it]Running Inference:  17%|█▋        | 34/200 [13:06<1:04:02, 23.15s/it]Running Inference:  18%|█▊        | 35/200 [13:29<1:03:36, 23.13s/it]Running Inference:  18%|█▊        | 36/200 [13:52<1:03:12, 23.13s/it]Running Inference:  18%|█▊        | 37/200 [14:16<1:03:05, 23.22s/it]Running Inference:  19%|█▉        | 38/200 [14:39<1:03:03, 23.35s/it]Running Inference:  20%|█▉        | 39/200 [15:03<1:02:26, 23.27s/it]Running Inference:  20%|██        | 40/200 [15:24<1:00:56, 22.85s/it]Running Inference:  20%|██        | 41/200 [15:48<1:00:47, 22.94s/it]Running Inference:  21%|██        | 42/200 [16:11<1:00:41, 23.05s/it]Running Inference:  22%|██▏       | 43/200 [16:34<1:00:16, 23.03s/it]Running Inference:  22%|██▏       | 44/200 [16:37<44:18, 17.04s/it]  Running Inference:  22%|██▎       | 45/200 [17:00<48:44, 18.87s/it]Running Inference:  23%|██▎       | 46/200 [17:23<51:36, 20.11s/it]Running Inference:  24%|██▎       | 47/200 [17:47<53:50, 21.11s/it]Running Inference:  24%|██▍       | 48/200 [18:10<55:08, 21.77s/it]Running Inference:  24%|██▍       | 49/200 [18:33<55:41, 22.13s/it]Running Inference:  25%|██▌       | 50/200 [18:56<56:08, 22.46s/it]Running Inference:  26%|██▌       | 51/200 [19:19<56:15, 22.65s/it]Running Inference:  26%|██▌       | 52/200 [19:42<56:14, 22.80s/it]Running Inference:  26%|██▋       | 53/200 [20:05<56:02, 22.87s/it]Running Inference:  27%|██▋       | 54/200 [20:28<55:48, 22.93s/it]Running Inference:  28%|██▊       | 55/200 [20:52<55:35, 23.01s/it]Running Inference:  28%|██▊       | 56/200 [21:15<55:18, 23.05s/it]Running Inference:  28%|██▊       | 57/200 [21:38<55:02, 23.10s/it]Running Inference:  29%|██▉       | 58/200 [22:01<54:35, 23.07s/it]Running Inference:  30%|██▉       | 59/200 [22:24<54:13, 23.07s/it]Running Inference:  30%|███       | 60/200 [22:47<53:49, 23.07s/it]Running Inference:  30%|███       | 61/200 [23:10<53:29, 23.09s/it]Running Inference:  31%|███       | 62/200 [23:33<53:05, 23.08s/it]Running Inference:  32%|███▏      | 63/200 [23:56<52:42, 23.08s/it]Running Inference:  32%|███▏      | 64/200 [24:19<52:16, 23.06s/it]Running Inference:  32%|███▎      | 65/200 [24:43<52:10, 23.19s/it]Running Inference:  33%|███▎      | 66/200 [25:06<51:43, 23.16s/it]Running Inference:  34%|███▎      | 67/200 [25:30<51:37, 23.29s/it]Running Inference:  34%|███▍      | 68/200 [25:53<51:09, 23.25s/it]Running Inference:  34%|███▍      | 69/200 [26:16<50:39, 23.20s/it]Running Inference:  35%|███▌      | 70/200 [26:39<50:09, 23.15s/it]Running Inference:  36%|███▌      | 71/200 [27:02<49:43, 23.13s/it]Running Inference:  36%|███▌      | 72/200 [27:25<49:26, 23.17s/it]Running Inference:  36%|███▋      | 73/200 [27:48<48:54, 23.11s/it]Running Inference:  37%|███▋      | 74/200 [28:11<48:30, 23.10s/it]Running Inference:  38%|███▊      | 75/200 [28:34<48:05, 23.09s/it]Running Inference:  38%|███▊      | 76/200 [28:57<47:45, 23.11s/it]Running Inference:  38%|███▊      | 77/200 [29:21<47:20, 23.10s/it]Running Inference:  39%|███▉      | 78/200 [29:44<46:57, 23.09s/it]Running Inference:  40%|███▉      | 79/200 [30:07<46:37, 23.12s/it]Running Inference:  40%|████      | 80/200 [30:30<46:15, 23.13s/it]Running Inference:  40%|████      | 81/200 [30:53<45:50, 23.11s/it]Running Inference:  41%|████      | 82/200 [31:16<45:23, 23.08s/it]Running Inference:  42%|████▏     | 83/200 [31:39<45:01, 23.09s/it]Running Inference:  42%|████▏     | 84/200 [32:02<44:44, 23.14s/it]Running Inference:  42%|████▎     | 85/200 [32:25<44:19, 23.12s/it]Running Inference:  43%|████▎     | 86/200 [32:49<44:05, 23.21s/it]Running Inference:  44%|████▎     | 87/200 [33:12<43:42, 23.21s/it]Running Inference:  44%|████▍     | 88/200 [33:35<43:21, 23.23s/it]Running Inference:  44%|████▍     | 89/200 [33:59<43:02, 23.27s/it]Running Inference:  45%|████▌     | 90/200 [34:22<42:39, 23.27s/it]Running Inference:  46%|████▌     | 91/200 [34:45<42:11, 23.22s/it]Running Inference:  46%|████▌     | 92/200 [35:08<41:43, 23.18s/it]Running Inference:  46%|████▋     | 93/200 [35:31<41:18, 23.16s/it]Running Inference:  47%|████▋     | 94/200 [35:54<40:55, 23.16s/it]Running Inference:  48%|████▊     | 95/200 [36:18<40:33, 23.18s/it]Running Inference:  48%|████▊     | 96/200 [36:41<40:08, 23.15s/it]Running Inference:  48%|████▊     | 97/200 [37:04<39:45, 23.16s/it]Running Inference:  49%|████▉     | 98/200 [37:27<39:22, 23.16s/it]Running Inference:  50%|████▉     | 99/200 [37:52<39:41, 23.58s/it]Running Inference:  50%|█████     | 100/200 [38:15<39:05, 23.45s/it]Running Inference:  50%|█████     | 101/200 [38:38<38:33, 23.37s/it]Running Inference:  51%|█████     | 102/200 [38:51<32:54, 20.15s/it]Running Inference:  52%|█████▏    | 103/200 [39:14<34:14, 21.18s/it]Running Inference:  52%|█████▏    | 104/200 [39:33<32:30, 20.32s/it]Running Inference:  52%|█████▎    | 105/200 [39:56<33:31, 21.18s/it]Running Inference:  53%|█████▎    | 106/200 [40:19<34:04, 21.75s/it]Running Inference:  54%|█████▎    | 107/200 [40:42<34:21, 22.17s/it]Running Inference:  54%|█████▍    | 108/200 [41:05<34:26, 22.47s/it]Running Inference:  55%|█████▍    | 109/200 [41:28<34:19, 22.64s/it]Running Inference:  55%|█████▌    | 110/200 [41:51<34:06, 22.74s/it]Running Inference:  56%|█████▌    | 111/200 [42:14<33:53, 22.85s/it]Running Inference:  56%|█████▌    | 112/200 [42:37<33:35, 22.91s/it]Running Inference:  56%|█████▋    | 113/200 [42:57<31:50, 21.96s/it]Running Inference:  57%|█████▋    | 114/200 [43:20<31:59, 22.31s/it]Running Inference:  57%|█████▊    | 115/200 [43:43<31:54, 22.53s/it]Running Inference:  58%|█████▊    | 116/200 [44:06<31:45, 22.68s/it]Running Inference:  58%|█████▊    | 117/200 [44:29<31:28, 22.75s/it]Running Inference:  59%|█████▉    | 118/200 [44:52<31:05, 22.75s/it]Running Inference:  60%|█████▉    | 119/200 [45:15<30:49, 22.84s/it]Running Inference:  60%|██████    | 120/200 [45:38<30:22, 22.78s/it]Running Inference:  60%|██████    | 121/200 [45:57<28:35, 21.72s/it]Running Inference:  61%|██████    | 122/200 [46:19<28:35, 21.99s/it]Running Inference:  62%|██████▏   | 123/200 [46:42<28:32, 22.25s/it]Running Inference:  62%|██████▏   | 124/200 [47:05<28:20, 22.38s/it]Running Inference:  62%|██████▎   | 125/200 [47:21<25:37, 20.50s/it]Running Inference:  63%|██████▎   | 126/200 [47:44<26:06, 21.16s/it]Running Inference:  64%|██████▎   | 127/200 [48:07<26:20, 21.65s/it]Running Inference:  64%|██████▍   | 128/200 [48:30<26:41, 22.24s/it]Running Inference:  64%|██████▍   | 129/200 [48:53<26:30, 22.41s/it]Running Inference:  65%|██████▌   | 130/200 [49:16<26:14, 22.50s/it]Running Inference:  66%|██████▌   | 131/200 [49:36<25:13, 21.93s/it]Running Inference:  66%|██████▌   | 132/200 [49:59<25:08, 22.19s/it]Running Inference:  66%|██████▋   | 133/200 [50:22<24:56, 22.34s/it]Running Inference:  67%|██████▋   | 134/200 [50:45<24:43, 22.47s/it]Running Inference:  68%|██████▊   | 135/200 [51:08<24:37, 22.73s/it]Running Inference:  68%|██████▊   | 136/200 [51:31<24:19, 22.80s/it]Running Inference:  68%|██████▊   | 137/200 [51:54<23:57, 22.82s/it]Running Inference:  69%|██████▉   | 138/200 [52:16<23:31, 22.77s/it]Running Inference:  70%|██████▉   | 139/200 [52:39<23:07, 22.74s/it]Running Inference:  70%|███████   | 140/200 [53:02<22:45, 22.75s/it]Running Inference:  70%|███████   | 141/200 [53:25<22:23, 22.76s/it]Running Inference:  71%|███████   | 142/200 [53:47<21:59, 22.76s/it]Running Inference:  72%|███████▏  | 143/200 [54:10<21:36, 22.75s/it]Running Inference:  72%|███████▏  | 144/200 [54:33<21:12, 22.73s/it]Running Inference:  72%|███████▎  | 145/200 [54:55<20:49, 22.73s/it]Running Inference:  73%|███████▎  | 146/200 [55:18<20:26, 22.72s/it]Running Inference:  74%|███████▎  | 147/200 [55:41<20:04, 22.72s/it]Running Inference:  74%|███████▍  | 148/200 [56:04<19:43, 22.76s/it]Running Inference:  74%|███████▍  | 149/200 [56:21<18:03, 21.24s/it]Running Inference:  75%|███████▌  | 150/200 [56:45<18:09, 21.80s/it]Running Inference:  76%|███████▌  | 151/200 [57:07<18:01, 22.08s/it]Running Inference:  76%|███████▌  | 152/200 [57:30<17:50, 22.30s/it]Running Inference:  76%|███████▋  | 153/200 [57:53<17:38, 22.52s/it]Running Inference:  77%|███████▋  | 154/200 [58:16<17:19, 22.60s/it]Running Inference:  78%|███████▊  | 155/200 [58:39<17:02, 22.73s/it]Running Inference:  78%|███████▊  | 156/200 [59:02<16:40, 22.74s/it]Running Inference:  78%|███████▊  | 157/200 [59:08<12:48, 17.88s/it]Running Inference:  79%|███████▉  | 158/200 [59:31<13:34, 19.40s/it]Running Inference:  80%|███████▉  | 159/200 [59:54<13:55, 20.39s/it]Running Inference:  80%|████████  | 160/200 [1:00:17<14:07, 21.19s/it]Running Inference:  80%|████████  | 161/200 [1:00:40<14:04, 21.65s/it]Running Inference:  81%|████████  | 162/200 [1:01:02<13:54, 21.97s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:25<13:41, 22.20s/it]Running Inference:  82%|████████▏ | 164/200 [1:01:48<13:27, 22.42s/it]Running Inference:  82%|████████▎ | 165/200 [1:02:11<13:06, 22.47s/it]Running Inference:  83%|████████▎ | 166/200 [1:02:33<12:46, 22.56s/it]Running Inference:  84%|████████▎ | 167/200 [1:02:56<12:26, 22.63s/it]Running Inference:  84%|████████▍ | 168/200 [1:03:19<12:04, 22.65s/it]Running Inference:  84%|████████▍ | 169/200 [1:03:42<11:43, 22.69s/it]Running Inference:  85%|████████▌ | 170/200 [1:04:05<11:22, 22.76s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:27<11:00, 22.77s/it]Running Inference:  86%|████████▌ | 172/200 [1:04:50<10:37, 22.78s/it]Running Inference:  86%|████████▋ | 173/200 [1:05:13<10:14, 22.77s/it]Running Inference:  87%|████████▋ | 174/200 [1:05:36<09:51, 22.76s/it]Running Inference:  88%|████████▊ | 175/200 [1:05:42<07:23, 17.74s/it]Running Inference:  88%|████████▊ | 176/200 [1:06:04<07:41, 19.24s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:27<07:47, 20.31s/it]Running Inference:  89%|████████▉ | 178/200 [1:06:50<07:45, 21.16s/it]Running Inference:  90%|████████▉ | 179/200 [1:07:13<07:33, 21.62s/it]Running Inference:  90%|█████████ | 180/200 [1:07:36<07:18, 21.94s/it]Running Inference:  90%|█████████ | 181/200 [1:07:58<07:01, 22.16s/it]Running Inference:  91%|█████████ | 182/200 [1:08:21<06:41, 22.30s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:44<06:20, 22.40s/it]Running Inference:  92%|█████████▏| 184/200 [1:09:08<06:05, 22.84s/it]Running Inference:  92%|█████████▎| 185/200 [1:09:30<05:42, 22.82s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:53<05:18, 22.77s/it]Running Inference:  94%|█████████▎| 187/200 [1:10:16<04:55, 22.73s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:38<04:32, 22.74s/it]Running Inference:  94%|█████████▍| 189/200 [1:11:01<04:10, 22.74s/it]Running Inference:  95%|█████████▌| 190/200 [1:11:24<03:47, 22.73s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:47<03:25, 22.82s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:48<02:10, 16.31s/it]Running Inference:  96%|█████████▋| 193/200 [1:12:11<02:08, 18.30s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:34<01:57, 19.61s/it]Running Inference:  98%|█████████▊| 195/200 [1:12:56<01:42, 20.58s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:20<01:25, 21.33s/it]Running Inference:  98%|█████████▊| 197/200 [1:13:42<01:05, 21.73s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:05<00:44, 22.01s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:28<00:22, 22.20s/it]Running Inference: 100%|██████████| 200/200 [1:14:50<00:00, 22.42s/it]Running Inference: 100%|██████████| 200/200 [1:14:50<00:00, 22.45s/it]
2025-12-14 07:12:14,031 - INFO - Inference completed.
2025-12-14 07:12:14,052 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 07:12:14,052 - INFO - Calculating metrics for dataset: longbench
2025-12-14 07:12:21,447 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 07:12:21,447 - INFO - Metrics:
11.79
2025-12-14 07:12:21,448 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news, ratio=0.5) on GPU 1


========================================
LongBench Task: qmsum
========================================
----------------------------------------
Task: qmsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 07:12:28,125 - INFO - Set deterministic seeds to 42
2025-12-14 07:12:28,125 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 07:12:28,125 - INFO - Starting evaluation run...
2025-12-14 07:12:28,125 - INFO - Output directory set to: longbenchresult
2025-12-14 07:12:28,125 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 07:12:28,125 - INFO - KV Press 'tova' setup.
2025-12-14 07:12:28,125 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 07:12:28,125 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.73it/s]
Device set to use cuda:0
2025-12-14 07:12:39,733 - INFO - Model pipeline loaded.
2025-12-14 07:12:39,733 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 2901.99 examples/s]
2025-12-14 07:12:43,809 - INFO - Dataset loaded with 200 entries.
2025-12-14 07:12:43,809 - INFO - Dataset processed with 200 entries.
2025-12-14 07:12:43,824 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:15<42:33, 75.09s/it]Running Inference:   6%|▌         | 2/35 [01:47<27:20, 49.71s/it]Running Inference:   9%|▊         | 3/35 [02:49<29:30, 55.32s/it]Running Inference:  11%|█▏        | 4/35 [03:32<26:12, 50.73s/it]Running Inference:  14%|█▍        | 5/35 [04:10<23:05, 46.18s/it]Running Inference:  17%|█▋        | 6/35 [04:50<21:15, 43.98s/it]Running Inference:  20%|██        | 7/35 [05:23<18:53, 40.47s/it]Running Inference:  23%|██▎       | 8/35 [06:37<23:00, 51.13s/it]Running Inference:  26%|██▌       | 9/35 [08:49<33:00, 76.19s/it]Running Inference:  29%|██▊       | 10/35 [10:04<31:42, 76.09s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [11:32<31:53, 79.73s/it]Running Inference:  34%|███▍      | 12/35 [12:00<24:26, 63.75s/it]Running Inference:  37%|███▋      | 13/35 [12:49<21:45, 59.36s/it]Running Inference:  40%|████      | 14/35 [14:30<25:09, 71.90s/it]Running Inference:  43%|████▎     | 15/35 [14:58<19:37, 58.87s/it]Running Inference:  46%|████▌     | 16/35 [15:54<18:21, 58.00s/it]Running Inference:  49%|████▊     | 17/35 [16:35<15:47, 52.64s/it]Running Inference:  51%|█████▏    | 18/35 [17:05<13:03, 46.06s/it]Running Inference:  54%|█████▍    | 19/35 [17:19<09:41, 36.34s/it]Running Inference:  57%|█████▋    | 20/35 [17:40<07:56, 31.78s/it]Running Inference:  60%|██████    | 21/35 [18:12<07:26, 31.92s/it]Running Inference:  63%|██████▎   | 22/35 [18:28<05:50, 27.00s/it]Running Inference:  66%|██████▌   | 23/35 [20:09<09:49, 49.16s/it]Running Inference:  69%|██████▊   | 24/35 [21:35<11:03, 60.28s/it]Running Inference:  71%|███████▏  | 25/35 [22:17<09:09, 54.91s/it]Running Inference:  74%|███████▍  | 26/35 [22:43<06:53, 45.98s/it]Running Inference:  77%|███████▋  | 27/35 [23:04<05:08, 38.59s/it]Running Inference:  80%|████████  | 28/35 [24:26<06:01, 51.70s/it]Running Inference:  83%|████████▎ | 29/35 [25:15<05:05, 50.91s/it]Running Inference:  86%|████████▌ | 30/35 [25:38<03:32, 42.52s/it]Running Inference:  89%|████████▊ | 31/35 [26:23<02:52, 43.21s/it]Running Inference:  91%|█████████▏| 32/35 [27:30<02:30, 50.30s/it]Running Inference:  94%|█████████▍| 33/35 [29:58<02:39, 79.54s/it]Running Inference:  97%|█████████▋| 34/35 [30:51<01:11, 71.69s/it]Running Inference: 100%|██████████| 35/35 [31:32<00:00, 62.35s/it]Running Inference: 100%|██████████| 35/35 [31:32<00:00, 54.06s/it]
2025-12-14 07:44:15,842 - INFO - Inference completed.
2025-12-14 07:44:15,855 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 07:44:15,855 - INFO - Calculating metrics for dataset: longbench
2025-12-14 07:44:17,038 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 07:44:17,039 - INFO - Metrics:
19.58
2025-12-14 07:44:17,040 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qmsum, ratio=0.1) on GPU 1

----------------------------------------
Task: qmsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 07:44:23,490 - INFO - Set deterministic seeds to 42
2025-12-14 07:44:23,490 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 07:44:23,490 - INFO - Starting evaluation run...
2025-12-14 07:44:23,490 - INFO - Output directory set to: longbenchresult
2025-12-14 07:44:23,490 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 07:44:23,490 - INFO - KV Press 'tova' setup.
2025-12-14 07:44:23,490 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 07:44:23,490 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.08it/s]
Device set to use cuda:0
2025-12-14 07:44:36,010 - INFO - Model pipeline loaded.
2025-12-14 07:44:36,010 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 07:44:39,247 - INFO - Dataset loaded with 200 entries.
2025-12-14 07:44:39,247 - INFO - Dataset processed with 200 entries.
2025-12-14 07:44:39,261 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:14<42:03, 74.21s/it]Running Inference:   6%|▌         | 2/35 [02:05<33:19, 60.58s/it]Running Inference:   9%|▊         | 3/35 [03:23<36:39, 68.75s/it]Running Inference:  11%|█▏        | 4/35 [04:08<30:32, 59.12s/it]Running Inference:  14%|█▍        | 5/35 [04:46<25:44, 51.49s/it]Running Inference:  17%|█▋        | 6/35 [05:38<25:06, 51.95s/it]Running Inference:  20%|██        | 7/35 [06:11<21:13, 45.47s/it]Running Inference:  23%|██▎       | 8/35 [07:25<24:34, 54.63s/it]Running Inference:  26%|██▌       | 9/35 [09:53<36:22, 83.96s/it]Running Inference:  29%|██▊       | 10/35 [10:51<31:35, 75.83s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [12:22<32:14, 80.62s/it]Running Inference:  34%|███▍      | 12/35 [12:46<24:12, 63.17s/it]Running Inference:  37%|███▋      | 13/35 [14:28<27:30, 75.03s/it]Running Inference:  40%|████      | 14/35 [15:32<25:03, 71.59s/it]Running Inference:  43%|████▎     | 15/35 [16:00<19:34, 58.72s/it]Running Inference:  46%|████▌     | 16/35 [16:54<18:05, 57.13s/it]Running Inference:  49%|████▊     | 17/35 [17:51<17:05, 56.99s/it]Running Inference:  51%|█████▏    | 18/35 [18:21<13:54, 49.06s/it]Running Inference:  54%|█████▍    | 19/35 [18:34<10:10, 38.14s/it]Running Inference:  57%|█████▋    | 20/35 [19:09<09:19, 37.29s/it]Running Inference:  60%|██████    | 21/35 [19:23<07:05, 30.38s/it]Running Inference:  63%|██████▎   | 22/35 [19:36<05:25, 25.04s/it]Running Inference:  66%|██████▌   | 23/35 [21:01<08:37, 43.15s/it]Running Inference:  69%|██████▊   | 24/35 [22:29<10:21, 56.46s/it]Running Inference:  71%|███████▏  | 25/35 [23:10<08:39, 51.93s/it]Running Inference:  74%|███████▍  | 26/35 [24:11<08:11, 54.59s/it]Running Inference:  77%|███████▋  | 27/35 [24:48<06:35, 49.39s/it]Running Inference:  80%|████████  | 28/35 [26:09<06:52, 58.92s/it]Running Inference:  83%|████████▎ | 29/35 [27:33<06:37, 66.24s/it]Running Inference:  86%|████████▌ | 30/35 [27:56<04:25, 53.19s/it]Running Inference:  89%|████████▊ | 31/35 [29:19<04:09, 62.35s/it]Running Inference:  91%|█████████▏| 32/35 [30:04<02:51, 57.04s/it]Running Inference:  94%|█████████▍| 33/35 [31:47<02:21, 70.97s/it]Running Inference:  97%|█████████▋| 34/35 [32:22<01:00, 60.05s/it]Running Inference: 100%|██████████| 35/35 [33:20<00:00, 59.34s/it]Running Inference: 100%|██████████| 35/35 [33:20<00:00, 57.15s/it]
2025-12-14 08:17:59,397 - INFO - Inference completed.
2025-12-14 08:17:59,411 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 08:17:59,411 - INFO - Calculating metrics for dataset: longbench
2025-12-14 08:18:00,678 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 08:18:00,678 - INFO - Metrics:
19.12
2025-12-14 08:18:00,679 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qmsum, ratio=0.2) on GPU 1

----------------------------------------
Task: qmsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 08:18:07,151 - INFO - Set deterministic seeds to 42
2025-12-14 08:18:07,151 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 08:18:07,151 - INFO - Starting evaluation run...
2025-12-14 08:18:07,151 - INFO - Output directory set to: longbenchresult
2025-12-14 08:18:07,152 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 08:18:07,152 - INFO - KV Press 'tova' setup.
2025-12-14 08:18:07,152 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 08:18:07,152 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.54it/s]
Device set to use cuda:0
2025-12-14 08:18:19,215 - INFO - Model pipeline loaded.
2025-12-14 08:18:19,215 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 08:18:22,834 - INFO - Dataset loaded with 200 entries.
2025-12-14 08:18:22,834 - INFO - Dataset processed with 200 entries.
2025-12-14 08:18:22,848 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:01<34:49, 61.44s/it]Running Inference:   6%|▌         | 2/35 [01:52<30:24, 55.30s/it]Running Inference:   9%|▊         | 3/35 [03:09<34:51, 65.36s/it]Running Inference:  11%|█▏        | 4/35 [04:13<33:24, 64.65s/it]Running Inference:  14%|█▍        | 5/35 [05:07<30:30, 61.03s/it]Running Inference:  17%|█▋        | 6/35 [05:40<24:46, 51.25s/it]Running Inference:  20%|██        | 7/35 [05:55<18:28, 39.60s/it]Running Inference:  23%|██▎       | 8/35 [07:11<22:58, 51.05s/it]Running Inference:  26%|██▌       | 9/35 [09:52<37:02, 85.49s/it]Running Inference:  29%|██▊       | 10/35 [11:08<34:20, 82.43s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [12:26<32:28, 81.19s/it]Running Inference:  34%|███▍      | 12/35 [12:50<24:29, 63.87s/it]Running Inference:  37%|███▋      | 13/35 [13:57<23:42, 64.67s/it]Running Inference:  40%|████      | 14/35 [15:14<23:55, 68.34s/it]Running Inference:  43%|████▎     | 15/35 [15:28<17:19, 51.97s/it]Running Inference:  46%|████▌     | 16/35 [16:03<14:50, 46.84s/it]Running Inference:  49%|████▊     | 17/35 [16:40<13:13, 44.06s/it]Running Inference:  51%|█████▏    | 18/35 [17:06<10:54, 38.50s/it]Running Inference:  54%|█████▍    | 19/35 [17:43<10:12, 38.26s/it]Running Inference:  57%|█████▋    | 20/35 [18:19<09:22, 37.51s/it]Running Inference:  60%|██████    | 21/35 [18:33<07:06, 30.45s/it]Running Inference:  63%|██████▎   | 22/35 [18:44<05:17, 24.45s/it]Running Inference:  66%|██████▌   | 23/35 [20:24<09:27, 47.28s/it]Running Inference:  69%|██████▊   | 24/35 [22:15<12:10, 66.45s/it]Running Inference:  71%|███████▏  | 25/35 [22:55<09:44, 58.46s/it]Running Inference:  74%|███████▍  | 26/35 [23:54<08:47, 58.66s/it]Running Inference:  77%|███████▋  | 27/35 [24:32<06:58, 52.36s/it]Running Inference:  80%|████████  | 28/35 [25:53<07:07, 61.07s/it]Running Inference:  83%|████████▎ | 29/35 [27:05<06:24, 64.09s/it]Running Inference:  86%|████████▌ | 30/35 [27:27<04:18, 51.70s/it]Running Inference:  89%|████████▊ | 31/35 [28:32<03:42, 55.69s/it]Running Inference:  91%|█████████▏| 32/35 [29:55<03:11, 63.92s/it]Running Inference:  94%|█████████▍| 33/35 [31:41<02:33, 76.50s/it]Running Inference:  97%|█████████▋| 34/35 [32:35<01:09, 69.76s/it]Running Inference: 100%|██████████| 35/35 [33:35<00:00, 66.59s/it]Running Inference: 100%|██████████| 35/35 [33:35<00:00, 57.57s/it]
2025-12-14 08:51:57,878 - INFO - Inference completed.
2025-12-14 08:51:57,891 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 08:51:57,891 - INFO - Calculating metrics for dataset: longbench
2025-12-14 08:51:59,119 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 08:51:59,119 - INFO - Metrics:
18.94
2025-12-14 08:51:59,121 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qmsum, ratio=0.3) on GPU 1

----------------------------------------
Task: qmsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 08:52:05,581 - INFO - Set deterministic seeds to 42
2025-12-14 08:52:05,582 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 08:52:05,582 - INFO - Starting evaluation run...
2025-12-14 08:52:05,582 - INFO - Output directory set to: longbenchresult
2025-12-14 08:52:05,582 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 08:52:05,582 - INFO - KV Press 'tova' setup.
2025-12-14 08:52:05,582 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 08:52:05,582 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.19it/s]
Device set to use cuda:0
2025-12-14 08:52:16,351 - INFO - Model pipeline loaded.
2025-12-14 08:52:16,351 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 08:52:19,524 - INFO - Dataset loaded with 200 entries.
2025-12-14 08:52:19,524 - INFO - Dataset processed with 200 entries.
2025-12-14 08:52:19,539 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:14<42:08, 74.37s/it]Running Inference:   6%|▌         | 2/35 [01:45<26:51, 48.83s/it]Running Inference:   9%|▊         | 3/35 [03:01<32:40, 61.27s/it]Running Inference:  11%|█▏        | 4/35 [03:44<28:00, 54.22s/it]Running Inference:  14%|█▍        | 5/35 [04:40<27:23, 54.80s/it]Running Inference:  17%|█▋        | 6/35 [05:32<26:04, 53.96s/it]Running Inference:  20%|██        | 7/35 [05:47<19:12, 41.16s/it]Running Inference:  23%|██▎       | 8/35 [07:22<26:10, 58.16s/it]Running Inference:  26%|██▌       | 9/35 [09:01<30:45, 70.96s/it]Running Inference:  29%|██▊       | 10/35 [09:58<27:42, 66.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [11:50<32:13, 80.55s/it]Running Inference:  34%|███▍      | 12/35 [12:50<28:28, 74.28s/it]Running Inference:  37%|███▋      | 13/35 [14:32<30:22, 82.82s/it]Running Inference:  40%|████      | 14/35 [15:34<26:44, 76.38s/it]Running Inference:  43%|████▎     | 15/35 [15:46<19:00, 57.04s/it]Running Inference:  46%|████▌     | 16/35 [16:22<16:03, 50.70s/it]Running Inference:  49%|████▊     | 17/35 [17:18<15:39, 52.17s/it]Running Inference:  51%|█████▏    | 18/35 [18:06<14:25, 50.90s/it]Running Inference:  54%|█████▍    | 19/35 [18:41<12:21, 46.37s/it]Running Inference:  57%|█████▋    | 20/35 [19:18<10:50, 43.38s/it]Running Inference:  60%|██████    | 21/35 [19:32<08:03, 34.54s/it]Running Inference:  63%|██████▎   | 22/35 [20:03<07:15, 33.47s/it]Running Inference:  66%|██████▌   | 23/35 [21:10<08:43, 43.63s/it]Running Inference:  69%|██████▊   | 24/35 [22:58<11:32, 62.96s/it]Running Inference:  71%|███████▏  | 25/35 [23:18<08:20, 50.00s/it]Running Inference:  74%|███████▍  | 26/35 [23:38<06:09, 41.08s/it]Running Inference:  77%|███████▋  | 27/35 [24:35<06:07, 45.92s/it]Running Inference:  80%|████████  | 28/35 [26:15<07:13, 61.95s/it]Running Inference:  83%|████████▎ | 29/35 [27:00<05:42, 57.09s/it]Running Inference:  86%|████████▌ | 30/35 [27:23<03:54, 46.85s/it]Running Inference:  89%|████████▊ | 31/35 [28:48<03:53, 58.30s/it]Running Inference:  91%|█████████▏| 32/35 [29:51<02:58, 59.66s/it]Running Inference:  94%|█████████▍| 33/35 [31:57<02:39, 79.52s/it]Running Inference:  97%|█████████▋| 34/35 [32:15<01:01, 61.15s/it]Running Inference: 100%|██████████| 35/35 [33:10<00:00, 59.18s/it]Running Inference: 100%|██████████| 35/35 [33:10<00:00, 56.87s/it]
2025-12-14 09:25:29,988 - INFO - Inference completed.
2025-12-14 09:25:30,001 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 09:25:30,001 - INFO - Calculating metrics for dataset: longbench
2025-12-14 09:25:31,249 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 09:25:31,249 - INFO - Metrics:
18.13
2025-12-14 09:25:31,251 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=qmsum, ratio=0.5) on GPU 1


========================================
LongBench Task: samsum
========================================
----------------------------------------
Task: samsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 09:25:37,771 - INFO - Set deterministic seeds to 42
2025-12-14 09:25:37,771 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 09:25:37,771 - INFO - Starting evaluation run...
2025-12-14 09:25:37,771 - INFO - Output directory set to: longbenchresult
2025-12-14 09:25:37,771 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 09:25:37,771 - INFO - KV Press 'tova' setup.
2025-12-14 09:25:37,771 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 09:25:37,771 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.75it/s]
Device set to use cuda:0
2025-12-14 09:25:48,872 - INFO - Model pipeline loaded.
2025-12-14 09:25:48,873 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 2291.10 examples/s]
2025-12-14 09:25:53,702 - INFO - Dataset loaded with 200 entries.
2025-12-14 09:25:53,702 - INFO - Dataset processed with 200 entries.
2025-12-14 09:25:53,727 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<10:03,  3.03s/it]Running Inference:   1%|          | 2/200 [00:10<19:12,  5.82s/it]Running Inference:   2%|▏         | 3/200 [00:12<13:31,  4.12s/it]Running Inference:   2%|▏         | 4/200 [00:14<10:31,  3.22s/it]Running Inference:   2%|▎         | 5/200 [00:16<08:11,  2.52s/it]Running Inference:   3%|▎         | 6/200 [00:19<09:01,  2.79s/it]Running Inference:   4%|▎         | 7/200 [00:21<08:11,  2.55s/it]Running Inference:   4%|▍         | 8/200 [00:24<08:27,  2.64s/it]Running Inference:   4%|▍         | 9/200 [00:31<12:39,  3.97s/it]Running Inference:   5%|▌         | 10/200 [00:34<12:23,  3.91s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:37<11:03,  3.51s/it]Running Inference:   6%|▌         | 12/200 [00:40<10:13,  3.26s/it]Running Inference:   6%|▋         | 13/200 [00:42<09:15,  2.97s/it]Running Inference:   7%|▋         | 14/200 [00:46<10:30,  3.39s/it]Running Inference:   8%|▊         | 15/200 [00:49<09:27,  3.07s/it]Running Inference:   8%|▊         | 16/200 [00:56<12:58,  4.23s/it]Running Inference:   8%|▊         | 17/200 [00:59<12:09,  3.99s/it]Running Inference:   9%|▉         | 18/200 [01:00<09:36,  3.16s/it]Running Inference:  10%|▉         | 19/200 [01:08<13:50,  4.59s/it]Running Inference:  10%|█         | 20/200 [01:11<11:44,  3.91s/it]Running Inference:  10%|█         | 21/200 [01:13<10:26,  3.50s/it]Running Inference:  11%|█         | 22/200 [01:18<12:00,  4.05s/it]Running Inference:  12%|█▏        | 23/200 [01:22<11:32,  3.91s/it]Running Inference:  12%|█▏        | 24/200 [01:24<09:50,  3.36s/it]Running Inference:  12%|█▎        | 25/200 [01:27<09:12,  3.15s/it]Running Inference:  13%|█▎        | 26/200 [01:32<10:36,  3.66s/it]Running Inference:  14%|█▎        | 27/200 [01:40<14:40,  5.09s/it]Running Inference:  14%|█▍        | 28/200 [01:43<12:58,  4.53s/it]Running Inference:  14%|█▍        | 29/200 [01:44<10:08,  3.56s/it]Running Inference:  15%|█▌        | 30/200 [01:47<08:57,  3.16s/it]Running Inference:  16%|█▌        | 31/200 [01:49<07:58,  2.83s/it]Running Inference:  16%|█▌        | 32/200 [01:52<08:08,  2.91s/it]Running Inference:  16%|█▋        | 33/200 [01:58<11:07,  4.00s/it]Running Inference:  17%|█▋        | 34/200 [02:02<10:32,  3.81s/it]Running Inference:  18%|█▊        | 35/200 [02:05<10:03,  3.66s/it]Running Inference:  18%|█▊        | 36/200 [02:13<13:27,  4.92s/it]Running Inference:  18%|█▊        | 37/200 [02:15<11:15,  4.14s/it]Running Inference:  19%|█▉        | 38/200 [02:18<10:02,  3.72s/it]Running Inference:  20%|█▉        | 39/200 [02:26<13:46,  5.13s/it]Running Inference:  20%|██        | 40/200 [02:35<16:03,  6.02s/it]Running Inference:  20%|██        | 41/200 [02:36<12:38,  4.77s/it]Running Inference:  21%|██        | 42/200 [02:43<14:09,  5.37s/it]Running Inference:  22%|██▏       | 43/200 [02:45<11:31,  4.40s/it]Running Inference:  22%|██▏       | 44/200 [02:53<14:02,  5.40s/it]Running Inference:  22%|██▎       | 45/200 [02:56<11:51,  4.59s/it]Running Inference:  23%|██▎       | 46/200 [03:04<14:13,  5.54s/it]Running Inference:  24%|██▎       | 47/200 [03:06<11:50,  4.65s/it]Running Inference:  24%|██▍       | 48/200 [03:09<10:40,  4.21s/it]Running Inference:  24%|██▍       | 49/200 [03:17<13:17,  5.28s/it]Running Inference:  25%|██▌       | 50/200 [03:19<10:38,  4.26s/it]Running Inference:  26%|██▌       | 51/200 [03:25<12:17,  4.95s/it]Running Inference:  26%|██▌       | 52/200 [03:29<10:49,  4.39s/it]Running Inference:  26%|██▋       | 53/200 [03:31<09:09,  3.74s/it]Running Inference:  27%|██▋       | 54/200 [03:37<11:06,  4.57s/it]Running Inference:  28%|██▊       | 55/200 [03:41<10:10,  4.21s/it]Running Inference:  28%|██▊       | 56/200 [03:48<12:24,  5.17s/it]Running Inference:  28%|██▊       | 57/200 [03:51<10:39,  4.47s/it]Running Inference:  29%|██▉       | 58/200 [03:59<13:16,  5.61s/it]Running Inference:  30%|██▉       | 59/200 [04:06<13:58,  5.94s/it]Running Inference:  30%|███       | 60/200 [04:13<14:28,  6.20s/it]Running Inference:  30%|███       | 61/200 [04:14<10:47,  4.66s/it]Running Inference:  31%|███       | 62/200 [04:21<12:19,  5.36s/it]Running Inference:  32%|███▏      | 63/200 [04:23<10:14,  4.49s/it]Running Inference:  32%|███▏      | 64/200 [04:26<09:14,  4.08s/it]Running Inference:  32%|███▎      | 65/200 [04:30<08:58,  3.99s/it]Running Inference:  33%|███▎      | 66/200 [04:33<07:54,  3.54s/it]Running Inference:  34%|███▎      | 67/200 [04:35<07:06,  3.21s/it]Running Inference:  34%|███▍      | 68/200 [04:42<09:22,  4.26s/it]Running Inference:  34%|███▍      | 69/200 [04:44<07:49,  3.58s/it]Running Inference:  35%|███▌      | 70/200 [04:51<10:21,  4.78s/it]Running Inference:  36%|███▌      | 71/200 [04:53<08:13,  3.83s/it]Running Inference:  36%|███▌      | 72/200 [04:55<07:05,  3.33s/it]Running Inference:  36%|███▋      | 73/200 [04:57<06:24,  3.03s/it]Running Inference:  37%|███▋      | 74/200 [04:59<05:35,  2.66s/it]Running Inference:  38%|███▊      | 75/200 [05:00<04:39,  2.23s/it]Running Inference:  38%|███▊      | 76/200 [05:02<04:14,  2.05s/it]Running Inference:  38%|███▊      | 77/200 [05:05<04:44,  2.31s/it]Running Inference:  39%|███▉      | 78/200 [05:08<05:03,  2.49s/it]Running Inference:  40%|███▉      | 79/200 [05:10<04:53,  2.43s/it]Running Inference:  40%|████      | 80/200 [05:11<04:10,  2.09s/it]Running Inference:  40%|████      | 81/200 [05:13<03:36,  1.82s/it]Running Inference:  41%|████      | 82/200 [05:14<03:26,  1.75s/it]Running Inference:  42%|████▏     | 83/200 [05:19<05:15,  2.69s/it]Running Inference:  42%|████▏     | 84/200 [05:23<05:59,  3.10s/it]Running Inference:  42%|████▎     | 85/200 [05:27<06:16,  3.27s/it]Running Inference:  43%|████▎     | 86/200 [05:28<04:55,  2.59s/it]Running Inference:  44%|████▎     | 87/200 [05:31<04:59,  2.65s/it]Running Inference:  44%|████▍     | 88/200 [05:33<04:32,  2.43s/it]Running Inference:  44%|████▍     | 89/200 [05:40<07:09,  3.87s/it]Running Inference:  45%|████▌     | 90/200 [05:42<06:23,  3.48s/it]Running Inference:  46%|████▌     | 91/200 [05:47<06:58,  3.84s/it]Running Inference:  46%|████▌     | 92/200 [05:54<08:50,  4.91s/it]Running Inference:  46%|████▋     | 93/200 [05:58<07:55,  4.44s/it]Running Inference:  47%|████▋     | 94/200 [05:59<06:12,  3.51s/it]Running Inference:  48%|████▊     | 95/200 [06:07<08:13,  4.70s/it]Running Inference:  48%|████▊     | 96/200 [06:10<07:18,  4.22s/it]Running Inference:  48%|████▊     | 97/200 [06:11<05:41,  3.31s/it]Running Inference:  49%|████▉     | 98/200 [06:19<08:10,  4.81s/it]Running Inference:  50%|████▉     | 99/200 [06:26<09:08,  5.43s/it]Running Inference:  50%|█████     | 100/200 [06:28<07:14,  4.34s/it]Running Inference:  50%|█████     | 101/200 [06:30<06:14,  3.79s/it]Running Inference:  51%|█████     | 102/200 [06:33<05:36,  3.43s/it]Running Inference:  52%|█████▏    | 103/200 [06:38<06:17,  3.89s/it]Running Inference:  52%|█████▏    | 104/200 [06:40<05:09,  3.23s/it]Running Inference:  52%|█████▎    | 105/200 [06:42<04:33,  2.88s/it]Running Inference:  53%|█████▎    | 106/200 [06:50<06:52,  4.39s/it]Running Inference:  54%|█████▎    | 107/200 [06:51<05:32,  3.58s/it]Running Inference:  54%|█████▍    | 108/200 [06:55<05:21,  3.50s/it]Running Inference:  55%|█████▍    | 109/200 [06:56<04:20,  2.86s/it]Running Inference:  55%|█████▌    | 110/200 [06:59<04:17,  2.86s/it]Running Inference:  56%|█████▌    | 111/200 [07:01<04:01,  2.72s/it]Running Inference:  56%|█████▌    | 112/200 [07:08<05:47,  3.95s/it]Running Inference:  56%|█████▋    | 113/200 [07:10<04:44,  3.27s/it]Running Inference:  57%|█████▋    | 114/200 [07:17<06:10,  4.31s/it]Running Inference:  57%|█████▊    | 115/200 [07:21<06:04,  4.29s/it]Running Inference:  58%|█████▊    | 116/200 [07:24<05:27,  3.90s/it]Running Inference:  58%|█████▊    | 117/200 [07:25<04:25,  3.19s/it]Running Inference:  59%|█████▉    | 118/200 [07:29<04:24,  3.23s/it]Running Inference:  60%|█████▉    | 119/200 [07:31<03:58,  2.94s/it]Running Inference:  60%|██████    | 120/200 [07:39<05:54,  4.43s/it]Running Inference:  60%|██████    | 121/200 [07:41<05:08,  3.91s/it]Running Inference:  61%|██████    | 122/200 [07:45<04:53,  3.76s/it]Running Inference:  62%|██████▏   | 123/200 [07:52<06:02,  4.70s/it]Running Inference:  62%|██████▏   | 124/200 [07:53<04:27,  3.52s/it]Running Inference:  62%|██████▎   | 125/200 [08:01<06:07,  4.90s/it]Running Inference:  63%|██████▎   | 126/200 [08:04<05:22,  4.35s/it]Running Inference:  64%|██████▎   | 127/200 [08:06<04:39,  3.83s/it]Running Inference:  64%|██████▍   | 128/200 [08:10<04:27,  3.71s/it]Running Inference:  64%|██████▍   | 129/200 [08:12<03:58,  3.36s/it]Running Inference:  65%|██████▌   | 130/200 [08:20<05:26,  4.66s/it]Running Inference:  66%|██████▌   | 131/200 [08:24<05:01,  4.37s/it]Running Inference:  66%|██████▌   | 132/200 [08:26<04:11,  3.71s/it]Running Inference:  66%|██████▋   | 133/200 [08:29<03:55,  3.52s/it]Running Inference:  67%|██████▋   | 134/200 [08:32<03:34,  3.25s/it]Running Inference:  68%|██████▊   | 135/200 [08:39<04:43,  4.37s/it]Running Inference:  68%|██████▊   | 136/200 [08:46<05:34,  5.22s/it]Running Inference:  68%|██████▊   | 137/200 [08:48<04:29,  4.28s/it]Running Inference:  69%|██████▉   | 138/200 [08:51<04:07,  4.00s/it]Running Inference:  70%|██████▉   | 139/200 [08:58<04:53,  4.81s/it]Running Inference:  70%|███████   | 140/200 [09:02<04:34,  4.57s/it]Running Inference:  70%|███████   | 141/200 [09:09<05:17,  5.38s/it]Running Inference:  71%|███████   | 142/200 [09:11<04:04,  4.21s/it]Running Inference:  72%|███████▏  | 143/200 [09:18<04:53,  5.14s/it]Running Inference:  72%|███████▏  | 144/200 [09:25<05:27,  5.84s/it]Running Inference:  72%|███████▎  | 145/200 [09:28<04:33,  4.97s/it]Running Inference:  73%|███████▎  | 146/200 [09:31<03:50,  4.26s/it]Running Inference:  74%|███████▎  | 147/200 [09:34<03:25,  3.88s/it]Running Inference:  74%|███████▍  | 148/200 [09:36<02:46,  3.20s/it]Running Inference:  74%|███████▍  | 149/200 [09:40<03:03,  3.60s/it]Running Inference:  75%|███████▌  | 150/200 [09:47<03:54,  4.68s/it]Running Inference:  76%|███████▌  | 151/200 [09:51<03:30,  4.30s/it]Running Inference:  76%|███████▌  | 152/200 [09:52<02:37,  3.28s/it]Running Inference:  76%|███████▋  | 153/200 [09:55<02:28,  3.17s/it]Running Inference:  77%|███████▋  | 154/200 [09:56<02:02,  2.67s/it]Running Inference:  78%|███████▊  | 155/200 [09:58<01:45,  2.35s/it]Running Inference:  78%|███████▊  | 156/200 [10:05<02:55,  3.99s/it]Running Inference:  78%|███████▊  | 157/200 [10:07<02:18,  3.22s/it]Running Inference:  79%|███████▉  | 158/200 [10:09<02:01,  2.89s/it]Running Inference:  80%|███████▉  | 159/200 [10:14<02:18,  3.38s/it]Running Inference:  80%|████████  | 160/200 [10:20<02:56,  4.41s/it]Running Inference:  80%|████████  | 161/200 [10:27<03:20,  5.13s/it]Running Inference:  81%|████████  | 162/200 [10:29<02:41,  4.26s/it]Running Inference:  82%|████████▏ | 163/200 [10:38<03:23,  5.50s/it]Running Inference:  82%|████████▏ | 164/200 [10:41<02:49,  4.72s/it]Running Inference:  82%|████████▎ | 165/200 [10:47<03:06,  5.32s/it]Running Inference:  83%|████████▎ | 166/200 [10:54<03:17,  5.80s/it]Running Inference:  84%|████████▎ | 167/200 [10:56<02:31,  4.59s/it]Running Inference:  84%|████████▍ | 168/200 [11:04<02:57,  5.56s/it]Running Inference:  84%|████████▍ | 169/200 [11:06<02:22,  4.58s/it]Running Inference:  85%|████████▌ | 170/200 [11:08<01:51,  3.71s/it]Running Inference:  86%|████████▌ | 171/200 [11:11<01:41,  3.52s/it]Running Inference:  86%|████████▌ | 172/200 [11:13<01:26,  3.10s/it]Running Inference:  86%|████████▋ | 173/200 [11:16<01:18,  2.91s/it]Running Inference:  87%|████████▋ | 174/200 [11:23<01:48,  4.16s/it]Running Inference:  88%|████████▊ | 175/200 [11:25<01:32,  3.71s/it]Running Inference:  88%|████████▊ | 176/200 [11:27<01:12,  3.01s/it]Running Inference:  88%|████████▊ | 177/200 [11:33<01:34,  4.10s/it]Running Inference:  89%|████████▉ | 178/200 [11:38<01:32,  4.19s/it]Running Inference:  90%|████████▉ | 179/200 [11:39<01:12,  3.46s/it]Running Inference:  90%|█████████ | 180/200 [11:41<00:58,  2.92s/it]Running Inference:  90%|█████████ | 181/200 [11:44<00:52,  2.76s/it]Running Inference:  91%|█████████ | 182/200 [11:45<00:43,  2.40s/it]Running Inference:  92%|█████████▏| 183/200 [11:53<01:09,  4.07s/it]Running Inference:  92%|█████████▏| 184/200 [11:58<01:09,  4.36s/it]Running Inference:  92%|█████████▎| 185/200 [12:01<00:57,  3.83s/it]Running Inference:  93%|█████████▎| 186/200 [12:02<00:45,  3.22s/it]Running Inference:  94%|█████████▎| 187/200 [12:04<00:34,  2.66s/it]Running Inference:  94%|█████████▍| 188/200 [12:06<00:30,  2.54s/it]Running Inference:  94%|█████████▍| 189/200 [12:09<00:29,  2.71s/it]Running Inference:  95%|█████████▌| 190/200 [12:12<00:28,  2.83s/it]Running Inference:  96%|█████████▌| 191/200 [12:15<00:25,  2.80s/it]Running Inference:  96%|█████████▌| 192/200 [12:18<00:22,  2.84s/it]Running Inference:  96%|█████████▋| 193/200 [12:26<00:31,  4.43s/it]Running Inference:  97%|█████████▋| 194/200 [12:28<00:22,  3.69s/it]Running Inference:  98%|█████████▊| 195/200 [12:30<00:15,  3.12s/it]Running Inference:  98%|█████████▊| 196/200 [12:38<00:18,  4.59s/it]Running Inference:  98%|█████████▊| 197/200 [12:39<00:10,  3.63s/it]Running Inference:  99%|█████████▉| 198/200 [12:47<00:09,  4.85s/it]Running Inference: 100%|█████████▉| 199/200 [12:54<00:05,  5.40s/it]Running Inference: 100%|██████████| 200/200 [13:01<00:00,  5.90s/it]Running Inference: 100%|██████████| 200/200 [13:01<00:00,  3.91s/it]
2025-12-14 09:38:54,958 - INFO - Inference completed.
2025-12-14 09:38:54,969 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 09:38:54,969 - INFO - Calculating metrics for dataset: longbench
2025-12-14 09:38:55,080 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 09:38:55,080 - INFO - Metrics:
27.15
2025-12-14 09:38:55,081 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=samsum, ratio=0.1) on GPU 1

----------------------------------------
Task: samsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 09:39:01,599 - INFO - Set deterministic seeds to 42
2025-12-14 09:39:01,599 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 09:39:01,599 - INFO - Starting evaluation run...
2025-12-14 09:39:01,599 - INFO - Output directory set to: longbenchresult
2025-12-14 09:39:01,599 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 09:39:01,599 - INFO - KV Press 'tova' setup.
2025-12-14 09:39:01,599 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 09:39:01,599 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.92it/s]
Device set to use cuda:0
2025-12-14 09:39:13,269 - INFO - Model pipeline loaded.
2025-12-14 09:39:13,269 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 09:39:16,309 - INFO - Dataset loaded with 200 entries.
2025-12-14 09:39:16,309 - INFO - Dataset processed with 200 entries.
2025-12-14 09:39:16,336 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:24,  3.44s/it]Running Inference:   1%|          | 2/200 [00:11<19:23,  5.88s/it]Running Inference:   2%|▏         | 3/200 [00:13<13:33,  4.13s/it]Running Inference:   2%|▏         | 4/200 [00:14<10:30,  3.22s/it]Running Inference:   2%|▎         | 5/200 [00:16<08:08,  2.50s/it]Running Inference:   3%|▎         | 6/200 [00:19<08:57,  2.77s/it]Running Inference:   4%|▎         | 7/200 [00:21<08:06,  2.52s/it]Running Inference:   4%|▍         | 8/200 [00:24<08:24,  2.63s/it]Running Inference:   4%|▍         | 9/200 [00:31<12:27,  3.91s/it]Running Inference:   5%|▌         | 10/200 [00:34<12:11,  3.85s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:37<10:59,  3.49s/it]Running Inference:   6%|▌         | 12/200 [00:40<10:08,  3.24s/it]Running Inference:   6%|▋         | 13/200 [00:42<09:08,  2.93s/it]Running Inference:   7%|▋         | 14/200 [00:46<10:21,  3.34s/it]Running Inference:   8%|▊         | 15/200 [00:48<09:19,  3.02s/it]Running Inference:   8%|▊         | 16/200 [00:55<12:44,  4.16s/it]Running Inference:   8%|▊         | 17/200 [00:59<11:57,  3.92s/it]Running Inference:   9%|▉         | 18/200 [01:00<09:27,  3.12s/it]Running Inference:  10%|▉         | 19/200 [01:08<13:39,  4.53s/it]Running Inference:  10%|█         | 20/200 [01:14<15:39,  5.22s/it]Running Inference:  10%|█         | 21/200 [01:17<13:08,  4.41s/it]Running Inference:  11%|█         | 22/200 [01:22<13:35,  4.58s/it]Running Inference:  12%|█▏        | 23/200 [01:25<12:34,  4.26s/it]Running Inference:  12%|█▏        | 24/200 [01:27<10:25,  3.56s/it]Running Inference:  12%|█▎        | 25/200 [01:29<09:07,  3.13s/it]Running Inference:  13%|█▎        | 26/200 [01:37<12:34,  4.34s/it]Running Inference:  14%|█▎        | 27/200 [01:45<15:51,  5.50s/it]Running Inference:  14%|█▍        | 28/200 [01:48<13:44,  4.80s/it]Running Inference:  14%|█▍        | 29/200 [01:49<10:46,  3.78s/it]Running Inference:  15%|█▌        | 30/200 [01:52<09:23,  3.31s/it]Running Inference:  16%|█▌        | 31/200 [01:54<08:14,  2.92s/it]Running Inference:  16%|█▌        | 32/200 [01:57<08:21,  2.99s/it]Running Inference:  16%|█▋        | 33/200 [02:03<11:11,  4.02s/it]Running Inference:  17%|█▋        | 34/200 [02:07<10:31,  3.81s/it]Running Inference:  18%|█▊        | 35/200 [02:09<09:15,  3.37s/it]Running Inference:  18%|█▊        | 36/200 [02:17<12:45,  4.67s/it]Running Inference:  18%|█▊        | 37/200 [02:19<10:44,  3.95s/it]Running Inference:  19%|█▉        | 38/200 [02:22<09:39,  3.58s/it]Running Inference:  20%|█▉        | 39/200 [02:30<13:20,  4.97s/it]Running Inference:  20%|██        | 40/200 [02:38<15:35,  5.85s/it]Running Inference:  20%|██        | 41/200 [02:39<12:18,  4.64s/it]Running Inference:  21%|██        | 42/200 [02:46<13:50,  5.26s/it]Running Inference:  22%|██▏       | 43/200 [02:48<11:17,  4.31s/it]Running Inference:  22%|██▏       | 44/200 [02:56<13:44,  5.29s/it]Running Inference:  22%|██▎       | 45/200 [02:59<11:37,  4.50s/it]Running Inference:  23%|██▎       | 46/200 [03:06<13:54,  5.42s/it]Running Inference:  24%|██▎       | 47/200 [03:09<11:35,  4.55s/it]Running Inference:  24%|██▍       | 48/200 [03:12<10:37,  4.19s/it]Running Inference:  24%|██▍       | 49/200 [03:20<13:11,  5.24s/it]Running Inference:  25%|██▌       | 50/200 [03:22<10:34,  4.23s/it]Running Inference:  26%|██▌       | 51/200 [03:28<12:09,  4.90s/it]Running Inference:  26%|██▌       | 52/200 [03:31<10:42,  4.34s/it]Running Inference:  26%|██▋       | 53/200 [03:33<09:04,  3.70s/it]Running Inference:  27%|██▋       | 54/200 [03:40<10:58,  4.51s/it]Running Inference:  28%|██▊       | 55/200 [03:43<10:02,  4.16s/it]Running Inference:  28%|██▊       | 56/200 [03:46<09:31,  3.97s/it]Running Inference:  28%|██▊       | 57/200 [03:49<08:37,  3.62s/it]Running Inference:  29%|██▉       | 58/200 [03:57<11:44,  4.96s/it]Running Inference:  30%|██▉       | 59/200 [04:04<12:45,  5.43s/it]Running Inference:  30%|███       | 60/200 [04:07<10:55,  4.68s/it]Running Inference:  30%|███       | 61/200 [04:08<08:18,  3.59s/it]Running Inference:  31%|███       | 62/200 [04:13<09:24,  4.09s/it]Running Inference:  32%|███▏      | 63/200 [04:16<08:14,  3.61s/it]Running Inference:  32%|███▏      | 64/200 [04:19<07:44,  3.41s/it]Running Inference:  32%|███▎      | 65/200 [04:23<08:21,  3.71s/it]Running Inference:  33%|███▎      | 66/200 [04:26<07:39,  3.43s/it]Running Inference:  34%|███▎      | 67/200 [04:28<06:53,  3.11s/it]Running Inference:  34%|███▍      | 68/200 [04:35<09:06,  4.14s/it]Running Inference:  34%|███▍      | 69/200 [04:37<07:36,  3.48s/it]Running Inference:  35%|███▌      | 70/200 [04:44<10:05,  4.66s/it]Running Inference:  36%|███▌      | 71/200 [04:46<08:01,  3.73s/it]Running Inference:  36%|███▌      | 72/200 [04:48<06:58,  3.27s/it]Running Inference:  36%|███▋      | 73/200 [04:49<05:51,  2.77s/it]Running Inference:  37%|███▋      | 74/200 [04:51<05:11,  2.47s/it]Running Inference:  38%|███▊      | 75/200 [04:52<04:21,  2.10s/it]Running Inference:  38%|███▊      | 76/200 [04:54<04:01,  1.95s/it]Running Inference:  38%|███▊      | 77/200 [04:57<04:35,  2.24s/it]Running Inference:  39%|███▉      | 78/200 [05:00<04:56,  2.43s/it]Running Inference:  40%|███▉      | 79/200 [05:02<04:48,  2.38s/it]Running Inference:  40%|████      | 80/200 [05:03<03:55,  1.96s/it]Running Inference:  40%|████      | 81/200 [05:04<03:25,  1.73s/it]Running Inference:  41%|████      | 82/200 [05:06<03:11,  1.62s/it]Running Inference:  42%|████▏     | 83/200 [05:10<04:32,  2.33s/it]Running Inference:  42%|████▏     | 84/200 [05:14<05:27,  2.82s/it]Running Inference:  42%|████▎     | 85/200 [05:17<05:35,  2.91s/it]Running Inference:  43%|████▎     | 86/200 [05:18<04:26,  2.34s/it]Running Inference:  44%|████▎     | 87/200 [05:20<04:38,  2.47s/it]Running Inference:  44%|████▍     | 88/200 [05:27<07:03,  3.78s/it]Running Inference:  44%|████▍     | 89/200 [05:34<08:51,  4.78s/it]Running Inference:  45%|████▌     | 90/200 [05:37<07:32,  4.12s/it]Running Inference:  46%|████▌     | 91/200 [05:42<07:44,  4.26s/it]Running Inference:  46%|████▌     | 92/200 [05:49<09:19,  5.18s/it]Running Inference:  46%|████▋     | 93/200 [05:52<08:14,  4.62s/it]Running Inference:  47%|████▋     | 94/200 [05:54<06:24,  3.63s/it]Running Inference:  48%|████▊     | 95/200 [05:55<05:08,  2.94s/it]Running Inference:  48%|████▊     | 96/200 [05:58<05:09,  2.97s/it]Running Inference:  48%|████▊     | 97/200 [05:59<04:15,  2.48s/it]Running Inference:  49%|████▉     | 98/200 [06:07<07:05,  4.17s/it]Running Inference:  50%|████▉     | 99/200 [06:14<08:17,  4.93s/it]Running Inference:  50%|█████     | 100/200 [06:16<06:38,  3.98s/it]Running Inference:  50%|█████     | 101/200 [06:18<05:37,  3.41s/it]Running Inference:  51%|█████     | 102/200 [06:21<05:22,  3.29s/it]Running Inference:  52%|█████▏    | 103/200 [06:28<07:02,  4.35s/it]Running Inference:  52%|█████▏    | 104/200 [06:30<06:03,  3.78s/it]Running Inference:  52%|█████▎    | 105/200 [06:32<05:04,  3.20s/it]Running Inference:  53%|█████▎    | 106/200 [06:40<07:10,  4.58s/it]Running Inference:  54%|█████▎    | 107/200 [06:42<05:44,  3.71s/it]Running Inference:  54%|█████▍    | 108/200 [06:45<05:29,  3.58s/it]Running Inference:  55%|█████▍    | 109/200 [06:46<04:25,  2.92s/it]Running Inference:  55%|█████▌    | 110/200 [06:49<04:28,  2.98s/it]Running Inference:  56%|█████▌    | 111/200 [06:52<04:09,  2.80s/it]Running Inference:  56%|█████▌    | 112/200 [06:58<05:50,  3.98s/it]Running Inference:  56%|█████▋    | 113/200 [07:00<04:45,  3.29s/it]Running Inference:  57%|█████▋    | 114/200 [07:04<04:48,  3.36s/it]Running Inference:  57%|█████▊    | 115/200 [07:08<05:01,  3.54s/it]Running Inference:  58%|█████▊    | 116/200 [07:11<04:42,  3.37s/it]Running Inference:  58%|█████▊    | 117/200 [07:12<03:53,  2.81s/it]Running Inference:  59%|█████▉    | 118/200 [07:15<03:58,  2.91s/it]Running Inference:  60%|█████▉    | 119/200 [07:17<03:39,  2.71s/it]Running Inference:  60%|██████    | 120/200 [07:25<05:37,  4.22s/it]Running Inference:  60%|██████    | 121/200 [07:28<04:56,  3.75s/it]Running Inference:  61%|██████    | 122/200 [07:31<04:43,  3.63s/it]Running Inference:  62%|██████▏   | 123/200 [07:38<05:51,  4.56s/it]Running Inference:  62%|██████▏   | 124/200 [07:39<04:19,  3.41s/it]Running Inference:  62%|██████▎   | 125/200 [07:47<05:59,  4.79s/it]Running Inference:  63%|██████▎   | 126/200 [07:50<05:15,  4.27s/it]Running Inference:  64%|██████▎   | 127/200 [07:52<04:34,  3.76s/it]Running Inference:  64%|██████▍   | 128/200 [07:56<04:19,  3.61s/it]Running Inference:  64%|██████▍   | 129/200 [07:58<03:53,  3.29s/it]Running Inference:  65%|██████▌   | 130/200 [08:05<05:11,  4.45s/it]Running Inference:  66%|██████▌   | 131/200 [08:13<06:22,  5.55s/it]Running Inference:  66%|██████▌   | 132/200 [08:15<05:07,  4.52s/it]Running Inference:  66%|██████▋   | 133/200 [08:19<04:33,  4.08s/it]Running Inference:  67%|██████▋   | 134/200 [08:21<03:59,  3.63s/it]Running Inference:  68%|██████▊   | 135/200 [08:28<04:57,  4.58s/it]Running Inference:  68%|██████▊   | 136/200 [08:35<05:40,  5.32s/it]Running Inference:  68%|██████▊   | 137/200 [08:37<04:33,  4.34s/it]Running Inference:  69%|██████▉   | 138/200 [08:41<04:16,  4.13s/it]Running Inference:  70%|██████▉   | 139/200 [08:47<04:55,  4.85s/it]Running Inference:  70%|███████   | 140/200 [08:51<04:33,  4.56s/it]Running Inference:  70%|███████   | 141/200 [08:58<05:14,  5.32s/it]Running Inference:  71%|███████   | 142/200 [09:00<04:01,  4.17s/it]Running Inference:  72%|███████▏  | 143/200 [09:07<04:48,  5.06s/it]Running Inference:  72%|███████▏  | 144/200 [09:14<05:21,  5.74s/it]Running Inference:  72%|███████▎  | 145/200 [09:17<04:20,  4.74s/it]Running Inference:  73%|███████▎  | 146/200 [09:19<03:41,  4.10s/it]Running Inference:  74%|███████▎  | 147/200 [09:22<03:19,  3.76s/it]Running Inference:  74%|███████▍  | 148/200 [09:24<02:43,  3.14s/it]Running Inference:  74%|███████▍  | 149/200 [09:28<02:59,  3.51s/it]Running Inference:  75%|███████▌  | 150/200 [09:35<03:49,  4.59s/it]Running Inference:  76%|███████▌  | 151/200 [09:39<03:26,  4.22s/it]Running Inference:  76%|███████▌  | 152/200 [09:39<02:34,  3.22s/it]Running Inference:  76%|███████▋  | 153/200 [09:42<02:26,  3.11s/it]Running Inference:  77%|███████▋  | 154/200 [09:44<02:00,  2.63s/it]Running Inference:  78%|███████▊  | 155/200 [09:45<01:44,  2.32s/it]Running Inference:  78%|███████▊  | 156/200 [09:53<02:52,  3.92s/it]Running Inference:  78%|███████▊  | 157/200 [09:55<02:16,  3.17s/it]Running Inference:  79%|███████▉  | 158/200 [09:57<01:59,  2.84s/it]Running Inference:  80%|███████▉  | 159/200 [10:00<02:08,  3.13s/it]Running Inference:  80%|████████  | 160/200 [10:07<02:47,  4.19s/it]Running Inference:  80%|████████  | 161/200 [10:14<03:12,  4.94s/it]Running Inference:  81%|████████  | 162/200 [10:16<02:36,  4.12s/it]Running Inference:  82%|████████▏ | 163/200 [10:24<03:17,  5.35s/it]Running Inference:  82%|████████▏ | 164/200 [10:27<02:45,  4.60s/it]Running Inference:  82%|████████▎ | 165/200 [10:34<03:02,  5.21s/it]Running Inference:  83%|████████▎ | 166/200 [10:40<03:12,  5.68s/it]Running Inference:  84%|████████▎ | 167/200 [10:42<02:28,  4.50s/it]Running Inference:  84%|████████▍ | 168/200 [10:50<02:54,  5.47s/it]Running Inference:  84%|████████▍ | 169/200 [10:52<02:19,  4.50s/it]Running Inference:  85%|████████▌ | 170/200 [10:54<01:49,  3.64s/it]Running Inference:  86%|████████▌ | 171/200 [10:57<01:40,  3.47s/it]Running Inference:  86%|████████▌ | 172/200 [11:04<02:04,  4.44s/it]Running Inference:  86%|████████▋ | 173/200 [11:10<02:20,  5.19s/it]Running Inference:  87%|████████▋ | 174/200 [11:17<02:28,  5.71s/it]Running Inference:  88%|████████▊ | 175/200 [11:20<02:01,  4.85s/it]Running Inference:  88%|████████▊ | 176/200 [11:22<01:31,  3.81s/it]Running Inference:  88%|████████▊ | 177/200 [11:23<01:08,  3.00s/it]Running Inference:  89%|████████▉ | 178/200 [11:27<01:14,  3.39s/it]Running Inference:  90%|████████▉ | 179/200 [11:29<01:00,  2.89s/it]Running Inference:  90%|█████████ | 180/200 [11:30<00:50,  2.51s/it]Running Inference:  90%|█████████ | 181/200 [11:33<00:46,  2.46s/it]Running Inference:  91%|█████████ | 182/200 [11:34<00:39,  2.18s/it]Running Inference:  92%|█████████▏| 183/200 [11:39<00:48,  2.86s/it]Running Inference:  92%|█████████▏| 184/200 [11:43<00:54,  3.41s/it]Running Inference:  92%|█████████▎| 185/200 [11:46<00:47,  3.15s/it]Running Inference:  93%|█████████▎| 186/200 [11:48<00:38,  2.75s/it]Running Inference:  94%|█████████▎| 187/200 [11:49<00:30,  2.32s/it]Running Inference:  94%|█████████▍| 188/200 [11:51<00:27,  2.30s/it]Running Inference:  94%|█████████▍| 189/200 [11:54<00:27,  2.54s/it]Running Inference:  95%|█████████▌| 190/200 [11:58<00:26,  2.70s/it]Running Inference:  96%|█████████▌| 191/200 [12:01<00:26,  2.95s/it]Running Inference:  96%|█████████▌| 192/200 [12:03<00:21,  2.73s/it]Running Inference:  96%|█████████▋| 193/200 [12:11<00:30,  4.30s/it]Running Inference:  97%|█████████▋| 194/200 [12:13<00:21,  3.59s/it]Running Inference:  98%|█████████▊| 195/200 [12:15<00:15,  3.04s/it]Running Inference:  98%|█████████▊| 196/200 [12:23<00:17,  4.48s/it]Running Inference:  98%|█████████▊| 197/200 [12:24<00:10,  3.56s/it]Running Inference:  99%|█████████▉| 198/200 [12:32<00:09,  4.74s/it]Running Inference: 100%|█████████▉| 199/200 [12:34<00:03,  3.94s/it]Running Inference: 100%|██████████| 200/200 [12:41<00:00,  4.84s/it]Running Inference: 100%|██████████| 200/200 [12:41<00:00,  3.81s/it]
2025-12-14 09:51:57,523 - INFO - Inference completed.
2025-12-14 09:51:57,534 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 09:51:57,534 - INFO - Calculating metrics for dataset: longbench
2025-12-14 09:51:57,648 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 09:51:57,648 - INFO - Metrics:
26.88
2025-12-14 09:51:57,650 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=samsum, ratio=0.2) on GPU 1

----------------------------------------
Task: samsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 09:52:04,086 - INFO - Set deterministic seeds to 42
2025-12-14 09:52:04,086 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 09:52:04,086 - INFO - Starting evaluation run...
2025-12-14 09:52:04,086 - INFO - Output directory set to: longbenchresult
2025-12-14 09:52:04,087 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 09:52:04,087 - INFO - KV Press 'tova' setup.
2025-12-14 09:52:04,087 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 09:52:04,087 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.68it/s]
Device set to use cuda:0
2025-12-14 09:52:15,792 - INFO - Model pipeline loaded.
2025-12-14 09:52:15,792 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 09:52:19,853 - INFO - Dataset loaded with 200 entries.
2025-12-14 09:52:19,853 - INFO - Dataset processed with 200 entries.
2025-12-14 09:52:19,880 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:28,  3.46s/it]Running Inference:   1%|          | 2/200 [00:07<12:06,  3.67s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:45,  2.97s/it]Running Inference:   2%|▏         | 4/200 [00:11<08:16,  2.53s/it]Running Inference:   2%|▎         | 5/200 [00:12<06:44,  2.08s/it]Running Inference:   3%|▎         | 6/200 [00:15<07:59,  2.47s/it]Running Inference:   4%|▎         | 7/200 [00:17<07:30,  2.34s/it]Running Inference:   4%|▍         | 8/200 [00:20<07:59,  2.50s/it]Running Inference:   4%|▍         | 9/200 [00:27<12:24,  3.90s/it]Running Inference:   5%|▌         | 10/200 [00:31<12:13,  3.86s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:34<11:00,  3.50s/it]Running Inference:   6%|▌         | 12/200 [00:36<10:11,  3.25s/it]Running Inference:   6%|▋         | 13/200 [00:39<09:14,  2.96s/it]Running Inference:   7%|▋         | 14/200 [00:43<10:30,  3.39s/it]Running Inference:   8%|▊         | 15/200 [00:46<09:40,  3.14s/it]Running Inference:   8%|▊         | 16/200 [00:53<13:11,  4.30s/it]Running Inference:   8%|▊         | 17/200 [01:01<16:42,  5.48s/it]Running Inference:   9%|▉         | 18/200 [01:02<12:46,  4.21s/it]Running Inference:  10%|▉         | 19/200 [01:10<16:10,  5.36s/it]Running Inference:  10%|█         | 20/200 [01:17<17:36,  5.87s/it]Running Inference:  10%|█         | 21/200 [01:20<14:31,  4.87s/it]Running Inference:  11%|█         | 22/200 [01:25<14:39,  4.94s/it]Running Inference:  12%|█▏        | 23/200 [01:28<13:24,  4.55s/it]Running Inference:  12%|█▏        | 24/200 [01:30<11:04,  3.77s/it]Running Inference:  12%|█▎        | 25/200 [01:33<09:42,  3.33s/it]Running Inference:  13%|█▎        | 26/200 [01:40<13:10,  4.55s/it]Running Inference:  14%|█▎        | 27/200 [01:46<14:23,  4.99s/it]Running Inference:  14%|█▍        | 28/200 [01:49<12:46,  4.46s/it]Running Inference:  14%|█▍        | 29/200 [01:51<10:07,  3.55s/it]Running Inference:  15%|█▌        | 30/200 [01:53<08:57,  3.16s/it]Running Inference:  16%|█▌        | 31/200 [01:55<07:57,  2.83s/it]Running Inference:  16%|█▌        | 32/200 [01:58<08:13,  2.94s/it]Running Inference:  16%|█▋        | 33/200 [02:05<11:17,  4.06s/it]Running Inference:  17%|█▋        | 34/200 [02:08<10:38,  3.85s/it]Running Inference:  18%|█▊        | 35/200 [02:11<09:44,  3.54s/it]Running Inference:  18%|█▊        | 36/200 [02:19<13:16,  4.86s/it]Running Inference:  18%|█▊        | 37/200 [02:21<11:08,  4.10s/it]Running Inference:  19%|█▉        | 38/200 [02:24<09:57,  3.69s/it]Running Inference:  20%|█▉        | 39/200 [02:33<13:44,  5.12s/it]Running Inference:  20%|██        | 40/200 [02:41<16:03,  6.02s/it]Running Inference:  20%|██        | 41/200 [02:43<12:39,  4.77s/it]Running Inference:  21%|██        | 42/200 [02:49<14:16,  5.42s/it]Running Inference:  22%|██▏       | 43/200 [02:52<11:40,  4.46s/it]Running Inference:  22%|██▏       | 44/200 [02:59<14:11,  5.46s/it]Running Inference:  22%|██▎       | 45/200 [03:02<11:52,  4.60s/it]Running Inference:  23%|██▎       | 46/200 [03:04<10:04,  3.93s/it]Running Inference:  24%|██▎       | 47/200 [03:07<08:57,  3.52s/it]Running Inference:  24%|██▍       | 48/200 [03:10<08:52,  3.50s/it]Running Inference:  24%|██▍       | 49/200 [03:18<12:08,  4.82s/it]Running Inference:  25%|██▌       | 50/200 [03:20<09:51,  3.94s/it]Running Inference:  26%|██▌       | 51/200 [03:27<11:48,  4.76s/it]Running Inference:  26%|██▌       | 52/200 [03:30<10:30,  4.26s/it]Running Inference:  26%|██▋       | 53/200 [03:32<08:57,  3.66s/it]Running Inference:  27%|██▋       | 54/200 [03:39<11:04,  4.55s/it]Running Inference:  28%|██▊       | 55/200 [03:42<09:55,  4.11s/it]Running Inference:  28%|██▊       | 56/200 [03:49<12:16,  5.11s/it]Running Inference:  28%|██▊       | 57/200 [03:52<10:33,  4.43s/it]Running Inference:  29%|██▉       | 58/200 [04:01<13:14,  5.60s/it]Running Inference:  30%|██▉       | 59/200 [04:07<13:58,  5.95s/it]Running Inference:  30%|███       | 60/200 [04:14<14:31,  6.22s/it]Running Inference:  30%|███       | 61/200 [04:15<10:49,  4.67s/it]Running Inference:  31%|███       | 62/200 [04:21<11:16,  4.90s/it]Running Inference:  32%|███▏      | 63/200 [04:23<09:34,  4.19s/it]Running Inference:  32%|███▏      | 64/200 [04:26<08:41,  3.84s/it]Running Inference:  32%|███▎      | 65/200 [04:31<09:05,  4.04s/it]Running Inference:  33%|███▎      | 66/200 [04:33<07:59,  3.58s/it]Running Inference:  34%|███▎      | 67/200 [04:34<06:02,  2.73s/it]Running Inference:  34%|███▍      | 68/200 [04:41<08:40,  3.95s/it]Running Inference:  34%|███▍      | 69/200 [04:43<07:20,  3.37s/it]Running Inference:  35%|███▌      | 70/200 [04:45<06:50,  3.16s/it]Running Inference:  36%|███▌      | 71/200 [04:47<05:47,  2.70s/it]Running Inference:  36%|███▌      | 72/200 [04:49<05:26,  2.55s/it]Running Inference:  36%|███▋      | 73/200 [04:51<05:05,  2.41s/it]Running Inference:  37%|███▋      | 74/200 [04:53<04:41,  2.24s/it]Running Inference:  38%|███▊      | 75/200 [04:54<04:02,  1.94s/it]Running Inference:  38%|███▊      | 76/200 [04:56<03:49,  1.85s/it]Running Inference:  38%|███▊      | 77/200 [04:59<04:27,  2.18s/it]Running Inference:  39%|███▉      | 78/200 [05:02<04:52,  2.40s/it]Running Inference:  40%|███▉      | 79/200 [05:04<04:46,  2.36s/it]Running Inference:  40%|████      | 80/200 [05:06<04:06,  2.05s/it]Running Inference:  40%|████      | 81/200 [05:07<03:34,  1.80s/it]Running Inference:  41%|████      | 82/200 [05:08<03:18,  1.69s/it]Running Inference:  42%|████▏     | 83/200 [05:17<07:12,  3.69s/it]Running Inference:  42%|████▏     | 84/200 [05:21<07:20,  3.80s/it]Running Inference:  42%|████▎     | 85/200 [05:25<07:20,  3.83s/it]Running Inference:  43%|████▎     | 86/200 [05:26<05:40,  2.98s/it]Running Inference:  44%|████▎     | 87/200 [05:29<05:43,  3.04s/it]Running Inference:  44%|████▍     | 88/200 [05:36<07:56,  4.25s/it]Running Inference:  44%|████▍     | 89/200 [05:43<09:35,  5.19s/it]Running Inference:  45%|████▌     | 90/200 [05:45<07:50,  4.28s/it]Running Inference:  46%|████▌     | 91/200 [05:50<08:03,  4.44s/it]Running Inference:  46%|████▌     | 92/200 [05:58<09:40,  5.37s/it]Running Inference:  46%|████▋     | 93/200 [06:01<08:29,  4.77s/it]Running Inference:  47%|████▋     | 94/200 [06:02<06:36,  3.74s/it]Running Inference:  48%|████▊     | 95/200 [06:04<05:17,  3.02s/it]Running Inference:  48%|████▊     | 96/200 [06:07<05:17,  3.05s/it]Running Inference:  48%|████▊     | 97/200 [06:08<04:21,  2.54s/it]Running Inference:  49%|████▉     | 98/200 [06:10<04:09,  2.44s/it]Running Inference:  50%|████▉     | 99/200 [06:17<06:22,  3.79s/it]Running Inference:  50%|█████     | 100/200 [06:19<05:19,  3.19s/it]Running Inference:  50%|█████     | 101/200 [06:21<04:44,  2.87s/it]Running Inference:  51%|█████     | 102/200 [06:24<04:34,  2.80s/it]Running Inference:  52%|█████▏    | 103/200 [06:31<06:35,  4.07s/it]Running Inference:  52%|█████▏    | 104/200 [06:33<05:44,  3.58s/it]Running Inference:  52%|█████▎    | 105/200 [06:35<04:51,  3.07s/it]Running Inference:  53%|█████▎    | 106/200 [06:43<07:08,  4.56s/it]Running Inference:  54%|█████▎    | 107/200 [06:45<05:44,  3.70s/it]Running Inference:  54%|█████▍    | 108/200 [06:48<05:31,  3.60s/it]Running Inference:  55%|█████▍    | 109/200 [06:50<04:27,  2.94s/it]Running Inference:  55%|█████▌    | 110/200 [06:53<04:33,  3.04s/it]Running Inference:  56%|█████▌    | 111/200 [06:55<04:13,  2.85s/it]Running Inference:  56%|█████▌    | 112/200 [07:02<05:59,  4.09s/it]Running Inference:  56%|█████▋    | 113/200 [07:04<04:53,  3.37s/it]Running Inference:  57%|█████▋    | 114/200 [07:06<04:21,  3.04s/it]Running Inference:  57%|█████▊    | 115/200 [07:10<04:45,  3.36s/it]Running Inference:  58%|█████▊    | 116/200 [07:13<04:32,  3.25s/it]Running Inference:  58%|█████▊    | 117/200 [07:15<03:46,  2.73s/it]Running Inference:  59%|█████▉    | 118/200 [07:18<03:55,  2.87s/it]Running Inference:  60%|█████▉    | 119/200 [07:20<03:38,  2.69s/it]Running Inference:  60%|██████    | 120/200 [07:28<05:42,  4.28s/it]Running Inference:  60%|██████    | 121/200 [07:31<05:00,  3.80s/it]Running Inference:  61%|██████    | 122/200 [07:34<04:43,  3.64s/it]Running Inference:  62%|██████▏   | 123/200 [07:41<05:56,  4.63s/it]Running Inference:  62%|██████▏   | 124/200 [07:42<04:23,  3.47s/it]Running Inference:  62%|██████▎   | 125/200 [07:50<06:07,  4.90s/it]Running Inference:  63%|██████▎   | 126/200 [07:53<05:22,  4.36s/it]Running Inference:  64%|██████▎   | 127/200 [07:56<04:40,  3.84s/it]Running Inference:  64%|██████▍   | 128/200 [07:59<04:25,  3.68s/it]Running Inference:  64%|██████▍   | 129/200 [08:02<03:59,  3.38s/it]Running Inference:  65%|██████▌   | 130/200 [08:10<05:28,  4.69s/it]Running Inference:  66%|██████▌   | 131/200 [08:13<05:03,  4.39s/it]Running Inference:  66%|██████▌   | 132/200 [08:16<04:13,  3.72s/it]Running Inference:  66%|██████▋   | 133/200 [08:19<03:56,  3.53s/it]Running Inference:  67%|██████▋   | 134/200 [08:21<03:37,  3.30s/it]Running Inference:  68%|██████▊   | 135/200 [08:29<04:47,  4.42s/it]Running Inference:  68%|██████▊   | 136/200 [08:36<05:37,  5.27s/it]Running Inference:  68%|██████▊   | 137/200 [08:38<04:32,  4.32s/it]Running Inference:  69%|██████▉   | 138/200 [08:42<04:18,  4.17s/it]Running Inference:  70%|██████▉   | 139/200 [08:48<05:01,  4.94s/it]Running Inference:  70%|███████   | 140/200 [08:53<04:49,  4.82s/it]Running Inference:  70%|███████   | 141/200 [09:00<05:28,  5.57s/it]Running Inference:  71%|███████   | 142/200 [09:02<04:12,  4.35s/it]Running Inference:  72%|███████▏  | 143/200 [09:09<04:59,  5.25s/it]Running Inference:  72%|███████▏  | 144/200 [09:17<05:32,  5.93s/it]Running Inference:  72%|███████▎  | 145/200 [09:19<04:28,  4.89s/it]Running Inference:  73%|███████▎  | 146/200 [09:22<03:47,  4.21s/it]Running Inference:  74%|███████▎  | 147/200 [09:25<03:23,  3.85s/it]Running Inference:  74%|███████▍  | 148/200 [09:26<02:45,  3.18s/it]Running Inference:  74%|███████▍  | 149/200 [09:31<02:58,  3.50s/it]Running Inference:  75%|███████▌  | 150/200 [09:38<03:52,  4.65s/it]Running Inference:  76%|███████▌  | 151/200 [09:41<03:27,  4.22s/it]Running Inference:  76%|███████▌  | 152/200 [09:42<02:35,  3.23s/it]Running Inference:  76%|███████▋  | 153/200 [09:45<02:27,  3.14s/it]Running Inference:  77%|███████▋  | 154/200 [09:47<02:01,  2.65s/it]Running Inference:  78%|███████▊  | 155/200 [09:48<01:45,  2.34s/it]Running Inference:  78%|███████▊  | 156/200 [09:56<02:56,  4.01s/it]Running Inference:  78%|███████▊  | 157/200 [09:58<02:19,  3.24s/it]Running Inference:  79%|███████▉  | 158/200 [10:00<02:02,  2.91s/it]Running Inference:  80%|███████▉  | 159/200 [10:04<02:11,  3.20s/it]Running Inference:  80%|████████  | 160/200 [10:06<01:56,  2.90s/it]Running Inference:  80%|████████  | 161/200 [10:13<02:40,  4.11s/it]Running Inference:  81%|████████  | 162/200 [10:15<02:14,  3.55s/it]Running Inference:  82%|████████▏ | 163/200 [10:23<03:05,  5.02s/it]Running Inference:  82%|████████▏ | 164/200 [10:26<02:37,  4.38s/it]Running Inference:  82%|████████▎ | 165/200 [10:28<02:02,  3.51s/it]Running Inference:  83%|████████▎ | 166/200 [10:35<02:34,  4.56s/it]Running Inference:  84%|████████▎ | 167/200 [10:37<02:03,  3.75s/it]Running Inference:  84%|████████▍ | 168/200 [10:40<01:58,  3.69s/it]Running Inference:  84%|████████▍ | 169/200 [10:42<01:41,  3.28s/it]Running Inference:  85%|████████▌ | 170/200 [10:44<01:23,  2.79s/it]Running Inference:  86%|████████▌ | 171/200 [10:47<01:23,  2.88s/it]Running Inference:  86%|████████▌ | 172/200 [10:49<01:14,  2.66s/it]Running Inference:  86%|████████▋ | 173/200 [10:57<01:48,  4.01s/it]Running Inference:  87%|████████▋ | 174/200 [11:04<02:08,  4.96s/it]Running Inference:  88%|████████▊ | 175/200 [11:07<01:48,  4.33s/it]Running Inference:  88%|████████▊ | 176/200 [11:08<01:23,  3.46s/it]Running Inference:  88%|████████▊ | 177/200 [11:09<01:03,  2.76s/it]Running Inference:  89%|████████▉ | 178/200 [11:14<01:11,  3.26s/it]Running Inference:  90%|████████▉ | 179/200 [11:15<00:59,  2.81s/it]Running Inference:  90%|█████████ | 180/200 [11:17<00:49,  2.47s/it]Running Inference:  90%|█████████ | 181/200 [11:19<00:45,  2.41s/it]Running Inference:  91%|█████████ | 182/200 [11:21<00:38,  2.16s/it]Running Inference:  92%|█████████▏| 183/200 [11:29<01:06,  3.94s/it]Running Inference:  92%|█████████▏| 184/200 [11:34<01:07,  4.25s/it]Running Inference:  92%|█████████▎| 185/200 [11:36<00:56,  3.76s/it]Running Inference:  93%|█████████▎| 186/200 [11:38<00:44,  3.17s/it]Running Inference:  94%|█████████▎| 187/200 [11:40<00:34,  2.63s/it]Running Inference:  94%|█████████▍| 188/200 [11:42<00:30,  2.53s/it]Running Inference:  94%|█████████▍| 189/200 [11:45<00:29,  2.70s/it]Running Inference:  95%|█████████▌| 190/200 [11:48<00:28,  2.83s/it]Running Inference:  96%|█████████▌| 191/200 [11:52<00:27,  3.07s/it]Running Inference:  96%|█████████▌| 192/200 [11:54<00:23,  2.93s/it]Running Inference:  96%|█████████▋| 193/200 [12:03<00:31,  4.52s/it]Running Inference:  97%|█████████▋| 194/200 [12:05<00:22,  3.73s/it]Running Inference:  98%|█████████▊| 195/200 [12:06<00:15,  3.15s/it]Running Inference:  98%|█████████▊| 196/200 [12:14<00:18,  4.64s/it]Running Inference:  98%|█████████▊| 197/200 [12:16<00:11,  3.68s/it]Running Inference:  99%|█████████▉| 198/200 [12:24<00:09,  4.89s/it]Running Inference: 100%|█████████▉| 199/200 [12:26<00:04,  4.06s/it]Running Inference: 100%|██████████| 200/200 [12:33<00:00,  5.00s/it]Running Inference: 100%|██████████| 200/200 [12:33<00:00,  3.77s/it]
2025-12-14 10:04:53,348 - INFO - Inference completed.
2025-12-14 10:04:53,359 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 10:04:53,359 - INFO - Calculating metrics for dataset: longbench
2025-12-14 10:04:53,465 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 10:04:53,465 - INFO - Metrics:
26.89
2025-12-14 10:04:53,466 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=samsum, ratio=0.3) on GPU 1

----------------------------------------
Task: samsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 10:04:59,871 - INFO - Set deterministic seeds to 42
2025-12-14 10:04:59,871 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 10:04:59,871 - INFO - Starting evaluation run...
2025-12-14 10:04:59,871 - INFO - Output directory set to: longbenchresult
2025-12-14 10:04:59,871 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 10:04:59,871 - INFO - KV Press 'tova' setup.
2025-12-14 10:04:59,871 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 10:04:59,871 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.90it/s]
Device set to use cuda:0
2025-12-14 10:05:11,088 - INFO - Model pipeline loaded.
2025-12-14 10:05:11,088 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 10:05:16,183 - INFO - Dataset loaded with 200 entries.
2025-12-14 10:05:16,183 - INFO - Dataset processed with 200 entries.
2025-12-14 10:05:16,210 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:52,  3.58s/it]Running Inference:   1%|          | 2/200 [00:07<12:09,  3.69s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:44,  2.97s/it]Running Inference:   2%|▏         | 4/200 [00:11<08:11,  2.51s/it]Running Inference:   2%|▎         | 5/200 [00:12<06:53,  2.12s/it]Running Inference:   3%|▎         | 6/200 [00:19<12:30,  3.87s/it]Running Inference:   4%|▎         | 7/200 [00:21<10:21,  3.22s/it]Running Inference:   4%|▍         | 8/200 [00:28<13:55,  4.35s/it]Running Inference:   4%|▍         | 9/200 [00:35<16:24,  5.15s/it]Running Inference:   5%|▌         | 10/200 [00:39<15:01,  4.74s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:47<17:59,  5.71s/it]Running Inference:   6%|▌         | 12/200 [00:50<15:10,  4.84s/it]Running Inference:   6%|▋         | 13/200 [00:52<12:40,  4.06s/it]Running Inference:   7%|▋         | 14/200 [00:56<13:03,  4.21s/it]Running Inference:   8%|▊         | 15/200 [00:59<11:13,  3.64s/it]Running Inference:   8%|▊         | 16/200 [01:06<14:12,  4.63s/it]Running Inference:   8%|▊         | 17/200 [01:14<17:20,  5.68s/it]Running Inference:   9%|▉         | 18/200 [01:15<13:11,  4.35s/it]Running Inference:  10%|▉         | 19/200 [01:23<16:25,  5.44s/it]Running Inference:  10%|█         | 20/200 [01:25<12:59,  4.33s/it]Running Inference:  10%|█         | 21/200 [01:28<11:36,  3.89s/it]Running Inference:  11%|█         | 22/200 [01:33<12:46,  4.31s/it]Running Inference:  12%|█▏        | 23/200 [01:40<15:32,  5.27s/it]Running Inference:  12%|█▏        | 24/200 [01:42<12:36,  4.30s/it]Running Inference:  12%|█▎        | 25/200 [01:45<11:04,  3.79s/it]Running Inference:  13%|█▎        | 26/200 [01:49<11:13,  3.87s/it]Running Inference:  14%|█▎        | 27/200 [01:54<12:08,  4.21s/it]Running Inference:  14%|█▍        | 28/200 [01:57<10:59,  3.84s/it]Running Inference:  14%|█▍        | 29/200 [01:59<08:52,  3.12s/it]Running Inference:  15%|█▌        | 30/200 [02:01<08:04,  2.85s/it]Running Inference:  16%|█▌        | 31/200 [02:03<07:20,  2.61s/it]Running Inference:  16%|█▌        | 32/200 [02:06<07:46,  2.78s/it]Running Inference:  16%|█▋        | 33/200 [02:13<10:55,  3.93s/it]Running Inference:  17%|█▋        | 34/200 [02:16<10:22,  3.75s/it]Running Inference:  18%|█▊        | 35/200 [02:18<09:05,  3.30s/it]Running Inference:  18%|█▊        | 36/200 [02:26<12:45,  4.67s/it]Running Inference:  18%|█▊        | 37/200 [02:28<10:45,  3.96s/it]Running Inference:  19%|█▉        | 38/200 [02:31<09:41,  3.59s/it]Running Inference:  20%|█▉        | 39/200 [02:38<12:15,  4.57s/it]Running Inference:  20%|██        | 40/200 [02:41<11:21,  4.26s/it]Running Inference:  20%|██        | 41/200 [02:43<09:22,  3.54s/it]Running Inference:  21%|██        | 42/200 [02:50<11:56,  4.53s/it]Running Inference:  22%|██▏       | 43/200 [02:52<10:01,  3.83s/it]Running Inference:  22%|██▏       | 44/200 [02:55<08:55,  3.43s/it]Running Inference:  22%|██▎       | 45/200 [02:57<08:11,  3.17s/it]Running Inference:  23%|██▎       | 46/200 [03:05<11:39,  4.54s/it]Running Inference:  24%|██▎       | 47/200 [03:08<10:04,  3.95s/it]Running Inference:  24%|██▍       | 48/200 [03:11<09:30,  3.75s/it]Running Inference:  24%|██▍       | 49/200 [03:19<12:31,  4.98s/it]Running Inference:  25%|██▌       | 50/200 [03:21<10:06,  4.05s/it]Running Inference:  26%|██▌       | 51/200 [03:27<11:58,  4.82s/it]Running Inference:  26%|██▌       | 52/200 [03:30<10:36,  4.30s/it]Running Inference:  26%|██▋       | 53/200 [03:33<09:00,  3.68s/it]Running Inference:  27%|██▋       | 54/200 [03:39<11:03,  4.55s/it]Running Inference:  28%|██▊       | 55/200 [03:42<09:56,  4.11s/it]Running Inference:  28%|██▊       | 56/200 [03:45<09:05,  3.79s/it]Running Inference:  28%|██▊       | 57/200 [03:48<08:20,  3.50s/it]Running Inference:  29%|██▉       | 58/200 [03:56<11:39,  4.93s/it]Running Inference:  30%|██▉       | 59/200 [04:03<12:49,  5.46s/it]Running Inference:  30%|███       | 60/200 [04:10<13:39,  5.86s/it]Running Inference:  30%|███       | 61/200 [04:11<10:13,  4.41s/it]Running Inference:  31%|███       | 62/200 [04:14<09:07,  3.97s/it]Running Inference:  32%|███▏      | 63/200 [04:16<08:00,  3.51s/it]Running Inference:  32%|███▏      | 64/200 [04:19<07:35,  3.35s/it]Running Inference:  32%|███▎      | 65/200 [04:23<08:02,  3.58s/it]Running Inference:  33%|███▎      | 66/200 [04:26<07:15,  3.25s/it]Running Inference:  34%|███▎      | 67/200 [04:27<05:32,  2.50s/it]Running Inference:  34%|███▍      | 68/200 [04:33<08:16,  3.76s/it]Running Inference:  34%|███▍      | 69/200 [04:35<07:03,  3.23s/it]Running Inference:  35%|███▌      | 70/200 [04:43<09:48,  4.53s/it]Running Inference:  36%|███▌      | 71/200 [04:45<08:02,  3.74s/it]Running Inference:  36%|███▌      | 72/200 [04:47<06:59,  3.28s/it]Running Inference:  36%|███▋      | 73/200 [04:49<05:59,  2.83s/it]Running Inference:  37%|███▋      | 74/200 [04:51<05:19,  2.53s/it]Running Inference:  38%|███▊      | 75/200 [04:52<04:28,  2.15s/it]Running Inference:  38%|███▊      | 76/200 [04:53<04:03,  1.97s/it]Running Inference:  38%|███▊      | 77/200 [04:56<04:36,  2.25s/it]Running Inference:  39%|███▉      | 78/200 [04:59<04:58,  2.44s/it]Running Inference:  40%|███▉      | 79/200 [05:02<04:49,  2.40s/it]Running Inference:  40%|████      | 80/200 [05:03<04:08,  2.07s/it]Running Inference:  40%|████      | 81/200 [05:04<03:35,  1.81s/it]Running Inference:  41%|████      | 82/200 [05:11<06:26,  3.27s/it]Running Inference:  42%|████▏     | 83/200 [05:19<09:19,  4.79s/it]Running Inference:  42%|████▏     | 84/200 [05:22<08:19,  4.31s/it]Running Inference:  42%|████▎     | 85/200 [05:27<08:15,  4.31s/it]Running Inference:  43%|████▎     | 86/200 [05:33<09:33,  5.03s/it]Running Inference:  44%|████▎     | 87/200 [05:36<08:12,  4.36s/it]Running Inference:  44%|████▍     | 88/200 [05:38<06:48,  3.64s/it]Running Inference:  44%|████▍     | 89/200 [05:42<06:38,  3.59s/it]Running Inference:  45%|████▌     | 90/200 [05:44<05:47,  3.16s/it]Running Inference:  46%|████▌     | 91/200 [05:48<06:34,  3.62s/it]Running Inference:  46%|████▌     | 92/200 [05:56<08:35,  4.78s/it]Running Inference:  46%|████▋     | 93/200 [06:04<10:24,  5.84s/it]Running Inference:  47%|████▋     | 94/200 [06:05<07:53,  4.46s/it]Running Inference:  48%|████▊     | 95/200 [06:07<06:10,  3.52s/it]Running Inference:  48%|████▊     | 96/200 [06:10<05:50,  3.37s/it]Running Inference:  48%|████▊     | 97/200 [06:11<04:44,  2.76s/it]Running Inference:  49%|████▉     | 98/200 [06:19<07:30,  4.42s/it]Running Inference:  50%|████▉     | 99/200 [06:26<08:40,  5.16s/it]Running Inference:  50%|█████     | 100/200 [06:28<06:54,  4.14s/it]Running Inference:  50%|█████     | 101/200 [06:30<05:46,  3.50s/it]Running Inference:  51%|█████     | 102/200 [06:33<05:14,  3.21s/it]Running Inference:  52%|█████▏    | 103/200 [06:40<07:00,  4.34s/it]Running Inference:  52%|█████▏    | 104/200 [06:42<06:06,  3.81s/it]Running Inference:  52%|█████▎    | 105/200 [06:44<05:06,  3.23s/it]Running Inference:  53%|█████▎    | 106/200 [06:52<07:16,  4.65s/it]Running Inference:  54%|█████▎    | 107/200 [06:54<05:49,  3.76s/it]Running Inference:  54%|█████▍    | 108/200 [06:56<05:18,  3.47s/it]Running Inference:  55%|█████▍    | 109/200 [06:58<04:17,  2.83s/it]Running Inference:  55%|█████▌    | 110/200 [07:01<04:25,  2.95s/it]Running Inference:  56%|█████▌    | 111/200 [07:03<04:07,  2.78s/it]Running Inference:  56%|█████▌    | 112/200 [07:10<05:53,  4.02s/it]Running Inference:  56%|█████▋    | 113/200 [07:12<04:48,  3.32s/it]Running Inference:  57%|█████▋    | 114/200 [07:14<04:15,  2.97s/it]Running Inference:  57%|█████▊    | 115/200 [07:18<04:40,  3.30s/it]Running Inference:  58%|█████▊    | 116/200 [07:21<04:33,  3.25s/it]Running Inference:  58%|█████▊    | 117/200 [07:23<03:46,  2.73s/it]Running Inference:  59%|█████▉    | 118/200 [07:26<03:54,  2.87s/it]Running Inference:  60%|█████▉    | 119/200 [07:28<03:37,  2.69s/it]Running Inference:  60%|██████    | 120/200 [07:36<05:40,  4.25s/it]Running Inference:  60%|██████    | 121/200 [07:39<04:58,  3.78s/it]Running Inference:  61%|██████    | 122/200 [07:42<04:45,  3.67s/it]Running Inference:  62%|██████▏   | 123/200 [07:49<05:56,  4.63s/it]Running Inference:  62%|██████▏   | 124/200 [07:50<04:30,  3.56s/it]Running Inference:  62%|██████▎   | 125/200 [07:58<06:10,  4.93s/it]Running Inference:  63%|██████▎   | 126/200 [08:01<05:24,  4.38s/it]Running Inference:  64%|██████▎   | 127/200 [08:04<04:40,  3.85s/it]Running Inference:  64%|██████▍   | 128/200 [08:12<06:02,  5.03s/it]Running Inference:  64%|██████▍   | 129/200 [08:14<05:06,  4.31s/it]Running Inference:  65%|██████▌   | 130/200 [08:22<06:12,  5.33s/it]Running Inference:  66%|██████▌   | 131/200 [08:25<05:25,  4.71s/it]Running Inference:  66%|██████▌   | 132/200 [08:28<04:28,  3.94s/it]Running Inference:  66%|██████▋   | 133/200 [08:31<04:06,  3.68s/it]Running Inference:  67%|██████▋   | 134/200 [08:33<03:44,  3.40s/it]Running Inference:  68%|██████▊   | 135/200 [08:40<04:50,  4.47s/it]Running Inference:  68%|██████▊   | 136/200 [08:48<05:38,  5.29s/it]Running Inference:  68%|██████▊   | 137/200 [08:50<04:34,  4.36s/it]Running Inference:  69%|██████▉   | 138/200 [08:54<04:18,  4.17s/it]Running Inference:  70%|██████▉   | 139/200 [09:00<05:00,  4.93s/it]Running Inference:  70%|███████   | 140/200 [09:05<04:47,  4.80s/it]Running Inference:  70%|███████   | 141/200 [09:07<03:52,  3.94s/it]Running Inference:  71%|███████   | 142/200 [09:08<03:11,  3.30s/it]Running Inference:  72%|███████▏  | 143/200 [09:16<04:16,  4.50s/it]Running Inference:  72%|███████▏  | 144/200 [09:23<05:01,  5.39s/it]Running Inference:  72%|███████▎  | 145/200 [09:26<04:07,  4.50s/it]Running Inference:  73%|███████▎  | 146/200 [09:28<03:32,  3.94s/it]Running Inference:  74%|███████▎  | 147/200 [09:36<04:33,  5.16s/it]Running Inference:  74%|███████▍  | 148/200 [09:38<03:33,  4.10s/it]Running Inference:  74%|███████▍  | 149/200 [09:42<03:31,  4.14s/it]Running Inference:  75%|███████▌  | 150/200 [09:49<04:13,  5.08s/it]Running Inference:  76%|███████▌  | 151/200 [09:52<03:39,  4.48s/it]Running Inference:  76%|███████▌  | 152/200 [09:53<02:43,  3.41s/it]Running Inference:  76%|███████▋  | 153/200 [09:56<02:32,  3.25s/it]Running Inference:  77%|███████▋  | 154/200 [09:58<02:05,  2.73s/it]Running Inference:  78%|███████▊  | 155/200 [09:59<01:47,  2.40s/it]Running Inference:  78%|███████▊  | 156/200 [10:07<02:57,  4.03s/it]Running Inference:  78%|███████▊  | 157/200 [10:09<02:17,  3.20s/it]Running Inference:  79%|███████▉  | 158/200 [10:11<02:01,  2.88s/it]Running Inference:  80%|███████▉  | 159/200 [10:15<02:13,  3.25s/it]Running Inference:  80%|████████  | 160/200 [10:17<01:57,  2.93s/it]Running Inference:  80%|████████  | 161/200 [10:24<02:40,  4.11s/it]Running Inference:  81%|████████  | 162/200 [10:26<02:16,  3.59s/it]Running Inference:  82%|████████▏ | 163/200 [10:35<03:05,  5.02s/it]Running Inference:  82%|████████▏ | 164/200 [10:37<02:37,  4.38s/it]Running Inference:  82%|████████▎ | 165/200 [10:39<02:02,  3.51s/it]Running Inference:  83%|████████▎ | 166/200 [10:46<02:34,  4.54s/it]Running Inference:  84%|████████▎ | 167/200 [10:48<02:02,  3.71s/it]Running Inference:  84%|████████▍ | 168/200 [10:51<01:56,  3.65s/it]Running Inference:  84%|████████▍ | 169/200 [10:53<01:40,  3.25s/it]Running Inference:  85%|████████▌ | 170/200 [10:55<01:22,  2.75s/it]Running Inference:  86%|████████▌ | 171/200 [10:58<01:22,  2.84s/it]Running Inference:  86%|████████▌ | 172/200 [11:05<01:53,  4.06s/it]Running Inference:  86%|████████▋ | 173/200 [11:12<02:14,  4.97s/it]Running Inference:  87%|████████▋ | 174/200 [11:19<02:25,  5.61s/it]Running Inference:  88%|████████▊ | 175/200 [11:22<01:57,  4.68s/it]Running Inference:  88%|████████▊ | 176/200 [11:23<01:28,  3.70s/it]Running Inference:  88%|████████▊ | 177/200 [11:24<01:07,  2.92s/it]Running Inference:  89%|████████▉ | 178/200 [11:29<01:13,  3.36s/it]Running Inference:  90%|████████▉ | 179/200 [11:30<01:00,  2.88s/it]Running Inference:  90%|█████████ | 180/200 [11:32<00:50,  2.51s/it]Running Inference:  90%|█████████ | 181/200 [11:35<00:48,  2.55s/it]Running Inference:  91%|█████████ | 182/200 [11:36<00:40,  2.25s/it]Running Inference:  92%|█████████▏| 183/200 [11:44<01:07,  3.99s/it]Running Inference:  92%|█████████▏| 184/200 [11:49<01:07,  4.22s/it]Running Inference:  92%|█████████▎| 185/200 [11:52<00:55,  3.73s/it]Running Inference:  93%|█████████▎| 186/200 [11:53<00:44,  3.15s/it]Running Inference:  94%|█████████▎| 187/200 [11:55<00:33,  2.61s/it]Running Inference:  94%|█████████▍| 188/200 [11:57<00:30,  2.51s/it]Running Inference:  94%|█████████▍| 189/200 [12:00<00:29,  2.68s/it]Running Inference:  95%|█████████▌| 190/200 [12:03<00:27,  2.77s/it]Running Inference:  96%|█████████▌| 191/200 [12:06<00:26,  2.92s/it]Running Inference:  96%|█████████▌| 192/200 [12:09<00:22,  2.76s/it]Running Inference:  96%|█████████▋| 193/200 [12:17<00:30,  4.37s/it]Running Inference:  97%|█████████▋| 194/200 [12:23<00:30,  5.03s/it]Running Inference:  98%|█████████▊| 195/200 [12:25<00:20,  4.05s/it]Running Inference:  98%|█████████▊| 196/200 [12:33<00:20,  5.24s/it]Running Inference:  98%|█████████▊| 197/200 [12:35<00:12,  4.10s/it]Running Inference:  99%|█████████▉| 198/200 [12:42<00:10,  5.17s/it]Running Inference: 100%|█████████▉| 199/200 [12:45<00:04,  4.29s/it]Running Inference: 100%|██████████| 200/200 [12:52<00:00,  5.14s/it]Running Inference: 100%|██████████| 200/200 [12:52<00:00,  3.86s/it]
2025-12-14 10:18:08,419 - INFO - Inference completed.
2025-12-14 10:18:08,429 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 10:18:08,429 - INFO - Calculating metrics for dataset: longbench
2025-12-14 10:18:08,540 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 10:18:08,540 - INFO - Metrics:
27.01
2025-12-14 10:18:08,541 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=samsum, ratio=0.5) on GPU 1


========================================
LongBench Task: vcsum
========================================
----------------------------------------
Task: vcsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 10:18:14,936 - INFO - Set deterministic seeds to 42
2025-12-14 10:18:14,936 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 10:18:14,936 - INFO - Starting evaluation run...
2025-12-14 10:18:14,936 - INFO - Output directory set to: longbenchresult
2025-12-14 10:18:14,936 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 10:18:14,936 - INFO - KV Press 'tova' setup.
2025-12-14 10:18:14,936 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 10:18:14,936 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.07it/s]
Device set to use cuda:0
2025-12-14 10:18:26,917 - INFO - Model pipeline loaded.
2025-12-14 10:18:26,917 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 1314.40 examples/s]Generating test split: 200 examples [00:00, 1309.94 examples/s]
2025-12-14 10:18:31,602 - INFO - Dataset loaded with 200 entries.
2025-12-14 10:18:31,602 - INFO - Dataset processed with 200 entries.
2025-12-14 10:18:31,628 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:16, 23.90s/it]Running Inference:   1%|          | 2/200 [00:47<1:18:39, 23.84s/it]Running Inference:   2%|▏         | 3/200 [01:11<1:17:56, 23.74s/it]Running Inference:   2%|▏         | 4/200 [01:34<1:16:54, 23.54s/it]Running Inference:   2%|▎         | 5/200 [01:58<1:17:10, 23.75s/it]Running Inference:   3%|▎         | 6/200 [02:21<1:15:44, 23.43s/it]Running Inference:   4%|▎         | 7/200 [02:40<1:11:13, 22.14s/it]Running Inference:   4%|▍         | 8/200 [03:03<1:11:28, 22.34s/it]Running Inference:   4%|▍         | 9/200 [03:26<1:11:43, 22.53s/it]Running Inference:   5%|▌         | 10/200 [03:51<1:13:27, 23.20s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:14<1:13:17, 23.27s/it]Running Inference:   6%|▌         | 12/200 [04:37<1:12:36, 23.17s/it]Running Inference:   6%|▋         | 13/200 [05:01<1:12:56, 23.40s/it]Running Inference:   7%|▋         | 14/200 [05:18<1:06:23, 21.41s/it]Running Inference:   8%|▊         | 15/200 [05:41<1:07:19, 21.84s/it]Running Inference:   8%|▊         | 16/200 [06:05<1:09:09, 22.55s/it]Running Inference:   8%|▊         | 17/200 [06:21<1:02:29, 20.49s/it]Running Inference:   9%|▉         | 18/200 [06:46<1:06:44, 22.00s/it]Running Inference:  10%|▉         | 19/200 [07:08<1:05:52, 21.84s/it]Running Inference:  10%|█         | 20/200 [07:32<1:07:41, 22.57s/it]Running Inference:  10%|█         | 21/200 [07:56<1:08:22, 22.92s/it]Running Inference:  11%|█         | 22/200 [08:19<1:08:33, 23.11s/it]Running Inference:  12%|█▏        | 23/200 [08:42<1:08:06, 23.09s/it]Running Inference:  12%|█▏        | 24/200 [09:08<1:10:03, 23.88s/it]Running Inference:  12%|█▎        | 25/200 [09:31<1:09:05, 23.69s/it]Running Inference:  13%|█▎        | 26/200 [09:56<1:09:39, 24.02s/it]Running Inference:  14%|█▎        | 27/200 [10:20<1:08:46, 23.85s/it]Running Inference:  14%|█▍        | 28/200 [10:43<1:07:53, 23.69s/it]Running Inference:  14%|█▍        | 29/200 [11:06<1:07:25, 23.66s/it]Running Inference:  15%|█▌        | 30/200 [11:27<1:04:19, 22.70s/it]Running Inference:  16%|█▌        | 31/200 [11:50<1:04:16, 22.82s/it]Running Inference:  16%|█▌        | 32/200 [11:59<52:42, 18.83s/it]  Running Inference:  16%|█▋        | 33/200 [12:17<51:08, 18.38s/it]Running Inference:  17%|█▋        | 34/200 [12:40<54:25, 19.67s/it]Running Inference:  18%|█▊        | 35/200 [13:03<57:22, 20.86s/it]Running Inference:  18%|█▊        | 36/200 [13:26<58:53, 21.55s/it]Running Inference:  18%|█▊        | 37/200 [13:49<59:43, 21.99s/it]Running Inference:  19%|█▉        | 38/200 [14:14<1:01:39, 22.83s/it]Running Inference:  20%|█▉        | 39/200 [14:38<1:01:53, 23.07s/it]Running Inference:  20%|██        | 40/200 [15:03<1:03:08, 23.68s/it]Running Inference:  20%|██        | 41/200 [15:30<1:05:45, 24.81s/it]Running Inference:  21%|██        | 42/200 [15:54<1:04:12, 24.39s/it]Running Inference:  22%|██▏       | 43/200 [16:17<1:03:13, 24.16s/it]Running Inference:  22%|██▏       | 44/200 [16:42<1:03:32, 24.44s/it]Running Inference:  22%|██▎       | 45/200 [17:06<1:02:14, 24.10s/it]Running Inference:  23%|██▎       | 46/200 [17:22<56:07, 21.87s/it]  Running Inference:  24%|██▎       | 47/200 [17:47<57:54, 22.71s/it]Running Inference:  24%|██▍       | 48/200 [18:10<57:43, 22.79s/it]Running Inference:  24%|██▍       | 49/200 [18:33<57:32, 22.86s/it]Running Inference:  25%|██▌       | 50/200 [18:55<56:14, 22.49s/it]Running Inference:  26%|██▌       | 51/200 [19:19<56:56, 22.93s/it]Running Inference:  26%|██▌       | 52/200 [19:42<57:14, 23.20s/it]Running Inference:  26%|██▋       | 53/200 [20:06<56:53, 23.22s/it]Running Inference:  27%|██▋       | 54/200 [20:29<56:31, 23.23s/it]Running Inference:  28%|██▊       | 55/200 [20:52<56:03, 23.20s/it]Running Inference:  28%|██▊       | 56/200 [21:15<55:37, 23.18s/it]Running Inference:  28%|██▊       | 57/200 [21:38<54:54, 23.04s/it]Running Inference:  29%|██▉       | 58/200 [22:02<55:02, 23.26s/it]Running Inference:  30%|██▉       | 59/200 [22:25<54:58, 23.39s/it]Running Inference:  30%|███       | 60/200 [22:50<55:04, 23.60s/it]Running Inference:  30%|███       | 61/200 [23:13<54:15, 23.42s/it]Running Inference:  31%|███       | 62/200 [23:35<53:30, 23.26s/it]Running Inference:  32%|███▏      | 63/200 [23:59<53:11, 23.30s/it]Running Inference:  32%|███▏      | 64/200 [24:22<53:03, 23.40s/it]Running Inference:  32%|███▎      | 65/200 [24:46<52:51, 23.49s/it]Running Inference:  33%|███▎      | 66/200 [25:06<50:03, 22.42s/it]Running Inference:  34%|███▎      | 67/200 [25:30<50:23, 22.73s/it]Running Inference:  34%|███▍      | 68/200 [25:53<50:28, 22.94s/it]Running Inference:  34%|███▍      | 69/200 [26:18<51:12, 23.45s/it]Running Inference:  35%|███▌      | 70/200 [26:36<47:37, 21.98s/it]Running Inference:  36%|███▌      | 71/200 [26:59<47:56, 22.30s/it]Running Inference:  36%|███▌      | 72/200 [27:23<48:40, 22.82s/it]Running Inference:  36%|███▋      | 73/200 [27:47<48:50, 23.07s/it]Running Inference:  37%|███▋      | 74/200 [28:10<48:34, 23.13s/it]Running Inference:  38%|███▊      | 75/200 [28:33<48:19, 23.20s/it]Running Inference:  38%|███▊      | 76/200 [28:57<47:51, 23.16s/it]Running Inference:  38%|███▊      | 77/200 [29:21<48:27, 23.64s/it]Running Inference:  39%|███▉      | 78/200 [29:44<47:15, 23.24s/it]Running Inference:  40%|███▉      | 79/200 [30:07<46:56, 23.28s/it]Running Inference:  40%|████      | 80/200 [30:30<46:26, 23.22s/it]Running Inference:  40%|████      | 81/200 [30:54<46:12, 23.29s/it]Running Inference:  41%|████      | 82/200 [31:17<45:48, 23.29s/it]Running Inference:  42%|████▏     | 83/200 [31:40<45:27, 23.32s/it]Running Inference:  42%|████▏     | 84/200 [32:04<45:31, 23.55s/it]Running Inference:  42%|████▎     | 85/200 [32:28<45:12, 23.58s/it]Running Inference:  43%|████▎     | 86/200 [32:52<45:03, 23.71s/it]Running Inference:  44%|████▎     | 87/200 [33:15<44:27, 23.60s/it]Running Inference:  44%|████▍     | 88/200 [33:39<43:53, 23.52s/it]Running Inference:  44%|████▍     | 89/200 [34:01<42:58, 23.23s/it]Running Inference:  45%|████▌     | 90/200 [34:17<38:22, 20.94s/it]Running Inference:  46%|████▌     | 91/200 [34:41<39:45, 21.88s/it]Running Inference:  46%|████▌     | 92/200 [35:04<39:50, 22.14s/it]Running Inference:  46%|████▋     | 93/200 [35:28<40:37, 22.78s/it]Running Inference:  47%|████▋     | 94/200 [35:53<41:14, 23.34s/it]Running Inference:  48%|████▊     | 95/200 [36:18<41:43, 23.84s/it]Running Inference:  48%|████▊     | 96/200 [36:41<40:53, 23.59s/it]Running Inference:  48%|████▊     | 97/200 [37:04<40:10, 23.41s/it]Running Inference:  49%|████▉     | 98/200 [37:27<39:55, 23.49s/it]Running Inference:  50%|████▉     | 99/200 [37:48<38:20, 22.78s/it]Running Inference:  50%|█████     | 100/200 [38:12<38:27, 23.07s/it]Running Inference:  50%|█████     | 101/200 [38:37<39:04, 23.69s/it]Running Inference:  51%|█████     | 102/200 [39:05<40:31, 24.81s/it]Running Inference:  52%|█████▏    | 103/200 [39:29<40:03, 24.77s/it]Running Inference:  52%|█████▏    | 104/200 [39:53<39:00, 24.38s/it]Running Inference:  52%|█████▎    | 105/200 [40:19<39:41, 25.06s/it]Running Inference:  53%|█████▎    | 106/200 [40:43<38:27, 24.55s/it]Running Inference:  54%|█████▎    | 107/200 [40:56<32:57, 21.26s/it]Running Inference:  54%|█████▍    | 108/200 [41:20<33:39, 21.95s/it]Running Inference:  55%|█████▍    | 109/200 [41:43<33:45, 22.26s/it]Running Inference:  55%|█████▌    | 110/200 [42:07<34:00, 22.67s/it]Running Inference:  56%|█████▌    | 111/200 [42:29<33:38, 22.68s/it]Running Inference:  56%|█████▌    | 112/200 [42:53<33:37, 22.93s/it]Running Inference:  56%|█████▋    | 113/200 [43:16<33:23, 23.03s/it]Running Inference:  57%|█████▋    | 114/200 [43:39<33:04, 23.07s/it]Running Inference:  57%|█████▊    | 115/200 [44:03<32:55, 23.24s/it]Running Inference:  58%|█████▊    | 116/200 [44:26<32:33, 23.26s/it]Running Inference:  58%|█████▊    | 117/200 [44:49<32:12, 23.28s/it]Running Inference:  59%|█████▉    | 118/200 [45:12<31:34, 23.10s/it]Running Inference:  60%|█████▉    | 119/200 [45:36<31:28, 23.31s/it]Running Inference:  60%|██████    | 120/200 [46:00<31:28, 23.61s/it]Running Inference:  60%|██████    | 121/200 [46:24<31:18, 23.77s/it]Running Inference:  61%|██████    | 122/200 [46:47<30:26, 23.42s/it]Running Inference:  62%|██████▏   | 123/200 [47:12<30:35, 23.84s/it]Running Inference:  62%|██████▏   | 124/200 [47:36<30:25, 24.03s/it]Running Inference:  62%|██████▎   | 125/200 [47:59<29:43, 23.78s/it]Running Inference:  63%|██████▎   | 126/200 [48:23<29:12, 23.69s/it]Running Inference:  64%|██████▎   | 127/200 [48:47<29:04, 23.90s/it]Running Inference:  64%|██████▍   | 128/200 [49:11<28:34, 23.82s/it]Running Inference:  64%|██████▍   | 129/200 [49:35<28:05, 23.74s/it]Running Inference:  65%|██████▌   | 130/200 [49:51<25:01, 21.44s/it]Running Inference:  66%|██████▌   | 131/200 [50:14<25:19, 22.02s/it]Running Inference:  66%|██████▌   | 132/200 [50:37<25:20, 22.36s/it]Running Inference:  66%|██████▋   | 133/200 [51:01<25:27, 22.80s/it]Running Inference:  67%|██████▋   | 134/200 [51:24<25:06, 22.83s/it]Running Inference:  68%|██████▊   | 135/200 [51:47<24:51, 22.95s/it]Running Inference:  68%|██████▊   | 136/200 [52:10<24:24, 22.89s/it]Running Inference:  68%|██████▊   | 137/200 [52:33<24:07, 22.98s/it]Running Inference:  69%|██████▉   | 138/200 [52:49<21:28, 20.79s/it]Running Inference:  70%|██████▉   | 139/200 [53:13<22:10, 21.82s/it]Running Inference:  70%|███████   | 140/200 [53:37<22:31, 22.53s/it]Running Inference:  70%|███████   | 141/200 [54:00<22:22, 22.75s/it]Running Inference:  71%|███████   | 142/200 [54:22<21:31, 22.27s/it]Running Inference:  72%|███████▏  | 143/200 [54:45<21:36, 22.74s/it]Running Inference:  72%|███████▏  | 144/200 [55:09<21:22, 22.89s/it]Running Inference:  72%|███████▎  | 145/200 [55:34<21:35, 23.56s/it]Running Inference:  73%|███████▎  | 146/200 [55:57<21:06, 23.46s/it]Running Inference:  74%|███████▎  | 147/200 [56:20<20:34, 23.30s/it]Running Inference:  74%|███████▍  | 148/200 [56:43<20:04, 23.16s/it]Running Inference:  74%|███████▍  | 149/200 [57:06<19:48, 23.31s/it]Running Inference:  75%|███████▌  | 150/200 [57:31<19:44, 23.70s/it]Running Inference:  76%|███████▌  | 151/200 [57:55<19:30, 23.88s/it]Running Inference:  76%|███████▌  | 152/200 [58:20<19:13, 24.04s/it]Running Inference:  76%|███████▋  | 153/200 [58:43<18:39, 23.83s/it]Running Inference:  77%|███████▋  | 154/200 [59:06<18:01, 23.52s/it]Running Inference:  78%|███████▊  | 155/200 [59:29<17:32, 23.39s/it]Running Inference:  78%|███████▊  | 156/200 [59:52<17:01, 23.22s/it]Running Inference:  78%|███████▊  | 157/200 [1:00:15<16:37, 23.19s/it]Running Inference:  79%|███████▉  | 158/200 [1:00:38<16:17, 23.28s/it]Running Inference:  80%|███████▉  | 159/200 [1:00:49<13:14, 19.38s/it]Running Inference:  80%|████████  | 160/200 [1:01:14<14:02, 21.07s/it]Running Inference:  80%|████████  | 161/200 [1:01:32<13:14, 20.37s/it]Running Inference:  81%|████████  | 162/200 [1:01:57<13:39, 21.57s/it]Running Inference:  82%|████████▏ | 163/200 [1:02:23<14:07, 22.90s/it]Running Inference:  82%|████████▏ | 164/200 [1:02:46<13:49, 23.04s/it]Running Inference:  82%|████████▎ | 165/200 [1:03:11<13:45, 23.57s/it]Running Inference:  83%|████████▎ | 166/200 [1:03:34<13:13, 23.35s/it]Running Inference:  84%|████████▎ | 167/200 [1:03:57<12:48, 23.28s/it]Running Inference:  84%|████████▍ | 168/200 [1:04:20<12:25, 23.28s/it]Running Inference:  84%|████████▍ | 169/200 [1:04:43<11:59, 23.20s/it]Running Inference:  85%|████████▌ | 170/200 [1:05:07<11:36, 23.23s/it]Running Inference:  86%|████████▌ | 171/200 [1:05:29<11:09, 23.08s/it]Running Inference:  86%|████████▌ | 172/200 [1:05:53<10:49, 23.19s/it]Running Inference:  86%|████████▋ | 173/200 [1:06:17<10:39, 23.67s/it]Running Inference:  87%|████████▋ | 174/200 [1:06:41<10:11, 23.52s/it]Running Inference:  88%|████████▊ | 175/200 [1:07:03<09:39, 23.19s/it]Running Inference:  88%|████████▊ | 176/200 [1:07:18<08:20, 20.85s/it]Running Inference:  88%|████████▊ | 177/200 [1:07:41<08:13, 21.44s/it]Running Inference:  89%|████████▉ | 178/200 [1:08:04<07:58, 21.76s/it]Running Inference:  90%|████████▉ | 179/200 [1:08:28<07:50, 22.38s/it]Running Inference:  90%|█████████ | 180/200 [1:08:50<07:29, 22.49s/it]Running Inference:  90%|█████████ | 181/200 [1:09:13<07:10, 22.64s/it]Running Inference:  91%|█████████ | 182/200 [1:09:37<06:52, 22.94s/it]Running Inference:  92%|█████████▏| 183/200 [1:10:00<06:30, 22.97s/it]Running Inference:  92%|█████████▏| 184/200 [1:10:16<05:32, 20.79s/it]Running Inference:  92%|█████████▎| 185/200 [1:10:31<04:45, 19.07s/it]Running Inference:  93%|█████████▎| 186/200 [1:10:47<04:15, 18.23s/it]Running Inference:  94%|█████████▎| 187/200 [1:11:10<04:15, 19.62s/it]Running Inference:  94%|█████████▍| 188/200 [1:11:33<04:09, 20.79s/it]Running Inference:  94%|█████████▍| 189/200 [1:11:56<03:55, 21.37s/it]Running Inference:  95%|█████████▌| 190/200 [1:12:13<03:19, 19.92s/it]Running Inference:  96%|█████████▌| 191/200 [1:12:36<03:07, 20.79s/it]Running Inference:  96%|█████████▌| 192/200 [1:12:38<02:02, 15.30s/it]Running Inference:  96%|█████████▋| 193/200 [1:13:01<02:02, 17.57s/it]Running Inference:  97%|█████████▋| 194/200 [1:13:24<01:56, 19.38s/it]Running Inference:  98%|█████████▊| 195/200 [1:13:50<01:46, 21.24s/it]Running Inference:  98%|█████████▊| 196/200 [1:14:14<01:27, 21.91s/it]Running Inference:  98%|█████████▊| 197/200 [1:14:35<01:05, 21.68s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:58<00:44, 22.09s/it]Running Inference: 100%|█████████▉| 199/200 [1:15:21<00:22, 22.38s/it]Running Inference: 100%|██████████| 200/200 [1:15:44<00:00, 22.65s/it]Running Inference: 100%|██████████| 200/200 [1:15:44<00:00, 22.72s/it]
2025-12-14 11:34:16,216 - INFO - Inference completed.
2025-12-14 11:34:16,229 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 11:34:16,229 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Loading model cost 0.538 seconds.
Prefix dict has been built successfully.
2025-12-14 11:34:22,927 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 11:34:22,927 - INFO - Metrics:
11.61
2025-12-14 11:34:22,928 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: tova (task=vcsum, ratio=0.1) on GPU 1

----------------------------------------
Task: vcsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 11:34:29,934 - INFO - Set deterministic seeds to 42
2025-12-14 11:34:29,935 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 11:34:29,935 - INFO - Starting evaluation run...
2025-12-14 11:34:29,935 - INFO - Output directory set to: longbenchresult
2025-12-14 11:34:29,935 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 11:34:29,935 - INFO - KV Press 'tova' setup.
2025-12-14 11:34:29,935 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 11:34:29,935 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.99it/s]
Device set to use cuda:0
2025-12-14 11:34:42,883 - INFO - Model pipeline loaded.
2025-12-14 11:34:42,883 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 11:34:48,249 - INFO - Dataset loaded with 200 entries.
2025-12-14 11:34:48,249 - INFO - Dataset processed with 200 entries.
2025-12-14 11:34:48,276 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:28, 23.96s/it]Running Inference:   1%|          | 2/200 [00:47<1:18:43, 23.86s/it]Running Inference:   2%|▏         | 3/200 [01:11<1:17:54, 23.73s/it]Running Inference:   2%|▏         | 4/200 [01:34<1:16:52, 23.53s/it]Running Inference:   2%|▎         | 5/200 [01:58<1:16:53, 23.66s/it]Running Inference:   3%|▎         | 6/200 [02:21<1:15:33, 23.37s/it]Running Inference:   4%|▎         | 7/200 [02:44<1:15:17, 23.41s/it]Running Inference:   4%|▍         | 8/200 [02:56<1:02:38, 19.58s/it]Running Inference:   4%|▍         | 9/200 [03:10<57:12, 17.97s/it]  Running Inference:   5%|▌         | 10/200 [03:35<1:03:18, 19.99s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:58<1:06:20, 21.06s/it]Running Inference:   6%|▌         | 12/200 [04:21<1:07:38, 21.59s/it]Running Inference:   6%|▋         | 13/200 [04:45<1:09:30, 22.30s/it]Running Inference:   7%|▋         | 14/200 [05:02<1:03:54, 20.62s/it]Running Inference:   8%|▊         | 15/200 [05:24<1:05:38, 21.29s/it]Running Inference:   8%|▊         | 16/200 [05:49<1:08:00, 22.18s/it]Running Inference:   8%|▊         | 17/200 [06:11<1:08:14, 22.38s/it]Running Inference:   9%|▉         | 18/200 [06:37<1:10:31, 23.25s/it]Running Inference:  10%|▉         | 19/200 [07:00<1:10:14, 23.28s/it]Running Inference:  10%|█         | 20/200 [07:24<1:10:45, 23.58s/it]Running Inference:  10%|█         | 21/200 [07:48<1:10:29, 23.63s/it]Running Inference:  11%|█         | 22/200 [08:12<1:10:03, 23.62s/it]Running Inference:  12%|█▏        | 23/200 [08:27<1:02:11, 21.08s/it]Running Inference:  12%|█▏        | 24/200 [08:52<1:05:36, 22.37s/it]Running Inference:  12%|█▎        | 25/200 [09:15<1:05:59, 22.63s/it]Running Inference:  13%|█▎        | 26/200 [09:40<1:07:19, 23.21s/it]Running Inference:  14%|█▎        | 27/200 [10:03<1:07:06, 23.28s/it]Running Inference:  14%|█▍        | 28/200 [10:27<1:06:45, 23.29s/it]Running Inference:  14%|█▍        | 29/200 [10:50<1:06:36, 23.37s/it]Running Inference:  15%|█▌        | 30/200 [11:08<1:01:02, 21.55s/it]Running Inference:  16%|█▌        | 31/200 [11:31<1:01:59, 22.01s/it]Running Inference:  16%|█▌        | 32/200 [11:39<50:27, 18.02s/it]  Running Inference:  16%|█▋        | 33/200 [11:55<48:02, 17.26s/it]Running Inference:  17%|█▋        | 34/200 [12:18<52:13, 18.88s/it]Running Inference:  18%|█▊        | 35/200 [12:41<55:52, 20.32s/it]Running Inference:  18%|█▊        | 36/200 [13:04<57:49, 21.16s/it]Running Inference:  18%|█▊        | 37/200 [13:27<59:00, 21.72s/it]Running Inference:  19%|█▉        | 38/200 [13:52<1:00:58, 22.58s/it]Running Inference:  20%|█▉        | 39/200 [14:16<1:01:24, 22.88s/it]Running Inference:  20%|██        | 40/200 [14:40<1:02:37, 23.48s/it]Running Inference:  20%|██        | 41/200 [15:08<1:05:04, 24.56s/it]Running Inference:  21%|██        | 42/200 [15:31<1:03:47, 24.23s/it]Running Inference:  22%|██▏       | 43/200 [15:55<1:02:57, 24.06s/it]Running Inference:  22%|██▏       | 44/200 [16:19<1:03:09, 24.29s/it]Running Inference:  22%|██▎       | 45/200 [16:43<1:01:58, 23.99s/it]Running Inference:  23%|██▎       | 46/200 [17:00<56:07, 21.86s/it]  Running Inference:  24%|██▎       | 47/200 [17:24<57:45, 22.65s/it]Running Inference:  24%|██▍       | 48/200 [17:47<57:45, 22.80s/it]Running Inference:  24%|██▍       | 49/200 [18:10<57:31, 22.86s/it]Running Inference:  25%|██▌       | 50/200 [18:33<57:12, 22.88s/it]Running Inference:  26%|██▌       | 51/200 [18:57<57:37, 23.21s/it]Running Inference:  26%|██▌       | 52/200 [19:21<57:45, 23.41s/it]Running Inference:  26%|██▋       | 53/200 [19:44<57:14, 23.37s/it]Running Inference:  27%|██▋       | 54/200 [20:08<56:46, 23.33s/it]Running Inference:  28%|██▊       | 55/200 [20:31<56:14, 23.28s/it]Running Inference:  28%|██▊       | 56/200 [20:54<55:45, 23.24s/it]Running Inference:  28%|██▊       | 57/200 [21:17<55:02, 23.09s/it]Running Inference:  29%|██▉       | 58/200 [21:40<55:05, 23.28s/it]Running Inference:  30%|██▉       | 59/200 [22:04<55:01, 23.42s/it]Running Inference:  30%|███       | 60/200 [22:28<55:07, 23.63s/it]Running Inference:  30%|███       | 61/200 [22:51<54:17, 23.43s/it]Running Inference:  31%|███       | 62/200 [23:07<48:53, 21.25s/it]Running Inference:  32%|███▏      | 63/200 [23:31<50:01, 21.91s/it]Running Inference:  32%|███▏      | 64/200 [23:54<50:49, 22.42s/it]Running Inference:  32%|███▎      | 65/200 [24:18<51:18, 22.80s/it]Running Inference:  33%|███▎      | 66/200 [24:37<48:18, 21.63s/it]Running Inference:  34%|███▎      | 67/200 [25:00<49:10, 22.19s/it]Running Inference:  34%|███▍      | 68/200 [25:24<49:41, 22.58s/it]Running Inference:  34%|███▍      | 69/200 [25:48<50:33, 23.15s/it]Running Inference:  35%|███▌      | 70/200 [26:10<48:52, 22.56s/it]Running Inference:  36%|███▌      | 71/200 [26:33<48:48, 22.70s/it]Running Inference:  36%|███▌      | 72/200 [26:57<49:15, 23.09s/it]Running Inference:  36%|███▋      | 73/200 [27:20<49:13, 23.25s/it]Running Inference:  37%|███▋      | 74/200 [27:40<46:19, 22.06s/it]Running Inference:  38%|███▊      | 75/200 [28:03<46:41, 22.41s/it]Running Inference:  38%|███▊      | 76/200 [28:26<46:41, 22.60s/it]Running Inference:  38%|███▊      | 77/200 [28:50<47:32, 23.19s/it]Running Inference:  39%|███▉      | 78/200 [29:08<43:43, 21.51s/it]Running Inference:  40%|███▉      | 79/200 [29:31<44:29, 22.06s/it]Running Inference:  40%|████      | 80/200 [29:54<44:44, 22.37s/it]Running Inference:  40%|████      | 81/200 [30:18<44:59, 22.68s/it]Running Inference:  41%|████      | 82/200 [30:41<44:57, 22.86s/it]Running Inference:  42%|████▏     | 83/200 [31:04<44:46, 22.96s/it]Running Inference:  42%|████▏     | 84/200 [31:28<45:01, 23.29s/it]Running Inference:  42%|████▎     | 85/200 [31:52<44:48, 23.38s/it]Running Inference:  43%|████▎     | 86/200 [32:16<44:46, 23.56s/it]Running Inference:  44%|████▎     | 87/200 [32:39<44:12, 23.48s/it]Running Inference:  44%|████▍     | 88/200 [33:03<43:43, 23.42s/it]Running Inference:  44%|████▍     | 89/200 [33:25<42:52, 23.17s/it]Running Inference:  45%|████▌     | 90/200 [33:44<39:53, 21.76s/it]Running Inference:  46%|████▌     | 91/200 [34:08<40:47, 22.45s/it]Running Inference:  46%|████▌     | 92/200 [34:30<40:21, 22.43s/it]Running Inference:  46%|████▋     | 93/200 [34:54<40:58, 22.98s/it]Running Inference:  47%|████▋     | 94/200 [35:19<41:24, 23.43s/it]Running Inference:  48%|████▊     | 95/200 [35:44<41:42, 23.83s/it]Running Inference:  48%|████▊     | 96/200 [36:07<40:51, 23.57s/it]Running Inference:  48%|████▊     | 97/200 [36:29<40:09, 23.39s/it]Running Inference:  49%|████▉     | 98/200 [36:53<39:52, 23.46s/it]Running Inference:  50%|████▉     | 99/200 [37:13<37:37, 22.35s/it]Running Inference:  50%|█████     | 100/200 [37:37<37:58, 22.78s/it]Running Inference:  50%|█████     | 101/200 [38:02<38:36, 23.40s/it]Running Inference:  51%|█████     | 102/200 [38:29<40:00, 24.49s/it]Running Inference:  52%|█████▏    | 103/200 [38:53<39:34, 24.48s/it]Running Inference:  52%|█████▏    | 104/200 [39:16<38:40, 24.17s/it]Running Inference:  52%|█████▎    | 105/200 [39:43<39:17, 24.81s/it]Running Inference:  53%|█████▎    | 106/200 [39:57<33:59, 21.70s/it]Running Inference:  54%|█████▎    | 107/200 [40:13<31:02, 20.02s/it]Running Inference:  54%|█████▍    | 108/200 [40:37<32:19, 21.08s/it]Running Inference:  55%|█████▍    | 109/200 [41:00<32:49, 21.64s/it]Running Inference:  55%|█████▌    | 110/200 [41:23<33:20, 22.23s/it]Running Inference:  56%|█████▌    | 111/200 [41:39<30:11, 20.36s/it]Running Inference:  56%|█████▌    | 112/200 [42:03<31:15, 21.31s/it]Running Inference:  56%|█████▋    | 113/200 [42:26<31:44, 21.89s/it]Running Inference:  57%|█████▋    | 114/200 [42:49<31:54, 22.26s/it]Running Inference:  57%|█████▊    | 115/200 [43:13<32:06, 22.66s/it]Running Inference:  58%|█████▊    | 116/200 [43:36<31:59, 22.85s/it]Running Inference:  58%|█████▊    | 117/200 [44:00<31:48, 22.99s/it]Running Inference:  59%|█████▉    | 118/200 [44:14<27:52, 20.39s/it]Running Inference:  60%|█████▉    | 119/200 [44:38<28:53, 21.41s/it]Running Inference:  60%|██████    | 120/200 [45:02<29:40, 22.26s/it]Running Inference:  60%|██████    | 121/200 [45:26<30:03, 22.83s/it]Running Inference:  61%|██████    | 122/200 [45:49<29:34, 22.75s/it]Running Inference:  62%|██████▏   | 123/200 [46:13<29:54, 23.30s/it]Running Inference:  62%|██████▏   | 124/200 [46:38<29:54, 23.61s/it]Running Inference:  62%|██████▎   | 125/200 [47:01<29:17, 23.43s/it]Running Inference:  63%|██████▎   | 126/200 [47:24<28:54, 23.44s/it]Running Inference:  64%|██████▎   | 127/200 [47:48<28:51, 23.71s/it]Running Inference:  64%|██████▍   | 128/200 [48:12<28:24, 23.67s/it]Running Inference:  64%|██████▍   | 129/200 [48:35<27:57, 23.63s/it]Running Inference:  65%|██████▌   | 130/200 [48:58<27:12, 23.32s/it]Running Inference:  66%|██████▌   | 131/200 [49:21<26:50, 23.34s/it]Running Inference:  66%|██████▌   | 132/200 [49:45<26:22, 23.27s/it]Running Inference:  66%|██████▋   | 133/200 [50:08<26:10, 23.44s/it]Running Inference:  67%|██████▋   | 134/200 [50:31<25:36, 23.27s/it]Running Inference:  68%|██████▊   | 135/200 [50:54<25:10, 23.24s/it]Running Inference:  68%|██████▊   | 136/200 [51:17<24:36, 23.08s/it]Running Inference:  68%|██████▊   | 137/200 [51:40<24:15, 23.11s/it]Running Inference:  69%|██████▉   | 138/200 [51:52<20:13, 19.57s/it]Running Inference:  70%|██████▉   | 139/200 [52:16<21:17, 20.95s/it]Running Inference:  70%|███████   | 140/200 [52:40<21:54, 21.91s/it]Running Inference:  70%|███████   | 141/200 [53:03<21:56, 22.32s/it]Running Inference:  71%|███████   | 142/200 [53:26<21:43, 22.47s/it]Running Inference:  72%|███████▏  | 143/200 [53:50<21:44, 22.89s/it]Running Inference:  72%|███████▏  | 144/200 [54:13<21:27, 22.99s/it]Running Inference:  72%|███████▎  | 145/200 [54:38<21:36, 23.57s/it]Running Inference:  73%|███████▎  | 146/200 [55:00<20:48, 23.13s/it]Running Inference:  74%|███████▎  | 147/200 [55:23<20:22, 23.07s/it]Running Inference:  74%|███████▍  | 148/200 [55:46<19:59, 23.07s/it]Running Inference:  74%|███████▍  | 149/200 [56:10<19:44, 23.23s/it]Running Inference:  75%|███████▌  | 150/200 [56:34<19:39, 23.59s/it]Running Inference:  76%|███████▌  | 151/200 [56:58<19:26, 23.80s/it]Running Inference:  76%|███████▌  | 152/200 [57:23<19:10, 23.96s/it]Running Inference:  76%|███████▋  | 153/200 [57:46<18:35, 23.74s/it]Running Inference:  77%|███████▋  | 154/200 [57:58<15:34, 20.31s/it]Running Inference:  78%|███████▊  | 155/200 [58:21<15:51, 21.14s/it]Running Inference:  78%|███████▊  | 156/200 [58:44<15:52, 21.65s/it]Running Inference:  78%|███████▊  | 157/200 [59:07<15:49, 22.09s/it]Running Inference:  79%|███████▉  | 158/200 [59:31<15:44, 22.49s/it]Running Inference:  80%|███████▉  | 159/200 [59:53<15:22, 22.49s/it]Running Inference:  80%|████████  | 160/200 [1:00:18<15:26, 23.17s/it]Running Inference:  80%|████████  | 161/200 [1:00:36<13:59, 21.52s/it]Running Inference:  81%|████████  | 162/200 [1:01:00<14:09, 22.34s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:26<14:24, 23.35s/it]Running Inference:  82%|████████▏ | 164/200 [1:01:49<14:00, 23.34s/it]Running Inference:  82%|████████▎ | 165/200 [1:02:14<13:49, 23.71s/it]Running Inference:  83%|████████▎ | 166/200 [1:02:36<13:17, 23.45s/it]Running Inference:  84%|████████▎ | 167/200 [1:02:55<12:08, 22.07s/it]Running Inference:  84%|████████▍ | 168/200 [1:03:19<11:57, 22.44s/it]Running Inference:  84%|████████▍ | 169/200 [1:03:42<11:41, 22.62s/it]Running Inference:  85%|████████▌ | 170/200 [1:04:05<11:24, 22.81s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:28<11:00, 22.79s/it]Running Inference:  86%|████████▌ | 172/200 [1:04:51<10:43, 22.99s/it]Running Inference:  86%|████████▋ | 173/200 [1:05:16<10:33, 23.48s/it]Running Inference:  87%|████████▋ | 174/200 [1:05:39<10:08, 23.40s/it]Running Inference:  88%|████████▊ | 175/200 [1:06:02<09:44, 23.38s/it]Running Inference:  88%|████████▊ | 176/200 [1:06:19<08:35, 21.49s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:42<08:23, 21.89s/it]Running Inference:  89%|████████▉ | 178/200 [1:07:02<07:50, 21.41s/it]Running Inference:  90%|████████▉ | 179/200 [1:07:26<07:45, 22.15s/it]Running Inference:  90%|█████████ | 180/200 [1:07:49<07:26, 22.33s/it]Running Inference:  90%|█████████ | 181/200 [1:07:52<05:11, 16.38s/it]Running Inference:  91%|█████████ | 182/200 [1:08:15<05:33, 18.54s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:38<05:38, 19.90s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:57<05:11, 19.48s/it]Running Inference:  92%|█████████▎| 185/200 [1:09:19<05:06, 20.44s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:37<04:35, 19.68s/it]Running Inference:  94%|█████████▎| 187/200 [1:10:00<04:28, 20.64s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:24<04:18, 21.51s/it]Running Inference:  94%|█████████▍| 189/200 [1:10:46<04:00, 21.87s/it]Running Inference:  95%|█████████▌| 190/200 [1:11:05<03:29, 20.92s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:28<03:13, 21.47s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:30<02:06, 15.77s/it]Running Inference:  96%|█████████▋| 193/200 [1:11:51<02:00, 17.24s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:15<01:54, 19.15s/it]Running Inference:  98%|█████████▊| 195/200 [1:12:40<01:44, 21.00s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:03<01:26, 21.74s/it]Running Inference:  98%|█████████▊| 197/200 [1:13:27<01:06, 22.30s/it]Running Inference:  99%|█████████▉| 198/200 [1:13:50<00:45, 22.52s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:13<00:22, 22.69s/it]Running Inference: 100%|██████████| 200/200 [1:14:33<00:00, 21.91s/it]Running Inference: 100%|██████████| 200/200 [1:14:33<00:00, 22.37s/it]
2025-12-14 12:49:21,980 - INFO - Inference completed.
2025-12-14 12:49:21,993 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 12:49:21,993 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.667 seconds.
Prefix dict has been built successfully.
2025-12-14 12:49:28,816 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 12:49:28,816 - INFO - Metrics:
11.85
2025-12-14 12:49:28,817 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: tova (task=vcsum, ratio=0.2) on GPU 1

----------------------------------------
Task: vcsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 12:49:35,790 - INFO - Set deterministic seeds to 42
2025-12-14 12:49:35,790 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 12:49:35,790 - INFO - Starting evaluation run...
2025-12-14 12:49:35,790 - INFO - Output directory set to: longbenchresult
2025-12-14 12:49:35,790 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 12:49:35,790 - INFO - KV Press 'tova' setup.
2025-12-14 12:49:35,790 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 12:49:35,790 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.88it/s]
Device set to use cuda:0
2025-12-14 12:49:49,223 - INFO - Model pipeline loaded.
2025-12-14 12:49:49,223 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 12:49:53,726 - INFO - Dataset loaded with 200 entries.
2025-12-14 12:49:53,726 - INFO - Dataset processed with 200 entries.
2025-12-14 12:49:53,750 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:31, 23.98s/it]Running Inference:   1%|          | 2/200 [00:47<1:18:56, 23.92s/it]Running Inference:   2%|▏         | 3/200 [01:11<1:18:08, 23.80s/it]Running Inference:   2%|▏         | 4/200 [01:34<1:17:04, 23.59s/it]Running Inference:   2%|▎         | 5/200 [01:58<1:17:02, 23.70s/it]Running Inference:   3%|▎         | 6/200 [02:21<1:15:43, 23.42s/it]Running Inference:   4%|▎         | 7/200 [02:45<1:15:28, 23.46s/it]Running Inference:   4%|▍         | 8/200 [03:07<1:14:25, 23.26s/it]Running Inference:   4%|▍         | 9/200 [03:23<1:06:31, 20.90s/it]Running Inference:   5%|▌         | 10/200 [03:48<1:09:45, 22.03s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:11<1:10:50, 22.49s/it]Running Inference:   6%|▌         | 12/200 [04:24<1:01:21, 19.58s/it]Running Inference:   6%|▋         | 13/200 [04:48<1:05:11, 20.92s/it]Running Inference:   7%|▋         | 14/200 [05:04<1:00:08, 19.40s/it]Running Inference:   8%|▊         | 15/200 [05:27<1:03:03, 20.45s/it]Running Inference:   8%|▊         | 16/200 [05:51<1:06:14, 21.60s/it]Running Inference:   8%|▊         | 17/200 [06:08<1:01:28, 20.16s/it]Running Inference:   9%|▉         | 18/200 [06:33<1:05:34, 21.62s/it]Running Inference:  10%|▉         | 19/200 [06:48<59:11, 19.62s/it]  Running Inference:  10%|█         | 20/200 [07:12<1:03:04, 21.02s/it]Running Inference:  10%|█         | 21/200 [07:36<1:05:11, 21.85s/it]Running Inference:  11%|█         | 22/200 [08:00<1:06:23, 22.38s/it]Running Inference:  12%|█▏        | 23/200 [08:17<1:01:28, 20.84s/it]Running Inference:  12%|█▏        | 24/200 [08:42<1:04:54, 22.13s/it]Running Inference:  12%|█▎        | 25/200 [09:05<1:05:33, 22.48s/it]Running Inference:  13%|█▎        | 26/200 [09:30<1:07:04, 23.13s/it]Running Inference:  14%|█▎        | 27/200 [09:54<1:07:02, 23.25s/it]Running Inference:  14%|█▍        | 28/200 [10:17<1:06:47, 23.30s/it]Running Inference:  14%|█▍        | 29/200 [10:41<1:06:43, 23.42s/it]Running Inference:  15%|█▌        | 30/200 [11:00<1:02:43, 22.14s/it]Running Inference:  16%|█▌        | 31/200 [11:23<1:03:11, 22.44s/it]Running Inference:  16%|█▌        | 32/200 [11:33<52:08, 18.62s/it]  Running Inference:  16%|█▋        | 33/200 [11:55<55:15, 19.85s/it]Running Inference:  17%|█▋        | 34/200 [12:18<57:20, 20.73s/it]Running Inference:  18%|█▊        | 35/200 [12:42<59:26, 21.61s/it]Running Inference:  18%|█▊        | 36/200 [12:59<55:46, 20.41s/it]Running Inference:  18%|█▊        | 37/200 [13:23<57:37, 21.21s/it]Running Inference:  19%|█▉        | 38/200 [13:47<1:00:04, 22.25s/it]Running Inference:  20%|█▉        | 39/200 [14:11<1:00:51, 22.68s/it]Running Inference:  20%|██        | 40/200 [14:36<1:02:11, 23.32s/it]Running Inference:  20%|██        | 41/200 [15:02<1:04:31, 24.35s/it]Running Inference:  21%|██        | 42/200 [15:26<1:03:26, 24.09s/it]Running Inference:  22%|██▏       | 43/200 [15:50<1:02:44, 23.98s/it]Running Inference:  22%|██▏       | 44/200 [16:14<1:02:59, 24.23s/it]Running Inference:  22%|██▎       | 45/200 [16:38<1:01:55, 23.97s/it]Running Inference:  23%|██▎       | 46/200 [16:55<56:02, 21.83s/it]  Running Inference:  24%|██▎       | 47/200 [17:19<57:45, 22.65s/it]Running Inference:  24%|██▍       | 48/200 [17:42<57:48, 22.82s/it]Running Inference:  24%|██▍       | 49/200 [18:05<57:36, 22.89s/it]Running Inference:  25%|██▌       | 50/200 [18:29<57:19, 22.93s/it]Running Inference:  26%|██▌       | 51/200 [18:53<57:46, 23.26s/it]Running Inference:  26%|██▌       | 52/200 [19:16<57:51, 23.46s/it]Running Inference:  26%|██▋       | 53/200 [19:40<57:24, 23.43s/it]Running Inference:  27%|██▋       | 54/200 [20:03<56:50, 23.36s/it]Running Inference:  28%|██▊       | 55/200 [20:26<56:21, 23.32s/it]Running Inference:  28%|██▊       | 56/200 [20:47<54:00, 22.50s/it]Running Inference:  28%|██▊       | 57/200 [21:10<53:50, 22.59s/it]Running Inference:  29%|██▉       | 58/200 [21:33<54:20, 22.96s/it]Running Inference:  30%|██▉       | 59/200 [21:57<54:33, 23.21s/it]Running Inference:  30%|███       | 60/200 [22:21<54:50, 23.50s/it]Running Inference:  30%|███       | 61/200 [22:45<54:09, 23.37s/it]Running Inference:  31%|███       | 62/200 [23:08<53:29, 23.26s/it]Running Inference:  32%|███▏      | 63/200 [23:31<53:15, 23.32s/it]Running Inference:  32%|███▏      | 64/200 [23:55<53:07, 23.44s/it]Running Inference:  32%|███▎      | 65/200 [24:19<52:59, 23.55s/it]Running Inference:  33%|███▎      | 66/200 [24:41<52:12, 23.38s/it]Running Inference:  34%|███▎      | 67/200 [25:05<51:55, 23.43s/it]Running Inference:  34%|███▍      | 68/200 [25:26<50:04, 22.76s/it]Running Inference:  34%|███▍      | 69/200 [25:51<50:55, 23.33s/it]Running Inference:  35%|███▌      | 70/200 [26:07<45:45, 21.12s/it]Running Inference:  36%|███▌      | 71/200 [26:30<46:40, 21.71s/it]Running Inference:  36%|███▌      | 72/200 [26:54<47:49, 22.42s/it]Running Inference:  36%|███▋      | 73/200 [27:18<48:15, 22.80s/it]Running Inference:  37%|███▋      | 74/200 [27:37<45:30, 21.67s/it]Running Inference:  38%|███▊      | 75/200 [28:00<46:10, 22.16s/it]Running Inference:  38%|███▊      | 76/200 [28:20<44:15, 21.42s/it]Running Inference:  38%|███▊      | 77/200 [28:44<45:52, 22.38s/it]Running Inference:  39%|███▉      | 78/200 [29:07<45:49, 22.54s/it]Running Inference:  40%|███▉      | 79/200 [29:31<45:57, 22.79s/it]Running Inference:  40%|████      | 80/200 [29:54<45:47, 22.89s/it]Running Inference:  40%|████      | 81/200 [30:17<45:45, 23.07s/it]Running Inference:  41%|████      | 82/200 [30:41<45:30, 23.14s/it]Running Inference:  42%|████▏     | 83/200 [31:04<45:10, 23.17s/it]Running Inference:  42%|████▏     | 84/200 [31:28<45:20, 23.45s/it]Running Inference:  42%|████▎     | 85/200 [31:52<45:04, 23.52s/it]Running Inference:  43%|████▎     | 86/200 [32:16<44:59, 23.68s/it]Running Inference:  44%|████▎     | 87/200 [32:39<44:24, 23.58s/it]Running Inference:  44%|████▍     | 88/200 [33:02<43:53, 23.51s/it]Running Inference:  44%|████▍     | 89/200 [33:25<42:59, 23.24s/it]Running Inference:  45%|████▌     | 90/200 [33:39<37:23, 20.40s/it]Running Inference:  46%|████▌     | 91/200 [34:03<39:05, 21.52s/it]Running Inference:  46%|████▌     | 92/200 [34:25<38:53, 21.61s/it]Running Inference:  46%|████▋     | 93/200 [34:49<39:59, 22.43s/it]Running Inference:  47%|████▋     | 94/200 [35:14<40:45, 23.07s/it]Running Inference:  48%|████▊     | 95/200 [35:38<41:14, 23.57s/it]Running Inference:  48%|████▊     | 96/200 [36:01<40:12, 23.19s/it]Running Inference:  48%|████▊     | 97/200 [36:24<39:42, 23.14s/it]Running Inference:  49%|████▉     | 98/200 [36:47<39:37, 23.31s/it]Running Inference:  50%|████▉     | 99/200 [37:05<36:31, 21.70s/it]Running Inference:  50%|█████     | 100/200 [37:29<37:12, 22.33s/it]Running Inference:  50%|█████     | 101/200 [37:54<38:03, 23.07s/it]Running Inference:  51%|█████     | 102/200 [38:21<39:26, 24.15s/it]Running Inference:  52%|█████▏    | 103/200 [38:45<39:13, 24.27s/it]Running Inference:  52%|█████▏    | 104/200 [39:09<38:28, 24.04s/it]Running Inference:  52%|█████▎    | 105/200 [39:35<38:59, 24.63s/it]Running Inference:  53%|█████▎    | 106/200 [39:48<33:20, 21.28s/it]Running Inference:  54%|█████▎    | 107/200 [40:06<31:13, 20.15s/it]Running Inference:  54%|█████▍    | 108/200 [40:29<32:30, 21.20s/it]Running Inference:  55%|█████▍    | 109/200 [40:52<32:58, 21.74s/it]Running Inference:  55%|█████▌    | 110/200 [41:16<33:28, 22.32s/it]Running Inference:  56%|█████▌    | 111/200 [41:34<31:07, 20.98s/it]Running Inference:  56%|█████▌    | 112/200 [41:57<31:54, 21.76s/it]Running Inference:  56%|█████▋    | 113/200 [42:16<30:19, 20.92s/it]Running Inference:  57%|█████▋    | 114/200 [42:39<30:58, 21.61s/it]Running Inference:  57%|█████▊    | 115/200 [43:03<31:29, 22.23s/it]Running Inference:  58%|█████▊    | 116/200 [43:26<31:35, 22.57s/it]Running Inference:  58%|█████▊    | 117/200 [43:50<31:33, 22.82s/it]Running Inference:  59%|█████▉    | 118/200 [44:13<31:07, 22.77s/it]Running Inference:  60%|█████▉    | 119/200 [44:36<31:10, 23.10s/it]Running Inference:  60%|██████    | 120/200 [45:01<31:21, 23.52s/it]Running Inference:  60%|██████    | 121/200 [45:25<31:15, 23.74s/it]Running Inference:  61%|██████    | 122/200 [45:48<30:25, 23.41s/it]Running Inference:  62%|██████▏   | 123/200 [46:12<30:30, 23.78s/it]Running Inference:  62%|██████▏   | 124/200 [46:37<30:21, 23.97s/it]Running Inference:  62%|██████▎   | 125/200 [47:00<29:41, 23.75s/it]Running Inference:  63%|██████▎   | 126/200 [47:24<29:12, 23.69s/it]Running Inference:  64%|██████▎   | 127/200 [47:48<29:05, 23.91s/it]Running Inference:  64%|██████▍   | 128/200 [48:12<28:36, 23.84s/it]Running Inference:  64%|██████▍   | 129/200 [48:35<28:07, 23.76s/it]Running Inference:  65%|██████▌   | 130/200 [48:58<27:20, 23.44s/it]Running Inference:  66%|██████▌   | 131/200 [49:21<26:57, 23.45s/it]Running Inference:  66%|██████▌   | 132/200 [49:45<26:29, 23.37s/it]Running Inference:  66%|██████▋   | 133/200 [50:09<26:15, 23.52s/it]Running Inference:  67%|██████▋   | 134/200 [50:32<25:41, 23.35s/it]Running Inference:  68%|██████▊   | 135/200 [50:55<25:16, 23.33s/it]Running Inference:  68%|██████▊   | 136/200 [51:18<24:42, 23.16s/it]Running Inference:  68%|██████▊   | 137/200 [51:41<24:21, 23.19s/it]Running Inference:  69%|██████▉   | 138/200 [51:49<19:11, 18.57s/it]Running Inference:  70%|██████▉   | 139/200 [52:13<20:37, 20.29s/it]Running Inference:  70%|███████   | 140/200 [52:37<21:28, 21.48s/it]Running Inference:  70%|███████   | 141/200 [53:01<21:40, 22.04s/it]Running Inference:  71%|███████   | 142/200 [53:17<19:43, 20.40s/it]Running Inference:  72%|███████▏  | 143/200 [53:41<20:23, 21.46s/it]Running Inference:  72%|███████▏  | 144/200 [54:04<20:32, 22.01s/it]Running Inference:  72%|███████▎  | 145/200 [54:29<20:56, 22.85s/it]Running Inference:  73%|███████▎  | 146/200 [54:52<20:41, 22.98s/it]Running Inference:  74%|███████▎  | 147/200 [55:15<20:18, 22.99s/it]Running Inference:  74%|███████▍  | 148/200 [55:34<18:50, 21.74s/it]Running Inference:  74%|███████▍  | 149/200 [55:58<18:58, 22.33s/it]Running Inference:  75%|███████▌  | 150/200 [56:22<19:09, 22.99s/it]Running Inference:  76%|███████▌  | 151/200 [56:47<19:06, 23.40s/it]Running Inference:  76%|███████▌  | 152/200 [57:11<18:57, 23.71s/it]Running Inference:  76%|███████▋  | 153/200 [57:35<18:29, 23.60s/it]Running Inference:  77%|███████▋  | 154/200 [57:46<15:14, 19.88s/it]Running Inference:  78%|███████▊  | 155/200 [58:09<15:38, 20.86s/it]Running Inference:  78%|███████▊  | 156/200 [58:32<15:44, 21.47s/it]Running Inference:  78%|███████▊  | 157/200 [58:55<15:45, 21.98s/it]Running Inference:  79%|███████▉  | 158/200 [59:19<15:42, 22.44s/it]Running Inference:  80%|███████▉  | 159/200 [59:41<15:23, 22.52s/it]Running Inference:  80%|████████  | 160/200 [1:00:06<15:27, 23.19s/it]Running Inference:  80%|████████  | 161/200 [1:00:29<15:03, 23.17s/it]Running Inference:  81%|████████  | 162/200 [1:00:54<14:54, 23.54s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:19<14:52, 24.11s/it]Running Inference:  82%|████████▏ | 164/200 [1:01:42<14:20, 23.90s/it]Running Inference:  82%|████████▎ | 165/200 [1:02:07<14:04, 24.13s/it]Running Inference:  83%|████████▎ | 166/200 [1:02:30<13:27, 23.75s/it]Running Inference:  84%|████████▎ | 167/200 [1:02:49<12:16, 22.33s/it]Running Inference:  84%|████████▍ | 168/200 [1:03:04<10:47, 20.23s/it]Running Inference:  84%|████████▍ | 169/200 [1:03:27<10:53, 21.09s/it]Running Inference:  85%|████████▌ | 170/200 [1:03:51<10:53, 21.78s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:14<10:40, 22.08s/it]Running Inference:  86%|████████▌ | 172/200 [1:04:37<10:30, 22.52s/it]Running Inference:  86%|████████▋ | 173/200 [1:05:02<10:25, 23.17s/it]Running Inference:  87%|████████▋ | 174/200 [1:05:25<10:03, 23.20s/it]Running Inference:  88%|████████▊ | 175/200 [1:05:48<09:41, 23.26s/it]Running Inference:  88%|████████▊ | 176/200 [1:06:08<08:48, 22.01s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:30<08:32, 22.27s/it]Running Inference:  89%|████████▉ | 178/200 [1:06:53<08:13, 22.43s/it]Running Inference:  90%|████████▉ | 179/200 [1:07:17<08:00, 22.87s/it]Running Inference:  90%|█████████ | 180/200 [1:07:37<07:21, 22.10s/it]Running Inference:  90%|█████████ | 181/200 [1:07:40<05:08, 16.21s/it]Running Inference:  91%|█████████ | 182/200 [1:08:04<05:32, 18.45s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:27<05:37, 19.86s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:45<05:11, 19.46s/it]Running Inference:  92%|█████████▎| 185/200 [1:09:08<05:06, 20.44s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:31<04:57, 21.26s/it]Running Inference:  94%|█████████▎| 187/200 [1:09:54<04:43, 21.77s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:18<04:27, 22.33s/it]Running Inference:  94%|█████████▍| 189/200 [1:10:41<04:07, 22.48s/it]Running Inference:  95%|█████████▌| 190/200 [1:10:59<03:32, 21.26s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:22<03:15, 21.73s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:45<02:57, 22.13s/it]Running Inference:  96%|█████████▋| 193/200 [1:12:06<02:32, 21.72s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:29<02:13, 22.32s/it]Running Inference:  98%|█████████▊| 195/200 [1:12:54<01:55, 23.15s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:18<01:33, 23.27s/it]Running Inference:  98%|█████████▊| 197/200 [1:13:42<01:10, 23.38s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:05<00:46, 23.30s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:28<00:23, 23.26s/it]Running Inference: 100%|██████████| 200/200 [1:14:51<00:00, 23.29s/it]Running Inference: 100%|██████████| 200/200 [1:14:51<00:00, 22.46s/it]
2025-12-14 14:04:45,522 - INFO - Inference completed.
2025-12-14 14:04:45,534 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 14:04:45,535 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.662 seconds.
Prefix dict has been built successfully.
2025-12-14 14:04:52,353 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 14:04:52,353 - INFO - Metrics:
11.62
2025-12-14 14:04:52,354 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: tova (task=vcsum, ratio=0.3) on GPU 1

----------------------------------------
Task: vcsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 14:04:59,440 - INFO - Set deterministic seeds to 42
2025-12-14 14:04:59,440 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 14:04:59,440 - INFO - Starting evaluation run...
2025-12-14 14:04:59,440 - INFO - Output directory set to: longbenchresult
2025-12-14 14:04:59,440 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 14:04:59,440 - INFO - KV Press 'tova' setup.
2025-12-14 14:04:59,440 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 14:04:59,440 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.47it/s]
Device set to use cuda:0
2025-12-14 14:05:11,584 - INFO - Model pipeline loaded.
2025-12-14 14:05:11,584 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 14:05:15,657 - INFO - Dataset loaded with 200 entries.
2025-12-14 14:05:15,657 - INFO - Dataset processed with 200 entries.
2025-12-14 14:05:15,682 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:16<55:50, 16.84s/it]Running Inference:   1%|          | 2/200 [00:40<1:09:05, 20.94s/it]Running Inference:   2%|▏         | 3/200 [01:04<1:12:46, 22.16s/it]Running Inference:   2%|▏         | 4/200 [01:27<1:13:44, 22.58s/it]Running Inference:   2%|▎         | 5/200 [01:51<1:14:51, 23.03s/it]Running Inference:   3%|▎         | 6/200 [02:14<1:14:10, 22.94s/it]Running Inference:   4%|▎         | 7/200 [02:37<1:14:20, 23.11s/it]Running Inference:   4%|▍         | 8/200 [03:00<1:13:34, 22.99s/it]Running Inference:   4%|▍         | 9/200 [03:23<1:13:09, 22.98s/it]Running Inference:   5%|▌         | 10/200 [03:47<1:14:15, 23.45s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:11<1:13:51, 23.45s/it]Running Inference:   6%|▌         | 12/200 [04:18<57:48, 18.45s/it]  Running Inference:   6%|▋         | 13/200 [04:42<1:02:40, 20.11s/it]Running Inference:   7%|▋         | 14/200 [04:57<57:39, 18.60s/it]  Running Inference:   8%|▊         | 15/200 [05:20<1:01:15, 19.87s/it]Running Inference:   8%|▊         | 16/200 [05:44<1:04:56, 21.17s/it]Running Inference:   8%|▊         | 17/200 [06:07<1:06:04, 21.66s/it]Running Inference:   9%|▉         | 18/200 [06:32<1:08:44, 22.66s/it]Running Inference:  10%|▉         | 19/200 [06:55<1:08:58, 22.87s/it]Running Inference:  10%|█         | 20/200 [07:19<1:09:50, 23.28s/it]Running Inference:  10%|█         | 21/200 [07:43<1:09:51, 23.41s/it]Running Inference:  11%|█         | 22/200 [08:06<1:09:35, 23.46s/it]Running Inference:  12%|█▏        | 23/200 [08:29<1:08:50, 23.34s/it]Running Inference:  12%|█▏        | 24/200 [08:55<1:09:59, 23.86s/it]Running Inference:  12%|█▎        | 25/200 [09:18<1:09:01, 23.67s/it]Running Inference:  13%|█▎        | 26/200 [09:42<1:09:25, 23.94s/it]Running Inference:  14%|█▎        | 27/200 [10:06<1:08:38, 23.81s/it]Running Inference:  14%|█▍        | 28/200 [10:29<1:07:48, 23.66s/it]Running Inference:  14%|█▍        | 29/200 [10:53<1:07:25, 23.66s/it]Running Inference:  15%|█▌        | 30/200 [11:11<1:02:11, 21.95s/it]Running Inference:  16%|█▌        | 31/200 [11:34<1:02:46, 22.28s/it]Running Inference:  16%|█▌        | 32/200 [11:46<54:12, 19.36s/it]  Running Inference:  16%|█▋        | 33/200 [12:09<56:40, 20.36s/it]Running Inference:  17%|█▋        | 34/200 [12:32<58:16, 21.06s/it]Running Inference:  18%|█▊        | 35/200 [12:55<1:00:03, 21.84s/it]Running Inference:  18%|█▊        | 36/200 [13:19<1:00:46, 22.24s/it]Running Inference:  18%|█▊        | 37/200 [13:42<1:01:02, 22.47s/it]Running Inference:  19%|█▉        | 38/200 [14:06<1:02:24, 23.11s/it]Running Inference:  20%|█▉        | 39/200 [14:30<1:02:25, 23.27s/it]Running Inference:  20%|██        | 40/200 [14:55<1:03:14, 23.72s/it]Running Inference:  20%|██        | 41/200 [15:21<1:04:43, 24.42s/it]Running Inference:  21%|██        | 42/200 [15:44<1:03:29, 24.11s/it]Running Inference:  22%|██▏       | 43/200 [16:08<1:02:45, 23.98s/it]Running Inference:  22%|██▏       | 44/200 [16:32<1:02:57, 24.21s/it]Running Inference:  22%|██▎       | 45/200 [16:56<1:01:50, 23.94s/it]Running Inference:  23%|██▎       | 46/200 [17:10<53:43, 20.93s/it]  Running Inference:  24%|██▎       | 47/200 [17:31<54:00, 21.18s/it]Running Inference:  24%|██▍       | 48/200 [17:55<55:10, 21.78s/it]Running Inference:  24%|██▍       | 49/200 [18:18<55:43, 22.14s/it]Running Inference:  25%|██▌       | 50/200 [18:41<55:58, 22.39s/it]Running Inference:  26%|██▌       | 51/200 [19:05<56:44, 22.85s/it]Running Inference:  26%|██▌       | 52/200 [19:28<57:07, 23.16s/it]Running Inference:  26%|██▋       | 53/200 [19:52<56:48, 23.19s/it]Running Inference:  27%|██▋       | 54/200 [20:15<56:28, 23.21s/it]Running Inference:  28%|██▊       | 55/200 [20:38<56:02, 23.19s/it]Running Inference:  28%|██▊       | 56/200 [21:01<55:37, 23.18s/it]Running Inference:  28%|██▊       | 57/200 [21:24<54:54, 23.04s/it]Running Inference:  29%|██▉       | 58/200 [21:48<55:00, 23.24s/it]Running Inference:  30%|██▉       | 59/200 [22:04<49:37, 21.12s/it]Running Inference:  30%|███       | 60/200 [22:28<51:21, 22.01s/it]Running Inference:  30%|███       | 61/200 [22:51<51:41, 22.31s/it]Running Inference:  31%|███       | 62/200 [23:14<51:44, 22.50s/it]Running Inference:  32%|███▏      | 63/200 [23:37<51:58, 22.77s/it]Running Inference:  32%|███▏      | 64/200 [24:01<52:13, 23.04s/it]Running Inference:  32%|███▎      | 65/200 [24:25<52:17, 23.24s/it]Running Inference:  33%|███▎      | 66/200 [24:38<45:02, 20.17s/it]Running Inference:  34%|███▎      | 67/200 [25:01<46:56, 21.18s/it]Running Inference:  34%|███▍      | 68/200 [25:22<46:19, 21.06s/it]Running Inference:  34%|███▍      | 69/200 [25:46<48:12, 22.08s/it]Running Inference:  35%|███▌      | 70/200 [26:10<48:31, 22.39s/it]Running Inference:  36%|███▌      | 71/200 [26:33<48:32, 22.58s/it]Running Inference:  36%|███▌      | 72/200 [26:57<49:04, 23.00s/it]Running Inference:  36%|███▋      | 73/200 [27:20<49:05, 23.19s/it]Running Inference:  37%|███▋      | 74/200 [27:37<44:48, 21.34s/it]Running Inference:  38%|███▊      | 75/200 [28:00<45:39, 21.92s/it]Running Inference:  38%|███▊      | 76/200 [28:16<41:18, 19.99s/it]Running Inference:  38%|███▊      | 77/200 [28:41<43:47, 21.36s/it]Running Inference:  39%|███▉      | 78/200 [28:57<40:36, 19.97s/it]Running Inference:  40%|███▉      | 79/200 [29:21<42:19, 20.99s/it]Running Inference:  40%|████      | 80/200 [29:44<43:18, 21.65s/it]Running Inference:  40%|████      | 81/200 [30:07<44:00, 22.19s/it]Running Inference:  41%|████      | 82/200 [30:31<44:16, 22.51s/it]Running Inference:  42%|████▏     | 83/200 [30:54<44:17, 22.71s/it]Running Inference:  42%|████▏     | 84/200 [31:18<44:41, 23.12s/it]Running Inference:  42%|████▎     | 85/200 [31:41<44:35, 23.26s/it]Running Inference:  43%|████▎     | 86/200 [32:05<44:36, 23.48s/it]Running Inference:  44%|████▎     | 87/200 [32:29<44:06, 23.42s/it]Running Inference:  44%|████▍     | 88/200 [32:52<43:39, 23.39s/it]Running Inference:  44%|████▍     | 89/200 [33:14<42:48, 23.14s/it]Running Inference:  45%|████▌     | 90/200 [33:38<42:23, 23.12s/it]Running Inference:  46%|████▌     | 91/200 [34:02<42:30, 23.40s/it]Running Inference:  46%|████▌     | 92/200 [34:24<41:44, 23.19s/it]Running Inference:  46%|████▋     | 93/200 [34:49<41:57, 23.52s/it]Running Inference:  47%|████▋     | 94/200 [35:13<42:04, 23.81s/it]Running Inference:  48%|████▊     | 95/200 [35:38<42:07, 24.07s/it]Running Inference:  48%|████▊     | 96/200 [35:59<40:10, 23.18s/it]Running Inference:  48%|████▊     | 97/200 [36:22<39:39, 23.11s/it]Running Inference:  49%|████▉     | 98/200 [36:45<39:33, 23.27s/it]Running Inference:  50%|████▉     | 99/200 [37:09<39:07, 23.25s/it]Running Inference:  50%|█████     | 100/200 [37:32<38:58, 23.39s/it]Running Inference:  50%|█████     | 101/200 [37:57<39:15, 23.79s/it]Running Inference:  51%|█████     | 102/200 [38:23<39:55, 24.44s/it]Running Inference:  52%|█████▏    | 103/200 [38:48<39:32, 24.46s/it]Running Inference:  52%|█████▏    | 104/200 [39:11<38:39, 24.16s/it]Running Inference:  52%|█████▎    | 105/200 [39:37<38:54, 24.58s/it]Running Inference:  53%|█████▎    | 106/200 [40:00<37:56, 24.22s/it]Running Inference:  54%|█████▎    | 107/200 [40:23<36:53, 23.81s/it]Running Inference:  54%|█████▍    | 108/200 [40:46<36:22, 23.72s/it]Running Inference:  55%|█████▍    | 109/200 [41:09<35:38, 23.50s/it]Running Inference:  55%|█████▌    | 110/200 [41:33<35:18, 23.54s/it]Running Inference:  56%|█████▌    | 111/200 [41:56<34:31, 23.28s/it]Running Inference:  56%|█████▌    | 112/200 [42:19<34:14, 23.35s/it]Running Inference:  56%|█████▋    | 113/200 [42:41<33:00, 22.76s/it]Running Inference:  57%|█████▋    | 114/200 [43:04<32:48, 22.88s/it]Running Inference:  57%|█████▊    | 115/200 [43:27<32:44, 23.11s/it]Running Inference:  58%|█████▊    | 116/200 [43:51<32:25, 23.16s/it]Running Inference:  58%|█████▊    | 117/200 [44:14<31:58, 23.11s/it]Running Inference:  59%|█████▉    | 118/200 [44:36<31:24, 22.98s/it]Running Inference:  60%|█████▉    | 119/200 [45:00<31:21, 23.23s/it]Running Inference:  60%|██████    | 120/200 [45:24<31:23, 23.54s/it]Running Inference:  60%|██████    | 121/200 [45:48<31:13, 23.72s/it]Running Inference:  61%|██████    | 122/200 [46:11<30:23, 23.38s/it]Running Inference:  62%|██████▏   | 123/200 [46:36<30:27, 23.74s/it]Running Inference:  62%|██████▏   | 124/200 [47:00<30:22, 23.98s/it]Running Inference:  62%|██████▎   | 125/200 [47:23<29:41, 23.75s/it]Running Inference:  63%|██████▎   | 126/200 [47:47<29:11, 23.67s/it]Running Inference:  64%|██████▎   | 127/200 [48:11<29:07, 23.94s/it]Running Inference:  64%|██████▍   | 128/200 [48:35<28:38, 23.87s/it]Running Inference:  64%|██████▍   | 129/200 [48:59<28:07, 23.77s/it]Running Inference:  65%|██████▌   | 130/200 [49:21<27:21, 23.45s/it]Running Inference:  66%|██████▌   | 131/200 [49:45<26:57, 23.44s/it]Running Inference:  66%|██████▌   | 132/200 [50:08<26:29, 23.37s/it]Running Inference:  66%|██████▋   | 133/200 [50:32<26:16, 23.53s/it]Running Inference:  67%|██████▋   | 134/200 [50:51<24:18, 22.10s/it]Running Inference:  68%|██████▊   | 135/200 [51:14<24:19, 22.45s/it]Running Inference:  68%|██████▊   | 136/200 [51:37<24:02, 22.54s/it]Running Inference:  68%|██████▊   | 137/200 [52:00<23:53, 22.76s/it]Running Inference:  69%|██████▉   | 138/200 [52:17<21:50, 21.14s/it]Running Inference:  70%|██████▉   | 139/200 [52:42<22:26, 22.07s/it]Running Inference:  70%|███████   | 140/200 [53:06<22:43, 22.73s/it]Running Inference:  70%|███████   | 141/200 [53:29<22:31, 22.91s/it]Running Inference:  71%|███████   | 142/200 [53:49<21:06, 21.83s/it]Running Inference:  72%|███████▏  | 143/200 [54:12<21:20, 22.46s/it]Running Inference:  72%|███████▏  | 144/200 [54:36<21:11, 22.70s/it]Running Inference:  72%|███████▎  | 145/200 [55:01<21:23, 23.34s/it]Running Inference:  73%|███████▎  | 146/200 [55:24<20:59, 23.33s/it]Running Inference:  74%|███████▎  | 147/200 [55:47<20:30, 23.23s/it]Running Inference:  74%|███████▍  | 148/200 [56:10<20:06, 23.20s/it]Running Inference:  74%|███████▍  | 149/200 [56:34<19:50, 23.34s/it]Running Inference:  75%|███████▌  | 150/200 [56:58<19:44, 23.69s/it]Running Inference:  76%|███████▌  | 151/200 [57:23<19:30, 23.89s/it]Running Inference:  76%|███████▌  | 152/200 [57:47<19:14, 24.04s/it]Running Inference:  76%|███████▋  | 153/200 [58:10<18:40, 23.83s/it]Running Inference:  77%|███████▋  | 154/200 [58:33<18:01, 23.51s/it]Running Inference:  78%|███████▊  | 155/200 [58:56<17:32, 23.39s/it]Running Inference:  78%|███████▊  | 156/200 [59:13<15:47, 21.54s/it]Running Inference:  78%|███████▊  | 157/200 [59:36<15:46, 22.02s/it]Running Inference:  79%|███████▉  | 158/200 [1:00:00<15:43, 22.47s/it]Running Inference:  80%|███████▉  | 159/200 [1:00:23<15:24, 22.54s/it]Running Inference:  80%|████████  | 160/200 [1:00:47<15:27, 23.20s/it]Running Inference:  80%|████████  | 161/200 [1:01:05<13:53, 21.37s/it]Running Inference:  81%|████████  | 162/200 [1:01:29<14:06, 22.26s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:54<14:16, 23.16s/it]Running Inference:  82%|████████▏ | 164/200 [1:02:18<13:55, 23.22s/it]Running Inference:  82%|████████▎ | 165/200 [1:02:42<13:47, 23.64s/it]Running Inference:  83%|████████▎ | 166/200 [1:03:03<12:54, 22.79s/it]Running Inference:  84%|████████▎ | 167/200 [1:03:26<12:35, 22.89s/it]Running Inference:  84%|████████▍ | 168/200 [1:03:45<11:37, 21.80s/it]Running Inference:  84%|████████▍ | 169/200 [1:04:08<11:27, 22.18s/it]Running Inference:  85%|████████▌ | 170/200 [1:04:29<10:50, 21.67s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:52<10:38, 22.00s/it]Running Inference:  86%|████████▌ | 172/200 [1:05:15<10:28, 22.46s/it]Running Inference:  86%|████████▋ | 173/200 [1:05:40<10:23, 23.11s/it]Running Inference:  87%|████████▋ | 174/200 [1:06:03<10:01, 23.15s/it]Running Inference:  88%|████████▊ | 175/200 [1:06:18<08:38, 20.74s/it]Running Inference:  88%|████████▊ | 176/200 [1:06:36<07:54, 19.76s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:54<07:26, 19.41s/it]Running Inference:  89%|████████▉ | 178/200 [1:07:13<07:02, 19.22s/it]Running Inference:  90%|████████▉ | 179/200 [1:07:37<07:12, 20.61s/it]Running Inference:  90%|█████████ | 180/200 [1:08:00<07:05, 21.26s/it]Running Inference:  90%|█████████ | 181/200 [1:08:23<06:54, 21.79s/it]Running Inference:  91%|█████████ | 182/200 [1:08:46<06:42, 22.35s/it]Running Inference:  92%|█████████▏| 183/200 [1:09:09<06:23, 22.58s/it]Running Inference:  92%|█████████▏| 184/200 [1:09:24<05:20, 20.04s/it]Running Inference:  92%|█████████▎| 185/200 [1:09:35<04:20, 17.34s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:58<04:27, 19.09s/it]Running Inference:  94%|█████████▎| 187/200 [1:10:21<04:23, 20.25s/it]Running Inference:  94%|█████████▍| 188/200 [1:10:44<04:14, 21.24s/it]Running Inference:  94%|█████████▍| 189/200 [1:11:07<03:58, 21.69s/it]Running Inference:  95%|█████████▌| 190/200 [1:11:28<03:34, 21.45s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:51<03:16, 21.84s/it]Running Inference:  96%|█████████▌| 192/200 [1:12:14<02:57, 22.19s/it]Running Inference:  96%|█████████▋| 193/200 [1:12:37<02:37, 22.51s/it]Running Inference:  97%|█████████▋| 194/200 [1:13:01<02:17, 22.85s/it]Running Inference:  98%|█████████▊| 195/200 [1:13:26<01:57, 23.51s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:49<01:34, 23.51s/it]Running Inference:  98%|█████████▊| 197/200 [1:14:13<01:10, 23.55s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:36<00:46, 23.41s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:59<00:23, 23.33s/it]Running Inference: 100%|██████████| 200/200 [1:15:18<00:00, 22.10s/it]Running Inference: 100%|██████████| 200/200 [1:15:18<00:00, 22.59s/it]
2025-12-14 15:20:34,402 - INFO - Inference completed.
2025-12-14 15:20:34,415 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 15:20:34,415 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.663 seconds.
Prefix dict has been built successfully.
2025-12-14 15:20:41,219 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 15:20:41,219 - INFO - Metrics:
11.43
2025-12-14 15:20:41,221 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: tova (task=vcsum, ratio=0.5) on GPU 1


========================================
LongBench Task: passage_count
========================================
----------------------------------------
Task: passage_count | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 15:20:48,185 - INFO - Set deterministic seeds to 42
2025-12-14 15:20:48,185 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 15:20:48,185 - INFO - Starting evaluation run...
2025-12-14 15:20:48,185 - INFO - Output directory set to: longbenchresult
2025-12-14 15:20:48,185 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 15:20:48,185 - INFO - KV Press 'tova' setup.
2025-12-14 15:20:48,185 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 15:20:48,185 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.52it/s]
Device set to use cuda:0
2025-12-14 15:21:02,918 - INFO - Model pipeline loaded.
2025-12-14 15:21:02,919 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 1034.43 examples/s]Generating test split: 200 examples [00:00, 1030.26 examples/s]
2025-12-14 15:21:15,854 - INFO - Dataset loaded with 200 entries.
2025-12-14 15:21:15,854 - INFO - Dataset processed with 200 entries.
2025-12-14 15:21:15,894 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:36,  1.99s/it]Running Inference:   1%|          | 2/200 [00:04<07:08,  2.16s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:16,  1.91s/it]Running Inference:   2%|▏         | 4/200 [00:06<04:53,  1.50s/it]Running Inference:   2%|▎         | 5/200 [00:11<08:24,  2.59s/it]Running Inference:   3%|▎         | 6/200 [00:13<07:33,  2.34s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:24,  1.99s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:16,  1.65s/it]Running Inference:   4%|▍         | 9/200 [00:18<06:30,  2.05s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:35,  2.08s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:42,  2.45s/it]Running Inference:   6%|▌         | 12/200 [00:27<08:49,  2.81s/it]Running Inference:   6%|▋         | 13/200 [00:28<07:34,  2.43s/it]Running Inference:   7%|▋         | 14/200 [00:29<06:12,  2.00s/it]Running Inference:   8%|▊         | 15/200 [00:33<07:24,  2.40s/it]Running Inference:   8%|▊         | 16/200 [00:35<06:57,  2.27s/it]Running Inference:   8%|▊         | 17/200 [00:36<06:13,  2.04s/it]Running Inference:   9%|▉         | 18/200 [00:38<05:57,  1.96s/it]Running Inference:  10%|▉         | 19/200 [00:40<05:42,  1.89s/it]Running Inference:  10%|█         | 20/200 [00:42<05:54,  1.97s/it]Running Inference:  10%|█         | 21/200 [00:45<06:52,  2.31s/it]Running Inference:  11%|█         | 22/200 [00:47<06:42,  2.26s/it]Running Inference:  12%|█▏        | 23/200 [00:50<07:01,  2.38s/it]Running Inference:  12%|█▏        | 24/200 [00:52<07:02,  2.40s/it]Running Inference:  12%|█▎        | 25/200 [00:55<06:55,  2.38s/it]Running Inference:  13%|█▎        | 26/200 [00:57<06:47,  2.34s/it]Running Inference:  14%|█▎        | 27/200 [00:59<06:20,  2.20s/it]Running Inference:  14%|█▍        | 28/200 [01:03<08:18,  2.90s/it]Running Inference:  14%|█▍        | 29/200 [01:06<08:35,  3.01s/it]Running Inference:  15%|█▌        | 30/200 [01:08<07:05,  2.50s/it]Running Inference:  16%|█▌        | 31/200 [01:09<06:09,  2.18s/it]Running Inference:  16%|█▌        | 32/200 [01:13<07:12,  2.58s/it]Running Inference:  16%|█▋        | 33/200 [01:14<05:46,  2.08s/it]Running Inference:  17%|█▋        | 34/200 [01:16<06:04,  2.19s/it]Running Inference:  18%|█▊        | 35/200 [01:18<06:07,  2.23s/it]Running Inference:  18%|█▊        | 36/200 [01:21<06:16,  2.30s/it]Running Inference:  18%|█▊        | 37/200 [01:23<05:54,  2.18s/it]Running Inference:  19%|█▉        | 38/200 [01:24<05:28,  2.03s/it]Running Inference:  20%|█▉        | 39/200 [01:27<05:53,  2.19s/it]Running Inference:  20%|██        | 40/200 [01:29<05:28,  2.05s/it]Running Inference:  20%|██        | 41/200 [01:31<05:24,  2.04s/it]Running Inference:  21%|██        | 42/200 [01:32<04:40,  1.77s/it]Running Inference:  22%|██▏       | 43/200 [01:34<04:56,  1.89s/it]Running Inference:  22%|██▏       | 44/200 [01:36<05:02,  1.94s/it]Running Inference:  22%|██▎       | 45/200 [01:39<05:44,  2.22s/it]Running Inference:  23%|██▎       | 46/200 [01:41<05:49,  2.27s/it]Running Inference:  24%|██▎       | 47/200 [01:43<04:55,  1.93s/it]Running Inference:  24%|██▍       | 48/200 [01:43<04:06,  1.62s/it]Running Inference:  24%|██▍       | 49/200 [01:45<04:00,  1.59s/it]Running Inference:  25%|██▌       | 50/200 [01:46<03:36,  1.45s/it]Running Inference:  26%|██▌       | 51/200 [01:48<04:15,  1.72s/it]Running Inference:  26%|██▌       | 52/200 [01:50<04:00,  1.63s/it]Running Inference:  26%|██▋       | 53/200 [01:53<04:51,  1.98s/it]Running Inference:  27%|██▋       | 54/200 [01:55<04:59,  2.05s/it]Running Inference:  28%|██▊       | 55/200 [01:56<04:14,  1.76s/it]Running Inference:  28%|██▊       | 56/200 [01:58<04:17,  1.79s/it]Running Inference:  28%|██▊       | 57/200 [02:02<05:42,  2.40s/it]Running Inference:  29%|██▉       | 58/200 [02:04<05:28,  2.32s/it]Running Inference:  30%|██▉       | 59/200 [02:07<06:16,  2.67s/it]Running Inference:  30%|███       | 60/200 [02:10<06:22,  2.73s/it]Running Inference:  30%|███       | 61/200 [02:12<05:29,  2.37s/it]Running Inference:  31%|███       | 62/200 [02:13<04:31,  1.96s/it]Running Inference:  32%|███▏      | 63/200 [02:15<04:28,  1.96s/it]Running Inference:  32%|███▏      | 64/200 [02:17<04:56,  2.18s/it]Running Inference:  32%|███▎      | 65/200 [02:20<05:04,  2.25s/it]Running Inference:  33%|███▎      | 66/200 [02:22<05:09,  2.31s/it]Running Inference:  34%|███▎      | 67/200 [02:24<04:43,  2.13s/it]Running Inference:  34%|███▍      | 68/200 [02:26<04:41,  2.13s/it]Running Inference:  34%|███▍      | 69/200 [02:29<05:24,  2.48s/it]Running Inference:  35%|███▌      | 70/200 [02:31<05:06,  2.36s/it]Running Inference:  36%|███▌      | 71/200 [02:34<05:16,  2.45s/it]Running Inference:  36%|███▌      | 72/200 [02:36<04:57,  2.32s/it]Running Inference:  36%|███▋      | 73/200 [02:39<05:27,  2.58s/it]Running Inference:  37%|███▋      | 74/200 [02:42<05:19,  2.53s/it]Running Inference:  38%|███▊      | 75/200 [02:43<04:43,  2.27s/it]Running Inference:  38%|███▊      | 76/200 [02:45<04:02,  1.96s/it]Running Inference:  38%|███▊      | 77/200 [02:49<05:27,  2.66s/it]Running Inference:  39%|███▉      | 78/200 [02:51<05:02,  2.48s/it]Running Inference:  40%|███▉      | 79/200 [02:53<04:55,  2.44s/it]Running Inference:  40%|████      | 80/200 [02:56<04:52,  2.44s/it]Running Inference:  40%|████      | 81/200 [02:57<04:09,  2.10s/it]Running Inference:  41%|████      | 82/200 [02:58<03:35,  1.83s/it]Running Inference:  42%|████▏     | 83/200 [03:01<03:53,  2.00s/it]Running Inference:  42%|████▏     | 84/200 [03:03<03:51,  1.99s/it]Running Inference:  42%|████▎     | 85/200 [03:05<04:21,  2.27s/it]Running Inference:  43%|████▎     | 86/200 [03:08<04:20,  2.28s/it]Running Inference:  44%|████▎     | 87/200 [03:10<04:07,  2.19s/it]Running Inference:  44%|████▍     | 88/200 [03:12<04:06,  2.20s/it]Running Inference:  44%|████▍     | 89/200 [03:15<04:25,  2.39s/it]Running Inference:  45%|████▌     | 90/200 [03:18<04:35,  2.50s/it]Running Inference:  46%|████▌     | 91/200 [03:19<03:49,  2.10s/it]Running Inference:  46%|████▌     | 92/200 [03:20<03:10,  1.76s/it]Running Inference:  46%|████▋     | 93/200 [03:21<03:03,  1.72s/it]Running Inference:  47%|████▋     | 94/200 [03:23<03:06,  1.76s/it]Running Inference:  48%|████▊     | 95/200 [03:25<03:19,  1.90s/it]Running Inference:  48%|████▊     | 96/200 [03:27<03:19,  1.91s/it]Running Inference:  48%|████▊     | 97/200 [03:30<03:33,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [03:33<04:17,  2.53s/it]Running Inference:  50%|████▉     | 99/200 [03:36<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [03:38<03:57,  2.37s/it]Running Inference:  50%|█████     | 101/200 [03:39<03:24,  2.07s/it]Running Inference:  51%|█████     | 102/200 [03:41<03:11,  1.95s/it]Running Inference:  52%|█████▏    | 103/200 [03:44<03:44,  2.31s/it]Running Inference:  52%|█████▏    | 104/200 [03:47<03:45,  2.35s/it]Running Inference:  52%|█████▎    | 105/200 [03:47<02:56,  1.85s/it]Running Inference:  53%|█████▎    | 106/200 [03:50<03:13,  2.06s/it]Running Inference:  54%|█████▎    | 107/200 [03:53<03:41,  2.38s/it]Running Inference:  54%|█████▍    | 108/200 [03:55<03:31,  2.30s/it]Running Inference:  55%|█████▍    | 109/200 [03:58<03:46,  2.49s/it]Running Inference:  55%|█████▌    | 110/200 [04:00<03:17,  2.19s/it]Running Inference:  56%|█████▌    | 111/200 [04:01<02:48,  1.89s/it]Running Inference:  56%|█████▌    | 112/200 [04:02<02:23,  1.63s/it]Running Inference:  56%|█████▋    | 113/200 [04:03<02:19,  1.60s/it]Running Inference:  57%|█████▋    | 114/200 [04:05<02:13,  1.55s/it]Running Inference:  57%|█████▊    | 115/200 [04:07<02:39,  1.87s/it]Running Inference:  58%|█████▊    | 116/200 [04:10<02:52,  2.06s/it]Running Inference:  58%|█████▊    | 117/200 [04:11<02:37,  1.90s/it]Running Inference:  59%|█████▉    | 118/200 [04:12<02:09,  1.58s/it]Running Inference:  60%|█████▉    | 119/200 [04:14<02:14,  1.66s/it]Running Inference:  60%|██████    | 120/200 [04:19<03:20,  2.50s/it]Running Inference:  60%|██████    | 121/200 [04:22<03:38,  2.77s/it]Running Inference:  61%|██████    | 122/200 [04:25<03:45,  2.89s/it]Running Inference:  62%|██████▏   | 123/200 [04:26<02:56,  2.30s/it]Running Inference:  62%|██████▏   | 124/200 [04:29<02:59,  2.37s/it]Running Inference:  62%|██████▎   | 125/200 [04:30<02:45,  2.21s/it]Running Inference:  63%|██████▎   | 126/200 [04:31<02:12,  1.80s/it]Running Inference:  64%|██████▎   | 127/200 [04:34<02:24,  1.98s/it]Running Inference:  64%|██████▍   | 128/200 [04:36<02:26,  2.04s/it]Running Inference:  64%|██████▍   | 129/200 [04:39<02:42,  2.29s/it]Running Inference:  65%|██████▌   | 130/200 [04:40<02:14,  1.93s/it]Running Inference:  66%|██████▌   | 131/200 [04:43<02:44,  2.39s/it]Running Inference:  66%|██████▌   | 132/200 [04:46<02:53,  2.55s/it]Running Inference:  66%|██████▋   | 133/200 [04:47<02:24,  2.15s/it]Running Inference:  67%|██████▋   | 134/200 [04:50<02:34,  2.34s/it]Running Inference:  68%|██████▊   | 135/200 [04:52<02:19,  2.15s/it]Running Inference:  68%|██████▊   | 136/200 [04:54<02:10,  2.05s/it]Running Inference:  68%|██████▊   | 137/200 [04:56<02:14,  2.14s/it]Running Inference:  69%|██████▉   | 138/200 [04:58<02:09,  2.09s/it]Running Inference:  70%|██████▉   | 139/200 [05:01<02:28,  2.43s/it]Running Inference:  70%|███████   | 140/200 [05:04<02:32,  2.54s/it]Running Inference:  70%|███████   | 141/200 [05:06<02:14,  2.29s/it]Running Inference:  71%|███████   | 142/200 [05:08<02:16,  2.36s/it]Running Inference:  72%|███████▏  | 143/200 [05:12<02:31,  2.65s/it]Running Inference:  72%|███████▏  | 144/200 [05:14<02:17,  2.45s/it]Running Inference:  72%|███████▎  | 145/200 [05:16<02:13,  2.43s/it]Running Inference:  73%|███████▎  | 146/200 [05:18<02:09,  2.39s/it]Running Inference:  74%|███████▎  | 147/200 [05:21<02:19,  2.63s/it]Running Inference:  74%|███████▍  | 148/200 [05:24<02:12,  2.55s/it]Running Inference:  74%|███████▍  | 149/200 [05:25<01:48,  2.12s/it]Running Inference:  75%|███████▌  | 150/200 [05:27<01:51,  2.23s/it]Running Inference:  76%|███████▌  | 151/200 [05:28<01:29,  1.83s/it]Running Inference:  76%|███████▌  | 152/200 [05:31<01:35,  1.99s/it]Running Inference:  76%|███████▋  | 153/200 [05:31<01:15,  1.60s/it]Running Inference:  77%|███████▋  | 154/200 [05:34<01:28,  1.93s/it]Running Inference:  78%|███████▊  | 155/200 [05:36<01:21,  1.82s/it]Running Inference:  78%|███████▊  | 156/200 [05:38<01:21,  1.86s/it]Running Inference:  78%|███████▊  | 157/200 [05:39<01:11,  1.66s/it]Running Inference:  79%|███████▉  | 158/200 [05:41<01:15,  1.79s/it]Running Inference:  80%|███████▉  | 159/200 [05:43<01:14,  1.82s/it]Running Inference:  80%|████████  | 160/200 [05:46<01:32,  2.32s/it]Running Inference:  80%|████████  | 161/200 [05:49<01:33,  2.40s/it]Running Inference:  81%|████████  | 162/200 [05:51<01:27,  2.31s/it]Running Inference:  82%|████████▏ | 163/200 [05:56<01:51,  3.00s/it]Running Inference:  82%|████████▏ | 164/200 [05:57<01:34,  2.61s/it]Running Inference:  82%|████████▎ | 165/200 [06:00<01:28,  2.54s/it]Running Inference:  83%|████████▎ | 166/200 [06:03<01:33,  2.74s/it]Running Inference:  84%|████████▎ | 167/200 [06:04<01:19,  2.41s/it]Running Inference:  84%|████████▍ | 168/200 [06:07<01:20,  2.53s/it]Running Inference:  84%|████████▍ | 169/200 [06:08<01:01,  2.00s/it]Running Inference:  85%|████████▌ | 170/200 [06:10<01:03,  2.11s/it]Running Inference:  86%|████████▌ | 171/200 [06:13<01:03,  2.17s/it]Running Inference:  86%|████████▌ | 172/200 [06:14<00:52,  1.86s/it]Running Inference:  86%|████████▋ | 173/200 [06:16<00:53,  1.97s/it]Running Inference:  87%|████████▋ | 174/200 [06:18<00:49,  1.90s/it]Running Inference:  88%|████████▊ | 175/200 [06:20<00:50,  2.01s/it]Running Inference:  88%|████████▊ | 176/200 [06:22<00:46,  1.94s/it]Running Inference:  88%|████████▊ | 177/200 [06:23<00:42,  1.84s/it]Running Inference:  89%|████████▉ | 178/200 [06:26<00:44,  2.03s/it]Running Inference:  90%|████████▉ | 179/200 [06:29<00:46,  2.20s/it]Running Inference:  90%|█████████ | 180/200 [06:31<00:44,  2.20s/it]Running Inference:  90%|█████████ | 181/200 [06:32<00:37,  1.99s/it]Running Inference:  91%|█████████ | 182/200 [06:34<00:36,  2.03s/it]Running Inference:  92%|█████████▏| 183/200 [06:36<00:30,  1.77s/it]Running Inference:  92%|█████████▏| 184/200 [06:39<00:36,  2.28s/it]Running Inference:  92%|█████████▎| 185/200 [06:41<00:32,  2.16s/it]Running Inference:  93%|█████████▎| 186/200 [06:45<00:38,  2.76s/it]Running Inference:  94%|█████████▎| 187/200 [06:48<00:36,  2.80s/it]Running Inference:  94%|█████████▍| 188/200 [06:51<00:32,  2.74s/it]Running Inference:  94%|█████████▍| 189/200 [06:52<00:24,  2.23s/it]Running Inference:  95%|█████████▌| 190/200 [06:55<00:24,  2.47s/it]Running Inference:  96%|█████████▌| 191/200 [06:58<00:23,  2.65s/it]Running Inference:  96%|█████████▌| 192/200 [06:59<00:17,  2.20s/it]Running Inference:  96%|█████████▋| 193/200 [07:00<00:13,  1.92s/it]Running Inference:  97%|█████████▋| 194/200 [07:03<00:12,  2.15s/it]Running Inference:  98%|█████████▊| 195/200 [07:05<00:10,  2.18s/it]Running Inference:  98%|█████████▊| 196/200 [07:06<00:07,  1.96s/it]Running Inference:  98%|█████████▊| 197/200 [07:08<00:05,  1.79s/it]Running Inference:  99%|█████████▉| 198/200 [07:09<00:03,  1.63s/it]Running Inference: 100%|█████████▉| 199/200 [07:11<00:01,  1.57s/it]Running Inference: 100%|██████████| 200/200 [07:12<00:00,  1.62s/it]Running Inference: 100%|██████████| 200/200 [07:12<00:00,  2.16s/it]
2025-12-14 15:28:28,692 - INFO - Inference completed.
2025-12-14 15:28:28,701 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 15:28:28,701 - INFO - Calculating metrics for dataset: longbench
2025-12-14 15:28:28,702 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 15:28:28,702 - INFO - Metrics:
9.25
2025-12-14 15:28:28,703 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_count, ratio=0.1) on GPU 1

----------------------------------------
Task: passage_count | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 15:28:35,242 - INFO - Set deterministic seeds to 42
2025-12-14 15:28:35,243 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 15:28:35,243 - INFO - Starting evaluation run...
2025-12-14 15:28:35,243 - INFO - Output directory set to: longbenchresult
2025-12-14 15:28:35,243 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 15:28:35,243 - INFO - KV Press 'tova' setup.
2025-12-14 15:28:35,243 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 15:28:35,243 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.29it/s]
Device set to use cuda:0
2025-12-14 15:28:50,503 - INFO - Model pipeline loaded.
2025-12-14 15:28:50,504 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 15:28:56,521 - INFO - Dataset loaded with 200 entries.
2025-12-14 15:28:56,521 - INFO - Dataset processed with 200 entries.
2025-12-14 15:28:56,563 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:28,  1.95s/it]Running Inference:   1%|          | 2/200 [00:04<07:05,  2.15s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:12,  1.89s/it]Running Inference:   2%|▏         | 4/200 [00:06<04:51,  1.49s/it]Running Inference:   2%|▎         | 5/200 [00:11<08:23,  2.58s/it]Running Inference:   3%|▎         | 6/200 [00:13<07:33,  2.34s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:24,  1.99s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:17,  1.65s/it]Running Inference:   4%|▍         | 9/200 [00:18<06:31,  2.05s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:36,  2.09s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:43,  2.45s/it]Running Inference:   6%|▌         | 12/200 [00:26<07:41,  2.46s/it]Running Inference:   6%|▋         | 13/200 [00:27<06:48,  2.18s/it]Running Inference:   7%|▋         | 14/200 [00:28<05:40,  1.83s/it]Running Inference:   8%|▊         | 15/200 [00:32<07:01,  2.28s/it]Running Inference:   8%|▊         | 16/200 [00:33<06:40,  2.18s/it]Running Inference:   8%|▊         | 17/200 [00:35<06:01,  1.98s/it]Running Inference:   9%|▉         | 18/200 [00:37<05:49,  1.92s/it]Running Inference:  10%|▉         | 19/200 [00:38<05:36,  1.86s/it]Running Inference:  10%|█         | 20/200 [00:41<05:50,  1.95s/it]Running Inference:  10%|█         | 21/200 [00:44<06:49,  2.29s/it]Running Inference:  11%|█         | 22/200 [00:46<06:40,  2.25s/it]Running Inference:  12%|█▏        | 23/200 [00:49<07:00,  2.37s/it]Running Inference:  12%|█▏        | 24/200 [00:51<06:59,  2.39s/it]Running Inference:  12%|█▎        | 25/200 [00:53<06:56,  2.38s/it]Running Inference:  13%|█▎        | 26/200 [00:56<06:47,  2.34s/it]Running Inference:  14%|█▎        | 27/200 [00:57<06:18,  2.19s/it]Running Inference:  14%|█▍        | 28/200 [01:02<08:16,  2.89s/it]Running Inference:  14%|█▍        | 29/200 [01:05<08:34,  3.01s/it]Running Inference:  15%|█▌        | 30/200 [01:07<07:05,  2.50s/it]Running Inference:  16%|█▌        | 31/200 [01:08<06:09,  2.18s/it]Running Inference:  16%|█▌        | 32/200 [01:11<07:14,  2.59s/it]Running Inference:  16%|█▋        | 33/200 [01:12<05:48,  2.09s/it]Running Inference:  17%|█▋        | 34/200 [01:15<06:04,  2.20s/it]Running Inference:  18%|█▊        | 35/200 [01:17<06:08,  2.23s/it]Running Inference:  18%|█▊        | 36/200 [01:20<06:17,  2.30s/it]Running Inference:  18%|█▊        | 37/200 [01:22<05:55,  2.18s/it]Running Inference:  19%|█▉        | 38/200 [01:23<05:29,  2.03s/it]Running Inference:  20%|█▉        | 39/200 [01:26<05:51,  2.18s/it]Running Inference:  20%|██        | 40/200 [01:27<05:27,  2.05s/it]Running Inference:  20%|██        | 41/200 [01:29<05:23,  2.03s/it]Running Inference:  21%|██        | 42/200 [01:31<04:39,  1.77s/it]Running Inference:  22%|██▏       | 43/200 [01:33<04:56,  1.89s/it]Running Inference:  22%|██▏       | 44/200 [01:36<05:40,  2.18s/it]Running Inference:  22%|██▎       | 45/200 [01:39<06:10,  2.39s/it]Running Inference:  23%|██▎       | 46/200 [01:41<06:07,  2.38s/it]Running Inference:  24%|██▎       | 47/200 [01:42<05:08,  2.01s/it]Running Inference:  24%|██▍       | 48/200 [01:43<04:15,  1.68s/it]Running Inference:  24%|██▍       | 49/200 [01:44<04:06,  1.63s/it]Running Inference:  25%|██▌       | 50/200 [01:46<03:41,  1.47s/it]Running Inference:  26%|██▌       | 51/200 [01:48<04:20,  1.75s/it]Running Inference:  26%|██▌       | 52/200 [01:49<04:04,  1.65s/it]Running Inference:  26%|██▋       | 53/200 [01:52<04:53,  1.99s/it]Running Inference:  27%|██▋       | 54/200 [01:55<05:40,  2.33s/it]Running Inference:  28%|██▊       | 55/200 [01:56<04:43,  1.95s/it]Running Inference:  28%|██▊       | 56/200 [01:58<04:39,  1.94s/it]Running Inference:  28%|██▊       | 57/200 [02:02<06:09,  2.58s/it]Running Inference:  29%|██▉       | 58/200 [02:05<05:47,  2.45s/it]Running Inference:  30%|██▉       | 59/200 [02:08<06:29,  2.76s/it]Running Inference:  30%|███       | 60/200 [02:11<06:30,  2.79s/it]Running Inference:  30%|███       | 61/200 [02:12<05:34,  2.41s/it]Running Inference:  31%|███       | 62/200 [02:13<04:35,  1.99s/it]Running Inference:  32%|███▏      | 63/200 [02:17<05:55,  2.60s/it]Running Inference:  32%|███▏      | 64/200 [02:20<05:57,  2.63s/it]Running Inference:  32%|███▎      | 65/200 [02:23<05:45,  2.56s/it]Running Inference:  33%|███▎      | 66/200 [02:25<05:38,  2.52s/it]Running Inference:  34%|███▎      | 67/200 [02:27<05:02,  2.27s/it]Running Inference:  34%|███▍      | 68/200 [02:29<04:53,  2.23s/it]Running Inference:  34%|███▍      | 69/200 [02:32<05:35,  2.56s/it]Running Inference:  35%|███▌      | 70/200 [02:34<05:14,  2.42s/it]Running Inference:  36%|███▌      | 71/200 [02:37<05:20,  2.49s/it]Running Inference:  36%|███▌      | 72/200 [02:39<05:00,  2.35s/it]Running Inference:  36%|███▋      | 73/200 [02:42<05:29,  2.59s/it]Running Inference:  37%|███▋      | 74/200 [02:44<05:20,  2.55s/it]Running Inference:  38%|███▊      | 75/200 [02:46<04:44,  2.28s/it]Running Inference:  38%|███▊      | 76/200 [02:47<04:03,  1.96s/it]Running Inference:  38%|███▊      | 77/200 [02:52<05:28,  2.67s/it]Running Inference:  39%|███▉      | 78/200 [02:54<05:03,  2.49s/it]Running Inference:  40%|███▉      | 79/200 [02:56<04:56,  2.45s/it]Running Inference:  40%|████      | 80/200 [02:58<04:52,  2.44s/it]Running Inference:  40%|████      | 81/200 [03:00<04:10,  2.10s/it]Running Inference:  41%|████      | 82/200 [03:01<03:35,  1.83s/it]Running Inference:  42%|████▏     | 83/200 [03:03<03:53,  2.00s/it]Running Inference:  42%|████▏     | 84/200 [03:05<03:51,  1.99s/it]Running Inference:  42%|████▎     | 85/200 [03:08<04:21,  2.27s/it]Running Inference:  43%|████▎     | 86/200 [03:11<04:20,  2.28s/it]Running Inference:  44%|████▎     | 87/200 [03:13<04:06,  2.19s/it]Running Inference:  44%|████▍     | 88/200 [03:15<04:06,  2.20s/it]Running Inference:  44%|████▍     | 89/200 [03:18<04:25,  2.39s/it]Running Inference:  45%|████▌     | 90/200 [03:20<04:35,  2.50s/it]Running Inference:  46%|████▌     | 91/200 [03:22<03:49,  2.11s/it]Running Inference:  46%|████▌     | 92/200 [03:23<03:10,  1.76s/it]Running Inference:  46%|████▋     | 93/200 [03:24<03:03,  1.72s/it]Running Inference:  47%|████▋     | 94/200 [03:26<03:06,  1.76s/it]Running Inference:  48%|████▊     | 95/200 [03:28<03:19,  1.90s/it]Running Inference:  48%|████▊     | 96/200 [03:30<03:19,  1.91s/it]Running Inference:  48%|████▊     | 97/200 [03:33<03:33,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [03:36<04:17,  2.53s/it]Running Inference:  50%|████▉     | 99/200 [03:39<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [03:41<03:57,  2.37s/it]Running Inference:  50%|█████     | 101/200 [03:42<03:23,  2.05s/it]Running Inference:  51%|█████     | 102/200 [03:44<03:10,  1.94s/it]Running Inference:  52%|█████▏    | 103/200 [03:47<03:44,  2.31s/it]Running Inference:  52%|█████▏    | 104/200 [03:49<03:45,  2.35s/it]Running Inference:  52%|█████▎    | 105/200 [03:50<02:55,  1.85s/it]Running Inference:  53%|█████▎    | 106/200 [03:53<03:13,  2.06s/it]Running Inference:  54%|█████▎    | 107/200 [03:56<03:40,  2.37s/it]Running Inference:  54%|█████▍    | 108/200 [03:58<03:31,  2.30s/it]Running Inference:  55%|█████▍    | 109/200 [04:01<03:46,  2.48s/it]Running Inference:  55%|█████▌    | 110/200 [04:02<03:15,  2.18s/it]Running Inference:  56%|█████▌    | 111/200 [04:04<02:47,  1.88s/it]Running Inference:  56%|█████▌    | 112/200 [04:05<02:22,  1.62s/it]Running Inference:  56%|█████▋    | 113/200 [04:06<02:18,  1.60s/it]Running Inference:  57%|█████▋    | 114/200 [04:07<02:13,  1.55s/it]Running Inference:  57%|█████▊    | 115/200 [04:10<02:38,  1.87s/it]Running Inference:  58%|█████▊    | 116/200 [04:13<02:52,  2.06s/it]Running Inference:  58%|█████▊    | 117/200 [04:14<02:37,  1.90s/it]Running Inference:  59%|█████▉    | 118/200 [04:15<02:09,  1.58s/it]Running Inference:  60%|█████▉    | 119/200 [04:17<02:14,  1.66s/it]Running Inference:  60%|██████    | 120/200 [04:19<02:28,  1.86s/it]Running Inference:  60%|██████    | 121/200 [04:23<03:03,  2.32s/it]Running Inference:  61%|██████    | 122/200 [04:26<03:21,  2.59s/it]Running Inference:  62%|██████▏   | 123/200 [04:27<02:40,  2.08s/it]Running Inference:  62%|██████▏   | 124/200 [04:29<02:48,  2.22s/it]Running Inference:  62%|██████▎   | 125/200 [04:31<02:38,  2.11s/it]Running Inference:  63%|██████▎   | 126/200 [04:32<02:07,  1.73s/it]Running Inference:  64%|██████▎   | 127/200 [04:34<02:21,  1.93s/it]Running Inference:  64%|██████▍   | 128/200 [04:36<02:24,  2.00s/it]Running Inference:  64%|██████▍   | 129/200 [04:39<02:39,  2.25s/it]Running Inference:  65%|██████▌   | 130/200 [04:40<02:12,  1.90s/it]Running Inference:  66%|██████▌   | 131/200 [04:44<02:43,  2.37s/it]Running Inference:  66%|██████▌   | 132/200 [04:47<02:52,  2.53s/it]Running Inference:  66%|██████▋   | 133/200 [04:48<02:23,  2.14s/it]Running Inference:  67%|██████▋   | 134/200 [04:51<02:33,  2.33s/it]Running Inference:  68%|██████▊   | 135/200 [04:52<02:20,  2.16s/it]Running Inference:  68%|██████▊   | 136/200 [04:54<02:11,  2.05s/it]Running Inference:  68%|██████▊   | 137/200 [04:57<02:14,  2.13s/it]Running Inference:  69%|██████▉   | 138/200 [04:59<02:09,  2.08s/it]Running Inference:  70%|██████▉   | 139/200 [05:02<02:28,  2.44s/it]Running Inference:  70%|███████   | 140/200 [05:05<02:32,  2.54s/it]Running Inference:  70%|███████   | 141/200 [05:06<02:14,  2.29s/it]Running Inference:  71%|███████   | 142/200 [05:09<02:16,  2.35s/it]Running Inference:  72%|███████▏  | 143/200 [05:12<02:31,  2.66s/it]Running Inference:  72%|███████▏  | 144/200 [05:14<02:17,  2.45s/it]Running Inference:  72%|███████▎  | 145/200 [05:17<02:13,  2.43s/it]Running Inference:  73%|███████▎  | 146/200 [05:19<02:09,  2.39s/it]Running Inference:  74%|███████▎  | 147/200 [05:22<02:20,  2.66s/it]Running Inference:  74%|███████▍  | 148/200 [05:25<02:13,  2.57s/it]Running Inference:  74%|███████▍  | 149/200 [05:26<01:48,  2.13s/it]Running Inference:  75%|███████▌  | 150/200 [05:28<01:52,  2.24s/it]Running Inference:  76%|███████▌  | 151/200 [05:29<01:30,  1.84s/it]Running Inference:  76%|███████▌  | 152/200 [05:31<01:35,  2.00s/it]Running Inference:  76%|███████▋  | 153/200 [05:32<01:15,  1.61s/it]Running Inference:  77%|███████▋  | 154/200 [05:35<01:28,  1.93s/it]Running Inference:  78%|███████▊  | 155/200 [05:36<01:21,  1.82s/it]Running Inference:  78%|███████▊  | 156/200 [05:38<01:21,  1.86s/it]Running Inference:  78%|███████▊  | 157/200 [05:39<01:11,  1.66s/it]Running Inference:  79%|███████▉  | 158/200 [05:42<01:14,  1.78s/it]Running Inference:  80%|███████▉  | 159/200 [05:43<01:14,  1.82s/it]Running Inference:  80%|████████  | 160/200 [05:47<01:32,  2.30s/it]Running Inference:  80%|████████  | 161/200 [05:49<01:33,  2.39s/it]Running Inference:  81%|████████  | 162/200 [05:52<01:27,  2.29s/it]Running Inference:  82%|████████▏ | 163/200 [05:55<01:41,  2.74s/it]Running Inference:  82%|████████▏ | 164/200 [05:57<01:27,  2.43s/it]Running Inference:  82%|████████▎ | 165/200 [05:59<01:24,  2.41s/it]Running Inference:  83%|████████▎ | 166/200 [06:03<01:29,  2.64s/it]Running Inference:  84%|████████▎ | 167/200 [06:04<01:17,  2.34s/it]Running Inference:  84%|████████▍ | 168/200 [06:07<01:19,  2.48s/it]Running Inference:  84%|████████▍ | 169/200 [06:08<01:00,  1.97s/it]Running Inference:  85%|████████▌ | 170/200 [06:10<01:02,  2.08s/it]Running Inference:  86%|████████▌ | 171/200 [06:12<01:02,  2.16s/it]Running Inference:  86%|████████▌ | 172/200 [06:14<00:51,  1.85s/it]Running Inference:  86%|████████▋ | 173/200 [06:16<00:53,  1.96s/it]Running Inference:  87%|████████▋ | 174/200 [06:18<00:49,  1.89s/it]Running Inference:  88%|████████▊ | 175/200 [06:20<00:50,  2.01s/it]Running Inference:  88%|████████▊ | 176/200 [06:22<00:46,  1.93s/it]Running Inference:  88%|████████▊ | 177/200 [06:23<00:42,  1.84s/it]Running Inference:  89%|████████▉ | 178/200 [06:26<00:44,  2.03s/it]Running Inference:  90%|████████▉ | 179/200 [06:28<00:46,  2.20s/it]Running Inference:  90%|█████████ | 180/200 [06:31<00:43,  2.20s/it]Running Inference:  90%|█████████ | 181/200 [06:32<00:37,  1.99s/it]Running Inference:  91%|█████████ | 182/200 [06:34<00:36,  2.03s/it]Running Inference:  92%|█████████▏| 183/200 [06:35<00:30,  1.77s/it]Running Inference:  92%|█████████▏| 184/200 [06:39<00:36,  2.28s/it]Running Inference:  92%|█████████▎| 185/200 [06:41<00:32,  2.15s/it]Running Inference:  93%|█████████▎| 186/200 [06:45<00:38,  2.76s/it]Running Inference:  94%|█████████▎| 187/200 [06:48<00:36,  2.79s/it]Running Inference:  94%|█████████▍| 188/200 [06:50<00:32,  2.73s/it]Running Inference:  94%|█████████▍| 189/200 [06:51<00:24,  2.22s/it]Running Inference:  95%|█████████▌| 190/200 [06:54<00:24,  2.46s/it]Running Inference:  96%|█████████▌| 191/200 [06:57<00:23,  2.65s/it]Running Inference:  96%|█████████▌| 192/200 [06:59<00:17,  2.19s/it]Running Inference:  96%|█████████▋| 193/200 [07:00<00:13,  1.92s/it]Running Inference:  97%|█████████▋| 194/200 [07:02<00:12,  2.13s/it]Running Inference:  98%|█████████▊| 195/200 [07:05<00:10,  2.17s/it]Running Inference:  98%|█████████▊| 196/200 [07:06<00:07,  1.95s/it]Running Inference:  98%|█████████▊| 197/200 [07:08<00:05,  1.78s/it]Running Inference:  99%|█████████▉| 198/200 [07:09<00:03,  1.62s/it]Running Inference: 100%|█████████▉| 199/200 [07:10<00:01,  1.56s/it]Running Inference: 100%|██████████| 200/200 [07:12<00:00,  1.61s/it]Running Inference: 100%|██████████| 200/200 [07:12<00:00,  2.16s/it]
2025-12-14 15:36:08,998 - INFO - Inference completed.
2025-12-14 15:36:09,006 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 15:36:09,006 - INFO - Calculating metrics for dataset: longbench
2025-12-14 15:36:09,007 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 15:36:09,007 - INFO - Metrics:
10.0
2025-12-14 15:36:09,009 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_count, ratio=0.2) on GPU 1

----------------------------------------
Task: passage_count | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 15:36:15,569 - INFO - Set deterministic seeds to 42
2025-12-14 15:36:15,569 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 15:36:15,569 - INFO - Starting evaluation run...
2025-12-14 15:36:15,570 - INFO - Output directory set to: longbenchresult
2025-12-14 15:36:15,570 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 15:36:15,570 - INFO - KV Press 'tova' setup.
2025-12-14 15:36:15,570 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 15:36:15,570 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.46it/s]
Device set to use cuda:0
2025-12-14 15:36:27,564 - INFO - Model pipeline loaded.
2025-12-14 15:36:27,564 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 15:36:32,931 - INFO - Dataset loaded with 200 entries.
2025-12-14 15:36:32,931 - INFO - Dataset processed with 200 entries.
2025-12-14 15:36:32,970 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:35,  1.99s/it]Running Inference:   1%|          | 2/200 [00:04<07:08,  2.16s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:17,  1.92s/it]Running Inference:   2%|▏         | 4/200 [00:06<04:54,  1.50s/it]Running Inference:   2%|▎         | 5/200 [00:11<08:23,  2.58s/it]Running Inference:   3%|▎         | 6/200 [00:13<07:34,  2.34s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:24,  1.99s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:16,  1.65s/it]Running Inference:   4%|▍         | 9/200 [00:18<06:30,  2.04s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:35,  2.08s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:42,  2.45s/it]Running Inference:   6%|▌         | 12/200 [00:26<07:40,  2.45s/it]Running Inference:   6%|▋         | 13/200 [00:27<06:47,  2.18s/it]Running Inference:   7%|▋         | 14/200 [00:28<05:39,  1.82s/it]Running Inference:   8%|▊         | 15/200 [00:32<07:00,  2.27s/it]Running Inference:   8%|▊         | 16/200 [00:33<06:39,  2.17s/it]Running Inference:   8%|▊         | 17/200 [00:35<06:00,  1.97s/it]Running Inference:   9%|▉         | 18/200 [00:37<05:48,  1.91s/it]Running Inference:  10%|▉         | 19/200 [00:38<05:35,  1.85s/it]Running Inference:  10%|█         | 20/200 [00:41<05:49,  1.94s/it]Running Inference:  10%|█         | 21/200 [00:44<06:49,  2.28s/it]Running Inference:  11%|█         | 22/200 [00:46<06:39,  2.24s/it]Running Inference:  12%|█▏        | 23/200 [00:48<06:59,  2.37s/it]Running Inference:  12%|█▏        | 24/200 [00:51<06:59,  2.38s/it]Running Inference:  12%|█▎        | 25/200 [00:53<06:53,  2.36s/it]Running Inference:  13%|█▎        | 26/200 [00:55<06:45,  2.33s/it]Running Inference:  14%|█▎        | 27/200 [00:57<06:18,  2.19s/it]Running Inference:  14%|█▍        | 28/200 [01:02<08:16,  2.89s/it]Running Inference:  14%|█▍        | 29/200 [01:05<08:33,  3.00s/it]Running Inference:  15%|█▌        | 30/200 [01:06<07:04,  2.50s/it]Running Inference:  16%|█▌        | 31/200 [01:08<06:08,  2.18s/it]Running Inference:  16%|█▌        | 32/200 [01:11<07:13,  2.58s/it]Running Inference:  16%|█▋        | 33/200 [01:12<05:46,  2.08s/it]Running Inference:  17%|█▋        | 34/200 [01:15<06:01,  2.18s/it]Running Inference:  18%|█▊        | 35/200 [01:17<06:03,  2.20s/it]Running Inference:  18%|█▊        | 36/200 [01:19<06:13,  2.28s/it]Running Inference:  18%|█▊        | 37/200 [01:21<05:52,  2.16s/it]Running Inference:  19%|█▉        | 38/200 [01:23<05:25,  2.01s/it]Running Inference:  20%|█▉        | 39/200 [01:25<05:49,  2.17s/it]Running Inference:  20%|██        | 40/200 [01:27<05:25,  2.03s/it]Running Inference:  20%|██        | 41/200 [01:29<05:22,  2.03s/it]Running Inference:  21%|██        | 42/200 [01:30<04:38,  1.76s/it]Running Inference:  22%|██▏       | 43/200 [01:33<04:55,  1.88s/it]Running Inference:  22%|██▏       | 44/200 [01:35<05:01,  1.93s/it]Running Inference:  22%|██▎       | 45/200 [01:37<05:43,  2.21s/it]Running Inference:  23%|██▎       | 46/200 [01:40<05:47,  2.26s/it]Running Inference:  24%|██▎       | 47/200 [01:41<04:54,  1.93s/it]Running Inference:  24%|██▍       | 48/200 [01:42<04:05,  1.62s/it]Running Inference:  24%|██▍       | 49/200 [01:43<03:59,  1.59s/it]Running Inference:  25%|██▌       | 50/200 [01:44<03:36,  1.44s/it]Running Inference:  26%|██▌       | 51/200 [01:47<04:14,  1.71s/it]Running Inference:  26%|██▌       | 52/200 [01:48<04:00,  1.62s/it]Running Inference:  26%|██▋       | 53/200 [01:51<04:50,  1.98s/it]Running Inference:  27%|██▋       | 54/200 [01:54<05:38,  2.32s/it]Running Inference:  28%|██▊       | 55/200 [01:55<04:41,  1.94s/it]Running Inference:  28%|██▊       | 56/200 [01:57<04:38,  1.93s/it]Running Inference:  28%|██▊       | 57/200 [02:01<06:08,  2.57s/it]Running Inference:  29%|██▉       | 58/200 [02:03<05:46,  2.44s/it]Running Inference:  30%|██▉       | 59/200 [02:07<06:28,  2.75s/it]Running Inference:  30%|███       | 60/200 [02:10<06:29,  2.78s/it]Running Inference:  30%|███       | 61/200 [02:11<05:34,  2.40s/it]Running Inference:  31%|███       | 62/200 [02:12<04:34,  1.99s/it]Running Inference:  32%|███▏      | 63/200 [02:16<05:55,  2.59s/it]Running Inference:  32%|███▏      | 64/200 [02:19<05:56,  2.62s/it]Running Inference:  32%|███▎      | 65/200 [02:21<05:44,  2.55s/it]Running Inference:  33%|███▎      | 66/200 [02:24<05:36,  2.51s/it]Running Inference:  34%|███▎      | 67/200 [02:25<05:00,  2.26s/it]Running Inference:  34%|███▍      | 68/200 [02:27<04:52,  2.22s/it]Running Inference:  34%|███▍      | 69/200 [02:31<05:32,  2.54s/it]Running Inference:  35%|███▌      | 70/200 [02:33<05:12,  2.40s/it]Running Inference:  36%|███▌      | 71/200 [02:36<05:19,  2.48s/it]Running Inference:  36%|███▌      | 72/200 [02:38<04:59,  2.34s/it]Running Inference:  36%|███▋      | 73/200 [02:41<05:27,  2.58s/it]Running Inference:  37%|███▋      | 74/200 [02:43<05:18,  2.53s/it]Running Inference:  38%|███▊      | 75/200 [02:45<04:42,  2.26s/it]Running Inference:  38%|███▊      | 76/200 [02:46<04:02,  1.95s/it]Running Inference:  38%|███▊      | 77/200 [02:50<05:27,  2.66s/it]Running Inference:  39%|███▉      | 78/200 [02:52<05:02,  2.48s/it]Running Inference:  40%|███▉      | 79/200 [02:55<04:55,  2.44s/it]Running Inference:  40%|████      | 80/200 [02:57<04:51,  2.43s/it]Running Inference:  40%|████      | 81/200 [02:58<04:09,  2.10s/it]Running Inference:  41%|████      | 82/200 [03:00<03:35,  1.82s/it]Running Inference:  42%|████▏     | 83/200 [03:02<03:52,  1.99s/it]Running Inference:  42%|████▏     | 84/200 [03:04<03:50,  1.99s/it]Running Inference:  42%|████▎     | 85/200 [03:07<04:18,  2.25s/it]Running Inference:  43%|████▎     | 86/200 [03:09<04:18,  2.27s/it]Running Inference:  44%|████▎     | 87/200 [03:11<04:05,  2.17s/it]Running Inference:  44%|████▍     | 88/200 [03:13<04:05,  2.19s/it]Running Inference:  44%|████▍     | 89/200 [03:16<04:24,  2.38s/it]Running Inference:  45%|████▌     | 90/200 [03:19<04:35,  2.50s/it]Running Inference:  46%|████▌     | 91/200 [03:20<03:49,  2.11s/it]Running Inference:  46%|████▌     | 92/200 [03:21<03:10,  1.76s/it]Running Inference:  46%|████▋     | 93/200 [03:23<03:03,  1.72s/it]Running Inference:  47%|████▋     | 94/200 [03:25<03:06,  1.76s/it]Running Inference:  48%|████▊     | 95/200 [03:27<03:19,  1.90s/it]Running Inference:  48%|████▊     | 96/200 [03:29<03:18,  1.91s/it]Running Inference:  48%|████▊     | 97/200 [03:31<03:32,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [03:35<04:17,  2.53s/it]Running Inference:  50%|████▉     | 99/200 [03:38<04:28,  2.65s/it]Running Inference:  50%|█████     | 100/200 [03:39<03:56,  2.37s/it]Running Inference:  50%|█████     | 101/200 [03:41<03:24,  2.06s/it]Running Inference:  51%|█████     | 102/200 [03:42<03:11,  1.95s/it]Running Inference:  52%|█████▏    | 103/200 [03:46<03:44,  2.31s/it]Running Inference:  52%|█████▏    | 104/200 [03:48<03:45,  2.35s/it]Running Inference:  52%|█████▎    | 105/200 [03:49<02:56,  1.86s/it]Running Inference:  53%|█████▎    | 106/200 [03:51<03:13,  2.06s/it]Running Inference:  54%|█████▎    | 107/200 [03:54<03:42,  2.39s/it]Running Inference:  54%|█████▍    | 108/200 [03:57<03:32,  2.31s/it]Running Inference:  55%|█████▍    | 109/200 [03:59<03:45,  2.48s/it]Running Inference:  55%|█████▌    | 110/200 [04:01<03:15,  2.17s/it]Running Inference:  56%|█████▌    | 111/200 [04:02<02:46,  1.87s/it]Running Inference:  56%|█████▌    | 112/200 [04:03<02:21,  1.61s/it]Running Inference:  56%|█████▋    | 113/200 [04:05<02:18,  1.59s/it]Running Inference:  57%|█████▋    | 114/200 [04:06<02:12,  1.54s/it]Running Inference:  57%|█████▊    | 115/200 [04:09<02:37,  1.85s/it]Running Inference:  58%|█████▊    | 116/200 [04:11<02:51,  2.04s/it]Running Inference:  58%|█████▊    | 117/200 [04:13<02:36,  1.89s/it]Running Inference:  59%|█████▉    | 118/200 [04:13<02:08,  1.57s/it]Running Inference:  60%|█████▉    | 119/200 [04:15<02:13,  1.65s/it]Running Inference:  60%|██████    | 120/200 [04:20<03:19,  2.50s/it]Running Inference:  60%|██████    | 121/200 [04:23<03:38,  2.76s/it]Running Inference:  61%|██████    | 122/200 [04:26<03:45,  2.89s/it]Running Inference:  62%|██████▏   | 123/200 [04:27<02:56,  2.29s/it]Running Inference:  62%|██████▏   | 124/200 [04:30<02:59,  2.36s/it]Running Inference:  62%|██████▎   | 125/200 [04:32<02:45,  2.21s/it]Running Inference:  63%|██████▎   | 126/200 [04:32<02:12,  1.79s/it]Running Inference:  64%|██████▎   | 127/200 [04:35<02:24,  1.98s/it]Running Inference:  64%|██████▍   | 128/200 [04:37<02:26,  2.03s/it]Running Inference:  64%|██████▍   | 129/200 [04:40<02:42,  2.28s/it]Running Inference:  65%|██████▌   | 130/200 [04:41<02:14,  1.92s/it]Running Inference:  66%|██████▌   | 131/200 [04:44<02:44,  2.39s/it]Running Inference:  66%|██████▌   | 132/200 [04:47<02:52,  2.54s/it]Running Inference:  66%|██████▋   | 133/200 [04:49<02:24,  2.15s/it]Running Inference:  67%|██████▋   | 134/200 [04:51<02:34,  2.34s/it]Running Inference:  68%|██████▊   | 135/200 [04:53<02:20,  2.16s/it]Running Inference:  68%|██████▊   | 136/200 [04:55<02:11,  2.05s/it]Running Inference:  68%|██████▊   | 137/200 [04:57<02:14,  2.13s/it]Running Inference:  69%|██████▉   | 138/200 [04:59<02:08,  2.08s/it]Running Inference:  70%|██████▉   | 139/200 [05:02<02:28,  2.43s/it]Running Inference:  70%|███████   | 140/200 [05:05<02:31,  2.53s/it]Running Inference:  70%|███████   | 141/200 [05:07<02:14,  2.28s/it]Running Inference:  71%|███████   | 142/200 [05:09<02:16,  2.35s/it]Running Inference:  72%|███████▏  | 143/200 [05:13<02:31,  2.65s/it]Running Inference:  72%|███████▏  | 144/200 [05:15<02:17,  2.45s/it]Running Inference:  72%|███████▎  | 145/200 [05:17<02:13,  2.42s/it]Running Inference:  73%|███████▎  | 146/200 [05:19<02:08,  2.39s/it]Running Inference:  74%|███████▎  | 147/200 [05:23<02:19,  2.63s/it]Running Inference:  74%|███████▍  | 148/200 [05:25<02:12,  2.55s/it]Running Inference:  74%|███████▍  | 149/200 [05:26<01:48,  2.12s/it]Running Inference:  75%|███████▌  | 150/200 [05:28<01:51,  2.23s/it]Running Inference:  76%|███████▌  | 151/200 [05:29<01:29,  1.83s/it]Running Inference:  76%|███████▌  | 152/200 [05:32<01:35,  1.99s/it]Running Inference:  76%|███████▋  | 153/200 [05:32<01:14,  1.59s/it]Running Inference:  77%|███████▋  | 154/200 [05:35<01:28,  1.92s/it]Running Inference:  78%|███████▊  | 155/200 [05:37<01:21,  1.81s/it]Running Inference:  78%|███████▊  | 156/200 [05:39<01:21,  1.86s/it]Running Inference:  78%|███████▊  | 157/200 [05:40<01:11,  1.66s/it]Running Inference:  79%|███████▉  | 158/200 [05:42<01:15,  1.80s/it]Running Inference:  80%|███████▉  | 159/200 [05:44<01:14,  1.83s/it]Running Inference:  80%|████████  | 160/200 [05:47<01:32,  2.30s/it]Running Inference:  80%|████████  | 161/200 [05:50<01:32,  2.37s/it]Running Inference:  81%|████████  | 162/200 [05:52<01:26,  2.28s/it]Running Inference:  82%|████████▏ | 163/200 [05:56<01:50,  2.97s/it]Running Inference:  82%|████████▏ | 164/200 [05:58<01:33,  2.59s/it]Running Inference:  82%|████████▎ | 165/200 [06:01<01:28,  2.52s/it]Running Inference:  83%|████████▎ | 166/200 [06:04<01:32,  2.71s/it]Running Inference:  84%|████████▎ | 167/200 [06:05<01:18,  2.39s/it]Running Inference:  84%|████████▍ | 168/200 [06:08<01:20,  2.51s/it]Running Inference:  84%|████████▍ | 169/200 [06:09<01:01,  1.99s/it]Running Inference:  85%|████████▌ | 170/200 [06:11<01:02,  2.09s/it]Running Inference:  86%|████████▌ | 171/200 [06:14<01:02,  2.16s/it]Running Inference:  86%|████████▌ | 172/200 [06:15<00:51,  1.85s/it]Running Inference:  86%|████████▋ | 173/200 [06:17<00:53,  1.97s/it]Running Inference:  87%|████████▋ | 174/200 [06:19<00:49,  1.89s/it]Running Inference:  88%|████████▊ | 175/200 [06:21<00:50,  2.01s/it]Running Inference:  88%|████████▊ | 176/200 [06:23<00:46,  1.93s/it]Running Inference:  88%|████████▊ | 177/200 [06:24<00:42,  1.84s/it]Running Inference:  89%|████████▉ | 178/200 [06:27<00:44,  2.03s/it]Running Inference:  90%|████████▉ | 179/200 [06:29<00:46,  2.20s/it]Running Inference:  90%|█████████ | 180/200 [06:32<00:43,  2.20s/it]Running Inference:  90%|█████████ | 181/200 [06:33<00:37,  1.98s/it]Running Inference:  91%|█████████ | 182/200 [06:35<00:36,  2.03s/it]Running Inference:  92%|█████████▏| 183/200 [06:36<00:30,  1.77s/it]Running Inference:  92%|█████████▏| 184/200 [06:40<00:36,  2.27s/it]Running Inference:  92%|█████████▎| 185/200 [06:42<00:32,  2.15s/it]Running Inference:  93%|█████████▎| 186/200 [06:46<00:39,  2.85s/it]Running Inference:  94%|█████████▎| 187/200 [06:49<00:37,  2.86s/it]Running Inference:  94%|█████████▍| 188/200 [06:52<00:33,  2.78s/it]Running Inference:  94%|█████████▍| 189/200 [06:53<00:24,  2.25s/it]Running Inference:  95%|█████████▌| 190/200 [06:56<00:24,  2.48s/it]Running Inference:  96%|█████████▌| 191/200 [06:59<00:23,  2.65s/it]Running Inference:  96%|█████████▌| 192/200 [07:00<00:17,  2.19s/it]Running Inference:  96%|█████████▋| 193/200 [07:01<00:13,  1.92s/it]Running Inference:  97%|█████████▋| 194/200 [07:04<00:12,  2.13s/it]Running Inference:  98%|█████████▊| 195/200 [07:06<00:10,  2.17s/it]Running Inference:  98%|█████████▊| 196/200 [07:07<00:07,  1.93s/it]Running Inference:  98%|█████████▊| 197/200 [07:09<00:05,  1.77s/it]Running Inference:  99%|█████████▉| 198/200 [07:10<00:03,  1.61s/it]Running Inference: 100%|█████████▉| 199/200 [07:11<00:01,  1.56s/it]Running Inference: 100%|██████████| 200/200 [07:13<00:00,  1.61s/it]Running Inference: 100%|██████████| 200/200 [07:13<00:00,  2.17s/it]
2025-12-14 15:43:46,612 - INFO - Inference completed.
2025-12-14 15:43:46,620 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 15:43:46,620 - INFO - Calculating metrics for dataset: longbench
2025-12-14 15:43:46,621 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 15:43:46,621 - INFO - Metrics:
8.5
2025-12-14 15:43:46,623 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_count, ratio=0.3) on GPU 1

----------------------------------------
Task: passage_count | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 15:43:53,110 - INFO - Set deterministic seeds to 42
2025-12-14 15:43:53,110 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 15:43:53,110 - INFO - Starting evaluation run...
2025-12-14 15:43:53,110 - INFO - Output directory set to: longbenchresult
2025-12-14 15:43:53,111 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 15:43:53,111 - INFO - KV Press 'tova' setup.
2025-12-14 15:43:53,111 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 15:43:53,111 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.93it/s]
Device set to use cuda:0
2025-12-14 15:44:06,230 - INFO - Model pipeline loaded.
2025-12-14 15:44:06,230 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 15:44:12,781 - INFO - Dataset loaded with 200 entries.
2025-12-14 15:44:12,781 - INFO - Dataset processed with 200 entries.
2025-12-14 15:44:12,819 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:32,  1.97s/it]Running Inference:   1%|          | 2/200 [00:04<07:06,  2.15s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:11,  1.89s/it]Running Inference:   2%|▏         | 4/200 [00:06<04:52,  1.49s/it]Running Inference:   2%|▎         | 5/200 [00:11<08:20,  2.57s/it]Running Inference:   3%|▎         | 6/200 [00:13<07:31,  2.33s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:22,  1.98s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:15,  1.64s/it]Running Inference:   4%|▍         | 9/200 [00:18<06:29,  2.04s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:34,  2.08s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:41,  2.44s/it]Running Inference:   6%|▌         | 12/200 [00:27<08:47,  2.81s/it]Running Inference:   6%|▋         | 13/200 [00:28<07:33,  2.42s/it]Running Inference:   7%|▋         | 14/200 [00:29<06:11,  2.00s/it]Running Inference:   8%|▊         | 15/200 [00:33<07:22,  2.39s/it]Running Inference:   8%|▊         | 16/200 [00:35<06:56,  2.26s/it]Running Inference:   8%|▊         | 17/200 [00:36<06:12,  2.03s/it]Running Inference:   9%|▉         | 18/200 [00:38<05:56,  1.96s/it]Running Inference:  10%|▉         | 19/200 [00:40<05:41,  1.88s/it]Running Inference:  10%|█         | 20/200 [00:42<05:53,  1.96s/it]Running Inference:  10%|█         | 21/200 [00:45<06:51,  2.30s/it]Running Inference:  11%|█         | 22/200 [00:47<06:40,  2.25s/it]Running Inference:  12%|█▏        | 23/200 [00:50<06:57,  2.36s/it]Running Inference:  12%|█▏        | 24/200 [00:52<06:57,  2.37s/it]Running Inference:  12%|█▎        | 25/200 [00:54<06:51,  2.35s/it]Running Inference:  13%|█▎        | 26/200 [00:56<06:44,  2.32s/it]Running Inference:  14%|█▎        | 27/200 [00:58<06:17,  2.18s/it]Running Inference:  14%|█▍        | 28/200 [01:03<08:14,  2.88s/it]Running Inference:  14%|█▍        | 29/200 [01:06<08:29,  2.98s/it]Running Inference:  15%|█▌        | 30/200 [01:07<07:01,  2.48s/it]Running Inference:  16%|█▌        | 31/200 [01:09<06:06,  2.17s/it]Running Inference:  16%|█▌        | 32/200 [01:12<07:10,  2.56s/it]Running Inference:  16%|█▋        | 33/200 [01:13<05:44,  2.06s/it]Running Inference:  17%|█▋        | 34/200 [01:16<06:00,  2.17s/it]Running Inference:  18%|█▊        | 35/200 [01:18<06:02,  2.19s/it]Running Inference:  18%|█▊        | 36/200 [01:20<06:12,  2.27s/it]Running Inference:  18%|█▊        | 37/200 [01:22<05:49,  2.15s/it]Running Inference:  19%|█▉        | 38/200 [01:24<05:22,  1.99s/it]Running Inference:  20%|█▉        | 39/200 [01:26<05:46,  2.15s/it]Running Inference:  20%|██        | 40/200 [01:28<05:23,  2.02s/it]Running Inference:  20%|██        | 41/200 [01:30<05:18,  2.00s/it]Running Inference:  21%|██        | 42/200 [01:33<06:15,  2.38s/it]Running Inference:  22%|██▏       | 43/200 [01:35<06:02,  2.31s/it]Running Inference:  22%|██▏       | 44/200 [01:37<05:47,  2.23s/it]Running Inference:  22%|██▎       | 45/200 [01:40<06:15,  2.42s/it]Running Inference:  23%|██▎       | 46/200 [01:43<06:09,  2.40s/it]Running Inference:  24%|██▎       | 47/200 [01:44<05:09,  2.03s/it]Running Inference:  24%|██▍       | 48/200 [01:45<04:16,  1.69s/it]Running Inference:  24%|██▍       | 49/200 [01:46<04:06,  1.63s/it]Running Inference:  25%|██▌       | 50/200 [01:47<03:40,  1.47s/it]Running Inference:  26%|██▌       | 51/200 [01:50<04:17,  1.73s/it]Running Inference:  26%|██▌       | 52/200 [01:51<04:02,  1.64s/it]Running Inference:  26%|██▋       | 53/200 [01:54<04:51,  1.98s/it]Running Inference:  27%|██▋       | 54/200 [01:56<04:59,  2.05s/it]Running Inference:  28%|██▊       | 55/200 [01:57<04:14,  1.75s/it]Running Inference:  28%|██▊       | 56/200 [01:59<04:17,  1.79s/it]Running Inference:  28%|██▊       | 57/200 [02:03<05:54,  2.48s/it]Running Inference:  29%|██▉       | 58/200 [02:05<05:36,  2.37s/it]Running Inference:  30%|██▉       | 59/200 [02:09<06:21,  2.70s/it]Running Inference:  30%|███       | 60/200 [02:11<06:24,  2.75s/it]Running Inference:  30%|███       | 61/200 [02:13<05:30,  2.38s/it]Running Inference:  31%|███       | 62/200 [02:14<04:31,  1.97s/it]Running Inference:  32%|███▏      | 63/200 [02:18<05:51,  2.57s/it]Running Inference:  32%|███▏      | 64/200 [02:21<05:53,  2.60s/it]Running Inference:  32%|███▎      | 65/200 [02:23<05:43,  2.54s/it]Running Inference:  33%|███▎      | 66/200 [02:25<05:35,  2.50s/it]Running Inference:  34%|███▎      | 67/200 [02:27<04:59,  2.25s/it]Running Inference:  34%|███▍      | 68/200 [02:29<04:51,  2.21s/it]Running Inference:  34%|███▍      | 69/200 [02:33<05:31,  2.53s/it]Running Inference:  35%|███▌      | 70/200 [02:35<05:11,  2.39s/it]Running Inference:  36%|███▌      | 71/200 [02:37<05:18,  2.47s/it]Running Inference:  36%|███▌      | 72/200 [02:39<04:57,  2.32s/it]Running Inference:  36%|███▋      | 73/200 [02:42<05:24,  2.56s/it]Running Inference:  37%|███▋      | 74/200 [02:45<05:17,  2.52s/it]Running Inference:  38%|███▊      | 75/200 [02:46<04:41,  2.25s/it]Running Inference:  38%|███▊      | 76/200 [02:48<04:01,  1.95s/it]Running Inference:  38%|███▊      | 77/200 [02:52<05:26,  2.65s/it]Running Inference:  39%|███▉      | 78/200 [02:54<05:01,  2.47s/it]Running Inference:  40%|███▉      | 79/200 [02:56<04:54,  2.43s/it]Running Inference:  40%|████      | 80/200 [02:59<04:50,  2.42s/it]Running Inference:  40%|████      | 81/200 [03:00<04:08,  2.09s/it]Running Inference:  41%|████      | 82/200 [03:01<03:34,  1.82s/it]Running Inference:  42%|████▏     | 83/200 [03:04<03:52,  1.99s/it]Running Inference:  42%|████▏     | 84/200 [03:06<03:48,  1.97s/it]Running Inference:  42%|████▎     | 85/200 [03:08<04:17,  2.24s/it]Running Inference:  43%|████▎     | 86/200 [03:11<04:17,  2.26s/it]Running Inference:  44%|████▎     | 87/200 [03:13<04:04,  2.17s/it]Running Inference:  44%|████▍     | 88/200 [03:15<04:03,  2.17s/it]Running Inference:  44%|████▍     | 89/200 [03:18<04:22,  2.37s/it]Running Inference:  45%|████▌     | 90/200 [03:20<04:32,  2.48s/it]Running Inference:  46%|████▌     | 91/200 [03:22<03:47,  2.09s/it]Running Inference:  46%|████▌     | 92/200 [03:23<03:08,  1.75s/it]Running Inference:  46%|████▋     | 93/200 [03:24<03:02,  1.71s/it]Running Inference:  47%|████▋     | 94/200 [03:26<03:05,  1.75s/it]Running Inference:  48%|████▊     | 95/200 [03:28<03:18,  1.89s/it]Running Inference:  48%|████▊     | 96/200 [03:30<03:17,  1.90s/it]Running Inference:  48%|████▊     | 97/200 [03:33<03:32,  2.06s/it]Running Inference:  49%|████▉     | 98/200 [03:36<04:16,  2.51s/it]Running Inference:  50%|████▉     | 99/200 [03:39<04:26,  2.64s/it]Running Inference:  50%|█████     | 100/200 [03:41<03:55,  2.36s/it]Running Inference:  50%|█████     | 101/200 [03:42<03:22,  2.04s/it]Running Inference:  51%|█████     | 102/200 [03:45<03:41,  2.26s/it]Running Inference:  52%|█████▏    | 103/200 [03:48<04:04,  2.52s/it]Running Inference:  52%|█████▏    | 104/200 [03:50<03:59,  2.49s/it]Running Inference:  52%|█████▎    | 105/200 [03:51<03:05,  1.95s/it]Running Inference:  53%|█████▎    | 106/200 [03:54<03:19,  2.12s/it]Running Inference:  54%|█████▎    | 107/200 [03:57<03:44,  2.42s/it]Running Inference:  54%|█████▍    | 108/200 [03:59<03:34,  2.33s/it]Running Inference:  55%|█████▍    | 109/200 [04:02<03:46,  2.49s/it]Running Inference:  55%|█████▌    | 110/200 [04:03<03:15,  2.18s/it]Running Inference:  56%|█████▌    | 111/200 [04:05<03:00,  2.03s/it]Running Inference:  56%|█████▌    | 112/200 [04:06<02:31,  1.72s/it]Running Inference:  56%|█████▋    | 113/200 [04:07<02:24,  1.67s/it]Running Inference:  57%|█████▋    | 114/200 [04:09<02:17,  1.59s/it]Running Inference:  57%|█████▊    | 115/200 [04:11<02:40,  1.89s/it]Running Inference:  58%|█████▊    | 116/200 [04:14<02:53,  2.06s/it]Running Inference:  58%|█████▊    | 117/200 [04:15<02:37,  1.90s/it]Running Inference:  59%|█████▉    | 118/200 [04:16<02:09,  1.58s/it]Running Inference:  60%|█████▉    | 119/200 [04:18<02:14,  1.66s/it]Running Inference:  60%|██████    | 120/200 [04:23<03:19,  2.49s/it]Running Inference:  60%|██████    | 121/200 [04:26<03:36,  2.74s/it]Running Inference:  61%|██████    | 122/200 [04:29<03:43,  2.87s/it]Running Inference:  62%|██████▏   | 123/200 [04:30<02:55,  2.28s/it]Running Inference:  62%|██████▏   | 124/200 [04:32<02:58,  2.35s/it]Running Inference:  62%|██████▎   | 125/200 [04:34<02:44,  2.20s/it]Running Inference:  63%|██████▎   | 126/200 [04:35<02:12,  1.79s/it]Running Inference:  64%|██████▎   | 127/200 [04:37<02:23,  1.97s/it]Running Inference:  64%|██████▍   | 128/200 [04:40<02:26,  2.03s/it]Running Inference:  64%|██████▍   | 129/200 [04:42<02:41,  2.28s/it]Running Inference:  65%|██████▌   | 130/200 [04:44<02:14,  1.92s/it]Running Inference:  66%|██████▌   | 131/200 [04:47<02:44,  2.38s/it]Running Inference:  66%|██████▌   | 132/200 [04:50<02:52,  2.53s/it]Running Inference:  66%|██████▋   | 133/200 [04:51<02:23,  2.14s/it]Running Inference:  67%|██████▋   | 134/200 [04:54<02:33,  2.33s/it]Running Inference:  68%|██████▊   | 135/200 [04:56<02:18,  2.14s/it]Running Inference:  68%|██████▊   | 136/200 [04:57<02:10,  2.04s/it]Running Inference:  68%|██████▊   | 137/200 [05:00<02:13,  2.12s/it]Running Inference:  69%|██████▉   | 138/200 [05:02<02:08,  2.07s/it]Running Inference:  70%|██████▉   | 139/200 [05:05<02:27,  2.42s/it]Running Inference:  70%|███████   | 140/200 [05:08<02:31,  2.52s/it]Running Inference:  70%|███████   | 141/200 [05:09<02:13,  2.27s/it]Running Inference:  71%|███████   | 142/200 [05:12<02:15,  2.34s/it]Running Inference:  72%|███████▏  | 143/200 [05:15<02:30,  2.64s/it]Running Inference:  72%|███████▏  | 144/200 [05:17<02:16,  2.43s/it]Running Inference:  72%|███████▎  | 145/200 [05:19<02:12,  2.41s/it]Running Inference:  73%|███████▎  | 146/200 [05:22<02:08,  2.38s/it]Running Inference:  74%|███████▎  | 147/200 [05:25<02:19,  2.63s/it]Running Inference:  74%|███████▍  | 148/200 [05:27<02:12,  2.55s/it]Running Inference:  74%|███████▍  | 149/200 [05:28<01:47,  2.12s/it]Running Inference:  75%|███████▌  | 150/200 [05:30<01:35,  1.91s/it]Running Inference:  76%|███████▌  | 151/200 [05:31<01:18,  1.60s/it]Running Inference:  76%|███████▌  | 152/200 [05:33<01:27,  1.83s/it]Running Inference:  76%|███████▋  | 153/200 [05:34<01:09,  1.48s/it]Running Inference:  77%|███████▋  | 154/200 [05:37<01:24,  1.84s/it]Running Inference:  78%|███████▊  | 155/200 [05:38<01:18,  1.75s/it]Running Inference:  78%|███████▊  | 156/200 [05:40<01:19,  1.80s/it]Running Inference:  78%|███████▊  | 157/200 [05:41<01:09,  1.62s/it]Running Inference:  79%|███████▉  | 158/200 [05:43<01:14,  1.76s/it]Running Inference:  80%|███████▉  | 159/200 [05:45<01:13,  1.80s/it]Running Inference:  80%|████████  | 160/200 [05:49<01:31,  2.28s/it]Running Inference:  80%|████████  | 161/200 [05:51<01:31,  2.36s/it]Running Inference:  81%|████████  | 162/200 [05:53<01:26,  2.27s/it]Running Inference:  82%|████████▏ | 163/200 [05:57<01:40,  2.71s/it]Running Inference:  82%|████████▏ | 164/200 [05:59<01:26,  2.40s/it]Running Inference:  82%|████████▎ | 165/200 [06:01<01:23,  2.39s/it]Running Inference:  83%|████████▎ | 166/200 [06:04<01:28,  2.61s/it]Running Inference:  84%|████████▎ | 167/200 [06:06<01:16,  2.32s/it]Running Inference:  84%|████████▍ | 168/200 [06:08<01:18,  2.47s/it]Running Inference:  84%|████████▍ | 169/200 [06:09<01:00,  1.95s/it]Running Inference:  85%|████████▌ | 170/200 [06:12<01:02,  2.07s/it]Running Inference:  86%|████████▌ | 171/200 [06:14<01:02,  2.14s/it]Running Inference:  86%|████████▌ | 172/200 [06:15<00:51,  1.84s/it]Running Inference:  86%|████████▋ | 173/200 [06:17<00:52,  1.95s/it]Running Inference:  87%|████████▋ | 174/200 [06:19<00:48,  1.88s/it]Running Inference:  88%|████████▊ | 175/200 [06:21<00:50,  2.00s/it]Running Inference:  88%|████████▊ | 176/200 [06:23<00:46,  1.92s/it]Running Inference:  88%|████████▊ | 177/200 [06:25<00:42,  1.83s/it]Running Inference:  89%|████████▉ | 178/200 [06:27<00:44,  2.02s/it]Running Inference:  90%|████████▉ | 179/200 [06:30<00:45,  2.19s/it]Running Inference:  90%|█████████ | 180/200 [06:32<00:43,  2.19s/it]Running Inference:  90%|█████████ | 181/200 [06:33<00:37,  1.98s/it]Running Inference:  91%|█████████ | 182/200 [06:35<00:36,  2.02s/it]Running Inference:  92%|█████████▏| 183/200 [06:37<00:29,  1.76s/it]Running Inference:  92%|█████████▏| 184/200 [06:40<00:36,  2.26s/it]Running Inference:  92%|█████████▎| 185/200 [06:42<00:32,  2.14s/it]Running Inference:  93%|█████████▎| 186/200 [06:46<00:38,  2.73s/it]Running Inference:  94%|█████████▎| 187/200 [06:49<00:36,  2.77s/it]Running Inference:  94%|█████████▍| 188/200 [06:51<00:32,  2.72s/it]Running Inference:  94%|█████████▍| 189/200 [06:52<00:24,  2.21s/it]Running Inference:  95%|█████████▌| 190/200 [06:56<00:24,  2.45s/it]Running Inference:  96%|█████████▌| 191/200 [06:59<00:23,  2.62s/it]Running Inference:  96%|█████████▌| 192/200 [07:00<00:17,  2.18s/it]Running Inference:  96%|█████████▋| 193/200 [07:01<00:13,  1.90s/it]Running Inference:  97%|█████████▋| 194/200 [07:04<00:12,  2.12s/it]Running Inference:  98%|█████████▊| 195/200 [07:06<00:10,  2.16s/it]Running Inference:  98%|█████████▊| 196/200 [07:07<00:07,  1.92s/it]Running Inference:  98%|█████████▊| 197/200 [07:09<00:05,  1.77s/it]Running Inference:  99%|█████████▉| 198/200 [07:10<00:03,  1.61s/it]Running Inference: 100%|█████████▉| 199/200 [07:11<00:01,  1.55s/it]Running Inference: 100%|██████████| 200/200 [07:13<00:00,  1.61s/it]Running Inference: 100%|██████████| 200/200 [07:13<00:00,  2.17s/it]
2025-12-14 15:51:26,312 - INFO - Inference completed.
2025-12-14 15:51:26,320 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 15:51:26,320 - INFO - Calculating metrics for dataset: longbench
2025-12-14 15:51:26,321 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 15:51:26,321 - INFO - Metrics:
10.64
2025-12-14 15:51:26,322 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_count, ratio=0.5) on GPU 1


========================================
LongBench Task: passage_retrieval_en
========================================
----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 15:51:32,767 - INFO - Set deterministic seeds to 42
2025-12-14 15:51:32,768 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 15:51:32,768 - INFO - Starting evaluation run...
2025-12-14 15:51:32,768 - INFO - Output directory set to: longbenchresult
2025-12-14 15:51:32,768 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 15:51:32,768 - INFO - KV Press 'tova' setup.
2025-12-14 15:51:32,768 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 15:51:32,768 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.42it/s]
Device set to use cuda:0
2025-12-14 15:51:47,207 - INFO - Model pipeline loaded.
2025-12-14 15:51:47,207 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 2649.58 examples/s]
2025-12-14 15:51:53,541 - INFO - Dataset loaded with 200 entries.
2025-12-14 15:51:53,541 - INFO - Dataset processed with 200 entries.
2025-12-14 15:51:53,572 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:37,  4.41s/it]Running Inference:   1%|          | 2/200 [00:07<12:51,  3.90s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:53,  3.01s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:41,  2.96s/it]Running Inference:   2%|▎         | 5/200 [00:14<08:17,  2.55s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:16,  2.87s/it]Running Inference:   4%|▎         | 7/200 [00:21<10:16,  3.19s/it]Running Inference:   4%|▍         | 8/200 [00:25<10:45,  3.36s/it]Running Inference:   4%|▍         | 9/200 [00:27<09:11,  2.89s/it]Running Inference:   5%|▌         | 10/200 [00:30<09:42,  3.06s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:34<10:04,  3.20s/it]Running Inference:   6%|▌         | 12/200 [00:36<08:48,  2.81s/it]Running Inference:   6%|▋         | 13/200 [00:40<09:33,  3.07s/it]Running Inference:   7%|▋         | 14/200 [00:43<10:04,  3.25s/it]Running Inference:   8%|▊         | 15/200 [00:46<09:52,  3.20s/it]Running Inference:   8%|▊         | 16/200 [00:50<10:14,  3.34s/it]Running Inference:   8%|▊         | 17/200 [00:54<10:30,  3.44s/it]Running Inference:   9%|▉         | 18/200 [00:57<10:35,  3.49s/it]Running Inference:  10%|▉         | 19/200 [01:01<10:43,  3.55s/it]Running Inference:  10%|█         | 20/200 [01:03<09:01,  3.01s/it]Running Inference:  10%|█         | 21/200 [01:07<09:45,  3.27s/it]Running Inference:  11%|█         | 22/200 [01:10<10:01,  3.38s/it]Running Inference:  12%|█▏        | 23/200 [01:14<10:06,  3.42s/it]Running Inference:  12%|█▏        | 24/200 [01:17<10:07,  3.45s/it]Running Inference:  12%|█▎        | 25/200 [01:20<09:24,  3.23s/it]Running Inference:  13%|█▎        | 26/200 [01:22<08:03,  2.78s/it]Running Inference:  14%|█▎        | 27/200 [01:26<09:09,  3.18s/it]Running Inference:  14%|█▍        | 28/200 [01:29<09:17,  3.24s/it]Running Inference:  14%|█▍        | 29/200 [01:33<09:25,  3.31s/it]Running Inference:  15%|█▌        | 30/200 [01:36<09:39,  3.41s/it]Running Inference:  16%|█▌        | 31/200 [01:40<10:06,  3.59s/it]Running Inference:  16%|█▌        | 32/200 [01:44<10:18,  3.68s/it]Running Inference:  16%|█▋        | 33/200 [01:48<10:17,  3.70s/it]Running Inference:  17%|█▋        | 34/200 [01:51<09:47,  3.54s/it]Running Inference:  18%|█▊        | 35/200 [01:55<09:59,  3.63s/it]Running Inference:  18%|█▊        | 36/200 [01:58<09:46,  3.58s/it]Running Inference:  18%|█▊        | 37/200 [02:02<09:50,  3.62s/it]Running Inference:  19%|█▉        | 38/200 [02:06<09:53,  3.66s/it]Running Inference:  20%|█▉        | 39/200 [02:09<09:19,  3.48s/it]Running Inference:  20%|██        | 40/200 [02:13<09:26,  3.54s/it]Running Inference:  20%|██        | 41/200 [02:17<09:39,  3.64s/it]Running Inference:  21%|██        | 42/200 [02:20<09:26,  3.58s/it]Running Inference:  22%|██▏       | 43/200 [02:24<09:38,  3.68s/it]Running Inference:  22%|██▏       | 44/200 [02:26<08:08,  3.13s/it]Running Inference:  22%|██▎       | 45/200 [02:29<08:09,  3.16s/it]Running Inference:  23%|██▎       | 46/200 [02:32<08:07,  3.17s/it]Running Inference:  24%|██▎       | 47/200 [02:35<07:43,  3.03s/it]Running Inference:  24%|██▍       | 48/200 [02:38<08:00,  3.16s/it]Running Inference:  24%|██▍       | 49/200 [02:42<08:31,  3.39s/it]Running Inference:  25%|██▌       | 50/200 [02:46<08:40,  3.47s/it]Running Inference:  26%|██▌       | 51/200 [02:50<08:46,  3.53s/it]Running Inference:  26%|██▌       | 52/200 [02:52<08:12,  3.33s/it]Running Inference:  26%|██▋       | 53/200 [02:56<08:12,  3.35s/it]Running Inference:  27%|██▋       | 54/200 [02:59<07:53,  3.24s/it]Running Inference:  28%|██▊       | 55/200 [03:02<07:30,  3.10s/it]Running Inference:  28%|██▊       | 56/200 [03:05<07:54,  3.30s/it]Running Inference:  28%|██▊       | 57/200 [03:09<07:57,  3.34s/it]Running Inference:  29%|██▉       | 58/200 [03:12<07:36,  3.22s/it]Running Inference:  30%|██▉       | 59/200 [03:16<08:02,  3.42s/it]Running Inference:  30%|███       | 60/200 [03:19<08:09,  3.50s/it]Running Inference:  30%|███       | 61/200 [03:23<08:09,  3.52s/it]Running Inference:  31%|███       | 62/200 [03:26<07:58,  3.47s/it]Running Inference:  32%|███▏      | 63/200 [03:30<08:01,  3.51s/it]Running Inference:  32%|███▏      | 64/200 [03:33<08:01,  3.54s/it]Running Inference:  32%|███▎      | 65/200 [03:35<06:36,  2.94s/it]Running Inference:  33%|███▎      | 66/200 [03:39<07:15,  3.25s/it]Running Inference:  34%|███▎      | 67/200 [03:42<07:19,  3.30s/it]Running Inference:  34%|███▍      | 68/200 [03:46<07:17,  3.32s/it]Running Inference:  34%|███▍      | 69/200 [03:50<07:37,  3.50s/it]Running Inference:  35%|███▌      | 70/200 [03:54<07:49,  3.61s/it]Running Inference:  36%|███▌      | 71/200 [03:57<07:57,  3.70s/it]Running Inference:  36%|███▌      | 72/200 [04:01<07:40,  3.60s/it]Running Inference:  36%|███▋      | 73/200 [04:04<07:35,  3.58s/it]Running Inference:  37%|███▋      | 74/200 [04:08<07:36,  3.62s/it]Running Inference:  38%|███▊      | 75/200 [04:12<07:29,  3.60s/it]Running Inference:  38%|███▊      | 76/200 [04:13<06:01,  2.92s/it]Running Inference:  38%|███▊      | 77/200 [04:17<06:27,  3.15s/it]Running Inference:  39%|███▉      | 78/200 [04:20<06:26,  3.17s/it]Running Inference:  40%|███▉      | 79/200 [04:24<06:42,  3.32s/it]Running Inference:  40%|████      | 80/200 [04:25<05:37,  2.81s/it]Running Inference:  40%|████      | 81/200 [04:27<04:49,  2.43s/it]Running Inference:  41%|████      | 82/200 [04:30<05:21,  2.73s/it]Running Inference:  42%|████▏     | 83/200 [04:34<05:53,  3.02s/it]Running Inference:  42%|████▏     | 84/200 [04:37<06:09,  3.18s/it]Running Inference:  42%|████▎     | 85/200 [04:41<06:21,  3.31s/it]Running Inference:  43%|████▎     | 86/200 [04:45<06:28,  3.41s/it]Running Inference:  44%|████▎     | 87/200 [04:48<06:36,  3.51s/it]Running Inference:  44%|████▍     | 88/200 [04:52<06:33,  3.52s/it]Running Inference:  44%|████▍     | 89/200 [04:55<06:30,  3.52s/it]Running Inference:  45%|████▌     | 90/200 [04:59<06:27,  3.53s/it]Running Inference:  46%|████▌     | 91/200 [05:01<05:22,  2.96s/it]Running Inference:  46%|████▌     | 92/200 [05:04<05:47,  3.21s/it]Running Inference:  46%|████▋     | 93/200 [05:08<05:53,  3.31s/it]Running Inference:  47%|████▋     | 94/200 [05:12<06:08,  3.48s/it]Running Inference:  48%|████▊     | 95/200 [05:15<06:09,  3.52s/it]Running Inference:  48%|████▊     | 96/200 [05:19<06:07,  3.54s/it]Running Inference:  48%|████▊     | 97/200 [05:21<05:06,  2.97s/it]Running Inference:  49%|████▉     | 98/200 [05:25<05:30,  3.24s/it]Running Inference:  50%|████▉     | 99/200 [05:28<05:37,  3.34s/it]Running Inference:  50%|█████     | 100/200 [05:31<05:31,  3.32s/it]Running Inference:  50%|█████     | 101/200 [05:35<05:34,  3.38s/it]Running Inference:  51%|█████     | 102/200 [05:39<05:38,  3.45s/it]Running Inference:  52%|█████▏    | 103/200 [05:42<05:38,  3.49s/it]Running Inference:  52%|█████▏    | 104/200 [05:44<04:39,  2.91s/it]Running Inference:  52%|█████▎    | 105/200 [05:47<04:58,  3.15s/it]Running Inference:  53%|█████▎    | 106/200 [05:49<04:13,  2.69s/it]Running Inference:  54%|█████▎    | 107/200 [05:52<04:31,  2.92s/it]Running Inference:  54%|█████▍    | 108/200 [05:56<04:48,  3.14s/it]Running Inference:  55%|█████▍    | 109/200 [05:59<04:33,  3.01s/it]Running Inference:  55%|█████▌    | 110/200 [06:01<04:06,  2.74s/it]Running Inference:  56%|█████▌    | 111/200 [06:04<04:24,  2.97s/it]Running Inference:  56%|█████▌    | 112/200 [06:08<04:38,  3.17s/it]Running Inference:  56%|█████▋    | 113/200 [06:11<04:41,  3.24s/it]Running Inference:  57%|█████▋    | 114/200 [06:15<04:38,  3.23s/it]Running Inference:  57%|█████▊    | 115/200 [06:18<04:44,  3.35s/it]Running Inference:  58%|█████▊    | 116/200 [06:21<04:28,  3.19s/it]Running Inference:  58%|█████▊    | 117/200 [06:23<03:42,  2.68s/it]Running Inference:  59%|█████▉    | 118/200 [06:26<04:06,  3.01s/it]Running Inference:  60%|█████▉    | 119/200 [06:30<04:24,  3.27s/it]Running Inference:  60%|██████    | 120/200 [06:34<04:36,  3.46s/it]Running Inference:  60%|██████    | 121/200 [06:38<04:38,  3.52s/it]Running Inference:  61%|██████    | 122/200 [06:42<04:41,  3.61s/it]Running Inference:  62%|██████▏   | 123/200 [06:45<04:39,  3.63s/it]Running Inference:  62%|██████▏   | 124/200 [06:49<04:27,  3.51s/it]Running Inference:  62%|██████▎   | 125/200 [06:52<04:26,  3.55s/it]Running Inference:  63%|██████▎   | 126/200 [06:56<04:24,  3.57s/it]Running Inference:  64%|██████▎   | 127/200 [06:59<04:21,  3.59s/it]Running Inference:  64%|██████▍   | 128/200 [07:03<04:16,  3.57s/it]Running Inference:  64%|██████▍   | 129/200 [07:04<03:25,  2.90s/it]Running Inference:  65%|██████▌   | 130/200 [07:08<03:39,  3.13s/it]Running Inference:  66%|██████▌   | 131/200 [07:11<03:26,  2.99s/it]Running Inference:  66%|██████▌   | 132/200 [07:15<03:41,  3.26s/it]Running Inference:  66%|██████▋   | 133/200 [07:18<03:41,  3.31s/it]Running Inference:  67%|██████▋   | 134/200 [07:22<03:46,  3.43s/it]Running Inference:  68%|██████▊   | 135/200 [07:25<03:46,  3.49s/it]Running Inference:  68%|██████▊   | 136/200 [07:29<03:42,  3.47s/it]Running Inference:  68%|██████▊   | 137/200 [07:33<03:44,  3.56s/it]Running Inference:  69%|██████▉   | 138/200 [07:36<03:42,  3.59s/it]Running Inference:  70%|██████▉   | 139/200 [07:40<03:41,  3.62s/it]Running Inference:  70%|███████   | 140/200 [07:43<03:35,  3.59s/it]Running Inference:  70%|███████   | 141/200 [07:47<03:32,  3.60s/it]Running Inference:  71%|███████   | 142/200 [07:51<03:28,  3.59s/it]Running Inference:  72%|███████▏  | 143/200 [07:52<02:53,  3.04s/it]Running Inference:  72%|███████▏  | 144/200 [07:56<03:07,  3.35s/it]Running Inference:  72%|███████▎  | 145/200 [08:00<03:05,  3.37s/it]Running Inference:  73%|███████▎  | 146/200 [08:04<03:09,  3.50s/it]Running Inference:  74%|███████▎  | 147/200 [08:07<03:02,  3.45s/it]Running Inference:  74%|███████▍  | 148/200 [08:09<02:38,  3.05s/it]Running Inference:  74%|███████▍  | 149/200 [08:13<02:45,  3.24s/it]Running Inference:  75%|███████▌  | 150/200 [08:16<02:46,  3.33s/it]Running Inference:  76%|███████▌  | 151/200 [08:20<02:50,  3.48s/it]Running Inference:  76%|███████▌  | 152/200 [08:24<02:49,  3.53s/it]Running Inference:  76%|███████▋  | 153/200 [08:25<02:17,  2.94s/it]Running Inference:  77%|███████▋  | 154/200 [08:29<02:21,  3.07s/it]Running Inference:  78%|███████▊  | 155/200 [08:30<01:59,  2.66s/it]Running Inference:  78%|███████▊  | 156/200 [08:34<02:13,  3.04s/it]Running Inference:  78%|███████▊  | 157/200 [08:38<02:18,  3.23s/it]Running Inference:  79%|███████▉  | 158/200 [08:42<02:20,  3.34s/it]Running Inference:  80%|███████▉  | 159/200 [08:45<02:18,  3.39s/it]Running Inference:  80%|████████  | 160/200 [08:48<02:11,  3.28s/it]Running Inference:  80%|████████  | 161/200 [08:52<02:11,  3.37s/it]Running Inference:  81%|████████  | 162/200 [08:55<02:11,  3.47s/it]Running Inference:  82%|████████▏ | 163/200 [08:58<01:59,  3.22s/it]Running Inference:  82%|████████▏ | 164/200 [09:02<02:01,  3.39s/it]Running Inference:  82%|████████▎ | 165/200 [09:06<02:02,  3.51s/it]Running Inference:  83%|████████▎ | 166/200 [09:10<02:03,  3.63s/it]Running Inference:  84%|████████▎ | 167/200 [09:13<02:00,  3.65s/it]Running Inference:  84%|████████▍ | 168/200 [09:17<01:57,  3.67s/it]Running Inference:  84%|████████▍ | 169/200 [09:21<01:55,  3.72s/it]Running Inference:  85%|████████▌ | 170/200 [09:23<01:35,  3.18s/it]Running Inference:  86%|████████▌ | 171/200 [09:26<01:35,  3.28s/it]Running Inference:  86%|████████▌ | 172/200 [09:30<01:33,  3.36s/it]Running Inference:  86%|████████▋ | 173/200 [09:33<01:33,  3.46s/it]Running Inference:  87%|████████▋ | 174/200 [09:37<01:33,  3.59s/it]Running Inference:  88%|████████▊ | 175/200 [09:39<01:15,  3.01s/it]Running Inference:  88%|████████▊ | 176/200 [09:42<01:15,  3.14s/it]Running Inference:  88%|████████▊ | 177/200 [09:45<01:11,  3.11s/it]Running Inference:  89%|████████▉ | 178/200 [09:48<01:07,  3.06s/it]Running Inference:  90%|████████▉ | 179/200 [09:52<01:07,  3.20s/it]Running Inference:  90%|█████████ | 180/200 [09:56<01:06,  3.33s/it]Running Inference:  90%|█████████ | 181/200 [09:57<00:53,  2.79s/it]Running Inference:  91%|█████████ | 182/200 [10:01<00:54,  3.05s/it]Running Inference:  92%|█████████▏| 183/200 [10:04<00:55,  3.24s/it]Running Inference:  92%|█████████▏| 184/200 [10:06<00:44,  2.81s/it]Running Inference:  92%|█████████▎| 185/200 [10:10<00:46,  3.10s/it]Running Inference:  93%|█████████▎| 186/200 [10:14<00:45,  3.28s/it]Running Inference:  94%|█████████▎| 187/200 [10:17<00:44,  3.40s/it]Running Inference:  94%|█████████▍| 188/200 [10:21<00:43,  3.58s/it]Running Inference:  94%|█████████▍| 189/200 [10:25<00:39,  3.61s/it]Running Inference:  95%|█████████▌| 190/200 [10:29<00:35,  3.57s/it]Running Inference:  96%|█████████▌| 191/200 [10:32<00:32,  3.59s/it]Running Inference:  96%|█████████▌| 192/200 [10:36<00:29,  3.64s/it]Running Inference:  96%|█████████▋| 193/200 [10:40<00:25,  3.67s/it]Running Inference:  97%|█████████▋| 194/200 [10:44<00:22,  3.72s/it]Running Inference:  98%|█████████▊| 195/200 [10:47<00:18,  3.74s/it]Running Inference:  98%|█████████▊| 196/200 [10:51<00:14,  3.75s/it]Running Inference:  98%|█████████▊| 197/200 [10:55<00:11,  3.76s/it]Running Inference:  99%|█████████▉| 198/200 [10:58<00:07,  3.65s/it]Running Inference: 100%|█████████▉| 199/200 [11:01<00:03,  3.45s/it]Running Inference: 100%|██████████| 200/200 [11:05<00:00,  3.47s/it]Running Inference: 100%|██████████| 200/200 [11:05<00:00,  3.33s/it]
2025-12-14 16:02:58,868 - INFO - Inference completed.
2025-12-14 16:02:58,880 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 16:02:58,880 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:02:58,881 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 16:02:58,881 - INFO - Metrics:
49.88
2025-12-14 16:02:58,883 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_en, ratio=0.1) on GPU 1

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:03:05,330 - INFO - Set deterministic seeds to 42
2025-12-14 16:03:05,330 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:03:05,330 - INFO - Starting evaluation run...
2025-12-14 16:03:05,330 - INFO - Output directory set to: longbenchresult
2025-12-14 16:03:05,330 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 16:03:05,330 - INFO - KV Press 'tova' setup.
2025-12-14 16:03:05,330 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:03:05,330 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.34it/s]
Device set to use cuda:0
2025-12-14 16:03:21,181 - INFO - Model pipeline loaded.
2025-12-14 16:03:21,181 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 16:03:27,150 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:03:27,150 - INFO - Dataset processed with 200 entries.
2025-12-14 16:03:27,181 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:36,  4.41s/it]Running Inference:   1%|          | 2/200 [00:07<12:26,  3.77s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:39,  2.94s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:34,  2.93s/it]Running Inference:   2%|▎         | 5/200 [00:14<08:12,  2.53s/it]Running Inference:   3%|▎         | 6/200 [00:17<09:13,  2.85s/it]Running Inference:   4%|▎         | 7/200 [00:21<10:13,  3.18s/it]Running Inference:   4%|▍         | 8/200 [00:24<10:05,  3.15s/it]Running Inference:   4%|▍         | 9/200 [00:26<08:43,  2.74s/it]Running Inference:   5%|▌         | 10/200 [00:30<09:22,  2.96s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:33<09:51,  3.13s/it]Running Inference:   6%|▌         | 12/200 [00:36<09:44,  3.11s/it]Running Inference:   6%|▋         | 13/200 [00:40<10:12,  3.28s/it]Running Inference:   7%|▋         | 14/200 [00:43<10:19,  3.33s/it]Running Inference:   8%|▊         | 15/200 [00:47<10:24,  3.38s/it]Running Inference:   8%|▊         | 16/200 [00:50<10:37,  3.47s/it]Running Inference:   8%|▊         | 17/200 [00:52<08:54,  2.92s/it]Running Inference:   9%|▉         | 18/200 [00:56<09:28,  3.12s/it]Running Inference:  10%|▉         | 19/200 [00:59<09:56,  3.30s/it]Running Inference:  10%|█         | 20/200 [01:01<08:29,  2.83s/it]Running Inference:  10%|█         | 21/200 [01:05<09:23,  3.15s/it]Running Inference:  11%|█         | 22/200 [01:09<09:45,  3.29s/it]Running Inference:  12%|█▏        | 23/200 [01:12<09:53,  3.35s/it]Running Inference:  12%|█▏        | 24/200 [01:16<09:58,  3.40s/it]Running Inference:  12%|█▎        | 25/200 [01:18<09:18,  3.19s/it]Running Inference:  13%|█▎        | 26/200 [01:20<07:59,  2.76s/it]Running Inference:  14%|█▎        | 27/200 [01:24<09:06,  3.16s/it]Running Inference:  14%|█▍        | 28/200 [01:28<09:15,  3.23s/it]Running Inference:  14%|█▍        | 29/200 [01:31<09:24,  3.30s/it]Running Inference:  15%|█▌        | 30/200 [01:35<09:37,  3.40s/it]Running Inference:  16%|█▌        | 31/200 [01:39<10:00,  3.55s/it]Running Inference:  16%|█▌        | 32/200 [01:43<10:13,  3.65s/it]Running Inference:  16%|█▋        | 33/200 [01:46<10:15,  3.68s/it]Running Inference:  17%|█▋        | 34/200 [01:49<09:46,  3.53s/it]Running Inference:  18%|█▊        | 35/200 [01:53<09:57,  3.62s/it]Running Inference:  18%|█▊        | 36/200 [01:57<09:44,  3.57s/it]Running Inference:  18%|█▊        | 37/200 [02:00<09:50,  3.62s/it]Running Inference:  19%|█▉        | 38/200 [02:04<09:54,  3.67s/it]Running Inference:  20%|█▉        | 39/200 [02:07<09:21,  3.49s/it]Running Inference:  20%|██        | 40/200 [02:11<09:28,  3.56s/it]Running Inference:  20%|██        | 41/200 [02:15<09:41,  3.66s/it]Running Inference:  21%|██        | 42/200 [02:18<09:28,  3.60s/it]Running Inference:  22%|██▏       | 43/200 [02:22<09:40,  3.70s/it]Running Inference:  22%|██▏       | 44/200 [02:24<08:09,  3.14s/it]Running Inference:  22%|██▎       | 45/200 [02:28<08:25,  3.26s/it]Running Inference:  23%|██▎       | 46/200 [02:31<08:18,  3.24s/it]Running Inference:  24%|██▎       | 47/200 [02:34<08:30,  3.34s/it]Running Inference:  24%|██▍       | 48/200 [02:38<08:34,  3.38s/it]Running Inference:  24%|██▍       | 49/200 [02:42<08:56,  3.55s/it]Running Inference:  25%|██▌       | 50/200 [02:46<08:57,  3.58s/it]Running Inference:  26%|██▌       | 51/200 [02:49<08:43,  3.51s/it]Running Inference:  26%|██▌       | 52/200 [02:52<08:10,  3.32s/it]Running Inference:  26%|██▋       | 53/200 [02:56<08:34,  3.50s/it]Running Inference:  27%|██▋       | 54/200 [02:59<08:08,  3.35s/it]Running Inference:  28%|██▊       | 55/200 [03:02<07:46,  3.22s/it]Running Inference:  28%|██▊       | 56/200 [03:05<08:06,  3.38s/it]Running Inference:  28%|██▊       | 57/200 [03:09<08:06,  3.40s/it]Running Inference:  29%|██▉       | 58/200 [03:12<07:43,  3.27s/it]Running Inference:  30%|██▉       | 59/200 [03:16<08:07,  3.46s/it]Running Inference:  30%|███       | 60/200 [03:19<08:13,  3.53s/it]Running Inference:  30%|███       | 61/200 [03:23<08:13,  3.55s/it]Running Inference:  31%|███       | 62/200 [03:27<08:24,  3.66s/it]Running Inference:  32%|███▏      | 63/200 [03:30<08:19,  3.65s/it]Running Inference:  32%|███▏      | 64/200 [03:34<08:15,  3.64s/it]Running Inference:  32%|███▎      | 65/200 [03:38<08:08,  3.62s/it]Running Inference:  33%|███▎      | 66/200 [03:40<06:58,  3.12s/it]Running Inference:  34%|███▎      | 67/200 [03:43<07:07,  3.22s/it]Running Inference:  34%|███▍      | 68/200 [03:45<06:11,  2.82s/it]Running Inference:  34%|███▍      | 69/200 [03:49<06:52,  3.15s/it]Running Inference:  35%|███▌      | 70/200 [03:53<07:18,  3.38s/it]Running Inference:  36%|███▌      | 71/200 [03:57<07:37,  3.54s/it]Running Inference:  36%|███▌      | 72/200 [04:00<07:26,  3.49s/it]Running Inference:  36%|███▋      | 73/200 [04:04<07:25,  3.51s/it]Running Inference:  37%|███▋      | 74/200 [04:07<07:30,  3.57s/it]Running Inference:  38%|███▊      | 75/200 [04:11<07:25,  3.57s/it]Running Inference:  38%|███▊      | 76/200 [04:12<05:59,  2.90s/it]Running Inference:  38%|███▊      | 77/200 [04:16<06:26,  3.14s/it]Running Inference:  39%|███▉      | 78/200 [04:19<06:25,  3.16s/it]Running Inference:  40%|███▉      | 79/200 [04:23<06:41,  3.32s/it]Running Inference:  40%|████      | 80/200 [04:24<05:36,  2.80s/it]Running Inference:  40%|████      | 81/200 [04:26<04:48,  2.43s/it]Running Inference:  41%|████      | 82/200 [04:29<05:21,  2.73s/it]Running Inference:  42%|████▏     | 83/200 [04:33<05:53,  3.03s/it]Running Inference:  42%|████▏     | 84/200 [04:37<06:10,  3.19s/it]Running Inference:  42%|████▎     | 85/200 [04:40<06:22,  3.32s/it]Running Inference:  43%|████▎     | 86/200 [04:44<06:34,  3.46s/it]Running Inference:  44%|████▎     | 87/200 [04:48<06:40,  3.55s/it]Running Inference:  44%|████▍     | 88/200 [04:51<06:37,  3.55s/it]Running Inference:  44%|████▍     | 89/200 [04:55<06:32,  3.54s/it]Running Inference:  45%|████▌     | 90/200 [04:59<06:30,  3.55s/it]Running Inference:  46%|████▌     | 91/200 [05:00<05:23,  2.97s/it]Running Inference:  46%|████▌     | 92/200 [05:04<05:49,  3.23s/it]Running Inference:  46%|████▋     | 93/200 [05:08<05:56,  3.33s/it]Running Inference:  47%|████▋     | 94/200 [05:11<06:12,  3.51s/it]Running Inference:  48%|████▊     | 95/200 [05:15<06:13,  3.56s/it]Running Inference:  48%|████▊     | 96/200 [05:17<05:17,  3.05s/it]Running Inference:  48%|████▊     | 97/200 [05:19<04:31,  2.64s/it]Running Inference:  49%|████▉     | 98/200 [05:23<05:08,  3.02s/it]Running Inference:  50%|████▉     | 99/200 [05:26<05:23,  3.20s/it]Running Inference:  50%|█████     | 100/200 [05:29<05:21,  3.21s/it]Running Inference:  50%|█████     | 101/200 [05:33<05:28,  3.31s/it]Running Inference:  51%|█████     | 102/200 [05:37<05:35,  3.42s/it]Running Inference:  52%|█████▏    | 103/200 [05:40<05:37,  3.48s/it]Running Inference:  52%|█████▏    | 104/200 [05:42<04:38,  2.90s/it]Running Inference:  52%|█████▎    | 105/200 [05:46<05:05,  3.21s/it]Running Inference:  53%|█████▎    | 106/200 [05:47<04:17,  2.74s/it]Running Inference:  54%|█████▎    | 107/200 [05:51<04:35,  2.96s/it]Running Inference:  54%|█████▍    | 108/200 [05:55<04:51,  3.17s/it]Running Inference:  55%|█████▍    | 109/200 [05:57<04:36,  3.04s/it]Running Inference:  55%|█████▌    | 110/200 [06:01<04:57,  3.30s/it]Running Inference:  56%|█████▌    | 111/200 [06:05<04:59,  3.37s/it]Running Inference:  56%|█████▌    | 112/200 [06:08<05:03,  3.45s/it]Running Inference:  56%|█████▋    | 113/200 [06:12<05:08,  3.55s/it]Running Inference:  57%|█████▋    | 114/200 [06:15<04:57,  3.45s/it]Running Inference:  57%|█████▊    | 115/200 [06:19<04:58,  3.51s/it]Running Inference:  58%|█████▊    | 116/200 [06:22<04:38,  3.31s/it]Running Inference:  58%|█████▊    | 117/200 [06:23<03:49,  2.76s/it]Running Inference:  59%|█████▉    | 118/200 [06:26<03:54,  2.86s/it]Running Inference:  60%|█████▉    | 119/200 [06:30<04:17,  3.17s/it]Running Inference:  60%|██████    | 120/200 [06:34<04:31,  3.40s/it]Running Inference:  60%|██████    | 121/200 [06:38<04:35,  3.49s/it]Running Inference:  61%|██████    | 122/200 [06:41<04:11,  3.22s/it]Running Inference:  62%|██████▏   | 123/200 [06:44<04:16,  3.33s/it]Running Inference:  62%|██████▏   | 124/200 [06:47<04:12,  3.32s/it]Running Inference:  62%|██████▎   | 125/200 [06:51<04:16,  3.42s/it]Running Inference:  63%|██████▎   | 126/200 [06:55<04:18,  3.49s/it]Running Inference:  64%|██████▎   | 127/200 [06:57<03:40,  3.01s/it]Running Inference:  64%|██████▍   | 128/200 [06:59<03:25,  2.85s/it]Running Inference:  64%|██████▍   | 129/200 [07:00<02:50,  2.39s/it]Running Inference:  65%|██████▌   | 130/200 [07:04<03:14,  2.78s/it]Running Inference:  66%|██████▌   | 131/200 [07:07<03:09,  2.74s/it]Running Inference:  66%|██████▌   | 132/200 [07:11<03:30,  3.09s/it]Running Inference:  66%|██████▋   | 133/200 [07:14<03:36,  3.24s/it]Running Inference:  67%|██████▋   | 134/200 [07:18<03:43,  3.39s/it]Running Inference:  68%|██████▊   | 135/200 [07:22<03:44,  3.46s/it]Running Inference:  68%|██████▊   | 136/200 [07:23<03:02,  2.85s/it]Running Inference:  68%|██████▊   | 137/200 [07:26<03:07,  2.97s/it]Running Inference:  69%|██████▉   | 138/200 [07:30<03:17,  3.19s/it]Running Inference:  70%|██████▉   | 139/200 [07:34<03:24,  3.35s/it]Running Inference:  70%|███████   | 140/200 [07:37<03:24,  3.40s/it]Running Inference:  70%|███████   | 141/200 [07:41<03:24,  3.47s/it]Running Inference:  71%|███████   | 142/200 [07:45<03:23,  3.51s/it]Running Inference:  72%|███████▏  | 143/200 [07:46<02:50,  2.99s/it]Running Inference:  72%|███████▏  | 144/200 [07:50<03:05,  3.31s/it]Running Inference:  72%|███████▎  | 145/200 [07:54<03:04,  3.35s/it]Running Inference:  73%|███████▎  | 146/200 [07:58<03:08,  3.50s/it]Running Inference:  74%|███████▎  | 147/200 [08:01<03:00,  3.40s/it]Running Inference:  74%|███████▍  | 148/200 [08:03<02:36,  3.01s/it]Running Inference:  74%|███████▍  | 149/200 [08:07<02:44,  3.22s/it]Running Inference:  75%|███████▌  | 150/200 [08:10<02:46,  3.32s/it]Running Inference:  76%|███████▌  | 151/200 [08:14<02:50,  3.48s/it]Running Inference:  76%|███████▌  | 152/200 [08:18<02:49,  3.53s/it]Running Inference:  76%|███████▋  | 153/200 [08:19<02:17,  2.93s/it]Running Inference:  77%|███████▋  | 154/200 [08:23<02:21,  3.08s/it]Running Inference:  78%|███████▊  | 155/200 [08:24<01:59,  2.66s/it]Running Inference:  78%|███████▊  | 156/200 [08:28<02:13,  3.04s/it]Running Inference:  78%|███████▊  | 157/200 [08:32<02:19,  3.24s/it]Running Inference:  79%|███████▉  | 158/200 [08:34<01:54,  2.74s/it]Running Inference:  80%|███████▉  | 159/200 [08:37<02:01,  2.96s/it]Running Inference:  80%|████████  | 160/200 [08:40<01:59,  2.99s/it]Running Inference:  80%|████████  | 161/200 [08:42<01:39,  2.56s/it]Running Inference:  81%|████████  | 162/200 [08:43<01:27,  2.30s/it]Running Inference:  82%|████████▏ | 163/200 [08:46<01:29,  2.41s/it]Running Inference:  82%|████████▏ | 164/200 [08:50<01:41,  2.83s/it]Running Inference:  82%|████████▎ | 165/200 [08:54<01:49,  3.14s/it]Running Inference:  83%|████████▎ | 166/200 [08:57<01:46,  3.14s/it]Running Inference:  84%|████████▎ | 167/200 [09:01<01:49,  3.32s/it]Running Inference:  84%|████████▍ | 168/200 [09:04<01:50,  3.45s/it]Running Inference:  84%|████████▍ | 169/200 [09:08<01:51,  3.58s/it]Running Inference:  85%|████████▌ | 170/200 [09:10<01:32,  3.08s/it]Running Inference:  86%|████████▌ | 171/200 [09:14<01:33,  3.22s/it]Running Inference:  86%|████████▌ | 172/200 [09:17<01:33,  3.32s/it]Running Inference:  86%|████████▋ | 173/200 [09:21<01:32,  3.44s/it]Running Inference:  87%|████████▋ | 174/200 [09:25<01:33,  3.58s/it]Running Inference:  88%|████████▊ | 175/200 [09:27<01:15,  3.01s/it]Running Inference:  88%|████████▊ | 176/200 [09:30<01:14,  3.11s/it]Running Inference:  88%|████████▊ | 177/200 [09:33<01:11,  3.09s/it]Running Inference:  89%|████████▉ | 178/200 [09:36<01:07,  3.05s/it]Running Inference:  90%|████████▉ | 179/200 [09:39<01:07,  3.20s/it]Running Inference:  90%|█████████ | 180/200 [09:43<01:06,  3.34s/it]Running Inference:  90%|█████████ | 181/200 [09:45<00:53,  2.80s/it]Running Inference:  91%|█████████ | 182/200 [09:48<00:55,  3.07s/it]Running Inference:  92%|█████████▏| 183/200 [09:52<00:55,  3.26s/it]Running Inference:  92%|█████████▏| 184/200 [09:54<00:45,  2.82s/it]Running Inference:  92%|█████████▎| 185/200 [09:58<00:46,  3.12s/it]Running Inference:  93%|█████████▎| 186/200 [10:01<00:46,  3.31s/it]Running Inference:  94%|█████████▎| 187/200 [10:05<00:44,  3.43s/it]Running Inference:  94%|█████████▍| 188/200 [10:07<00:36,  3.06s/it]Running Inference:  94%|█████████▍| 189/200 [10:11<00:35,  3.26s/it]Running Inference:  95%|█████████▌| 190/200 [10:15<00:33,  3.33s/it]Running Inference:  96%|█████████▌| 191/200 [10:18<00:30,  3.44s/it]Running Inference:  96%|█████████▌| 192/200 [10:22<00:28,  3.54s/it]Running Inference:  96%|█████████▋| 193/200 [10:24<00:20,  2.99s/it]Running Inference:  97%|█████████▋| 194/200 [10:28<00:19,  3.26s/it]Running Inference:  98%|█████████▊| 195/200 [10:31<00:17,  3.44s/it]Running Inference:  98%|█████████▊| 196/200 [10:35<00:14,  3.55s/it]Running Inference:  98%|█████████▊| 197/200 [10:39<00:10,  3.63s/it]Running Inference:  99%|█████████▉| 198/200 [10:42<00:07,  3.57s/it]Running Inference: 100%|█████████▉| 199/200 [10:47<00:03,  3.73s/it]Running Inference: 100%|██████████| 200/200 [10:50<00:00,  3.68s/it]Running Inference: 100%|██████████| 200/200 [10:50<00:00,  3.25s/it]
2025-12-14 16:14:17,836 - INFO - Inference completed.
2025-12-14 16:14:17,847 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 16:14:17,847 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:14:17,849 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 16:14:17,849 - INFO - Metrics:
50.75
2025-12-14 16:14:17,850 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_en, ratio=0.2) on GPU 1

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:14:24,335 - INFO - Set deterministic seeds to 42
2025-12-14 16:14:24,335 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:14:24,335 - INFO - Starting evaluation run...
2025-12-14 16:14:24,335 - INFO - Output directory set to: longbenchresult
2025-12-14 16:14:24,335 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 16:14:24,335 - INFO - KV Press 'tova' setup.
2025-12-14 16:14:24,335 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:14:24,335 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.71it/s]
Device set to use cuda:0
2025-12-14 16:14:40,692 - INFO - Model pipeline loaded.
2025-12-14 16:14:40,692 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 16:14:48,521 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:14:48,521 - INFO - Dataset processed with 200 entries.
2025-12-14 16:14:48,553 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:43,  4.44s/it]Running Inference:   1%|          | 2/200 [00:08<12:57,  3.93s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:55,  3.02s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:43,  2.98s/it]Running Inference:   2%|▎         | 5/200 [00:14<08:17,  2.55s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:19,  2.88s/it]Running Inference:   4%|▎         | 7/200 [00:22<10:19,  3.21s/it]Running Inference:   4%|▍         | 8/200 [00:25<10:21,  3.24s/it]Running Inference:   4%|▍         | 9/200 [00:27<08:54,  2.80s/it]Running Inference:   5%|▌         | 10/200 [00:30<09:33,  3.02s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:34<10:00,  3.18s/it]Running Inference:   6%|▌         | 12/200 [00:37<09:51,  3.14s/it]Running Inference:   6%|▋         | 13/200 [00:40<10:14,  3.29s/it]Running Inference:   7%|▋         | 14/200 [00:44<10:21,  3.34s/it]Running Inference:   8%|▊         | 15/200 [00:47<10:05,  3.27s/it]Running Inference:   8%|▊         | 16/200 [00:50<09:44,  3.18s/it]Running Inference:   8%|▊         | 17/200 [00:52<08:17,  2.72s/it]Running Inference:   9%|▉         | 18/200 [00:55<09:03,  2.99s/it]Running Inference:  10%|▉         | 19/200 [00:59<09:41,  3.21s/it]Running Inference:  10%|█         | 20/200 [01:01<08:18,  2.77s/it]Running Inference:  10%|█         | 21/200 [01:04<09:03,  3.03s/it]Running Inference:  11%|█         | 22/200 [01:08<09:33,  3.22s/it]Running Inference:  12%|█▏        | 23/200 [01:12<10:01,  3.40s/it]Running Inference:  12%|█▏        | 24/200 [01:16<10:14,  3.49s/it]Running Inference:  12%|█▎        | 25/200 [01:18<09:31,  3.27s/it]Running Inference:  13%|█▎        | 26/200 [01:22<09:38,  3.32s/it]Running Inference:  14%|█▎        | 27/200 [01:26<10:14,  3.55s/it]Running Inference:  14%|█▍        | 28/200 [01:29<10:05,  3.52s/it]Running Inference:  14%|█▍        | 29/200 [01:33<10:01,  3.52s/it]Running Inference:  15%|█▌        | 30/200 [01:36<10:05,  3.56s/it]Running Inference:  16%|█▌        | 31/200 [01:40<10:21,  3.68s/it]Running Inference:  16%|█▌        | 32/200 [01:44<10:29,  3.75s/it]Running Inference:  16%|█▋        | 33/200 [01:48<10:26,  3.75s/it]Running Inference:  17%|█▋        | 34/200 [01:51<09:55,  3.59s/it]Running Inference:  18%|█▊        | 35/200 [01:55<10:04,  3.66s/it]Running Inference:  18%|█▊        | 36/200 [01:59<09:51,  3.61s/it]Running Inference:  18%|█▊        | 37/200 [02:02<09:56,  3.66s/it]Running Inference:  19%|█▉        | 38/200 [02:06<09:59,  3.70s/it]Running Inference:  20%|█▉        | 39/200 [02:09<09:25,  3.51s/it]Running Inference:  20%|██        | 40/200 [02:13<09:32,  3.58s/it]Running Inference:  20%|██        | 41/200 [02:17<09:45,  3.68s/it]Running Inference:  21%|██        | 42/200 [02:20<09:32,  3.62s/it]Running Inference:  22%|██▏       | 43/200 [02:24<09:48,  3.75s/it]Running Inference:  22%|██▏       | 44/200 [02:26<08:15,  3.17s/it]Running Inference:  22%|██▎       | 45/200 [02:30<08:26,  3.27s/it]Running Inference:  23%|██▎       | 46/200 [02:34<08:53,  3.47s/it]Running Inference:  24%|██▎       | 47/200 [02:37<08:55,  3.50s/it]Running Inference:  24%|██▍       | 48/200 [02:41<08:53,  3.51s/it]Running Inference:  24%|██▍       | 49/200 [02:45<09:10,  3.65s/it]Running Inference:  25%|██▌       | 50/200 [02:48<08:32,  3.42s/it]Running Inference:  26%|██▌       | 51/200 [02:51<08:26,  3.40s/it]Running Inference:  26%|██▌       | 52/200 [02:54<07:59,  3.24s/it]Running Inference:  26%|██▋       | 53/200 [02:58<08:28,  3.46s/it]Running Inference:  27%|██▋       | 54/200 [03:01<08:04,  3.32s/it]Running Inference:  28%|██▊       | 55/200 [03:04<07:44,  3.21s/it]Running Inference:  28%|██▊       | 56/200 [03:08<08:06,  3.38s/it]Running Inference:  28%|██▊       | 57/200 [03:09<06:50,  2.87s/it]Running Inference:  29%|██▉       | 58/200 [03:12<06:51,  2.90s/it]Running Inference:  30%|██▉       | 59/200 [03:16<07:32,  3.21s/it]Running Inference:  30%|███       | 60/200 [03:20<07:50,  3.36s/it]Running Inference:  30%|███       | 61/200 [03:23<07:58,  3.44s/it]Running Inference:  31%|███       | 62/200 [03:27<07:53,  3.43s/it]Running Inference:  32%|███▏      | 63/200 [03:31<07:59,  3.50s/it]Running Inference:  32%|███▏      | 64/200 [03:34<08:02,  3.55s/it]Running Inference:  32%|███▎      | 65/200 [03:38<08:00,  3.56s/it]Running Inference:  33%|███▎      | 66/200 [03:40<06:53,  3.08s/it]Running Inference:  34%|███▎      | 67/200 [03:43<07:00,  3.16s/it]Running Inference:  34%|███▍      | 68/200 [03:45<06:06,  2.78s/it]Running Inference:  34%|███▍      | 69/200 [03:49<06:50,  3.13s/it]Running Inference:  35%|███▌      | 70/200 [03:53<07:18,  3.37s/it]Running Inference:  36%|███▌      | 71/200 [03:57<07:38,  3.55s/it]Running Inference:  36%|███▌      | 72/200 [04:00<07:28,  3.50s/it]Running Inference:  36%|███▋      | 73/200 [04:02<06:09,  2.91s/it]Running Inference:  37%|███▋      | 74/200 [04:06<06:38,  3.16s/it]Running Inference:  38%|███▊      | 75/200 [04:07<05:33,  2.67s/it]Running Inference:  38%|███▊      | 76/200 [04:08<04:41,  2.27s/it]Running Inference:  38%|███▊      | 77/200 [04:12<05:33,  2.71s/it]Running Inference:  39%|███▉      | 78/200 [04:15<05:49,  2.86s/it]Running Inference:  40%|███▉      | 79/200 [04:19<06:17,  3.12s/it]Running Inference:  40%|████      | 80/200 [04:21<05:19,  2.67s/it]Running Inference:  40%|████      | 81/200 [04:22<04:37,  2.33s/it]Running Inference:  41%|████      | 82/200 [04:26<05:14,  2.67s/it]Running Inference:  42%|████▏     | 83/200 [04:29<05:49,  2.99s/it]Running Inference:  42%|████▏     | 84/200 [04:33<06:08,  3.18s/it]Running Inference:  42%|████▎     | 85/200 [04:37<06:22,  3.32s/it]Running Inference:  43%|████▎     | 86/200 [04:40<06:35,  3.47s/it]Running Inference:  44%|████▎     | 87/200 [04:44<06:42,  3.56s/it]Running Inference:  44%|████▍     | 88/200 [04:48<06:39,  3.56s/it]Running Inference:  44%|████▍     | 89/200 [04:51<06:35,  3.56s/it]Running Inference:  45%|████▌     | 90/200 [04:55<06:32,  3.57s/it]Running Inference:  46%|████▌     | 91/200 [04:57<05:25,  2.99s/it]Running Inference:  46%|████▌     | 92/200 [05:00<05:50,  3.25s/it]Running Inference:  46%|████▋     | 93/200 [05:04<05:57,  3.35s/it]Running Inference:  47%|████▋     | 94/200 [05:08<06:13,  3.52s/it]Running Inference:  48%|████▊     | 95/200 [05:11<06:08,  3.51s/it]Running Inference:  48%|████▊     | 96/200 [05:13<05:13,  3.02s/it]Running Inference:  48%|████▊     | 97/200 [05:15<04:28,  2.61s/it]Running Inference:  49%|████▉     | 98/200 [05:19<05:05,  3.00s/it]Running Inference:  50%|████▉     | 99/200 [05:23<05:22,  3.19s/it]Running Inference:  50%|█████     | 100/200 [05:26<05:35,  3.35s/it]Running Inference:  50%|█████     | 101/200 [05:30<05:30,  3.34s/it]Running Inference:  51%|█████     | 102/200 [05:33<05:37,  3.44s/it]Running Inference:  52%|█████▏    | 103/200 [05:37<05:39,  3.50s/it]Running Inference:  52%|█████▏    | 104/200 [05:38<04:39,  2.92s/it]Running Inference:  52%|█████▎    | 105/200 [05:42<05:06,  3.22s/it]Running Inference:  53%|█████▎    | 106/200 [05:44<04:18,  2.75s/it]Running Inference:  54%|█████▎    | 107/200 [05:48<04:36,  2.97s/it]Running Inference:  54%|█████▍    | 108/200 [05:50<04:06,  2.68s/it]Running Inference:  55%|█████▍    | 109/200 [05:53<04:39,  3.07s/it]Running Inference:  55%|█████▌    | 110/200 [05:56<04:09,  2.78s/it]Running Inference:  56%|█████▌    | 111/200 [05:59<04:27,  3.01s/it]Running Inference:  56%|█████▌    | 112/200 [06:03<04:42,  3.21s/it]Running Inference:  56%|█████▋    | 113/200 [06:06<04:45,  3.28s/it]Running Inference:  57%|█████▋    | 114/200 [06:10<04:41,  3.27s/it]Running Inference:  57%|█████▊    | 115/200 [06:13<04:48,  3.39s/it]Running Inference:  58%|█████▊    | 116/200 [06:16<04:31,  3.23s/it]Running Inference:  58%|█████▊    | 117/200 [06:18<03:44,  2.71s/it]Running Inference:  59%|█████▉    | 118/200 [06:19<03:18,  2.42s/it]Running Inference:  60%|█████▉    | 119/200 [06:23<03:52,  2.87s/it]Running Inference:  60%|██████    | 120/200 [06:27<04:15,  3.20s/it]Running Inference:  60%|██████    | 121/200 [06:31<04:25,  3.36s/it]Running Inference:  61%|██████    | 122/200 [06:34<04:04,  3.14s/it]Running Inference:  62%|██████▏   | 123/200 [06:37<04:08,  3.22s/it]Running Inference:  62%|██████▏   | 124/200 [06:40<04:08,  3.27s/it]Running Inference:  62%|██████▎   | 125/200 [06:42<03:34,  2.86s/it]Running Inference:  63%|██████▎   | 126/200 [06:46<03:49,  3.11s/it]Running Inference:  64%|██████▎   | 127/200 [06:50<03:59,  3.27s/it]Running Inference:  64%|██████▍   | 128/200 [06:53<03:59,  3.33s/it]Running Inference:  64%|██████▍   | 129/200 [06:54<03:13,  2.73s/it]Running Inference:  65%|██████▌   | 130/200 [06:58<03:31,  3.02s/it]Running Inference:  66%|██████▌   | 131/200 [07:02<03:43,  3.24s/it]Running Inference:  66%|██████▌   | 132/200 [07:06<03:54,  3.46s/it]Running Inference:  66%|██████▋   | 133/200 [07:09<03:52,  3.46s/it]Running Inference:  67%|██████▋   | 134/200 [07:13<03:54,  3.55s/it]Running Inference:  68%|██████▊   | 135/200 [07:17<03:53,  3.59s/it]Running Inference:  68%|██████▊   | 136/200 [07:18<03:07,  2.93s/it]Running Inference:  68%|██████▊   | 137/200 [07:22<03:21,  3.20s/it]Running Inference:  69%|██████▉   | 138/200 [07:25<03:15,  3.15s/it]Running Inference:  70%|██████▉   | 139/200 [07:29<03:23,  3.33s/it]Running Inference:  70%|███████   | 140/200 [07:32<03:24,  3.40s/it]Running Inference:  70%|███████   | 141/200 [07:36<03:25,  3.48s/it]Running Inference:  71%|███████   | 142/200 [07:40<03:24,  3.52s/it]Running Inference:  72%|███████▏  | 143/200 [07:43<03:26,  3.61s/it]Running Inference:  72%|███████▏  | 144/200 [07:47<03:30,  3.76s/it]Running Inference:  72%|███████▎  | 145/200 [07:51<03:21,  3.67s/it]Running Inference:  73%|███████▎  | 146/200 [07:54<03:14,  3.60s/it]Running Inference:  74%|███████▎  | 147/200 [07:57<03:00,  3.40s/it]Running Inference:  74%|███████▍  | 148/200 [07:59<02:36,  3.01s/it]Running Inference:  74%|███████▍  | 149/200 [08:03<02:44,  3.23s/it]Running Inference:  75%|███████▌  | 150/200 [08:07<02:46,  3.34s/it]Running Inference:  76%|███████▌  | 151/200 [08:11<02:51,  3.49s/it]Running Inference:  76%|███████▌  | 152/200 [08:14<02:50,  3.55s/it]Running Inference:  76%|███████▋  | 153/200 [08:16<02:18,  2.95s/it]Running Inference:  77%|███████▋  | 154/200 [08:19<02:22,  3.10s/it]Running Inference:  78%|███████▊  | 155/200 [08:21<02:00,  2.67s/it]Running Inference:  78%|███████▊  | 156/200 [08:25<02:14,  3.06s/it]Running Inference:  78%|███████▊  | 157/200 [08:29<02:20,  3.26s/it]Running Inference:  79%|███████▉  | 158/200 [08:32<02:21,  3.37s/it]Running Inference:  80%|███████▉  | 159/200 [08:36<02:21,  3.44s/it]Running Inference:  80%|████████  | 160/200 [08:39<02:14,  3.36s/it]Running Inference:  80%|████████  | 161/200 [08:41<01:49,  2.82s/it]Running Inference:  81%|████████  | 162/200 [08:42<01:34,  2.47s/it]Running Inference:  82%|████████▏ | 163/200 [08:45<01:33,  2.53s/it]Running Inference:  82%|████████▏ | 164/200 [08:49<01:44,  2.91s/it]Running Inference:  82%|████████▎ | 165/200 [08:53<01:52,  3.20s/it]Running Inference:  83%|████████▎ | 166/200 [08:56<01:49,  3.21s/it]Running Inference:  84%|████████▎ | 167/200 [09:00<01:51,  3.37s/it]Running Inference:  84%|████████▍ | 168/200 [09:03<01:51,  3.49s/it]Running Inference:  84%|████████▍ | 169/200 [09:07<01:51,  3.60s/it]Running Inference:  85%|████████▌ | 170/200 [09:09<01:32,  3.09s/it]Running Inference:  86%|████████▌ | 171/200 [09:13<01:33,  3.23s/it]Running Inference:  86%|████████▌ | 172/200 [09:16<01:33,  3.33s/it]Running Inference:  86%|████████▋ | 173/200 [09:20<01:33,  3.45s/it]Running Inference:  87%|████████▋ | 174/200 [09:24<01:33,  3.59s/it]Running Inference:  88%|████████▊ | 175/200 [09:26<01:15,  3.01s/it]Running Inference:  88%|████████▊ | 176/200 [09:29<01:13,  3.07s/it]Running Inference:  88%|████████▊ | 177/200 [09:32<01:10,  3.06s/it]Running Inference:  89%|████████▉ | 178/200 [09:35<01:06,  3.04s/it]Running Inference:  90%|████████▉ | 179/200 [09:37<00:55,  2.66s/it]Running Inference:  90%|█████████ | 180/200 [09:40<00:59,  2.97s/it]Running Inference:  90%|█████████ | 181/200 [09:42<00:48,  2.54s/it]Running Inference:  91%|█████████ | 182/200 [09:45<00:51,  2.89s/it]Running Inference:  92%|█████████▏| 183/200 [09:49<00:53,  3.15s/it]Running Inference:  92%|█████████▏| 184/200 [09:51<00:43,  2.75s/it]Running Inference:  92%|█████████▎| 185/200 [09:55<00:46,  3.07s/it]Running Inference:  93%|█████████▎| 186/200 [09:59<00:45,  3.28s/it]Running Inference:  94%|█████████▎| 187/200 [10:02<00:44,  3.41s/it]Running Inference:  94%|█████████▍| 188/200 [10:06<00:43,  3.60s/it]Running Inference:  94%|█████████▍| 189/200 [10:10<00:40,  3.64s/it]Running Inference:  95%|█████████▌| 190/200 [10:14<00:36,  3.60s/it]Running Inference:  96%|█████████▌| 191/200 [10:17<00:32,  3.63s/it]Running Inference:  96%|█████████▌| 192/200 [10:21<00:29,  3.68s/it]Running Inference:  96%|█████████▋| 193/200 [10:25<00:25,  3.71s/it]Running Inference:  97%|█████████▋| 194/200 [10:29<00:22,  3.76s/it]Running Inference:  98%|█████████▊| 195/200 [10:33<00:18,  3.74s/it]Running Inference:  98%|█████████▊| 196/200 [10:36<00:15,  3.77s/it]Running Inference:  98%|█████████▊| 197/200 [10:40<00:11,  3.80s/it]Running Inference:  99%|█████████▉| 198/200 [10:43<00:07,  3.58s/it]Running Inference: 100%|█████████▉| 199/200 [10:47<00:03,  3.74s/it]Running Inference: 100%|██████████| 200/200 [10:51<00:00,  3.69s/it]Running Inference: 100%|██████████| 200/200 [10:51<00:00,  3.26s/it]
2025-12-14 16:25:40,055 - INFO - Inference completed.
2025-12-14 16:25:40,066 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 16:25:40,067 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:25:40,068 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 16:25:40,068 - INFO - Metrics:
50.83
2025-12-14 16:25:40,069 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_en, ratio=0.3) on GPU 1

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:25:46,594 - INFO - Set deterministic seeds to 42
2025-12-14 16:25:46,594 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:25:46,594 - INFO - Starting evaluation run...
2025-12-14 16:25:46,594 - INFO - Output directory set to: longbenchresult
2025-12-14 16:25:46,594 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 16:25:46,594 - INFO - KV Press 'tova' setup.
2025-12-14 16:25:46,594 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:25:46,594 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.02it/s]
Device set to use cuda:0
2025-12-14 16:26:00,486 - INFO - Model pipeline loaded.
2025-12-14 16:26:00,486 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 16:26:07,998 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:26:07,998 - INFO - Dataset processed with 200 entries.
2025-12-14 16:26:08,030 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:35,  4.40s/it]Running Inference:   1%|          | 2/200 [00:07<12:52,  3.90s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:52,  3.01s/it]Running Inference:   2%|▏         | 4/200 [00:11<08:04,  2.47s/it]Running Inference:   2%|▎         | 5/200 [00:13<07:14,  2.23s/it]Running Inference:   3%|▎         | 6/200 [00:16<08:35,  2.66s/it]Running Inference:   4%|▎         | 7/200 [00:20<09:24,  2.92s/it]Running Inference:   4%|▍         | 8/200 [00:23<09:20,  2.92s/it]Running Inference:   4%|▍         | 9/200 [00:26<10:02,  3.15s/it]Running Inference:   5%|▌         | 10/200 [00:28<08:17,  2.62s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:31<09:07,  2.90s/it]Running Inference:   6%|▌         | 12/200 [00:33<08:07,  2.59s/it]Running Inference:   6%|▋         | 13/200 [00:37<09:05,  2.92s/it]Running Inference:   7%|▋         | 14/200 [00:41<09:44,  3.14s/it]Running Inference:   8%|▊         | 15/200 [00:44<09:38,  3.13s/it]Running Inference:   8%|▊         | 16/200 [00:47<09:25,  3.08s/it]Running Inference:   8%|▊         | 17/200 [00:50<09:56,  3.26s/it]Running Inference:   9%|▉         | 18/200 [00:54<10:11,  3.36s/it]Running Inference:  10%|▉         | 19/200 [00:58<10:27,  3.47s/it]Running Inference:  10%|█         | 20/200 [00:59<08:50,  2.95s/it]Running Inference:  10%|█         | 21/200 [01:03<09:38,  3.23s/it]Running Inference:  11%|█         | 22/200 [01:07<09:56,  3.35s/it]Running Inference:  12%|█▏        | 23/200 [01:11<10:16,  3.48s/it]Running Inference:  12%|█▏        | 24/200 [01:14<10:21,  3.53s/it]Running Inference:  12%|█▎        | 25/200 [01:18<10:12,  3.50s/it]Running Inference:  13%|█▎        | 26/200 [01:21<10:05,  3.48s/it]Running Inference:  14%|█▎        | 27/200 [01:25<10:30,  3.64s/it]Running Inference:  14%|█▍        | 28/200 [01:29<10:14,  3.57s/it]Running Inference:  14%|█▍        | 29/200 [01:32<09:49,  3.45s/it]Running Inference:  15%|█▌        | 30/200 [01:35<09:55,  3.50s/it]Running Inference:  16%|█▌        | 31/200 [01:37<08:28,  3.01s/it]Running Inference:  16%|█▌        | 32/200 [01:39<07:26,  2.66s/it]Running Inference:  16%|█▋        | 33/200 [01:43<08:18,  2.99s/it]Running Inference:  17%|█▋        | 34/200 [01:46<08:25,  3.05s/it]Running Inference:  18%|█▊        | 35/200 [01:50<09:01,  3.28s/it]Running Inference:  18%|█▊        | 36/200 [01:53<09:06,  3.33s/it]Running Inference:  18%|█▊        | 37/200 [01:57<09:24,  3.46s/it]Running Inference:  19%|█▉        | 38/200 [02:01<09:36,  3.56s/it]Running Inference:  20%|█▉        | 39/200 [02:04<09:31,  3.55s/it]Running Inference:  20%|██        | 40/200 [02:08<09:35,  3.60s/it]Running Inference:  20%|██        | 41/200 [02:12<09:45,  3.69s/it]Running Inference:  21%|██        | 42/200 [02:15<09:01,  3.43s/it]Running Inference:  22%|██▏       | 43/200 [02:19<09:24,  3.59s/it]Running Inference:  22%|██▏       | 44/200 [02:21<07:57,  3.06s/it]Running Inference:  22%|██▎       | 45/200 [02:24<08:16,  3.21s/it]Running Inference:  23%|██▎       | 46/200 [02:28<08:46,  3.42s/it]Running Inference:  24%|██▎       | 47/200 [02:32<08:49,  3.46s/it]Running Inference:  24%|██▍       | 48/200 [02:35<08:47,  3.47s/it]Running Inference:  24%|██▍       | 49/200 [02:39<09:05,  3.61s/it]Running Inference:  25%|██▌       | 50/200 [02:42<08:27,  3.38s/it]Running Inference:  26%|██▌       | 51/200 [02:45<08:01,  3.23s/it]Running Inference:  26%|██▌       | 52/200 [02:49<08:30,  3.45s/it]Running Inference:  26%|██▋       | 53/200 [02:53<08:47,  3.59s/it]Running Inference:  27%|██▋       | 54/200 [02:56<08:17,  3.41s/it]Running Inference:  28%|██▊       | 55/200 [02:58<07:47,  3.23s/it]Running Inference:  28%|██▊       | 56/200 [03:02<08:07,  3.39s/it]Running Inference:  28%|██▊       | 57/200 [03:04<06:50,  2.87s/it]Running Inference:  29%|██▉       | 58/200 [03:06<06:03,  2.56s/it]Running Inference:  30%|██▉       | 59/200 [03:10<06:58,  2.97s/it]Running Inference:  30%|███       | 60/200 [03:13<07:26,  3.19s/it]Running Inference:  30%|███       | 61/200 [03:17<07:40,  3.31s/it]Running Inference:  31%|███       | 62/200 [03:21<08:01,  3.49s/it]Running Inference:  32%|███▏      | 63/200 [03:24<07:53,  3.46s/it]Running Inference:  32%|███▏      | 64/200 [03:28<07:57,  3.51s/it]Running Inference:  32%|███▎      | 65/200 [03:31<07:56,  3.53s/it]Running Inference:  33%|███▎      | 66/200 [03:33<06:49,  3.06s/it]Running Inference:  34%|███▎      | 67/200 [03:37<07:02,  3.17s/it]Running Inference:  34%|███▍      | 68/200 [03:39<06:07,  2.78s/it]Running Inference:  34%|███▍      | 69/200 [03:43<06:49,  3.12s/it]Running Inference:  35%|███▌      | 70/200 [03:47<07:16,  3.36s/it]Running Inference:  36%|███▌      | 71/200 [03:50<07:35,  3.53s/it]Running Inference:  36%|███▌      | 72/200 [03:54<07:34,  3.55s/it]Running Inference:  36%|███▋      | 73/200 [03:58<07:31,  3.56s/it]Running Inference:  37%|███▋      | 74/200 [04:01<07:34,  3.61s/it]Running Inference:  38%|███▊      | 75/200 [04:03<06:12,  2.98s/it]Running Inference:  38%|███▊      | 76/200 [04:06<06:24,  3.10s/it]Running Inference:  38%|███▊      | 77/200 [04:08<05:28,  2.67s/it]Running Inference:  39%|███▉      | 78/200 [04:11<05:31,  2.72s/it]Running Inference:  40%|███▉      | 79/200 [04:14<06:04,  3.01s/it]Running Inference:  40%|████      | 80/200 [04:18<06:16,  3.14s/it]Running Inference:  40%|████      | 81/200 [04:19<05:16,  2.66s/it]Running Inference:  41%|████      | 82/200 [04:23<05:34,  2.84s/it]Running Inference:  42%|████▏     | 83/200 [04:26<06:02,  3.10s/it]Running Inference:  42%|████▏     | 84/200 [04:30<06:16,  3.25s/it]Running Inference:  42%|████▎     | 85/200 [04:34<06:26,  3.36s/it]Running Inference:  43%|████▎     | 86/200 [04:37<06:37,  3.49s/it]Running Inference:  44%|████▎     | 87/200 [04:41<06:43,  3.57s/it]Running Inference:  44%|████▍     | 88/200 [04:45<06:39,  3.56s/it]Running Inference:  44%|████▍     | 89/200 [04:46<05:33,  3.00s/it]Running Inference:  45%|████▌     | 90/200 [04:50<05:40,  3.10s/it]Running Inference:  46%|████▌     | 91/200 [04:51<04:49,  2.65s/it]Running Inference:  46%|████▌     | 92/200 [04:55<05:24,  3.01s/it]Running Inference:  46%|████▋     | 93/200 [04:59<05:38,  3.17s/it]Running Inference:  47%|████▋     | 94/200 [05:03<05:59,  3.39s/it]Running Inference:  48%|████▊     | 95/200 [05:06<06:03,  3.46s/it]Running Inference:  48%|████▊     | 96/200 [05:08<05:10,  2.98s/it]Running Inference:  48%|████▊     | 97/200 [05:10<04:25,  2.58s/it]Running Inference:  49%|████▉     | 98/200 [05:14<05:03,  2.97s/it]Running Inference:  50%|████▉     | 99/200 [05:17<05:19,  3.16s/it]Running Inference:  50%|█████     | 100/200 [05:21<05:21,  3.22s/it]Running Inference:  50%|█████     | 101/200 [05:23<05:09,  3.12s/it]Running Inference:  51%|█████     | 102/200 [05:27<05:21,  3.28s/it]Running Inference:  52%|█████▏    | 103/200 [05:31<05:27,  3.38s/it]Running Inference:  52%|█████▏    | 104/200 [05:32<04:31,  2.83s/it]Running Inference:  52%|█████▎    | 105/200 [05:36<05:00,  3.16s/it]Running Inference:  53%|█████▎    | 106/200 [05:38<04:13,  2.70s/it]Running Inference:  54%|█████▎    | 107/200 [05:41<04:32,  2.93s/it]Running Inference:  54%|█████▍    | 108/200 [05:45<04:51,  3.17s/it]Running Inference:  55%|█████▍    | 109/200 [05:49<05:09,  3.41s/it]Running Inference:  55%|█████▌    | 110/200 [05:53<05:20,  3.56s/it]Running Inference:  56%|█████▌    | 111/200 [05:56<05:15,  3.55s/it]Running Inference:  56%|█████▌    | 112/200 [06:00<05:14,  3.58s/it]Running Inference:  56%|█████▋    | 113/200 [06:04<05:16,  3.64s/it]Running Inference:  57%|█████▋    | 114/200 [06:07<05:02,  3.51s/it]Running Inference:  57%|█████▊    | 115/200 [06:11<05:01,  3.55s/it]Running Inference:  58%|█████▊    | 116/200 [06:12<04:04,  2.91s/it]Running Inference:  58%|█████▊    | 117/200 [06:14<03:26,  2.48s/it]Running Inference:  59%|█████▉    | 118/200 [06:17<03:55,  2.87s/it]Running Inference:  60%|█████▉    | 119/200 [06:21<04:17,  3.18s/it]Running Inference:  60%|██████    | 120/200 [06:25<04:31,  3.40s/it]Running Inference:  60%|██████    | 121/200 [06:29<04:35,  3.48s/it]Running Inference:  61%|██████    | 122/200 [06:32<04:27,  3.43s/it]Running Inference:  62%|██████▏   | 123/200 [06:36<04:30,  3.51s/it]Running Inference:  62%|██████▏   | 124/200 [06:39<04:24,  3.49s/it]Running Inference:  62%|██████▎   | 125/200 [06:41<03:45,  3.01s/it]Running Inference:  63%|██████▎   | 126/200 [06:45<03:56,  3.20s/it]Running Inference:  64%|██████▎   | 127/200 [06:47<03:25,  2.81s/it]Running Inference:  64%|██████▍   | 128/200 [06:50<03:38,  3.03s/it]Running Inference:  64%|██████▍   | 129/200 [06:52<02:58,  2.52s/it]Running Inference:  65%|██████▌   | 130/200 [06:55<03:18,  2.84s/it]Running Inference:  66%|██████▌   | 131/200 [06:58<03:11,  2.78s/it]Running Inference:  66%|██████▌   | 132/200 [07:02<03:33,  3.14s/it]Running Inference:  66%|██████▋   | 133/200 [07:05<03:36,  3.23s/it]Running Inference:  67%|██████▋   | 134/200 [07:09<03:43,  3.39s/it]Running Inference:  68%|██████▊   | 135/200 [07:13<03:43,  3.44s/it]Running Inference:  68%|██████▊   | 136/200 [07:14<03:00,  2.83s/it]Running Inference:  68%|██████▊   | 137/200 [07:18<03:17,  3.13s/it]Running Inference:  69%|██████▉   | 138/200 [07:22<03:24,  3.30s/it]Running Inference:  70%|██████▉   | 139/200 [07:25<03:28,  3.42s/it]Running Inference:  70%|███████   | 140/200 [07:29<03:26,  3.45s/it]Running Inference:  70%|███████   | 141/200 [07:32<03:26,  3.51s/it]Running Inference:  71%|███████   | 142/200 [07:36<03:19,  3.44s/it]Running Inference:  72%|███████▏  | 143/200 [07:37<02:47,  2.94s/it]Running Inference:  72%|███████▏  | 144/200 [07:39<02:29,  2.66s/it]Running Inference:  72%|███████▎  | 145/200 [07:43<02:39,  2.90s/it]Running Inference:  73%|███████▎  | 146/200 [07:46<02:44,  3.05s/it]Running Inference:  74%|███████▎  | 147/200 [07:49<02:41,  3.05s/it]Running Inference:  74%|███████▍  | 148/200 [07:51<02:23,  2.76s/it]Running Inference:  74%|███████▍  | 149/200 [07:55<02:35,  3.04s/it]Running Inference:  75%|███████▌  | 150/200 [07:59<02:40,  3.20s/it]Running Inference:  76%|███████▌  | 151/200 [08:03<02:46,  3.39s/it]Running Inference:  76%|███████▌  | 152/200 [08:06<02:46,  3.47s/it]Running Inference:  76%|███████▋  | 153/200 [08:08<02:15,  2.89s/it]Running Inference:  77%|███████▋  | 154/200 [08:11<02:20,  3.05s/it]Running Inference:  78%|███████▊  | 155/200 [08:13<01:58,  2.64s/it]Running Inference:  78%|███████▊  | 156/200 [08:17<02:13,  3.03s/it]Running Inference:  78%|███████▊  | 157/200 [08:20<02:18,  3.23s/it]Running Inference:  79%|███████▉  | 158/200 [08:22<01:54,  2.72s/it]Running Inference:  80%|███████▉  | 159/200 [08:26<02:01,  2.97s/it]Running Inference:  80%|████████  | 160/200 [08:29<02:01,  3.03s/it]Running Inference:  80%|████████  | 161/200 [08:30<01:40,  2.58s/it]Running Inference:  81%|████████  | 162/200 [08:34<01:49,  2.88s/it]Running Inference:  82%|████████▏ | 163/200 [08:38<01:57,  3.19s/it]Running Inference:  82%|████████▏ | 164/200 [08:40<01:39,  2.75s/it]Running Inference:  82%|████████▎ | 165/200 [08:43<01:46,  3.04s/it]Running Inference:  83%|████████▎ | 166/200 [08:46<01:44,  3.08s/it]Running Inference:  84%|████████▎ | 167/200 [08:50<01:47,  3.27s/it]Running Inference:  84%|████████▍ | 168/200 [08:52<01:32,  2.89s/it]Running Inference:  84%|████████▍ | 169/200 [08:54<01:24,  2.72s/it]Running Inference:  85%|████████▌ | 170/200 [08:56<01:14,  2.47s/it]Running Inference:  86%|████████▌ | 171/200 [09:00<01:23,  2.90s/it]Running Inference:  86%|████████▌ | 172/200 [09:04<01:26,  3.09s/it]Running Inference:  86%|████████▋ | 173/200 [09:07<01:28,  3.28s/it]Running Inference:  87%|████████▋ | 174/200 [09:11<01:30,  3.47s/it]Running Inference:  88%|████████▊ | 175/200 [09:13<01:12,  2.92s/it]Running Inference:  88%|████████▊ | 176/200 [09:16<01:13,  3.05s/it]Running Inference:  88%|████████▊ | 177/200 [09:20<01:15,  3.29s/it]Running Inference:  89%|████████▉ | 178/200 [09:23<01:05,  2.99s/it]Running Inference:  90%|████████▉ | 179/200 [09:24<00:55,  2.63s/it]Running Inference:  90%|█████████ | 180/200 [09:28<00:58,  2.94s/it]Running Inference:  90%|█████████ | 181/200 [09:29<00:47,  2.52s/it]Running Inference:  91%|█████████ | 182/200 [09:33<00:51,  2.86s/it]Running Inference:  92%|█████████▏| 183/200 [09:36<00:49,  2.94s/it]Running Inference:  92%|█████████▏| 184/200 [09:38<00:41,  2.59s/it]Running Inference:  92%|█████████▎| 185/200 [09:42<00:44,  2.96s/it]Running Inference:  93%|█████████▎| 186/200 [09:46<00:44,  3.18s/it]Running Inference:  94%|█████████▎| 187/200 [09:47<00:35,  2.72s/it]Running Inference:  94%|█████████▍| 188/200 [09:51<00:37,  3.12s/it]Running Inference:  94%|█████████▍| 189/200 [09:55<00:35,  3.24s/it]Running Inference:  95%|█████████▌| 190/200 [09:58<00:33,  3.32s/it]Running Inference:  96%|█████████▌| 191/200 [10:02<00:30,  3.42s/it]Running Inference:  96%|█████████▌| 192/200 [10:06<00:28,  3.52s/it]Running Inference:  96%|█████████▋| 193/200 [10:07<00:20,  2.98s/it]Running Inference:  97%|█████████▋| 194/200 [10:11<00:19,  3.24s/it]Running Inference:  98%|█████████▊| 195/200 [10:15<00:17,  3.42s/it]Running Inference:  98%|█████████▊| 196/200 [10:19<00:14,  3.53s/it]Running Inference:  98%|█████████▊| 197/200 [10:23<00:10,  3.61s/it]Running Inference:  99%|█████████▉| 198/200 [10:24<00:06,  3.03s/it]Running Inference: 100%|█████████▉| 199/200 [10:27<00:03,  3.01s/it]Running Inference: 100%|██████████| 200/200 [10:31<00:00,  3.17s/it]Running Inference: 100%|██████████| 200/200 [10:31<00:00,  3.16s/it]
2025-12-14 16:36:39,440 - INFO - Inference completed.
2025-12-14 16:36:39,452 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 16:36:39,452 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:36:39,454 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 16:36:39,454 - INFO - Metrics:
56.32
2025-12-14 16:36:39,455 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_en, ratio=0.5) on GPU 1


========================================
LongBench Task: trec
========================================
----------------------------------------
Task: trec | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:36:45,917 - INFO - Set deterministic seeds to 42
2025-12-14 16:36:45,917 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:36:45,918 - INFO - Starting evaluation run...
2025-12-14 16:36:45,918 - INFO - Output directory set to: longbenchresult
2025-12-14 16:36:45,918 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 16:36:45,918 - INFO - KV Press 'tova' setup.
2025-12-14 16:36:45,918 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:36:45,918 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.66it/s]
Device set to use cuda:0
2025-12-14 16:36:58,661 - INFO - Model pipeline loaded.
2025-12-14 16:36:58,662 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 3842.33 examples/s]
2025-12-14 16:37:02,731 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:37:02,731 - INFO - Dataset processed with 200 entries.
2025-12-14 16:37:02,744 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:35,  1.38s/it]Running Inference:   1%|          | 2/200 [00:02<03:07,  1.06it/s]Running Inference:   2%|▏         | 3/200 [00:03<03:10,  1.04it/s]Running Inference:   2%|▏         | 4/200 [00:03<02:52,  1.14it/s]Running Inference:   2%|▎         | 5/200 [00:05<03:30,  1.08s/it]Running Inference:   3%|▎         | 6/200 [00:06<03:11,  1.01it/s]Running Inference:   4%|▎         | 7/200 [00:06<02:56,  1.09it/s]Running Inference:   4%|▍         | 8/200 [00:07<02:34,  1.24it/s]Running Inference:   4%|▍         | 9/200 [00:07<02:23,  1.33it/s]Running Inference:   5%|▌         | 10/200 [00:09<02:54,  1.09it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:10<03:02,  1.04it/s]Running Inference:   6%|▌         | 12/200 [00:11<03:12,  1.03s/it]Running Inference:   6%|▋         | 13/200 [00:12<02:42,  1.15it/s]Running Inference:   7%|▋         | 14/200 [00:13<03:02,  1.02it/s]Running Inference:   8%|▊         | 15/200 [00:14<02:49,  1.09it/s]Running Inference:   8%|▊         | 16/200 [00:14<02:47,  1.10it/s]Running Inference:   8%|▊         | 17/200 [00:16<03:11,  1.04s/it]Running Inference:   9%|▉         | 18/200 [00:16<02:44,  1.10it/s]Running Inference:  10%|▉         | 19/200 [00:21<05:53,  1.95s/it]Running Inference:  10%|█         | 20/200 [00:22<05:23,  1.80s/it]Running Inference:  10%|█         | 21/200 [00:23<04:25,  1.48s/it]Running Inference:  11%|█         | 22/200 [00:27<06:34,  2.22s/it]Running Inference:  12%|█▏        | 23/200 [00:27<05:05,  1.72s/it]Running Inference:  12%|█▏        | 24/200 [00:29<04:41,  1.60s/it]Running Inference:  12%|█▎        | 25/200 [00:30<04:28,  1.54s/it]Running Inference:  13%|█▎        | 26/200 [00:31<04:06,  1.42s/it]Running Inference:  14%|█▎        | 27/200 [00:33<04:02,  1.40s/it]Running Inference:  14%|█▍        | 28/200 [00:34<04:00,  1.40s/it]Running Inference:  14%|█▍        | 29/200 [00:35<03:19,  1.16s/it]Running Inference:  15%|█▌        | 30/200 [00:36<03:34,  1.26s/it]Running Inference:  16%|█▌        | 31/200 [00:37<03:31,  1.25s/it]Running Inference:  16%|█▌        | 32/200 [00:38<03:11,  1.14s/it]Running Inference:  16%|█▋        | 33/200 [00:39<02:38,  1.05it/s]Running Inference:  17%|█▋        | 34/200 [00:39<02:16,  1.22it/s]Running Inference:  18%|█▊        | 35/200 [00:40<02:24,  1.14it/s]Running Inference:  18%|█▊        | 36/200 [00:41<02:17,  1.20it/s]Running Inference:  18%|█▊        | 37/200 [00:42<02:43,  1.00s/it]Running Inference:  19%|█▉        | 38/200 [00:44<02:51,  1.06s/it]Running Inference:  20%|█▉        | 39/200 [00:44<02:35,  1.03it/s]Running Inference:  20%|██        | 40/200 [00:45<02:23,  1.12it/s]Running Inference:  20%|██        | 41/200 [00:46<02:47,  1.05s/it]Running Inference:  21%|██        | 42/200 [00:48<02:55,  1.11s/it]Running Inference:  22%|██▏       | 43/200 [00:48<02:30,  1.04it/s]Running Inference:  22%|██▏       | 44/200 [00:49<02:24,  1.08it/s]Running Inference:  22%|██▎       | 45/200 [00:51<02:52,  1.11s/it]Running Inference:  23%|██▎       | 46/200 [00:51<02:22,  1.08it/s]Running Inference:  24%|██▎       | 47/200 [00:56<04:58,  1.95s/it]Running Inference:  24%|██▍       | 48/200 [00:56<03:47,  1.50s/it]Running Inference:  24%|██▍       | 49/200 [00:57<03:17,  1.31s/it]Running Inference:  25%|██▌       | 50/200 [00:57<02:39,  1.06s/it]Running Inference:  26%|██▌       | 51/200 [00:58<02:40,  1.08s/it]Running Inference:  26%|██▌       | 52/200 [00:59<02:21,  1.04it/s]Running Inference:  26%|██▋       | 53/200 [01:03<04:18,  1.76s/it]Running Inference:  27%|██▋       | 54/200 [01:03<03:22,  1.39s/it]Running Inference:  28%|██▊       | 55/200 [01:04<02:43,  1.13s/it]Running Inference:  28%|██▊       | 56/200 [01:05<02:37,  1.09s/it]Running Inference:  28%|██▊       | 57/200 [01:05<02:11,  1.09it/s]Running Inference:  29%|██▉       | 58/200 [01:07<02:26,  1.03s/it]Running Inference:  30%|██▉       | 59/200 [01:08<02:34,  1.10s/it]Running Inference:  30%|███       | 60/200 [01:08<02:07,  1.10it/s]Running Inference:  30%|███       | 61/200 [01:10<02:25,  1.05s/it]Running Inference:  31%|███       | 62/200 [01:10<02:08,  1.08it/s]Running Inference:  32%|███▏      | 63/200 [01:11<02:12,  1.03it/s]Running Inference:  32%|███▏      | 64/200 [01:12<02:07,  1.07it/s]Running Inference:  32%|███▎      | 65/200 [01:13<01:51,  1.21it/s]Running Inference:  33%|███▎      | 66/200 [01:17<03:58,  1.78s/it]Running Inference:  34%|███▎      | 67/200 [01:18<03:11,  1.44s/it]Running Inference:  34%|███▍      | 68/200 [01:22<04:51,  2.21s/it]Running Inference:  34%|███▍      | 69/200 [01:23<04:01,  1.85s/it]Running Inference:  35%|███▌      | 70/200 [01:24<03:29,  1.61s/it]Running Inference:  36%|███▌      | 71/200 [01:25<03:15,  1.51s/it]Running Inference:  36%|███▌      | 72/200 [01:29<05:11,  2.44s/it]Running Inference:  36%|███▋      | 73/200 [01:30<04:11,  1.98s/it]Running Inference:  37%|███▋      | 74/200 [01:34<05:14,  2.49s/it]Running Inference:  38%|███▊      | 75/200 [01:35<04:07,  1.98s/it]Running Inference:  38%|███▊      | 76/200 [01:36<03:26,  1.66s/it]Running Inference:  38%|███▊      | 77/200 [01:37<02:55,  1.43s/it]Running Inference:  39%|███▉      | 78/200 [01:38<02:49,  1.39s/it]Running Inference:  40%|███▉      | 79/200 [01:42<04:19,  2.15s/it]Running Inference:  40%|████      | 80/200 [01:43<03:28,  1.74s/it]Running Inference:  40%|████      | 81/200 [01:44<03:04,  1.55s/it]Running Inference:  41%|████      | 82/200 [01:45<02:57,  1.50s/it]Running Inference:  42%|████▏     | 83/200 [01:46<02:33,  1.31s/it]Running Inference:  42%|████▏     | 84/200 [01:47<02:35,  1.34s/it]Running Inference:  42%|████▎     | 85/200 [01:52<04:10,  2.18s/it]Running Inference:  43%|████▎     | 86/200 [01:52<03:20,  1.76s/it]Running Inference:  44%|████▎     | 87/200 [01:53<02:54,  1.55s/it]Running Inference:  44%|████▍     | 88/200 [01:55<02:47,  1.49s/it]Running Inference:  44%|████▍     | 89/200 [01:59<04:32,  2.46s/it]Running Inference:  45%|████▌     | 90/200 [02:00<03:43,  2.03s/it]Running Inference:  46%|████▌     | 91/200 [02:01<03:01,  1.66s/it]Running Inference:  46%|████▌     | 92/200 [02:03<02:46,  1.54s/it]Running Inference:  46%|████▋     | 93/200 [02:07<04:14,  2.38s/it]Running Inference:  47%|████▋     | 94/200 [02:08<03:43,  2.11s/it]Running Inference:  48%|████▊     | 95/200 [02:09<02:50,  1.62s/it]Running Inference:  48%|████▊     | 96/200 [02:10<02:38,  1.52s/it]Running Inference:  48%|████▊     | 97/200 [02:15<04:17,  2.50s/it]Running Inference:  49%|████▉     | 98/200 [02:16<03:43,  2.19s/it]Running Inference:  50%|████▉     | 99/200 [02:18<03:17,  1.96s/it]Running Inference:  50%|█████     | 100/200 [02:19<03:02,  1.83s/it]Running Inference:  50%|█████     | 101/200 [02:23<04:10,  2.53s/it]Running Inference:  51%|█████     | 102/200 [02:24<03:12,  1.96s/it]Running Inference:  52%|█████▏    | 103/200 [02:25<02:48,  1.74s/it]Running Inference:  52%|█████▏    | 104/200 [02:27<02:34,  1.60s/it]Running Inference:  52%|█████▎    | 105/200 [02:28<02:20,  1.47s/it]Running Inference:  53%|█████▎    | 106/200 [02:29<02:08,  1.36s/it]Running Inference:  54%|█████▎    | 107/200 [02:30<01:56,  1.25s/it]Running Inference:  54%|█████▍    | 108/200 [02:34<03:27,  2.26s/it]Running Inference:  55%|█████▍    | 109/200 [02:35<02:50,  1.87s/it]Running Inference:  55%|█████▌    | 110/200 [02:36<02:19,  1.55s/it]Running Inference:  56%|█████▌    | 111/200 [02:40<03:17,  2.22s/it]Running Inference:  56%|█████▌    | 112/200 [02:41<02:45,  1.88s/it]Running Inference:  56%|█████▋    | 113/200 [02:42<02:16,  1.57s/it]Running Inference:  57%|█████▋    | 114/200 [02:46<03:22,  2.35s/it]Running Inference:  57%|█████▊    | 115/200 [02:47<02:38,  1.87s/it]Running Inference:  58%|█████▊    | 116/200 [02:48<02:12,  1.58s/it]Running Inference:  58%|█████▊    | 117/200 [02:49<02:10,  1.57s/it]Running Inference:  59%|█████▉    | 118/200 [02:54<03:20,  2.44s/it]Running Inference:  60%|█████▉    | 119/200 [02:55<02:54,  2.15s/it]Running Inference:  60%|██████    | 120/200 [02:57<02:37,  1.97s/it]Running Inference:  60%|██████    | 121/200 [02:57<01:58,  1.50s/it]Running Inference:  61%|██████    | 122/200 [02:59<01:58,  1.51s/it]Running Inference:  62%|██████▏   | 123/200 [03:00<01:38,  1.27s/it]Running Inference:  62%|██████▏   | 124/200 [03:01<01:36,  1.27s/it]Running Inference:  62%|██████▎   | 125/200 [03:02<01:37,  1.29s/it]Running Inference:  63%|██████▎   | 126/200 [03:06<02:31,  2.05s/it]Running Inference:  64%|██████▎   | 127/200 [03:10<03:18,  2.72s/it]Running Inference:  64%|██████▍   | 128/200 [03:14<03:41,  3.07s/it]Running Inference:  64%|██████▍   | 129/200 [03:15<03:00,  2.54s/it]Running Inference:  65%|██████▌   | 130/200 [03:17<02:34,  2.21s/it]Running Inference:  66%|██████▌   | 131/200 [03:18<02:06,  1.83s/it]Running Inference:  66%|██████▌   | 132/200 [03:19<01:54,  1.68s/it]Running Inference:  66%|██████▋   | 133/200 [03:20<01:45,  1.57s/it]Running Inference:  67%|██████▋   | 134/200 [03:21<01:19,  1.21s/it]Running Inference:  68%|██████▊   | 135/200 [03:21<01:03,  1.02it/s]Running Inference:  68%|██████▊   | 136/200 [03:22<01:00,  1.06it/s]Running Inference:  68%|██████▊   | 137/200 [03:27<02:10,  2.08s/it]Running Inference:  69%|██████▉   | 138/200 [03:27<01:38,  1.59s/it]Running Inference:  70%|██████▉   | 139/200 [03:28<01:25,  1.41s/it]Running Inference:  70%|███████   | 140/200 [03:30<01:24,  1.41s/it]Running Inference:  70%|███████   | 141/200 [03:31<01:28,  1.50s/it]Running Inference:  71%|███████   | 142/200 [03:33<01:22,  1.41s/it]Running Inference:  72%|███████▏  | 143/200 [03:34<01:24,  1.48s/it]Running Inference:  72%|███████▏  | 144/200 [03:38<02:06,  2.25s/it]Running Inference:  72%|███████▎  | 145/200 [03:39<01:36,  1.76s/it]Running Inference:  73%|███████▎  | 146/200 [03:44<02:22,  2.64s/it]Running Inference:  74%|███████▎  | 147/200 [03:44<01:46,  2.01s/it]Running Inference:  74%|███████▍  | 148/200 [03:45<01:21,  1.57s/it]Running Inference:  74%|███████▍  | 149/200 [03:45<01:05,  1.28s/it]Running Inference:  75%|███████▌  | 150/200 [03:46<00:56,  1.13s/it]Running Inference:  76%|███████▌  | 151/200 [03:48<01:00,  1.23s/it]Running Inference:  76%|███████▌  | 152/200 [03:49<00:59,  1.24s/it]Running Inference:  76%|███████▋  | 153/200 [03:49<00:50,  1.07s/it]Running Inference:  77%|███████▋  | 154/200 [03:51<00:53,  1.15s/it]Running Inference:  78%|███████▊  | 155/200 [03:51<00:45,  1.00s/it]Running Inference:  78%|███████▊  | 156/200 [03:52<00:44,  1.01s/it]Running Inference:  78%|███████▊  | 157/200 [03:54<00:48,  1.12s/it]Running Inference:  79%|███████▉  | 158/200 [03:55<00:49,  1.18s/it]Running Inference:  80%|███████▉  | 159/200 [04:00<01:28,  2.16s/it]Running Inference:  80%|████████  | 160/200 [04:00<01:06,  1.66s/it]Running Inference:  80%|████████  | 161/200 [04:01<00:53,  1.37s/it]Running Inference:  81%|████████  | 162/200 [04:02<00:45,  1.20s/it]Running Inference:  82%|████████▏ | 163/200 [04:03<00:42,  1.15s/it]Running Inference:  82%|████████▏ | 164/200 [04:04<00:46,  1.29s/it]Running Inference:  82%|████████▎ | 165/200 [04:05<00:43,  1.23s/it]Running Inference:  83%|████████▎ | 166/200 [04:06<00:34,  1.02s/it]Running Inference:  84%|████████▎ | 167/200 [04:10<01:06,  2.02s/it]Running Inference:  84%|████████▍ | 168/200 [04:15<01:28,  2.76s/it]Running Inference:  84%|████████▍ | 169/200 [04:15<01:03,  2.06s/it]Running Inference:  85%|████████▌ | 170/200 [04:16<00:48,  1.61s/it]Running Inference:  86%|████████▌ | 171/200 [04:16<00:35,  1.23s/it]Running Inference:  86%|████████▌ | 172/200 [04:21<01:01,  2.19s/it]Running Inference:  86%|████████▋ | 173/200 [04:21<00:46,  1.74s/it]Running Inference:  87%|████████▋ | 174/200 [04:25<01:00,  2.34s/it]Running Inference:  88%|████████▊ | 175/200 [04:30<01:16,  3.05s/it]Running Inference:  88%|████████▊ | 176/200 [04:31<01:00,  2.52s/it]Running Inference:  88%|████████▊ | 177/200 [04:31<00:43,  1.90s/it]Running Inference:  89%|████████▉ | 178/200 [04:33<00:39,  1.81s/it]Running Inference:  90%|████████▉ | 179/200 [04:33<00:29,  1.42s/it]Running Inference:  90%|█████████ | 180/200 [04:34<00:22,  1.10s/it]Running Inference:  90%|█████████ | 181/200 [04:35<00:19,  1.05s/it]Running Inference:  91%|█████████ | 182/200 [04:36<00:18,  1.05s/it]Running Inference:  92%|█████████▏| 183/200 [04:40<00:35,  2.12s/it]Running Inference:  92%|█████████▏| 184/200 [04:41<00:27,  1.73s/it]Running Inference:  92%|█████████▎| 185/200 [04:42<00:22,  1.48s/it]Running Inference:  93%|█████████▎| 186/200 [04:43<00:18,  1.30s/it]Running Inference:  94%|█████████▎| 187/200 [04:44<00:15,  1.22s/it]Running Inference:  94%|█████████▍| 188/200 [04:45<00:12,  1.03s/it]Running Inference:  94%|█████████▍| 189/200 [04:46<00:11,  1.01s/it]Running Inference:  95%|█████████▌| 190/200 [04:46<00:09,  1.07it/s]Running Inference:  96%|█████████▌| 191/200 [04:47<00:07,  1.16it/s]Running Inference:  96%|█████████▌| 192/200 [04:48<00:07,  1.03it/s]Running Inference:  96%|█████████▋| 193/200 [04:53<00:14,  2.06s/it]Running Inference:  97%|█████████▋| 194/200 [04:54<00:10,  1.79s/it]Running Inference:  98%|█████████▊| 195/200 [04:55<00:07,  1.41s/it]Running Inference:  98%|█████████▊| 196/200 [04:55<00:04,  1.21s/it]Running Inference:  98%|█████████▊| 197/200 [05:00<00:06,  2.20s/it]Running Inference:  99%|█████████▉| 198/200 [05:01<00:03,  1.80s/it]Running Inference: 100%|█████████▉| 199/200 [05:05<00:02,  2.68s/it]Running Inference: 100%|██████████| 200/200 [05:06<00:00,  2.19s/it]Running Inference: 100%|██████████| 200/200 [05:06<00:00,  1.53s/it]
2025-12-14 16:42:09,708 - INFO - Inference completed.
2025-12-14 16:42:09,730 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 16:42:09,730 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:42:09,731 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 16:42:09,731 - INFO - Metrics:
22.0
2025-12-14 16:42:09,733 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=trec, ratio=0.1) on GPU 1

----------------------------------------
Task: trec | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:42:16,204 - INFO - Set deterministic seeds to 42
2025-12-14 16:42:16,204 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:42:16,204 - INFO - Starting evaluation run...
2025-12-14 16:42:16,204 - INFO - Output directory set to: longbenchresult
2025-12-14 16:42:16,205 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 16:42:16,205 - INFO - KV Press 'tova' setup.
2025-12-14 16:42:16,205 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:42:16,205 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.21it/s]
Device set to use cuda:0
2025-12-14 16:42:30,410 - INFO - Model pipeline loaded.
2025-12-14 16:42:30,410 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 16:42:43,111 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:42:43,111 - INFO - Dataset processed with 200 entries.
2025-12-14 16:42:43,124 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:34,  1.38s/it]Running Inference:   1%|          | 2/200 [00:01<03:04,  1.07it/s]Running Inference:   2%|▏         | 3/200 [00:03<03:10,  1.03it/s]Running Inference:   2%|▏         | 4/200 [00:03<02:52,  1.14it/s]Running Inference:   2%|▎         | 5/200 [00:08<07:30,  2.31s/it]Running Inference:   3%|▎         | 6/200 [00:09<05:49,  1.80s/it]Running Inference:   4%|▎         | 7/200 [00:10<04:42,  1.47s/it]Running Inference:   4%|▍         | 8/200 [00:10<03:46,  1.18s/it]Running Inference:   4%|▍         | 9/200 [00:11<03:13,  1.01s/it]Running Inference:   5%|▌         | 10/200 [00:12<03:30,  1.11s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:16<06:21,  2.02s/it]Running Inference:   6%|▌         | 12/200 [00:17<05:31,  1.76s/it]Running Inference:   6%|▋         | 13/200 [00:18<04:19,  1.39s/it]Running Inference:   7%|▋         | 14/200 [00:19<04:11,  1.35s/it]Running Inference:   8%|▊         | 15/200 [00:20<03:37,  1.18s/it]Running Inference:   8%|▊         | 16/200 [00:21<03:21,  1.09s/it]Running Inference:   8%|▊         | 17/200 [00:22<03:34,  1.17s/it]Running Inference:   9%|▉         | 18/200 [00:23<03:01,  1.00it/s]Running Inference:  10%|▉         | 19/200 [00:27<06:10,  2.04s/it]Running Inference:  10%|█         | 20/200 [00:29<05:35,  1.86s/it]Running Inference:  10%|█         | 21/200 [00:30<04:33,  1.53s/it]Running Inference:  11%|█         | 22/200 [00:31<04:24,  1.49s/it]Running Inference:  12%|█▏        | 23/200 [00:31<03:31,  1.19s/it]Running Inference:  12%|█▏        | 24/200 [00:33<03:36,  1.23s/it]Running Inference:  12%|█▎        | 25/200 [00:34<03:43,  1.28s/it]Running Inference:  13%|█▎        | 26/200 [00:35<03:40,  1.27s/it]Running Inference:  14%|█▎        | 27/200 [00:37<03:44,  1.30s/it]Running Inference:  14%|█▍        | 28/200 [00:38<03:45,  1.31s/it]Running Inference:  14%|█▍        | 29/200 [00:39<03:09,  1.11s/it]Running Inference:  15%|█▌        | 30/200 [00:40<03:27,  1.22s/it]Running Inference:  16%|█▌        | 31/200 [00:41<03:27,  1.23s/it]Running Inference:  16%|█▌        | 32/200 [00:42<03:08,  1.12s/it]Running Inference:  16%|█▋        | 33/200 [00:43<02:36,  1.07it/s]Running Inference:  17%|█▋        | 34/200 [00:43<02:14,  1.23it/s]Running Inference:  18%|█▊        | 35/200 [00:44<02:24,  1.14it/s]Running Inference:  18%|█▊        | 36/200 [00:45<02:17,  1.19it/s]Running Inference:  18%|█▊        | 37/200 [00:47<02:43,  1.00s/it]Running Inference:  19%|█▉        | 38/200 [00:48<02:52,  1.06s/it]Running Inference:  20%|█▉        | 39/200 [00:48<02:35,  1.03it/s]Running Inference:  20%|██        | 40/200 [00:49<02:27,  1.08it/s]Running Inference:  20%|██        | 41/200 [00:51<02:50,  1.07s/it]Running Inference:  21%|██        | 42/200 [00:51<02:28,  1.06it/s]Running Inference:  22%|██▏       | 43/200 [00:52<02:16,  1.15it/s]Running Inference:  22%|██▏       | 44/200 [00:53<02:15,  1.15it/s]Running Inference:  22%|██▎       | 45/200 [00:54<02:39,  1.03s/it]Running Inference:  23%|██▎       | 46/200 [00:55<02:13,  1.15it/s]Running Inference:  24%|██▎       | 47/200 [00:59<04:56,  1.94s/it]Running Inference:  24%|██▍       | 48/200 [01:00<03:44,  1.48s/it]Running Inference:  24%|██▍       | 49/200 [01:01<03:15,  1.30s/it]Running Inference:  25%|██▌       | 50/200 [01:01<02:38,  1.05s/it]Running Inference:  26%|██▌       | 51/200 [01:02<02:39,  1.07s/it]Running Inference:  26%|██▌       | 52/200 [01:03<02:21,  1.05it/s]Running Inference:  26%|██▋       | 53/200 [01:07<04:26,  1.81s/it]Running Inference:  27%|██▋       | 54/200 [01:07<03:24,  1.40s/it]Running Inference:  28%|██▊       | 55/200 [01:08<02:45,  1.14s/it]Running Inference:  28%|██▊       | 56/200 [01:09<02:38,  1.10s/it]Running Inference:  28%|██▊       | 57/200 [01:09<02:12,  1.08it/s]Running Inference:  29%|██▉       | 58/200 [01:10<02:23,  1.01s/it]Running Inference:  30%|██▉       | 59/200 [01:12<02:33,  1.09s/it]Running Inference:  30%|███       | 60/200 [01:12<02:06,  1.11it/s]Running Inference:  30%|███       | 61/200 [01:13<02:24,  1.04s/it]Running Inference:  31%|███       | 62/200 [01:14<02:07,  1.08it/s]Running Inference:  32%|███▏      | 63/200 [01:15<02:12,  1.03it/s]Running Inference:  32%|███▏      | 64/200 [01:16<02:07,  1.07it/s]Running Inference:  32%|███▎      | 65/200 [01:17<01:51,  1.21it/s]Running Inference:  33%|███▎      | 66/200 [01:21<04:02,  1.81s/it]Running Inference:  34%|███▎      | 67/200 [01:21<03:14,  1.46s/it]Running Inference:  34%|███▍      | 68/200 [01:25<04:56,  2.25s/it]Running Inference:  34%|███▍      | 69/200 [01:26<04:06,  1.88s/it]Running Inference:  35%|███▌      | 70/200 [01:28<03:32,  1.63s/it]Running Inference:  36%|███▌      | 71/200 [01:29<03:17,  1.53s/it]Running Inference:  36%|███▌      | 72/200 [01:34<05:17,  2.48s/it]Running Inference:  36%|███▋      | 73/200 [01:34<04:15,  2.01s/it]Running Inference:  37%|███▋      | 74/200 [01:38<05:20,  2.54s/it]Running Inference:  38%|███▊      | 75/200 [01:39<04:12,  2.02s/it]Running Inference:  38%|███▊      | 76/200 [01:40<03:29,  1.69s/it]Running Inference:  38%|███▊      | 77/200 [01:41<02:57,  1.45s/it]Running Inference:  39%|███▉      | 78/200 [01:42<02:50,  1.40s/it]Running Inference:  40%|███▉      | 79/200 [01:46<04:24,  2.19s/it]Running Inference:  40%|████      | 80/200 [01:47<03:32,  1.77s/it]Running Inference:  40%|████      | 81/200 [01:48<03:06,  1.57s/it]Running Inference:  41%|████      | 82/200 [01:52<04:47,  2.44s/it]Running Inference:  42%|████▏     | 83/200 [01:53<03:49,  1.96s/it]Running Inference:  42%|████▏     | 84/200 [01:55<03:29,  1.80s/it]Running Inference:  42%|████▎     | 85/200 [01:59<04:50,  2.52s/it]Running Inference:  43%|████▎     | 86/200 [02:00<03:47,  2.00s/it]Running Inference:  44%|████▎     | 87/200 [02:01<03:14,  1.72s/it]Running Inference:  44%|████▍     | 88/200 [02:02<03:01,  1.62s/it]Running Inference:  44%|████▍     | 89/200 [02:07<04:45,  2.57s/it]Running Inference:  45%|████▌     | 90/200 [02:08<03:49,  2.08s/it]Running Inference:  46%|████▌     | 91/200 [02:09<03:05,  1.70s/it]Running Inference:  46%|████▌     | 92/200 [02:13<04:39,  2.59s/it]Running Inference:  46%|████▋     | 93/200 [02:18<05:35,  3.14s/it]Running Inference:  47%|████▋     | 94/200 [02:19<04:37,  2.62s/it]Running Inference:  48%|████▊     | 95/200 [02:20<03:28,  1.98s/it]Running Inference:  48%|████▊     | 96/200 [02:21<03:05,  1.78s/it]Running Inference:  48%|████▊     | 97/200 [02:26<04:38,  2.71s/it]Running Inference:  49%|████▉     | 98/200 [02:27<03:58,  2.34s/it]Running Inference:  50%|████▉     | 99/200 [02:29<03:28,  2.07s/it]Running Inference:  50%|█████     | 100/200 [02:30<03:10,  1.91s/it]Running Inference:  50%|█████     | 101/200 [02:35<04:18,  2.61s/it]Running Inference:  51%|█████     | 102/200 [02:35<03:15,  2.00s/it]Running Inference:  52%|█████▏    | 103/200 [02:36<02:50,  1.76s/it]Running Inference:  52%|█████▏    | 104/200 [02:38<02:36,  1.63s/it]Running Inference:  52%|█████▎    | 105/200 [02:39<02:21,  1.49s/it]Running Inference:  53%|█████▎    | 106/200 [02:40<02:11,  1.40s/it]Running Inference:  54%|█████▎    | 107/200 [02:41<01:58,  1.28s/it]Running Inference:  54%|█████▍    | 108/200 [02:46<03:31,  2.30s/it]Running Inference:  55%|█████▍    | 109/200 [02:47<02:52,  1.90s/it]Running Inference:  55%|█████▌    | 110/200 [02:47<02:21,  1.57s/it]Running Inference:  56%|█████▌    | 111/200 [02:51<03:21,  2.26s/it]Running Inference:  56%|█████▌    | 112/200 [02:53<02:50,  1.94s/it]Running Inference:  56%|█████▋    | 113/200 [02:53<02:20,  1.61s/it]Running Inference:  57%|█████▋    | 114/200 [02:58<03:27,  2.41s/it]Running Inference:  57%|█████▊    | 115/200 [02:59<02:46,  1.96s/it]Running Inference:  58%|█████▊    | 116/200 [03:03<03:38,  2.60s/it]Running Inference:  58%|█████▊    | 117/200 [03:04<03:10,  2.29s/it]Running Inference:  59%|█████▉    | 118/200 [03:09<04:03,  2.97s/it]Running Inference:  60%|█████▉    | 119/200 [03:13<04:36,  3.41s/it]Running Inference:  60%|██████    | 120/200 [03:15<03:48,  2.85s/it]Running Inference:  60%|██████    | 121/200 [03:15<02:48,  2.14s/it]Running Inference:  61%|██████    | 122/200 [03:17<02:32,  1.96s/it]Running Inference:  62%|██████▏   | 123/200 [03:18<02:02,  1.59s/it]Running Inference:  62%|██████▏   | 124/200 [03:19<01:53,  1.49s/it]Running Inference:  62%|██████▎   | 125/200 [03:20<01:48,  1.45s/it]Running Inference:  63%|██████▎   | 126/200 [03:24<02:41,  2.18s/it]Running Inference:  64%|██████▎   | 127/200 [03:28<03:27,  2.84s/it]Running Inference:  64%|██████▍   | 128/200 [03:32<03:48,  3.18s/it]Running Inference:  64%|██████▍   | 129/200 [03:34<03:05,  2.61s/it]Running Inference:  65%|██████▌   | 130/200 [03:35<02:38,  2.27s/it]Running Inference:  66%|██████▌   | 131/200 [03:36<02:09,  1.87s/it]Running Inference:  66%|██████▌   | 132/200 [03:37<01:56,  1.71s/it]Running Inference:  66%|██████▋   | 133/200 [03:39<01:46,  1.60s/it]Running Inference:  67%|██████▋   | 134/200 [03:39<01:20,  1.22s/it]Running Inference:  68%|██████▊   | 135/200 [03:40<01:03,  1.02it/s]Running Inference:  68%|██████▊   | 136/200 [03:44<02:04,  1.94s/it]Running Inference:  68%|██████▊   | 137/200 [03:48<02:56,  2.80s/it]Running Inference:  69%|██████▉   | 138/200 [03:49<02:10,  2.10s/it]Running Inference:  70%|██████▉   | 139/200 [03:50<01:47,  1.76s/it]Running Inference:  70%|███████   | 140/200 [03:51<01:39,  1.65s/it]Running Inference:  70%|███████   | 141/200 [03:53<01:40,  1.70s/it]Running Inference:  71%|███████   | 142/200 [03:54<01:29,  1.55s/it]Running Inference:  72%|███████▏  | 143/200 [03:56<01:28,  1.55s/it]Running Inference:  72%|███████▏  | 144/200 [03:57<01:12,  1.29s/it]Running Inference:  72%|███████▎  | 145/200 [03:57<00:59,  1.09s/it]Running Inference:  73%|███████▎  | 146/200 [04:02<01:58,  2.19s/it]Running Inference:  74%|███████▎  | 147/200 [04:03<01:30,  1.70s/it]Running Inference:  74%|███████▍  | 148/200 [04:03<01:10,  1.35s/it]Running Inference:  74%|███████▍  | 149/200 [04:04<00:57,  1.13s/it]Running Inference:  75%|███████▌  | 150/200 [04:04<00:51,  1.02s/it]Running Inference:  76%|███████▌  | 151/200 [04:06<00:56,  1.16s/it]Running Inference:  76%|███████▌  | 152/200 [04:11<01:45,  2.19s/it]Running Inference:  76%|███████▋  | 153/200 [04:11<01:21,  1.73s/it]Running Inference:  77%|███████▋  | 154/200 [04:13<01:14,  1.62s/it]Running Inference:  78%|███████▊  | 155/200 [04:13<00:59,  1.33s/it]Running Inference:  78%|███████▊  | 156/200 [04:14<00:54,  1.24s/it]Running Inference:  78%|███████▊  | 157/200 [04:16<00:54,  1.28s/it]Running Inference:  79%|███████▉  | 158/200 [04:17<00:54,  1.30s/it]Running Inference:  80%|███████▉  | 159/200 [04:21<01:32,  2.27s/it]Running Inference:  80%|████████  | 160/200 [04:22<01:09,  1.74s/it]Running Inference:  80%|████████  | 161/200 [04:23<00:55,  1.42s/it]Running Inference:  81%|████████  | 162/200 [04:23<00:46,  1.23s/it]Running Inference:  82%|████████▏ | 163/200 [04:24<00:43,  1.18s/it]Running Inference:  82%|████████▏ | 164/200 [04:26<00:46,  1.29s/it]Running Inference:  82%|████████▎ | 165/200 [04:27<00:43,  1.23s/it]Running Inference:  83%|████████▎ | 166/200 [04:28<00:34,  1.02s/it]Running Inference:  84%|████████▎ | 167/200 [04:32<01:07,  2.04s/it]Running Inference:  84%|████████▍ | 168/200 [04:37<01:29,  2.80s/it]Running Inference:  84%|████████▍ | 169/200 [04:37<01:04,  2.10s/it]Running Inference:  85%|████████▌ | 170/200 [04:38<00:50,  1.68s/it]Running Inference:  86%|████████▌ | 171/200 [04:38<00:37,  1.28s/it]Running Inference:  86%|████████▌ | 172/200 [04:43<01:03,  2.25s/it]Running Inference:  86%|████████▋ | 173/200 [04:43<00:48,  1.78s/it]Running Inference:  87%|████████▋ | 174/200 [04:47<01:02,  2.40s/it]Running Inference:  88%|████████▊ | 175/200 [04:52<01:17,  3.11s/it]Running Inference:  88%|████████▊ | 176/200 [04:53<01:01,  2.57s/it]Running Inference:  88%|████████▊ | 177/200 [04:54<00:44,  1.93s/it]Running Inference:  89%|████████▉ | 178/200 [04:55<00:40,  1.83s/it]Running Inference:  90%|████████▉ | 179/200 [04:56<00:30,  1.44s/it]Running Inference:  90%|█████████ | 180/200 [04:56<00:22,  1.12s/it]Running Inference:  90%|█████████ | 181/200 [04:57<00:20,  1.06s/it]Running Inference:  91%|█████████ | 182/200 [04:58<00:19,  1.06s/it]Running Inference:  92%|█████████▏| 183/200 [05:03<00:36,  2.15s/it]Running Inference:  92%|█████████▏| 184/200 [05:04<00:28,  1.75s/it]Running Inference:  92%|█████████▎| 185/200 [05:05<00:22,  1.50s/it]Running Inference:  93%|█████████▎| 186/200 [05:06<00:18,  1.31s/it]Running Inference:  94%|█████████▎| 187/200 [05:07<00:15,  1.23s/it]Running Inference:  94%|█████████▍| 188/200 [05:07<00:12,  1.06s/it]Running Inference:  94%|█████████▍| 189/200 [05:08<00:11,  1.03s/it]Running Inference:  95%|█████████▌| 190/200 [05:12<00:19,  1.93s/it]Running Inference:  96%|█████████▌| 191/200 [05:13<00:14,  1.56s/it]Running Inference:  96%|█████████▌| 192/200 [05:14<00:11,  1.50s/it]Running Inference:  96%|█████████▋| 193/200 [05:19<00:17,  2.46s/it]Running Inference:  97%|█████████▋| 194/200 [05:20<00:12,  2.07s/it]Running Inference:  98%|█████████▊| 195/200 [05:21<00:08,  1.61s/it]Running Inference:  98%|█████████▊| 196/200 [05:21<00:05,  1.35s/it]Running Inference:  98%|█████████▊| 197/200 [05:26<00:06,  2.33s/it]Running Inference:  99%|█████████▉| 198/200 [05:27<00:03,  1.89s/it]Running Inference: 100%|█████████▉| 199/200 [05:32<00:02,  2.77s/it]Running Inference: 100%|██████████| 200/200 [05:33<00:00,  2.25s/it]Running Inference: 100%|██████████| 200/200 [05:33<00:00,  1.67s/it]
2025-12-14 16:48:16,367 - INFO - Inference completed.
2025-12-14 16:48:16,388 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 16:48:16,389 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:48:16,390 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 16:48:16,390 - INFO - Metrics:
22.0
2025-12-14 16:48:16,391 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=trec, ratio=0.2) on GPU 1

----------------------------------------
Task: trec | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:48:22,867 - INFO - Set deterministic seeds to 42
2025-12-14 16:48:22,867 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:48:22,867 - INFO - Starting evaluation run...
2025-12-14 16:48:22,867 - INFO - Output directory set to: longbenchresult
2025-12-14 16:48:22,868 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 16:48:22,868 - INFO - KV Press 'tova' setup.
2025-12-14 16:48:22,868 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:48:22,868 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.79it/s]
Device set to use cuda:0
2025-12-14 16:48:35,044 - INFO - Model pipeline loaded.
2025-12-14 16:48:35,045 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 16:48:49,858 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:48:49,858 - INFO - Dataset processed with 200 entries.
2025-12-14 16:48:49,872 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:38,  1.40s/it]Running Inference:   1%|          | 2/200 [00:02<03:15,  1.01it/s]Running Inference:   2%|▏         | 3/200 [00:03<03:16,  1.00it/s]Running Inference:   2%|▏         | 4/200 [00:03<02:55,  1.12it/s]Running Inference:   2%|▎         | 5/200 [00:05<03:35,  1.11s/it]Running Inference:   3%|▎         | 6/200 [00:06<03:14,  1.00s/it]Running Inference:   4%|▎         | 7/200 [00:06<02:59,  1.08it/s]Running Inference:   4%|▍         | 8/200 [00:07<02:36,  1.23it/s]Running Inference:   4%|▍         | 9/200 [00:08<02:25,  1.32it/s]Running Inference:   5%|▌         | 10/200 [00:12<05:54,  1.86s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:16<07:58,  2.53s/it]Running Inference:   6%|▌         | 12/200 [00:17<06:38,  2.12s/it]Running Inference:   6%|▋         | 13/200 [00:18<05:05,  1.63s/it]Running Inference:   7%|▋         | 14/200 [00:19<04:43,  1.52s/it]Running Inference:   8%|▊         | 15/200 [00:20<03:59,  1.29s/it]Running Inference:   8%|▊         | 16/200 [00:21<03:36,  1.18s/it]Running Inference:   8%|▊         | 17/200 [00:22<03:56,  1.29s/it]Running Inference:   9%|▉         | 18/200 [00:23<03:16,  1.08s/it]Running Inference:  10%|▉         | 19/200 [00:24<03:31,  1.17s/it]Running Inference:  10%|█         | 20/200 [00:26<03:45,  1.25s/it]Running Inference:  10%|█         | 21/200 [00:26<03:16,  1.10s/it]Running Inference:  11%|█         | 22/200 [00:30<05:49,  1.96s/it]Running Inference:  12%|█▏        | 23/200 [00:31<04:29,  1.53s/it]Running Inference:  12%|█▏        | 24/200 [00:32<04:14,  1.45s/it]Running Inference:  12%|█▎        | 25/200 [00:33<04:10,  1.43s/it]Running Inference:  13%|█▎        | 26/200 [00:35<03:53,  1.34s/it]Running Inference:  14%|█▎        | 27/200 [00:36<03:53,  1.35s/it]Running Inference:  14%|█▍        | 28/200 [00:37<03:51,  1.35s/it]Running Inference:  14%|█▍        | 29/200 [00:38<03:17,  1.16s/it]Running Inference:  15%|█▌        | 30/200 [00:40<03:33,  1.25s/it]Running Inference:  16%|█▌        | 31/200 [00:41<03:30,  1.25s/it]Running Inference:  16%|█▌        | 32/200 [00:42<03:10,  1.14s/it]Running Inference:  16%|█▋        | 33/200 [00:45<05:23,  1.94s/it]Running Inference:  17%|█▋        | 34/200 [00:46<04:11,  1.51s/it]Running Inference:  18%|█▊        | 35/200 [00:47<03:45,  1.36s/it]Running Inference:  18%|█▊        | 36/200 [00:51<05:49,  2.13s/it]Running Inference:  18%|█▊        | 37/200 [00:52<05:11,  1.91s/it]Running Inference:  19%|█▉        | 38/200 [00:53<04:34,  1.70s/it]Running Inference:  20%|█▉        | 39/200 [00:54<03:47,  1.41s/it]Running Inference:  20%|██        | 40/200 [00:55<03:17,  1.23s/it]Running Inference:  20%|██        | 41/200 [00:57<03:30,  1.32s/it]Running Inference:  21%|██        | 42/200 [00:57<02:56,  1.12s/it]Running Inference:  22%|██▏       | 43/200 [00:58<02:35,  1.01it/s]Running Inference:  22%|██▏       | 44/200 [00:59<02:28,  1.05it/s]Running Inference:  22%|██▎       | 45/200 [01:00<02:54,  1.13s/it]Running Inference:  23%|██▎       | 46/200 [01:01<02:24,  1.07it/s]Running Inference:  24%|██▎       | 47/200 [01:05<05:01,  1.97s/it]Running Inference:  24%|██▍       | 48/200 [01:06<03:51,  1.53s/it]Running Inference:  24%|██▍       | 49/200 [01:07<03:20,  1.33s/it]Running Inference:  25%|██▌       | 50/200 [01:07<02:37,  1.05s/it]Running Inference:  26%|██▌       | 51/200 [01:08<02:30,  1.01s/it]Running Inference:  26%|██▌       | 52/200 [01:09<02:14,  1.10it/s]Running Inference:  26%|██▋       | 53/200 [01:12<04:16,  1.74s/it]Running Inference:  27%|██▋       | 54/200 [01:13<03:20,  1.37s/it]Running Inference:  28%|██▊       | 55/200 [01:13<02:42,  1.12s/it]Running Inference:  28%|██▊       | 56/200 [01:14<02:36,  1.09s/it]Running Inference:  28%|██▊       | 57/200 [01:15<02:10,  1.09it/s]Running Inference:  29%|██▉       | 58/200 [01:16<02:18,  1.02it/s]Running Inference:  30%|██▉       | 59/200 [01:17<02:29,  1.06s/it]Running Inference:  30%|███       | 60/200 [01:18<02:04,  1.13it/s]Running Inference:  30%|███       | 61/200 [01:19<02:23,  1.03s/it]Running Inference:  31%|███       | 62/200 [01:20<02:06,  1.09it/s]Running Inference:  32%|███▏      | 63/200 [01:21<02:11,  1.04it/s]Running Inference:  32%|███▏      | 64/200 [01:22<02:08,  1.06it/s]Running Inference:  32%|███▎      | 65/200 [01:22<01:52,  1.20it/s]Running Inference:  33%|███▎      | 66/200 [01:26<04:01,  1.80s/it]Running Inference:  34%|███▎      | 67/200 [01:27<03:13,  1.46s/it]Running Inference:  34%|███▍      | 68/200 [01:28<02:45,  1.25s/it]Running Inference:  34%|███▍      | 69/200 [01:29<02:34,  1.18s/it]Running Inference:  35%|███▌      | 70/200 [01:30<02:29,  1.15s/it]Running Inference:  36%|███▌      | 71/200 [01:31<02:31,  1.18s/it]Running Inference:  36%|███▌      | 72/200 [01:36<04:43,  2.22s/it]Running Inference:  36%|███▋      | 73/200 [01:37<03:51,  1.83s/it]Running Inference:  37%|███▋      | 74/200 [01:39<04:31,  2.15s/it]Running Inference:  38%|███▊      | 75/200 [01:40<03:37,  1.74s/it]Running Inference:  38%|███▊      | 76/200 [01:41<03:05,  1.50s/it]Running Inference:  38%|███▊      | 77/200 [01:42<02:41,  1.31s/it]Running Inference:  39%|███▉      | 78/200 [01:43<02:39,  1.30s/it]Running Inference:  40%|███▉      | 79/200 [01:47<04:15,  2.11s/it]Running Inference:  40%|████      | 80/200 [01:48<03:25,  1.71s/it]Running Inference:  40%|████      | 81/200 [01:49<03:02,  1.53s/it]Running Inference:  41%|████      | 82/200 [01:51<03:16,  1.67s/it]Running Inference:  42%|████▏     | 83/200 [01:52<02:46,  1.42s/it]Running Inference:  42%|████▏     | 84/200 [01:54<02:45,  1.43s/it]Running Inference:  42%|████▎     | 85/200 [01:58<04:18,  2.25s/it]Running Inference:  43%|████▎     | 86/200 [01:58<03:25,  1.81s/it]Running Inference:  44%|████▎     | 87/200 [02:00<02:58,  1.58s/it]Running Inference:  44%|████▍     | 88/200 [02:03<04:17,  2.30s/it]Running Inference:  44%|████▍     | 89/200 [02:08<05:37,  3.04s/it]Running Inference:  45%|████▌     | 90/200 [02:09<04:24,  2.41s/it]Running Inference:  46%|████▌     | 91/200 [02:10<03:30,  1.93s/it]Running Inference:  46%|████▌     | 92/200 [02:15<04:55,  2.73s/it]Running Inference:  46%|████▋     | 93/200 [02:19<05:45,  3.23s/it]Running Inference:  47%|████▋     | 94/200 [02:21<05:01,  2.85s/it]Running Inference:  48%|████▊     | 95/200 [02:21<03:44,  2.14s/it]Running Inference:  48%|████▊     | 96/200 [02:23<03:16,  1.89s/it]Running Inference:  48%|████▊     | 97/200 [02:28<04:45,  2.77s/it]Running Inference:  49%|████▉     | 98/200 [02:29<04:02,  2.38s/it]Running Inference:  50%|████▉     | 99/200 [02:31<03:32,  2.11s/it]Running Inference:  50%|█████     | 100/200 [02:32<03:10,  1.91s/it]Running Inference:  50%|█████     | 101/200 [02:36<04:17,  2.60s/it]Running Inference:  51%|█████     | 102/200 [02:37<03:17,  2.02s/it]Running Inference:  52%|█████▏    | 103/200 [02:38<02:55,  1.81s/it]Running Inference:  52%|█████▏    | 104/200 [02:39<02:39,  1.66s/it]Running Inference:  52%|█████▎    | 105/200 [02:41<02:23,  1.51s/it]Running Inference:  53%|█████▎    | 106/200 [02:42<02:10,  1.39s/it]Running Inference:  54%|█████▎    | 107/200 [02:43<01:58,  1.27s/it]Running Inference:  54%|█████▍    | 108/200 [02:44<01:57,  1.28s/it]Running Inference:  55%|█████▍    | 109/200 [02:45<01:47,  1.19s/it]Running Inference:  55%|█████▌    | 110/200 [02:46<01:38,  1.10s/it]Running Inference:  56%|█████▌    | 111/200 [02:50<02:52,  1.94s/it]Running Inference:  56%|█████▌    | 112/200 [02:51<02:30,  1.71s/it]Running Inference:  56%|█████▋    | 113/200 [02:52<02:06,  1.45s/it]Running Inference:  57%|█████▋    | 114/200 [02:53<01:56,  1.36s/it]Running Inference:  57%|█████▊    | 115/200 [02:54<01:43,  1.22s/it]Running Inference:  58%|█████▊    | 116/200 [02:55<01:31,  1.09s/it]Running Inference:  58%|█████▊    | 117/200 [02:56<01:42,  1.23s/it]Running Inference:  59%|█████▉    | 118/200 [03:01<03:02,  2.22s/it]Running Inference:  60%|█████▉    | 119/200 [03:02<02:43,  2.02s/it]Running Inference:  60%|██████    | 120/200 [03:04<02:30,  1.88s/it]Running Inference:  60%|██████    | 121/200 [03:04<01:52,  1.43s/it]Running Inference:  61%|██████    | 122/200 [03:06<01:52,  1.44s/it]Running Inference:  62%|██████▏   | 123/200 [03:06<01:33,  1.22s/it]Running Inference:  62%|██████▏   | 124/200 [03:08<01:33,  1.23s/it]Running Inference:  62%|██████▎   | 125/200 [03:09<01:35,  1.27s/it]Running Inference:  63%|██████▎   | 126/200 [03:13<02:31,  2.05s/it]Running Inference:  64%|██████▎   | 127/200 [03:17<03:19,  2.73s/it]Running Inference:  64%|██████▍   | 128/200 [03:21<03:42,  3.09s/it]Running Inference:  64%|██████▍   | 129/200 [03:23<03:04,  2.60s/it]Running Inference:  65%|██████▌   | 130/200 [03:24<02:38,  2.26s/it]Running Inference:  66%|██████▌   | 131/200 [03:25<02:08,  1.86s/it]Running Inference:  66%|██████▌   | 132/200 [03:26<01:56,  1.71s/it]Running Inference:  66%|██████▋   | 133/200 [03:28<01:49,  1.63s/it]Running Inference:  67%|██████▋   | 134/200 [03:28<01:21,  1.24s/it]Running Inference:  68%|██████▊   | 135/200 [03:29<01:04,  1.00it/s]Running Inference:  68%|██████▊   | 136/200 [03:29<01:01,  1.04it/s]Running Inference:  68%|██████▊   | 137/200 [03:31<01:07,  1.07s/it]Running Inference:  69%|██████▉   | 138/200 [03:31<00:55,  1.12it/s]Running Inference:  70%|██████▉   | 139/200 [03:32<00:55,  1.09it/s]Running Inference:  70%|███████   | 140/200 [03:34<01:03,  1.06s/it]Running Inference:  70%|███████   | 141/200 [03:35<01:15,  1.28s/it]Running Inference:  71%|███████   | 142/200 [03:37<01:12,  1.25s/it]Running Inference:  72%|███████▏  | 143/200 [03:38<01:16,  1.34s/it]Running Inference:  72%|███████▏  | 144/200 [03:39<01:04,  1.14s/it]Running Inference:  72%|███████▎  | 145/200 [03:43<01:46,  1.94s/it]Running Inference:  73%|███████▎  | 146/200 [03:47<02:29,  2.78s/it]Running Inference:  74%|███████▎  | 147/200 [03:48<01:51,  2.11s/it]Running Inference:  74%|███████▍  | 148/200 [03:48<01:25,  1.63s/it]Running Inference:  74%|███████▍  | 149/200 [03:49<01:07,  1.33s/it]Running Inference:  75%|███████▌  | 150/200 [03:50<00:59,  1.20s/it]Running Inference:  76%|███████▌  | 151/200 [03:51<01:03,  1.29s/it]Running Inference:  76%|███████▌  | 152/200 [03:56<01:48,  2.27s/it]Running Inference:  76%|███████▋  | 153/200 [03:57<01:23,  1.79s/it]Running Inference:  77%|███████▋  | 154/200 [03:58<01:16,  1.66s/it]Running Inference:  78%|███████▊  | 155/200 [03:59<01:01,  1.36s/it]Running Inference:  78%|███████▊  | 156/200 [04:00<00:55,  1.25s/it]Running Inference:  78%|███████▊  | 157/200 [04:01<00:56,  1.32s/it]Running Inference:  79%|███████▉  | 158/200 [04:02<00:55,  1.33s/it]Running Inference:  80%|███████▉  | 159/200 [04:04<00:54,  1.32s/it]Running Inference:  80%|████████  | 160/200 [04:04<00:44,  1.10s/it]Running Inference:  80%|████████  | 161/200 [04:05<00:38,  1.02it/s]Running Inference:  81%|████████  | 162/200 [04:06<00:35,  1.07it/s]Running Inference:  82%|████████▏ | 163/200 [04:07<00:35,  1.03it/s]Running Inference:  82%|████████▏ | 164/200 [04:08<00:40,  1.14s/it]Running Inference:  82%|████████▎ | 165/200 [04:13<01:11,  2.04s/it]Running Inference:  83%|████████▎ | 166/200 [04:13<00:54,  1.59s/it]Running Inference:  84%|████████▎ | 167/200 [04:18<01:20,  2.43s/it]Running Inference:  84%|████████▍ | 168/200 [04:22<01:38,  3.07s/it]Running Inference:  84%|████████▍ | 169/200 [04:23<01:10,  2.28s/it]Running Inference:  85%|████████▌ | 170/200 [04:23<00:54,  1.80s/it]Running Inference:  86%|████████▌ | 171/200 [04:24<00:39,  1.37s/it]Running Inference:  86%|████████▌ | 172/200 [04:28<01:04,  2.30s/it]Running Inference:  86%|████████▋ | 173/200 [04:29<00:48,  1.81s/it]Running Inference:  87%|████████▋ | 174/200 [04:33<01:02,  2.41s/it]Running Inference:  88%|████████▊ | 175/200 [04:37<01:17,  3.11s/it]Running Inference:  88%|████████▊ | 176/200 [04:39<01:01,  2.57s/it]Running Inference:  88%|████████▊ | 177/200 [04:39<00:44,  1.93s/it]Running Inference:  89%|████████▉ | 178/200 [04:40<00:37,  1.70s/it]Running Inference:  90%|████████▉ | 179/200 [04:41<00:28,  1.35s/it]Running Inference:  90%|█████████ | 180/200 [04:41<00:21,  1.05s/it]Running Inference:  90%|█████████ | 181/200 [04:42<00:19,  1.01s/it]Running Inference:  91%|█████████ | 182/200 [04:43<00:18,  1.04s/it]Running Inference:  92%|█████████▏| 183/200 [04:44<00:19,  1.14s/it]Running Inference:  92%|█████████▏| 184/200 [04:46<00:18,  1.13s/it]Running Inference:  92%|█████████▎| 185/200 [04:47<00:16,  1.07s/it]Running Inference:  93%|█████████▎| 186/200 [04:47<00:14,  1.01s/it]Running Inference:  94%|█████████▎| 187/200 [04:48<00:13,  1.01s/it]Running Inference:  94%|█████████▍| 188/200 [04:49<00:10,  1.13it/s]Running Inference:  94%|█████████▍| 189/200 [04:50<00:10,  1.10it/s]Running Inference:  95%|█████████▌| 190/200 [04:54<00:18,  1.83s/it]Running Inference:  96%|█████████▌| 191/200 [04:55<00:13,  1.49s/it]Running Inference:  96%|█████████▌| 192/200 [04:56<00:11,  1.44s/it]Running Inference:  96%|█████████▋| 193/200 [05:01<00:16,  2.41s/it]Running Inference:  97%|█████████▋| 194/200 [05:02<00:12,  2.04s/it]Running Inference:  98%|█████████▊| 195/200 [05:02<00:07,  1.58s/it]Running Inference:  98%|█████████▊| 196/200 [05:03<00:05,  1.33s/it]Running Inference:  98%|█████████▊| 197/200 [05:04<00:03,  1.32s/it]Running Inference:  99%|█████████▉| 198/200 [05:05<00:02,  1.18s/it]Running Inference: 100%|█████████▉| 199/200 [05:10<00:02,  2.27s/it]Running Inference: 100%|██████████| 200/200 [05:11<00:00,  1.91s/it]Running Inference: 100%|██████████| 200/200 [05:11<00:00,  1.56s/it]
2025-12-14 16:54:01,484 - INFO - Inference completed.
2025-12-14 16:54:01,505 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 16:54:01,505 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:54:01,507 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 16:54:01,507 - INFO - Metrics:
18.5
2025-12-14 16:54:01,508 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=trec, ratio=0.3) on GPU 1

----------------------------------------
Task: trec | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:54:08,336 - INFO - Set deterministic seeds to 42
2025-12-14 16:54:08,336 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:54:08,336 - INFO - Starting evaluation run...
2025-12-14 16:54:08,336 - INFO - Output directory set to: longbenchresult
2025-12-14 16:54:08,336 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 16:54:08,336 - INFO - KV Press 'tova' setup.
2025-12-14 16:54:08,336 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:54:08,336 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.41it/s]
Device set to use cuda:0
2025-12-14 16:54:22,148 - INFO - Model pipeline loaded.
2025-12-14 16:54:22,149 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 16:54:31,388 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:54:31,388 - INFO - Dataset processed with 200 entries.
2025-12-14 16:54:31,402 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:42,  1.42s/it]Running Inference:   1%|          | 2/200 [00:05<09:51,  2.99s/it]Running Inference:   2%|▏         | 3/200 [00:06<06:48,  2.07s/it]Running Inference:   2%|▏         | 4/200 [00:07<04:59,  1.53s/it]Running Inference:   2%|▎         | 5/200 [00:08<04:56,  1.52s/it]Running Inference:   3%|▎         | 6/200 [00:09<04:07,  1.28s/it]Running Inference:   4%|▎         | 7/200 [00:10<03:34,  1.11s/it]Running Inference:   4%|▍         | 8/200 [00:10<03:00,  1.06it/s]Running Inference:   4%|▍         | 9/200 [00:11<02:41,  1.18it/s]Running Inference:   5%|▌         | 10/200 [00:12<03:05,  1.02it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:16<06:05,  1.93s/it]Running Inference:   6%|▌         | 12/200 [00:18<05:19,  1.70s/it]Running Inference:   6%|▋         | 13/200 [00:18<04:16,  1.37s/it]Running Inference:   7%|▋         | 14/200 [00:19<04:09,  1.34s/it]Running Inference:   8%|▊         | 15/200 [00:20<03:35,  1.17s/it]Running Inference:   8%|▊         | 16/200 [00:21<03:24,  1.11s/it]Running Inference:   8%|▊         | 17/200 [00:23<03:37,  1.19s/it]Running Inference:   9%|▉         | 18/200 [00:23<03:03,  1.01s/it]Running Inference:  10%|▉         | 19/200 [00:24<03:22,  1.12s/it]Running Inference:  10%|█         | 20/200 [00:26<03:43,  1.24s/it]Running Inference:  10%|█         | 21/200 [00:27<03:15,  1.09s/it]Running Inference:  11%|█         | 22/200 [00:27<02:49,  1.05it/s]Running Inference:  12%|█▏        | 23/200 [00:28<02:25,  1.22it/s]Running Inference:  12%|█▏        | 24/200 [00:29<02:49,  1.04it/s]Running Inference:  12%|█▎        | 25/200 [00:31<03:11,  1.09s/it]Running Inference:  13%|█▎        | 26/200 [00:32<03:04,  1.06s/it]Running Inference:  14%|█▎        | 27/200 [00:33<03:12,  1.11s/it]Running Inference:  14%|█▍        | 28/200 [00:34<03:23,  1.18s/it]Running Inference:  14%|█▍        | 29/200 [00:35<02:55,  1.03s/it]Running Inference:  15%|█▌        | 30/200 [00:36<03:17,  1.16s/it]Running Inference:  16%|█▌        | 31/200 [00:38<03:20,  1.19s/it]Running Inference:  16%|█▌        | 32/200 [00:38<02:59,  1.07s/it]Running Inference:  16%|█▋        | 33/200 [00:39<02:26,  1.14it/s]Running Inference:  17%|█▋        | 34/200 [00:39<02:07,  1.30it/s]Running Inference:  18%|█▊        | 35/200 [00:40<02:19,  1.19it/s]Running Inference:  18%|█▊        | 36/200 [00:44<04:52,  1.78s/it]Running Inference:  18%|█▊        | 37/200 [00:46<04:31,  1.67s/it]Running Inference:  19%|█▉        | 38/200 [00:47<04:07,  1.52s/it]Running Inference:  20%|█▉        | 39/200 [00:48<03:28,  1.29s/it]Running Inference:  20%|██        | 40/200 [00:48<02:59,  1.12s/it]Running Inference:  20%|██        | 41/200 [00:50<03:10,  1.20s/it]Running Inference:  21%|██        | 42/200 [00:50<02:42,  1.03s/it]Running Inference:  22%|██▏       | 43/200 [00:51<02:29,  1.05it/s]Running Inference:  22%|██▏       | 44/200 [00:52<02:32,  1.02it/s]Running Inference:  22%|██▎       | 45/200 [00:54<02:51,  1.11s/it]Running Inference:  23%|██▎       | 46/200 [00:54<02:22,  1.08it/s]Running Inference:  24%|██▎       | 47/200 [00:58<05:02,  1.98s/it]Running Inference:  24%|██▍       | 48/200 [00:59<03:50,  1.52s/it]Running Inference:  24%|██▍       | 49/200 [01:00<03:19,  1.32s/it]Running Inference:  25%|██▌       | 50/200 [01:00<02:37,  1.05s/it]Running Inference:  26%|██▌       | 51/200 [01:01<02:29,  1.01s/it]Running Inference:  26%|██▌       | 52/200 [01:02<02:14,  1.10it/s]Running Inference:  26%|██▋       | 53/200 [01:06<04:21,  1.78s/it]Running Inference:  27%|██▋       | 54/200 [01:06<03:26,  1.41s/it]Running Inference:  28%|██▊       | 55/200 [01:07<02:46,  1.15s/it]Running Inference:  28%|██▊       | 56/200 [01:08<02:38,  1.10s/it]Running Inference:  28%|██▊       | 57/200 [01:08<02:12,  1.08it/s]Running Inference:  29%|██▉       | 58/200 [01:09<02:23,  1.01s/it]Running Inference:  30%|██▉       | 59/200 [01:11<02:33,  1.09s/it]Running Inference:  30%|███       | 60/200 [01:11<02:06,  1.11it/s]Running Inference:  30%|███       | 61/200 [01:13<02:26,  1.05s/it]Running Inference:  31%|███       | 62/200 [01:13<02:09,  1.07it/s]Running Inference:  32%|███▏      | 63/200 [01:14<02:13,  1.03it/s]Running Inference:  32%|███▏      | 64/200 [01:15<02:09,  1.05it/s]Running Inference:  32%|███▎      | 65/200 [01:19<04:13,  1.88s/it]Running Inference:  33%|███▎      | 66/200 [01:23<05:40,  2.54s/it]Running Inference:  34%|███▎      | 67/200 [01:24<04:20,  1.96s/it]Running Inference:  34%|███▍      | 68/200 [01:25<03:30,  1.59s/it]Running Inference:  34%|███▍      | 69/200 [01:26<03:10,  1.46s/it]Running Inference:  35%|███▌      | 70/200 [01:27<02:54,  1.34s/it]Running Inference:  36%|███▌      | 71/200 [01:28<02:50,  1.33s/it]Running Inference:  36%|███▌      | 72/200 [01:29<02:45,  1.29s/it]Running Inference:  36%|███▋      | 73/200 [01:30<02:32,  1.20s/it]Running Inference:  37%|███▋      | 74/200 [01:32<02:57,  1.41s/it]Running Inference:  38%|███▊      | 75/200 [01:33<02:33,  1.22s/it]Running Inference:  38%|███▊      | 76/200 [01:34<02:20,  1.13s/it]Running Inference:  38%|███▊      | 77/200 [01:35<02:09,  1.06s/it]Running Inference:  39%|███▉      | 78/200 [01:36<02:17,  1.13s/it]Running Inference:  40%|███▉      | 79/200 [01:40<04:01,  2.00s/it]Running Inference:  40%|████      | 80/200 [01:41<03:16,  1.63s/it]Running Inference:  40%|████      | 81/200 [01:42<02:55,  1.47s/it]Running Inference:  41%|████      | 82/200 [01:46<04:39,  2.37s/it]Running Inference:  42%|████▏     | 83/200 [01:47<03:43,  1.91s/it]Running Inference:  42%|████▏     | 84/200 [01:49<03:25,  1.77s/it]Running Inference:  42%|████▎     | 85/200 [01:50<02:56,  1.53s/it]Running Inference:  43%|████▎     | 86/200 [01:51<02:29,  1.31s/it]Running Inference:  44%|████▎     | 87/200 [01:52<02:22,  1.26s/it]Running Inference:  44%|████▍     | 88/200 [01:56<03:53,  2.09s/it]Running Inference:  44%|████▍     | 89/200 [02:01<05:22,  2.90s/it]Running Inference:  45%|████▌     | 90/200 [02:02<04:17,  2.34s/it]Running Inference:  46%|████▌     | 91/200 [02:02<03:26,  1.90s/it]Running Inference:  46%|████▌     | 92/200 [02:07<04:55,  2.73s/it]Running Inference:  46%|████▋     | 93/200 [02:12<05:46,  3.24s/it]Running Inference:  47%|████▋     | 94/200 [02:16<06:24,  3.63s/it]Running Inference:  48%|████▊     | 95/200 [02:17<04:42,  2.69s/it]Running Inference:  48%|████▊     | 96/200 [02:21<05:31,  3.19s/it]Running Inference:  48%|████▊     | 97/200 [02:26<06:19,  3.69s/it]Running Inference:  49%|████▉     | 98/200 [02:27<04:51,  2.86s/it]Running Inference:  50%|████▉     | 99/200 [02:28<04:05,  2.43s/it]Running Inference:  50%|█████     | 100/200 [02:30<03:33,  2.13s/it]Running Inference:  50%|█████     | 101/200 [02:31<03:03,  1.85s/it]Running Inference:  51%|█████     | 102/200 [02:31<02:23,  1.47s/it]Running Inference:  52%|█████▏    | 103/200 [02:33<02:18,  1.43s/it]Running Inference:  52%|█████▏    | 104/200 [02:35<02:31,  1.58s/it]Running Inference:  52%|█████▎    | 105/200 [02:36<02:16,  1.43s/it]Running Inference:  53%|█████▎    | 106/200 [02:37<02:05,  1.33s/it]Running Inference:  54%|█████▎    | 107/200 [02:38<01:57,  1.27s/it]Running Inference:  54%|█████▍    | 108/200 [02:39<01:57,  1.28s/it]Running Inference:  55%|█████▍    | 109/200 [02:40<01:47,  1.19s/it]Running Inference:  55%|█████▌    | 110/200 [02:44<03:06,  2.07s/it]Running Inference:  56%|█████▌    | 111/200 [02:45<02:26,  1.65s/it]Running Inference:  56%|█████▌    | 112/200 [02:46<02:13,  1.51s/it]Running Inference:  56%|█████▋    | 113/200 [02:47<01:53,  1.31s/it]Running Inference:  57%|█████▋    | 114/200 [02:48<01:48,  1.26s/it]Running Inference:  57%|█████▊    | 115/200 [02:49<01:38,  1.16s/it]Running Inference:  58%|█████▊    | 116/200 [02:50<01:25,  1.02s/it]Running Inference:  58%|█████▊    | 117/200 [02:51<01:38,  1.18s/it]Running Inference:  59%|█████▉    | 118/200 [02:56<03:00,  2.20s/it]Running Inference:  60%|█████▉    | 119/200 [02:57<02:35,  1.92s/it]Running Inference:  60%|██████    | 120/200 [02:59<02:24,  1.81s/it]Running Inference:  60%|██████    | 121/200 [02:59<01:48,  1.38s/it]Running Inference:  61%|██████    | 122/200 [03:01<01:49,  1.40s/it]Running Inference:  62%|██████▏   | 123/200 [03:01<01:34,  1.22s/it]Running Inference:  62%|██████▏   | 124/200 [03:03<01:39,  1.31s/it]Running Inference:  62%|██████▎   | 125/200 [03:04<01:40,  1.35s/it]Running Inference:  63%|██████▎   | 126/200 [03:08<02:36,  2.12s/it]Running Inference:  64%|██████▎   | 127/200 [03:13<03:24,  2.80s/it]Running Inference:  64%|██████▍   | 128/200 [03:17<03:47,  3.16s/it]Running Inference:  64%|██████▍   | 129/200 [03:21<04:18,  3.64s/it]Running Inference:  65%|██████▌   | 130/200 [03:23<03:29,  2.99s/it]Running Inference:  66%|██████▌   | 131/200 [03:24<02:41,  2.35s/it]Running Inference:  66%|██████▌   | 132/200 [03:25<02:20,  2.06s/it]Running Inference:  66%|██████▋   | 133/200 [03:27<02:05,  1.87s/it]Running Inference:  67%|██████▋   | 134/200 [03:27<01:33,  1.41s/it]Running Inference:  68%|██████▊   | 135/200 [03:27<01:12,  1.12s/it]Running Inference:  68%|██████▊   | 136/200 [03:28<01:06,  1.04s/it]Running Inference:  68%|██████▊   | 137/200 [03:29<01:11,  1.13s/it]Running Inference:  69%|██████▉   | 138/200 [03:30<01:00,  1.03it/s]Running Inference:  70%|██████▉   | 139/200 [03:31<00:59,  1.03it/s]Running Inference:  70%|███████   | 140/200 [03:32<01:06,  1.10s/it]Running Inference:  70%|███████   | 141/200 [03:33<01:03,  1.07s/it]Running Inference:  71%|███████   | 142/200 [03:35<01:04,  1.11s/it]Running Inference:  72%|███████▏  | 143/200 [03:36<01:10,  1.24s/it]Running Inference:  72%|███████▏  | 144/200 [03:37<01:00,  1.07s/it]Running Inference:  72%|███████▎  | 145/200 [03:39<01:13,  1.33s/it]Running Inference:  73%|███████▎  | 146/200 [03:44<02:08,  2.37s/it]Running Inference:  74%|███████▎  | 147/200 [03:44<01:36,  1.82s/it]Running Inference:  74%|███████▍  | 148/200 [03:45<01:14,  1.44s/it]Running Inference:  74%|███████▍  | 149/200 [03:45<01:02,  1.22s/it]Running Inference:  75%|███████▌  | 150/200 [03:46<00:56,  1.13s/it]Running Inference:  76%|███████▌  | 151/200 [03:48<01:00,  1.24s/it]Running Inference:  76%|███████▌  | 152/200 [03:52<01:47,  2.25s/it]Running Inference:  76%|███████▋  | 153/200 [03:53<01:23,  1.77s/it]Running Inference:  77%|███████▋  | 154/200 [03:55<01:16,  1.67s/it]Running Inference:  78%|███████▊  | 155/200 [03:55<01:01,  1.37s/it]Running Inference:  78%|███████▊  | 156/200 [03:56<00:55,  1.26s/it]Running Inference:  78%|███████▊  | 157/200 [03:58<00:57,  1.33s/it]Running Inference:  79%|███████▉  | 158/200 [03:59<00:57,  1.36s/it]Running Inference:  80%|███████▉  | 159/200 [04:00<00:54,  1.32s/it]Running Inference:  80%|████████  | 160/200 [04:01<00:42,  1.07s/it]Running Inference:  80%|████████  | 161/200 [04:02<00:38,  1.02it/s]Running Inference:  81%|████████  | 162/200 [04:02<00:35,  1.08it/s]Running Inference:  82%|████████▏ | 163/200 [04:03<00:36,  1.02it/s]Running Inference:  82%|████████▏ | 164/200 [04:05<00:42,  1.17s/it]Running Inference:  82%|████████▎ | 165/200 [04:10<01:16,  2.19s/it]Running Inference:  83%|████████▎ | 166/200 [04:10<00:57,  1.70s/it]Running Inference:  84%|████████▎ | 167/200 [04:11<00:48,  1.47s/it]Running Inference:  84%|████████▍ | 168/200 [04:13<00:45,  1.44s/it]Running Inference:  84%|████████▍ | 169/200 [04:13<00:35,  1.14s/it]Running Inference:  85%|████████▌ | 170/200 [04:14<00:30,  1.01s/it]Running Inference:  86%|████████▌ | 171/200 [04:14<00:23,  1.23it/s]Running Inference:  86%|████████▌ | 172/200 [04:19<00:53,  1.93s/it]Running Inference:  86%|████████▋ | 173/200 [04:19<00:41,  1.55s/it]Running Inference:  87%|████████▋ | 174/200 [04:23<00:58,  2.24s/it]Running Inference:  88%|████████▊ | 175/200 [04:28<01:15,  3.01s/it]Running Inference:  88%|████████▊ | 176/200 [04:29<01:00,  2.52s/it]Running Inference:  88%|████████▊ | 177/200 [04:30<00:43,  1.90s/it]Running Inference:  89%|████████▉ | 178/200 [04:31<00:37,  1.71s/it]Running Inference:  90%|████████▉ | 179/200 [04:31<00:28,  1.35s/it]Running Inference:  90%|█████████ | 180/200 [04:32<00:21,  1.05s/it]Running Inference:  90%|█████████ | 181/200 [04:33<00:19,  1.01s/it]Running Inference:  91%|█████████ | 182/200 [04:34<00:18,  1.05s/it]Running Inference:  92%|█████████▏| 183/200 [04:35<00:19,  1.15s/it]Running Inference:  92%|█████████▏| 184/200 [04:36<00:16,  1.05s/it]Running Inference:  92%|█████████▎| 185/200 [04:37<00:15,  1.01s/it]Running Inference:  93%|█████████▎| 186/200 [04:38<00:14,  1.02s/it]Running Inference:  94%|█████████▎| 187/200 [04:39<00:13,  1.02s/it]Running Inference:  94%|█████████▍| 188/200 [04:41<00:15,  1.27s/it]Running Inference:  94%|█████████▍| 189/200 [04:42<00:13,  1.18s/it]Running Inference:  95%|█████████▌| 190/200 [04:46<00:20,  2.03s/it]Running Inference:  96%|█████████▌| 191/200 [04:47<00:14,  1.63s/it]Running Inference:  96%|█████████▌| 192/200 [04:48<00:12,  1.53s/it]Running Inference:  96%|█████████▋| 193/200 [04:49<00:10,  1.47s/it]Running Inference:  97%|█████████▋| 194/200 [04:50<00:08,  1.36s/it]Running Inference:  98%|█████████▊| 195/200 [04:51<00:05,  1.11s/it]Running Inference:  98%|█████████▊| 196/200 [04:55<00:07,  1.95s/it]Running Inference:  98%|█████████▊| 197/200 [04:56<00:05,  1.75s/it]Running Inference:  99%|█████████▉| 198/200 [04:57<00:03,  1.51s/it]Running Inference: 100%|█████████▉| 199/200 [05:02<00:02,  2.52s/it]Running Inference: 100%|██████████| 200/200 [05:03<00:00,  2.11s/it]Running Inference: 100%|██████████| 200/200 [05:03<00:00,  1.52s/it]
2025-12-14 16:59:34,985 - INFO - Inference completed.
2025-12-14 16:59:35,008 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 16:59:35,008 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:59:35,009 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 16:59:35,009 - INFO - Metrics:
14.5
2025-12-14 16:59:35,010 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=trec, ratio=0.5) on GPU 1


========================================
LongBench Task: dureader
========================================
----------------------------------------
Task: dureader | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:59:41,466 - INFO - Set deterministic seeds to 42
2025-12-14 16:59:41,466 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:59:41,466 - INFO - Starting evaluation run...
2025-12-14 16:59:41,466 - INFO - Output directory set to: longbenchresult
2025-12-14 16:59:41,466 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 16:59:41,466 - INFO - KV Press 'tova' setup.
2025-12-14 16:59:41,466 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:59:41,466 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.63it/s]
Device set to use cuda:0
2025-12-14 16:59:53,521 - INFO - Model pipeline loaded.
2025-12-14 16:59:53,521 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 1218.06 examples/s]Generating test split: 200 examples [00:00, 1210.48 examples/s]
2025-12-14 17:00:03,194 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:00:03,194 - INFO - Dataset processed with 200 entries.
2025-12-14 17:00:03,222 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:07,  8.78s/it]Running Inference:   1%|          | 2/200 [00:10<15:51,  4.80s/it]Running Inference:   2%|▏         | 3/200 [00:18<19:31,  5.95s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:29,  4.13s/it]Running Inference:   2%|▎         | 5/200 [00:21<11:31,  3.55s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:47,  4.89s/it]Running Inference:   4%|▎         | 7/200 [00:37<18:32,  5.76s/it]Running Inference:   4%|▍         | 8/200 [00:44<20:29,  6.40s/it]Running Inference:   4%|▍         | 9/200 [00:52<21:53,  6.88s/it]Running Inference:   5%|▌         | 10/200 [00:59<22:07,  6.99s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:06<22:03,  7.00s/it]Running Inference:   6%|▌         | 12/200 [01:14<22:06,  7.06s/it]Running Inference:   6%|▋         | 13/200 [01:16<17:20,  5.57s/it]Running Inference:   7%|▋         | 14/200 [01:23<18:56,  6.11s/it]Running Inference:   8%|▊         | 15/200 [01:30<19:52,  6.45s/it]Running Inference:   8%|▊         | 16/200 [01:37<20:15,  6.60s/it]Running Inference:   8%|▊         | 17/200 [01:45<20:39,  6.77s/it]Running Inference:   9%|▉         | 18/200 [01:52<21:18,  7.03s/it]Running Inference:  10%|▉         | 19/200 [01:59<21:25,  7.10s/it]Running Inference:  10%|█         | 20/200 [02:07<21:45,  7.25s/it]Running Inference:  10%|█         | 21/200 [02:15<22:03,  7.39s/it]Running Inference:  11%|█         | 22/200 [02:22<21:45,  7.33s/it]Running Inference:  12%|█▏        | 23/200 [02:29<21:24,  7.26s/it]Running Inference:  12%|█▏        | 24/200 [02:36<21:14,  7.24s/it]Running Inference:  12%|█▎        | 25/200 [02:43<20:48,  7.14s/it]Running Inference:  13%|█▎        | 26/200 [02:50<20:52,  7.20s/it]Running Inference:  14%|█▎        | 27/200 [02:58<20:49,  7.22s/it]Running Inference:  14%|█▍        | 28/200 [03:05<20:38,  7.20s/it]Running Inference:  14%|█▍        | 29/200 [03:12<20:48,  7.30s/it]Running Inference:  15%|█▌        | 30/200 [03:19<20:18,  7.17s/it]Running Inference:  16%|█▌        | 31/200 [03:27<20:22,  7.23s/it]Running Inference:  16%|█▌        | 32/200 [03:34<19:54,  7.11s/it]Running Inference:  16%|█▋        | 33/200 [03:41<19:56,  7.16s/it]Running Inference:  17%|█▋        | 34/200 [03:48<19:58,  7.22s/it]Running Inference:  18%|█▊        | 35/200 [03:55<19:44,  7.18s/it]Running Inference:  18%|█▊        | 36/200 [04:03<20:02,  7.33s/it]Running Inference:  18%|█▊        | 37/200 [04:08<18:27,  6.80s/it]Running Inference:  19%|█▉        | 38/200 [04:17<19:37,  7.27s/it]Running Inference:  20%|█▉        | 39/200 [04:24<19:42,  7.35s/it]Running Inference:  20%|██        | 40/200 [04:32<19:58,  7.49s/it]Running Inference:  20%|██        | 41/200 [04:35<16:31,  6.24s/it]Running Inference:  21%|██        | 42/200 [04:43<17:06,  6.49s/it]Running Inference:  22%|██▏       | 43/200 [04:50<17:54,  6.84s/it]Running Inference:  22%|██▏       | 44/200 [04:57<17:53,  6.88s/it]Running Inference:  22%|██▎       | 45/200 [05:00<14:36,  5.65s/it]Running Inference:  23%|██▎       | 46/200 [05:07<15:43,  6.13s/it]Running Inference:  24%|██▎       | 47/200 [05:10<12:47,  5.02s/it]Running Inference:  24%|██▍       | 48/200 [05:17<14:33,  5.75s/it]Running Inference:  24%|██▍       | 49/200 [05:21<13:01,  5.18s/it]Running Inference:  25%|██▌       | 50/200 [05:28<14:35,  5.84s/it]Running Inference:  26%|██▌       | 51/200 [05:36<15:40,  6.31s/it]Running Inference:  26%|██▌       | 52/200 [05:38<12:35,  5.10s/it]Running Inference:  26%|██▋       | 53/200 [05:46<14:18,  5.84s/it]Running Inference:  27%|██▋       | 54/200 [05:53<15:35,  6.41s/it]Running Inference:  28%|██▊       | 55/200 [06:00<15:52,  6.57s/it]Running Inference:  28%|██▊       | 56/200 [06:07<15:54,  6.63s/it]Running Inference:  28%|██▊       | 57/200 [06:14<16:08,  6.77s/it]Running Inference:  29%|██▉       | 58/200 [06:19<14:43,  6.22s/it]Running Inference:  30%|██▉       | 59/200 [06:27<15:34,  6.63s/it]Running Inference:  30%|███       | 60/200 [06:34<16:14,  6.96s/it]Running Inference:  30%|███       | 61/200 [06:42<16:53,  7.29s/it]Running Inference:  31%|███       | 62/200 [06:50<16:58,  7.38s/it]Running Inference:  32%|███▏      | 63/200 [06:58<17:10,  7.52s/it]Running Inference:  32%|███▏      | 64/200 [07:05<17:01,  7.51s/it]Running Inference:  32%|███▎      | 65/200 [07:13<17:06,  7.60s/it]Running Inference:  33%|███▎      | 66/200 [07:21<16:54,  7.57s/it]Running Inference:  34%|███▎      | 67/200 [07:29<16:56,  7.64s/it]Running Inference:  34%|███▍      | 68/200 [07:36<16:30,  7.50s/it]Running Inference:  34%|███▍      | 69/200 [07:44<16:36,  7.61s/it]Running Inference:  35%|███▌      | 70/200 [07:52<16:44,  7.72s/it]Running Inference:  36%|███▌      | 71/200 [07:59<16:32,  7.69s/it]Running Inference:  36%|███▌      | 72/200 [08:06<15:54,  7.46s/it]Running Inference:  36%|███▋      | 73/200 [08:14<16:07,  7.62s/it]Running Inference:  37%|███▋      | 74/200 [08:22<15:54,  7.58s/it]Running Inference:  38%|███▊      | 75/200 [08:29<15:35,  7.48s/it]Running Inference:  38%|███▊      | 76/200 [08:36<15:35,  7.54s/it]Running Inference:  38%|███▊      | 77/200 [08:44<15:29,  7.55s/it]Running Inference:  39%|███▉      | 78/200 [08:52<15:30,  7.62s/it]Running Inference:  40%|███▉      | 79/200 [08:59<15:14,  7.56s/it]Running Inference:  40%|████      | 80/200 [09:07<15:06,  7.55s/it]Running Inference:  40%|████      | 81/200 [09:14<14:44,  7.44s/it]Running Inference:  41%|████      | 82/200 [09:21<14:36,  7.43s/it]Running Inference:  42%|████▏     | 83/200 [09:29<14:19,  7.35s/it]Running Inference:  42%|████▏     | 84/200 [09:36<14:09,  7.32s/it]Running Inference:  42%|████▎     | 85/200 [09:43<14:03,  7.34s/it]Running Inference:  43%|████▎     | 86/200 [09:50<13:51,  7.29s/it]Running Inference:  44%|████▎     | 87/200 [09:57<13:36,  7.23s/it]Running Inference:  44%|████▍     | 88/200 [10:05<13:27,  7.21s/it]Running Inference:  44%|████▍     | 89/200 [10:12<13:29,  7.30s/it]Running Inference:  45%|████▌     | 90/200 [10:19<13:14,  7.22s/it]Running Inference:  46%|████▌     | 91/200 [10:27<13:15,  7.30s/it]Running Inference:  46%|████▌     | 92/200 [10:34<13:09,  7.31s/it]Running Inference:  46%|████▋     | 93/200 [10:42<13:16,  7.44s/it]Running Inference:  47%|████▋     | 94/200 [10:50<13:20,  7.55s/it]Running Inference:  48%|████▊     | 95/200 [10:57<13:09,  7.52s/it]Running Inference:  48%|████▊     | 96/200 [11:04<12:55,  7.45s/it]Running Inference:  48%|████▊     | 97/200 [11:11<12:30,  7.28s/it]Running Inference:  49%|████▉     | 98/200 [11:16<11:17,  6.64s/it]Running Inference:  50%|████▉     | 99/200 [11:24<11:40,  6.93s/it]Running Inference:  50%|█████     | 100/200 [11:31<11:39,  6.99s/it]Running Inference:  50%|█████     | 101/200 [11:38<11:39,  7.06s/it]Running Inference:  51%|█████     | 102/200 [11:46<11:38,  7.12s/it]Running Inference:  52%|█████▏    | 103/200 [11:53<11:41,  7.23s/it]Running Inference:  52%|█████▏    | 104/200 [12:00<11:35,  7.25s/it]Running Inference:  52%|█████▎    | 105/200 [12:08<11:29,  7.25s/it]Running Inference:  53%|█████▎    | 106/200 [12:15<11:24,  7.28s/it]Running Inference:  54%|█████▎    | 107/200 [12:22<11:24,  7.36s/it]Running Inference:  54%|█████▍    | 108/200 [12:30<11:12,  7.31s/it]Running Inference:  55%|█████▍    | 109/200 [12:37<11:13,  7.41s/it]Running Inference:  55%|█████▌    | 110/200 [12:46<11:30,  7.67s/it]Running Inference:  56%|█████▌    | 111/200 [12:53<11:10,  7.53s/it]Running Inference:  56%|█████▌    | 112/200 [13:00<10:47,  7.36s/it]Running Inference:  56%|█████▋    | 113/200 [13:07<10:44,  7.41s/it]Running Inference:  57%|█████▋    | 114/200 [13:15<10:36,  7.40s/it]Running Inference:  57%|█████▊    | 115/200 [13:22<10:30,  7.42s/it]Running Inference:  58%|█████▊    | 116/200 [13:30<10:24,  7.44s/it]Running Inference:  58%|█████▊    | 117/200 [13:33<08:38,  6.24s/it]Running Inference:  59%|█████▉    | 118/200 [13:41<09:20,  6.84s/it]Running Inference:  60%|█████▉    | 119/200 [13:49<09:46,  7.24s/it]Running Inference:  60%|██████    | 120/200 [13:58<10:09,  7.61s/it]Running Inference:  60%|██████    | 121/200 [14:05<09:59,  7.58s/it]Running Inference:  61%|██████    | 122/200 [14:12<09:36,  7.39s/it]Running Inference:  62%|██████▏   | 123/200 [14:20<09:37,  7.50s/it]Running Inference:  62%|██████▏   | 124/200 [14:27<09:25,  7.44s/it]Running Inference:  62%|██████▎   | 125/200 [14:34<09:08,  7.31s/it]Running Inference:  63%|██████▎   | 126/200 [14:41<08:45,  7.10s/it]Running Inference:  64%|██████▎   | 127/200 [14:48<08:44,  7.18s/it]Running Inference:  64%|██████▍   | 128/200 [14:56<08:37,  7.19s/it]Running Inference:  64%|██████▍   | 129/200 [15:01<07:41,  6.50s/it]Running Inference:  65%|██████▌   | 130/200 [15:08<07:50,  6.72s/it]Running Inference:  66%|██████▌   | 131/200 [15:15<07:52,  6.84s/it]Running Inference:  66%|██████▌   | 132/200 [15:22<07:54,  6.98s/it]Running Inference:  66%|██████▋   | 133/200 [15:29<07:53,  7.07s/it]Running Inference:  67%|██████▋   | 134/200 [15:37<08:03,  7.32s/it]Running Inference:  68%|██████▊   | 135/200 [15:45<07:58,  7.36s/it]Running Inference:  68%|██████▊   | 136/200 [15:52<07:57,  7.46s/it]Running Inference:  68%|██████▊   | 137/200 [16:00<07:43,  7.36s/it]Running Inference:  69%|██████▉   | 138/200 [16:08<07:52,  7.62s/it]Running Inference:  70%|██████▉   | 139/200 [16:15<07:44,  7.61s/it]Running Inference:  70%|███████   | 140/200 [16:22<07:23,  7.39s/it]Running Inference:  70%|███████   | 141/200 [16:30<07:18,  7.44s/it]Running Inference:  71%|███████   | 142/200 [16:37<07:11,  7.44s/it]Running Inference:  72%|███████▏  | 143/200 [16:45<07:03,  7.42s/it]Running Inference:  72%|███████▏  | 144/200 [16:52<06:48,  7.29s/it]Running Inference:  72%|███████▎  | 145/200 [16:59<06:43,  7.34s/it]Running Inference:  73%|███████▎  | 146/200 [17:06<06:36,  7.33s/it]Running Inference:  74%|███████▎  | 147/200 [17:13<06:23,  7.23s/it]Running Inference:  74%|███████▍  | 148/200 [17:21<06:27,  7.46s/it]Running Inference:  74%|███████▍  | 149/200 [17:29<06:27,  7.60s/it]Running Inference:  75%|███████▌  | 150/200 [17:37<06:15,  7.51s/it]Running Inference:  76%|███████▌  | 151/200 [17:44<06:08,  7.51s/it]Running Inference:  76%|███████▌  | 152/200 [17:51<05:53,  7.37s/it]Running Inference:  76%|███████▋  | 153/200 [17:59<05:51,  7.47s/it]Running Inference:  77%|███████▋  | 154/200 [18:03<04:59,  6.50s/it]Running Inference:  78%|███████▊  | 155/200 [18:11<05:06,  6.80s/it]Running Inference:  78%|███████▊  | 156/200 [18:17<04:48,  6.55s/it]Running Inference:  78%|███████▊  | 157/200 [18:24<04:58,  6.94s/it]Running Inference:  79%|███████▉  | 158/200 [18:32<04:53,  6.99s/it]Running Inference:  80%|███████▉  | 159/200 [18:39<04:56,  7.23s/it]Running Inference:  80%|████████  | 160/200 [18:46<04:44,  7.11s/it]Running Inference:  80%|████████  | 161/200 [18:53<04:39,  7.17s/it]Running Inference:  81%|████████  | 162/200 [18:59<04:14,  6.70s/it]Running Inference:  82%|████████▏ | 163/200 [19:02<03:27,  5.62s/it]Running Inference:  82%|████████▏ | 164/200 [19:11<03:51,  6.44s/it]Running Inference:  82%|████████▎ | 165/200 [19:18<03:55,  6.74s/it]Running Inference:  83%|████████▎ | 166/200 [19:25<03:56,  6.96s/it]Running Inference:  84%|████████▎ | 167/200 [19:33<03:54,  7.09s/it]Running Inference:  84%|████████▍ | 168/200 [19:41<03:52,  7.28s/it]Running Inference:  84%|████████▍ | 169/200 [19:48<03:47,  7.35s/it]Running Inference:  85%|████████▌ | 170/200 [19:55<03:37,  7.25s/it]Running Inference:  86%|████████▌ | 171/200 [20:03<03:35,  7.42s/it]Running Inference:  86%|████████▌ | 172/200 [20:11<03:30,  7.53s/it]Running Inference:  86%|████████▋ | 173/200 [20:18<03:21,  7.47s/it]Running Inference:  87%|████████▋ | 174/200 [20:25<03:09,  7.27s/it]Running Inference:  88%|████████▊ | 175/200 [20:32<02:58,  7.16s/it]Running Inference:  88%|████████▊ | 176/200 [20:39<02:55,  7.32s/it]Running Inference:  88%|████████▊ | 177/200 [20:47<02:51,  7.44s/it]Running Inference:  89%|████████▉ | 178/200 [20:55<02:46,  7.57s/it]Running Inference:  90%|████████▉ | 179/200 [21:02<02:33,  7.33s/it]Running Inference:  90%|█████████ | 180/200 [21:11<02:35,  7.75s/it]Running Inference:  90%|█████████ | 181/200 [21:18<02:26,  7.72s/it]Running Inference:  91%|█████████ | 182/200 [21:26<02:18,  7.71s/it]Running Inference:  92%|█████████▏| 183/200 [21:33<02:08,  7.55s/it]Running Inference:  92%|█████████▏| 184/200 [21:41<02:02,  7.65s/it]Running Inference:  92%|█████████▎| 185/200 [21:48<01:52,  7.50s/it]Running Inference:  93%|█████████▎| 186/200 [21:56<01:47,  7.67s/it]Running Inference:  94%|█████████▎| 187/200 [22:04<01:39,  7.66s/it]Running Inference:  94%|█████████▍| 188/200 [22:11<01:30,  7.52s/it]Running Inference:  94%|█████████▍| 189/200 [22:18<01:20,  7.29s/it]Running Inference:  95%|█████████▌| 190/200 [22:25<01:12,  7.25s/it]Running Inference:  96%|█████████▌| 191/200 [22:30<00:58,  6.49s/it]Running Inference:  96%|█████████▌| 192/200 [22:37<00:53,  6.74s/it]Running Inference:  96%|█████████▋| 193/200 [22:45<00:49,  7.09s/it]Running Inference:  97%|█████████▋| 194/200 [22:53<00:43,  7.28s/it]Running Inference:  98%|█████████▊| 195/200 [23:00<00:36,  7.24s/it]Running Inference:  98%|█████████▊| 196/200 [23:07<00:29,  7.36s/it]Running Inference:  98%|█████████▊| 197/200 [23:15<00:22,  7.47s/it]Running Inference:  99%|█████████▉| 198/200 [23:22<00:14,  7.31s/it]Running Inference: 100%|█████████▉| 199/200 [23:30<00:07,  7.50s/it]Running Inference: 100%|██████████| 200/200 [23:37<00:00,  7.46s/it]Running Inference: 100%|██████████| 200/200 [23:37<00:00,  7.09s/it]
2025-12-14 17:23:41,065 - INFO - Inference completed.
2025-12-14 17:23:41,076 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 17:23:41,076 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.668 seconds.
Prefix dict has been built successfully.
2025-12-14 17:23:43,605 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 17:23:43,605 - INFO - Metrics:
16.37
2025-12-14 17:23:43,606 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=dureader, ratio=0.1) on GPU 1

----------------------------------------
Task: dureader | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:23:50,144 - INFO - Set deterministic seeds to 42
2025-12-14 17:23:50,145 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:23:50,145 - INFO - Starting evaluation run...
2025-12-14 17:23:50,145 - INFO - Output directory set to: longbenchresult
2025-12-14 17:23:50,145 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 17:23:50,145 - INFO - KV Press 'tova' setup.
2025-12-14 17:23:50,145 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:23:50,145 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.32it/s]
Device set to use cuda:0
2025-12-14 17:24:01,215 - INFO - Model pipeline loaded.
2025-12-14 17:24:01,215 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 17:24:08,302 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:24:08,302 - INFO - Dataset processed with 200 entries.
2025-12-14 17:24:08,331 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:09<29:52,  9.01s/it]Running Inference:   1%|          | 2/200 [00:11<16:10,  4.90s/it]Running Inference:   2%|▏         | 3/200 [00:18<19:52,  6.05s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:42,  4.20s/it]Running Inference:   2%|▎         | 5/200 [00:22<11:26,  3.52s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:53,  4.92s/it]Running Inference:   4%|▎         | 7/200 [00:37<18:46,  5.83s/it]Running Inference:   4%|▍         | 8/200 [00:45<20:46,  6.49s/it]Running Inference:   4%|▍         | 9/200 [00:53<22:14,  6.99s/it]Running Inference:   5%|▌         | 10/200 [01:00<22:28,  7.10s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:07<22:26,  7.13s/it]Running Inference:   6%|▌         | 12/200 [01:15<22:29,  7.18s/it]Running Inference:   6%|▋         | 13/200 [01:17<17:37,  5.66s/it]Running Inference:   7%|▋         | 14/200 [01:24<19:16,  6.22s/it]Running Inference:   8%|▊         | 15/200 [01:32<20:12,  6.56s/it]Running Inference:   8%|▊         | 16/200 [01:39<20:37,  6.73s/it]Running Inference:   8%|▊         | 17/200 [01:46<21:01,  6.89s/it]Running Inference:   9%|▉         | 18/200 [01:54<21:43,  7.16s/it]Running Inference:  10%|▉         | 19/200 [02:00<20:53,  6.92s/it]Running Inference:  10%|█         | 20/200 [02:08<21:30,  7.17s/it]Running Inference:  10%|█         | 21/200 [02:16<21:59,  7.37s/it]Running Inference:  11%|█         | 22/200 [02:23<21:50,  7.36s/it]Running Inference:  12%|█▏        | 23/200 [02:30<21:34,  7.32s/it]Running Inference:  12%|█▏        | 24/200 [02:38<21:29,  7.32s/it]Running Inference:  12%|█▎        | 25/200 [02:45<21:06,  7.24s/it]Running Inference:  13%|█▎        | 26/200 [02:52<21:12,  7.32s/it]Running Inference:  14%|█▎        | 27/200 [03:00<21:10,  7.34s/it]Running Inference:  14%|█▍        | 28/200 [03:07<21:00,  7.33s/it]Running Inference:  14%|█▍        | 29/200 [03:15<21:09,  7.43s/it]Running Inference:  15%|█▌        | 30/200 [03:22<20:40,  7.30s/it]Running Inference:  16%|█▌        | 31/200 [03:29<20:43,  7.36s/it]Running Inference:  16%|█▌        | 32/200 [03:36<20:17,  7.24s/it]Running Inference:  16%|█▋        | 33/200 [03:44<20:18,  7.30s/it]Running Inference:  17%|█▋        | 34/200 [03:51<20:20,  7.35s/it]Running Inference:  18%|█▊        | 35/200 [03:58<20:07,  7.32s/it]Running Inference:  18%|█▊        | 36/200 [04:06<20:25,  7.47s/it]Running Inference:  18%|█▊        | 37/200 [04:12<18:54,  6.96s/it]Running Inference:  19%|█▉        | 38/200 [04:20<20:00,  7.41s/it]Running Inference:  20%|█▉        | 39/200 [04:28<20:09,  7.52s/it]Running Inference:  20%|██        | 40/200 [04:36<20:23,  7.65s/it]Running Inference:  20%|██        | 41/200 [04:39<16:49,  6.35s/it]Running Inference:  21%|██        | 42/200 [04:47<17:24,  6.61s/it]Running Inference:  22%|██▏       | 43/200 [04:54<18:14,  6.97s/it]Running Inference:  22%|██▏       | 44/200 [05:02<18:13,  7.01s/it]Running Inference:  22%|██▎       | 45/200 [05:10<18:50,  7.29s/it]Running Inference:  23%|██▎       | 46/200 [05:17<18:46,  7.31s/it]Running Inference:  24%|██▎       | 47/200 [05:19<14:55,  5.85s/it]Running Inference:  24%|██▍       | 48/200 [05:27<16:07,  6.37s/it]Running Inference:  24%|██▍       | 49/200 [05:31<14:09,  5.63s/it]Running Inference:  25%|██▌       | 50/200 [05:38<15:28,  6.19s/it]Running Inference:  26%|██▌       | 51/200 [05:46<16:22,  6.59s/it]Running Inference:  26%|██▌       | 52/200 [05:48<13:04,  5.30s/it]Running Inference:  26%|██▋       | 53/200 [05:56<14:45,  6.02s/it]Running Inference:  27%|██▋       | 54/200 [06:04<16:01,  6.58s/it]Running Inference:  28%|██▊       | 55/200 [06:11<16:16,  6.74s/it]Running Inference:  28%|██▊       | 56/200 [06:18<16:16,  6.78s/it]Running Inference:  28%|██▊       | 57/200 [06:25<16:22,  6.87s/it]Running Inference:  29%|██▉       | 58/200 [06:32<16:21,  6.91s/it]Running Inference:  30%|██▉       | 59/200 [06:39<16:41,  7.10s/it]Running Inference:  30%|███       | 60/200 [06:47<16:59,  7.28s/it]Running Inference:  30%|███       | 61/200 [06:55<17:24,  7.51s/it]Running Inference:  31%|███       | 62/200 [07:03<17:18,  7.53s/it]Running Inference:  32%|███▏      | 63/200 [07:10<17:24,  7.63s/it]Running Inference:  32%|███▏      | 64/200 [07:18<17:12,  7.59s/it]Running Inference:  32%|███▎      | 65/200 [07:26<17:12,  7.65s/it]Running Inference:  33%|███▎      | 66/200 [07:33<16:58,  7.60s/it]Running Inference:  34%|███▎      | 67/200 [07:41<17:00,  7.67s/it]Running Inference:  34%|███▍      | 68/200 [07:48<16:33,  7.53s/it]Running Inference:  34%|███▍      | 69/200 [07:56<16:39,  7.63s/it]Running Inference:  35%|███▌      | 70/200 [08:04<16:46,  7.74s/it]Running Inference:  36%|███▌      | 71/200 [08:12<16:34,  7.71s/it]Running Inference:  36%|███▌      | 72/200 [08:19<15:57,  7.48s/it]Running Inference:  36%|███▋      | 73/200 [08:27<16:10,  7.64s/it]Running Inference:  37%|███▋      | 74/200 [08:34<15:57,  7.60s/it]Running Inference:  38%|███▊      | 75/200 [08:42<15:39,  7.51s/it]Running Inference:  38%|███▊      | 76/200 [08:49<15:41,  7.59s/it]Running Inference:  38%|███▊      | 77/200 [08:57<15:37,  7.62s/it]Running Inference:  39%|███▉      | 78/200 [09:00<12:34,  6.18s/it]Running Inference:  40%|███▉      | 79/200 [09:07<13:13,  6.56s/it]Running Inference:  40%|████      | 80/200 [09:15<13:43,  6.86s/it]Running Inference:  40%|████      | 81/200 [09:22<13:48,  6.96s/it]Running Inference:  41%|████      | 82/200 [09:30<13:57,  7.10s/it]Running Inference:  42%|████▏     | 83/200 [09:37<13:53,  7.13s/it]Running Inference:  42%|████▏     | 84/200 [09:44<13:52,  7.17s/it]Running Inference:  42%|████▎     | 85/200 [09:51<13:52,  7.24s/it]Running Inference:  43%|████▎     | 86/200 [09:59<13:44,  7.23s/it]Running Inference:  44%|████▎     | 87/200 [10:06<13:32,  7.19s/it]Running Inference:  44%|████▍     | 88/200 [10:13<13:25,  7.19s/it]Running Inference:  44%|████▍     | 89/200 [10:20<13:28,  7.28s/it]Running Inference:  45%|████▌     | 90/200 [10:27<13:13,  7.21s/it]Running Inference:  46%|████▌     | 91/200 [10:35<13:16,  7.30s/it]Running Inference:  46%|████▌     | 92/200 [10:42<13:11,  7.32s/it]Running Inference:  46%|████▋     | 93/200 [10:50<13:17,  7.46s/it]Running Inference:  47%|████▋     | 94/200 [10:58<13:22,  7.58s/it]Running Inference:  48%|████▊     | 95/200 [11:05<13:12,  7.54s/it]Running Inference:  48%|████▊     | 96/200 [11:13<12:58,  7.48s/it]Running Inference:  48%|████▊     | 97/200 [11:20<12:33,  7.31s/it]Running Inference:  49%|████▉     | 98/200 [11:27<12:17,  7.23s/it]Running Inference:  50%|████▉     | 99/200 [11:34<12:21,  7.35s/it]Running Inference:  50%|█████     | 100/200 [11:41<12:09,  7.29s/it]Running Inference:  50%|█████     | 101/200 [11:49<12:00,  7.28s/it]Running Inference:  51%|█████     | 102/200 [11:56<11:54,  7.29s/it]Running Inference:  52%|█████▏    | 103/200 [12:04<11:53,  7.35s/it]Running Inference:  52%|█████▏    | 104/200 [12:11<11:44,  7.34s/it]Running Inference:  52%|█████▎    | 105/200 [12:18<11:36,  7.33s/it]Running Inference:  53%|█████▎    | 106/200 [12:26<11:30,  7.35s/it]Running Inference:  54%|█████▎    | 107/200 [12:33<11:29,  7.41s/it]Running Inference:  54%|█████▍    | 108/200 [12:40<11:16,  7.35s/it]Running Inference:  55%|█████▍    | 109/200 [12:48<11:15,  7.43s/it]Running Inference:  55%|█████▌    | 110/200 [12:56<11:32,  7.70s/it]Running Inference:  56%|█████▌    | 111/200 [13:03<11:13,  7.56s/it]Running Inference:  56%|█████▌    | 112/200 [13:11<10:51,  7.40s/it]Running Inference:  56%|█████▋    | 113/200 [13:18<10:48,  7.45s/it]Running Inference:  57%|█████▋    | 114/200 [13:26<10:41,  7.46s/it]Running Inference:  57%|█████▊    | 115/200 [13:33<10:35,  7.48s/it]Running Inference:  58%|█████▊    | 116/200 [13:41<10:30,  7.51s/it]Running Inference:  58%|█████▊    | 117/200 [13:46<09:16,  6.71s/it]Running Inference:  59%|█████▉    | 118/200 [13:54<09:49,  7.19s/it]Running Inference:  60%|█████▉    | 119/200 [14:02<10:08,  7.51s/it]Running Inference:  60%|██████    | 120/200 [14:11<10:24,  7.81s/it]Running Inference:  60%|██████    | 121/200 [14:18<10:11,  7.75s/it]Running Inference:  61%|██████    | 122/200 [14:25<09:47,  7.53s/it]Running Inference:  62%|██████▏   | 123/200 [14:33<09:47,  7.64s/it]Running Inference:  62%|██████▏   | 124/200 [14:40<09:34,  7.56s/it]Running Inference:  62%|██████▎   | 125/200 [14:48<09:16,  7.42s/it]Running Inference:  63%|██████▎   | 126/200 [14:54<08:42,  7.07s/it]Running Inference:  64%|██████▎   | 127/200 [15:01<08:44,  7.19s/it]Running Inference:  64%|██████▍   | 128/200 [15:09<08:40,  7.22s/it]Running Inference:  64%|██████▍   | 129/200 [15:13<07:29,  6.33s/it]Running Inference:  65%|██████▌   | 130/200 [15:20<07:44,  6.63s/it]Running Inference:  66%|██████▌   | 131/200 [15:27<07:49,  6.81s/it]Running Inference:  66%|██████▌   | 132/200 [15:35<07:54,  6.98s/it]Running Inference:  66%|██████▋   | 133/200 [15:42<07:56,  7.11s/it]Running Inference:  67%|██████▋   | 134/200 [15:50<08:07,  7.38s/it]Running Inference:  68%|██████▊   | 135/200 [15:58<07:58,  7.35s/it]Running Inference:  68%|██████▊   | 136/200 [16:05<07:59,  7.49s/it]Running Inference:  68%|██████▊   | 137/200 [16:09<06:43,  6.41s/it]Running Inference:  69%|██████▉   | 138/200 [16:18<07:12,  6.98s/it]Running Inference:  70%|██████▉   | 139/200 [16:25<07:17,  7.17s/it]Running Inference:  70%|███████   | 140/200 [16:32<07:06,  7.10s/it]Running Inference:  70%|███████   | 141/200 [16:40<07:08,  7.26s/it]Running Inference:  71%|███████   | 142/200 [16:47<07:06,  7.36s/it]Running Inference:  72%|███████▏  | 143/200 [16:55<07:02,  7.41s/it]Running Inference:  72%|███████▏  | 144/200 [17:02<06:50,  7.32s/it]Running Inference:  72%|███████▎  | 145/200 [17:09<06:46,  7.39s/it]Running Inference:  73%|███████▎  | 146/200 [17:17<06:39,  7.40s/it]Running Inference:  74%|███████▎  | 147/200 [17:24<06:26,  7.30s/it]Running Inference:  74%|███████▍  | 148/200 [17:32<06:32,  7.54s/it]Running Inference:  74%|███████▍  | 149/200 [17:40<06:31,  7.68s/it]Running Inference:  75%|███████▌  | 150/200 [17:47<06:19,  7.59s/it]Running Inference:  76%|███████▌  | 151/200 [17:55<06:12,  7.60s/it]Running Inference:  76%|███████▌  | 152/200 [18:02<05:57,  7.45s/it]Running Inference:  76%|███████▋  | 153/200 [18:10<05:55,  7.56s/it]Running Inference:  77%|███████▋  | 154/200 [18:17<05:41,  7.43s/it]Running Inference:  78%|███████▊  | 155/200 [18:25<05:36,  7.47s/it]Running Inference:  78%|███████▊  | 156/200 [18:32<05:24,  7.37s/it]Running Inference:  78%|███████▊  | 157/200 [18:40<05:23,  7.53s/it]Running Inference:  79%|███████▉  | 158/200 [18:47<05:12,  7.43s/it]Running Inference:  80%|███████▉  | 159/200 [18:55<05:10,  7.58s/it]Running Inference:  80%|████████  | 160/200 [19:02<04:55,  7.38s/it]Running Inference:  80%|████████  | 161/200 [19:09<04:48,  7.39s/it]Running Inference:  81%|████████  | 162/200 [19:17<04:40,  7.39s/it]Running Inference:  82%|████████▏ | 163/200 [19:20<03:45,  6.10s/it]Running Inference:  82%|████████▏ | 164/200 [19:28<04:04,  6.79s/it]Running Inference:  82%|████████▎ | 165/200 [19:36<04:05,  7.01s/it]Running Inference:  83%|████████▎ | 166/200 [19:43<04:03,  7.17s/it]Running Inference:  84%|████████▎ | 167/200 [19:51<03:59,  7.26s/it]Running Inference:  84%|████████▍ | 168/200 [19:58<03:56,  7.41s/it]Running Inference:  84%|████████▍ | 169/200 [20:06<03:51,  7.45s/it]Running Inference:  85%|████████▌ | 170/200 [20:13<03:40,  7.36s/it]Running Inference:  86%|████████▌ | 171/200 [20:21<03:37,  7.51s/it]Running Inference:  86%|████████▌ | 172/200 [20:29<03:33,  7.62s/it]Running Inference:  86%|████████▋ | 173/200 [20:36<03:24,  7.56s/it]Running Inference:  87%|████████▋ | 174/200 [20:43<03:11,  7.37s/it]Running Inference:  88%|████████▊ | 175/200 [20:50<03:01,  7.24s/it]Running Inference:  88%|████████▊ | 176/200 [20:58<02:57,  7.41s/it]Running Inference:  88%|████████▊ | 177/200 [21:06<02:53,  7.53s/it]Running Inference:  89%|████████▉ | 178/200 [21:14<02:48,  7.66s/it]Running Inference:  90%|████████▉ | 179/200 [21:21<02:36,  7.43s/it]Running Inference:  90%|█████████ | 180/200 [21:29<02:36,  7.82s/it]Running Inference:  90%|█████████ | 181/200 [21:37<02:27,  7.79s/it]Running Inference:  91%|█████████ | 182/200 [21:45<02:20,  7.78s/it]Running Inference:  92%|█████████▏| 183/200 [21:52<02:09,  7.63s/it]Running Inference:  92%|█████████▏| 184/200 [22:00<02:03,  7.73s/it]Running Inference:  92%|█████████▎| 185/200 [22:07<01:53,  7.60s/it]Running Inference:  93%|█████████▎| 186/200 [22:15<01:48,  7.77s/it]Running Inference:  94%|█████████▎| 187/200 [22:23<01:40,  7.76s/it]Running Inference:  94%|█████████▍| 188/200 [22:30<01:31,  7.62s/it]Running Inference:  94%|█████████▍| 189/200 [22:36<01:17,  7.04s/it]Running Inference:  95%|█████████▌| 190/200 [22:43<01:11,  7.10s/it]Running Inference:  96%|█████████▌| 191/200 [22:48<00:57,  6.41s/it]Running Inference:  96%|█████████▌| 192/200 [22:56<00:53,  6.72s/it]Running Inference:  96%|█████████▋| 193/200 [23:04<00:49,  7.10s/it]Running Inference:  97%|█████████▋| 194/200 [23:11<00:43,  7.31s/it]Running Inference:  98%|█████████▊| 195/200 [23:19<00:36,  7.30s/it]Running Inference:  98%|█████████▊| 196/200 [23:26<00:29,  7.43s/it]Running Inference:  98%|█████████▊| 197/200 [23:34<00:22,  7.55s/it]Running Inference:  99%|█████████▉| 198/200 [23:41<00:14,  7.39s/it]Running Inference: 100%|█████████▉| 199/200 [23:49<00:07,  7.59s/it]Running Inference: 100%|██████████| 200/200 [23:57<00:00,  7.56s/it]Running Inference: 100%|██████████| 200/200 [23:57<00:00,  7.19s/it]
2025-12-14 17:48:05,683 - INFO - Inference completed.
2025-12-14 17:48:05,694 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 17:48:05,694 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.671 seconds.
Prefix dict has been built successfully.
2025-12-14 17:48:08,176 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 17:48:08,176 - INFO - Metrics:
15.89
2025-12-14 17:48:08,177 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=dureader, ratio=0.2) on GPU 1

----------------------------------------
Task: dureader | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:48:14,703 - INFO - Set deterministic seeds to 42
2025-12-14 17:48:14,703 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:48:14,703 - INFO - Starting evaluation run...
2025-12-14 17:48:14,703 - INFO - Output directory set to: longbenchresult
2025-12-14 17:48:14,703 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 17:48:14,703 - INFO - KV Press 'tova' setup.
2025-12-14 17:48:14,703 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:48:14,703 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.62it/s]
Device set to use cuda:0
2025-12-14 17:48:29,016 - INFO - Model pipeline loaded.
2025-12-14 17:48:29,016 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 17:48:34,019 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:48:34,019 - INFO - Dataset processed with 200 entries.
2025-12-14 17:48:34,048 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:09<29:54,  9.02s/it]Running Inference:   1%|          | 2/200 [00:11<16:13,  4.91s/it]Running Inference:   2%|▏         | 3/200 [00:18<19:57,  6.08s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:46,  4.22s/it]Running Inference:   2%|▎         | 5/200 [00:22<11:29,  3.53s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:57,  4.93s/it]Running Inference:   4%|▎         | 7/200 [00:37<18:49,  5.85s/it]Running Inference:   4%|▍         | 8/200 [00:45<20:50,  6.51s/it]Running Inference:   4%|▍         | 9/200 [00:53<22:17,  7.00s/it]Running Inference:   5%|▌         | 10/200 [01:00<22:31,  7.12s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:08<22:29,  7.14s/it]Running Inference:   6%|▌         | 12/200 [01:15<22:33,  7.20s/it]Running Inference:   6%|▋         | 13/200 [01:17<17:41,  5.68s/it]Running Inference:   7%|▋         | 14/200 [01:25<19:21,  6.24s/it]Running Inference:   8%|▊         | 15/200 [01:32<20:17,  6.58s/it]Running Inference:   8%|▊         | 16/200 [01:39<20:42,  6.75s/it]Running Inference:   8%|▊         | 17/200 [01:47<21:06,  6.92s/it]Running Inference:   9%|▉         | 18/200 [01:54<21:46,  7.18s/it]Running Inference:  10%|▉         | 19/200 [02:02<21:52,  7.25s/it]Running Inference:  10%|█         | 20/200 [02:10<22:13,  7.41s/it]Running Inference:  10%|█         | 21/200 [02:17<22:31,  7.55s/it]Running Inference:  11%|█         | 22/200 [02:25<22:15,  7.50s/it]Running Inference:  12%|█▏        | 23/200 [02:32<21:54,  7.43s/it]Running Inference:  12%|█▏        | 24/200 [02:39<21:44,  7.41s/it]Running Inference:  12%|█▎        | 25/200 [02:46<21:17,  7.30s/it]Running Inference:  13%|█▎        | 26/200 [02:54<21:21,  7.37s/it]Running Inference:  14%|█▎        | 27/200 [03:01<21:18,  7.39s/it]Running Inference:  14%|█▍        | 28/200 [03:09<21:06,  7.37s/it]Running Inference:  14%|█▍        | 29/200 [03:16<21:16,  7.46s/it]Running Inference:  15%|█▌        | 30/200 [03:23<20:46,  7.33s/it]Running Inference:  16%|█▌        | 31/200 [03:31<20:50,  7.40s/it]Running Inference:  16%|█▌        | 32/200 [03:38<20:23,  7.28s/it]Running Inference:  16%|█▋        | 33/200 [03:46<20:25,  7.34s/it]Running Inference:  17%|█▋        | 34/200 [03:53<20:28,  7.40s/it]Running Inference:  18%|█▊        | 35/200 [04:00<20:14,  7.36s/it]Running Inference:  18%|█▊        | 36/200 [04:08<20:31,  7.51s/it]Running Inference:  18%|█▊        | 37/200 [04:14<18:37,  6.86s/it]Running Inference:  19%|█▉        | 38/200 [04:22<19:45,  7.32s/it]Running Inference:  20%|█▉        | 39/200 [04:30<19:53,  7.41s/it]Running Inference:  20%|██        | 40/200 [04:37<20:11,  7.57s/it]Running Inference:  20%|██        | 41/200 [04:40<16:06,  6.08s/it]Running Inference:  21%|██        | 42/200 [04:47<16:52,  6.41s/it]Running Inference:  22%|██▏       | 43/200 [04:55<17:49,  6.81s/it]Running Inference:  22%|██▏       | 44/200 [05:02<17:54,  6.89s/it]Running Inference:  22%|██▎       | 45/200 [05:05<14:43,  5.70s/it]Running Inference:  23%|██▎       | 46/200 [05:12<15:52,  6.19s/it]Running Inference:  24%|██▎       | 47/200 [05:15<12:54,  5.06s/it]Running Inference:  24%|██▍       | 48/200 [05:22<14:42,  5.81s/it]Running Inference:  24%|██▍       | 49/200 [05:26<13:09,  5.23s/it]Running Inference:  25%|██▌       | 50/200 [05:34<14:45,  5.90s/it]Running Inference:  26%|██▌       | 51/200 [05:41<15:50,  6.38s/it]Running Inference:  26%|██▌       | 52/200 [05:43<12:42,  5.16s/it]Running Inference:  26%|██▋       | 53/200 [05:51<14:27,  5.90s/it]Running Inference:  27%|██▋       | 54/200 [05:59<15:45,  6.48s/it]Running Inference:  28%|██▊       | 55/200 [06:06<16:04,  6.65s/it]Running Inference:  28%|██▊       | 56/200 [06:13<16:07,  6.72s/it]Running Inference:  28%|██▊       | 57/200 [06:20<16:15,  6.82s/it]Running Inference:  29%|██▉       | 58/200 [06:27<16:15,  6.87s/it]Running Inference:  30%|██▉       | 59/200 [06:34<16:35,  7.06s/it]Running Inference:  30%|███       | 60/200 [06:42<16:54,  7.25s/it]Running Inference:  30%|███       | 61/200 [06:50<17:19,  7.48s/it]Running Inference:  31%|███       | 62/200 [06:58<17:14,  7.50s/it]Running Inference:  32%|███▏      | 63/200 [07:05<17:19,  7.59s/it]Running Inference:  32%|███▏      | 64/200 [07:13<17:07,  7.55s/it]Running Inference:  32%|███▎      | 65/200 [07:21<17:10,  7.63s/it]Running Inference:  33%|███▎      | 66/200 [07:28<16:55,  7.58s/it]Running Inference:  34%|███▎      | 67/200 [07:36<16:59,  7.66s/it]Running Inference:  34%|███▍      | 68/200 [07:43<16:32,  7.52s/it]Running Inference:  34%|███▍      | 69/200 [07:51<16:37,  7.61s/it]Running Inference:  35%|███▌      | 70/200 [07:59<16:43,  7.72s/it]Running Inference:  36%|███▌      | 71/200 [08:07<16:30,  7.68s/it]Running Inference:  36%|███▌      | 72/200 [08:14<15:53,  7.45s/it]Running Inference:  36%|███▋      | 73/200 [08:21<16:05,  7.60s/it]Running Inference:  37%|███▋      | 74/200 [08:29<15:52,  7.56s/it]Running Inference:  38%|███▊      | 75/200 [08:36<15:32,  7.46s/it]Running Inference:  38%|███▊      | 76/200 [08:44<15:33,  7.53s/it]Running Inference:  38%|███▊      | 77/200 [08:51<15:27,  7.54s/it]Running Inference:  39%|███▉      | 78/200 [08:59<15:27,  7.60s/it]Running Inference:  40%|███▉      | 79/200 [09:07<15:12,  7.54s/it]Running Inference:  40%|████      | 80/200 [09:14<15:04,  7.54s/it]Running Inference:  40%|████      | 81/200 [09:21<14:42,  7.42s/it]Running Inference:  41%|████      | 82/200 [09:29<14:34,  7.41s/it]Running Inference:  42%|████▏     | 83/200 [09:36<14:17,  7.33s/it]Running Inference:  42%|████▏     | 84/200 [09:43<14:07,  7.30s/it]Running Inference:  42%|████▎     | 85/200 [09:50<14:01,  7.31s/it]Running Inference:  43%|████▎     | 86/200 [09:58<13:48,  7.27s/it]Running Inference:  44%|████▎     | 87/200 [10:05<13:33,  7.20s/it]Running Inference:  44%|████▍     | 88/200 [10:12<13:24,  7.18s/it]Running Inference:  44%|████▍     | 89/200 [10:19<13:26,  7.26s/it]Running Inference:  45%|████▌     | 90/200 [10:26<13:11,  7.19s/it]Running Inference:  46%|████▌     | 91/200 [10:34<13:12,  7.27s/it]Running Inference:  46%|████▌     | 92/200 [10:41<13:07,  7.29s/it]Running Inference:  46%|████▋     | 93/200 [10:43<10:11,  5.72s/it]Running Inference:  47%|████▋     | 94/200 [10:51<11:11,  6.34s/it]Running Inference:  48%|████▊     | 95/200 [10:54<09:30,  5.43s/it]Running Inference:  48%|████▊     | 96/200 [11:01<10:22,  5.99s/it]Running Inference:  48%|████▊     | 97/200 [11:08<10:43,  6.25s/it]Running Inference:  49%|████▉     | 98/200 [11:15<10:59,  6.47s/it]Running Inference:  50%|████▉     | 99/200 [11:23<11:25,  6.79s/it]Running Inference:  50%|█████     | 100/200 [11:30<11:28,  6.89s/it]Running Inference:  50%|█████     | 101/200 [11:37<11:31,  6.99s/it]Running Inference:  51%|█████     | 102/200 [11:44<11:32,  7.07s/it]Running Inference:  52%|█████▏    | 103/200 [11:52<11:36,  7.18s/it]Running Inference:  52%|█████▏    | 104/200 [11:59<11:32,  7.21s/it]Running Inference:  52%|█████▎    | 105/200 [12:06<11:26,  7.22s/it]Running Inference:  53%|█████▎    | 106/200 [12:14<11:21,  7.25s/it]Running Inference:  54%|█████▎    | 107/200 [12:21<11:21,  7.33s/it]Running Inference:  54%|█████▍    | 108/200 [12:28<11:09,  7.28s/it]Running Inference:  55%|█████▍    | 109/200 [12:36<11:09,  7.36s/it]Running Inference:  55%|█████▌    | 110/200 [12:44<11:27,  7.64s/it]Running Inference:  56%|█████▌    | 111/200 [12:51<11:08,  7.51s/it]Running Inference:  56%|█████▌    | 112/200 [12:58<10:46,  7.35s/it]Running Inference:  56%|█████▋    | 113/200 [13:06<10:43,  7.40s/it]Running Inference:  57%|█████▋    | 114/200 [13:13<10:37,  7.41s/it]Running Inference:  57%|█████▊    | 115/200 [13:21<10:31,  7.43s/it]Running Inference:  58%|█████▊    | 116/200 [13:28<10:26,  7.46s/it]Running Inference:  58%|█████▊    | 117/200 [13:33<09:07,  6.59s/it]Running Inference:  59%|█████▉    | 118/200 [13:41<09:41,  7.09s/it]Running Inference:  60%|█████▉    | 119/200 [13:49<10:01,  7.42s/it]Running Inference:  60%|██████    | 120/200 [13:58<10:18,  7.73s/it]Running Inference:  60%|██████    | 121/200 [14:05<10:06,  7.67s/it]Running Inference:  61%|██████    | 122/200 [14:12<09:42,  7.47s/it]Running Inference:  62%|██████▏   | 123/200 [14:20<09:43,  7.57s/it]Running Inference:  62%|██████▏   | 124/200 [14:27<09:30,  7.50s/it]Running Inference:  62%|██████▎   | 125/200 [14:35<09:12,  7.37s/it]Running Inference:  63%|██████▎   | 126/200 [14:42<08:59,  7.28s/it]Running Inference:  64%|██████▎   | 127/200 [14:49<08:55,  7.33s/it]Running Inference:  64%|██████▍   | 128/200 [14:56<08:48,  7.34s/it]Running Inference:  64%|██████▍   | 129/200 [14:59<07:06,  6.01s/it]Running Inference:  65%|██████▌   | 130/200 [15:07<07:28,  6.41s/it]Running Inference:  66%|██████▌   | 131/200 [15:14<07:38,  6.64s/it]Running Inference:  66%|██████▌   | 132/200 [15:21<07:46,  6.85s/it]Running Inference:  66%|██████▋   | 133/200 [15:29<07:49,  7.00s/it]Running Inference:  67%|██████▋   | 134/200 [15:37<08:01,  7.29s/it]Running Inference:  68%|██████▊   | 135/200 [15:44<07:52,  7.27s/it]Running Inference:  68%|██████▊   | 136/200 [15:51<07:54,  7.41s/it]Running Inference:  68%|██████▊   | 137/200 [15:59<07:42,  7.35s/it]Running Inference:  69%|██████▉   | 138/200 [16:07<07:52,  7.62s/it]Running Inference:  70%|██████▉   | 139/200 [16:14<07:43,  7.61s/it]Running Inference:  70%|███████   | 140/200 [16:21<07:23,  7.39s/it]Running Inference:  70%|███████   | 141/200 [16:29<07:19,  7.46s/it]Running Inference:  71%|███████   | 142/200 [16:36<07:13,  7.47s/it]Running Inference:  72%|███████▏  | 143/200 [16:44<07:05,  7.46s/it]Running Inference:  72%|███████▏  | 144/200 [16:51<06:50,  7.34s/it]Running Inference:  72%|███████▎  | 145/200 [16:58<06:45,  7.38s/it]Running Inference:  73%|███████▎  | 146/200 [17:06<06:38,  7.38s/it]Running Inference:  74%|███████▎  | 147/200 [17:13<06:25,  7.27s/it]Running Inference:  74%|███████▍  | 148/200 [17:21<06:30,  7.50s/it]Running Inference:  74%|███████▍  | 149/200 [17:29<06:29,  7.64s/it]Running Inference:  75%|███████▌  | 150/200 [17:36<06:17,  7.55s/it]Running Inference:  76%|███████▌  | 151/200 [17:44<06:10,  7.55s/it]Running Inference:  76%|███████▌  | 152/200 [17:49<05:23,  6.75s/it]Running Inference:  76%|███████▋  | 153/200 [17:56<05:31,  7.05s/it]Running Inference:  77%|███████▋  | 154/200 [18:03<05:24,  7.05s/it]Running Inference:  78%|███████▊  | 155/200 [18:11<05:23,  7.19s/it]Running Inference:  78%|███████▊  | 156/200 [18:18<05:14,  7.16s/it]Running Inference:  78%|███████▊  | 157/200 [18:26<05:16,  7.36s/it]Running Inference:  79%|███████▉  | 158/200 [18:33<05:06,  7.30s/it]Running Inference:  80%|███████▉  | 159/200 [18:41<05:06,  7.47s/it]Running Inference:  80%|████████  | 160/200 [18:48<04:51,  7.28s/it]Running Inference:  80%|████████  | 161/200 [18:55<04:45,  7.31s/it]Running Inference:  81%|████████  | 162/200 [19:02<04:38,  7.32s/it]Running Inference:  82%|████████▏ | 163/200 [19:09<04:21,  7.07s/it]Running Inference:  82%|████████▏ | 164/200 [19:17<04:28,  7.46s/it]Running Inference:  82%|████████▎ | 165/200 [19:25<04:21,  7.46s/it]Running Inference:  83%|████████▎ | 166/200 [19:32<04:13,  7.46s/it]Running Inference:  84%|████████▎ | 167/200 [19:40<04:05,  7.45s/it]Running Inference:  84%|████████▍ | 168/200 [19:47<04:00,  7.52s/it]Running Inference:  84%|████████▍ | 169/200 [19:55<03:53,  7.52s/it]Running Inference:  85%|████████▌ | 170/200 [20:00<03:27,  6.90s/it]Running Inference:  86%|████████▌ | 171/200 [20:08<03:28,  7.18s/it]Running Inference:  86%|████████▌ | 172/200 [20:16<03:26,  7.37s/it]Running Inference:  86%|████████▋ | 173/200 [20:23<03:19,  7.37s/it]Running Inference:  87%|████████▋ | 174/200 [20:30<03:07,  7.21s/it]Running Inference:  88%|████████▊ | 175/200 [20:37<02:58,  7.12s/it]Running Inference:  88%|████████▊ | 176/200 [20:45<02:55,  7.31s/it]Running Inference:  88%|████████▊ | 177/200 [20:53<02:51,  7.44s/it]Running Inference:  89%|████████▉ | 178/200 [21:00<02:46,  7.58s/it]Running Inference:  90%|████████▉ | 179/200 [21:07<02:34,  7.36s/it]Running Inference:  90%|█████████ | 180/200 [21:16<02:34,  7.74s/it]Running Inference:  90%|█████████ | 181/200 [21:24<02:26,  7.71s/it]Running Inference:  91%|█████████ | 182/200 [21:31<02:18,  7.71s/it]Running Inference:  92%|█████████▏| 183/200 [21:39<02:08,  7.57s/it]Running Inference:  92%|█████████▏| 184/200 [21:47<02:03,  7.69s/it]Running Inference:  92%|█████████▎| 185/200 [21:54<01:53,  7.56s/it]Running Inference:  93%|█████████▎| 186/200 [22:02<01:48,  7.73s/it]Running Inference:  94%|█████████▎| 187/200 [22:10<01:40,  7.73s/it]Running Inference:  94%|█████████▍| 188/200 [22:17<01:31,  7.58s/it]Running Inference:  94%|█████████▍| 189/200 [22:23<01:17,  7.01s/it]Running Inference:  95%|█████████▌| 190/200 [22:30<01:10,  7.07s/it]Running Inference:  96%|█████████▌| 191/200 [22:37<01:04,  7.16s/it]Running Inference:  96%|█████████▌| 192/200 [22:44<00:57,  7.22s/it]Running Inference:  96%|█████████▋| 193/200 [22:52<00:52,  7.43s/it]Running Inference:  97%|█████████▋| 194/200 [23:00<00:45,  7.52s/it]Running Inference:  98%|█████████▊| 195/200 [23:07<00:37,  7.43s/it]Running Inference:  98%|█████████▊| 196/200 [23:15<00:29,  7.50s/it]Running Inference:  98%|█████████▊| 197/200 [23:23<00:22,  7.58s/it]Running Inference:  99%|█████████▉| 198/200 [23:30<00:14,  7.39s/it]Running Inference: 100%|█████████▉| 199/200 [23:38<00:07,  7.57s/it]Running Inference: 100%|██████████| 200/200 [23:45<00:00,  7.52s/it]Running Inference: 100%|██████████| 200/200 [23:45<00:00,  7.13s/it]
2025-12-14 18:12:19,688 - INFO - Inference completed.
2025-12-14 18:12:19,699 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 18:12:19,699 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.663 seconds.
Prefix dict has been built successfully.
2025-12-14 18:12:22,207 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 18:12:22,207 - INFO - Metrics:
16.2
2025-12-14 18:12:22,209 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=dureader, ratio=0.3) on GPU 1

----------------------------------------
Task: dureader | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:12:29,150 - INFO - Set deterministic seeds to 42
2025-12-14 18:12:29,150 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:12:29,150 - INFO - Starting evaluation run...
2025-12-14 18:12:29,150 - INFO - Output directory set to: longbenchresult
2025-12-14 18:12:29,150 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 18:12:29,150 - INFO - KV Press 'tova' setup.
2025-12-14 18:12:29,151 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:12:29,151 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.70it/s]
Device set to use cuda:0
2025-12-14 18:12:42,004 - INFO - Model pipeline loaded.
2025-12-14 18:12:42,005 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 18:12:45,643 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:12:45,643 - INFO - Dataset processed with 200 entries.
2025-12-14 18:12:45,668 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:22,  8.86s/it]Running Inference:   1%|          | 2/200 [00:10<15:56,  4.83s/it]Running Inference:   2%|▏         | 3/200 [00:18<19:40,  5.99s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:35,  4.16s/it]Running Inference:   2%|▎         | 5/200 [00:21<11:18,  3.48s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:44,  4.87s/it]Running Inference:   4%|▎         | 7/200 [00:37<18:36,  5.78s/it]Running Inference:   4%|▍         | 8/200 [00:44<20:37,  6.44s/it]Running Inference:   4%|▍         | 9/200 [00:52<22:03,  6.93s/it]Running Inference:   5%|▌         | 10/200 [01:00<22:18,  7.04s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:07<22:17,  7.08s/it]Running Inference:   6%|▌         | 12/200 [01:14<22:21,  7.13s/it]Running Inference:   6%|▋         | 13/200 [01:19<19:48,  6.35s/it]Running Inference:   7%|▋         | 14/200 [01:26<20:43,  6.69s/it]Running Inference:   8%|▊         | 15/200 [01:33<21:10,  6.87s/it]Running Inference:   8%|▊         | 16/200 [01:41<21:14,  6.93s/it]Running Inference:   8%|▊         | 17/200 [01:48<21:24,  7.02s/it]Running Inference:   9%|▉         | 18/200 [01:55<21:55,  7.23s/it]Running Inference:  10%|▉         | 19/200 [02:03<22:01,  7.30s/it]Running Inference:  10%|█         | 20/200 [02:11<22:18,  7.43s/it]Running Inference:  10%|█         | 21/200 [02:19<22:36,  7.58s/it]Running Inference:  11%|█         | 22/200 [02:26<22:16,  7.51s/it]Running Inference:  12%|█▏        | 23/200 [02:33<21:38,  7.34s/it]Running Inference:  12%|█▏        | 24/200 [02:40<21:31,  7.34s/it]Running Inference:  12%|█▎        | 25/200 [02:47<21:07,  7.24s/it]Running Inference:  13%|█▎        | 26/200 [02:55<21:12,  7.31s/it]Running Inference:  14%|█▎        | 27/200 [03:02<21:10,  7.34s/it]Running Inference:  14%|█▍        | 28/200 [03:09<20:59,  7.32s/it]Running Inference:  14%|█▍        | 29/200 [03:17<21:09,  7.43s/it]Running Inference:  15%|█▌        | 30/200 [03:24<20:40,  7.30s/it]Running Inference:  16%|█▌        | 31/200 [03:32<20:43,  7.36s/it]Running Inference:  16%|█▌        | 32/200 [03:39<20:16,  7.24s/it]Running Inference:  16%|█▋        | 33/200 [03:46<20:17,  7.29s/it]Running Inference:  17%|█▋        | 34/200 [03:53<20:19,  7.35s/it]Running Inference:  18%|█▊        | 35/200 [04:01<20:05,  7.31s/it]Running Inference:  18%|█▊        | 36/200 [04:09<20:26,  7.48s/it]Running Inference:  18%|█▊        | 37/200 [04:15<19:35,  7.21s/it]Running Inference:  19%|█▉        | 38/200 [04:24<20:29,  7.59s/it]Running Inference:  20%|█▉        | 39/200 [04:31<20:30,  7.64s/it]Running Inference:  20%|██        | 40/200 [04:39<20:39,  7.75s/it]Running Inference:  20%|██        | 41/200 [04:42<16:25,  6.20s/it]Running Inference:  21%|██        | 42/200 [04:49<17:09,  6.51s/it]Running Inference:  22%|██▏       | 43/200 [04:57<18:03,  6.90s/it]Running Inference:  22%|██▏       | 44/200 [05:04<18:05,  6.96s/it]Running Inference:  22%|██▎       | 45/200 [05:07<14:51,  5.75s/it]Running Inference:  23%|██▎       | 46/200 [05:14<16:00,  6.24s/it]Running Inference:  24%|██▎       | 47/200 [05:17<13:00,  5.10s/it]Running Inference:  24%|██▍       | 48/200 [05:24<14:48,  5.85s/it]Running Inference:  24%|██▍       | 49/200 [05:28<13:05,  5.20s/it]Running Inference:  25%|██▌       | 50/200 [05:36<14:43,  5.89s/it]Running Inference:  26%|██▌       | 51/200 [05:43<15:51,  6.38s/it]Running Inference:  26%|██▌       | 52/200 [05:45<12:43,  5.16s/it]Running Inference:  26%|██▋       | 53/200 [05:53<14:29,  5.92s/it]Running Inference:  27%|██▋       | 54/200 [06:01<15:49,  6.50s/it]Running Inference:  28%|██▊       | 55/200 [06:08<16:08,  6.68s/it]Running Inference:  28%|██▊       | 56/200 [06:15<16:10,  6.74s/it]Running Inference:  28%|██▊       | 57/200 [06:22<16:19,  6.85s/it]Running Inference:  29%|██▉       | 58/200 [06:29<16:20,  6.90s/it]Running Inference:  30%|██▉       | 59/200 [06:37<16:40,  7.10s/it]Running Inference:  30%|███       | 60/200 [06:44<16:58,  7.28s/it]Running Inference:  30%|███       | 61/200 [06:52<17:23,  7.51s/it]Running Inference:  31%|███       | 62/200 [07:00<17:18,  7.53s/it]Running Inference:  32%|███▏      | 63/200 [07:08<17:24,  7.62s/it]Running Inference:  32%|███▏      | 64/200 [07:15<17:12,  7.59s/it]Running Inference:  32%|███▎      | 65/200 [07:23<17:13,  7.66s/it]Running Inference:  33%|███▎      | 66/200 [07:31<16:59,  7.61s/it]Running Inference:  34%|███▎      | 67/200 [07:38<17:00,  7.68s/it]Running Inference:  34%|███▍      | 68/200 [07:46<16:34,  7.53s/it]Running Inference:  34%|███▍      | 69/200 [07:54<16:39,  7.63s/it]Running Inference:  35%|███▌      | 70/200 [08:02<16:46,  7.74s/it]Running Inference:  36%|███▌      | 71/200 [08:09<16:34,  7.71s/it]Running Inference:  36%|███▌      | 72/200 [08:16<15:57,  7.48s/it]Running Inference:  36%|███▋      | 73/200 [08:24<16:09,  7.64s/it]Running Inference:  37%|███▋      | 74/200 [08:32<15:56,  7.59s/it]Running Inference:  38%|███▊      | 75/200 [08:39<15:37,  7.50s/it]Running Inference:  38%|███▊      | 76/200 [08:47<15:37,  7.56s/it]Running Inference:  38%|███▊      | 77/200 [08:54<15:31,  7.57s/it]Running Inference:  39%|███▉      | 78/200 [08:59<13:37,  6.70s/it]Running Inference:  40%|███▉      | 79/200 [09:06<13:56,  6.92s/it]Running Inference:  40%|████      | 80/200 [09:14<14:13,  7.11s/it]Running Inference:  40%|████      | 81/200 [09:21<14:08,  7.13s/it]Running Inference:  41%|████      | 82/200 [09:28<14:12,  7.23s/it]Running Inference:  42%|████▏     | 83/200 [09:36<14:03,  7.21s/it]Running Inference:  42%|████▏     | 84/200 [09:43<13:58,  7.23s/it]Running Inference:  42%|████▎     | 85/200 [09:50<13:56,  7.27s/it]Running Inference:  43%|████▎     | 86/200 [09:58<13:46,  7.25s/it]Running Inference:  44%|████▎     | 87/200 [10:05<13:33,  7.20s/it]Running Inference:  44%|████▍     | 88/200 [10:12<13:25,  7.19s/it]Running Inference:  44%|████▍     | 89/200 [10:19<13:27,  7.28s/it]Running Inference:  45%|████▌     | 90/200 [10:26<13:12,  7.20s/it]Running Inference:  46%|████▌     | 91/200 [10:34<13:14,  7.29s/it]Running Inference:  46%|████▌     | 92/200 [10:41<13:11,  7.33s/it]Running Inference:  46%|████▋     | 93/200 [10:43<10:14,  5.75s/it]Running Inference:  47%|████▋     | 94/200 [10:51<11:15,  6.37s/it]Running Inference:  48%|████▊     | 95/200 [10:59<11:45,  6.72s/it]Running Inference:  48%|████▊     | 96/200 [11:06<11:57,  6.90s/it]Running Inference:  48%|████▊     | 97/200 [11:13<11:50,  6.89s/it]Running Inference:  49%|████▉     | 98/200 [11:20<11:47,  6.93s/it]Running Inference:  50%|████▉     | 99/200 [11:27<12:00,  7.13s/it]Running Inference:  50%|█████     | 100/200 [11:35<11:53,  7.13s/it]Running Inference:  50%|█████     | 101/200 [11:42<11:49,  7.16s/it]Running Inference:  51%|█████     | 102/200 [11:49<11:45,  7.20s/it]Running Inference:  52%|█████▏    | 103/200 [11:57<11:46,  7.29s/it]Running Inference:  52%|█████▏    | 104/200 [12:04<11:39,  7.29s/it]Running Inference:  52%|█████▎    | 105/200 [12:11<11:32,  7.29s/it]Running Inference:  53%|█████▎    | 106/200 [12:18<11:27,  7.31s/it]Running Inference:  54%|█████▎    | 107/200 [12:26<11:26,  7.38s/it]Running Inference:  54%|█████▍    | 108/200 [12:33<11:14,  7.33s/it]Running Inference:  55%|█████▍    | 109/200 [12:41<11:13,  7.40s/it]Running Inference:  55%|█████▌    | 110/200 [12:49<11:30,  7.67s/it]Running Inference:  56%|█████▌    | 111/200 [12:52<09:18,  6.28s/it]Running Inference:  56%|█████▌    | 112/200 [12:59<09:31,  6.50s/it]Running Inference:  56%|█████▋    | 113/200 [13:07<09:52,  6.81s/it]Running Inference:  57%|█████▋    | 114/200 [13:14<10:02,  7.01s/it]Running Inference:  57%|█████▊    | 115/200 [13:22<10:08,  7.16s/it]Running Inference:  58%|█████▊    | 116/200 [13:29<10:11,  7.28s/it]Running Inference:  58%|█████▊    | 117/200 [13:34<09:08,  6.61s/it]Running Inference:  59%|█████▉    | 118/200 [13:43<09:42,  7.11s/it]Running Inference:  60%|█████▉    | 119/200 [13:51<10:02,  7.44s/it]Running Inference:  60%|██████    | 120/200 [13:59<10:20,  7.76s/it]Running Inference:  60%|██████    | 121/200 [14:07<10:08,  7.71s/it]Running Inference:  61%|██████    | 122/200 [14:14<09:44,  7.50s/it]Running Inference:  62%|██████▏   | 123/200 [14:22<09:45,  7.60s/it]Running Inference:  62%|██████▏   | 124/200 [14:29<09:32,  7.53s/it]Running Inference:  62%|██████▎   | 125/200 [14:36<09:14,  7.40s/it]Running Inference:  63%|██████▎   | 126/200 [14:43<08:58,  7.27s/it]Running Inference:  64%|██████▎   | 127/200 [14:51<08:54,  7.32s/it]Running Inference:  64%|██████▍   | 128/200 [14:58<08:46,  7.31s/it]Running Inference:  64%|██████▍   | 129/200 [15:01<07:05,  5.99s/it]Running Inference:  65%|██████▌   | 130/200 [15:08<07:27,  6.39s/it]Running Inference:  66%|██████▌   | 131/200 [15:15<07:37,  6.64s/it]Running Inference:  66%|██████▌   | 132/200 [15:23<07:45,  6.85s/it]Running Inference:  66%|██████▋   | 133/200 [15:30<07:49,  7.00s/it]Running Inference:  67%|██████▋   | 134/200 [15:38<08:01,  7.30s/it]Running Inference:  68%|██████▊   | 135/200 [15:45<07:53,  7.29s/it]Running Inference:  68%|██████▊   | 136/200 [15:53<07:56,  7.44s/it]Running Inference:  68%|██████▊   | 137/200 [16:00<07:44,  7.38s/it]Running Inference:  69%|██████▉   | 138/200 [16:09<07:54,  7.65s/it]Running Inference:  70%|██████▉   | 139/200 [16:16<07:46,  7.64s/it]Running Inference:  70%|███████   | 140/200 [16:23<07:25,  7.43s/it]Running Inference:  70%|███████   | 141/200 [16:31<07:21,  7.49s/it]Running Inference:  71%|███████   | 142/200 [16:38<07:14,  7.49s/it]Running Inference:  72%|███████▏  | 143/200 [16:46<07:07,  7.49s/it]Running Inference:  72%|███████▏  | 144/200 [16:53<06:52,  7.37s/it]Running Inference:  72%|███████▎  | 145/200 [17:00<06:47,  7.41s/it]Running Inference:  73%|███████▎  | 146/200 [17:08<06:39,  7.41s/it]Running Inference:  74%|███████▎  | 147/200 [17:15<06:27,  7.30s/it]Running Inference:  74%|███████▍  | 148/200 [17:23<06:32,  7.54s/it]Running Inference:  74%|███████▍  | 149/200 [17:31<06:32,  7.70s/it]Running Inference:  75%|███████▌  | 150/200 [17:38<06:20,  7.61s/it]Running Inference:  76%|███████▌  | 151/200 [17:46<06:13,  7.61s/it]Running Inference:  76%|███████▌  | 152/200 [17:53<05:58,  7.46s/it]Running Inference:  76%|███████▋  | 153/200 [18:01<05:55,  7.56s/it]Running Inference:  77%|███████▋  | 154/200 [18:08<05:41,  7.42s/it]Running Inference:  78%|███████▊  | 155/200 [18:16<05:36,  7.47s/it]Running Inference:  78%|███████▊  | 156/200 [18:23<05:23,  7.36s/it]Running Inference:  78%|███████▊  | 157/200 [18:31<05:23,  7.52s/it]Running Inference:  79%|███████▉  | 158/200 [18:38<05:11,  7.42s/it]Running Inference:  80%|███████▉  | 159/200 [18:46<05:10,  7.56s/it]Running Inference:  80%|████████  | 160/200 [18:53<04:54,  7.36s/it]Running Inference:  80%|████████  | 161/200 [19:00<04:47,  7.37s/it]Running Inference:  81%|████████  | 162/200 [19:05<04:18,  6.81s/it]Running Inference:  82%|████████▏ | 163/200 [19:09<03:30,  5.69s/it]Running Inference:  82%|████████▏ | 164/200 [19:12<03:01,  5.04s/it]Running Inference:  82%|████████▎ | 165/200 [19:20<03:22,  5.78s/it]Running Inference:  83%|████████▎ | 166/200 [19:27<03:34,  6.30s/it]Running Inference:  84%|████████▎ | 167/200 [19:35<03:39,  6.65s/it]Running Inference:  84%|████████▍ | 168/200 [19:42<03:43,  6.97s/it]Running Inference:  84%|████████▍ | 169/200 [19:50<03:41,  7.14s/it]Running Inference:  85%|████████▌ | 170/200 [19:54<03:04,  6.14s/it]Running Inference:  86%|████████▌ | 171/200 [20:01<03:12,  6.65s/it]Running Inference:  86%|████████▌ | 172/200 [20:09<03:16,  7.01s/it]Running Inference:  86%|████████▋ | 173/200 [20:17<03:12,  7.13s/it]Running Inference:  87%|████████▋ | 174/200 [20:24<03:03,  7.06s/it]Running Inference:  88%|████████▊ | 175/200 [20:31<02:55,  7.03s/it]Running Inference:  88%|████████▊ | 176/200 [20:38<02:54,  7.25s/it]Running Inference:  88%|████████▊ | 177/200 [20:46<02:50,  7.42s/it]Running Inference:  89%|████████▉ | 178/200 [20:54<02:46,  7.57s/it]Running Inference:  90%|████████▉ | 179/200 [21:01<02:34,  7.36s/it]Running Inference:  90%|█████████ | 180/200 [21:10<02:35,  7.75s/it]Running Inference:  90%|█████████ | 181/200 [21:17<02:26,  7.73s/it]Running Inference:  91%|█████████ | 182/200 [21:25<02:19,  7.74s/it]Running Inference:  92%|█████████▏| 183/200 [21:32<02:09,  7.59s/it]Running Inference:  92%|█████████▏| 184/200 [21:40<02:03,  7.70s/it]Running Inference:  92%|█████████▎| 185/200 [21:47<01:53,  7.57s/it]Running Inference:  93%|█████████▎| 186/200 [21:56<01:48,  7.74s/it]Running Inference:  94%|█████████▎| 187/200 [22:03<01:40,  7.73s/it]Running Inference:  94%|█████████▍| 188/200 [22:11<01:31,  7.60s/it]Running Inference:  94%|█████████▍| 189/200 [22:13<01:05,  5.91s/it]Running Inference:  95%|█████████▌| 190/200 [22:20<01:03,  6.31s/it]Running Inference:  96%|█████████▌| 191/200 [22:27<00:59,  6.64s/it]Running Inference:  96%|█████████▌| 192/200 [22:35<00:54,  6.87s/it]Running Inference:  96%|█████████▋| 193/200 [22:43<00:50,  7.20s/it]Running Inference:  97%|█████████▋| 194/200 [22:50<00:44,  7.39s/it]Running Inference:  98%|█████████▊| 195/200 [22:58<00:36,  7.35s/it]Running Inference:  98%|█████████▊| 196/200 [23:05<00:29,  7.46s/it]Running Inference:  98%|█████████▊| 197/200 [23:13<00:22,  7.57s/it]Running Inference:  99%|█████████▉| 198/200 [23:20<00:14,  7.40s/it]Running Inference: 100%|█████████▉| 199/200 [23:28<00:07,  7.58s/it]Running Inference: 100%|██████████| 200/200 [23:36<00:00,  7.54s/it]Running Inference: 100%|██████████| 200/200 [23:36<00:00,  7.08s/it]
2025-12-14 18:36:21,931 - INFO - Inference completed.
2025-12-14 18:36:21,942 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 18:36:21,942 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.684 seconds.
Prefix dict has been built successfully.
2025-12-14 18:36:24,329 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 18:36:24,329 - INFO - Metrics:
16.77
2025-12-14 18:36:24,330 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=dureader, ratio=0.5) on GPU 1


========================================
LongBench Task: lcc
========================================
----------------------------------------
Task: lcc | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:36:30,994 - INFO - Set deterministic seeds to 42
2025-12-14 18:36:30,995 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:36:30,995 - INFO - Starting evaluation run...
2025-12-14 18:36:30,995 - INFO - Output directory set to: longbenchresult
2025-12-14 18:36:30,995 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 18:36:30,995 - INFO - KV Press 'tova' setup.
2025-12-14 18:36:30,995 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:36:30,995 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.43it/s]
Device set to use cuda:0
2025-12-14 18:36:42,865 - INFO - Model pipeline loaded.
2025-12-14 18:36:42,865 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 500 examples [00:00, 6755.46 examples/s]
2025-12-14 18:36:47,390 - INFO - Dataset loaded with 500 entries.
2025-12-14 18:36:47,390 - INFO - Dataset processed with 500 entries.
2025-12-14 18:36:47,404 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<40:30,  4.87s/it]Running Inference:   0%|          | 2/500 [00:08<35:11,  4.24s/it]Running Inference:   1%|          | 3/500 [00:12<35:10,  4.25s/it]Running Inference:   1%|          | 4/500 [00:16<33:35,  4.06s/it]Running Inference:   1%|          | 5/500 [00:20<32:43,  3.97s/it]Running Inference:   1%|          | 6/500 [00:24<32:44,  3.98s/it]Running Inference:   1%|▏         | 7/500 [00:28<32:20,  3.94s/it]Running Inference:   2%|▏         | 8/500 [00:32<31:45,  3.87s/it]Running Inference:   2%|▏         | 9/500 [00:35<31:21,  3.83s/it]Running Inference:   2%|▏         | 10/500 [00:39<31:03,  3.80s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:43<32:01,  3.93s/it]Running Inference:   2%|▏         | 12/500 [00:47<31:47,  3.91s/it]Running Inference:   3%|▎         | 13/500 [00:51<31:26,  3.87s/it]Running Inference:   3%|▎         | 14/500 [00:55<31:11,  3.85s/it]Running Inference:   3%|▎         | 15/500 [00:59<30:56,  3.83s/it]Running Inference:   3%|▎         | 16/500 [01:02<30:47,  3.82s/it]Running Inference:   3%|▎         | 17/500 [01:07<32:20,  4.02s/it]Running Inference:   4%|▎         | 18/500 [01:11<31:52,  3.97s/it]Running Inference:   4%|▍         | 19/500 [01:14<31:24,  3.92s/it]Running Inference:   4%|▍         | 20/500 [01:18<30:56,  3.87s/it]Running Inference:   4%|▍         | 21/500 [01:22<30:38,  3.84s/it]Running Inference:   4%|▍         | 22/500 [01:26<30:22,  3.81s/it]Running Inference:   5%|▍         | 23/500 [01:29<30:07,  3.79s/it]Running Inference:   5%|▍         | 24/500 [01:33<29:56,  3.77s/it]Running Inference:   5%|▌         | 25/500 [01:37<29:54,  3.78s/it]Running Inference:   5%|▌         | 26/500 [01:41<29:59,  3.80s/it]Running Inference:   5%|▌         | 27/500 [01:45<29:53,  3.79s/it]Running Inference:   6%|▌         | 28/500 [01:50<32:56,  4.19s/it]Running Inference:   6%|▌         | 29/500 [01:54<32:28,  4.14s/it]Running Inference:   6%|▌         | 30/500 [01:58<33:02,  4.22s/it]Running Inference:   6%|▌         | 31/500 [02:02<32:11,  4.12s/it]Running Inference:   6%|▋         | 32/500 [02:06<31:27,  4.03s/it]Running Inference:   7%|▋         | 33/500 [02:10<30:45,  3.95s/it]Running Inference:   7%|▋         | 34/500 [02:13<30:13,  3.89s/it]Running Inference:   7%|▋         | 35/500 [02:17<29:47,  3.84s/it]Running Inference:   7%|▋         | 36/500 [02:21<30:33,  3.95s/it]Running Inference:   7%|▋         | 37/500 [02:25<31:03,  4.03s/it]Running Inference:   8%|▊         | 38/500 [02:29<30:29,  3.96s/it]Running Inference:   8%|▊         | 39/500 [02:33<30:57,  4.03s/it]Running Inference:   8%|▊         | 40/500 [02:37<30:16,  3.95s/it]Running Inference:   8%|▊         | 41/500 [02:41<29:55,  3.91s/it]Running Inference:   8%|▊         | 42/500 [02:46<31:30,  4.13s/it]Running Inference:   9%|▊         | 43/500 [02:50<32:22,  4.25s/it]Running Inference:   9%|▉         | 44/500 [02:51<25:17,  3.33s/it]Running Inference:   9%|▉         | 45/500 [02:55<26:27,  3.49s/it]Running Inference:   9%|▉         | 46/500 [02:59<26:55,  3.56s/it]Running Inference:   9%|▉         | 47/500 [03:03<27:34,  3.65s/it]Running Inference:  10%|▉         | 48/500 [03:07<27:40,  3.67s/it]Running Inference:  10%|▉         | 49/500 [03:11<28:11,  3.75s/it]Running Inference:  10%|█         | 50/500 [03:15<29:50,  3.98s/it]Running Inference:  10%|█         | 51/500 [03:19<29:17,  3.91s/it]Running Inference:  10%|█         | 52/500 [03:23<28:57,  3.88s/it]Running Inference:  11%|█         | 53/500 [03:27<29:01,  3.90s/it]Running Inference:  11%|█         | 54/500 [03:30<28:54,  3.89s/it]Running Inference:  11%|█         | 55/500 [03:34<28:36,  3.86s/it]Running Inference:  11%|█         | 56/500 [03:38<28:24,  3.84s/it]Running Inference:  11%|█▏        | 57/500 [03:47<38:59,  5.28s/it]Running Inference:  12%|█▏        | 58/500 [03:50<35:27,  4.81s/it]Running Inference:  12%|█▏        | 59/500 [03:52<29:10,  3.97s/it]Running Inference:  12%|█▏        | 60/500 [03:56<28:37,  3.90s/it]Running Inference:  12%|█▏        | 61/500 [04:00<28:21,  3.88s/it]Running Inference:  12%|█▏        | 62/500 [04:04<28:00,  3.84s/it]Running Inference:  13%|█▎        | 63/500 [04:07<27:48,  3.82s/it]Running Inference:  13%|█▎        | 64/500 [04:11<27:34,  3.79s/it]Running Inference:  13%|█▎        | 65/500 [04:15<27:27,  3.79s/it]Running Inference:  13%|█▎        | 66/500 [04:19<27:59,  3.87s/it]Running Inference:  13%|█▎        | 67/500 [04:23<27:50,  3.86s/it]Running Inference:  14%|█▎        | 68/500 [04:27<27:38,  3.84s/it]Running Inference:  14%|█▍        | 69/500 [04:30<27:24,  3.82s/it]Running Inference:  14%|█▍        | 70/500 [04:34<27:12,  3.80s/it]Running Inference:  14%|█▍        | 71/500 [04:37<25:35,  3.58s/it]Running Inference:  14%|█▍        | 72/500 [04:41<26:27,  3.71s/it]Running Inference:  15%|█▍        | 73/500 [04:45<26:28,  3.72s/it]Running Inference:  15%|█▍        | 74/500 [04:49<26:38,  3.75s/it]Running Inference:  15%|█▌        | 75/500 [04:53<27:11,  3.84s/it]Running Inference:  15%|█▌        | 76/500 [04:57<27:19,  3.87s/it]Running Inference:  15%|█▌        | 77/500 [05:01<27:10,  3.86s/it]Running Inference:  16%|█▌        | 78/500 [05:05<27:53,  3.96s/it]Running Inference:  16%|█▌        | 79/500 [05:06<22:53,  3.26s/it]Running Inference:  16%|█▌        | 80/500 [05:10<23:58,  3.42s/it]Running Inference:  16%|█▌        | 81/500 [05:14<25:33,  3.66s/it]Running Inference:  16%|█▋        | 82/500 [05:19<26:49,  3.85s/it]Running Inference:  17%|█▋        | 83/500 [05:21<22:55,  3.30s/it]Running Inference:  17%|█▋        | 84/500 [05:25<23:58,  3.46s/it]Running Inference:  17%|█▋        | 85/500 [05:29<26:42,  3.86s/it]Running Inference:  17%|█▋        | 86/500 [05:33<26:23,  3.83s/it]Running Inference:  17%|█▋        | 87/500 [05:37<26:09,  3.80s/it]Running Inference:  18%|█▊        | 88/500 [05:41<26:01,  3.79s/it]Running Inference:  18%|█▊        | 89/500 [05:43<22:48,  3.33s/it]Running Inference:  18%|█▊        | 90/500 [05:47<24:20,  3.56s/it]Running Inference:  18%|█▊        | 91/500 [05:51<25:12,  3.70s/it]Running Inference:  18%|█▊        | 92/500 [05:55<25:28,  3.75s/it]Running Inference:  19%|█▊        | 93/500 [05:59<25:31,  3.76s/it]Running Inference:  19%|█▉        | 94/500 [06:02<25:26,  3.76s/it]Running Inference:  19%|█▉        | 95/500 [06:03<19:34,  2.90s/it]Running Inference:  19%|█▉        | 96/500 [06:07<21:27,  3.19s/it]Running Inference:  19%|█▉        | 97/500 [06:11<22:37,  3.37s/it]Running Inference:  20%|█▉        | 98/500 [06:15<23:18,  3.48s/it]Running Inference:  20%|█▉        | 99/500 [06:18<23:45,  3.56s/it]Running Inference:  20%|██        | 100/500 [06:23<24:50,  3.73s/it]Running Inference:  20%|██        | 101/500 [06:27<25:14,  3.79s/it]Running Inference:  20%|██        | 102/500 [06:30<24:43,  3.73s/it]Running Inference:  21%|██        | 103/500 [06:34<24:40,  3.73s/it]Running Inference:  21%|██        | 104/500 [06:40<28:44,  4.35s/it]Running Inference:  21%|██        | 105/500 [06:44<29:04,  4.42s/it]Running Inference:  21%|██        | 106/500 [06:50<30:49,  4.69s/it]Running Inference:  21%|██▏       | 107/500 [06:54<29:19,  4.48s/it]Running Inference:  22%|██▏       | 108/500 [06:57<27:52,  4.27s/it]Running Inference:  22%|██▏       | 109/500 [07:01<26:57,  4.14s/it]Running Inference:  22%|██▏       | 110/500 [07:05<26:22,  4.06s/it]Running Inference:  22%|██▏       | 111/500 [07:09<25:40,  3.96s/it]Running Inference:  22%|██▏       | 112/500 [07:13<25:15,  3.91s/it]Running Inference:  23%|██▎       | 113/500 [07:17<25:37,  3.97s/it]Running Inference:  23%|██▎       | 114/500 [07:22<27:49,  4.33s/it]Running Inference:  23%|██▎       | 115/500 [07:26<27:13,  4.24s/it]Running Inference:  23%|██▎       | 116/500 [07:30<26:53,  4.20s/it]Running Inference:  23%|██▎       | 117/500 [07:34<25:54,  4.06s/it]Running Inference:  24%|██▎       | 118/500 [07:38<26:29,  4.16s/it]Running Inference:  24%|██▍       | 119/500 [07:42<25:34,  4.03s/it]Running Inference:  24%|██▍       | 120/500 [07:46<25:41,  4.06s/it]Running Inference:  24%|██▍       | 121/500 [07:50<25:14,  4.00s/it]Running Inference:  24%|██▍       | 122/500 [07:53<24:37,  3.91s/it]Running Inference:  25%|██▍       | 123/500 [07:57<24:14,  3.86s/it]Running Inference:  25%|██▍       | 124/500 [08:01<23:56,  3.82s/it]Running Inference:  25%|██▌       | 125/500 [08:05<24:19,  3.89s/it]Running Inference:  25%|██▌       | 126/500 [08:09<23:58,  3.85s/it]Running Inference:  25%|██▌       | 127/500 [08:10<18:50,  3.03s/it]Running Inference:  26%|██▌       | 128/500 [08:14<20:29,  3.31s/it]Running Inference:  26%|██▌       | 129/500 [08:18<21:17,  3.44s/it]Running Inference:  26%|██▌       | 130/500 [08:21<22:04,  3.58s/it]Running Inference:  26%|██▌       | 131/500 [08:25<22:21,  3.64s/it]Running Inference:  26%|██▋       | 132/500 [08:29<22:33,  3.68s/it]Running Inference:  27%|██▋       | 133/500 [08:33<22:47,  3.73s/it]Running Inference:  27%|██▋       | 134/500 [08:37<22:52,  3.75s/it]Running Inference:  27%|██▋       | 135/500 [08:41<23:05,  3.79s/it]Running Inference:  27%|██▋       | 136/500 [08:44<23:10,  3.82s/it]Running Inference:  27%|██▋       | 137/500 [08:46<18:37,  3.08s/it]Running Inference:  28%|██▊       | 138/500 [08:50<20:04,  3.33s/it]Running Inference:  28%|██▊       | 139/500 [08:53<20:43,  3.44s/it]Running Inference:  28%|██▊       | 140/500 [08:57<21:29,  3.58s/it]Running Inference:  28%|██▊       | 141/500 [09:02<22:33,  3.77s/it]Running Inference:  28%|██▊       | 142/500 [09:06<22:55,  3.84s/it]Running Inference:  29%|██▊       | 143/500 [09:09<22:44,  3.82s/it]Running Inference:  29%|██▉       | 144/500 [09:13<22:31,  3.80s/it]Running Inference:  29%|██▉       | 145/500 [09:17<22:40,  3.83s/it]Running Inference:  29%|██▉       | 146/500 [09:21<22:56,  3.89s/it]Running Inference:  29%|██▉       | 147/500 [09:25<23:09,  3.94s/it]Running Inference:  30%|██▉       | 148/500 [09:29<22:42,  3.87s/it]Running Inference:  30%|██▉       | 149/500 [09:32<22:23,  3.83s/it]Running Inference:  30%|███       | 150/500 [09:36<22:06,  3.79s/it]Running Inference:  30%|███       | 151/500 [09:40<22:07,  3.80s/it]Running Inference:  30%|███       | 152/500 [09:41<16:29,  2.84s/it]Running Inference:  31%|███       | 153/500 [09:44<18:00,  3.11s/it]Running Inference:  31%|███       | 154/500 [09:48<19:01,  3.30s/it]Running Inference:  31%|███       | 155/500 [09:52<19:40,  3.42s/it]Running Inference:  31%|███       | 156/500 [09:56<20:11,  3.52s/it]Running Inference:  31%|███▏      | 157/500 [10:00<20:54,  3.66s/it]Running Inference:  32%|███▏      | 158/500 [10:03<21:04,  3.70s/it]Running Inference:  32%|███▏      | 159/500 [10:07<21:04,  3.71s/it]Running Inference:  32%|███▏      | 160/500 [10:11<21:03,  3.72s/it]Running Inference:  32%|███▏      | 161/500 [10:15<21:06,  3.74s/it]Running Inference:  32%|███▏      | 162/500 [10:18<21:17,  3.78s/it]Running Inference:  33%|███▎      | 163/500 [10:22<21:07,  3.76s/it]Running Inference:  33%|███▎      | 164/500 [10:26<21:05,  3.77s/it]Running Inference:  33%|███▎      | 165/500 [10:30<21:07,  3.78s/it]Running Inference:  33%|███▎      | 166/500 [10:34<20:56,  3.76s/it]Running Inference:  33%|███▎      | 167/500 [10:38<21:29,  3.87s/it]Running Inference:  34%|███▎      | 168/500 [10:38<16:10,  2.92s/it]Running Inference:  34%|███▍      | 169/500 [10:42<17:38,  3.20s/it]Running Inference:  34%|███▍      | 170/500 [10:45<16:52,  3.07s/it]Running Inference:  34%|███▍      | 171/500 [10:49<18:08,  3.31s/it]Running Inference:  34%|███▍      | 172/500 [10:53<18:46,  3.44s/it]Running Inference:  35%|███▍      | 173/500 [10:56<19:15,  3.53s/it]Running Inference:  35%|███▍      | 174/500 [11:00<20:01,  3.69s/it]Running Inference:  35%|███▌      | 175/500 [11:05<21:03,  3.89s/it]Running Inference:  35%|███▌      | 176/500 [11:08<20:47,  3.85s/it]Running Inference:  35%|███▌      | 177/500 [11:13<21:42,  4.03s/it]Running Inference:  36%|███▌      | 178/500 [11:17<21:04,  3.93s/it]Running Inference:  36%|███▌      | 179/500 [11:20<20:42,  3.87s/it]Running Inference:  36%|███▌      | 180/500 [11:24<20:35,  3.86s/it]Running Inference:  36%|███▌      | 181/500 [11:28<20:17,  3.82s/it]Running Inference:  36%|███▋      | 182/500 [11:32<20:03,  3.79s/it]Running Inference:  37%|███▋      | 183/500 [11:36<20:15,  3.83s/it]Running Inference:  37%|███▋      | 184/500 [11:39<20:13,  3.84s/it]Running Inference:  37%|███▋      | 185/500 [11:41<16:27,  3.14s/it]Running Inference:  37%|███▋      | 186/500 [11:45<17:26,  3.33s/it]Running Inference:  37%|███▋      | 187/500 [11:49<18:15,  3.50s/it]Running Inference:  38%|███▊      | 188/500 [11:52<18:34,  3.57s/it]Running Inference:  38%|███▊      | 189/500 [11:56<18:54,  3.65s/it]Running Inference:  38%|███▊      | 190/500 [12:00<19:36,  3.79s/it]Running Inference:  38%|███▊      | 191/500 [12:04<19:25,  3.77s/it]Running Inference:  38%|███▊      | 192/500 [12:08<19:26,  3.79s/it]Running Inference:  39%|███▊      | 193/500 [12:12<19:23,  3.79s/it]Running Inference:  39%|███▉      | 194/500 [12:15<19:18,  3.79s/it]Running Inference:  39%|███▉      | 195/500 [12:19<19:13,  3.78s/it]Running Inference:  39%|███▉      | 196/500 [12:23<19:07,  3.78s/it]Running Inference:  39%|███▉      | 197/500 [12:27<19:14,  3.81s/it]Running Inference:  40%|███▉      | 198/500 [12:31<19:01,  3.78s/it]Running Inference:  40%|███▉      | 199/500 [12:34<18:51,  3.76s/it]Running Inference:  40%|████      | 200/500 [12:38<18:55,  3.79s/it]Running Inference:  40%|████      | 201/500 [12:42<19:04,  3.83s/it]Running Inference:  40%|████      | 202/500 [12:46<18:52,  3.80s/it]Running Inference:  41%|████      | 203/500 [12:51<20:54,  4.23s/it]Running Inference:  41%|████      | 204/500 [12:55<20:12,  4.10s/it]Running Inference:  41%|████      | 205/500 [12:59<19:37,  3.99s/it]Running Inference:  41%|████      | 206/500 [13:02<19:08,  3.91s/it]Running Inference:  41%|████▏     | 207/500 [13:06<18:54,  3.87s/it]Running Inference:  42%|████▏     | 208/500 [13:10<19:16,  3.96s/it]Running Inference:  42%|████▏     | 209/500 [13:14<19:34,  4.04s/it]Running Inference:  42%|████▏     | 210/500 [13:18<19:06,  3.95s/it]Running Inference:  42%|████▏     | 211/500 [13:22<19:03,  3.96s/it]Running Inference:  42%|████▏     | 212/500 [13:26<18:38,  3.89s/it]Running Inference:  43%|████▎     | 213/500 [13:28<15:23,  3.22s/it]Running Inference:  43%|████▎     | 214/500 [13:31<16:08,  3.39s/it]Running Inference:  43%|████▎     | 215/500 [13:35<16:43,  3.52s/it]Running Inference:  43%|████▎     | 216/500 [13:39<16:58,  3.59s/it]Running Inference:  43%|████▎     | 217/500 [13:43<17:33,  3.72s/it]Running Inference:  44%|████▎     | 218/500 [13:47<17:50,  3.80s/it]Running Inference:  44%|████▍     | 219/500 [13:51<17:40,  3.77s/it]Running Inference:  44%|████▍     | 220/500 [13:51<13:26,  2.88s/it]Running Inference:  44%|████▍     | 221/500 [13:55<14:11,  3.05s/it]Running Inference:  44%|████▍     | 222/500 [13:56<11:15,  2.43s/it]Running Inference:  45%|████▍     | 223/500 [14:00<13:03,  2.83s/it]Running Inference:  45%|████▍     | 224/500 [14:03<14:01,  3.05s/it]Running Inference:  45%|████▌     | 225/500 [14:07<14:53,  3.25s/it]Running Inference:  45%|████▌     | 226/500 [14:11<15:28,  3.39s/it]Running Inference:  45%|████▌     | 227/500 [14:14<15:54,  3.50s/it]Running Inference:  46%|████▌     | 228/500 [14:18<16:03,  3.54s/it]Running Inference:  46%|████▌     | 229/500 [14:22<16:12,  3.59s/it]Running Inference:  46%|████▌     | 230/500 [14:25<16:20,  3.63s/it]Running Inference:  46%|████▌     | 231/500 [14:29<16:26,  3.67s/it]Running Inference:  46%|████▋     | 232/500 [14:33<16:30,  3.69s/it]Running Inference:  47%|████▋     | 233/500 [14:37<16:39,  3.74s/it]Running Inference:  47%|████▋     | 234/500 [14:41<16:41,  3.76s/it]Running Inference:  47%|████▋     | 235/500 [14:45<17:04,  3.87s/it]Running Inference:  47%|████▋     | 236/500 [14:48<16:47,  3.82s/it]Running Inference:  47%|████▋     | 237/500 [14:52<16:35,  3.78s/it]Running Inference:  48%|████▊     | 238/500 [14:56<16:28,  3.77s/it]Running Inference:  48%|████▊     | 239/500 [15:00<16:31,  3.80s/it]Running Inference:  48%|████▊     | 240/500 [15:03<16:26,  3.79s/it]Running Inference:  48%|████▊     | 241/500 [15:07<16:21,  3.79s/it]Running Inference:  48%|████▊     | 242/500 [15:11<16:10,  3.76s/it]Running Inference:  49%|████▊     | 243/500 [15:15<17:00,  3.97s/it]Running Inference:  49%|████▉     | 244/500 [15:20<17:18,  4.06s/it]Running Inference:  49%|████▉     | 245/500 [15:24<17:00,  4.00s/it]Running Inference:  49%|████▉     | 246/500 [15:27<16:33,  3.91s/it]Running Inference:  49%|████▉     | 247/500 [15:31<16:25,  3.89s/it]Running Inference:  50%|████▉     | 248/500 [15:35<16:11,  3.85s/it]Running Inference:  50%|████▉     | 249/500 [15:39<16:06,  3.85s/it]Running Inference:  50%|█████     | 250/500 [15:40<12:14,  2.94s/it]Running Inference:  50%|█████     | 251/500 [15:44<13:38,  3.29s/it]Running Inference:  50%|█████     | 252/500 [15:46<12:34,  3.04s/it]Running Inference:  51%|█████     | 253/500 [15:50<13:25,  3.26s/it]Running Inference:  51%|█████     | 254/500 [15:54<14:00,  3.41s/it]Running Inference:  51%|█████     | 255/500 [15:58<14:34,  3.57s/it]Running Inference:  51%|█████     | 256/500 [16:01<14:43,  3.62s/it]Running Inference:  51%|█████▏    | 257/500 [16:05<15:05,  3.73s/it]Running Inference:  52%|█████▏    | 258/500 [16:09<15:03,  3.73s/it]Running Inference:  52%|█████▏    | 259/500 [16:13<15:04,  3.75s/it]Running Inference:  52%|█████▏    | 260/500 [16:17<14:58,  3.74s/it]Running Inference:  52%|█████▏    | 261/500 [16:20<14:51,  3.73s/it]Running Inference:  52%|█████▏    | 262/500 [16:24<14:56,  3.77s/it]Running Inference:  53%|█████▎    | 263/500 [16:28<14:53,  3.77s/it]Running Inference:  53%|█████▎    | 264/500 [16:29<11:17,  2.87s/it]Running Inference:  53%|█████▎    | 265/500 [16:33<12:28,  3.19s/it]Running Inference:  53%|█████▎    | 266/500 [16:36<13:06,  3.36s/it]Running Inference:  53%|█████▎    | 267/500 [16:41<14:04,  3.63s/it]Running Inference:  54%|█████▎    | 268/500 [16:44<14:10,  3.67s/it]Running Inference:  54%|█████▍    | 269/500 [16:48<14:16,  3.71s/it]Running Inference:  54%|█████▍    | 270/500 [16:52<14:16,  3.72s/it]Running Inference:  54%|█████▍    | 271/500 [16:56<14:10,  3.71s/it]Running Inference:  54%|█████▍    | 272/500 [16:59<14:11,  3.74s/it]Running Inference:  55%|█████▍    | 273/500 [17:03<14:16,  3.77s/it]Running Inference:  55%|█████▍    | 274/500 [17:07<14:08,  3.76s/it]Running Inference:  55%|█████▌    | 275/500 [17:11<14:04,  3.75s/it]Running Inference:  55%|█████▌    | 276/500 [17:15<14:03,  3.77s/it]Running Inference:  55%|█████▌    | 277/500 [17:18<14:00,  3.77s/it]Running Inference:  56%|█████▌    | 278/500 [17:22<14:01,  3.79s/it]Running Inference:  56%|█████▌    | 279/500 [17:26<14:08,  3.84s/it]Running Inference:  56%|█████▌    | 280/500 [17:30<14:16,  3.90s/it]Running Inference:  56%|█████▌    | 281/500 [17:34<14:10,  3.88s/it]Running Inference:  56%|█████▋    | 282/500 [17:38<14:01,  3.86s/it]Running Inference:  57%|█████▋    | 283/500 [17:42<13:51,  3.83s/it]Running Inference:  57%|█████▋    | 284/500 [17:45<13:51,  3.85s/it]Running Inference:  57%|█████▋    | 285/500 [17:49<13:48,  3.85s/it]Running Inference:  57%|█████▋    | 286/500 [17:53<13:35,  3.81s/it]Running Inference:  57%|█████▋    | 287/500 [17:58<14:21,  4.04s/it]Running Inference:  58%|█████▊    | 288/500 [18:02<14:14,  4.03s/it]Running Inference:  58%|█████▊    | 289/500 [18:05<13:53,  3.95s/it]Running Inference:  58%|█████▊    | 290/500 [18:09<13:46,  3.93s/it]Running Inference:  58%|█████▊    | 291/500 [18:13<13:43,  3.94s/it]Running Inference:  58%|█████▊    | 292/500 [18:17<13:25,  3.87s/it]Running Inference:  59%|█████▊    | 293/500 [18:18<10:26,  3.03s/it]Running Inference:  59%|█████▉    | 294/500 [18:22<11:06,  3.24s/it]Running Inference:  59%|█████▉    | 295/500 [18:25<11:35,  3.39s/it]Running Inference:  59%|█████▉    | 296/500 [18:29<11:56,  3.51s/it]Running Inference:  59%|█████▉    | 297/500 [18:33<12:13,  3.62s/it]Running Inference:  60%|█████▉    | 298/500 [18:37<12:16,  3.65s/it]Running Inference:  60%|█████▉    | 299/500 [18:39<10:53,  3.25s/it]Running Inference:  60%|██████    | 300/500 [18:43<11:16,  3.38s/it]Running Inference:  60%|██████    | 301/500 [18:47<11:51,  3.57s/it]Running Inference:  60%|██████    | 302/500 [18:51<11:59,  3.63s/it]Running Inference:  61%|██████    | 303/500 [18:54<12:04,  3.68s/it]Running Inference:  61%|██████    | 304/500 [18:59<12:37,  3.86s/it]Running Inference:  61%|██████    | 305/500 [19:02<12:23,  3.82s/it]Running Inference:  61%|██████    | 306/500 [19:07<12:43,  3.94s/it]Running Inference:  61%|██████▏   | 307/500 [19:10<12:31,  3.90s/it]Running Inference:  62%|██████▏   | 308/500 [19:14<12:17,  3.84s/it]Running Inference:  62%|██████▏   | 309/500 [19:18<12:06,  3.81s/it]Running Inference:  62%|██████▏   | 310/500 [19:22<12:00,  3.79s/it]Running Inference:  62%|██████▏   | 311/500 [19:25<12:01,  3.82s/it]Running Inference:  62%|██████▏   | 312/500 [19:29<11:54,  3.80s/it]Running Inference:  63%|██████▎   | 313/500 [19:33<11:55,  3.82s/it]Running Inference:  63%|██████▎   | 314/500 [19:37<11:45,  3.79s/it]Running Inference:  63%|██████▎   | 315/500 [19:41<11:35,  3.76s/it]Running Inference:  63%|██████▎   | 316/500 [19:44<11:27,  3.74s/it]Running Inference:  63%|██████▎   | 317/500 [19:48<11:22,  3.73s/it]Running Inference:  64%|██████▎   | 318/500 [19:52<11:19,  3.73s/it]Running Inference:  64%|██████▍   | 319/500 [19:55<11:14,  3.72s/it]Running Inference:  64%|██████▍   | 320/500 [20:00<11:41,  3.90s/it]Running Inference:  64%|██████▍   | 321/500 [20:04<11:47,  3.95s/it]Running Inference:  64%|██████▍   | 322/500 [20:08<11:33,  3.90s/it]Running Inference:  65%|██████▍   | 323/500 [20:11<11:21,  3.85s/it]Running Inference:  65%|██████▍   | 324/500 [20:15<11:14,  3.83s/it]Running Inference:  65%|██████▌   | 325/500 [20:19<11:07,  3.82s/it]Running Inference:  65%|██████▌   | 326/500 [20:23<11:22,  3.92s/it]Running Inference:  65%|██████▌   | 327/500 [20:28<12:30,  4.34s/it]Running Inference:  66%|██████▌   | 328/500 [20:33<12:21,  4.31s/it]Running Inference:  66%|██████▌   | 329/500 [20:36<11:45,  4.13s/it]Running Inference:  66%|██████▌   | 330/500 [20:40<11:19,  4.00s/it]Running Inference:  66%|██████▌   | 331/500 [20:44<11:38,  4.14s/it]Running Inference:  66%|██████▋   | 332/500 [20:48<11:27,  4.09s/it]Running Inference:  67%|██████▋   | 333/500 [20:52<11:06,  3.99s/it]Running Inference:  67%|██████▋   | 334/500 [20:56<10:51,  3.92s/it]Running Inference:  67%|██████▋   | 335/500 [21:00<10:39,  3.87s/it]Running Inference:  67%|██████▋   | 336/500 [21:03<10:27,  3.83s/it]Running Inference:  67%|██████▋   | 337/500 [21:07<10:20,  3.80s/it]Running Inference:  68%|██████▊   | 338/500 [21:11<10:15,  3.80s/it]Running Inference:  68%|██████▊   | 339/500 [21:15<10:29,  3.91s/it]Running Inference:  68%|██████▊   | 340/500 [21:21<11:58,  4.49s/it]Running Inference:  68%|██████▊   | 341/500 [21:25<11:20,  4.28s/it]Running Inference:  68%|██████▊   | 342/500 [21:29<10:53,  4.14s/it]Running Inference:  69%|██████▊   | 343/500 [21:33<10:48,  4.13s/it]Running Inference:  69%|██████▉   | 344/500 [21:37<10:40,  4.11s/it]Running Inference:  69%|██████▉   | 345/500 [21:41<10:24,  4.03s/it]Running Inference:  69%|██████▉   | 346/500 [21:45<10:17,  4.01s/it]Running Inference:  69%|██████▉   | 347/500 [21:48<10:06,  3.96s/it]Running Inference:  70%|██████▉   | 348/500 [21:52<09:56,  3.93s/it]Running Inference:  70%|██████▉   | 349/500 [21:56<09:47,  3.89s/it]Running Inference:  70%|███████   | 350/500 [22:02<11:19,  4.53s/it]Running Inference:  70%|███████   | 351/500 [22:06<11:02,  4.45s/it]Running Inference:  70%|███████   | 352/500 [22:10<10:25,  4.22s/it]Running Inference:  71%|███████   | 353/500 [22:14<09:58,  4.07s/it]Running Inference:  71%|███████   | 354/500 [22:18<09:47,  4.02s/it]Running Inference:  71%|███████   | 355/500 [22:24<11:38,  4.82s/it]Running Inference:  71%|███████   | 356/500 [22:28<10:48,  4.50s/it]Running Inference:  71%|███████▏  | 357/500 [22:32<10:29,  4.40s/it]Running Inference:  72%|███████▏  | 358/500 [22:36<09:57,  4.21s/it]Running Inference:  72%|███████▏  | 359/500 [22:40<09:32,  4.06s/it]Running Inference:  72%|███████▏  | 360/500 [22:43<09:15,  3.97s/it]Running Inference:  72%|███████▏  | 361/500 [22:47<09:07,  3.94s/it]Running Inference:  72%|███████▏  | 362/500 [22:51<09:04,  3.94s/it]Running Inference:  73%|███████▎  | 363/500 [22:55<08:58,  3.93s/it]Running Inference:  73%|███████▎  | 364/500 [22:59<08:54,  3.93s/it]Running Inference:  73%|███████▎  | 365/500 [23:04<09:32,  4.24s/it]Running Inference:  73%|███████▎  | 366/500 [23:08<09:29,  4.25s/it]Running Inference:  73%|███████▎  | 367/500 [23:12<09:09,  4.13s/it]Running Inference:  74%|███████▎  | 368/500 [23:16<08:58,  4.08s/it]Running Inference:  74%|███████▍  | 369/500 [23:20<08:58,  4.11s/it]Running Inference:  74%|███████▍  | 370/500 [23:25<09:04,  4.19s/it]Running Inference:  74%|███████▍  | 371/500 [23:29<08:44,  4.07s/it]Running Inference:  74%|███████▍  | 372/500 [23:32<08:35,  4.03s/it]Running Inference:  75%|███████▍  | 373/500 [23:36<08:23,  3.96s/it]Running Inference:  75%|███████▍  | 374/500 [23:40<08:16,  3.94s/it]Running Inference:  75%|███████▌  | 375/500 [23:44<08:04,  3.88s/it]Running Inference:  75%|███████▌  | 376/500 [23:48<07:59,  3.86s/it]Running Inference:  75%|███████▌  | 377/500 [23:51<07:51,  3.84s/it]Running Inference:  76%|███████▌  | 378/500 [23:55<07:48,  3.84s/it]Running Inference:  76%|███████▌  | 379/500 [23:59<07:50,  3.89s/it]Running Inference:  76%|███████▌  | 380/500 [24:03<07:42,  3.85s/it]Running Inference:  76%|███████▌  | 381/500 [24:07<07:35,  3.83s/it]Running Inference:  76%|███████▋  | 382/500 [24:11<07:43,  3.93s/it]Running Inference:  77%|███████▋  | 383/500 [24:15<07:40,  3.94s/it]Running Inference:  77%|███████▋  | 384/500 [24:16<06:01,  3.12s/it]Running Inference:  77%|███████▋  | 385/500 [24:20<06:29,  3.39s/it]Running Inference:  77%|███████▋  | 386/500 [24:24<06:45,  3.56s/it]Running Inference:  77%|███████▋  | 387/500 [24:28<06:53,  3.66s/it]Running Inference:  78%|███████▊  | 388/500 [24:32<06:53,  3.69s/it]Running Inference:  78%|███████▊  | 389/500 [24:36<06:55,  3.74s/it]Running Inference:  78%|███████▊  | 390/500 [24:39<06:40,  3.64s/it]Running Inference:  78%|███████▊  | 391/500 [24:43<06:41,  3.68s/it]Running Inference:  78%|███████▊  | 392/500 [24:47<06:39,  3.70s/it]Running Inference:  79%|███████▊  | 393/500 [24:50<06:35,  3.70s/it]Running Inference:  79%|███████▉  | 394/500 [24:54<06:36,  3.74s/it]Running Inference:  79%|███████▉  | 395/500 [24:58<06:35,  3.77s/it]Running Inference:  79%|███████▉  | 396/500 [25:02<06:29,  3.75s/it]Running Inference:  79%|███████▉  | 397/500 [25:06<06:36,  3.85s/it]Running Inference:  80%|███████▉  | 398/500 [25:09<06:28,  3.81s/it]Running Inference:  80%|███████▉  | 399/500 [25:13<06:21,  3.78s/it]Running Inference:  80%|████████  | 400/500 [25:17<06:15,  3.76s/it]Running Inference:  80%|████████  | 401/500 [25:21<06:10,  3.75s/it]Running Inference:  80%|████████  | 402/500 [25:24<06:05,  3.73s/it]Running Inference:  81%|████████  | 403/500 [25:28<06:02,  3.73s/it]Running Inference:  81%|████████  | 404/500 [25:32<05:57,  3.73s/it]Running Inference:  81%|████████  | 405/500 [25:36<05:56,  3.75s/it]Running Inference:  81%|████████  | 406/500 [25:39<05:52,  3.75s/it]Running Inference:  81%|████████▏ | 407/500 [25:43<05:48,  3.75s/it]Running Inference:  82%|████████▏ | 408/500 [25:47<05:44,  3.75s/it]Running Inference:  82%|████████▏ | 409/500 [25:51<05:41,  3.75s/it]Running Inference:  82%|████████▏ | 410/500 [25:54<05:39,  3.77s/it]Running Inference:  82%|████████▏ | 411/500 [25:58<05:37,  3.79s/it]Running Inference:  82%|████████▏ | 412/500 [26:02<05:33,  3.79s/it]Running Inference:  83%|████████▎ | 413/500 [26:06<05:27,  3.76s/it]Running Inference:  83%|████████▎ | 414/500 [26:09<05:24,  3.77s/it]Running Inference:  83%|████████▎ | 415/500 [26:13<05:19,  3.76s/it]Running Inference:  83%|████████▎ | 416/500 [26:17<05:13,  3.74s/it]Running Inference:  83%|████████▎ | 417/500 [26:21<05:09,  3.73s/it]Running Inference:  84%|████████▎ | 418/500 [26:24<05:07,  3.75s/it]Running Inference:  84%|████████▍ | 419/500 [26:28<05:03,  3.74s/it]Running Inference:  84%|████████▍ | 420/500 [26:32<04:58,  3.73s/it]Running Inference:  84%|████████▍ | 421/500 [26:36<04:55,  3.74s/it]Running Inference:  84%|████████▍ | 422/500 [26:39<04:50,  3.73s/it]Running Inference:  85%|████████▍ | 423/500 [26:43<04:46,  3.72s/it]Running Inference:  85%|████████▍ | 424/500 [26:47<04:43,  3.73s/it]Running Inference:  85%|████████▌ | 425/500 [26:51<04:46,  3.82s/it]Running Inference:  85%|████████▌ | 426/500 [26:55<04:41,  3.80s/it]Running Inference:  85%|████████▌ | 427/500 [26:58<04:34,  3.76s/it]Running Inference:  86%|████████▌ | 428/500 [27:01<04:15,  3.55s/it]Running Inference:  86%|████████▌ | 429/500 [27:05<04:15,  3.60s/it]Running Inference:  86%|████████▌ | 430/500 [27:09<04:23,  3.76s/it]Running Inference:  86%|████████▌ | 431/500 [27:13<04:20,  3.78s/it]Running Inference:  86%|████████▋ | 432/500 [27:17<04:22,  3.86s/it]Running Inference:  87%|████████▋ | 433/500 [27:21<04:21,  3.90s/it]Running Inference:  87%|████████▋ | 434/500 [27:25<04:18,  3.92s/it]Running Inference:  87%|████████▋ | 435/500 [27:29<04:11,  3.87s/it]Running Inference:  87%|████████▋ | 436/500 [27:33<04:06,  3.86s/it]Running Inference:  87%|████████▋ | 437/500 [27:36<04:01,  3.83s/it]Running Inference:  88%|████████▊ | 438/500 [27:40<03:55,  3.80s/it]Running Inference:  88%|████████▊ | 439/500 [27:44<03:52,  3.81s/it]Running Inference:  88%|████████▊ | 440/500 [27:48<03:48,  3.80s/it]Running Inference:  88%|████████▊ | 441/500 [27:51<03:42,  3.77s/it]Running Inference:  88%|████████▊ | 442/500 [27:55<03:37,  3.74s/it]Running Inference:  89%|████████▊ | 443/500 [27:59<03:33,  3.74s/it]Running Inference:  89%|████████▉ | 444/500 [28:03<03:31,  3.78s/it]Running Inference:  89%|████████▉ | 445/500 [28:06<03:29,  3.81s/it]Running Inference:  89%|████████▉ | 446/500 [28:10<03:27,  3.83s/it]Running Inference:  89%|████████▉ | 447/500 [28:14<03:21,  3.81s/it]Running Inference:  90%|████████▉ | 448/500 [28:18<03:18,  3.82s/it]Running Inference:  90%|████████▉ | 449/500 [28:22<03:14,  3.80s/it]Running Inference:  90%|█████████ | 450/500 [28:26<03:10,  3.81s/it]Running Inference:  90%|█████████ | 451/500 [28:29<03:05,  3.79s/it]Running Inference:  90%|█████████ | 452/500 [28:33<03:01,  3.79s/it]Running Inference:  91%|█████████ | 453/500 [28:37<02:59,  3.82s/it]Running Inference:  91%|█████████ | 454/500 [28:41<02:54,  3.79s/it]Running Inference:  91%|█████████ | 455/500 [28:44<02:49,  3.76s/it]Running Inference:  91%|█████████ | 456/500 [28:48<02:44,  3.75s/it]Running Inference:  91%|█████████▏| 457/500 [28:52<02:41,  3.76s/it]Running Inference:  92%|█████████▏| 458/500 [28:56<02:37,  3.76s/it]Running Inference:  92%|█████████▏| 459/500 [28:57<02:04,  3.04s/it]Running Inference:  92%|█████████▏| 460/500 [29:01<02:14,  3.36s/it]Running Inference:  92%|█████████▏| 461/500 [29:05<02:16,  3.51s/it]Running Inference:  92%|█████████▏| 462/500 [29:09<02:20,  3.71s/it]Running Inference:  93%|█████████▎| 463/500 [29:13<02:17,  3.71s/it]Running Inference:  93%|█████████▎| 464/500 [29:17<02:13,  3.71s/it]Running Inference:  93%|█████████▎| 465/500 [29:20<02:09,  3.70s/it]Running Inference:  93%|█████████▎| 466/500 [29:24<02:06,  3.73s/it]Running Inference:  93%|█████████▎| 467/500 [29:28<02:02,  3.72s/it]Running Inference:  94%|█████████▎| 468/500 [29:32<02:01,  3.79s/it]Running Inference:  94%|█████████▍| 469/500 [29:35<01:56,  3.77s/it]Running Inference:  94%|█████████▍| 470/500 [29:39<01:52,  3.75s/it]Running Inference:  94%|█████████▍| 471/500 [29:43<01:49,  3.79s/it]Running Inference:  94%|█████████▍| 472/500 [29:47<01:45,  3.77s/it]Running Inference:  95%|█████████▍| 473/500 [29:51<01:44,  3.86s/it]Running Inference:  95%|█████████▍| 474/500 [29:55<01:39,  3.83s/it]Running Inference:  95%|█████████▌| 475/500 [29:58<01:35,  3.81s/it]Running Inference:  95%|█████████▌| 476/500 [30:02<01:31,  3.80s/it]Running Inference:  95%|█████████▌| 477/500 [30:06<01:27,  3.80s/it]Running Inference:  96%|█████████▌| 478/500 [30:08<01:09,  3.18s/it]Running Inference:  96%|█████████▌| 479/500 [30:13<01:17,  3.68s/it]Running Inference:  96%|█████████▌| 480/500 [30:16<01:14,  3.74s/it]Running Inference:  96%|█████████▌| 481/500 [30:21<01:15,  3.98s/it]Running Inference:  96%|█████████▋| 482/500 [30:25<01:10,  3.91s/it]Running Inference:  97%|█████████▋| 483/500 [30:28<01:05,  3.86s/it]Running Inference:  97%|█████████▋| 484/500 [30:32<01:02,  3.91s/it]Running Inference:  97%|█████████▋| 485/500 [30:38<01:04,  4.29s/it]Running Inference:  97%|█████████▋| 486/500 [30:41<00:57,  4.12s/it]Running Inference:  97%|█████████▋| 487/500 [30:45<00:52,  4.01s/it]Running Inference:  98%|█████████▊| 488/500 [30:49<00:46,  3.91s/it]Running Inference:  98%|█████████▊| 489/500 [30:53<00:42,  3.90s/it]Running Inference:  98%|█████████▊| 490/500 [30:53<00:29,  2.94s/it]Running Inference:  98%|█████████▊| 491/500 [30:57<00:28,  3.19s/it]Running Inference:  98%|█████████▊| 492/500 [31:01<00:27,  3.40s/it]Running Inference:  99%|█████████▊| 493/500 [31:05<00:24,  3.49s/it]Running Inference:  99%|█████████▉| 494/500 [31:09<00:21,  3.61s/it]Running Inference:  99%|█████████▉| 495/500 [31:12<00:18,  3.67s/it]Running Inference:  99%|█████████▉| 496/500 [31:16<00:14,  3.73s/it]Running Inference:  99%|█████████▉| 497/500 [31:20<00:11,  3.74s/it]Running Inference: 100%|█████████▉| 498/500 [31:24<00:07,  3.76s/it]Running Inference: 100%|█████████▉| 499/500 [31:28<00:03,  3.75s/it]Running Inference: 100%|██████████| 500/500 [31:32<00:00,  3.85s/it]Running Inference: 100%|██████████| 500/500 [31:32<00:00,  3.78s/it]
2025-12-14 19:08:19,581 - INFO - Inference completed.
2025-12-14 19:08:19,596 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 19:08:19,596 - INFO - Calculating metrics for dataset: longbench
2025-12-14 19:08:19,683 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 19:08:19,683 - INFO - Metrics:
35.75
2025-12-14 19:08:19,684 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc, ratio=0.1) on GPU 1

----------------------------------------
Task: lcc | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 19:08:26,059 - INFO - Set deterministic seeds to 42
2025-12-14 19:08:26,059 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 19:08:26,059 - INFO - Starting evaluation run...
2025-12-14 19:08:26,059 - INFO - Output directory set to: longbenchresult
2025-12-14 19:08:26,059 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 19:08:26,059 - INFO - KV Press 'tova' setup.
2025-12-14 19:08:26,059 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 19:08:26,059 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.99it/s]
Device set to use cuda:0
2025-12-14 19:08:40,702 - INFO - Model pipeline loaded.
2025-12-14 19:08:40,702 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 19:08:54,824 - INFO - Dataset loaded with 500 entries.
2025-12-14 19:08:54,825 - INFO - Dataset processed with 500 entries.
2025-12-14 19:08:54,839 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<41:51,  5.03s/it]Running Inference:   0%|          | 2/500 [00:08<36:18,  4.37s/it]Running Inference:   1%|          | 3/500 [00:13<36:14,  4.37s/it]Running Inference:   1%|          | 4/500 [00:17<34:35,  4.19s/it]Running Inference:   1%|          | 5/500 [00:21<33:41,  4.08s/it]Running Inference:   1%|          | 6/500 [00:25<33:40,  4.09s/it]Running Inference:   1%|▏         | 7/500 [00:29<33:16,  4.05s/it]Running Inference:   2%|▏         | 8/500 [00:33<32:42,  3.99s/it]Running Inference:   2%|▏         | 9/500 [00:36<32:17,  3.95s/it]Running Inference:   2%|▏         | 10/500 [00:40<32:00,  3.92s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:45<32:58,  4.05s/it]Running Inference:   2%|▏         | 12/500 [00:48<32:33,  4.00s/it]Running Inference:   3%|▎         | 13/500 [00:52<32:11,  3.97s/it]Running Inference:   3%|▎         | 14/500 [00:56<31:55,  3.94s/it]Running Inference:   3%|▎         | 15/500 [01:00<31:42,  3.92s/it]Running Inference:   3%|▎         | 16/500 [01:04<31:29,  3.90s/it]Running Inference:   3%|▎         | 17/500 [01:09<33:06,  4.11s/it]Running Inference:   4%|▎         | 18/500 [01:13<32:42,  4.07s/it]Running Inference:   4%|▍         | 19/500 [01:16<32:14,  4.02s/it]Running Inference:   4%|▍         | 20/500 [01:20<31:49,  3.98s/it]Running Inference:   4%|▍         | 21/500 [01:24<31:31,  3.95s/it]Running Inference:   4%|▍         | 22/500 [01:28<31:15,  3.92s/it]Running Inference:   5%|▍         | 23/500 [01:32<31:01,  3.90s/it]Running Inference:   5%|▍         | 24/500 [01:36<30:50,  3.89s/it]Running Inference:   5%|▌         | 25/500 [01:40<30:48,  3.89s/it]Running Inference:   5%|▌         | 26/500 [01:44<30:52,  3.91s/it]Running Inference:   5%|▌         | 27/500 [01:48<30:46,  3.90s/it]Running Inference:   6%|▌         | 28/500 [01:53<33:48,  4.30s/it]Running Inference:   6%|▌         | 29/500 [01:57<33:19,  4.25s/it]Running Inference:   6%|▌         | 30/500 [02:01<33:52,  4.33s/it]Running Inference:   6%|▌         | 31/500 [02:05<33:01,  4.23s/it]Running Inference:   6%|▋         | 32/500 [02:09<32:19,  4.14s/it]Running Inference:   7%|▋         | 33/500 [02:13<31:36,  4.06s/it]Running Inference:   7%|▋         | 34/500 [02:17<31:06,  4.00s/it]Running Inference:   7%|▋         | 35/500 [02:21<30:41,  3.96s/it]Running Inference:   7%|▋         | 36/500 [02:25<31:25,  4.06s/it]Running Inference:   7%|▋         | 37/500 [02:30<31:55,  4.14s/it]Running Inference:   8%|▊         | 38/500 [02:33<31:18,  4.07s/it]Running Inference:   8%|▊         | 39/500 [02:38<31:47,  4.14s/it]Running Inference:   8%|▊         | 40/500 [02:42<31:06,  4.06s/it]Running Inference:   8%|▊         | 41/500 [02:46<30:45,  4.02s/it]Running Inference:   8%|▊         | 42/500 [02:50<32:20,  4.24s/it]Running Inference:   9%|▊         | 43/500 [02:55<33:12,  4.36s/it]Running Inference:   9%|▉         | 44/500 [02:59<32:14,  4.24s/it]Running Inference:   9%|▉         | 45/500 [03:03<31:34,  4.16s/it]Running Inference:   9%|▉         | 46/500 [03:07<30:45,  4.06s/it]Running Inference:   9%|▉         | 47/500 [03:11<30:31,  4.04s/it]Running Inference:  10%|▉         | 48/500 [03:15<29:58,  3.98s/it]Running Inference:  10%|▉         | 49/500 [03:19<30:03,  4.00s/it]Running Inference:  10%|█         | 50/500 [03:23<31:24,  4.19s/it]Running Inference:  10%|█         | 51/500 [03:27<30:38,  4.10s/it]Running Inference:  10%|█         | 52/500 [03:31<30:08,  4.04s/it]Running Inference:  11%|█         | 53/500 [03:35<29:55,  4.02s/it]Running Inference:  11%|█         | 54/500 [03:39<29:36,  3.98s/it]Running Inference:  11%|█         | 55/500 [03:43<29:15,  3.95s/it]Running Inference:  11%|█         | 56/500 [03:47<29:04,  3.93s/it]Running Inference:  11%|█▏        | 57/500 [03:55<39:26,  5.34s/it]Running Inference:  12%|█▏        | 58/500 [03:59<36:01,  4.89s/it]Running Inference:  12%|█▏        | 59/500 [04:01<29:39,  4.04s/it]Running Inference:  12%|█▏        | 60/500 [04:05<29:14,  3.99s/it]Running Inference:  12%|█▏        | 61/500 [04:09<29:00,  3.97s/it]Running Inference:  12%|█▏        | 62/500 [04:12<26:43,  3.66s/it]Running Inference:  13%|█▎        | 63/500 [04:16<27:09,  3.73s/it]Running Inference:  13%|█▎        | 64/500 [04:20<27:21,  3.77s/it]Running Inference:  13%|█▎        | 65/500 [04:24<27:33,  3.80s/it]Running Inference:  13%|█▎        | 66/500 [04:28<28:18,  3.91s/it]Running Inference:  13%|█▎        | 67/500 [04:32<28:18,  3.92s/it]Running Inference:  14%|█▎        | 68/500 [04:36<28:12,  3.92s/it]Running Inference:  14%|█▍        | 69/500 [04:39<28:02,  3.90s/it]Running Inference:  14%|█▍        | 70/500 [04:43<27:53,  3.89s/it]Running Inference:  14%|█▍        | 71/500 [04:46<26:16,  3.67s/it]Running Inference:  14%|█▍        | 72/500 [04:51<27:10,  3.81s/it]Running Inference:  15%|█▍        | 73/500 [04:54<27:13,  3.82s/it]Running Inference:  15%|█▍        | 74/500 [04:58<27:23,  3.86s/it]Running Inference:  15%|█▌        | 75/500 [05:03<27:57,  3.95s/it]Running Inference:  15%|█▌        | 76/500 [05:07<28:05,  3.98s/it]Running Inference:  15%|█▌        | 77/500 [05:11<27:57,  3.97s/it]Running Inference:  16%|█▌        | 78/500 [05:15<28:39,  4.08s/it]Running Inference:  16%|█▌        | 79/500 [05:17<23:31,  3.35s/it]Running Inference:  16%|█▌        | 80/500 [05:20<24:37,  3.52s/it]Running Inference:  16%|█▌        | 81/500 [05:25<26:14,  3.76s/it]Running Inference:  16%|█▋        | 82/500 [05:29<27:32,  3.95s/it]Running Inference:  17%|█▋        | 83/500 [05:31<23:48,  3.43s/it]Running Inference:  17%|█▋        | 84/500 [05:35<24:50,  3.58s/it]Running Inference:  17%|█▋        | 85/500 [05:40<27:32,  3.98s/it]Running Inference:  17%|█▋        | 86/500 [05:44<27:13,  3.95s/it]Running Inference:  17%|█▋        | 87/500 [05:48<26:59,  3.92s/it]Running Inference:  18%|█▊        | 88/500 [05:52<26:50,  3.91s/it]Running Inference:  18%|█▊        | 89/500 [05:56<26:54,  3.93s/it]Running Inference:  18%|█▊        | 90/500 [06:00<27:25,  4.01s/it]Running Inference:  18%|█▊        | 91/500 [06:04<27:35,  4.05s/it]Running Inference:  18%|█▊        | 92/500 [06:08<27:22,  4.03s/it]Running Inference:  19%|█▊        | 93/500 [06:12<27:05,  3.99s/it]Running Inference:  19%|█▉        | 94/500 [06:16<26:46,  3.96s/it]Running Inference:  19%|█▉        | 95/500 [06:20<26:37,  3.94s/it]Running Inference:  19%|█▉        | 96/500 [06:24<26:36,  3.95s/it]Running Inference:  19%|█▉        | 97/500 [06:28<26:25,  3.93s/it]Running Inference:  20%|█▉        | 98/500 [06:32<26:11,  3.91s/it]Running Inference:  20%|█▉        | 99/500 [06:35<26:01,  3.89s/it]Running Inference:  20%|██        | 100/500 [06:40<26:39,  4.00s/it]Running Inference:  20%|██        | 101/500 [06:44<26:43,  4.02s/it]Running Inference:  20%|██        | 102/500 [06:48<26:21,  3.97s/it]Running Inference:  21%|██        | 103/500 [06:51<26:03,  3.94s/it]Running Inference:  21%|██        | 104/500 [06:57<29:50,  4.52s/it]Running Inference:  21%|██        | 105/500 [07:02<30:04,  4.57s/it]Running Inference:  21%|██        | 106/500 [07:07<31:43,  4.83s/it]Running Inference:  21%|██▏       | 107/500 [07:11<30:10,  4.61s/it]Running Inference:  22%|██▏       | 108/500 [07:15<28:42,  4.39s/it]Running Inference:  22%|██▏       | 109/500 [07:19<27:46,  4.26s/it]Running Inference:  22%|██▏       | 110/500 [07:23<27:10,  4.18s/it]Running Inference:  22%|██▏       | 111/500 [07:27<26:29,  4.09s/it]Running Inference:  22%|██▏       | 112/500 [07:31<26:02,  4.03s/it]Running Inference:  23%|██▎       | 113/500 [07:35<26:23,  4.09s/it]Running Inference:  23%|██▎       | 114/500 [07:41<28:34,  4.44s/it]Running Inference:  23%|██▎       | 115/500 [07:45<27:58,  4.36s/it]Running Inference:  23%|██▎       | 116/500 [07:49<27:37,  4.32s/it]Running Inference:  23%|██▎       | 117/500 [07:53<26:38,  4.17s/it]Running Inference:  24%|██▎       | 118/500 [07:57<27:13,  4.28s/it]Running Inference:  24%|██▍       | 119/500 [08:01<26:31,  4.18s/it]Running Inference:  24%|██▍       | 120/500 [08:06<26:34,  4.19s/it]Running Inference:  24%|██▍       | 121/500 [08:09<26:04,  4.13s/it]Running Inference:  24%|██▍       | 122/500 [08:12<22:22,  3.55s/it]Running Inference:  25%|██▍       | 123/500 [08:16<22:52,  3.64s/it]Running Inference:  25%|██▍       | 124/500 [08:19<23:12,  3.70s/it]Running Inference:  25%|██▌       | 125/500 [08:24<24:06,  3.86s/it]Running Inference:  25%|██▌       | 126/500 [08:27<24:05,  3.87s/it]Running Inference:  25%|██▌       | 127/500 [08:29<19:00,  3.06s/it]Running Inference:  26%|██▌       | 128/500 [08:33<20:52,  3.37s/it]Running Inference:  26%|██▌       | 129/500 [08:37<21:44,  3.52s/it]Running Inference:  26%|██▌       | 130/500 [08:41<22:31,  3.65s/it]Running Inference:  26%|██▌       | 131/500 [08:44<22:49,  3.71s/it]Running Inference:  26%|██▋       | 132/500 [08:48<22:59,  3.75s/it]Running Inference:  27%|██▋       | 133/500 [08:52<23:14,  3.80s/it]Running Inference:  27%|██▋       | 134/500 [08:56<23:20,  3.83s/it]Running Inference:  27%|██▋       | 135/500 [09:00<23:33,  3.87s/it]Running Inference:  27%|██▋       | 136/500 [09:04<23:37,  3.89s/it]Running Inference:  27%|██▋       | 137/500 [09:05<18:56,  3.13s/it]Running Inference:  28%|██▊       | 138/500 [09:09<20:25,  3.39s/it]Running Inference:  28%|██▊       | 139/500 [09:13<21:07,  3.51s/it]Running Inference:  28%|██▊       | 140/500 [09:17<21:52,  3.65s/it]Running Inference:  28%|██▊       | 141/500 [09:21<22:59,  3.84s/it]Running Inference:  28%|██▊       | 142/500 [09:25<23:23,  3.92s/it]Running Inference:  29%|██▊       | 143/500 [09:29<23:13,  3.90s/it]Running Inference:  29%|██▉       | 144/500 [09:33<23:01,  3.88s/it]Running Inference:  29%|██▉       | 145/500 [09:37<23:11,  3.92s/it]Running Inference:  29%|██▉       | 146/500 [09:41<23:23,  3.97s/it]Running Inference:  29%|██▉       | 147/500 [09:45<23:36,  4.01s/it]Running Inference:  30%|██▉       | 148/500 [09:49<23:10,  3.95s/it]Running Inference:  30%|██▉       | 149/500 [09:53<22:54,  3.92s/it]Running Inference:  30%|███       | 150/500 [09:57<22:37,  3.88s/it]Running Inference:  30%|███       | 151/500 [10:01<22:38,  3.89s/it]Running Inference:  30%|███       | 152/500 [10:01<16:51,  2.91s/it]Running Inference:  31%|███       | 153/500 [10:05<18:26,  3.19s/it]Running Inference:  31%|███       | 154/500 [10:09<19:30,  3.38s/it]Running Inference:  31%|███       | 155/500 [10:13<20:09,  3.51s/it]Running Inference:  31%|███       | 156/500 [10:17<20:41,  3.61s/it]Running Inference:  31%|███▏      | 157/500 [10:21<21:22,  3.74s/it]Running Inference:  32%|███▏      | 158/500 [10:25<21:32,  3.78s/it]Running Inference:  32%|███▏      | 159/500 [10:28<21:33,  3.79s/it]Running Inference:  32%|███▏      | 160/500 [10:32<21:33,  3.80s/it]Running Inference:  32%|███▏      | 161/500 [10:36<21:36,  3.82s/it]Running Inference:  32%|███▏      | 162/500 [10:40<21:47,  3.87s/it]Running Inference:  33%|███▎      | 163/500 [10:44<21:36,  3.85s/it]Running Inference:  33%|███▎      | 164/500 [10:48<21:33,  3.85s/it]Running Inference:  33%|███▎      | 165/500 [10:52<21:35,  3.87s/it]Running Inference:  33%|███▎      | 166/500 [10:55<21:24,  3.84s/it]Running Inference:  33%|███▎      | 167/500 [11:00<21:57,  3.96s/it]Running Inference:  34%|███▎      | 168/500 [11:00<16:31,  2.99s/it]Running Inference:  34%|███▍      | 169/500 [11:04<18:02,  3.27s/it]Running Inference:  34%|███▍      | 170/500 [11:07<17:16,  3.14s/it]Running Inference:  34%|███▍      | 171/500 [11:11<18:35,  3.39s/it]Running Inference:  34%|███▍      | 172/500 [11:15<19:16,  3.53s/it]Running Inference:  35%|███▍      | 173/500 [11:19<19:46,  3.63s/it]Running Inference:  35%|███▍      | 174/500 [11:23<20:32,  3.78s/it]Running Inference:  35%|███▌      | 175/500 [11:27<21:34,  3.98s/it]Running Inference:  35%|███▌      | 176/500 [11:31<21:19,  3.95s/it]Running Inference:  35%|███▌      | 177/500 [11:36<22:12,  4.13s/it]Running Inference:  36%|███▌      | 178/500 [11:40<21:34,  4.02s/it]Running Inference:  36%|███▌      | 179/500 [11:43<21:12,  3.96s/it]Running Inference:  36%|███▌      | 180/500 [11:47<21:04,  3.95s/it]Running Inference:  36%|███▌      | 181/500 [11:51<20:47,  3.91s/it]Running Inference:  36%|███▋      | 182/500 [11:55<20:33,  3.88s/it]Running Inference:  37%|███▋      | 183/500 [11:59<20:44,  3.93s/it]Running Inference:  37%|███▋      | 184/500 [12:03<20:42,  3.93s/it]Running Inference:  37%|███▋      | 185/500 [12:04<16:50,  3.21s/it]Running Inference:  37%|███▋      | 186/500 [12:08<17:50,  3.41s/it]Running Inference:  37%|███▋      | 187/500 [12:12<18:39,  3.58s/it]Running Inference:  38%|███▊      | 188/500 [12:16<19:00,  3.65s/it]Running Inference:  38%|███▊      | 189/500 [12:20<19:21,  3.74s/it]Running Inference:  38%|███▊      | 190/500 [12:24<20:03,  3.88s/it]Running Inference:  38%|███▊      | 191/500 [12:28<19:52,  3.86s/it]Running Inference:  38%|███▊      | 192/500 [12:32<19:54,  3.88s/it]Running Inference:  39%|███▊      | 193/500 [12:36<19:50,  3.88s/it]Running Inference:  39%|███▉      | 194/500 [12:40<19:46,  3.88s/it]Running Inference:  39%|███▉      | 195/500 [12:44<19:42,  3.88s/it]Running Inference:  39%|███▉      | 196/500 [12:48<19:38,  3.88s/it]Running Inference:  39%|███▉      | 197/500 [12:52<19:45,  3.91s/it]Running Inference:  40%|███▉      | 198/500 [12:55<19:33,  3.89s/it]Running Inference:  40%|███▉      | 199/500 [12:59<19:22,  3.86s/it]Running Inference:  40%|████      | 200/500 [13:03<19:26,  3.89s/it]Running Inference:  40%|████      | 201/500 [13:07<19:35,  3.93s/it]Running Inference:  40%|████      | 202/500 [13:11<19:23,  3.90s/it]Running Inference:  41%|████      | 203/500 [13:16<21:24,  4.33s/it]Running Inference:  41%|████      | 204/500 [13:20<20:42,  4.20s/it]Running Inference:  41%|████      | 205/500 [13:24<20:06,  4.09s/it]Running Inference:  41%|████      | 206/500 [13:28<19:36,  4.00s/it]Running Inference:  41%|████▏     | 207/500 [13:32<19:19,  3.96s/it]Running Inference:  42%|████▏     | 208/500 [13:36<19:39,  4.04s/it]Running Inference:  42%|████▏     | 209/500 [13:40<19:55,  4.11s/it]Running Inference:  42%|████▏     | 210/500 [13:44<19:27,  4.02s/it]Running Inference:  42%|████▏     | 211/500 [13:48<19:25,  4.03s/it]Running Inference:  42%|████▏     | 212/500 [13:52<19:01,  3.96s/it]Running Inference:  43%|████▎     | 213/500 [13:54<15:42,  3.28s/it]Running Inference:  43%|████▎     | 214/500 [13:57<16:28,  3.46s/it]Running Inference:  43%|████▎     | 215/500 [14:01<17:04,  3.59s/it]Running Inference:  43%|████▎     | 216/500 [14:05<17:21,  3.67s/it]Running Inference:  43%|████▎     | 217/500 [14:09<17:56,  3.81s/it]Running Inference:  44%|████▎     | 218/500 [14:13<18:14,  3.88s/it]Running Inference:  44%|████▍     | 219/500 [14:17<18:03,  3.86s/it]Running Inference:  44%|████▍     | 220/500 [14:21<17:59,  3.86s/it]Running Inference:  44%|████▍     | 221/500 [14:24<16:02,  3.45s/it]Running Inference:  44%|████▍     | 222/500 [14:25<12:34,  2.72s/it]Running Inference:  45%|████▍     | 223/500 [14:28<14:06,  3.06s/it]Running Inference:  45%|████▍     | 224/500 [14:32<15:09,  3.29s/it]Running Inference:  45%|████▌     | 225/500 [14:35<13:54,  3.03s/it]Running Inference:  45%|████▌     | 226/500 [14:38<14:54,  3.26s/it]Running Inference:  45%|████▌     | 227/500 [14:42<15:37,  3.43s/it]Running Inference:  46%|████▌     | 228/500 [14:46<15:59,  3.53s/it]Running Inference:  46%|████▌     | 229/500 [14:50<16:16,  3.60s/it]Running Inference:  46%|████▌     | 230/500 [14:54<16:30,  3.67s/it]Running Inference:  46%|████▌     | 231/500 [14:57<16:40,  3.72s/it]Running Inference:  46%|████▋     | 232/500 [15:01<16:46,  3.75s/it]Running Inference:  47%|████▋     | 233/500 [15:05<16:57,  3.81s/it]Running Inference:  47%|████▋     | 234/500 [15:09<16:59,  3.83s/it]Running Inference:  47%|████▋     | 235/500 [15:13<17:24,  3.94s/it]Running Inference:  47%|████▋     | 236/500 [15:17<17:07,  3.89s/it]Running Inference:  47%|████▋     | 237/500 [15:21<16:57,  3.87s/it]Running Inference:  48%|████▊     | 238/500 [15:25<16:50,  3.86s/it]Running Inference:  48%|████▊     | 239/500 [15:29<16:52,  3.88s/it]Running Inference:  48%|████▊     | 240/500 [15:33<16:47,  3.87s/it]Running Inference:  48%|████▊     | 241/500 [15:36<16:42,  3.87s/it]Running Inference:  48%|████▊     | 242/500 [15:40<16:32,  3.85s/it]Running Inference:  49%|████▊     | 243/500 [15:45<17:20,  4.05s/it]Running Inference:  49%|████▉     | 244/500 [15:49<17:36,  4.13s/it]Running Inference:  49%|████▉     | 245/500 [15:53<17:18,  4.07s/it]Running Inference:  49%|████▉     | 246/500 [15:57<16:51,  3.98s/it]Running Inference:  49%|████▉     | 247/500 [16:01<16:44,  3.97s/it]Running Inference:  50%|████▉     | 248/500 [16:05<16:30,  3.93s/it]Running Inference:  50%|████▉     | 249/500 [16:08<16:23,  3.92s/it]Running Inference:  50%|█████     | 250/500 [16:09<12:26,  2.99s/it]Running Inference:  50%|█████     | 251/500 [16:13<13:51,  3.34s/it]Running Inference:  50%|█████     | 252/500 [16:18<15:20,  3.71s/it]Running Inference:  51%|█████     | 253/500 [16:22<15:26,  3.75s/it]Running Inference:  51%|█████     | 254/500 [16:26<15:31,  3.79s/it]Running Inference:  51%|█████     | 255/500 [16:30<15:44,  3.85s/it]Running Inference:  51%|█████     | 256/500 [16:34<15:39,  3.85s/it]Running Inference:  51%|█████▏    | 257/500 [16:38<15:50,  3.91s/it]Running Inference:  52%|█████▏    | 258/500 [16:41<15:41,  3.89s/it]Running Inference:  52%|█████▏    | 259/500 [16:45<15:36,  3.89s/it]Running Inference:  52%|█████▏    | 260/500 [16:49<15:26,  3.86s/it]Running Inference:  52%|█████▏    | 261/500 [16:53<15:17,  3.84s/it]Running Inference:  52%|█████▏    | 262/500 [16:57<15:20,  3.87s/it]Running Inference:  53%|█████▎    | 263/500 [17:01<15:16,  3.87s/it]Running Inference:  53%|█████▎    | 264/500 [17:01<11:34,  2.94s/it]Running Inference:  53%|█████▎    | 265/500 [17:06<12:46,  3.26s/it]Running Inference:  53%|█████▎    | 266/500 [17:09<13:24,  3.44s/it]Running Inference:  53%|█████▎    | 267/500 [17:14<14:23,  3.71s/it]Running Inference:  54%|█████▎    | 268/500 [17:18<14:29,  3.75s/it]Running Inference:  54%|█████▍    | 269/500 [17:21<14:35,  3.79s/it]Running Inference:  54%|█████▍    | 270/500 [17:25<14:35,  3.81s/it]Running Inference:  54%|█████▍    | 271/500 [17:29<14:29,  3.80s/it]Running Inference:  54%|█████▍    | 272/500 [17:33<14:30,  3.82s/it]Running Inference:  55%|█████▍    | 273/500 [17:37<14:35,  3.86s/it]Running Inference:  55%|█████▍    | 274/500 [17:41<14:27,  3.84s/it]Running Inference:  55%|█████▌    | 275/500 [17:44<14:23,  3.84s/it]Running Inference:  55%|█████▌    | 276/500 [17:48<14:22,  3.85s/it]Running Inference:  55%|█████▌    | 277/500 [17:52<14:18,  3.85s/it]Running Inference:  56%|█████▌    | 278/500 [17:56<14:19,  3.87s/it]Running Inference:  56%|█████▌    | 279/500 [18:00<14:26,  3.92s/it]Running Inference:  56%|█████▌    | 280/500 [18:04<14:34,  3.98s/it]Running Inference:  56%|█████▌    | 281/500 [18:08<14:27,  3.96s/it]Running Inference:  56%|█████▋    | 282/500 [18:12<14:18,  3.94s/it]Running Inference:  57%|█████▋    | 283/500 [18:16<14:08,  3.91s/it]Running Inference:  57%|█████▋    | 284/500 [18:20<14:08,  3.93s/it]Running Inference:  57%|█████▋    | 285/500 [18:24<14:05,  3.93s/it]Running Inference:  57%|█████▋    | 286/500 [18:28<13:52,  3.89s/it]Running Inference:  57%|█████▋    | 287/500 [18:32<14:38,  4.12s/it]Running Inference:  58%|█████▊    | 288/500 [18:36<14:31,  4.11s/it]Running Inference:  58%|█████▊    | 289/500 [18:40<14:10,  4.03s/it]Running Inference:  58%|█████▊    | 290/500 [18:44<14:03,  4.02s/it]Running Inference:  58%|█████▊    | 291/500 [18:48<14:01,  4.02s/it]Running Inference:  58%|█████▊    | 292/500 [18:52<13:42,  3.95s/it]Running Inference:  59%|█████▊    | 293/500 [18:53<10:40,  3.09s/it]Running Inference:  59%|█████▉    | 294/500 [18:57<11:21,  3.31s/it]Running Inference:  59%|█████▉    | 295/500 [19:01<11:50,  3.47s/it]Running Inference:  59%|█████▉    | 296/500 [19:05<12:11,  3.59s/it]Running Inference:  59%|█████▉    | 297/500 [19:06<09:57,  2.94s/it]Running Inference:  60%|█████▉    | 298/500 [19:10<10:46,  3.20s/it]Running Inference:  60%|█████▉    | 299/500 [19:12<09:51,  2.94s/it]Running Inference:  60%|██████    | 300/500 [19:16<10:37,  3.19s/it]Running Inference:  60%|██████    | 301/500 [19:20<11:29,  3.46s/it]Running Inference:  60%|██████    | 302/500 [19:24<11:49,  3.58s/it]Running Inference:  61%|██████    | 303/500 [19:28<12:02,  3.67s/it]Running Inference:  61%|██████    | 304/500 [19:32<12:39,  3.88s/it]Running Inference:  61%|██████    | 305/500 [19:36<12:31,  3.85s/it]Running Inference:  61%|██████    | 306/500 [19:40<12:53,  3.99s/it]Running Inference:  61%|██████▏   | 307/500 [19:44<12:43,  3.96s/it]Running Inference:  62%|██████▏   | 308/500 [19:48<12:30,  3.91s/it]Running Inference:  62%|██████▏   | 309/500 [19:52<12:20,  3.87s/it]Running Inference:  62%|██████▏   | 310/500 [19:56<12:14,  3.86s/it]Running Inference:  62%|██████▏   | 311/500 [20:00<12:15,  3.89s/it]Running Inference:  62%|██████▏   | 312/500 [20:03<12:09,  3.88s/it]Running Inference:  63%|██████▎   | 313/500 [20:07<12:10,  3.91s/it]Running Inference:  63%|██████▎   | 314/500 [20:11<12:00,  3.87s/it]Running Inference:  63%|██████▎   | 315/500 [20:15<11:50,  3.84s/it]Running Inference:  63%|██████▎   | 316/500 [20:19<11:42,  3.82s/it]Running Inference:  63%|██████▎   | 317/500 [20:23<11:38,  3.81s/it]Running Inference:  64%|██████▎   | 318/500 [20:26<11:34,  3.82s/it]Running Inference:  64%|██████▍   | 319/500 [20:30<11:27,  3.80s/it]Running Inference:  64%|██████▍   | 320/500 [20:34<11:55,  3.97s/it]Running Inference:  64%|██████▍   | 321/500 [20:39<12:00,  4.03s/it]Running Inference:  64%|██████▍   | 322/500 [20:42<11:48,  3.98s/it]Running Inference:  65%|██████▍   | 323/500 [20:46<11:36,  3.93s/it]Running Inference:  65%|██████▍   | 324/500 [20:50<11:28,  3.91s/it]Running Inference:  65%|██████▌   | 325/500 [20:54<11:21,  3.90s/it]Running Inference:  65%|██████▌   | 326/500 [20:58<11:37,  4.01s/it]Running Inference:  65%|██████▌   | 327/500 [21:03<12:02,  4.17s/it]Running Inference:  66%|██████▌   | 328/500 [21:07<12:05,  4.22s/it]Running Inference:  66%|██████▌   | 329/500 [21:11<11:40,  4.10s/it]Running Inference:  66%|██████▌   | 330/500 [21:15<11:21,  4.01s/it]Running Inference:  66%|██████▌   | 331/500 [21:19<11:45,  4.18s/it]Running Inference:  66%|██████▋   | 332/500 [21:23<11:37,  4.15s/it]Running Inference:  67%|██████▋   | 333/500 [21:27<11:18,  4.06s/it]Running Inference:  67%|██████▋   | 334/500 [21:31<11:04,  4.00s/it]Running Inference:  67%|██████▋   | 335/500 [21:35<10:53,  3.96s/it]Running Inference:  67%|██████▋   | 336/500 [21:39<10:41,  3.91s/it]Running Inference:  67%|██████▋   | 337/500 [21:43<10:34,  3.89s/it]Running Inference:  68%|██████▊   | 338/500 [21:47<10:29,  3.88s/it]Running Inference:  68%|██████▊   | 339/500 [21:51<10:43,  3.99s/it]Running Inference:  68%|██████▊   | 340/500 [21:57<12:09,  4.56s/it]Running Inference:  68%|██████▊   | 341/500 [22:01<11:31,  4.35s/it]Running Inference:  68%|██████▊   | 342/500 [22:04<11:05,  4.21s/it]Running Inference:  69%|██████▊   | 343/500 [22:09<11:00,  4.21s/it]Running Inference:  69%|██████▉   | 344/500 [22:13<10:52,  4.19s/it]Running Inference:  69%|██████▉   | 345/500 [22:17<10:37,  4.11s/it]Running Inference:  69%|██████▉   | 346/500 [22:21<10:30,  4.10s/it]Running Inference:  69%|██████▉   | 347/500 [22:25<10:19,  4.05s/it]Running Inference:  70%|██████▉   | 348/500 [22:29<10:10,  4.01s/it]Running Inference:  70%|██████▉   | 349/500 [22:33<10:00,  3.98s/it]Running Inference:  70%|███████   | 350/500 [22:39<11:29,  4.60s/it]Running Inference:  70%|███████   | 351/500 [22:43<11:13,  4.52s/it]Running Inference:  70%|███████   | 352/500 [22:47<10:36,  4.30s/it]Running Inference:  71%|███████   | 353/500 [22:51<10:10,  4.15s/it]Running Inference:  71%|███████   | 354/500 [22:54<09:59,  4.10s/it]Running Inference:  71%|███████   | 355/500 [22:58<09:37,  3.98s/it]Running Inference:  71%|███████   | 356/500 [23:02<09:27,  3.94s/it]Running Inference:  71%|███████▏  | 357/500 [23:06<09:36,  4.03s/it]Running Inference:  72%|███████▏  | 358/500 [23:10<09:25,  3.98s/it]Running Inference:  72%|███████▏  | 359/500 [23:14<09:13,  3.93s/it]Running Inference:  72%|███████▏  | 360/500 [23:18<09:05,  3.90s/it]Running Inference:  72%|███████▏  | 361/500 [23:22<09:06,  3.93s/it]Running Inference:  72%|███████▏  | 362/500 [23:26<09:07,  3.97s/it]Running Inference:  73%|███████▎  | 363/500 [23:28<07:44,  3.39s/it]Running Inference:  73%|███████▎  | 364/500 [23:32<08:06,  3.58s/it]Running Inference:  73%|███████▎  | 365/500 [23:37<09:02,  4.02s/it]Running Inference:  73%|███████▎  | 366/500 [23:41<09:12,  4.12s/it]Running Inference:  73%|███████▎  | 367/500 [23:45<09:00,  4.06s/it]Running Inference:  74%|███████▎  | 368/500 [23:49<08:55,  4.05s/it]Running Inference:  74%|███████▍  | 369/500 [23:54<08:58,  4.11s/it]Running Inference:  74%|███████▍  | 370/500 [23:58<09:05,  4.20s/it]Running Inference:  74%|███████▍  | 371/500 [24:02<08:48,  4.10s/it]Running Inference:  74%|███████▍  | 372/500 [24:06<08:41,  4.07s/it]Running Inference:  75%|███████▍  | 373/500 [24:10<08:29,  4.02s/it]Running Inference:  75%|███████▍  | 374/500 [24:14<08:24,  4.00s/it]Running Inference:  75%|███████▌  | 375/500 [24:17<08:12,  3.94s/it]Running Inference:  75%|███████▌  | 376/500 [24:21<08:07,  3.94s/it]Running Inference:  75%|███████▌  | 377/500 [24:25<08:01,  3.91s/it]Running Inference:  76%|███████▌  | 378/500 [24:29<07:58,  3.92s/it]Running Inference:  76%|███████▌  | 379/500 [24:33<07:59,  3.97s/it]Running Inference:  76%|███████▌  | 380/500 [24:37<07:52,  3.93s/it]Running Inference:  76%|███████▌  | 381/500 [24:41<07:45,  3.91s/it]Running Inference:  76%|███████▋  | 382/500 [24:45<07:53,  4.01s/it]Running Inference:  77%|███████▋  | 383/500 [24:49<07:50,  4.02s/it]Running Inference:  77%|███████▋  | 384/500 [24:53<07:40,  3.97s/it]Running Inference:  77%|███████▋  | 385/500 [24:57<07:40,  4.01s/it]Running Inference:  77%|███████▋  | 386/500 [25:01<07:38,  4.02s/it]Running Inference:  77%|███████▋  | 387/500 [25:05<07:32,  4.01s/it]Running Inference:  78%|███████▊  | 388/500 [25:09<07:23,  3.96s/it]Running Inference:  78%|███████▊  | 389/500 [25:13<07:19,  3.96s/it]Running Inference:  78%|███████▊  | 390/500 [25:17<07:12,  3.93s/it]Running Inference:  78%|███████▊  | 391/500 [25:21<07:06,  3.91s/it]Running Inference:  78%|███████▊  | 392/500 [25:25<06:59,  3.89s/it]Running Inference:  79%|███████▊  | 393/500 [25:28<06:52,  3.85s/it]Running Inference:  79%|███████▉  | 394/500 [25:32<06:50,  3.87s/it]Running Inference:  79%|███████▉  | 395/500 [25:36<06:47,  3.89s/it]Running Inference:  79%|███████▉  | 396/500 [25:40<06:40,  3.85s/it]Running Inference:  79%|███████▉  | 397/500 [25:44<06:46,  3.94s/it]Running Inference:  80%|███████▉  | 398/500 [25:48<06:37,  3.90s/it]Running Inference:  80%|███████▉  | 399/500 [25:52<06:30,  3.87s/it]Running Inference:  80%|████████  | 400/500 [25:56<06:24,  3.84s/it]Running Inference:  80%|████████  | 401/500 [25:59<06:19,  3.83s/it]Running Inference:  80%|████████  | 402/500 [26:03<06:14,  3.82s/it]Running Inference:  81%|████████  | 403/500 [26:07<06:10,  3.82s/it]Running Inference:  81%|████████  | 404/500 [26:11<06:06,  3.81s/it]Running Inference:  81%|████████  | 405/500 [26:15<06:04,  3.84s/it]Running Inference:  81%|████████  | 406/500 [26:18<06:00,  3.84s/it]Running Inference:  81%|████████▏ | 407/500 [26:22<05:56,  3.84s/it]Running Inference:  82%|████████▏ | 408/500 [26:26<05:53,  3.84s/it]Running Inference:  82%|████████▏ | 409/500 [26:30<05:49,  3.84s/it]Running Inference:  82%|████████▏ | 410/500 [26:34<05:46,  3.85s/it]Running Inference:  82%|████████▏ | 411/500 [26:38<05:44,  3.88s/it]Running Inference:  82%|████████▏ | 412/500 [26:42<05:40,  3.87s/it]Running Inference:  83%|████████▎ | 413/500 [26:45<05:35,  3.85s/it]Running Inference:  83%|████████▎ | 414/500 [26:49<05:31,  3.86s/it]Running Inference:  83%|████████▎ | 415/500 [26:53<05:27,  3.85s/it]Running Inference:  83%|████████▎ | 416/500 [26:57<05:21,  3.82s/it]Running Inference:  83%|████████▎ | 417/500 [27:01<05:16,  3.82s/it]Running Inference:  84%|████████▎ | 418/500 [27:05<05:14,  3.84s/it]Running Inference:  84%|████████▍ | 419/500 [27:08<05:10,  3.83s/it]Running Inference:  84%|████████▍ | 420/500 [27:12<05:05,  3.82s/it]Running Inference:  84%|████████▍ | 421/500 [27:16<05:02,  3.82s/it]Running Inference:  84%|████████▍ | 422/500 [27:20<04:57,  3.82s/it]Running Inference:  85%|████████▍ | 423/500 [27:24<04:53,  3.81s/it]Running Inference:  85%|████████▍ | 424/500 [27:27<04:50,  3.82s/it]Running Inference:  85%|████████▌ | 425/500 [27:32<04:53,  3.91s/it]Running Inference:  85%|████████▌ | 426/500 [27:35<04:47,  3.89s/it]Running Inference:  85%|████████▌ | 427/500 [27:39<04:40,  3.85s/it]Running Inference:  86%|████████▌ | 428/500 [27:43<04:35,  3.83s/it]Running Inference:  86%|████████▌ | 429/500 [27:47<04:31,  3.82s/it]Running Inference:  86%|████████▌ | 430/500 [27:51<04:36,  3.95s/it]Running Inference:  86%|████████▌ | 431/500 [27:55<04:31,  3.93s/it]Running Inference:  86%|████████▋ | 432/500 [27:59<04:31,  3.99s/it]Running Inference:  87%|████████▋ | 433/500 [28:03<04:29,  4.02s/it]Running Inference:  87%|████████▋ | 434/500 [28:07<04:25,  4.02s/it]Running Inference:  87%|████████▋ | 435/500 [28:11<04:18,  3.97s/it]Running Inference:  87%|████████▋ | 436/500 [28:15<04:13,  3.95s/it]Running Inference:  87%|████████▋ | 437/500 [28:19<04:07,  3.93s/it]Running Inference:  88%|████████▊ | 438/500 [28:23<04:01,  3.89s/it]Running Inference:  88%|████████▊ | 439/500 [28:27<03:57,  3.90s/it]Running Inference:  88%|████████▊ | 440/500 [28:30<03:53,  3.89s/it]Running Inference:  88%|████████▊ | 441/500 [28:34<03:47,  3.86s/it]Running Inference:  88%|████████▊ | 442/500 [28:38<03:42,  3.83s/it]Running Inference:  89%|████████▊ | 443/500 [28:42<03:38,  3.83s/it]Running Inference:  89%|████████▉ | 444/500 [28:46<03:36,  3.86s/it]Running Inference:  89%|████████▉ | 445/500 [28:50<03:33,  3.89s/it]Running Inference:  89%|████████▉ | 446/500 [28:54<03:31,  3.92s/it]Running Inference:  89%|████████▉ | 447/500 [28:57<03:26,  3.89s/it]Running Inference:  90%|████████▉ | 448/500 [29:01<03:22,  3.90s/it]Running Inference:  90%|████████▉ | 449/500 [29:05<03:18,  3.88s/it]Running Inference:  90%|█████████ | 450/500 [29:06<02:31,  3.03s/it]Running Inference:  90%|█████████ | 451/500 [29:10<02:40,  3.27s/it]Running Inference:  90%|█████████ | 452/500 [29:14<02:45,  3.45s/it]Running Inference:  91%|█████████ | 453/500 [29:18<02:49,  3.60s/it]Running Inference:  91%|█████████ | 454/500 [29:22<02:48,  3.67s/it]Running Inference:  91%|█████████ | 455/500 [29:26<02:46,  3.70s/it]Running Inference:  91%|█████████ | 456/500 [29:29<02:44,  3.73s/it]Running Inference:  91%|█████████▏| 457/500 [29:33<02:42,  3.77s/it]Running Inference:  92%|█████████▏| 458/500 [29:37<02:39,  3.79s/it]Running Inference:  92%|█████████▏| 459/500 [29:38<02:05,  3.07s/it]Running Inference:  92%|█████████▏| 460/500 [29:43<02:15,  3.40s/it]Running Inference:  92%|█████████▏| 461/500 [29:46<02:18,  3.56s/it]Running Inference:  92%|█████████▏| 462/500 [29:51<02:23,  3.77s/it]Running Inference:  93%|█████████▎| 463/500 [29:55<02:19,  3.78s/it]Running Inference:  93%|█████████▎| 464/500 [29:58<02:15,  3.78s/it]Running Inference:  93%|█████████▎| 465/500 [30:02<02:12,  3.78s/it]Running Inference:  93%|█████████▎| 466/500 [30:06<02:09,  3.81s/it]Running Inference:  93%|█████████▎| 467/500 [30:10<02:05,  3.79s/it]Running Inference:  94%|█████████▎| 468/500 [30:14<02:03,  3.86s/it]Running Inference:  94%|█████████▍| 469/500 [30:18<01:59,  3.85s/it]Running Inference:  94%|█████████▍| 470/500 [30:21<01:54,  3.83s/it]Running Inference:  94%|█████████▍| 471/500 [30:25<01:52,  3.87s/it]Running Inference:  94%|█████████▍| 472/500 [30:29<01:47,  3.85s/it]Running Inference:  95%|█████████▍| 473/500 [30:33<01:46,  3.93s/it]Running Inference:  95%|█████████▍| 474/500 [30:37<01:41,  3.91s/it]Running Inference:  95%|█████████▌| 475/500 [30:41<01:37,  3.89s/it]Running Inference:  95%|█████████▌| 476/500 [30:45<01:33,  3.89s/it]Running Inference:  95%|█████████▌| 477/500 [30:49<01:29,  3.88s/it]Running Inference:  96%|█████████▌| 478/500 [30:50<01:11,  3.25s/it]Running Inference:  96%|█████████▌| 479/500 [30:55<01:18,  3.75s/it]Running Inference:  96%|█████████▌| 480/500 [30:59<01:16,  3.81s/it]Running Inference:  96%|█████████▌| 481/500 [31:04<01:17,  4.06s/it]Running Inference:  96%|█████████▋| 482/500 [31:08<01:11,  3.98s/it]Running Inference:  97%|█████████▋| 483/500 [31:12<01:07,  3.94s/it]Running Inference:  97%|█████████▋| 484/500 [31:16<01:03,  3.99s/it]Running Inference:  97%|█████████▋| 485/500 [31:21<01:05,  4.37s/it]Running Inference:  97%|█████████▋| 486/500 [31:25<00:58,  4.20s/it]Running Inference:  97%|█████████▋| 487/500 [31:29<00:53,  4.10s/it]Running Inference:  98%|█████████▊| 488/500 [31:32<00:47,  3.99s/it]Running Inference:  98%|█████████▊| 489/500 [31:36<00:43,  3.98s/it]Running Inference:  98%|█████████▊| 490/500 [31:37<00:30,  3.00s/it]Running Inference:  98%|█████████▊| 491/500 [31:41<00:29,  3.25s/it]Running Inference:  98%|█████████▊| 492/500 [31:45<00:27,  3.47s/it]Running Inference:  99%|█████████▊| 493/500 [31:49<00:24,  3.57s/it]Running Inference:  99%|█████████▉| 494/500 [31:53<00:22,  3.69s/it]Running Inference:  99%|█████████▉| 495/500 [31:57<00:18,  3.74s/it]Running Inference:  99%|█████████▉| 496/500 [32:00<00:15,  3.80s/it]Running Inference:  99%|█████████▉| 497/500 [32:04<00:11,  3.81s/it]Running Inference: 100%|█████████▉| 498/500 [32:08<00:07,  3.84s/it]Running Inference: 100%|█████████▉| 499/500 [32:12<00:03,  3.84s/it]Running Inference: 100%|██████████| 500/500 [32:16<00:00,  3.93s/it]Running Inference: 100%|██████████| 500/500 [32:16<00:00,  3.87s/it]
2025-12-14 19:41:11,560 - INFO - Inference completed.
2025-12-14 19:41:11,575 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 19:41:11,575 - INFO - Calculating metrics for dataset: longbench
2025-12-14 19:41:11,577 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 19:41:11,577 - INFO - Metrics:
37.14
2025-12-14 19:41:11,579 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc, ratio=0.2) on GPU 1

----------------------------------------
Task: lcc | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 19:41:18,055 - INFO - Set deterministic seeds to 42
2025-12-14 19:41:18,055 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 19:41:18,055 - INFO - Starting evaluation run...
2025-12-14 19:41:18,055 - INFO - Output directory set to: longbenchresult
2025-12-14 19:41:18,055 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 19:41:18,055 - INFO - KV Press 'tova' setup.
2025-12-14 19:41:18,055 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 19:41:18,055 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.00it/s]
Device set to use cuda:0
2025-12-14 19:41:34,774 - INFO - Model pipeline loaded.
2025-12-14 19:41:34,774 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 19:41:51,540 - INFO - Dataset loaded with 500 entries.
2025-12-14 19:41:51,540 - INFO - Dataset processed with 500 entries.
2025-12-14 19:41:51,554 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<40:46,  4.90s/it]Running Inference:   0%|          | 2/500 [00:08<36:01,  4.34s/it]Running Inference:   1%|          | 3/500 [00:13<35:47,  4.32s/it]Running Inference:   1%|          | 4/500 [00:16<34:03,  4.12s/it]Running Inference:   1%|          | 5/500 [00:20<33:09,  4.02s/it]Running Inference:   1%|          | 6/500 [00:24<33:06,  4.02s/it]Running Inference:   1%|▏         | 7/500 [00:28<32:40,  3.98s/it]Running Inference:   2%|▏         | 8/500 [00:32<32:06,  3.92s/it]Running Inference:   2%|▏         | 9/500 [00:36<31:41,  3.87s/it]Running Inference:   2%|▏         | 10/500 [00:40<31:23,  3.84s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:44<32:20,  3.97s/it]Running Inference:   2%|▏         | 12/500 [00:46<28:21,  3.49s/it]Running Inference:   3%|▎         | 13/500 [00:50<29:05,  3.58s/it]Running Inference:   3%|▎         | 14/500 [00:54<29:35,  3.65s/it]Running Inference:   3%|▎         | 15/500 [00:58<29:54,  3.70s/it]Running Inference:   3%|▎         | 16/500 [01:01<30:03,  3.73s/it]Running Inference:   3%|▎         | 17/500 [01:06<31:53,  3.96s/it]Running Inference:   4%|▎         | 18/500 [01:10<31:40,  3.94s/it]Running Inference:   4%|▍         | 19/500 [01:14<31:19,  3.91s/it]Running Inference:   4%|▍         | 20/500 [01:17<31:01,  3.88s/it]Running Inference:   4%|▍         | 21/500 [01:21<30:47,  3.86s/it]Running Inference:   4%|▍         | 22/500 [01:25<30:36,  3.84s/it]Running Inference:   5%|▍         | 23/500 [01:29<30:24,  3.82s/it]Running Inference:   5%|▍         | 24/500 [01:33<30:14,  3.81s/it]Running Inference:   5%|▌         | 25/500 [01:36<30:13,  3.82s/it]Running Inference:   5%|▌         | 26/500 [01:40<30:16,  3.83s/it]Running Inference:   5%|▌         | 27/500 [01:44<30:10,  3.83s/it]Running Inference:   6%|▌         | 28/500 [01:49<33:15,  4.23s/it]Running Inference:   6%|▌         | 29/500 [01:52<28:53,  3.68s/it]Running Inference:   6%|▌         | 30/500 [01:56<30:36,  3.91s/it]Running Inference:   6%|▌         | 31/500 [02:00<30:34,  3.91s/it]Running Inference:   6%|▋         | 32/500 [02:04<30:25,  3.90s/it]Running Inference:   7%|▋         | 33/500 [02:08<30:08,  3.87s/it]Running Inference:   7%|▋         | 34/500 [02:12<29:55,  3.85s/it]Running Inference:   7%|▋         | 35/500 [02:15<29:41,  3.83s/it]Running Inference:   7%|▋         | 36/500 [02:20<30:34,  3.95s/it]Running Inference:   7%|▋         | 37/500 [02:24<31:10,  4.04s/it]Running Inference:   8%|▊         | 38/500 [02:28<30:38,  3.98s/it]Running Inference:   8%|▊         | 39/500 [02:32<31:10,  4.06s/it]Running Inference:   8%|▊         | 40/500 [02:36<30:31,  3.98s/it]Running Inference:   8%|▊         | 41/500 [02:40<30:11,  3.95s/it]Running Inference:   8%|▊         | 42/500 [02:44<31:47,  4.16s/it]Running Inference:   9%|▊         | 43/500 [02:49<32:39,  4.29s/it]Running Inference:   9%|▉         | 44/500 [02:52<30:08,  3.97s/it]Running Inference:   9%|▉         | 45/500 [02:56<29:55,  3.95s/it]Running Inference:   9%|▉         | 46/500 [03:00<29:26,  3.89s/it]Running Inference:   9%|▉         | 47/500 [03:04<29:25,  3.90s/it]Running Inference:  10%|▉         | 48/500 [03:07<29:02,  3.86s/it]Running Inference:  10%|▉         | 49/500 [03:11<29:14,  3.89s/it]Running Inference:  10%|█         | 50/500 [03:16<30:41,  4.09s/it]Running Inference:  10%|█         | 51/500 [03:20<29:59,  4.01s/it]Running Inference:  10%|█         | 52/500 [03:24<29:31,  3.95s/it]Running Inference:  11%|█         | 53/500 [03:27<29:19,  3.94s/it]Running Inference:  11%|█         | 54/500 [03:30<25:20,  3.41s/it]Running Inference:  11%|█         | 55/500 [03:33<26:08,  3.53s/it]Running Inference:  11%|█         | 56/500 [03:37<26:44,  3.61s/it]Running Inference:  11%|█▏        | 57/500 [03:46<37:28,  5.08s/it]Running Inference:  12%|█▏        | 58/500 [03:49<34:29,  4.68s/it]Running Inference:  12%|█▏        | 59/500 [03:50<25:13,  3.43s/it]Running Inference:  12%|█▏        | 60/500 [03:54<25:57,  3.54s/it]Running Inference:  12%|█▏        | 61/500 [03:58<26:32,  3.63s/it]Running Inference:  12%|█▏        | 62/500 [04:01<26:50,  3.68s/it]Running Inference:  13%|█▎        | 63/500 [04:05<27:04,  3.72s/it]Running Inference:  13%|█▎        | 64/500 [04:07<22:20,  3.07s/it]Running Inference:  13%|█▎        | 65/500 [04:11<23:53,  3.30s/it]Running Inference:  13%|█▎        | 66/500 [04:13<21:10,  2.93s/it]Running Inference:  13%|█▎        | 67/500 [04:17<23:08,  3.21s/it]Running Inference:  14%|█▎        | 68/500 [04:20<24:26,  3.39s/it]Running Inference:  14%|█▍        | 69/500 [04:24<25:15,  3.52s/it]Running Inference:  14%|█▍        | 70/500 [04:26<20:36,  2.88s/it]Running Inference:  14%|█▍        | 71/500 [04:29<21:04,  2.95s/it]Running Inference:  14%|█▍        | 72/500 [04:33<23:23,  3.28s/it]Running Inference:  15%|█▍        | 73/500 [04:37<24:25,  3.43s/it]Running Inference:  15%|█▍        | 74/500 [04:40<25:16,  3.56s/it]Running Inference:  15%|█▌        | 75/500 [04:44<26:19,  3.72s/it]Running Inference:  15%|█▌        | 76/500 [04:48<26:46,  3.79s/it]Running Inference:  15%|█▌        | 77/500 [04:52<26:52,  3.81s/it]Running Inference:  16%|█▌        | 78/500 [04:57<27:44,  3.94s/it]Running Inference:  16%|█▌        | 79/500 [04:58<22:49,  3.25s/it]Running Inference:  16%|█▌        | 80/500 [05:02<23:58,  3.42s/it]Running Inference:  16%|█▌        | 81/500 [05:06<25:38,  3.67s/it]Running Inference:  16%|█▋        | 82/500 [05:11<26:59,  3.87s/it]Running Inference:  17%|█▋        | 83/500 [05:13<23:04,  3.32s/it]Running Inference:  17%|█▋        | 84/500 [05:16<24:09,  3.48s/it]Running Inference:  17%|█▋        | 85/500 [05:21<26:55,  3.89s/it]Running Inference:  17%|█▋        | 86/500 [05:25<26:38,  3.86s/it]Running Inference:  17%|█▋        | 87/500 [05:29<26:25,  3.84s/it]Running Inference:  18%|█▊        | 88/500 [05:33<26:18,  3.83s/it]Running Inference:  18%|█▊        | 89/500 [05:37<26:22,  3.85s/it]Running Inference:  18%|█▊        | 90/500 [05:41<26:53,  3.94s/it]Running Inference:  18%|█▊        | 91/500 [05:45<27:03,  3.97s/it]Running Inference:  18%|█▊        | 92/500 [05:48<25:12,  3.71s/it]Running Inference:  19%|█▊        | 93/500 [05:52<25:24,  3.75s/it]Running Inference:  19%|█▉        | 94/500 [05:56<25:27,  3.76s/it]Running Inference:  19%|█▉        | 95/500 [05:59<25:31,  3.78s/it]Running Inference:  19%|█▉        | 96/500 [06:03<25:40,  3.81s/it]Running Inference:  19%|█▉        | 97/500 [06:07<25:36,  3.81s/it]Running Inference:  20%|█▉        | 98/500 [06:11<25:29,  3.81s/it]Running Inference:  20%|█▉        | 99/500 [06:15<25:23,  3.80s/it]Running Inference:  20%|██        | 100/500 [06:19<26:03,  3.91s/it]Running Inference:  20%|██        | 101/500 [06:23<26:10,  3.94s/it]Running Inference:  20%|██        | 102/500 [06:24<21:19,  3.22s/it]Running Inference:  21%|██        | 103/500 [06:28<22:25,  3.39s/it]Running Inference:  21%|██        | 104/500 [06:34<27:09,  4.12s/it]Running Inference:  21%|██        | 105/500 [06:39<28:04,  4.27s/it]Running Inference:  21%|██        | 106/500 [06:44<30:11,  4.60s/it]Running Inference:  21%|██▏       | 107/500 [06:48<28:57,  4.42s/it]Running Inference:  22%|██▏       | 108/500 [06:52<27:42,  4.24s/it]Running Inference:  22%|██▏       | 109/500 [06:56<26:54,  4.13s/it]Running Inference:  22%|██▏       | 110/500 [07:00<26:24,  4.06s/it]Running Inference:  22%|██▏       | 111/500 [07:03<25:48,  3.98s/it]Running Inference:  22%|██▏       | 112/500 [07:07<25:25,  3.93s/it]Running Inference:  23%|██▎       | 113/500 [07:11<25:49,  4.00s/it]Running Inference:  23%|██▎       | 114/500 [07:16<28:02,  4.36s/it]Running Inference:  23%|██▎       | 115/500 [07:21<27:27,  4.28s/it]Running Inference:  23%|██▎       | 116/500 [07:25<27:08,  4.24s/it]Running Inference:  23%|██▎       | 117/500 [07:28<26:09,  4.10s/it]Running Inference:  24%|██▎       | 118/500 [07:33<26:44,  4.20s/it]Running Inference:  24%|██▍       | 119/500 [07:37<25:50,  4.07s/it]Running Inference:  24%|██▍       | 120/500 [07:41<25:57,  4.10s/it]Running Inference:  24%|██▍       | 121/500 [07:45<25:29,  4.04s/it]Running Inference:  24%|██▍       | 122/500 [07:47<22:11,  3.52s/it]Running Inference:  25%|██▍       | 123/500 [07:51<22:38,  3.60s/it]Running Inference:  25%|██▍       | 124/500 [07:55<22:54,  3.66s/it]Running Inference:  25%|██▌       | 125/500 [07:59<23:41,  3.79s/it]Running Inference:  25%|██▌       | 126/500 [08:03<23:37,  3.79s/it]Running Inference:  25%|██▌       | 127/500 [08:04<18:37,  3.00s/it]Running Inference:  26%|██▌       | 128/500 [08:08<20:25,  3.29s/it]Running Inference:  26%|██▌       | 129/500 [08:11<21:17,  3.44s/it]Running Inference:  26%|██▌       | 130/500 [08:15<22:06,  3.58s/it]Running Inference:  26%|██▌       | 131/500 [08:19<22:27,  3.65s/it]Running Inference:  26%|██▋       | 132/500 [08:23<22:38,  3.69s/it]Running Inference:  27%|██▋       | 133/500 [08:27<22:53,  3.74s/it]Running Inference:  27%|██▋       | 134/500 [08:31<22:59,  3.77s/it]Running Inference:  27%|██▋       | 135/500 [08:35<23:13,  3.82s/it]Running Inference:  27%|██▋       | 136/500 [08:38<23:18,  3.84s/it]Running Inference:  27%|██▋       | 137/500 [08:40<18:41,  3.09s/it]Running Inference:  28%|██▊       | 138/500 [08:44<20:09,  3.34s/it]Running Inference:  28%|██▊       | 139/500 [08:48<20:50,  3.47s/it]Running Inference:  28%|██▊       | 140/500 [08:51<21:36,  3.60s/it]Running Inference:  28%|██▊       | 141/500 [08:56<22:42,  3.80s/it]Running Inference:  28%|██▊       | 142/500 [09:00<23:07,  3.88s/it]Running Inference:  29%|██▊       | 143/500 [09:04<22:57,  3.86s/it]Running Inference:  29%|██▉       | 144/500 [09:07<22:46,  3.84s/it]Running Inference:  29%|██▉       | 145/500 [09:11<22:56,  3.88s/it]Running Inference:  29%|██▉       | 146/500 [09:15<23:08,  3.92s/it]Running Inference:  29%|██▉       | 147/500 [09:19<23:21,  3.97s/it]Running Inference:  30%|██▉       | 148/500 [09:23<22:55,  3.91s/it]Running Inference:  30%|██▉       | 149/500 [09:27<22:38,  3.87s/it]Running Inference:  30%|███       | 150/500 [09:31<22:22,  3.84s/it]Running Inference:  30%|███       | 151/500 [09:35<22:22,  3.85s/it]Running Inference:  30%|███       | 152/500 [09:35<16:40,  2.87s/it]Running Inference:  31%|███       | 153/500 [09:39<18:14,  3.15s/it]Running Inference:  31%|███       | 154/500 [09:43<19:17,  3.34s/it]Running Inference:  31%|███       | 155/500 [09:47<19:56,  3.47s/it]Running Inference:  31%|███       | 156/500 [09:50<20:28,  3.57s/it]Running Inference:  31%|███▏      | 157/500 [09:54<21:07,  3.70s/it]Running Inference:  32%|███▏      | 158/500 [09:58<21:18,  3.74s/it]Running Inference:  32%|███▏      | 159/500 [10:02<21:19,  3.75s/it]Running Inference:  32%|███▏      | 160/500 [10:06<21:18,  3.76s/it]Running Inference:  32%|███▏      | 161/500 [10:10<21:21,  3.78s/it]Running Inference:  32%|███▏      | 162/500 [10:14<21:32,  3.82s/it]Running Inference:  33%|███▎      | 163/500 [10:17<21:21,  3.80s/it]Running Inference:  33%|███▎      | 164/500 [10:21<21:19,  3.81s/it]Running Inference:  33%|███▎      | 165/500 [10:25<21:20,  3.82s/it]Running Inference:  33%|███▎      | 166/500 [10:29<21:09,  3.80s/it]Running Inference:  33%|███▎      | 167/500 [10:33<21:42,  3.91s/it]Running Inference:  34%|███▎      | 168/500 [10:34<16:19,  2.95s/it]Running Inference:  34%|███▍      | 169/500 [10:37<17:49,  3.23s/it]Running Inference:  34%|███▍      | 170/500 [10:41<18:40,  3.40s/it]Running Inference:  34%|███▍      | 171/500 [10:45<19:29,  3.55s/it]Running Inference:  34%|███▍      | 172/500 [10:49<19:49,  3.63s/it]Running Inference:  35%|███▍      | 173/500 [10:53<20:04,  3.68s/it]Running Inference:  35%|███▍      | 174/500 [10:57<20:41,  3.81s/it]Running Inference:  35%|███▌      | 175/500 [11:01<21:36,  3.99s/it]Running Inference:  35%|███▌      | 176/500 [11:02<16:09,  2.99s/it]Running Inference:  35%|███▌      | 177/500 [11:06<18:32,  3.44s/it]Running Inference:  36%|███▌      | 178/500 [11:10<18:23,  3.43s/it]Running Inference:  36%|███▌      | 179/500 [11:14<18:54,  3.53s/it]Running Inference:  36%|███▌      | 180/500 [11:17<19:24,  3.64s/it]Running Inference:  36%|███▌      | 181/500 [11:21<19:33,  3.68s/it]Running Inference:  36%|███▋      | 182/500 [11:25<19:37,  3.70s/it]Running Inference:  37%|███▋      | 183/500 [11:29<20:01,  3.79s/it]Running Inference:  37%|███▋      | 184/500 [11:33<20:08,  3.82s/it]Running Inference:  37%|███▋      | 185/500 [11:34<16:24,  3.13s/it]Running Inference:  37%|███▋      | 186/500 [11:38<17:28,  3.34s/it]Running Inference:  37%|███▋      | 187/500 [11:42<18:19,  3.51s/it]Running Inference:  38%|███▊      | 188/500 [11:46<18:43,  3.60s/it]Running Inference:  38%|███▊      | 189/500 [11:50<19:04,  3.68s/it]Running Inference:  38%|███▊      | 190/500 [11:54<19:46,  3.83s/it]Running Inference:  38%|███▊      | 191/500 [11:58<19:35,  3.81s/it]Running Inference:  38%|███▊      | 192/500 [12:02<19:37,  3.82s/it]Running Inference:  39%|███▊      | 193/500 [12:05<19:34,  3.83s/it]Running Inference:  39%|███▉      | 194/500 [12:09<19:30,  3.83s/it]Running Inference:  39%|███▉      | 195/500 [12:13<19:26,  3.82s/it]Running Inference:  39%|███▉      | 196/500 [12:17<19:21,  3.82s/it]Running Inference:  39%|███▉      | 197/500 [12:21<19:28,  3.86s/it]Running Inference:  40%|███▉      | 198/500 [12:25<19:15,  3.83s/it]Running Inference:  40%|███▉      | 199/500 [12:28<19:05,  3.81s/it]Running Inference:  40%|████      | 200/500 [12:32<19:10,  3.84s/it]Running Inference:  40%|████      | 201/500 [12:36<19:18,  3.88s/it]Running Inference:  40%|████      | 202/500 [12:40<19:07,  3.85s/it]Running Inference:  41%|████      | 203/500 [12:45<21:09,  4.27s/it]Running Inference:  41%|████      | 204/500 [12:49<20:26,  4.14s/it]Running Inference:  41%|████      | 205/500 [12:53<19:52,  4.04s/it]Running Inference:  41%|████      | 206/500 [12:57<19:23,  3.96s/it]Running Inference:  41%|████▏     | 207/500 [13:01<19:06,  3.91s/it]Running Inference:  42%|████▏     | 208/500 [13:05<19:25,  3.99s/it]Running Inference:  42%|████▏     | 209/500 [13:09<19:42,  4.06s/it]Running Inference:  42%|████▏     | 210/500 [13:13<19:14,  3.98s/it]Running Inference:  42%|████▏     | 211/500 [13:17<19:12,  3.99s/it]Running Inference:  42%|████▏     | 212/500 [13:20<18:49,  3.92s/it]Running Inference:  43%|████▎     | 213/500 [13:22<15:32,  3.25s/it]Running Inference:  43%|████▎     | 214/500 [13:26<16:17,  3.42s/it]Running Inference:  43%|████▎     | 215/500 [13:30<16:52,  3.55s/it]Running Inference:  43%|████▎     | 216/500 [13:34<17:08,  3.62s/it]Running Inference:  43%|████▎     | 217/500 [13:38<17:43,  3.76s/it]Running Inference:  44%|████▎     | 218/500 [13:42<18:00,  3.83s/it]Running Inference:  44%|████▍     | 219/500 [13:45<17:50,  3.81s/it]Running Inference:  44%|████▍     | 220/500 [13:49<17:46,  3.81s/it]Running Inference:  44%|████▍     | 221/500 [13:52<15:50,  3.41s/it]Running Inference:  44%|████▍     | 222/500 [13:53<12:25,  2.68s/it]Running Inference:  45%|████▍     | 223/500 [13:57<13:55,  3.02s/it]Running Inference:  45%|████▍     | 224/500 [14:00<14:58,  3.26s/it]Running Inference:  45%|████▌     | 225/500 [14:03<13:40,  2.98s/it]Running Inference:  45%|████▌     | 226/500 [14:06<14:40,  3.21s/it]Running Inference:  45%|████▌     | 227/500 [14:10<15:24,  3.39s/it]Running Inference:  46%|████▌     | 228/500 [14:14<15:46,  3.48s/it]Running Inference:  46%|████▌     | 229/500 [14:18<16:03,  3.55s/it]Running Inference:  46%|████▌     | 230/500 [14:21<16:17,  3.62s/it]Running Inference:  46%|████▌     | 231/500 [14:25<16:27,  3.67s/it]Running Inference:  46%|████▋     | 232/500 [14:29<16:34,  3.71s/it]Running Inference:  47%|████▋     | 233/500 [14:33<16:45,  3.76s/it]Running Inference:  47%|████▋     | 234/500 [14:37<16:47,  3.79s/it]Running Inference:  47%|████▋     | 235/500 [14:41<17:11,  3.89s/it]Running Inference:  47%|████▋     | 236/500 [14:45<16:54,  3.84s/it]Running Inference:  47%|████▋     | 237/500 [14:48<16:43,  3.82s/it]Running Inference:  48%|████▊     | 238/500 [14:52<16:38,  3.81s/it]Running Inference:  48%|████▊     | 239/500 [14:56<16:40,  3.83s/it]Running Inference:  48%|████▊     | 240/500 [15:00<16:35,  3.83s/it]Running Inference:  48%|████▊     | 241/500 [15:04<16:30,  3.82s/it]Running Inference:  48%|████▊     | 242/500 [15:07<16:19,  3.79s/it]Running Inference:  49%|████▊     | 243/500 [15:12<17:08,  4.00s/it]Running Inference:  49%|████▉     | 244/500 [15:16<17:24,  4.08s/it]Running Inference:  49%|████▉     | 245/500 [15:20<17:05,  4.02s/it]Running Inference:  49%|████▉     | 246/500 [15:24<16:38,  3.93s/it]Running Inference:  49%|████▉     | 247/500 [15:28<16:31,  3.92s/it]Running Inference:  50%|████▉     | 248/500 [15:31<16:18,  3.88s/it]Running Inference:  50%|████▉     | 249/500 [15:35<16:10,  3.87s/it]Running Inference:  50%|█████     | 250/500 [15:36<12:17,  2.95s/it]Running Inference:  50%|█████     | 251/500 [15:40<13:41,  3.30s/it]Running Inference:  50%|█████     | 252/500 [15:45<15:09,  3.67s/it]Running Inference:  51%|█████     | 253/500 [15:49<15:14,  3.70s/it]Running Inference:  51%|█████     | 254/500 [15:52<15:19,  3.74s/it]Running Inference:  51%|█████     | 255/500 [15:56<15:32,  3.81s/it]Running Inference:  51%|█████     | 256/500 [16:00<15:27,  3.80s/it]Running Inference:  51%|█████▏    | 257/500 [16:04<15:38,  3.86s/it]Running Inference:  52%|█████▏    | 258/500 [16:08<15:30,  3.84s/it]Running Inference:  52%|█████▏    | 259/500 [16:12<15:25,  3.84s/it]Running Inference:  52%|█████▏    | 260/500 [16:15<15:15,  3.82s/it]Running Inference:  52%|█████▏    | 261/500 [16:19<15:05,  3.79s/it]Running Inference:  52%|█████▏    | 262/500 [16:23<15:09,  3.82s/it]Running Inference:  53%|█████▎    | 263/500 [16:27<15:05,  3.82s/it]Running Inference:  53%|█████▎    | 264/500 [16:28<11:32,  2.93s/it]Running Inference:  53%|█████▎    | 265/500 [16:32<12:42,  3.24s/it]Running Inference:  53%|█████▎    | 266/500 [16:36<13:18,  3.41s/it]Running Inference:  53%|█████▎    | 267/500 [16:40<14:15,  3.67s/it]Running Inference:  54%|█████▎    | 268/500 [16:44<14:21,  3.71s/it]Running Inference:  54%|█████▍    | 269/500 [16:47<14:26,  3.75s/it]Running Inference:  54%|█████▍    | 270/500 [16:51<14:25,  3.76s/it]Running Inference:  54%|█████▍    | 271/500 [16:55<14:18,  3.75s/it]Running Inference:  54%|█████▍    | 272/500 [16:59<14:19,  3.77s/it]Running Inference:  55%|█████▍    | 273/500 [17:03<14:24,  3.81s/it]Running Inference:  55%|█████▍    | 274/500 [17:06<14:16,  3.79s/it]Running Inference:  55%|█████▌    | 275/500 [17:10<14:13,  3.79s/it]Running Inference:  55%|█████▌    | 276/500 [17:14<14:12,  3.81s/it]Running Inference:  55%|█████▌    | 277/500 [17:18<14:09,  3.81s/it]Running Inference:  56%|█████▌    | 278/500 [17:22<14:09,  3.83s/it]Running Inference:  56%|█████▌    | 279/500 [17:26<14:16,  3.88s/it]Running Inference:  56%|█████▌    | 280/500 [17:30<14:25,  3.93s/it]Running Inference:  56%|█████▌    | 281/500 [17:34<14:17,  3.92s/it]Running Inference:  56%|█████▋    | 282/500 [17:38<14:09,  3.90s/it]Running Inference:  57%|█████▋    | 283/500 [17:41<13:59,  3.87s/it]Running Inference:  57%|█████▋    | 284/500 [17:45<13:59,  3.88s/it]Running Inference:  57%|█████▋    | 285/500 [17:49<13:55,  3.89s/it]Running Inference:  57%|█████▋    | 286/500 [17:53<13:43,  3.85s/it]Running Inference:  57%|█████▋    | 287/500 [17:58<14:28,  4.08s/it]Running Inference:  58%|█████▊    | 288/500 [18:02<14:21,  4.06s/it]Running Inference:  58%|█████▊    | 289/500 [18:05<14:01,  3.99s/it]Running Inference:  58%|█████▊    | 290/500 [18:09<13:54,  3.97s/it]Running Inference:  58%|█████▊    | 291/500 [18:13<13:52,  3.98s/it]Running Inference:  58%|█████▊    | 292/500 [18:17<13:33,  3.91s/it]Running Inference:  59%|█████▊    | 293/500 [18:18<10:33,  3.06s/it]Running Inference:  59%|█████▉    | 294/500 [18:22<11:14,  3.27s/it]Running Inference:  59%|█████▉    | 295/500 [18:26<11:43,  3.43s/it]Running Inference:  59%|█████▉    | 296/500 [18:30<12:03,  3.54s/it]Running Inference:  59%|█████▉    | 297/500 [18:31<09:50,  2.91s/it]Running Inference:  60%|█████▉    | 298/500 [18:35<10:38,  3.16s/it]Running Inference:  60%|█████▉    | 299/500 [18:38<11:11,  3.34s/it]Running Inference:  60%|██████    | 300/500 [18:42<11:30,  3.45s/it]Running Inference:  60%|██████    | 301/500 [18:46<12:03,  3.64s/it]Running Inference:  60%|██████    | 302/500 [18:50<12:10,  3.69s/it]Running Inference:  61%|██████    | 303/500 [18:54<12:14,  3.73s/it]Running Inference:  61%|██████    | 304/500 [18:58<12:45,  3.90s/it]Running Inference:  61%|██████    | 305/500 [19:02<12:31,  3.85s/it]Running Inference:  61%|██████    | 306/500 [19:06<12:51,  3.97s/it]Running Inference:  61%|██████▏   | 307/500 [19:10<12:38,  3.93s/it]Running Inference:  62%|██████▏   | 308/500 [19:14<12:24,  3.88s/it]Running Inference:  62%|██████▏   | 309/500 [19:18<12:13,  3.84s/it]Running Inference:  62%|██████▏   | 310/500 [19:21<11:30,  3.64s/it]Running Inference:  62%|██████▏   | 311/500 [19:25<11:43,  3.72s/it]Running Inference:  62%|██████▏   | 312/500 [19:28<11:44,  3.75s/it]Running Inference:  63%|██████▎   | 313/500 [19:32<11:50,  3.80s/it]Running Inference:  63%|██████▎   | 314/500 [19:36<11:43,  3.78s/it]Running Inference:  63%|██████▎   | 315/500 [19:40<11:36,  3.76s/it]Running Inference:  63%|██████▎   | 316/500 [19:44<11:29,  3.75s/it]Running Inference:  63%|██████▎   | 317/500 [19:47<11:25,  3.75s/it]Running Inference:  64%|██████▎   | 318/500 [19:51<11:23,  3.76s/it]Running Inference:  64%|██████▍   | 319/500 [19:55<11:16,  3.74s/it]Running Inference:  64%|██████▍   | 320/500 [19:59<11:45,  3.92s/it]Running Inference:  64%|██████▍   | 321/500 [20:03<11:51,  3.97s/it]Running Inference:  64%|██████▍   | 322/500 [20:07<11:39,  3.93s/it]Running Inference:  65%|██████▍   | 323/500 [20:10<11:00,  3.73s/it]Running Inference:  65%|██████▍   | 324/500 [20:14<11:01,  3.76s/it]Running Inference:  65%|██████▌   | 325/500 [20:18<11:00,  3.77s/it]Running Inference:  65%|██████▌   | 326/500 [20:22<11:19,  3.91s/it]Running Inference:  65%|██████▌   | 327/500 [20:27<11:48,  4.10s/it]Running Inference:  66%|██████▌   | 328/500 [20:31<11:56,  4.17s/it]Running Inference:  66%|██████▌   | 329/500 [20:35<11:34,  4.06s/it]Running Inference:  66%|██████▌   | 330/500 [20:39<11:16,  3.98s/it]Running Inference:  66%|██████▌   | 331/500 [20:43<11:39,  4.14s/it]Running Inference:  66%|██████▋   | 332/500 [20:47<11:30,  4.11s/it]Running Inference:  67%|██████▋   | 333/500 [20:51<11:11,  4.02s/it]Running Inference:  67%|██████▋   | 334/500 [20:55<10:57,  3.96s/it]Running Inference:  67%|██████▋   | 335/500 [20:59<10:46,  3.92s/it]Running Inference:  67%|██████▋   | 336/500 [21:02<10:35,  3.87s/it]Running Inference:  67%|██████▋   | 337/500 [21:06<10:27,  3.85s/it]Running Inference:  68%|██████▊   | 338/500 [21:10<10:22,  3.85s/it]Running Inference:  68%|██████▊   | 339/500 [21:14<10:36,  3.95s/it]Running Inference:  68%|██████▊   | 340/500 [21:20<12:03,  4.52s/it]Running Inference:  68%|██████▊   | 341/500 [21:24<11:24,  4.31s/it]Running Inference:  68%|██████▊   | 342/500 [21:28<10:58,  4.17s/it]Running Inference:  69%|██████▊   | 343/500 [21:32<10:53,  4.16s/it]Running Inference:  69%|██████▉   | 344/500 [21:36<10:45,  4.14s/it]Running Inference:  69%|██████▉   | 345/500 [21:40<10:30,  4.07s/it]Running Inference:  69%|██████▉   | 346/500 [21:44<10:22,  4.05s/it]Running Inference:  69%|██████▉   | 347/500 [21:48<10:11,  4.00s/it]Running Inference:  70%|██████▉   | 348/500 [21:52<10:01,  3.96s/it]Running Inference:  70%|██████▉   | 349/500 [21:55<09:52,  3.92s/it]Running Inference:  70%|███████   | 350/500 [22:01<11:21,  4.54s/it]Running Inference:  70%|███████   | 351/500 [22:06<11:05,  4.47s/it]Running Inference:  70%|███████   | 352/500 [22:09<10:28,  4.25s/it]Running Inference:  71%|███████   | 353/500 [22:13<10:03,  4.10s/it]Running Inference:  71%|███████   | 354/500 [22:17<09:51,  4.05s/it]Running Inference:  71%|███████   | 355/500 [22:24<11:37,  4.81s/it]Running Inference:  71%|███████   | 356/500 [22:28<10:49,  4.51s/it]Running Inference:  71%|███████▏  | 357/500 [22:32<10:31,  4.42s/it]Running Inference:  72%|███████▏  | 358/500 [22:36<10:01,  4.23s/it]Running Inference:  72%|███████▏  | 359/500 [22:39<09:36,  4.09s/it]Running Inference:  72%|███████▏  | 360/500 [22:43<09:20,  4.00s/it]Running Inference:  72%|███████▏  | 361/500 [22:47<09:11,  3.97s/it]Running Inference:  72%|███████▏  | 362/500 [22:51<09:08,  3.98s/it]Running Inference:  73%|███████▎  | 363/500 [22:53<07:44,  3.39s/it]Running Inference:  73%|███████▎  | 364/500 [22:56<07:42,  3.40s/it]Running Inference:  73%|███████▎  | 365/500 [23:01<08:42,  3.87s/it]Running Inference:  73%|███████▎  | 366/500 [23:06<08:53,  3.98s/it]Running Inference:  73%|███████▎  | 367/500 [23:10<08:45,  3.95s/it]Running Inference:  74%|███████▎  | 368/500 [23:13<08:43,  3.96s/it]Running Inference:  74%|███████▍  | 369/500 [23:18<08:47,  4.02s/it]Running Inference:  74%|███████▍  | 370/500 [23:22<08:56,  4.13s/it]Running Inference:  74%|███████▍  | 371/500 [23:26<08:40,  4.04s/it]Running Inference:  74%|███████▍  | 372/500 [23:30<08:34,  4.02s/it]Running Inference:  75%|███████▍  | 373/500 [23:34<08:24,  3.97s/it]Running Inference:  75%|███████▍  | 374/500 [23:38<08:18,  3.96s/it]Running Inference:  75%|███████▌  | 375/500 [23:41<08:07,  3.90s/it]Running Inference:  75%|███████▌  | 376/500 [23:45<08:02,  3.89s/it]Running Inference:  75%|███████▌  | 377/500 [23:49<07:56,  3.88s/it]Running Inference:  76%|███████▌  | 378/500 [23:53<07:54,  3.89s/it]Running Inference:  76%|███████▌  | 379/500 [23:57<07:55,  3.93s/it]Running Inference:  76%|███████▌  | 380/500 [24:01<07:47,  3.89s/it]Running Inference:  76%|███████▌  | 381/500 [24:05<07:40,  3.87s/it]Running Inference:  76%|███████▋  | 382/500 [24:09<07:48,  3.97s/it]Running Inference:  77%|███████▋  | 383/500 [24:13<07:45,  3.98s/it]Running Inference:  77%|███████▋  | 384/500 [24:17<07:35,  3.93s/it]Running Inference:  77%|███████▋  | 385/500 [24:21<07:35,  3.96s/it]Running Inference:  77%|███████▋  | 386/500 [24:25<07:32,  3.97s/it]Running Inference:  77%|███████▋  | 387/500 [24:29<07:27,  3.96s/it]Running Inference:  78%|███████▊  | 388/500 [24:32<07:18,  3.91s/it]Running Inference:  78%|███████▊  | 389/500 [24:36<07:14,  3.91s/it]Running Inference:  78%|███████▊  | 390/500 [24:40<06:54,  3.77s/it]Running Inference:  78%|███████▊  | 391/500 [24:44<06:52,  3.78s/it]Running Inference:  78%|███████▊  | 392/500 [24:46<06:12,  3.45s/it]Running Inference:  79%|███████▊  | 393/500 [24:50<06:18,  3.53s/it]Running Inference:  79%|███████▉  | 394/500 [24:54<06:24,  3.63s/it]Running Inference:  79%|███████▉  | 395/500 [24:58<06:30,  3.72s/it]Running Inference:  79%|███████▉  | 396/500 [25:02<06:27,  3.73s/it]Running Inference:  79%|███████▉  | 397/500 [25:06<06:35,  3.84s/it]Running Inference:  80%|███████▉  | 398/500 [25:09<06:28,  3.81s/it]Running Inference:  80%|███████▉  | 399/500 [25:13<06:23,  3.80s/it]Running Inference:  80%|████████  | 400/500 [25:17<06:17,  3.78s/it]Running Inference:  80%|████████  | 401/500 [25:21<06:14,  3.78s/it]Running Inference:  80%|████████  | 402/500 [25:24<06:10,  3.78s/it]Running Inference:  81%|████████  | 403/500 [25:28<06:06,  3.78s/it]Running Inference:  81%|████████  | 404/500 [25:32<06:02,  3.77s/it]Running Inference:  81%|████████  | 405/500 [25:36<06:00,  3.80s/it]Running Inference:  81%|████████  | 406/500 [25:40<05:56,  3.79s/it]Running Inference:  81%|████████▏ | 407/500 [25:43<05:52,  3.79s/it]Running Inference:  82%|████████▏ | 408/500 [25:47<05:49,  3.80s/it]Running Inference:  82%|████████▏ | 409/500 [25:51<05:45,  3.80s/it]Running Inference:  82%|████████▏ | 410/500 [25:55<05:43,  3.81s/it]Running Inference:  82%|████████▏ | 411/500 [25:59<05:40,  3.83s/it]Running Inference:  82%|████████▏ | 412/500 [26:03<05:36,  3.83s/it]Running Inference:  83%|████████▎ | 413/500 [26:06<05:31,  3.81s/it]Running Inference:  83%|████████▎ | 414/500 [26:10<05:27,  3.81s/it]Running Inference:  83%|████████▎ | 415/500 [26:14<05:23,  3.81s/it]Running Inference:  83%|████████▎ | 416/500 [26:18<05:17,  3.78s/it]Running Inference:  83%|████████▎ | 417/500 [26:21<05:13,  3.77s/it]Running Inference:  84%|████████▎ | 418/500 [26:25<05:10,  3.79s/it]Running Inference:  84%|████████▍ | 419/500 [26:29<05:06,  3.78s/it]Running Inference:  84%|████████▍ | 420/500 [26:33<05:01,  3.77s/it]Running Inference:  84%|████████▍ | 421/500 [26:37<04:58,  3.77s/it]Running Inference:  84%|████████▍ | 422/500 [26:40<04:53,  3.76s/it]Running Inference:  85%|████████▍ | 423/500 [26:44<04:49,  3.76s/it]Running Inference:  85%|████████▍ | 424/500 [26:48<04:46,  3.77s/it]Running Inference:  85%|████████▌ | 425/500 [26:52<04:49,  3.86s/it]Running Inference:  85%|████████▌ | 426/500 [26:56<04:44,  3.84s/it]Running Inference:  85%|████████▌ | 427/500 [26:59<04:37,  3.80s/it]Running Inference:  86%|████████▌ | 428/500 [27:01<03:57,  3.30s/it]Running Inference:  86%|████████▌ | 429/500 [27:05<04:04,  3.44s/it]Running Inference:  86%|████████▌ | 430/500 [27:09<04:16,  3.66s/it]Running Inference:  86%|████████▌ | 431/500 [27:13<04:16,  3.72s/it]Running Inference:  86%|████████▋ | 432/500 [27:17<04:20,  3.83s/it]Running Inference:  87%|████████▋ | 433/500 [27:21<04:21,  3.90s/it]Running Inference:  87%|████████▋ | 434/500 [27:25<04:19,  3.93s/it]Running Inference:  87%|████████▋ | 435/500 [27:29<04:12,  3.89s/it]Running Inference:  87%|████████▋ | 436/500 [27:33<04:08,  3.88s/it]Running Inference:  87%|████████▋ | 437/500 [27:37<04:03,  3.86s/it]Running Inference:  88%|████████▊ | 438/500 [27:41<03:57,  3.83s/it]Running Inference:  88%|████████▊ | 439/500 [27:45<03:54,  3.85s/it]Running Inference:  88%|████████▊ | 440/500 [27:48<03:50,  3.84s/it]Running Inference:  88%|████████▊ | 441/500 [27:52<03:44,  3.80s/it]Running Inference:  88%|████████▊ | 442/500 [27:56<03:39,  3.78s/it]Running Inference:  89%|████████▊ | 443/500 [28:00<03:35,  3.78s/it]Running Inference:  89%|████████▉ | 444/500 [28:04<03:33,  3.82s/it]Running Inference:  89%|████████▉ | 445/500 [28:07<03:31,  3.85s/it]Running Inference:  89%|████████▉ | 446/500 [28:11<03:29,  3.88s/it]Running Inference:  89%|████████▉ | 447/500 [28:15<03:24,  3.85s/it]Running Inference:  90%|████████▉ | 448/500 [28:19<03:20,  3.86s/it]Running Inference:  90%|████████▉ | 449/500 [28:23<03:16,  3.85s/it]Running Inference:  90%|█████████ | 450/500 [28:24<02:30,  3.00s/it]Running Inference:  90%|█████████ | 451/500 [28:28<02:38,  3.24s/it]Running Inference:  90%|█████████ | 452/500 [28:32<02:44,  3.42s/it]Running Inference:  91%|█████████ | 453/500 [28:35<02:47,  3.57s/it]Running Inference:  91%|█████████ | 454/500 [28:39<02:47,  3.63s/it]Running Inference:  91%|█████████ | 455/500 [28:43<02:44,  3.66s/it]Running Inference:  91%|█████████ | 456/500 [28:47<02:42,  3.69s/it]Running Inference:  91%|█████████▏| 457/500 [28:51<02:40,  3.73s/it]Running Inference:  92%|█████████▏| 458/500 [28:54<02:37,  3.75s/it]Running Inference:  92%|█████████▏| 459/500 [28:56<02:04,  3.04s/it]Running Inference:  92%|█████████▏| 460/500 [29:00<02:14,  3.37s/it]Running Inference:  92%|█████████▏| 461/500 [29:04<02:17,  3.53s/it]Running Inference:  92%|█████████▏| 462/500 [29:08<02:22,  3.74s/it]Running Inference:  93%|█████████▎| 463/500 [29:12<02:18,  3.74s/it]Running Inference:  93%|█████████▎| 464/500 [29:15<02:14,  3.74s/it]Running Inference:  93%|█████████▎| 465/500 [29:19<02:10,  3.74s/it]Running Inference:  93%|█████████▎| 466/500 [29:23<02:08,  3.77s/it]Running Inference:  93%|█████████▎| 467/500 [29:27<02:03,  3.75s/it]Running Inference:  94%|█████████▎| 468/500 [29:31<02:02,  3.82s/it]Running Inference:  94%|█████████▍| 469/500 [29:34<01:57,  3.80s/it]Running Inference:  94%|█████████▍| 470/500 [29:38<01:53,  3.79s/it]Running Inference:  94%|█████████▍| 471/500 [29:42<01:50,  3.82s/it]Running Inference:  94%|█████████▍| 472/500 [29:46<01:46,  3.81s/it]Running Inference:  95%|█████████▍| 473/500 [29:50<01:45,  3.89s/it]Running Inference:  95%|█████████▍| 474/500 [29:54<01:40,  3.87s/it]Running Inference:  95%|█████████▌| 475/500 [29:58<01:36,  3.85s/it]Running Inference:  95%|█████████▌| 476/500 [30:01<01:32,  3.84s/it]Running Inference:  95%|█████████▌| 477/500 [30:05<01:28,  3.83s/it]Running Inference:  96%|█████████▌| 478/500 [30:07<01:10,  3.21s/it]Running Inference:  96%|█████████▌| 479/500 [30:12<01:17,  3.71s/it]Running Inference:  96%|█████████▌| 480/500 [30:16<01:15,  3.77s/it]Running Inference:  96%|█████████▌| 481/500 [30:20<01:16,  4.02s/it]Running Inference:  96%|█████████▋| 482/500 [30:24<01:10,  3.94s/it]Running Inference:  97%|█████████▋| 483/500 [30:28<01:06,  3.90s/it]Running Inference:  97%|█████████▋| 484/500 [30:32<01:03,  3.94s/it]Running Inference:  97%|█████████▋| 485/500 [30:37<01:04,  4.32s/it]Running Inference:  97%|█████████▋| 486/500 [30:41<00:58,  4.15s/it]Running Inference:  97%|█████████▋| 487/500 [30:45<00:52,  4.05s/it]Running Inference:  98%|█████████▊| 488/500 [30:48<00:47,  3.94s/it]Running Inference:  98%|█████████▊| 489/500 [30:52<00:43,  3.93s/it]Running Inference:  98%|█████████▊| 490/500 [30:53<00:29,  2.96s/it]Running Inference:  98%|█████████▊| 491/500 [30:57<00:28,  3.21s/it]Running Inference:  98%|█████████▊| 492/500 [31:01<00:27,  3.42s/it]Running Inference:  99%|█████████▊| 493/500 [31:05<00:24,  3.52s/it]Running Inference:  99%|█████████▉| 494/500 [31:08<00:21,  3.64s/it]Running Inference:  99%|█████████▉| 495/500 [31:12<00:18,  3.70s/it]Running Inference:  99%|█████████▉| 496/500 [31:16<00:15,  3.76s/it]Running Inference:  99%|█████████▉| 497/500 [31:20<00:11,  3.77s/it]Running Inference: 100%|█████████▉| 498/500 [31:24<00:07,  3.80s/it]Running Inference: 100%|█████████▉| 499/500 [31:28<00:03,  3.79s/it]Running Inference: 100%|██████████| 500/500 [31:32<00:00,  3.88s/it]Running Inference: 100%|██████████| 500/500 [31:32<00:00,  3.78s/it]
2025-12-14 20:13:23,773 - INFO - Inference completed.
2025-12-14 20:13:23,788 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 20:13:23,788 - INFO - Calculating metrics for dataset: longbench
2025-12-14 20:13:23,790 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 20:13:23,790 - INFO - Metrics:
38.18
2025-12-14 20:13:23,792 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc, ratio=0.3) on GPU 1

----------------------------------------
Task: lcc | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 20:13:30,267 - INFO - Set deterministic seeds to 42
2025-12-14 20:13:30,267 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 20:13:30,267 - INFO - Starting evaluation run...
2025-12-14 20:13:30,267 - INFO - Output directory set to: longbenchresult
2025-12-14 20:13:30,267 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 20:13:30,267 - INFO - KV Press 'tova' setup.
2025-12-14 20:13:30,267 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 20:13:30,267 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.88it/s]
Device set to use cuda:0
2025-12-14 20:13:46,574 - INFO - Model pipeline loaded.
2025-12-14 20:13:46,574 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 20:13:51,069 - INFO - Dataset loaded with 500 entries.
2025-12-14 20:13:51,069 - INFO - Dataset processed with 500 entries.
2025-12-14 20:13:51,083 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<40:45,  4.90s/it]Running Inference:   0%|          | 2/500 [00:08<35:46,  4.31s/it]Running Inference:   1%|          | 3/500 [00:13<35:47,  4.32s/it]Running Inference:   1%|          | 4/500 [00:17<34:17,  4.15s/it]Running Inference:   1%|          | 5/500 [00:20<33:28,  4.06s/it]Running Inference:   1%|          | 6/500 [00:24<33:28,  4.07s/it]Running Inference:   1%|▏         | 7/500 [00:28<33:06,  4.03s/it]Running Inference:   2%|▏         | 8/500 [00:32<32:31,  3.97s/it]Running Inference:   2%|▏         | 9/500 [00:36<32:10,  3.93s/it]Running Inference:   2%|▏         | 10/500 [00:40<31:51,  3.90s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:44<32:48,  4.02s/it]Running Inference:   2%|▏         | 12/500 [00:47<28:46,  3.54s/it]Running Inference:   3%|▎         | 13/500 [00:51<29:31,  3.64s/it]Running Inference:   3%|▎         | 14/500 [00:54<30:02,  3.71s/it]Running Inference:   3%|▎         | 15/500 [00:58<30:21,  3.76s/it]Running Inference:   3%|▎         | 16/500 [01:02<30:29,  3.78s/it]Running Inference:   3%|▎         | 17/500 [01:07<32:24,  4.03s/it]Running Inference:   4%|▎         | 18/500 [01:11<32:09,  4.00s/it]Running Inference:   4%|▍         | 19/500 [01:15<31:50,  3.97s/it]Running Inference:   4%|▍         | 20/500 [01:18<31:30,  3.94s/it]Running Inference:   4%|▍         | 21/500 [01:22<31:16,  3.92s/it]Running Inference:   4%|▍         | 22/500 [01:26<31:04,  3.90s/it]Running Inference:   5%|▍         | 23/500 [01:30<30:50,  3.88s/it]Running Inference:   5%|▍         | 24/500 [01:34<30:40,  3.87s/it]Running Inference:   5%|▌         | 25/500 [01:38<30:40,  3.88s/it]Running Inference:   5%|▌         | 26/500 [01:42<30:45,  3.89s/it]Running Inference:   5%|▌         | 27/500 [01:46<30:40,  3.89s/it]Running Inference:   6%|▌         | 28/500 [01:51<33:43,  4.29s/it]Running Inference:   6%|▌         | 29/500 [01:55<33:14,  4.24s/it]Running Inference:   6%|▌         | 30/500 [01:59<33:47,  4.31s/it]Running Inference:   6%|▌         | 31/500 [02:03<32:54,  4.21s/it]Running Inference:   6%|▋         | 32/500 [02:07<32:12,  4.13s/it]Running Inference:   7%|▋         | 33/500 [02:11<31:30,  4.05s/it]Running Inference:   7%|▋         | 34/500 [02:15<30:58,  3.99s/it]Running Inference:   7%|▋         | 35/500 [02:19<30:32,  3.94s/it]Running Inference:   7%|▋         | 36/500 [02:23<31:17,  4.05s/it]Running Inference:   7%|▋         | 37/500 [02:27<31:47,  4.12s/it]Running Inference:   8%|▊         | 38/500 [02:31<31:12,  4.05s/it]Running Inference:   8%|▊         | 39/500 [02:36<31:41,  4.13s/it]Running Inference:   8%|▊         | 40/500 [02:39<31:00,  4.05s/it]Running Inference:   8%|▊         | 41/500 [02:43<30:40,  4.01s/it]Running Inference:   8%|▊         | 42/500 [02:48<32:14,  4.22s/it]Running Inference:   9%|▊         | 43/500 [02:53<33:05,  4.34s/it]Running Inference:   9%|▉         | 44/500 [02:57<32:07,  4.23s/it]Running Inference:   9%|▉         | 45/500 [03:01<31:25,  4.14s/it]Running Inference:   9%|▉         | 46/500 [03:04<30:35,  4.04s/it]Running Inference:   9%|▉         | 47/500 [03:08<30:21,  4.02s/it]Running Inference:  10%|▉         | 48/500 [03:12<29:48,  3.96s/it]Running Inference:  10%|▉         | 49/500 [03:16<29:53,  3.98s/it]Running Inference:  10%|█         | 50/500 [03:21<31:15,  4.17s/it]Running Inference:  10%|█         | 51/500 [03:25<30:30,  4.08s/it]Running Inference:  10%|█         | 52/500 [03:29<30:00,  4.02s/it]Running Inference:  11%|█         | 53/500 [03:33<29:47,  4.00s/it]Running Inference:  11%|█         | 54/500 [03:36<29:29,  3.97s/it]Running Inference:  11%|█         | 55/500 [03:40<29:09,  3.93s/it]Running Inference:  11%|█         | 56/500 [03:44<28:58,  3.91s/it]Running Inference:  11%|█▏        | 57/500 [03:53<38:46,  5.25s/it]Running Inference:  12%|█▏        | 58/500 [03:56<35:31,  4.82s/it]Running Inference:  12%|█▏        | 59/500 [03:57<25:57,  3.53s/it]Running Inference:  12%|█▏        | 60/500 [04:01<26:35,  3.63s/it]Running Inference:  12%|█▏        | 61/500 [04:05<27:07,  3.71s/it]Running Inference:  12%|█▏        | 62/500 [04:06<22:53,  3.14s/it]Running Inference:  13%|█▎        | 63/500 [04:10<24:27,  3.36s/it]Running Inference:  13%|█▎        | 64/500 [04:12<19:53,  2.74s/it]Running Inference:  13%|█▎        | 65/500 [04:15<22:17,  3.07s/it]Running Inference:  13%|█▎        | 66/500 [04:20<24:35,  3.40s/it]Running Inference:  13%|█▎        | 67/500 [04:24<25:40,  3.56s/it]Running Inference:  14%|█▎        | 68/500 [04:26<23:20,  3.24s/it]Running Inference:  14%|█▍        | 69/500 [04:30<24:36,  3.43s/it]Running Inference:  14%|█▍        | 70/500 [04:32<21:55,  3.06s/it]Running Inference:  14%|█▍        | 71/500 [04:35<21:52,  3.06s/it]Running Inference:  14%|█▍        | 72/500 [04:39<23:01,  3.23s/it]Running Inference:  15%|█▍        | 73/500 [04:43<24:16,  3.41s/it]Running Inference:  15%|█▍        | 74/500 [04:47<25:18,  3.56s/it]Running Inference:  15%|█▌        | 75/500 [04:51<26:26,  3.73s/it]Running Inference:  15%|█▌        | 76/500 [04:55<26:58,  3.82s/it]Running Inference:  15%|█▌        | 77/500 [04:59<27:07,  3.85s/it]Running Inference:  16%|█▌        | 78/500 [05:02<26:44,  3.80s/it]Running Inference:  16%|█▌        | 79/500 [05:04<22:09,  3.16s/it]Running Inference:  16%|█▌        | 80/500 [05:08<23:38,  3.38s/it]Running Inference:  16%|█▌        | 81/500 [05:12<25:29,  3.65s/it]Running Inference:  16%|█▋        | 82/500 [05:17<26:58,  3.87s/it]Running Inference:  17%|█▋        | 83/500 [05:19<24:44,  3.56s/it]Running Inference:  17%|█▋        | 84/500 [05:23<25:25,  3.67s/it]Running Inference:  17%|█▋        | 85/500 [05:28<27:54,  4.04s/it]Running Inference:  17%|█▋        | 86/500 [05:32<27:26,  3.98s/it]Running Inference:  17%|█▋        | 87/500 [05:36<27:05,  3.94s/it]Running Inference:  18%|█▊        | 88/500 [05:40<26:53,  3.92s/it]Running Inference:  18%|█▊        | 89/500 [05:44<26:53,  3.93s/it]Running Inference:  18%|█▊        | 90/500 [05:48<27:22,  4.01s/it]Running Inference:  18%|█▊        | 91/500 [05:52<27:31,  4.04s/it]Running Inference:  18%|█▊        | 92/500 [05:56<27:16,  4.01s/it]Running Inference:  19%|█▊        | 93/500 [06:00<26:59,  3.98s/it]Running Inference:  19%|█▉        | 94/500 [06:04<26:39,  3.94s/it]Running Inference:  19%|█▉        | 95/500 [06:08<26:29,  3.93s/it]Running Inference:  19%|█▉        | 96/500 [06:12<26:29,  3.93s/it]Running Inference:  19%|█▉        | 97/500 [06:15<26:19,  3.92s/it]Running Inference:  20%|█▉        | 98/500 [06:19<25:54,  3.87s/it]Running Inference:  20%|█▉        | 99/500 [06:23<25:46,  3.86s/it]Running Inference:  20%|██        | 100/500 [06:27<26:27,  3.97s/it]Running Inference:  20%|██        | 101/500 [06:31<26:32,  3.99s/it]Running Inference:  20%|██        | 102/500 [06:35<26:09,  3.94s/it]Running Inference:  21%|██        | 103/500 [06:39<25:53,  3.91s/it]Running Inference:  21%|██        | 104/500 [06:45<29:41,  4.50s/it]Running Inference:  21%|██        | 105/500 [06:49<29:56,  4.55s/it]Running Inference:  21%|██        | 106/500 [06:55<31:34,  4.81s/it]Running Inference:  21%|██▏       | 107/500 [06:59<30:01,  4.58s/it]Running Inference:  22%|██▏       | 108/500 [07:03<28:33,  4.37s/it]Running Inference:  22%|██▏       | 109/500 [07:07<27:36,  4.24s/it]Running Inference:  22%|██▏       | 110/500 [07:11<27:00,  4.15s/it]Running Inference:  22%|██▏       | 111/500 [07:15<26:18,  4.06s/it]Running Inference:  22%|██▏       | 112/500 [07:18<25:53,  4.00s/it]Running Inference:  23%|██▎       | 113/500 [07:23<26:14,  4.07s/it]Running Inference:  23%|██▎       | 114/500 [07:28<28:25,  4.42s/it]Running Inference:  23%|██▎       | 115/500 [07:32<27:48,  4.33s/it]Running Inference:  23%|██▎       | 116/500 [07:36<27:29,  4.29s/it]Running Inference:  23%|██▎       | 117/500 [07:40<26:28,  4.15s/it]Running Inference:  24%|██▎       | 118/500 [07:44<27:02,  4.25s/it]Running Inference:  24%|██▍       | 119/500 [07:48<26:07,  4.11s/it]Running Inference:  24%|██▍       | 120/500 [07:53<26:16,  4.15s/it]Running Inference:  24%|██▍       | 121/500 [07:56<25:50,  4.09s/it]Running Inference:  24%|██▍       | 122/500 [07:59<22:16,  3.53s/it]Running Inference:  25%|██▍       | 123/500 [08:03<22:45,  3.62s/it]Running Inference:  25%|██▍       | 124/500 [08:06<23:04,  3.68s/it]Running Inference:  25%|██▌       | 125/500 [08:11<23:53,  3.82s/it]Running Inference:  25%|██▌       | 126/500 [08:14<23:50,  3.82s/it]Running Inference:  25%|██▌       | 127/500 [08:16<18:47,  3.02s/it]Running Inference:  26%|██▌       | 128/500 [08:20<20:37,  3.33s/it]Running Inference:  26%|██▌       | 129/500 [08:23<21:31,  3.48s/it]Running Inference:  26%|██▌       | 130/500 [08:27<22:21,  3.62s/it]Running Inference:  26%|██▌       | 131/500 [08:31<22:42,  3.69s/it]Running Inference:  26%|██▋       | 132/500 [08:35<22:53,  3.73s/it]Running Inference:  27%|██▋       | 133/500 [08:39<23:10,  3.79s/it]Running Inference:  27%|██▋       | 134/500 [08:43<23:17,  3.82s/it]Running Inference:  27%|██▋       | 135/500 [08:44<18:14,  3.00s/it]Running Inference:  27%|██▋       | 136/500 [08:48<19:54,  3.28s/it]Running Inference:  27%|██▋       | 137/500 [08:52<20:48,  3.44s/it]Running Inference:  28%|██▊       | 138/500 [08:56<21:43,  3.60s/it]Running Inference:  28%|██▊       | 139/500 [08:59<22:01,  3.66s/it]Running Inference:  28%|██▊       | 140/500 [09:03<22:31,  3.75s/it]Running Inference:  28%|██▊       | 141/500 [09:08<23:26,  3.92s/it]Running Inference:  28%|██▊       | 142/500 [09:12<23:43,  3.98s/it]Running Inference:  29%|██▊       | 143/500 [09:16<23:27,  3.94s/it]Running Inference:  29%|██▉       | 144/500 [09:20<23:11,  3.91s/it]Running Inference:  29%|██▉       | 145/500 [09:24<23:18,  3.94s/it]Running Inference:  29%|██▉       | 146/500 [09:28<23:29,  3.98s/it]Running Inference:  29%|██▉       | 147/500 [09:29<18:31,  3.15s/it]Running Inference:  30%|██▉       | 148/500 [09:31<16:30,  2.81s/it]Running Inference:  30%|██▉       | 149/500 [09:35<18:14,  3.12s/it]Running Inference:  30%|███       | 150/500 [09:38<19:22,  3.32s/it]Running Inference:  30%|███       | 151/500 [09:42<20:22,  3.50s/it]Running Inference:  30%|███       | 152/500 [09:43<15:16,  2.63s/it]Running Inference:  31%|███       | 153/500 [09:47<17:19,  3.00s/it]Running Inference:  31%|███       | 154/500 [09:51<18:43,  3.25s/it]Running Inference:  31%|███       | 155/500 [09:54<19:37,  3.41s/it]Running Inference:  31%|███       | 156/500 [09:58<20:20,  3.55s/it]Running Inference:  31%|███▏      | 157/500 [10:02<21:07,  3.70s/it]Running Inference:  32%|███▏      | 158/500 [10:06<21:24,  3.75s/it]Running Inference:  32%|███▏      | 159/500 [10:10<21:28,  3.78s/it]Running Inference:  32%|███▏      | 160/500 [10:14<21:29,  3.79s/it]Running Inference:  32%|███▏      | 161/500 [10:18<21:36,  3.82s/it]Running Inference:  32%|███▏      | 162/500 [10:22<21:48,  3.87s/it]Running Inference:  33%|███▎      | 163/500 [10:26<21:36,  3.85s/it]Running Inference:  33%|███▎      | 164/500 [10:29<21:35,  3.86s/it]Running Inference:  33%|███▎      | 165/500 [10:33<21:38,  3.88s/it]Running Inference:  33%|███▎      | 166/500 [10:36<19:38,  3.53s/it]Running Inference:  33%|███▎      | 167/500 [10:40<20:44,  3.74s/it]Running Inference:  34%|███▎      | 168/500 [10:41<15:40,  2.83s/it]Running Inference:  34%|███▍      | 169/500 [10:45<17:27,  3.16s/it]Running Inference:  34%|███▍      | 170/500 [10:49<18:30,  3.36s/it]Running Inference:  34%|███▍      | 171/500 [10:53<19:26,  3.55s/it]Running Inference:  34%|███▍      | 172/500 [10:57<19:50,  3.63s/it]Running Inference:  35%|███▍      | 173/500 [11:01<20:10,  3.70s/it]Running Inference:  35%|███▍      | 174/500 [11:05<20:49,  3.83s/it]Running Inference:  35%|███▌      | 175/500 [11:09<21:46,  4.02s/it]Running Inference:  35%|███▌      | 176/500 [11:13<21:29,  3.98s/it]Running Inference:  35%|███▌      | 177/500 [11:18<22:21,  4.15s/it]Running Inference:  36%|███▌      | 178/500 [11:21<21:42,  4.04s/it]Running Inference:  36%|███▌      | 179/500 [11:25<21:18,  3.98s/it]Running Inference:  36%|███▌      | 180/500 [11:29<21:09,  3.97s/it]Running Inference:  36%|███▌      | 181/500 [11:33<20:50,  3.92s/it]Running Inference:  36%|███▋      | 182/500 [11:37<20:35,  3.89s/it]Running Inference:  37%|███▋      | 183/500 [11:41<20:45,  3.93s/it]Running Inference:  37%|███▋      | 184/500 [11:45<20:43,  3.93s/it]Running Inference:  37%|███▋      | 185/500 [11:49<20:43,  3.95s/it]Running Inference:  37%|███▋      | 186/500 [11:50<16:13,  3.10s/it]Running Inference:  37%|███▋      | 187/500 [11:54<17:32,  3.36s/it]Running Inference:  38%|███▊      | 188/500 [11:58<18:12,  3.50s/it]Running Inference:  38%|███▊      | 189/500 [12:02<18:48,  3.63s/it]Running Inference:  38%|███▊      | 190/500 [12:06<19:39,  3.81s/it]Running Inference:  38%|███▊      | 191/500 [12:10<19:36,  3.81s/it]Running Inference:  38%|███▊      | 192/500 [12:13<19:43,  3.84s/it]Running Inference:  39%|███▊      | 193/500 [12:17<19:45,  3.86s/it]Running Inference:  39%|███▉      | 194/500 [12:21<19:43,  3.87s/it]Running Inference:  39%|███▉      | 195/500 [12:25<19:40,  3.87s/it]Running Inference:  39%|███▉      | 196/500 [12:29<19:44,  3.90s/it]Running Inference:  39%|███▉      | 197/500 [12:30<15:17,  3.03s/it]Running Inference:  40%|███▉      | 198/500 [12:34<16:23,  3.26s/it]Running Inference:  40%|███▉      | 199/500 [12:37<15:32,  3.10s/it]Running Inference:  40%|████      | 200/500 [12:41<16:44,  3.35s/it]Running Inference:  40%|████      | 201/500 [12:45<17:41,  3.55s/it]Running Inference:  40%|████      | 202/500 [12:48<18:03,  3.63s/it]Running Inference:  41%|████      | 203/500 [12:54<20:27,  4.13s/it]Running Inference:  41%|████      | 204/500 [12:58<20:02,  4.06s/it]Running Inference:  41%|████      | 205/500 [13:01<19:38,  3.99s/it]Running Inference:  41%|████      | 206/500 [13:04<17:04,  3.48s/it]Running Inference:  41%|████▏     | 207/500 [13:08<17:33,  3.60s/it]Running Inference:  42%|████▏     | 208/500 [13:12<18:24,  3.78s/it]Running Inference:  42%|████▏     | 209/500 [13:16<19:03,  3.93s/it]Running Inference:  42%|████▏     | 210/500 [13:20<18:53,  3.91s/it]Running Inference:  42%|████▏     | 211/500 [13:24<19:03,  3.96s/it]Running Inference:  42%|████▏     | 212/500 [13:28<18:47,  3.91s/it]Running Inference:  43%|████▎     | 213/500 [13:32<18:32,  3.88s/it]Running Inference:  43%|████▎     | 214/500 [13:35<18:29,  3.88s/it]Running Inference:  43%|████▎     | 215/500 [13:39<18:29,  3.89s/it]Running Inference:  43%|████▎     | 216/500 [13:43<18:21,  3.88s/it]Running Inference:  43%|████▎     | 217/500 [13:47<18:40,  3.96s/it]Running Inference:  44%|████▎     | 218/500 [13:51<18:44,  3.99s/it]Running Inference:  44%|████▍     | 219/500 [13:55<18:25,  3.93s/it]Running Inference:  44%|████▍     | 220/500 [13:59<18:16,  3.92s/it]Running Inference:  44%|████▍     | 221/500 [14:03<18:04,  3.89s/it]Running Inference:  44%|████▍     | 222/500 [14:07<17:53,  3.86s/it]Running Inference:  45%|████▍     | 223/500 [14:11<17:51,  3.87s/it]Running Inference:  45%|████▍     | 224/500 [14:14<16:32,  3.60s/it]Running Inference:  45%|████▌     | 225/500 [14:16<14:59,  3.27s/it]Running Inference:  45%|████▌     | 226/500 [14:20<15:38,  3.43s/it]Running Inference:  45%|████▌     | 227/500 [14:24<16:08,  3.55s/it]Running Inference:  46%|████▌     | 228/500 [14:28<16:26,  3.63s/it]Running Inference:  46%|████▌     | 229/500 [14:31<16:36,  3.68s/it]Running Inference:  46%|████▌     | 230/500 [14:35<16:46,  3.73s/it]Running Inference:  46%|████▌     | 231/500 [14:39<16:52,  3.76s/it]Running Inference:  46%|████▋     | 232/500 [14:43<16:54,  3.79s/it]Running Inference:  47%|████▋     | 233/500 [14:47<17:03,  3.83s/it]Running Inference:  47%|████▋     | 234/500 [14:51<17:03,  3.85s/it]Running Inference:  47%|████▋     | 235/500 [14:55<17:26,  3.95s/it]Running Inference:  47%|████▋     | 236/500 [14:59<17:11,  3.91s/it]Running Inference:  47%|████▋     | 237/500 [15:03<17:00,  3.88s/it]Running Inference:  48%|████▊     | 238/500 [15:06<16:54,  3.87s/it]Running Inference:  48%|████▊     | 239/500 [15:10<16:56,  3.90s/it]Running Inference:  48%|████▊     | 240/500 [15:14<16:51,  3.89s/it]Running Inference:  48%|████▊     | 241/500 [15:18<16:46,  3.89s/it]Running Inference:  48%|████▊     | 242/500 [15:22<16:35,  3.86s/it]Running Inference:  49%|████▊     | 243/500 [15:26<17:22,  4.06s/it]Running Inference:  49%|████▉     | 244/500 [15:31<17:37,  4.13s/it]Running Inference:  49%|████▉     | 245/500 [15:35<17:19,  4.08s/it]Running Inference:  49%|████▉     | 246/500 [15:38<16:51,  3.98s/it]Running Inference:  49%|████▉     | 247/500 [15:42<16:43,  3.97s/it]Running Inference:  50%|████▉     | 248/500 [15:46<16:30,  3.93s/it]Running Inference:  50%|████▉     | 249/500 [15:50<16:23,  3.92s/it]Running Inference:  50%|█████     | 250/500 [15:54<16:18,  3.91s/it]Running Inference:  50%|█████     | 251/500 [15:58<16:32,  3.99s/it]Running Inference:  50%|█████     | 252/500 [16:03<17:11,  4.16s/it]Running Inference:  51%|█████     | 253/500 [16:07<16:43,  4.06s/it]Running Inference:  51%|█████     | 254/500 [16:10<16:25,  4.00s/it]Running Inference:  51%|█████     | 255/500 [16:14<16:21,  4.01s/it]Running Inference:  51%|█████     | 256/500 [16:18<16:04,  3.95s/it]Running Inference:  51%|█████▏    | 257/500 [16:22<16:07,  3.98s/it]Running Inference:  52%|█████▏    | 258/500 [16:25<14:38,  3.63s/it]Running Inference:  52%|█████▏    | 259/500 [16:29<14:53,  3.71s/it]Running Inference:  52%|█████▏    | 260/500 [16:33<14:56,  3.73s/it]Running Inference:  52%|█████▏    | 261/500 [16:37<14:55,  3.75s/it]Running Inference:  52%|█████▏    | 262/500 [16:41<15:05,  3.81s/it]Running Inference:  53%|█████▎    | 263/500 [16:44<15:06,  3.83s/it]Running Inference:  53%|█████▎    | 264/500 [16:48<15:00,  3.82s/it]Running Inference:  53%|█████▎    | 265/500 [16:52<15:10,  3.87s/it]Running Inference:  53%|█████▎    | 266/500 [16:56<15:04,  3.86s/it]Running Inference:  53%|█████▎    | 267/500 [17:00<15:35,  4.01s/it]Running Inference:  54%|█████▎    | 268/500 [17:04<15:21,  3.97s/it]Running Inference:  54%|█████▍    | 269/500 [17:08<15:12,  3.95s/it]Running Inference:  54%|█████▍    | 270/500 [17:12<15:02,  3.93s/it]Running Inference:  54%|█████▍    | 271/500 [17:16<14:49,  3.88s/it]Running Inference:  54%|█████▍    | 272/500 [17:20<14:45,  3.89s/it]Running Inference:  55%|█████▍    | 273/500 [17:24<14:46,  3.91s/it]Running Inference:  55%|█████▍    | 274/500 [17:27<14:36,  3.88s/it]Running Inference:  55%|█████▌    | 275/500 [17:31<14:30,  3.87s/it]Running Inference:  55%|█████▌    | 276/500 [17:35<14:30,  3.89s/it]Running Inference:  55%|█████▌    | 277/500 [17:39<14:25,  3.88s/it]Running Inference:  56%|█████▌    | 278/500 [17:43<14:25,  3.90s/it]Running Inference:  56%|█████▌    | 279/500 [17:47<14:31,  3.94s/it]Running Inference:  56%|█████▌    | 280/500 [17:51<14:39,  4.00s/it]Running Inference:  56%|█████▌    | 281/500 [17:55<14:31,  3.98s/it]Running Inference:  56%|█████▋    | 282/500 [17:59<14:23,  3.96s/it]Running Inference:  57%|█████▋    | 283/500 [18:03<14:17,  3.95s/it]Running Inference:  57%|█████▋    | 284/500 [18:07<14:16,  3.97s/it]Running Inference:  57%|█████▋    | 285/500 [18:11<14:14,  3.97s/it]Running Inference:  57%|█████▋    | 286/500 [18:15<13:59,  3.92s/it]Running Inference:  57%|█████▋    | 287/500 [18:20<14:44,  4.15s/it]Running Inference:  58%|█████▊    | 288/500 [18:24<14:36,  4.13s/it]Running Inference:  58%|█████▊    | 289/500 [18:27<14:15,  4.05s/it]Running Inference:  58%|█████▊    | 290/500 [18:31<14:07,  4.03s/it]Running Inference:  58%|█████▊    | 291/500 [18:36<14:03,  4.04s/it]Running Inference:  58%|█████▊    | 292/500 [18:39<13:45,  3.97s/it]Running Inference:  59%|█████▊    | 293/500 [18:40<10:39,  3.09s/it]Running Inference:  59%|█████▉    | 294/500 [18:44<11:23,  3.32s/it]Running Inference:  59%|█████▉    | 295/500 [18:48<11:52,  3.48s/it]Running Inference:  59%|█████▉    | 296/500 [18:52<12:14,  3.60s/it]Running Inference:  59%|█████▉    | 297/500 [18:55<12:01,  3.55s/it]Running Inference:  60%|█████▉    | 298/500 [18:59<12:13,  3.63s/it]Running Inference:  60%|█████▉    | 299/500 [19:03<12:20,  3.68s/it]Running Inference:  60%|██████    | 300/500 [19:07<12:21,  3.71s/it]Running Inference:  60%|██████    | 301/500 [19:11<12:42,  3.83s/it]Running Inference:  60%|██████    | 302/500 [19:15<12:41,  3.85s/it]Running Inference:  61%|██████    | 303/500 [19:19<12:41,  3.86s/it]Running Inference:  61%|██████    | 304/500 [19:23<13:07,  4.02s/it]Running Inference:  61%|██████    | 305/500 [19:27<12:50,  3.95s/it]Running Inference:  61%|██████    | 306/500 [19:31<13:07,  4.06s/it]Running Inference:  61%|██████▏   | 307/500 [19:35<12:54,  4.01s/it]Running Inference:  62%|██████▏   | 308/500 [19:39<12:38,  3.95s/it]Running Inference:  62%|██████▏   | 309/500 [19:43<12:25,  3.90s/it]Running Inference:  62%|██████▏   | 310/500 [19:47<12:19,  3.89s/it]Running Inference:  62%|██████▏   | 311/500 [19:51<12:21,  3.92s/it]Running Inference:  62%|██████▏   | 312/500 [19:54<12:14,  3.91s/it]Running Inference:  63%|██████▎   | 313/500 [19:58<12:14,  3.93s/it]Running Inference:  63%|██████▎   | 314/500 [20:02<12:03,  3.89s/it]Running Inference:  63%|██████▎   | 315/500 [20:06<11:53,  3.86s/it]Running Inference:  63%|██████▎   | 316/500 [20:10<11:46,  3.84s/it]Running Inference:  63%|██████▎   | 317/500 [20:14<11:40,  3.83s/it]Running Inference:  64%|██████▎   | 318/500 [20:17<11:37,  3.83s/it]Running Inference:  64%|██████▍   | 319/500 [20:21<11:37,  3.85s/it]Running Inference:  64%|██████▍   | 320/500 [20:26<12:05,  4.03s/it]Running Inference:  64%|██████▍   | 321/500 [20:30<12:08,  4.07s/it]Running Inference:  64%|██████▍   | 322/500 [20:34<11:54,  4.01s/it]Running Inference:  65%|██████▍   | 323/500 [20:38<11:41,  3.96s/it]Running Inference:  65%|██████▍   | 324/500 [20:41<11:32,  3.94s/it]Running Inference:  65%|██████▌   | 325/500 [20:45<11:24,  3.91s/it]Running Inference:  65%|██████▌   | 326/500 [20:50<11:37,  4.01s/it]Running Inference:  65%|██████▌   | 327/500 [20:55<12:43,  4.41s/it]Running Inference:  66%|██████▌   | 328/500 [20:59<12:35,  4.39s/it]Running Inference:  66%|██████▌   | 329/500 [21:03<11:59,  4.21s/it]Running Inference:  66%|██████▌   | 330/500 [21:07<11:33,  4.08s/it]Running Inference:  66%|██████▌   | 331/500 [21:11<11:52,  4.22s/it]Running Inference:  66%|██████▋   | 332/500 [21:15<11:40,  4.17s/it]Running Inference:  67%|██████▋   | 333/500 [21:19<11:19,  4.07s/it]Running Inference:  67%|██████▋   | 334/500 [21:23<11:05,  4.01s/it]Running Inference:  67%|██████▋   | 335/500 [21:27<10:53,  3.96s/it]Running Inference:  67%|██████▋   | 336/500 [21:31<10:41,  3.91s/it]Running Inference:  67%|██████▋   | 337/500 [21:35<10:34,  3.89s/it]Running Inference:  68%|██████▊   | 338/500 [21:39<10:29,  3.88s/it]Running Inference:  68%|██████▊   | 339/500 [21:43<10:42,  3.99s/it]Running Inference:  68%|██████▊   | 340/500 [21:49<12:08,  4.55s/it]Running Inference:  68%|██████▊   | 341/500 [21:52<11:30,  4.34s/it]Running Inference:  68%|██████▊   | 342/500 [21:56<11:04,  4.20s/it]Running Inference:  69%|██████▊   | 343/500 [22:01<10:58,  4.20s/it]Running Inference:  69%|██████▉   | 344/500 [22:05<10:52,  4.18s/it]Running Inference:  69%|██████▉   | 345/500 [22:09<10:37,  4.11s/it]Running Inference:  69%|██████▉   | 346/500 [22:13<10:29,  4.09s/it]Running Inference:  69%|██████▉   | 347/500 [22:17<10:17,  4.04s/it]Running Inference:  70%|██████▉   | 348/500 [22:20<10:08,  4.00s/it]Running Inference:  70%|██████▉   | 349/500 [22:24<09:58,  3.96s/it]Running Inference:  70%|███████   | 350/500 [22:30<11:26,  4.58s/it]Running Inference:  70%|███████   | 351/500 [22:35<11:10,  4.50s/it]Running Inference:  70%|███████   | 352/500 [22:38<10:35,  4.29s/it]Running Inference:  71%|███████   | 353/500 [22:42<10:09,  4.14s/it]Running Inference:  71%|███████   | 354/500 [22:46<09:57,  4.09s/it]Running Inference:  71%|███████   | 355/500 [22:53<11:42,  4.85s/it]Running Inference:  71%|███████   | 356/500 [22:57<10:54,  4.54s/it]Running Inference:  71%|███████▏  | 357/500 [23:01<10:36,  4.45s/it]Running Inference:  72%|███████▏  | 358/500 [23:05<10:05,  4.27s/it]Running Inference:  72%|███████▏  | 359/500 [23:09<09:40,  4.12s/it]Running Inference:  72%|███████▏  | 360/500 [23:12<09:25,  4.04s/it]Running Inference:  72%|███████▏  | 361/500 [23:16<09:16,  4.01s/it]Running Inference:  72%|███████▏  | 362/500 [23:20<09:13,  4.01s/it]Running Inference:  73%|███████▎  | 363/500 [23:24<09:07,  4.00s/it]Running Inference:  73%|███████▎  | 364/500 [23:28<09:03,  4.00s/it]Running Inference:  73%|███████▎  | 365/500 [23:33<09:40,  4.30s/it]Running Inference:  73%|███████▎  | 366/500 [23:38<09:35,  4.29s/it]Running Inference:  73%|███████▎  | 367/500 [23:42<09:16,  4.18s/it]Running Inference:  74%|███████▎  | 368/500 [23:46<09:06,  4.14s/it]Running Inference:  74%|███████▍  | 369/500 [23:50<09:04,  4.16s/it]Running Inference:  74%|███████▍  | 370/500 [23:54<09:09,  4.23s/it]Running Inference:  74%|███████▍  | 371/500 [23:58<08:51,  4.12s/it]Running Inference:  74%|███████▍  | 372/500 [24:02<08:42,  4.08s/it]Running Inference:  75%|███████▍  | 373/500 [24:06<08:30,  4.02s/it]Running Inference:  75%|███████▍  | 374/500 [24:10<08:23,  4.00s/it]Running Inference:  75%|███████▌  | 375/500 [24:14<08:12,  3.94s/it]Running Inference:  75%|███████▌  | 376/500 [24:15<06:50,  3.31s/it]Running Inference:  75%|███████▌  | 377/500 [24:19<07:07,  3.48s/it]Running Inference:  76%|███████▌  | 378/500 [24:23<07:20,  3.61s/it]Running Inference:  76%|███████▌  | 379/500 [24:27<07:33,  3.75s/it]Running Inference:  76%|███████▌  | 380/500 [24:31<07:33,  3.78s/it]Running Inference:  76%|███████▌  | 381/500 [24:35<07:35,  3.83s/it]Running Inference:  76%|███████▋  | 382/500 [24:39<07:45,  3.95s/it]Running Inference:  77%|███████▋  | 383/500 [24:43<07:44,  3.97s/it]Running Inference:  77%|███████▋  | 384/500 [24:47<07:24,  3.83s/it]Running Inference:  77%|███████▋  | 385/500 [24:51<07:29,  3.91s/it]Running Inference:  77%|███████▋  | 386/500 [24:55<07:31,  3.96s/it]Running Inference:  77%|███████▋  | 387/500 [24:59<07:28,  3.97s/it]Running Inference:  78%|███████▊  | 388/500 [25:03<07:19,  3.93s/it]Running Inference:  78%|███████▊  | 389/500 [25:07<07:16,  3.93s/it]Running Inference:  78%|███████▊  | 390/500 [25:10<06:58,  3.80s/it]Running Inference:  78%|███████▊  | 391/500 [25:14<06:56,  3.82s/it]Running Inference:  78%|███████▊  | 392/500 [25:15<05:10,  2.88s/it]Running Inference:  79%|███████▊  | 393/500 [25:19<05:36,  3.14s/it]Running Inference:  79%|███████▉  | 394/500 [25:23<05:57,  3.37s/it]Running Inference:  79%|███████▉  | 395/500 [25:26<06:12,  3.55s/it]Running Inference:  79%|███████▉  | 396/500 [25:30<06:16,  3.62s/it]Running Inference:  79%|███████▉  | 397/500 [25:34<06:29,  3.78s/it]Running Inference:  80%|███████▉  | 398/500 [25:38<06:25,  3.78s/it]Running Inference:  80%|███████▉  | 399/500 [25:42<06:21,  3.78s/it]Running Inference:  80%|████████  | 400/500 [25:46<06:17,  3.78s/it]Running Inference:  80%|████████  | 401/500 [25:50<06:14,  3.78s/it]Running Inference:  80%|████████  | 402/500 [25:53<06:10,  3.78s/it]Running Inference:  81%|████████  | 403/500 [25:57<06:07,  3.79s/it]Running Inference:  81%|████████  | 404/500 [26:01<06:03,  3.79s/it]Running Inference:  81%|████████  | 405/500 [26:05<06:02,  3.82s/it]Running Inference:  81%|████████  | 406/500 [26:09<05:58,  3.82s/it]Running Inference:  81%|████████▏ | 407/500 [26:12<05:55,  3.82s/it]Running Inference:  82%|████████▏ | 408/500 [26:16<05:51,  3.82s/it]Running Inference:  82%|████████▏ | 409/500 [26:20<05:48,  3.83s/it]Running Inference:  82%|████████▏ | 410/500 [26:24<05:46,  3.84s/it]Running Inference:  82%|████████▏ | 411/500 [26:28<05:44,  3.87s/it]Running Inference:  82%|████████▏ | 412/500 [26:32<05:37,  3.83s/it]Running Inference:  83%|████████▎ | 413/500 [26:35<05:32,  3.83s/it]Running Inference:  83%|████████▎ | 414/500 [26:39<05:29,  3.84s/it]Running Inference:  83%|████████▎ | 415/500 [26:43<05:25,  3.83s/it]Running Inference:  83%|████████▎ | 416/500 [26:47<05:19,  3.80s/it]Running Inference:  83%|████████▎ | 417/500 [26:51<05:15,  3.80s/it]Running Inference:  84%|████████▎ | 418/500 [26:55<05:13,  3.82s/it]Running Inference:  84%|████████▍ | 419/500 [26:58<05:08,  3.81s/it]Running Inference:  84%|████████▍ | 420/500 [27:00<04:17,  3.22s/it]Running Inference:  84%|████████▍ | 421/500 [27:04<04:28,  3.40s/it]Running Inference:  84%|████████▍ | 422/500 [27:08<04:34,  3.52s/it]Running Inference:  85%|████████▍ | 423/500 [27:12<04:36,  3.60s/it]Running Inference:  85%|████████▍ | 424/500 [27:15<04:38,  3.66s/it]Running Inference:  85%|████████▌ | 425/500 [27:20<04:44,  3.80s/it]Running Inference:  85%|████████▌ | 426/500 [27:23<04:41,  3.80s/it]Running Inference:  85%|████████▌ | 427/500 [27:27<04:36,  3.78s/it]Running Inference:  86%|████████▌ | 428/500 [27:31<04:32,  3.79s/it]Running Inference:  86%|████████▌ | 429/500 [27:35<04:28,  3.79s/it]Running Inference:  86%|████████▌ | 430/500 [27:39<04:34,  3.92s/it]Running Inference:  86%|████████▌ | 431/500 [27:43<04:29,  3.91s/it]Running Inference:  86%|████████▋ | 432/500 [27:47<04:30,  3.97s/it]Running Inference:  87%|████████▋ | 433/500 [27:51<04:28,  4.00s/it]Running Inference:  87%|████████▋ | 434/500 [27:55<04:24,  4.01s/it]Running Inference:  87%|████████▋ | 435/500 [27:59<04:16,  3.95s/it]Running Inference:  87%|████████▋ | 436/500 [28:03<04:12,  3.94s/it]Running Inference:  87%|████████▋ | 437/500 [28:07<04:06,  3.91s/it]Running Inference:  88%|████████▊ | 438/500 [28:10<04:00,  3.88s/it]Running Inference:  88%|████████▊ | 439/500 [28:14<03:57,  3.89s/it]Running Inference:  88%|████████▊ | 440/500 [28:18<03:53,  3.88s/it]Running Inference:  88%|████████▊ | 441/500 [28:22<03:47,  3.85s/it]Running Inference:  88%|████████▊ | 442/500 [28:26<03:41,  3.82s/it]Running Inference:  89%|████████▊ | 443/500 [28:29<03:37,  3.82s/it]Running Inference:  89%|████████▉ | 444/500 [28:33<03:35,  3.85s/it]Running Inference:  89%|████████▉ | 445/500 [28:37<03:33,  3.88s/it]Running Inference:  89%|████████▉ | 446/500 [28:41<03:30,  3.90s/it]Running Inference:  89%|████████▉ | 447/500 [28:45<03:25,  3.88s/it]Running Inference:  90%|████████▉ | 448/500 [28:49<03:22,  3.90s/it]Running Inference:  90%|████████▉ | 449/500 [28:53<03:18,  3.88s/it]Running Inference:  90%|█████████ | 450/500 [28:57<03:13,  3.88s/it]Running Inference:  90%|█████████ | 451/500 [29:01<03:09,  3.86s/it]Running Inference:  90%|█████████ | 452/500 [29:04<03:05,  3.86s/it]Running Inference:  91%|█████████ | 453/500 [29:08<03:02,  3.89s/it]Running Inference:  91%|█████████ | 454/500 [29:12<02:58,  3.88s/it]Running Inference:  91%|█████████ | 455/500 [29:16<02:53,  3.86s/it]Running Inference:  91%|█████████ | 456/500 [29:20<02:49,  3.85s/it]Running Inference:  91%|█████████▏| 457/500 [29:24<02:46,  3.87s/it]Running Inference:  92%|█████████▏| 458/500 [29:28<02:42,  3.87s/it]Running Inference:  92%|█████████▏| 459/500 [29:29<02:08,  3.13s/it]Running Inference:  92%|█████████▏| 460/500 [29:33<02:17,  3.44s/it]Running Inference:  92%|█████████▏| 461/500 [29:37<02:19,  3.59s/it]Running Inference:  92%|█████████▏| 462/500 [29:41<02:23,  3.79s/it]Running Inference:  93%|█████████▎| 463/500 [29:45<02:20,  3.79s/it]Running Inference:  93%|█████████▎| 464/500 [29:49<02:16,  3.78s/it]Running Inference:  93%|█████████▎| 465/500 [29:53<02:12,  3.78s/it]Running Inference:  93%|█████████▎| 466/500 [29:57<02:09,  3.82s/it]Running Inference:  93%|█████████▎| 467/500 [30:00<02:05,  3.80s/it]Running Inference:  94%|█████████▎| 468/500 [30:04<02:03,  3.86s/it]Running Inference:  94%|█████████▍| 469/500 [30:08<01:59,  3.84s/it]Running Inference:  94%|█████████▍| 470/500 [30:12<01:53,  3.77s/it]Running Inference:  94%|█████████▍| 471/500 [30:16<01:50,  3.83s/it]Running Inference:  94%|█████████▍| 472/500 [30:20<01:46,  3.82s/it]Running Inference:  95%|█████████▍| 473/500 [30:24<01:45,  3.91s/it]Running Inference:  95%|█████████▍| 474/500 [30:28<01:41,  3.90s/it]Running Inference:  95%|█████████▌| 475/500 [30:31<01:37,  3.88s/it]Running Inference:  95%|█████████▌| 476/500 [30:35<01:33,  3.88s/it]Running Inference:  95%|█████████▌| 477/500 [30:39<01:29,  3.87s/it]Running Inference:  96%|█████████▌| 478/500 [30:43<01:26,  3.92s/it]Running Inference:  96%|█████████▌| 479/500 [30:48<01:28,  4.22s/it]Running Inference:  96%|█████████▌| 480/500 [30:52<01:22,  4.14s/it]Running Inference:  96%|█████████▌| 481/500 [30:57<01:21,  4.28s/it]Running Inference:  96%|█████████▋| 482/500 [31:01<01:14,  4.13s/it]Running Inference:  97%|█████████▋| 483/500 [31:04<01:08,  4.05s/it]Running Inference:  97%|█████████▋| 484/500 [31:08<01:04,  4.06s/it]Running Inference:  97%|█████████▋| 485/500 [31:14<01:06,  4.42s/it]Running Inference:  97%|█████████▋| 486/500 [31:18<00:59,  4.23s/it]Running Inference:  97%|█████████▋| 487/500 [31:21<00:53,  4.12s/it]Running Inference:  98%|█████████▊| 488/500 [31:22<00:37,  3.16s/it]Running Inference:  98%|█████████▊| 489/500 [31:26<00:37,  3.39s/it]Running Inference:  98%|█████████▊| 490/500 [31:27<00:26,  2.60s/it]Running Inference:  98%|█████████▊| 491/500 [31:31<00:26,  2.97s/it]Running Inference:  98%|█████████▊| 492/500 [31:35<00:26,  3.27s/it]Running Inference:  99%|█████████▊| 493/500 [31:39<00:24,  3.43s/it]Running Inference:  99%|█████████▉| 494/500 [31:43<00:21,  3.59s/it]Running Inference:  99%|█████████▉| 495/500 [31:46<00:18,  3.68s/it]Running Inference:  99%|█████████▉| 496/500 [31:50<00:15,  3.75s/it]Running Inference:  99%|█████████▉| 497/500 [31:54<00:11,  3.77s/it]Running Inference: 100%|█████████▉| 498/500 [31:58<00:07,  3.81s/it]Running Inference: 100%|█████████▉| 499/500 [32:02<00:03,  3.81s/it]Running Inference: 100%|██████████| 500/500 [32:06<00:00,  3.91s/it]Running Inference: 100%|██████████| 500/500 [32:06<00:00,  3.85s/it]
2025-12-14 20:45:57,608 - INFO - Inference completed.
2025-12-14 20:45:57,623 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 20:45:57,623 - INFO - Calculating metrics for dataset: longbench
2025-12-14 20:45:57,625 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 20:45:57,625 - INFO - Metrics:
42.09
2025-12-14 20:45:57,626 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc, ratio=0.5) on GPU 1


========================================
LongBench Task: lsht
========================================
----------------------------------------
Task: lsht | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 20:46:07,467 - INFO - Set deterministic seeds to 42
2025-12-14 20:46:07,467 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 20:46:07,467 - INFO - Starting evaluation run...
2025-12-14 20:46:07,467 - INFO - Output directory set to: longbenchresult
2025-12-14 20:46:07,468 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 20:46:07,468 - INFO - KV Press 'tova' setup.
2025-12-14 20:46:07,468 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 20:46:07,468 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 95.70it/s]
Device set to use cuda:0
2025-12-14 20:46:38,041 - INFO - Model pipeline loaded.
2025-12-14 20:46:38,041 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 1085.60 examples/s]Generating test split: 200 examples [00:00, 1082.51 examples/s]
2025-12-14 20:47:01,705 - INFO - Dataset loaded with 200 entries.
2025-12-14 20:47:01,705 - INFO - Dataset processed with 200 entries.
2025-12-14 20:47:01,744 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:06,  2.15s/it]Running Inference:   1%|          | 2/200 [00:04<07:40,  2.33s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:30,  1.68s/it]Running Inference:   2%|▏         | 4/200 [00:06<05:12,  1.59s/it]Running Inference:   2%|▎         | 5/200 [00:09<06:42,  2.06s/it]Running Inference:   3%|▎         | 6/200 [00:12<06:45,  2.09s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:56,  2.16s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:55,  1.85s/it]Running Inference:   4%|▍         | 9/200 [00:17<06:10,  1.94s/it]Running Inference:   5%|▌         | 10/200 [00:18<05:26,  1.72s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:41,  1.81s/it]Running Inference:   6%|▌         | 12/200 [00:22<05:18,  1.69s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:55,  1.90s/it]Running Inference:   7%|▋         | 14/200 [00:26<05:24,  1.75s/it]Running Inference:   8%|▊         | 15/200 [00:29<06:33,  2.13s/it]Running Inference:   8%|▊         | 16/200 [00:30<05:45,  1.88s/it]Running Inference:   8%|▊         | 17/200 [00:37<10:09,  3.33s/it]Running Inference:   9%|▉         | 18/200 [00:39<09:11,  3.03s/it]Running Inference:  10%|▉         | 19/200 [00:40<07:30,  2.49s/it]Running Inference:  10%|█         | 20/200 [00:43<07:22,  2.46s/it]Running Inference:  10%|█         | 21/200 [00:45<07:05,  2.37s/it]Running Inference:  11%|█         | 22/200 [00:47<06:31,  2.20s/it]Running Inference:  12%|█▏        | 23/200 [00:48<05:29,  1.86s/it]Running Inference:  12%|█▏        | 24/200 [00:54<09:24,  3.21s/it]Running Inference:  12%|█▎        | 25/200 [00:55<07:14,  2.48s/it]Running Inference:  13%|█▎        | 26/200 [00:58<07:36,  2.62s/it]Running Inference:  14%|█▎        | 27/200 [01:01<07:45,  2.69s/it]Running Inference:  14%|█▍        | 28/200 [01:02<06:19,  2.20s/it]Running Inference:  14%|█▍        | 29/200 [01:03<05:24,  1.90s/it]Running Inference:  15%|█▌        | 30/200 [01:06<06:06,  2.15s/it]Running Inference:  16%|█▌        | 31/200 [01:12<09:22,  3.33s/it]Running Inference:  16%|█▌        | 32/200 [01:15<09:33,  3.42s/it]Running Inference:  16%|█▋        | 33/200 [01:18<09:01,  3.24s/it]Running Inference:  17%|█▋        | 34/200 [01:23<10:40,  3.86s/it]Running Inference:  18%|█▊        | 35/200 [01:28<10:58,  3.99s/it]Running Inference:  18%|█▊        | 36/200 [01:33<12:11,  4.46s/it]Running Inference:  18%|█▊        | 37/200 [01:35<10:18,  3.79s/it]Running Inference:  19%|█▉        | 38/200 [01:38<08:57,  3.32s/it]Running Inference:  20%|█▉        | 39/200 [01:40<07:43,  2.88s/it]Running Inference:  20%|██        | 40/200 [01:43<08:06,  3.04s/it]Running Inference:  20%|██        | 41/200 [01:46<08:10,  3.08s/it]Running Inference:  21%|██        | 42/200 [01:48<07:11,  2.73s/it]Running Inference:  22%|██▏       | 43/200 [01:50<06:10,  2.36s/it]Running Inference:  22%|██▏       | 44/200 [01:51<05:30,  2.12s/it]Running Inference:  22%|██▎       | 45/200 [01:53<05:05,  1.97s/it]Running Inference:  23%|██▎       | 46/200 [01:56<05:46,  2.25s/it]Running Inference:  24%|██▎       | 47/200 [01:57<05:17,  2.08s/it]Running Inference:  24%|██▍       | 48/200 [01:58<04:36,  1.82s/it]Running Inference:  24%|██▍       | 49/200 [02:05<07:57,  3.17s/it]Running Inference:  25%|██▌       | 50/200 [02:06<06:38,  2.66s/it]Running Inference:  26%|██▌       | 51/200 [02:07<05:19,  2.14s/it]Running Inference:  26%|██▌       | 52/200 [02:09<04:50,  1.96s/it]Running Inference:  26%|██▋       | 53/200 [02:10<04:31,  1.85s/it]Running Inference:  27%|██▋       | 54/200 [02:16<07:03,  2.90s/it]Running Inference:  28%|██▊       | 55/200 [02:23<09:52,  4.08s/it]Running Inference:  28%|██▊       | 56/200 [02:29<11:21,  4.73s/it]Running Inference:  28%|██▊       | 57/200 [02:31<09:45,  4.09s/it]Running Inference:  29%|██▉       | 58/200 [02:37<11:04,  4.68s/it]Running Inference:  30%|██▉       | 59/200 [02:39<08:28,  3.60s/it]Running Inference:  30%|███       | 60/200 [02:41<07:39,  3.28s/it]Running Inference:  30%|███       | 61/200 [02:42<05:51,  2.53s/it]Running Inference:  31%|███       | 62/200 [02:45<05:57,  2.59s/it]Running Inference:  32%|███▏      | 63/200 [02:46<04:57,  2.17s/it]Running Inference:  32%|███▏      | 64/200 [02:47<04:09,  1.84s/it]Running Inference:  32%|███▎      | 65/200 [02:48<03:29,  1.55s/it]Running Inference:  33%|███▎      | 66/200 [02:54<06:32,  2.93s/it]Running Inference:  34%|███▎      | 67/200 [02:55<05:12,  2.35s/it]Running Inference:  34%|███▍      | 68/200 [02:56<04:10,  1.90s/it]Running Inference:  34%|███▍      | 69/200 [02:58<04:32,  2.08s/it]Running Inference:  35%|███▌      | 70/200 [02:59<03:52,  1.79s/it]Running Inference:  36%|███▌      | 71/200 [03:00<03:09,  1.47s/it]Running Inference:  36%|███▌      | 72/200 [03:02<03:19,  1.56s/it]Running Inference:  36%|███▋      | 73/200 [03:04<03:48,  1.80s/it]Running Inference:  37%|███▋      | 74/200 [03:07<04:44,  2.26s/it]Running Inference:  38%|███▊      | 75/200 [03:09<04:15,  2.04s/it]Running Inference:  38%|███▊      | 76/200 [03:15<06:39,  3.22s/it]Running Inference:  38%|███▊      | 77/200 [03:17<05:35,  2.73s/it]Running Inference:  39%|███▉      | 78/200 [03:18<04:50,  2.38s/it]Running Inference:  40%|███▉      | 79/200 [03:19<03:58,  1.97s/it]Running Inference:  40%|████      | 80/200 [03:25<06:00,  3.00s/it]Running Inference:  40%|████      | 81/200 [03:27<05:24,  2.73s/it]Running Inference:  41%|████      | 82/200 [03:30<05:57,  3.03s/it]Running Inference:  42%|████▏     | 83/200 [03:32<05:10,  2.66s/it]Running Inference:  42%|████▏     | 84/200 [03:37<06:36,  3.42s/it]Running Inference:  42%|████▎     | 85/200 [03:40<06:08,  3.20s/it]Running Inference:  43%|████▎     | 86/200 [03:42<05:10,  2.72s/it]Running Inference:  44%|████▎     | 87/200 [03:47<06:41,  3.55s/it]Running Inference:  44%|████▍     | 88/200 [03:49<05:47,  3.10s/it]Running Inference:  44%|████▍     | 89/200 [03:50<04:42,  2.54s/it]Running Inference:  45%|████▌     | 90/200 [03:53<04:32,  2.48s/it]Running Inference:  46%|████▌     | 91/200 [03:55<04:09,  2.29s/it]Running Inference:  46%|████▌     | 92/200 [03:57<03:58,  2.21s/it]Running Inference:  46%|████▋     | 93/200 [04:03<05:55,  3.32s/it]Running Inference:  47%|████▋     | 94/200 [04:04<04:49,  2.73s/it]Running Inference:  48%|████▊     | 95/200 [04:10<06:41,  3.82s/it]Running Inference:  48%|████▊     | 96/200 [04:13<05:50,  3.37s/it]Running Inference:  48%|████▊     | 97/200 [04:19<07:36,  4.43s/it]Running Inference:  49%|████▉     | 98/200 [04:21<05:53,  3.47s/it]Running Inference:  50%|████▉     | 99/200 [04:27<07:02,  4.18s/it]Running Inference:  50%|█████     | 100/200 [04:27<05:20,  3.21s/it]Running Inference:  50%|█████     | 101/200 [04:30<04:44,  2.88s/it]Running Inference:  51%|█████     | 102/200 [04:30<03:43,  2.28s/it]Running Inference:  52%|█████▏    | 103/200 [04:38<06:04,  3.75s/it]Running Inference:  52%|█████▏    | 104/200 [04:45<07:30,  4.69s/it]Running Inference:  52%|█████▎    | 105/200 [04:46<06:03,  3.82s/it]Running Inference:  53%|█████▎    | 106/200 [04:54<07:36,  4.86s/it]Running Inference:  54%|█████▎    | 107/200 [04:55<05:47,  3.74s/it]Running Inference:  54%|█████▍    | 108/200 [04:57<04:50,  3.16s/it]Running Inference:  55%|█████▍    | 109/200 [04:58<04:00,  2.64s/it]Running Inference:  55%|█████▌    | 110/200 [05:03<05:10,  3.45s/it]Running Inference:  56%|█████▌    | 111/200 [05:06<04:33,  3.07s/it]Running Inference:  56%|█████▌    | 112/200 [05:11<05:40,  3.87s/it]Running Inference:  56%|█████▋    | 113/200 [05:13<04:39,  3.21s/it]Running Inference:  57%|█████▋    | 114/200 [05:14<03:50,  2.69s/it]Running Inference:  57%|█████▊    | 115/200 [05:19<04:29,  3.17s/it]Running Inference:  58%|█████▊    | 116/200 [05:25<05:44,  4.10s/it]Running Inference:  58%|█████▊    | 117/200 [05:32<06:50,  4.94s/it]Running Inference:  59%|█████▉    | 118/200 [05:33<05:21,  3.92s/it]Running Inference:  60%|█████▉    | 119/200 [05:34<04:08,  3.06s/it]Running Inference:  60%|██████    | 120/200 [05:40<05:01,  3.77s/it]Running Inference:  60%|██████    | 121/200 [05:42<04:27,  3.39s/it]Running Inference:  61%|██████    | 122/200 [05:44<03:49,  2.94s/it]Running Inference:  62%|██████▏   | 123/200 [05:45<02:55,  2.28s/it]Running Inference:  62%|██████▏   | 124/200 [05:48<03:08,  2.48s/it]Running Inference:  62%|██████▎   | 125/200 [05:53<04:09,  3.33s/it]Running Inference:  63%|██████▎   | 126/200 [05:55<03:25,  2.78s/it]Running Inference:  64%|██████▎   | 127/200 [05:56<02:51,  2.35s/it]Running Inference:  64%|██████▍   | 128/200 [06:00<03:13,  2.69s/it]Running Inference:  64%|██████▍   | 129/200 [06:01<02:38,  2.24s/it]Running Inference:  65%|██████▌   | 130/200 [06:03<02:36,  2.24s/it]Running Inference:  66%|██████▌   | 131/200 [06:04<02:12,  1.93s/it]Running Inference:  66%|██████▌   | 132/200 [06:06<02:00,  1.77s/it]Running Inference:  66%|██████▋   | 133/200 [06:07<01:56,  1.74s/it]Running Inference:  67%|██████▋   | 134/200 [06:09<01:46,  1.62s/it]Running Inference:  68%|██████▊   | 135/200 [06:10<01:41,  1.56s/it]Running Inference:  68%|██████▊   | 136/200 [06:11<01:32,  1.44s/it]Running Inference:  68%|██████▊   | 137/200 [06:15<02:13,  2.12s/it]Running Inference:  69%|██████▉   | 138/200 [06:17<02:16,  2.21s/it]Running Inference:  70%|██████▉   | 139/200 [06:20<02:30,  2.47s/it]Running Inference:  70%|███████   | 140/200 [06:22<02:11,  2.19s/it]Running Inference:  70%|███████   | 141/200 [06:28<03:17,  3.35s/it]Running Inference:  71%|███████   | 142/200 [06:30<02:42,  2.81s/it]Running Inference:  72%|███████▏  | 143/200 [06:30<02:08,  2.25s/it]Running Inference:  72%|███████▏  | 144/200 [06:36<03:02,  3.27s/it]Running Inference:  72%|███████▎  | 145/200 [06:39<02:49,  3.09s/it]Running Inference:  73%|███████▎  | 146/200 [06:40<02:22,  2.64s/it]Running Inference:  74%|███████▎  | 147/200 [06:47<03:21,  3.81s/it]Running Inference:  74%|███████▍  | 148/200 [06:49<02:57,  3.41s/it]Running Inference:  74%|███████▍  | 149/200 [06:51<02:22,  2.80s/it]Running Inference:  75%|███████▌  | 150/200 [06:53<02:04,  2.49s/it]Running Inference:  76%|███████▌  | 151/200 [06:55<02:00,  2.47s/it]Running Inference:  76%|███████▌  | 152/200 [06:56<01:36,  2.02s/it]Running Inference:  76%|███████▋  | 153/200 [07:03<02:42,  3.46s/it]Running Inference:  77%|███████▋  | 154/200 [07:04<02:03,  2.69s/it]Running Inference:  78%|███████▊  | 155/200 [07:05<01:47,  2.38s/it]Running Inference:  78%|███████▊  | 156/200 [07:07<01:34,  2.14s/it]Running Inference:  78%|███████▊  | 157/200 [07:08<01:19,  1.84s/it]Running Inference:  79%|███████▉  | 158/200 [07:09<01:05,  1.55s/it]Running Inference:  80%|███████▉  | 159/200 [07:11<01:05,  1.59s/it]Running Inference:  80%|████████  | 160/200 [07:12<01:02,  1.55s/it]Running Inference:  80%|████████  | 161/200 [07:15<01:11,  1.84s/it]Running Inference:  81%|████████  | 162/200 [07:16<01:00,  1.60s/it]Running Inference:  82%|████████▏ | 163/200 [07:17<00:58,  1.58s/it]Running Inference:  82%|████████▏ | 164/200 [07:19<00:56,  1.58s/it]Running Inference:  82%|████████▎ | 165/200 [07:24<01:33,  2.67s/it]Running Inference:  83%|████████▎ | 166/200 [07:25<01:13,  2.17s/it]Running Inference:  84%|████████▎ | 167/200 [07:26<01:03,  1.94s/it]Running Inference:  84%|████████▍ | 168/200 [07:27<00:54,  1.69s/it]Running Inference:  84%|████████▍ | 169/200 [07:34<01:39,  3.20s/it]Running Inference:  85%|████████▌ | 170/200 [07:35<01:16,  2.54s/it]Running Inference:  86%|████████▌ | 171/200 [07:37<01:04,  2.23s/it]Running Inference:  86%|████████▌ | 172/200 [07:38<00:54,  1.94s/it]Running Inference:  86%|████████▋ | 173/200 [07:40<00:55,  2.06s/it]Running Inference:  87%|████████▋ | 174/200 [07:43<00:54,  2.11s/it]Running Inference:  88%|████████▊ | 175/200 [07:44<00:50,  2.00s/it]Running Inference:  88%|████████▊ | 176/200 [07:46<00:44,  1.85s/it]Running Inference:  88%|████████▊ | 177/200 [07:51<01:08,  2.96s/it]Running Inference:  89%|████████▉ | 178/200 [07:57<01:20,  3.65s/it]Running Inference:  90%|████████▉ | 179/200 [07:58<01:01,  2.93s/it]Running Inference:  90%|█████████ | 180/200 [07:59<00:49,  2.47s/it]Running Inference:  90%|█████████ | 181/200 [08:05<01:03,  3.34s/it]Running Inference:  91%|█████████ | 182/200 [08:10<01:13,  4.10s/it]Running Inference:  92%|█████████▏| 183/200 [08:12<00:56,  3.32s/it]Running Inference:  92%|█████████▏| 184/200 [08:13<00:41,  2.61s/it]Running Inference:  92%|█████████▎| 185/200 [08:14<00:33,  2.21s/it]Running Inference:  93%|█████████▎| 186/200 [08:17<00:31,  2.26s/it]Running Inference:  94%|█████████▎| 187/200 [08:23<00:46,  3.58s/it]Running Inference:  94%|█████████▍| 188/200 [08:31<00:56,  4.72s/it]Running Inference:  94%|█████████▍| 189/200 [08:33<00:42,  3.87s/it]Running Inference:  95%|█████████▌| 190/200 [08:37<00:41,  4.19s/it]Running Inference:  96%|█████████▌| 191/200 [08:44<00:42,  4.75s/it]Running Inference:  96%|█████████▌| 192/200 [08:44<00:28,  3.53s/it]Running Inference:  96%|█████████▋| 193/200 [08:47<00:22,  3.18s/it]Running Inference:  97%|█████████▋| 194/200 [08:52<00:23,  3.94s/it]Running Inference:  98%|█████████▊| 195/200 [08:58<00:22,  4.59s/it]Running Inference:  98%|█████████▊| 196/200 [09:01<00:16,  4.12s/it]Running Inference:  98%|█████████▊| 197/200 [09:07<00:13,  4.62s/it]Running Inference:  99%|█████████▉| 198/200 [09:09<00:07,  3.64s/it]Running Inference: 100%|█████████▉| 199/200 [09:11<00:03,  3.41s/it]Running Inference: 100%|██████████| 200/200 [09:13<00:00,  2.89s/it]Running Inference: 100%|██████████| 200/200 [09:13<00:00,  2.77s/it]
2025-12-14 20:56:15,350 - INFO - Inference completed.
2025-12-14 20:56:15,369 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 20:56:15,369 - INFO - Calculating metrics for dataset: longbench
2025-12-14 20:56:15,370 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 20:56:15,370 - INFO - Metrics:
31.07
2025-12-14 20:56:15,371 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lsht, ratio=0.1) on GPU 1

----------------------------------------
Task: lsht | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 20:56:21,850 - INFO - Set deterministic seeds to 42
2025-12-14 20:56:21,850 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 20:56:21,850 - INFO - Starting evaluation run...
2025-12-14 20:56:21,850 - INFO - Output directory set to: longbenchresult
2025-12-14 20:56:21,850 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 20:56:21,850 - INFO - KV Press 'tova' setup.
2025-12-14 20:56:21,850 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 20:56:21,850 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.81it/s]
Device set to use cuda:0
2025-12-14 20:56:46,764 - INFO - Model pipeline loaded.
2025-12-14 20:56:46,764 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 20:56:59,447 - INFO - Dataset loaded with 200 entries.
2025-12-14 20:56:59,448 - INFO - Dataset processed with 200 entries.
2025-12-14 20:56:59,482 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:03,  2.13s/it]Running Inference:   1%|          | 2/200 [00:04<07:38,  2.31s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:28,  1.67s/it]Running Inference:   2%|▏         | 4/200 [00:06<05:09,  1.58s/it]Running Inference:   2%|▎         | 5/200 [00:09<06:39,  2.05s/it]Running Inference:   3%|▎         | 6/200 [00:11<06:42,  2.07s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:53,  2.14s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:53,  1.84s/it]Running Inference:   4%|▍         | 9/200 [00:17<06:07,  1.93s/it]Running Inference:   5%|▌         | 10/200 [00:18<05:25,  1.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:39,  1.80s/it]Running Inference:   6%|▌         | 12/200 [00:22<05:17,  1.69s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:53,  1.89s/it]Running Inference:   7%|▋         | 14/200 [00:25<05:23,  1.74s/it]Running Inference:   8%|▊         | 15/200 [00:28<06:32,  2.12s/it]Running Inference:   8%|▊         | 16/200 [00:30<05:44,  1.87s/it]Running Inference:   8%|▊         | 17/200 [00:36<10:00,  3.28s/it]Running Inference:   9%|▉         | 18/200 [00:39<09:03,  2.99s/it]Running Inference:  10%|▉         | 19/200 [00:40<07:24,  2.46s/it]Running Inference:  10%|█         | 20/200 [00:42<07:17,  2.43s/it]Running Inference:  10%|█         | 21/200 [00:44<07:00,  2.35s/it]Running Inference:  11%|█         | 22/200 [00:46<06:28,  2.18s/it]Running Inference:  12%|█▏        | 23/200 [00:47<05:26,  1.84s/it]Running Inference:  12%|█▏        | 24/200 [00:53<09:14,  3.15s/it]Running Inference:  12%|█▎        | 25/200 [00:54<07:06,  2.44s/it]Running Inference:  13%|█▎        | 26/200 [00:57<07:30,  2.59s/it]Running Inference:  14%|█▎        | 27/200 [01:00<07:40,  2.66s/it]Running Inference:  14%|█▍        | 28/200 [01:01<06:15,  2.18s/it]Running Inference:  14%|█▍        | 29/200 [01:02<05:21,  1.88s/it]Running Inference:  15%|█▌        | 30/200 [01:05<06:02,  2.13s/it]Running Inference:  16%|█▌        | 31/200 [01:11<09:13,  3.27s/it]Running Inference:  16%|█▌        | 32/200 [01:14<09:25,  3.37s/it]Running Inference:  16%|█▋        | 33/200 [01:17<08:48,  3.17s/it]Running Inference:  17%|█▋        | 34/200 [01:22<10:25,  3.77s/it]Running Inference:  18%|█▊        | 35/200 [01:27<10:47,  3.92s/it]Running Inference:  18%|█▊        | 36/200 [01:32<11:57,  4.38s/it]Running Inference:  18%|█▊        | 37/200 [01:34<10:07,  3.73s/it]Running Inference:  19%|█▉        | 38/200 [01:36<08:49,  3.27s/it]Running Inference:  20%|█▉        | 39/200 [01:38<07:37,  2.84s/it]Running Inference:  20%|██        | 40/200 [01:42<07:59,  3.00s/it]Running Inference:  20%|██        | 41/200 [01:45<08:03,  3.04s/it]Running Inference:  21%|██        | 42/200 [01:47<07:06,  2.70s/it]Running Inference:  22%|██▏       | 43/200 [01:48<06:07,  2.34s/it]Running Inference:  22%|██▏       | 44/200 [01:50<05:27,  2.10s/it]Running Inference:  22%|██▎       | 45/200 [01:51<05:02,  1.95s/it]Running Inference:  23%|██▎       | 46/200 [01:54<05:44,  2.23s/it]Running Inference:  24%|██▎       | 47/200 [01:56<05:15,  2.06s/it]Running Inference:  24%|██▍       | 48/200 [01:57<04:34,  1.81s/it]Running Inference:  24%|██▍       | 49/200 [02:03<07:49,  3.11s/it]Running Inference:  25%|██▌       | 50/200 [02:05<06:32,  2.62s/it]Running Inference:  26%|██▌       | 51/200 [02:06<05:15,  2.12s/it]Running Inference:  26%|██▌       | 52/200 [02:07<04:46,  1.94s/it]Running Inference:  26%|██▋       | 53/200 [02:09<04:28,  1.83s/it]Running Inference:  27%|██▋       | 54/200 [02:14<06:56,  2.86s/it]Running Inference:  28%|██▊       | 55/200 [02:21<09:38,  3.99s/it]Running Inference:  28%|██▊       | 56/200 [02:27<11:04,  4.62s/it]Running Inference:  28%|██▊       | 57/200 [02:29<09:33,  4.01s/it]Running Inference:  29%|██▉       | 58/200 [02:35<10:49,  4.57s/it]Running Inference:  30%|██▉       | 59/200 [02:36<08:17,  3.53s/it]Running Inference:  30%|███       | 60/200 [02:39<07:31,  3.22s/it]Running Inference:  30%|███       | 61/200 [02:40<05:44,  2.48s/it]Running Inference:  31%|███       | 62/200 [02:42<05:52,  2.55s/it]Running Inference:  32%|███▏      | 63/200 [02:43<04:53,  2.14s/it]Running Inference:  32%|███▏      | 64/200 [02:44<04:05,  1.81s/it]Running Inference:  32%|███▎      | 65/200 [02:45<03:27,  1.53s/it]Running Inference:  33%|███▎      | 66/200 [02:51<06:24,  2.87s/it]Running Inference:  34%|███▎      | 67/200 [02:52<05:06,  2.31s/it]Running Inference:  34%|███▍      | 68/200 [02:53<04:05,  1.86s/it]Running Inference:  34%|███▍      | 69/200 [02:56<04:28,  2.05s/it]Running Inference:  35%|███▌      | 70/200 [02:57<03:48,  1.76s/it]Running Inference:  36%|███▌      | 71/200 [02:57<03:07,  1.45s/it]Running Inference:  36%|███▌      | 72/200 [02:59<03:17,  1.54s/it]Running Inference:  36%|███▋      | 73/200 [03:02<03:46,  1.78s/it]Running Inference:  37%|███▋      | 74/200 [03:05<04:42,  2.24s/it]Running Inference:  38%|███▊      | 75/200 [03:06<04:13,  2.03s/it]Running Inference:  38%|███▊      | 76/200 [03:12<06:32,  3.16s/it]Running Inference:  38%|███▊      | 77/200 [03:14<05:29,  2.68s/it]Running Inference:  39%|███▉      | 78/200 [03:15<04:42,  2.31s/it]Running Inference:  40%|███▉      | 79/200 [03:16<03:52,  1.92s/it]Running Inference:  40%|████      | 80/200 [03:22<05:52,  2.93s/it]Running Inference:  40%|████      | 81/200 [03:24<05:18,  2.68s/it]Running Inference:  41%|████      | 82/200 [03:27<05:51,  2.98s/it]Running Inference:  42%|████▏     | 83/200 [03:29<05:06,  2.62s/it]Running Inference:  42%|████▏     | 84/200 [03:34<06:29,  3.36s/it]Running Inference:  42%|████▎     | 85/200 [03:37<06:02,  3.15s/it]Running Inference:  43%|████▎     | 86/200 [03:38<05:06,  2.68s/it]Running Inference:  44%|████▎     | 87/200 [03:44<06:34,  3.49s/it]Running Inference:  44%|████▍     | 88/200 [03:46<05:41,  3.05s/it]Running Inference:  44%|████▍     | 89/200 [03:47<04:38,  2.51s/it]Running Inference:  45%|████▌     | 90/200 [03:49<04:29,  2.45s/it]Running Inference:  46%|████▌     | 91/200 [03:51<04:06,  2.26s/it]Running Inference:  46%|████▌     | 92/200 [03:53<03:56,  2.19s/it]Running Inference:  46%|████▋     | 93/200 [03:59<05:48,  3.25s/it]Running Inference:  47%|████▋     | 94/200 [04:00<04:44,  2.68s/it]Running Inference:  48%|████▊     | 95/200 [04:03<04:50,  2.76s/it]Running Inference:  48%|████▊     | 96/200 [04:05<04:30,  2.60s/it]Running Inference:  48%|████▊     | 97/200 [04:12<06:35,  3.84s/it]Running Inference:  49%|████▉     | 98/200 [04:13<05:11,  3.05s/it]Running Inference:  50%|████▉     | 99/200 [04:19<06:29,  3.85s/it]Running Inference:  50%|█████     | 100/200 [04:20<04:57,  2.97s/it]Running Inference:  50%|█████     | 101/200 [04:22<04:28,  2.71s/it]Running Inference:  51%|█████     | 102/200 [04:23<03:31,  2.16s/it]Running Inference:  52%|█████▏    | 103/200 [04:30<05:50,  3.61s/it]Running Inference:  52%|█████▏    | 104/200 [04:37<07:16,  4.54s/it]Running Inference:  52%|█████▎    | 105/200 [04:39<05:52,  3.72s/it]Running Inference:  53%|█████▎    | 106/200 [04:46<07:24,  4.73s/it]Running Inference:  54%|█████▎    | 107/200 [04:47<05:39,  3.65s/it]Running Inference:  54%|█████▍    | 108/200 [04:49<04:43,  3.09s/it]Running Inference:  55%|█████▍    | 109/200 [04:50<03:55,  2.59s/it]Running Inference:  55%|█████▌    | 110/200 [04:55<05:03,  3.37s/it]Running Inference:  56%|█████▌    | 111/200 [04:57<04:28,  3.02s/it]Running Inference:  56%|█████▌    | 112/200 [05:03<05:33,  3.79s/it]Running Inference:  56%|█████▋    | 113/200 [05:05<04:34,  3.15s/it]Running Inference:  57%|█████▋    | 114/200 [05:06<03:47,  2.64s/it]Running Inference:  57%|█████▊    | 115/200 [05:12<05:04,  3.58s/it]Running Inference:  58%|█████▊    | 116/200 [05:18<06:05,  4.35s/it]Running Inference:  58%|█████▊    | 117/200 [05:25<07:00,  5.06s/it]Running Inference:  59%|█████▉    | 118/200 [05:26<05:28,  4.00s/it]Running Inference:  60%|█████▉    | 119/200 [05:27<04:11,  3.11s/it]Running Inference:  60%|██████    | 120/200 [05:32<04:57,  3.72s/it]Running Inference:  60%|██████    | 121/200 [05:35<04:24,  3.35s/it]Running Inference:  61%|██████    | 122/200 [05:37<03:44,  2.88s/it]Running Inference:  62%|██████▏   | 123/200 [05:37<02:52,  2.23s/it]Running Inference:  62%|██████▏   | 124/200 [05:40<03:05,  2.44s/it]Running Inference:  62%|██████▎   | 125/200 [05:45<04:04,  3.26s/it]Running Inference:  63%|██████▎   | 126/200 [05:47<03:21,  2.72s/it]Running Inference:  64%|██████▎   | 127/200 [05:48<02:48,  2.30s/it]Running Inference:  64%|██████▍   | 128/200 [05:52<03:11,  2.65s/it]Running Inference:  64%|██████▍   | 129/200 [05:53<02:36,  2.20s/it]Running Inference:  65%|██████▌   | 130/200 [05:55<02:34,  2.21s/it]Running Inference:  66%|██████▌   | 131/200 [05:56<02:11,  1.91s/it]Running Inference:  66%|██████▌   | 132/200 [05:58<01:59,  1.75s/it]Running Inference:  66%|██████▋   | 133/200 [05:59<01:55,  1.72s/it]Running Inference:  67%|██████▋   | 134/200 [06:01<01:45,  1.60s/it]Running Inference:  68%|██████▊   | 135/200 [06:02<01:40,  1.55s/it]Running Inference:  68%|██████▊   | 136/200 [06:03<01:31,  1.43s/it]Running Inference:  68%|██████▊   | 137/200 [06:07<02:12,  2.11s/it]Running Inference:  69%|██████▉   | 138/200 [06:09<02:15,  2.19s/it]Running Inference:  70%|██████▉   | 139/200 [06:12<02:29,  2.45s/it]Running Inference:  70%|███████   | 140/200 [06:14<02:12,  2.20s/it]Running Inference:  70%|███████   | 141/200 [06:20<03:15,  3.31s/it]Running Inference:  71%|███████   | 142/200 [06:21<02:40,  2.77s/it]Running Inference:  72%|███████▏  | 143/200 [06:22<02:06,  2.22s/it]Running Inference:  72%|███████▏  | 144/200 [06:28<02:59,  3.21s/it]Running Inference:  72%|███████▎  | 145/200 [06:31<02:47,  3.04s/it]Running Inference:  73%|███████▎  | 146/200 [06:32<02:20,  2.60s/it]Running Inference:  74%|███████▎  | 147/200 [06:38<03:18,  3.74s/it]Running Inference:  74%|███████▍  | 148/200 [06:41<02:54,  3.36s/it]Running Inference:  74%|███████▍  | 149/200 [06:42<02:20,  2.75s/it]Running Inference:  75%|███████▌  | 150/200 [06:44<02:02,  2.45s/it]Running Inference:  76%|███████▌  | 151/200 [06:46<01:59,  2.44s/it]Running Inference:  76%|███████▌  | 152/200 [06:47<01:35,  1.99s/it]Running Inference:  76%|███████▋  | 153/200 [06:54<02:39,  3.40s/it]Running Inference:  77%|███████▋  | 154/200 [06:55<02:01,  2.64s/it]Running Inference:  78%|███████▊  | 155/200 [06:57<01:45,  2.34s/it]Running Inference:  78%|███████▊  | 156/200 [06:58<01:32,  2.11s/it]Running Inference:  78%|███████▊  | 157/200 [06:59<01:18,  1.82s/it]Running Inference:  79%|███████▉  | 158/200 [07:00<01:04,  1.53s/it]Running Inference:  80%|███████▉  | 159/200 [07:02<01:04,  1.58s/it]Running Inference:  80%|████████  | 160/200 [07:03<01:01,  1.54s/it]Running Inference:  80%|████████  | 161/200 [07:06<01:10,  1.82s/it]Running Inference:  81%|████████  | 162/200 [07:07<01:00,  1.59s/it]Running Inference:  82%|████████▏ | 163/200 [07:08<00:58,  1.57s/it]Running Inference:  82%|████████▏ | 164/200 [07:10<00:56,  1.56s/it]Running Inference:  82%|████████▎ | 165/200 [07:12<00:56,  1.60s/it]Running Inference:  83%|████████▎ | 166/200 [07:13<00:48,  1.43s/it]Running Inference:  84%|████████▎ | 167/200 [07:14<00:46,  1.41s/it]Running Inference:  84%|████████▍ | 168/200 [07:15<00:42,  1.32s/it]Running Inference:  84%|████████▍ | 169/200 [07:22<01:29,  2.89s/it]Running Inference:  85%|████████▌ | 170/200 [07:23<01:09,  2.32s/it]Running Inference:  86%|████████▌ | 171/200 [07:24<01:00,  2.07s/it]Running Inference:  86%|████████▌ | 172/200 [07:25<00:51,  1.83s/it]Running Inference:  86%|████████▋ | 173/200 [07:28<00:53,  1.98s/it]Running Inference:  87%|████████▋ | 174/200 [07:30<00:52,  2.02s/it]Running Inference:  88%|████████▊ | 175/200 [07:32<00:48,  1.94s/it]Running Inference:  88%|████████▊ | 176/200 [07:33<00:43,  1.81s/it]Running Inference:  88%|████████▊ | 177/200 [07:38<01:06,  2.89s/it]Running Inference:  89%|████████▉ | 178/200 [07:44<01:18,  3.56s/it]Running Inference:  90%|████████▉ | 179/200 [07:45<00:59,  2.86s/it]Running Inference:  90%|█████████ | 180/200 [07:46<00:48,  2.42s/it]Running Inference:  90%|█████████ | 181/200 [07:51<01:02,  3.27s/it]Running Inference:  91%|█████████ | 182/200 [07:57<01:12,  4.00s/it]Running Inference:  92%|█████████▏| 183/200 [07:59<00:55,  3.25s/it]Running Inference:  92%|█████████▏| 184/200 [08:00<00:40,  2.55s/it]Running Inference:  92%|█████████▎| 185/200 [08:01<00:32,  2.16s/it]Running Inference:  93%|█████████▎| 186/200 [08:03<00:31,  2.23s/it]Running Inference:  94%|█████████▎| 187/200 [08:10<00:45,  3.51s/it]Running Inference:  94%|█████████▍| 188/200 [08:17<00:55,  4.64s/it]Running Inference:  94%|█████████▍| 189/200 [08:19<00:41,  3.81s/it]Running Inference:  95%|█████████▌| 190/200 [08:24<00:41,  4.12s/it]Running Inference:  96%|█████████▌| 191/200 [08:30<00:42,  4.68s/it]Running Inference:  96%|█████████▌| 192/200 [08:30<00:27,  3.48s/it]Running Inference:  96%|█████████▋| 193/200 [08:33<00:22,  3.15s/it]Running Inference:  97%|█████████▋| 194/200 [08:38<00:23,  3.88s/it]Running Inference:  98%|█████████▊| 195/200 [08:44<00:22,  4.51s/it]Running Inference:  98%|█████████▊| 196/200 [08:47<00:16,  4.06s/it]Running Inference:  98%|█████████▊| 197/200 [08:53<00:13,  4.55s/it]Running Inference:  99%|█████████▉| 198/200 [08:54<00:07,  3.59s/it]Running Inference: 100%|█████████▉| 199/200 [08:57<00:03,  3.37s/it]Running Inference: 100%|██████████| 200/200 [08:59<00:00,  2.86s/it]Running Inference: 100%|██████████| 200/200 [08:59<00:00,  2.70s/it]
2025-12-14 21:05:58,894 - INFO - Inference completed.
2025-12-14 21:05:58,912 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 21:05:58,912 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:05:58,913 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 21:05:58,913 - INFO - Metrics:
32.05
2025-12-14 21:05:58,915 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lsht, ratio=0.2) on GPU 1

----------------------------------------
Task: lsht | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:06:05,437 - INFO - Set deterministic seeds to 42
2025-12-14 21:06:05,437 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:06:05,437 - INFO - Starting evaluation run...
2025-12-14 21:06:05,437 - INFO - Output directory set to: longbenchresult
2025-12-14 21:06:05,437 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 21:06:05,437 - INFO - KV Press 'tova' setup.
2025-12-14 21:06:05,437 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:06:05,437 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.21it/s]
Device set to use cuda:0
2025-12-14 21:06:19,009 - INFO - Model pipeline loaded.
2025-12-14 21:06:19,009 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 21:06:27,515 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:06:27,515 - INFO - Dataset processed with 200 entries.
2025-12-14 21:06:27,551 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:04,  2.13s/it]Running Inference:   1%|          | 2/200 [00:04<07:38,  2.31s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:31,  1.68s/it]Running Inference:   2%|▏         | 4/200 [00:06<05:11,  1.59s/it]Running Inference:   2%|▎         | 5/200 [00:09<06:40,  2.05s/it]Running Inference:   3%|▎         | 6/200 [00:11<06:42,  2.08s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:53,  2.14s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:53,  1.84s/it]Running Inference:   4%|▍         | 9/200 [00:17<06:08,  1.93s/it]Running Inference:   5%|▌         | 10/200 [00:18<05:25,  1.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:39,  1.80s/it]Running Inference:   6%|▌         | 12/200 [00:22<05:17,  1.69s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:53,  1.89s/it]Running Inference:   7%|▋         | 14/200 [00:25<05:23,  1.74s/it]Running Inference:   8%|▊         | 15/200 [00:28<06:31,  2.12s/it]Running Inference:   8%|▊         | 16/200 [00:30<05:43,  1.87s/it]Running Inference:   8%|▊         | 17/200 [00:36<09:59,  3.28s/it]Running Inference:   9%|▉         | 18/200 [00:39<09:03,  2.99s/it]Running Inference:  10%|▉         | 19/200 [00:40<07:24,  2.46s/it]Running Inference:  10%|█         | 20/200 [00:42<07:17,  2.43s/it]Running Inference:  10%|█         | 21/200 [00:44<07:00,  2.35s/it]Running Inference:  11%|█         | 22/200 [00:46<06:27,  2.18s/it]Running Inference:  12%|█▏        | 23/200 [00:47<05:26,  1.84s/it]Running Inference:  12%|█▏        | 24/200 [00:53<09:15,  3.15s/it]Running Inference:  12%|█▎        | 25/200 [00:54<07:07,  2.44s/it]Running Inference:  13%|█▎        | 26/200 [01:01<10:31,  3.63s/it]Running Inference:  14%|█▎        | 27/200 [01:03<09:46,  3.39s/it]Running Inference:  14%|█▍        | 28/200 [01:04<07:42,  2.69s/it]Running Inference:  14%|█▍        | 29/200 [01:06<06:22,  2.23s/it]Running Inference:  15%|█▌        | 30/200 [01:08<06:45,  2.38s/it]Running Inference:  16%|█▌        | 31/200 [01:14<09:43,  3.45s/it]Running Inference:  16%|█▌        | 32/200 [01:21<12:31,  4.48s/it]Running Inference:  16%|█▋        | 33/200 [01:24<11:04,  3.98s/it]Running Inference:  17%|█▋        | 34/200 [01:29<12:02,  4.35s/it]Running Inference:  18%|█▊        | 35/200 [01:33<11:54,  4.33s/it]Running Inference:  18%|█▊        | 36/200 [01:39<12:47,  4.68s/it]Running Inference:  18%|█▊        | 37/200 [01:41<10:42,  3.94s/it]Running Inference:  19%|█▉        | 38/200 [01:43<09:13,  3.42s/it]Running Inference:  20%|█▉        | 39/200 [01:45<07:54,  2.94s/it]Running Inference:  20%|██        | 40/200 [01:52<10:54,  4.09s/it]Running Inference:  20%|██        | 41/200 [01:55<10:05,  3.81s/it]Running Inference:  21%|██        | 42/200 [01:57<08:30,  3.23s/it]Running Inference:  22%|██▏       | 43/200 [01:59<07:05,  2.71s/it]Running Inference:  22%|██▏       | 44/200 [02:00<06:07,  2.36s/it]Running Inference:  22%|██▎       | 45/200 [02:02<05:30,  2.13s/it]Running Inference:  23%|██▎       | 46/200 [02:05<06:03,  2.36s/it]Running Inference:  24%|██▎       | 47/200 [02:06<05:28,  2.15s/it]Running Inference:  24%|██▍       | 48/200 [02:07<04:43,  1.87s/it]Running Inference:  24%|██▍       | 49/200 [02:14<07:56,  3.16s/it]Running Inference:  25%|██▌       | 50/200 [02:15<06:37,  2.65s/it]Running Inference:  26%|██▌       | 51/200 [02:16<05:17,  2.13s/it]Running Inference:  26%|██▌       | 52/200 [02:18<04:48,  1.95s/it]Running Inference:  26%|██▋       | 53/200 [02:19<04:29,  1.83s/it]Running Inference:  27%|██▋       | 54/200 [02:24<06:59,  2.87s/it]Running Inference:  28%|██▊       | 55/200 [02:31<09:40,  4.00s/it]Running Inference:  28%|██▊       | 56/200 [02:37<11:06,  4.63s/it]Running Inference:  28%|██▊       | 57/200 [02:40<09:34,  4.02s/it]Running Inference:  29%|██▉       | 58/200 [02:46<10:51,  4.59s/it]Running Inference:  30%|██▉       | 59/200 [02:47<08:18,  3.54s/it]Running Inference:  30%|███       | 60/200 [02:49<07:32,  3.23s/it]Running Inference:  30%|███       | 61/200 [02:50<05:45,  2.48s/it]Running Inference:  31%|███       | 62/200 [02:53<05:52,  2.56s/it]Running Inference:  32%|███▏      | 63/200 [02:54<04:53,  2.15s/it]Running Inference:  32%|███▏      | 64/200 [02:55<04:06,  1.81s/it]Running Inference:  32%|███▎      | 65/200 [02:56<03:27,  1.54s/it]Running Inference:  33%|███▎      | 66/200 [03:02<06:25,  2.88s/it]Running Inference:  34%|███▎      | 67/200 [03:03<05:07,  2.31s/it]Running Inference:  34%|███▍      | 68/200 [03:04<04:06,  1.87s/it]Running Inference:  34%|███▍      | 69/200 [03:06<04:28,  2.05s/it]Running Inference:  35%|███▌      | 70/200 [03:07<03:49,  1.76s/it]Running Inference:  36%|███▌      | 71/200 [03:08<03:07,  1.45s/it]Running Inference:  36%|███▌      | 72/200 [03:10<03:17,  1.54s/it]Running Inference:  36%|███▋      | 73/200 [03:12<03:45,  1.78s/it]Running Inference:  37%|███▋      | 74/200 [03:15<04:41,  2.24s/it]Running Inference:  38%|███▊      | 75/200 [03:17<04:13,  2.03s/it]Running Inference:  38%|███▊      | 76/200 [03:23<06:33,  3.18s/it]Running Inference:  38%|███▊      | 77/200 [03:24<05:31,  2.69s/it]Running Inference:  39%|███▉      | 78/200 [03:26<04:47,  2.36s/it]Running Inference:  40%|███▉      | 79/200 [03:27<03:56,  1.95s/it]Running Inference:  40%|████      | 80/200 [03:32<05:56,  2.97s/it]Running Inference:  40%|████      | 81/200 [03:34<05:21,  2.70s/it]Running Inference:  41%|████      | 82/200 [03:38<05:53,  2.99s/it]Running Inference:  42%|████▏     | 83/200 [03:40<05:07,  2.63s/it]Running Inference:  42%|████▏     | 84/200 [03:45<06:32,  3.38s/it]Running Inference:  42%|████▎     | 85/200 [03:48<06:04,  3.17s/it]Running Inference:  43%|████▎     | 86/200 [03:49<05:07,  2.70s/it]Running Inference:  44%|████▎     | 87/200 [03:55<06:37,  3.51s/it]Running Inference:  44%|████▍     | 88/200 [03:57<05:43,  3.07s/it]Running Inference:  44%|████▍     | 89/200 [03:58<04:39,  2.52s/it]Running Inference:  45%|████▌     | 90/200 [04:00<04:30,  2.46s/it]Running Inference:  46%|████▌     | 91/200 [04:02<04:07,  2.27s/it]Running Inference:  46%|████▌     | 92/200 [04:04<03:56,  2.19s/it]Running Inference:  46%|████▋     | 93/200 [04:10<05:49,  3.27s/it]Running Inference:  47%|████▋     | 94/200 [04:11<04:45,  2.69s/it]Running Inference:  48%|████▊     | 95/200 [04:17<06:34,  3.75s/it]Running Inference:  48%|████▊     | 96/200 [04:20<05:42,  3.29s/it]Running Inference:  48%|████▊     | 97/200 [04:26<07:24,  4.32s/it]Running Inference:  49%|████▉     | 98/200 [04:27<05:45,  3.39s/it]Running Inference:  50%|████▉     | 99/200 [04:33<06:54,  4.10s/it]Running Inference:  50%|█████     | 100/200 [04:34<05:14,  3.14s/it]Running Inference:  50%|█████     | 101/200 [04:36<04:40,  2.83s/it]Running Inference:  51%|█████     | 102/200 [04:37<03:40,  2.25s/it]Running Inference:  52%|█████▏    | 103/200 [04:44<05:55,  3.67s/it]Running Inference:  52%|█████▏    | 104/200 [04:51<07:20,  4.59s/it]Running Inference:  52%|█████▎    | 105/200 [04:53<05:56,  3.75s/it]Running Inference:  53%|█████▎    | 106/200 [05:00<07:28,  4.77s/it]Running Inference:  54%|█████▎    | 107/200 [05:01<05:41,  3.67s/it]Running Inference:  54%|█████▍    | 108/200 [05:03<04:45,  3.10s/it]Running Inference:  55%|█████▍    | 109/200 [05:04<03:56,  2.60s/it]Running Inference:  55%|█████▌    | 110/200 [05:09<05:05,  3.40s/it]Running Inference:  56%|█████▌    | 111/200 [05:12<04:29,  3.03s/it]Running Inference:  56%|█████▌    | 112/200 [05:17<05:36,  3.82s/it]Running Inference:  56%|█████▋    | 113/200 [05:19<04:36,  3.18s/it]Running Inference:  57%|█████▋    | 114/200 [05:24<05:12,  3.64s/it]Running Inference:  57%|█████▊    | 115/200 [05:29<06:04,  4.29s/it]Running Inference:  58%|█████▊    | 116/200 [05:36<06:46,  4.84s/it]Running Inference:  58%|█████▊    | 117/200 [05:42<07:28,  5.41s/it]Running Inference:  59%|█████▉    | 118/200 [05:44<05:47,  4.24s/it]Running Inference:  60%|█████▉    | 119/200 [05:45<04:25,  3.28s/it]Running Inference:  60%|██████    | 120/200 [05:50<05:08,  3.86s/it]Running Inference:  60%|██████    | 121/200 [05:52<04:31,  3.44s/it]Running Inference:  61%|██████    | 122/200 [05:54<03:49,  2.94s/it]Running Inference:  62%|██████▏   | 123/200 [05:55<02:55,  2.28s/it]Running Inference:  62%|██████▏   | 124/200 [05:58<03:07,  2.47s/it]Running Inference:  62%|██████▎   | 125/200 [06:03<04:06,  3.29s/it]Running Inference:  63%|██████▎   | 126/200 [06:05<03:22,  2.74s/it]Running Inference:  64%|██████▎   | 127/200 [06:06<02:49,  2.32s/it]Running Inference:  64%|██████▍   | 128/200 [06:09<03:11,  2.66s/it]Running Inference:  64%|██████▍   | 129/200 [06:11<02:36,  2.21s/it]Running Inference:  65%|██████▌   | 130/200 [06:13<02:35,  2.22s/it]Running Inference:  66%|██████▌   | 131/200 [06:14<02:11,  1.91s/it]Running Inference:  66%|██████▌   | 132/200 [06:15<01:59,  1.76s/it]Running Inference:  66%|██████▋   | 133/200 [06:17<01:55,  1.72s/it]Running Inference:  67%|██████▋   | 134/200 [06:18<01:46,  1.61s/it]Running Inference:  68%|██████▊   | 135/200 [06:20<01:40,  1.55s/it]Running Inference:  68%|██████▊   | 136/200 [06:21<01:31,  1.43s/it]Running Inference:  68%|██████▊   | 137/200 [06:25<02:12,  2.10s/it]Running Inference:  69%|██████▉   | 138/200 [06:27<02:15,  2.19s/it]Running Inference:  70%|██████▉   | 139/200 [06:30<02:29,  2.45s/it]Running Inference:  70%|███████   | 140/200 [06:32<02:10,  2.18s/it]Running Inference:  70%|███████   | 141/200 [06:37<03:14,  3.30s/it]Running Inference:  71%|███████   | 142/200 [06:39<02:40,  2.76s/it]Running Inference:  72%|███████▏  | 143/200 [06:40<02:06,  2.21s/it]Running Inference:  72%|███████▏  | 144/200 [06:45<02:59,  3.21s/it]Running Inference:  72%|███████▎  | 145/200 [06:48<02:47,  3.04s/it]Running Inference:  73%|███████▎  | 146/200 [06:50<02:20,  2.60s/it]Running Inference:  74%|███████▎  | 147/200 [06:56<03:17,  3.73s/it]Running Inference:  74%|███████▍  | 148/200 [06:59<02:54,  3.35s/it]Running Inference:  74%|███████▍  | 149/200 [07:00<02:20,  2.75s/it]Running Inference:  75%|███████▌  | 150/200 [07:02<02:02,  2.45s/it]Running Inference:  76%|███████▌  | 151/200 [07:04<01:59,  2.43s/it]Running Inference:  76%|███████▌  | 152/200 [07:05<01:35,  1.99s/it]Running Inference:  76%|███████▋  | 153/200 [07:12<02:39,  3.39s/it]Running Inference:  77%|███████▋  | 154/200 [07:13<02:01,  2.64s/it]Running Inference:  78%|███████▊  | 155/200 [07:14<01:45,  2.34s/it]Running Inference:  78%|███████▊  | 156/200 [07:16<01:32,  2.11s/it]Running Inference:  78%|███████▊  | 157/200 [07:17<01:18,  1.82s/it]Running Inference:  79%|███████▉  | 158/200 [07:18<01:04,  1.53s/it]Running Inference:  80%|███████▉  | 159/200 [07:19<01:04,  1.57s/it]Running Inference:  80%|████████  | 160/200 [07:21<01:01,  1.54s/it]Running Inference:  80%|████████  | 161/200 [07:23<01:10,  1.82s/it]Running Inference:  81%|████████  | 162/200 [07:24<01:00,  1.59s/it]Running Inference:  82%|████████▏ | 163/200 [07:26<00:58,  1.57s/it]Running Inference:  82%|████████▏ | 164/200 [07:27<00:56,  1.56s/it]Running Inference:  82%|████████▎ | 165/200 [07:29<00:56,  1.60s/it]Running Inference:  83%|████████▎ | 166/200 [07:30<00:48,  1.43s/it]Running Inference:  84%|████████▎ | 167/200 [07:35<01:18,  2.39s/it]Running Inference:  84%|████████▍ | 168/200 [07:36<01:04,  2.01s/it]Running Inference:  84%|████████▍ | 169/200 [07:42<01:44,  3.37s/it]Running Inference:  85%|████████▌ | 170/200 [07:43<01:19,  2.65s/it]Running Inference:  86%|████████▌ | 171/200 [07:45<01:06,  2.30s/it]Running Inference:  86%|████████▌ | 172/200 [07:46<00:55,  1.99s/it]Running Inference:  86%|████████▋ | 173/200 [07:49<00:56,  2.09s/it]Running Inference:  87%|████████▋ | 174/200 [07:51<00:54,  2.10s/it]Running Inference:  88%|████████▊ | 175/200 [07:52<00:49,  1.99s/it]Running Inference:  88%|████████▊ | 176/200 [07:54<00:44,  1.84s/it]Running Inference:  88%|████████▊ | 177/200 [07:59<01:07,  2.92s/it]Running Inference:  89%|████████▉ | 178/200 [08:01<00:57,  2.59s/it]Running Inference:  90%|████████▉ | 179/200 [08:02<00:45,  2.18s/it]Running Inference:  90%|█████████ | 180/200 [08:04<00:38,  1.95s/it]Running Inference:  90%|█████████ | 181/200 [08:09<00:56,  2.95s/it]Running Inference:  91%|█████████ | 182/200 [08:15<01:08,  3.80s/it]Running Inference:  92%|█████████▏| 183/200 [08:16<00:52,  3.10s/it]Running Inference:  92%|█████████▏| 184/200 [08:17<00:39,  2.45s/it]Running Inference:  92%|█████████▎| 185/200 [08:18<00:31,  2.09s/it]Running Inference:  93%|█████████▎| 186/200 [08:21<00:30,  2.18s/it]Running Inference:  94%|█████████▎| 187/200 [08:27<00:45,  3.47s/it]Running Inference:  94%|█████████▍| 188/200 [08:35<00:55,  4.59s/it]Running Inference:  94%|█████████▍| 189/200 [08:36<00:41,  3.77s/it]Running Inference:  95%|█████████▌| 190/200 [08:41<00:40,  4.10s/it]Running Inference:  96%|█████████▌| 191/200 [08:47<00:41,  4.65s/it]Running Inference:  96%|█████████▌| 192/200 [08:48<00:27,  3.46s/it]Running Inference:  96%|█████████▋| 193/200 [08:50<00:21,  3.13s/it]Running Inference:  97%|█████████▋| 194/200 [08:56<00:23,  3.88s/it]Running Inference:  98%|█████████▊| 195/200 [09:02<00:22,  4.50s/it]Running Inference:  98%|█████████▊| 196/200 [09:05<00:16,  4.06s/it]Running Inference:  98%|█████████▊| 197/200 [09:11<00:13,  4.55s/it]Running Inference:  99%|█████████▉| 198/200 [09:12<00:07,  3.59s/it]Running Inference: 100%|█████████▉| 199/200 [09:15<00:03,  3.37s/it]Running Inference: 100%|██████████| 200/200 [09:16<00:00,  2.85s/it]Running Inference: 100%|██████████| 200/200 [09:16<00:00,  2.78s/it]
2025-12-14 21:15:44,493 - INFO - Inference completed.
2025-12-14 21:15:44,512 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 21:15:44,512 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:15:44,513 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 21:15:44,513 - INFO - Metrics:
31.0
2025-12-14 21:15:44,514 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lsht, ratio=0.3) on GPU 1

----------------------------------------
Task: lsht | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:15:51,056 - INFO - Set deterministic seeds to 42
2025-12-14 21:15:51,057 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:15:51,057 - INFO - Starting evaluation run...
2025-12-14 21:15:51,057 - INFO - Output directory set to: longbenchresult
2025-12-14 21:15:51,057 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 21:15:51,057 - INFO - KV Press 'tova' setup.
2025-12-14 21:15:51,057 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:15:51,057 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.30it/s]
Device set to use cuda:0
2025-12-14 21:16:05,200 - INFO - Model pipeline loaded.
2025-12-14 21:16:05,200 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 21:16:10,561 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:16:10,561 - INFO - Dataset processed with 200 entries.
2025-12-14 21:16:10,596 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:05,  2.14s/it]Running Inference:   1%|          | 2/200 [00:04<07:27,  2.26s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:24,  1.65s/it]Running Inference:   2%|▏         | 4/200 [00:06<05:05,  1.56s/it]Running Inference:   2%|▎         | 5/200 [00:09<06:34,  2.02s/it]Running Inference:   3%|▎         | 6/200 [00:11<06:38,  2.06s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:49,  2.12s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:50,  1.83s/it]Running Inference:   4%|▍         | 9/200 [00:17<06:05,  1.91s/it]Running Inference:   5%|▌         | 10/200 [00:18<05:22,  1.70s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:37,  1.79s/it]Running Inference:   6%|▌         | 12/200 [00:21<05:14,  1.67s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:51,  1.88s/it]Running Inference:   7%|▋         | 14/200 [00:25<05:20,  1.72s/it]Running Inference:   8%|▊         | 15/200 [00:28<06:29,  2.10s/it]Running Inference:   8%|▊         | 16/200 [00:29<05:41,  1.86s/it]Running Inference:   8%|▊         | 17/200 [00:36<09:51,  3.23s/it]Running Inference:   9%|▉         | 18/200 [00:38<08:56,  2.95s/it]Running Inference:  10%|▉         | 19/200 [00:39<07:19,  2.43s/it]Running Inference:  10%|█         | 20/200 [00:42<07:13,  2.41s/it]Running Inference:  10%|█         | 21/200 [00:44<06:55,  2.32s/it]Running Inference:  11%|█         | 22/200 [00:46<06:23,  2.16s/it]Running Inference:  12%|█▏        | 23/200 [00:47<05:22,  1.82s/it]Running Inference:  12%|█▏        | 24/200 [00:53<09:08,  3.12s/it]Running Inference:  12%|█▎        | 25/200 [00:54<07:02,  2.41s/it]Running Inference:  13%|█▎        | 26/200 [00:57<07:26,  2.56s/it]Running Inference:  14%|█▎        | 27/200 [00:59<07:35,  2.64s/it]Running Inference:  14%|█▍        | 28/200 [01:00<06:11,  2.16s/it]Running Inference:  14%|█▍        | 29/200 [01:02<05:17,  1.86s/it]Running Inference:  15%|█▌        | 30/200 [01:04<05:59,  2.12s/it]Running Inference:  16%|█▌        | 31/200 [01:10<09:07,  3.24s/it]Running Inference:  16%|█▌        | 32/200 [01:14<09:20,  3.33s/it]Running Inference:  16%|█▋        | 33/200 [01:16<08:49,  3.17s/it]Running Inference:  17%|█▋        | 34/200 [01:22<10:25,  3.77s/it]Running Inference:  18%|█▊        | 35/200 [01:26<10:45,  3.91s/it]Running Inference:  18%|█▊        | 36/200 [01:31<11:54,  4.36s/it]Running Inference:  18%|█▊        | 37/200 [01:33<10:04,  3.71s/it]Running Inference:  19%|█▉        | 38/200 [01:36<08:46,  3.25s/it]Running Inference:  20%|█▉        | 39/200 [01:37<07:34,  2.82s/it]Running Inference:  20%|██        | 40/200 [01:41<07:58,  2.99s/it]Running Inference:  20%|██        | 41/200 [01:44<08:01,  3.03s/it]Running Inference:  21%|██        | 42/200 [01:46<07:04,  2.69s/it]Running Inference:  22%|██▏       | 43/200 [01:51<08:43,  3.34s/it]Running Inference:  22%|██▏       | 44/200 [01:52<07:15,  2.79s/it]Running Inference:  22%|██▎       | 45/200 [01:54<06:17,  2.43s/it]Running Inference:  23%|██▎       | 46/200 [01:57<06:34,  2.56s/it]Running Inference:  24%|██▎       | 47/200 [01:58<05:50,  2.29s/it]Running Inference:  24%|██▍       | 48/200 [02:00<04:58,  1.96s/it]Running Inference:  24%|██▍       | 49/200 [02:06<08:02,  3.19s/it]Running Inference:  25%|██▌       | 50/200 [02:07<06:40,  2.67s/it]Running Inference:  26%|██▌       | 51/200 [02:08<05:19,  2.14s/it]Running Inference:  26%|██▌       | 52/200 [02:09<04:49,  1.95s/it]Running Inference:  26%|██▋       | 53/200 [02:11<04:29,  1.83s/it]Running Inference:  27%|██▋       | 54/200 [02:16<06:55,  2.85s/it]Running Inference:  28%|██▊       | 55/200 [02:23<09:31,  3.94s/it]Running Inference:  28%|██▊       | 56/200 [02:29<10:56,  4.56s/it]Running Inference:  28%|██▊       | 57/200 [02:31<09:30,  3.99s/it]Running Inference:  29%|██▉       | 58/200 [02:37<10:44,  4.54s/it]Running Inference:  30%|██▉       | 59/200 [02:38<08:13,  3.50s/it]Running Inference:  30%|███       | 60/200 [02:41<07:27,  3.20s/it]Running Inference:  30%|███       | 61/200 [02:42<05:41,  2.46s/it]Running Inference:  31%|███       | 62/200 [02:44<05:49,  2.53s/it]Running Inference:  32%|███▏      | 63/200 [02:45<04:51,  2.13s/it]Running Inference:  32%|███▏      | 64/200 [02:46<04:03,  1.79s/it]Running Inference:  32%|███▎      | 65/200 [02:47<03:25,  1.52s/it]Running Inference:  33%|███▎      | 66/200 [02:53<06:20,  2.84s/it]Running Inference:  34%|███▎      | 67/200 [02:54<05:03,  2.28s/it]Running Inference:  34%|███▍      | 68/200 [02:55<04:03,  1.84s/it]Running Inference:  34%|███▍      | 69/200 [02:57<04:26,  2.03s/it]Running Inference:  35%|███▌      | 70/200 [02:59<03:48,  1.76s/it]Running Inference:  36%|███▌      | 71/200 [02:59<03:06,  1.45s/it]Running Inference:  36%|███▌      | 72/200 [03:01<03:16,  1.53s/it]Running Inference:  36%|███▋      | 73/200 [03:03<03:44,  1.77s/it]Running Inference:  37%|███▋      | 74/200 [03:07<04:39,  2.22s/it]Running Inference:  38%|███▊      | 75/200 [03:08<04:11,  2.02s/it]Running Inference:  38%|███▊      | 76/200 [03:14<06:29,  3.14s/it]Running Inference:  38%|███▊      | 77/200 [03:15<05:27,  2.66s/it]Running Inference:  39%|███▉      | 78/200 [03:17<04:38,  2.28s/it]Running Inference:  40%|███▉      | 79/200 [03:18<03:49,  1.90s/it]Running Inference:  40%|████      | 80/200 [03:23<05:49,  2.91s/it]Running Inference:  40%|████      | 81/200 [03:25<05:16,  2.66s/it]Running Inference:  41%|████      | 82/200 [03:29<05:48,  2.95s/it]Running Inference:  42%|████▏     | 83/200 [03:31<05:03,  2.59s/it]Running Inference:  42%|████▏     | 84/200 [03:36<06:25,  3.32s/it]Running Inference:  42%|████▎     | 85/200 [03:38<05:59,  3.12s/it]Running Inference:  43%|████▎     | 86/200 [03:40<05:03,  2.66s/it]Running Inference:  44%|████▎     | 87/200 [03:45<06:31,  3.46s/it]Running Inference:  44%|████▍     | 88/200 [03:47<05:38,  3.02s/it]Running Inference:  44%|████▍     | 89/200 [03:49<04:38,  2.51s/it]Running Inference:  45%|████▌     | 90/200 [03:51<04:29,  2.45s/it]Running Inference:  46%|████▌     | 91/200 [03:53<04:06,  2.26s/it]Running Inference:  46%|████▌     | 92/200 [03:55<03:55,  2.18s/it]Running Inference:  46%|████▋     | 93/200 [04:00<05:46,  3.23s/it]Running Inference:  47%|████▋     | 94/200 [04:02<04:42,  2.67s/it]Running Inference:  48%|████▊     | 95/200 [04:04<04:42,  2.70s/it]Running Inference:  48%|████▊     | 96/200 [04:07<04:24,  2.55s/it]Running Inference:  48%|████▊     | 97/200 [04:13<06:25,  3.75s/it]Running Inference:  49%|████▉     | 98/200 [04:14<05:04,  2.99s/it]Running Inference:  50%|████▉     | 99/200 [04:20<06:23,  3.79s/it]Running Inference:  50%|█████     | 100/200 [04:21<04:52,  2.93s/it]Running Inference:  50%|█████     | 101/200 [04:23<04:24,  2.67s/it]Running Inference:  51%|█████     | 102/200 [04:24<03:29,  2.13s/it]Running Inference:  52%|█████▏    | 103/200 [04:31<05:42,  3.53s/it]Running Inference:  52%|█████▏    | 104/200 [04:37<07:07,  4.45s/it]Running Inference:  52%|█████▎    | 105/200 [04:39<05:46,  3.65s/it]Running Inference:  53%|█████▎    | 106/200 [04:44<06:31,  4.17s/it]Running Inference:  54%|█████▎    | 107/200 [04:46<05:02,  3.25s/it]Running Inference:  54%|█████▍    | 108/200 [04:47<04:17,  2.80s/it]Running Inference:  55%|█████▍    | 109/200 [04:49<03:37,  2.38s/it]Running Inference:  55%|█████▌    | 110/200 [04:54<04:49,  3.22s/it]Running Inference:  56%|█████▌    | 111/200 [04:56<04:18,  2.91s/it]Running Inference:  56%|█████▌    | 112/200 [05:02<05:25,  3.70s/it]Running Inference:  56%|█████▋    | 113/200 [05:03<04:28,  3.09s/it]Running Inference:  57%|█████▋    | 114/200 [05:08<05:05,  3.55s/it]Running Inference:  57%|█████▊    | 115/200 [05:14<05:57,  4.21s/it]Running Inference:  58%|█████▊    | 116/200 [05:20<06:39,  4.76s/it]Running Inference:  58%|█████▊    | 117/200 [05:26<07:20,  5.31s/it]Running Inference:  59%|█████▉    | 118/200 [05:28<05:42,  4.18s/it]Running Inference:  60%|█████▉    | 119/200 [05:29<04:19,  3.20s/it]Running Inference:  60%|██████    | 120/200 [05:34<05:02,  3.79s/it]Running Inference:  60%|██████    | 121/200 [05:36<04:27,  3.38s/it]Running Inference:  61%|██████    | 122/200 [05:38<03:46,  2.90s/it]Running Inference:  62%|██████▏   | 123/200 [05:39<02:53,  2.25s/it]Running Inference:  62%|██████▏   | 124/200 [05:42<03:05,  2.44s/it]Running Inference:  62%|██████▎   | 125/200 [05:47<04:03,  3.25s/it]Running Inference:  63%|██████▎   | 126/200 [05:48<03:20,  2.71s/it]Running Inference:  64%|██████▎   | 127/200 [05:50<02:47,  2.29s/it]Running Inference:  64%|██████▍   | 128/200 [05:53<03:09,  2.63s/it]Running Inference:  64%|██████▍   | 129/200 [05:54<02:35,  2.19s/it]Running Inference:  65%|██████▌   | 130/200 [05:56<02:33,  2.20s/it]Running Inference:  66%|██████▌   | 131/200 [05:58<02:10,  1.89s/it]Running Inference:  66%|██████▌   | 132/200 [05:59<01:58,  1.74s/it]Running Inference:  66%|██████▋   | 133/200 [06:01<01:54,  1.71s/it]Running Inference:  67%|██████▋   | 134/200 [06:02<01:45,  1.59s/it]Running Inference:  68%|██████▊   | 135/200 [06:03<01:40,  1.54s/it]Running Inference:  68%|██████▊   | 136/200 [06:05<01:30,  1.42s/it]Running Inference:  68%|██████▊   | 137/200 [06:08<02:11,  2.09s/it]Running Inference:  69%|██████▉   | 138/200 [06:11<02:14,  2.18s/it]Running Inference:  70%|██████▉   | 139/200 [06:14<02:28,  2.44s/it]Running Inference:  70%|███████   | 140/200 [06:15<02:09,  2.16s/it]Running Inference:  70%|███████   | 141/200 [06:21<03:12,  3.27s/it]Running Inference:  71%|███████   | 142/200 [06:23<02:39,  2.74s/it]Running Inference:  72%|███████▏  | 143/200 [06:23<02:05,  2.20s/it]Running Inference:  72%|███████▏  | 144/200 [06:29<02:57,  3.18s/it]Running Inference:  72%|███████▎  | 145/200 [06:32<02:46,  3.04s/it]Running Inference:  73%|███████▎  | 146/200 [06:33<02:19,  2.59s/it]Running Inference:  74%|███████▎  | 147/200 [06:36<02:25,  2.75s/it]Running Inference:  74%|███████▍  | 148/200 [06:39<02:18,  2.66s/it]Running Inference:  74%|███████▍  | 149/200 [06:40<01:55,  2.26s/it]Running Inference:  75%|███████▌  | 150/200 [06:42<01:45,  2.10s/it]Running Inference:  76%|███████▌  | 151/200 [06:44<01:47,  2.19s/it]Running Inference:  76%|███████▌  | 152/200 [06:45<01:27,  1.82s/it]Running Inference:  76%|███████▋  | 153/200 [06:52<02:32,  3.25s/it]Running Inference:  77%|███████▋  | 154/200 [06:53<01:56,  2.54s/it]Running Inference:  78%|███████▊  | 155/200 [06:54<01:42,  2.27s/it]Running Inference:  78%|███████▊  | 156/200 [06:56<01:30,  2.05s/it]Running Inference:  78%|███████▊  | 157/200 [06:57<01:16,  1.78s/it]Running Inference:  79%|███████▉  | 158/200 [06:58<01:02,  1.49s/it]Running Inference:  80%|███████▉  | 159/200 [06:59<01:03,  1.55s/it]Running Inference:  80%|████████  | 160/200 [07:01<01:00,  1.51s/it]Running Inference:  80%|████████  | 161/200 [07:03<01:10,  1.80s/it]Running Inference:  81%|████████  | 162/200 [07:04<00:59,  1.57s/it]Running Inference:  82%|████████▏ | 163/200 [07:06<00:57,  1.55s/it]Running Inference:  82%|████████▏ | 164/200 [07:07<00:55,  1.55s/it]Running Inference:  82%|████████▎ | 165/200 [07:09<00:55,  1.59s/it]Running Inference:  83%|████████▎ | 166/200 [07:10<00:47,  1.41s/it]Running Inference:  84%|████████▎ | 167/200 [07:15<01:17,  2.36s/it]Running Inference:  84%|████████▍ | 168/200 [07:16<01:03,  1.98s/it]Running Inference:  84%|████████▍ | 169/200 [07:22<01:42,  3.32s/it]Running Inference:  85%|████████▌ | 170/200 [07:23<01:18,  2.61s/it]Running Inference:  86%|████████▌ | 171/200 [07:25<01:05,  2.27s/it]Running Inference:  86%|████████▌ | 172/200 [07:26<00:55,  1.96s/it]Running Inference:  86%|████████▋ | 173/200 [07:28<00:55,  2.07s/it]Running Inference:  87%|████████▋ | 174/200 [07:30<00:54,  2.11s/it]Running Inference:  88%|████████▊ | 175/200 [07:32<00:49,  2.00s/it]Running Inference:  88%|████████▊ | 176/200 [07:34<00:44,  1.84s/it]Running Inference:  88%|████████▊ | 177/200 [07:36<00:44,  1.91s/it]Running Inference:  89%|████████▉ | 178/200 [07:38<00:41,  1.89s/it]Running Inference:  90%|████████▉ | 179/200 [07:39<00:35,  1.68s/it]Running Inference:  90%|█████████ | 180/200 [07:40<00:31,  1.60s/it]Running Inference:  90%|█████████ | 181/200 [07:45<00:50,  2.68s/it]Running Inference:  91%|█████████ | 182/200 [07:51<01:04,  3.59s/it]Running Inference:  92%|█████████▏| 183/200 [07:53<00:50,  2.95s/it]Running Inference:  92%|█████████▏| 184/200 [07:53<00:37,  2.34s/it]Running Inference:  92%|█████████▎| 185/200 [07:55<00:30,  2.02s/it]Running Inference:  93%|█████████▎| 186/200 [07:57<00:29,  2.12s/it]Running Inference:  94%|█████████▎| 187/200 [08:03<00:44,  3.40s/it]Running Inference:  94%|█████████▍| 188/200 [08:10<00:53,  4.49s/it]Running Inference:  94%|█████████▍| 189/200 [08:12<00:40,  3.69s/it]Running Inference:  95%|█████████▌| 190/200 [08:17<00:40,  4.02s/it]Running Inference:  96%|█████████▌| 191/200 [08:23<00:41,  4.58s/it]Running Inference:  96%|█████████▌| 192/200 [08:24<00:27,  3.40s/it]Running Inference:  96%|█████████▋| 193/200 [08:26<00:21,  3.09s/it]Running Inference:  97%|█████████▋| 194/200 [08:32<00:22,  3.82s/it]Running Inference:  98%|█████████▊| 195/200 [08:37<00:22,  4.44s/it]Running Inference:  98%|█████████▊| 196/200 [08:40<00:16,  4.01s/it]Running Inference:  98%|█████████▊| 197/200 [08:46<00:13,  4.49s/it]Running Inference:  99%|█████████▉| 198/200 [08:47<00:07,  3.55s/it]Running Inference: 100%|█████████▉| 199/200 [08:50<00:03,  3.33s/it]Running Inference: 100%|██████████| 200/200 [08:52<00:00,  2.83s/it]Running Inference: 100%|██████████| 200/200 [08:52<00:00,  2.66s/it]
2025-12-14 21:25:02,974 - INFO - Inference completed.
2025-12-14 21:25:02,993 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 21:25:02,993 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:25:02,994 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 21:25:02,994 - INFO - Metrics:
32.6
2025-12-14 21:25:02,995 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lsht, ratio=0.5) on GPU 1


========================================
LongBench Task: multifieldqa_zh
========================================
----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:25:09,440 - INFO - Set deterministic seeds to 42
2025-12-14 21:25:09,440 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:25:09,440 - INFO - Starting evaluation run...
2025-12-14 21:25:09,440 - INFO - Output directory set to: longbenchresult
2025-12-14 21:25:09,440 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 21:25:09,440 - INFO - KV Press 'tova' setup.
2025-12-14 21:25:09,440 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:25:09,440 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.16it/s]
Device set to use cuda:0
2025-12-14 21:25:26,692 - INFO - Model pipeline loaded.
2025-12-14 21:25:26,693 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 4295.84 examples/s]
2025-12-14 21:25:33,711 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:25:33,711 - INFO - Dataset processed with 200 entries.
2025-12-14 21:25:33,722 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:01<04:16,  1.82s/it]Running Inference:   1%|▏         | 2/142 [00:06<08:07,  3.48s/it]Running Inference:   2%|▏         | 3/142 [00:07<05:40,  2.45s/it]Running Inference:   3%|▎         | 4/142 [00:08<04:12,  1.83s/it]Running Inference:   4%|▎         | 5/142 [00:09<03:09,  1.38s/it]Running Inference:   4%|▍         | 6/142 [00:12<05:00,  2.21s/it]Running Inference:   5%|▍         | 7/142 [00:14<04:09,  1.85s/it]Running Inference:   6%|▌         | 8/142 [00:18<06:11,  2.77s/it]Running Inference:   6%|▋         | 9/142 [00:22<07:01,  3.17s/it]Running Inference:   7%|▋         | 10/142 [00:30<09:52,  4.49s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:32<08:07,  3.72s/it]Running Inference:   8%|▊         | 12/142 [00:33<06:24,  2.96s/it]Running Inference:   9%|▉         | 13/142 [00:34<05:12,  2.42s/it]Running Inference:  10%|▉         | 14/142 [00:39<06:39,  3.12s/it]Running Inference:  11%|█         | 15/142 [00:40<05:00,  2.37s/it]Running Inference:  11%|█▏        | 16/142 [00:44<06:27,  3.08s/it]Running Inference:  12%|█▏        | 17/142 [00:49<07:20,  3.52s/it]Running Inference:  13%|█▎        | 18/142 [00:53<07:25,  3.60s/it]Running Inference:  13%|█▎        | 19/142 [00:57<08:01,  3.92s/it]Running Inference:  14%|█▍        | 20/142 [00:58<06:04,  2.99s/it]Running Inference:  15%|█▍        | 21/142 [01:07<09:22,  4.65s/it]Running Inference:  15%|█▌        | 22/142 [01:11<08:59,  4.50s/it]Running Inference:  16%|█▌        | 23/142 [01:11<06:37,  3.34s/it]Running Inference:  17%|█▋        | 24/142 [01:13<05:43,  2.91s/it]Running Inference:  18%|█▊        | 25/142 [01:14<04:23,  2.26s/it]Running Inference:  18%|█▊        | 26/142 [01:18<05:30,  2.85s/it]Running Inference:  19%|█▉        | 27/142 [01:20<04:39,  2.43s/it]Running Inference:  20%|█▉        | 28/142 [01:22<04:31,  2.38s/it]Running Inference:  20%|██        | 29/142 [01:23<03:52,  2.06s/it]Running Inference:  21%|██        | 30/142 [01:24<03:05,  1.66s/it]Running Inference:  22%|██▏       | 31/142 [01:29<05:01,  2.71s/it]Running Inference:  23%|██▎       | 32/142 [01:32<04:51,  2.65s/it]Running Inference:  23%|██▎       | 33/142 [01:32<03:36,  1.99s/it]Running Inference:  24%|██▍       | 34/142 [01:33<02:55,  1.63s/it]Running Inference:  25%|██▍       | 35/142 [01:34<02:30,  1.41s/it]Running Inference:  25%|██▌       | 36/142 [01:35<02:14,  1.26s/it]Running Inference:  26%|██▌       | 37/142 [01:35<01:55,  1.10s/it]Running Inference:  27%|██▋       | 38/142 [01:36<01:49,  1.05s/it]Running Inference:  27%|██▋       | 39/142 [01:41<03:24,  1.98s/it]Running Inference:  28%|██▊       | 40/142 [01:44<04:17,  2.53s/it]Running Inference:  29%|██▉       | 41/142 [01:46<03:52,  2.30s/it]Running Inference:  30%|██▉       | 42/142 [01:47<03:21,  2.01s/it]Running Inference:  30%|███       | 43/142 [01:51<04:15,  2.58s/it]Running Inference:  31%|███       | 44/142 [01:53<03:33,  2.18s/it]Running Inference:  32%|███▏      | 45/142 [01:53<02:47,  1.72s/it]Running Inference:  32%|███▏      | 46/142 [01:54<02:17,  1.43s/it]Running Inference:  33%|███▎      | 47/142 [01:56<02:29,  1.57s/it]Running Inference:  34%|███▍      | 48/142 [01:56<01:56,  1.24s/it]Running Inference:  35%|███▍      | 49/142 [01:57<01:41,  1.09s/it]Running Inference:  35%|███▌      | 50/142 [01:58<01:41,  1.11s/it]Running Inference:  36%|███▌      | 51/142 [01:59<01:40,  1.10s/it]Running Inference:  37%|███▋      | 52/142 [02:01<01:42,  1.14s/it]Running Inference:  37%|███▋      | 53/142 [02:01<01:29,  1.01s/it]Running Inference:  38%|███▊      | 54/142 [02:02<01:24,  1.05it/s]Running Inference:  39%|███▊      | 55/142 [02:05<02:07,  1.46s/it]Running Inference:  39%|███▉      | 56/142 [02:07<02:18,  1.61s/it]Running Inference:  40%|████      | 57/142 [02:07<01:53,  1.34s/it]Running Inference:  41%|████      | 58/142 [02:08<01:36,  1.14s/it]Running Inference:  42%|████▏     | 59/142 [02:10<01:46,  1.28s/it]Running Inference:  42%|████▏     | 60/142 [02:10<01:26,  1.06s/it]Running Inference:  43%|████▎     | 61/142 [02:14<02:41,  1.99s/it]Running Inference:  44%|████▎     | 62/142 [02:15<02:04,  1.55s/it]Running Inference:  44%|████▍     | 63/142 [02:16<01:58,  1.49s/it]Running Inference:  45%|████▌     | 64/142 [02:17<01:48,  1.40s/it]Running Inference:  46%|████▌     | 65/142 [02:19<01:44,  1.36s/it]Running Inference:  46%|████▋     | 66/142 [02:20<01:35,  1.25s/it]Running Inference:  47%|████▋     | 67/142 [02:20<01:22,  1.10s/it]Running Inference:  48%|████▊     | 68/142 [02:21<01:07,  1.09it/s]Running Inference:  49%|████▊     | 69/142 [02:22<01:06,  1.10it/s]Running Inference:  49%|████▉     | 70/142 [02:23<01:16,  1.06s/it]Running Inference:  50%|█████     | 71/142 [02:27<02:19,  1.97s/it]Running Inference:  51%|█████     | 72/142 [02:28<01:50,  1.58s/it]Running Inference:  51%|█████▏    | 73/142 [02:29<01:34,  1.38s/it]Running Inference:  52%|█████▏    | 74/142 [02:30<01:26,  1.27s/it]Running Inference:  53%|█████▎    | 75/142 [02:30<01:07,  1.00s/it]Running Inference:  54%|█████▎    | 76/142 [02:31<00:57,  1.14it/s]Running Inference:  54%|█████▍    | 77/142 [02:32<00:58,  1.12it/s]Running Inference:  55%|█████▍    | 78/142 [02:36<01:53,  1.77s/it]Running Inference:  56%|█████▌    | 79/142 [02:40<02:44,  2.62s/it]Running Inference:  56%|█████▋    | 80/142 [02:41<02:03,  1.99s/it]Running Inference:  57%|█████▋    | 81/142 [02:42<01:54,  1.88s/it]Running Inference:  58%|█████▊    | 82/142 [02:43<01:28,  1.47s/it]Running Inference:  58%|█████▊    | 83/142 [02:44<01:23,  1.42s/it]Running Inference:  59%|█████▉    | 84/142 [02:46<01:28,  1.53s/it]Running Inference:  60%|█████▉    | 85/142 [02:47<01:12,  1.26s/it]Running Inference:  61%|██████    | 86/142 [02:48<01:04,  1.16s/it]Running Inference:  61%|██████▏   | 87/142 [02:52<01:50,  2.00s/it]Running Inference:  62%|██████▏   | 88/142 [02:56<02:24,  2.68s/it]Running Inference:  63%|██████▎   | 89/142 [02:57<02:00,  2.27s/it]Running Inference:  63%|██████▎   | 90/142 [02:58<01:36,  1.86s/it]Running Inference:  64%|██████▍   | 91/142 [03:02<02:10,  2.55s/it]Running Inference:  65%|██████▍   | 92/142 [03:05<02:04,  2.50s/it]Running Inference:  65%|██████▌   | 93/142 [03:06<01:44,  2.14s/it]Running Inference:  66%|██████▌   | 94/142 [03:06<01:20,  1.67s/it]Running Inference:  67%|██████▋   | 95/142 [03:11<01:53,  2.41s/it]Running Inference:  68%|██████▊   | 96/142 [03:12<01:31,  1.99s/it]Running Inference:  68%|██████▊   | 97/142 [03:16<01:55,  2.57s/it]Running Inference:  69%|██████▉   | 98/142 [03:16<01:28,  2.02s/it]Running Inference:  70%|██████▉   | 99/142 [03:20<01:55,  2.68s/it]Running Inference:  70%|███████   | 100/142 [03:21<01:25,  2.05s/it]Running Inference:  71%|███████   | 101/142 [03:22<01:07,  1.65s/it]Running Inference:  72%|███████▏  | 102/142 [03:26<01:32,  2.32s/it]Running Inference:  73%|███████▎  | 103/142 [03:28<01:31,  2.34s/it]Running Inference:  73%|███████▎  | 104/142 [03:32<01:50,  2.90s/it]Running Inference:  74%|███████▍  | 105/142 [03:37<02:02,  3.32s/it]Running Inference:  75%|███████▍  | 106/142 [03:40<01:56,  3.25s/it]Running Inference:  75%|███████▌  | 107/142 [03:44<02:06,  3.61s/it]Running Inference:  76%|███████▌  | 108/142 [03:45<01:31,  2.68s/it]Running Inference:  77%|███████▋  | 109/142 [03:49<01:41,  3.07s/it]Running Inference:  77%|███████▋  | 110/142 [03:50<01:26,  2.69s/it]Running Inference:  78%|███████▊  | 111/142 [03:51<01:04,  2.08s/it]Running Inference:  79%|███████▉  | 112/142 [03:53<01:01,  2.04s/it]Running Inference:  80%|███████▉  | 113/142 [03:54<00:50,  1.73s/it]Running Inference:  80%|████████  | 114/142 [03:55<00:40,  1.45s/it]Running Inference:  81%|████████  | 115/142 [03:55<00:32,  1.21s/it]Running Inference:  82%|████████▏ | 116/142 [03:56<00:25,  1.02it/s]Running Inference:  82%|████████▏ | 117/142 [03:57<00:25,  1.02s/it]Running Inference:  83%|████████▎ | 118/142 [03:58<00:26,  1.08s/it]Running Inference:  84%|████████▍ | 119/142 [03:59<00:19,  1.16it/s]Running Inference:  85%|████████▍ | 120/142 [04:00<00:20,  1.09it/s]Running Inference:  85%|████████▌ | 121/142 [04:04<00:40,  1.92s/it]Running Inference:  86%|████████▌ | 122/142 [04:05<00:34,  1.70s/it]Running Inference:  87%|████████▋ | 123/142 [04:09<00:44,  2.33s/it]Running Inference:  87%|████████▋ | 124/142 [04:09<00:32,  1.78s/it]Running Inference:  88%|████████▊ | 125/142 [04:10<00:26,  1.56s/it]Running Inference:  89%|████████▊ | 126/142 [04:14<00:37,  2.31s/it]Running Inference:  89%|████████▉ | 127/142 [04:16<00:32,  2.14s/it]Running Inference:  90%|█████████ | 128/142 [04:20<00:36,  2.62s/it]Running Inference:  91%|█████████ | 129/142 [04:21<00:27,  2.15s/it]Running Inference:  92%|█████████▏| 130/142 [04:26<00:34,  2.89s/it]Running Inference:  92%|█████████▏| 131/142 [04:26<00:24,  2.23s/it]Running Inference:  93%|█████████▎| 132/142 [04:27<00:17,  1.71s/it]Running Inference:  94%|█████████▎| 133/142 [04:29<00:16,  1.87s/it]Running Inference:  94%|█████████▍| 134/142 [04:30<00:13,  1.67s/it]Running Inference:  95%|█████████▌| 135/142 [04:31<00:09,  1.39s/it]Running Inference:  96%|█████████▌| 136/142 [04:33<00:08,  1.46s/it]Running Inference:  96%|█████████▋| 137/142 [04:37<00:11,  2.24s/it]Running Inference:  97%|█████████▋| 138/142 [04:37<00:06,  1.71s/it]Running Inference:  98%|█████████▊| 139/142 [04:39<00:05,  1.79s/it]Running Inference:  99%|█████████▊| 140/142 [04:40<00:03,  1.61s/it]Running Inference:  99%|█████████▉| 141/142 [04:45<00:02,  2.43s/it]Running Inference: 100%|██████████| 142/142 [04:46<00:00,  2.22s/it]Running Inference: 100%|██████████| 142/142 [04:46<00:00,  2.02s/it]
2025-12-14 21:30:20,647 - INFO - Inference completed.
2025-12-14 21:30:20,655 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 21:30:20,655 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.671 seconds.
Prefix dict has been built successfully.
2025-12-14 21:30:21,392 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 21:30:21,392 - INFO - Metrics:
43.88
2025-12-14 21:30:21,393 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_zh, ratio=0.1) on GPU 1

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:30:27,903 - INFO - Set deterministic seeds to 42
2025-12-14 21:30:27,904 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:30:27,904 - INFO - Starting evaluation run...
2025-12-14 21:30:27,904 - INFO - Output directory set to: longbenchresult
2025-12-14 21:30:27,904 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 21:30:27,904 - INFO - KV Press 'tova' setup.
2025-12-14 21:30:27,904 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:30:27,904 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.91it/s]
Device set to use cuda:0
2025-12-14 21:30:41,199 - INFO - Model pipeline loaded.
2025-12-14 21:30:41,199 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 21:30:49,124 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:30:49,124 - INFO - Dataset processed with 200 entries.
2025-12-14 21:30:49,134 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:01<04:16,  1.82s/it]Running Inference:   1%|▏         | 2/142 [00:06<08:07,  3.49s/it]Running Inference:   2%|▏         | 3/142 [00:07<05:40,  2.45s/it]Running Inference:   3%|▎         | 4/142 [00:08<04:12,  1.83s/it]Running Inference:   4%|▎         | 5/142 [00:09<03:09,  1.38s/it]Running Inference:   4%|▍         | 6/142 [00:09<02:23,  1.06s/it]Running Inference:   5%|▍         | 7/142 [00:10<02:24,  1.07s/it]Running Inference:   6%|▌         | 8/142 [00:15<05:00,  2.25s/it]Running Inference:   6%|▋         | 9/142 [00:19<06:13,  2.81s/it]Running Inference:   7%|▋         | 10/142 [00:27<09:47,  4.45s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:29<08:03,  3.69s/it]Running Inference:   8%|▊         | 12/142 [00:30<06:22,  2.94s/it]Running Inference:   9%|▉         | 13/142 [00:31<05:10,  2.41s/it]Running Inference:  10%|▉         | 14/142 [00:36<06:25,  3.02s/it]Running Inference:  11%|█         | 15/142 [00:37<04:51,  2.29s/it]Running Inference:  11%|█▏        | 16/142 [00:41<06:21,  3.03s/it]Running Inference:  12%|█▏        | 17/142 [00:46<07:18,  3.51s/it]Running Inference:  13%|█▎        | 18/142 [00:50<07:24,  3.59s/it]Running Inference:  13%|█▎        | 19/142 [00:51<06:05,  2.97s/it]Running Inference:  14%|█▍        | 20/142 [00:52<04:51,  2.39s/it]Running Inference:  15%|█▍        | 21/142 [00:57<06:32,  3.25s/it]Running Inference:  15%|█▌        | 22/142 [00:59<05:16,  2.64s/it]Running Inference:  16%|█▌        | 23/142 [00:59<04:02,  2.04s/it]Running Inference:  17%|█▋        | 24/142 [01:01<03:58,  2.02s/it]Running Inference:  18%|█▊        | 25/142 [01:02<03:03,  1.57s/it]Running Inference:  18%|█▊        | 26/142 [01:06<04:31,  2.34s/it]Running Inference:  19%|█▉        | 27/142 [01:07<03:57,  2.07s/it]Running Inference:  20%|█▉        | 28/142 [01:11<04:51,  2.56s/it]Running Inference:  20%|██        | 29/142 [01:12<04:06,  2.18s/it]Running Inference:  21%|██        | 30/142 [01:13<03:15,  1.74s/it]Running Inference:  22%|██▏       | 31/142 [01:18<05:08,  2.78s/it]Running Inference:  23%|██▎       | 32/142 [01:21<04:55,  2.69s/it]Running Inference:  23%|██▎       | 33/142 [01:21<03:39,  2.01s/it]Running Inference:  24%|██▍       | 34/142 [01:22<02:57,  1.64s/it]Running Inference:  25%|██▍       | 35/142 [01:25<03:36,  2.02s/it]Running Inference:  25%|██▌       | 36/142 [01:26<02:59,  1.69s/it]Running Inference:  26%|██▌       | 37/142 [01:27<02:26,  1.40s/it]Running Inference:  27%|██▋       | 38/142 [01:27<02:09,  1.25s/it]Running Inference:  27%|██▋       | 39/142 [01:32<03:37,  2.12s/it]Running Inference:  28%|██▊       | 40/142 [01:34<03:49,  2.25s/it]Running Inference:  29%|██▉       | 41/142 [01:36<03:32,  2.10s/it]Running Inference:  30%|██▉       | 42/142 [01:37<03:07,  1.88s/it]Running Inference:  30%|███       | 43/142 [01:41<04:05,  2.48s/it]Running Inference:  31%|███       | 44/142 [01:45<04:46,  2.93s/it]Running Inference:  32%|███▏      | 45/142 [01:46<03:37,  2.25s/it]Running Inference:  32%|███▏      | 46/142 [01:47<02:52,  1.80s/it]Running Inference:  33%|███▎      | 47/142 [01:48<02:53,  1.83s/it]Running Inference:  34%|███▍      | 48/142 [01:49<02:13,  1.42s/it]Running Inference:  35%|███▍      | 49/142 [01:50<01:53,  1.22s/it]Running Inference:  35%|███▌      | 50/142 [01:51<01:50,  1.20s/it]Running Inference:  36%|███▌      | 51/142 [01:52<01:43,  1.14s/it]Running Inference:  37%|███▋      | 52/142 [01:53<01:44,  1.16s/it]Running Inference:  37%|███▋      | 53/142 [01:54<01:31,  1.03s/it]Running Inference:  38%|███▊      | 54/142 [01:55<01:25,  1.03it/s]Running Inference:  39%|███▊      | 55/142 [01:57<02:01,  1.39s/it]Running Inference:  39%|███▉      | 56/142 [02:00<02:30,  1.75s/it]Running Inference:  40%|████      | 57/142 [02:00<02:02,  1.44s/it]Running Inference:  41%|████      | 58/142 [02:01<01:42,  1.22s/it]Running Inference:  42%|████▏     | 59/142 [02:02<01:48,  1.31s/it]Running Inference:  42%|████▏     | 60/142 [02:03<01:28,  1.08s/it]Running Inference:  43%|████▎     | 61/142 [02:07<02:43,  2.02s/it]Running Inference:  44%|████▎     | 62/142 [02:08<02:05,  1.57s/it]Running Inference:  44%|████▍     | 63/142 [02:09<01:59,  1.51s/it]Running Inference:  45%|████▌     | 64/142 [02:13<03:02,  2.34s/it]Running Inference:  46%|████▌     | 65/142 [02:15<02:35,  2.02s/it]Running Inference:  46%|████▋     | 66/142 [02:16<02:10,  1.72s/it]Running Inference:  47%|████▋     | 67/142 [02:16<01:47,  1.43s/it]Running Inference:  48%|████▊     | 68/142 [02:17<01:24,  1.14s/it]Running Inference:  49%|████▊     | 69/142 [02:18<01:18,  1.07s/it]Running Inference:  49%|████▉     | 70/142 [02:19<01:25,  1.18s/it]Running Inference:  50%|█████     | 71/142 [02:23<02:27,  2.07s/it]Running Inference:  51%|█████     | 72/142 [02:24<01:57,  1.68s/it]Running Inference:  51%|█████▏    | 73/142 [02:25<01:39,  1.45s/it]Running Inference:  52%|█████▏    | 74/142 [02:26<01:29,  1.32s/it]Running Inference:  53%|█████▎    | 75/142 [02:26<01:09,  1.04s/it]Running Inference:  54%|█████▎    | 76/142 [02:27<00:59,  1.11it/s]Running Inference:  54%|█████▍    | 77/142 [02:28<01:04,  1.01it/s]Running Inference:  55%|█████▍    | 78/142 [02:32<01:58,  1.86s/it]Running Inference:  56%|█████▌    | 79/142 [02:37<02:49,  2.69s/it]Running Inference:  56%|█████▋    | 80/142 [02:37<02:06,  2.04s/it]Running Inference:  57%|█████▋    | 81/142 [02:39<01:57,  1.92s/it]Running Inference:  58%|█████▊    | 82/142 [02:39<01:29,  1.49s/it]Running Inference:  58%|█████▊    | 83/142 [02:41<01:24,  1.43s/it]Running Inference:  59%|█████▉    | 84/142 [02:43<01:29,  1.54s/it]Running Inference:  60%|█████▉    | 85/142 [02:43<01:12,  1.27s/it]Running Inference:  61%|██████    | 86/142 [02:44<01:05,  1.17s/it]Running Inference:  61%|██████▏   | 87/142 [02:48<01:51,  2.03s/it]Running Inference:  62%|██████▏   | 88/142 [02:52<02:26,  2.72s/it]Running Inference:  63%|██████▎   | 89/142 [02:54<02:03,  2.33s/it]Running Inference:  63%|██████▎   | 90/142 [02:55<01:38,  1.90s/it]Running Inference:  64%|██████▍   | 91/142 [02:58<02:02,  2.41s/it]Running Inference:  65%|██████▍   | 92/142 [03:01<01:59,  2.39s/it]Running Inference:  65%|██████▌   | 93/142 [03:02<01:42,  2.10s/it]Running Inference:  66%|██████▌   | 94/142 [03:03<01:18,  1.64s/it]Running Inference:  67%|██████▋   | 95/142 [03:07<01:53,  2.41s/it]Running Inference:  68%|██████▊   | 96/142 [03:08<01:31,  1.98s/it]Running Inference:  68%|██████▊   | 97/142 [03:12<01:56,  2.58s/it]Running Inference:  69%|██████▉   | 98/142 [03:13<01:29,  2.03s/it]Running Inference:  70%|██████▉   | 99/142 [03:17<01:56,  2.70s/it]Running Inference:  70%|███████   | 100/142 [03:17<01:26,  2.06s/it]Running Inference:  71%|███████   | 101/142 [03:18<01:08,  1.67s/it]Running Inference:  72%|███████▏  | 102/142 [03:20<01:08,  1.72s/it]Running Inference:  73%|███████▎  | 103/142 [03:22<01:15,  1.93s/it]Running Inference:  73%|███████▎  | 104/142 [03:27<01:39,  2.63s/it]Running Inference:  74%|███████▍  | 105/142 [03:31<01:56,  3.15s/it]Running Inference:  75%|███████▍  | 106/142 [03:37<02:28,  4.11s/it]Running Inference:  75%|███████▌  | 107/142 [03:42<02:28,  4.23s/it]Running Inference:  76%|███████▌  | 108/142 [03:42<01:45,  3.12s/it]Running Inference:  77%|███████▋  | 109/142 [03:46<01:51,  3.39s/it]Running Inference:  77%|███████▋  | 110/142 [03:48<01:33,  2.91s/it]Running Inference:  78%|███████▊  | 111/142 [03:49<01:09,  2.24s/it]Running Inference:  79%|███████▉  | 112/142 [03:49<00:51,  1.72s/it]Running Inference:  80%|███████▉  | 113/142 [03:51<00:48,  1.67s/it]Running Inference:  80%|████████  | 114/142 [03:52<00:40,  1.44s/it]Running Inference:  81%|████████  | 115/142 [03:53<00:32,  1.21s/it]Running Inference:  82%|████████▏ | 116/142 [03:56<00:51,  1.97s/it]Running Inference:  82%|████████▏ | 117/142 [03:57<00:42,  1.71s/it]Running Inference:  83%|████████▎ | 118/142 [03:59<00:37,  1.57s/it]Running Inference:  84%|████████▍ | 119/142 [03:59<00:27,  1.20s/it]Running Inference:  85%|████████▍ | 120/142 [04:00<00:25,  1.16s/it]Running Inference:  85%|████████▌ | 121/142 [04:04<00:44,  2.11s/it]Running Inference:  86%|████████▌ | 122/142 [04:06<00:36,  1.84s/it]Running Inference:  87%|████████▋ | 123/142 [04:09<00:46,  2.44s/it]Running Inference:  87%|████████▋ | 124/142 [04:10<00:33,  1.86s/it]Running Inference:  88%|████████▊ | 125/142 [04:11<00:27,  1.61s/it]Running Inference:  89%|████████▊ | 126/142 [04:15<00:37,  2.37s/it]Running Inference:  89%|████████▉ | 127/142 [04:17<00:33,  2.21s/it]Running Inference:  90%|█████████ | 128/142 [04:21<00:37,  2.69s/it]Running Inference:  91%|█████████ | 129/142 [04:22<00:28,  2.20s/it]Running Inference:  92%|█████████▏| 130/142 [04:26<00:35,  2.94s/it]Running Inference:  92%|█████████▏| 131/142 [04:27<00:24,  2.26s/it]Running Inference:  93%|█████████▎| 132/142 [04:28<00:17,  1.74s/it]Running Inference:  94%|█████████▎| 133/142 [04:30<00:17,  1.89s/it]Running Inference:  94%|█████████▍| 134/142 [04:31<00:13,  1.68s/it]Running Inference:  95%|█████████▌| 135/142 [04:32<00:09,  1.41s/it]Running Inference:  96%|█████████▌| 136/142 [04:34<00:08,  1.48s/it]Running Inference:  96%|█████████▋| 137/142 [04:38<00:11,  2.26s/it]Running Inference:  97%|█████████▋| 138/142 [04:38<00:06,  1.72s/it]Running Inference:  98%|█████████▊| 139/142 [04:39<00:04,  1.56s/it]Running Inference:  99%|█████████▊| 140/142 [04:40<00:02,  1.45s/it]Running Inference:  99%|█████████▉| 141/142 [04:45<00:02,  2.33s/it]Running Inference: 100%|██████████| 142/142 [04:47<00:00,  2.16s/it]Running Inference: 100%|██████████| 142/142 [04:47<00:00,  2.02s/it]
2025-12-14 21:35:36,236 - INFO - Inference completed.
2025-12-14 21:35:36,244 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 21:35:36,244 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.661 seconds.
Prefix dict has been built successfully.
2025-12-14 21:35:36,971 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 21:35:36,971 - INFO - Metrics:
42.75
2025-12-14 21:35:36,972 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_zh, ratio=0.2) on GPU 1

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:35:43,495 - INFO - Set deterministic seeds to 42
2025-12-14 21:35:43,495 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:35:43,495 - INFO - Starting evaluation run...
2025-12-14 21:35:43,495 - INFO - Output directory set to: longbenchresult
2025-12-14 21:35:43,495 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 21:35:43,495 - INFO - KV Press 'tova' setup.
2025-12-14 21:35:43,495 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:35:43,495 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.76it/s]
Device set to use cuda:0
2025-12-14 21:35:55,695 - INFO - Model pipeline loaded.
2025-12-14 21:35:55,695 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 21:36:04,446 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:36:04,446 - INFO - Dataset processed with 200 entries.
2025-12-14 21:36:04,456 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:01<04:18,  1.83s/it]Running Inference:   1%|▏         | 2/142 [00:06<08:01,  3.44s/it]Running Inference:   2%|▏         | 3/142 [00:07<05:36,  2.42s/it]Running Inference:   3%|▎         | 4/142 [00:08<04:05,  1.78s/it]Running Inference:   4%|▎         | 5/142 [00:09<03:05,  1.35s/it]Running Inference:   4%|▍         | 6/142 [00:09<02:20,  1.03s/it]Running Inference:   5%|▍         | 7/142 [00:10<02:22,  1.06s/it]Running Inference:   6%|▌         | 8/142 [00:12<02:46,  1.24s/it]Running Inference:   6%|▋         | 9/142 [00:16<04:38,  2.10s/it]Running Inference:   7%|▋         | 10/142 [00:24<08:38,  3.92s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:26<07:15,  3.32s/it]Running Inference:   8%|▊         | 12/142 [00:27<05:49,  2.69s/it]Running Inference:   9%|▉         | 13/142 [00:28<04:47,  2.23s/it]Running Inference:  10%|▉         | 14/142 [00:33<06:16,  2.94s/it]Running Inference:  11%|█         | 15/142 [00:33<04:44,  2.24s/it]Running Inference:  11%|█▏        | 16/142 [00:38<06:17,  3.00s/it]Running Inference:  12%|█▏        | 17/142 [00:43<07:14,  3.48s/it]Running Inference:  13%|█▎        | 18/142 [00:45<06:16,  3.03s/it]Running Inference:  13%|█▎        | 19/142 [00:46<05:21,  2.62s/it]Running Inference:  14%|█▍        | 20/142 [00:47<04:21,  2.15s/it]Running Inference:  15%|█▍        | 21/142 [00:52<06:08,  3.05s/it]Running Inference:  15%|█▌        | 22/142 [00:57<06:44,  3.37s/it]Running Inference:  16%|█▌        | 23/142 [00:57<05:02,  2.54s/it]Running Inference:  17%|█▋        | 24/142 [00:59<04:38,  2.36s/it]Running Inference:  18%|█▊        | 25/142 [01:00<03:32,  1.81s/it]Running Inference:  18%|█▊        | 26/142 [01:04<04:47,  2.48s/it]Running Inference:  19%|█▉        | 27/142 [01:05<04:08,  2.16s/it]Running Inference:  20%|█▉        | 28/142 [01:07<03:46,  1.99s/it]Running Inference:  20%|██        | 29/142 [01:09<03:43,  1.98s/it]Running Inference:  21%|██        | 30/142 [01:10<03:18,  1.77s/it]Running Inference:  22%|██▏       | 31/142 [01:15<05:05,  2.76s/it]Running Inference:  23%|██▎       | 32/142 [01:17<04:52,  2.66s/it]Running Inference:  23%|██▎       | 33/142 [01:18<03:36,  1.99s/it]Running Inference:  24%|██▍       | 34/142 [01:19<02:55,  1.62s/it]Running Inference:  25%|██▍       | 35/142 [01:21<03:33,  1.99s/it]Running Inference:  25%|██▌       | 36/142 [01:22<02:56,  1.67s/it]Running Inference:  26%|██▌       | 37/142 [01:23<02:24,  1.38s/it]Running Inference:  27%|██▋       | 38/142 [01:24<02:08,  1.23s/it]Running Inference:  27%|██▋       | 39/142 [01:28<03:33,  2.08s/it]Running Inference:  28%|██▊       | 40/142 [01:29<02:55,  1.72s/it]Running Inference:  29%|██▉       | 41/142 [01:30<02:37,  1.56s/it]Running Inference:  30%|██▉       | 42/142 [01:32<02:34,  1.54s/it]Running Inference:  30%|███       | 43/142 [01:35<03:39,  2.22s/it]Running Inference:  31%|███       | 44/142 [01:39<04:26,  2.71s/it]Running Inference:  32%|███▏      | 45/142 [01:40<03:23,  2.09s/it]Running Inference:  32%|███▏      | 46/142 [01:41<02:44,  1.71s/it]Running Inference:  33%|███▎      | 47/142 [01:43<02:47,  1.76s/it]Running Inference:  34%|███▍      | 48/142 [01:43<02:10,  1.38s/it]Running Inference:  35%|███▍      | 49/142 [01:44<01:50,  1.19s/it]Running Inference:  35%|███▌      | 50/142 [01:45<01:50,  1.20s/it]Running Inference:  36%|███▌      | 51/142 [01:46<01:43,  1.13s/it]Running Inference:  37%|███▋      | 52/142 [01:47<01:44,  1.16s/it]Running Inference:  37%|███▋      | 53/142 [01:48<01:30,  1.02s/it]Running Inference:  38%|███▊      | 54/142 [01:49<01:24,  1.04it/s]Running Inference:  39%|███▊      | 55/142 [01:51<01:55,  1.33s/it]Running Inference:  39%|███▉      | 56/142 [01:53<02:02,  1.43s/it]Running Inference:  40%|████      | 57/142 [01:53<01:42,  1.21s/it]Running Inference:  41%|████      | 58/142 [01:54<01:28,  1.05s/it]Running Inference:  42%|████▏     | 59/142 [01:56<01:41,  1.22s/it]Running Inference:  42%|████▏     | 60/142 [01:56<01:23,  1.02s/it]Running Inference:  43%|████▎     | 61/142 [02:00<02:37,  1.95s/it]Running Inference:  44%|████▎     | 62/142 [02:01<02:01,  1.52s/it]Running Inference:  44%|████▍     | 63/142 [02:02<01:56,  1.47s/it]Running Inference:  45%|████▌     | 64/142 [02:03<01:42,  1.32s/it]Running Inference:  46%|████▌     | 65/142 [02:04<01:39,  1.29s/it]Running Inference:  46%|████▋     | 66/142 [02:05<01:31,  1.20s/it]Running Inference:  47%|████▋     | 67/142 [02:06<01:19,  1.05s/it]Running Inference:  48%|████▊     | 68/142 [02:06<01:04,  1.14it/s]Running Inference:  49%|████▊     | 69/142 [02:08<01:18,  1.08s/it]Running Inference:  49%|████▉     | 70/142 [02:09<01:24,  1.18s/it]Running Inference:  50%|█████     | 71/142 [02:13<02:24,  2.04s/it]Running Inference:  51%|█████     | 72/142 [02:14<01:55,  1.65s/it]Running Inference:  51%|█████▏    | 73/142 [02:15<01:38,  1.43s/it]Running Inference:  52%|█████▏    | 74/142 [02:16<01:29,  1.31s/it]Running Inference:  53%|█████▎    | 75/142 [02:17<01:09,  1.03s/it]Running Inference:  54%|█████▎    | 76/142 [02:17<00:59,  1.12it/s]Running Inference:  54%|█████▍    | 77/142 [02:18<01:06,  1.02s/it]Running Inference:  55%|█████▍    | 78/142 [02:22<01:58,  1.85s/it]Running Inference:  56%|█████▌    | 79/142 [02:27<02:47,  2.65s/it]Running Inference:  56%|█████▋    | 80/142 [02:27<02:04,  2.01s/it]Running Inference:  57%|█████▋    | 81/142 [02:29<01:55,  1.89s/it]Running Inference:  58%|█████▊    | 82/142 [02:29<01:27,  1.47s/it]Running Inference:  58%|█████▊    | 83/142 [02:31<01:21,  1.37s/it]Running Inference:  59%|█████▉    | 84/142 [02:32<01:26,  1.49s/it]Running Inference:  60%|█████▉    | 85/142 [02:33<01:10,  1.24s/it]Running Inference:  61%|██████    | 86/142 [02:34<01:03,  1.14s/it]Running Inference:  61%|██████▏   | 87/142 [02:36<01:21,  1.48s/it]Running Inference:  62%|██████▏   | 88/142 [02:40<02:04,  2.31s/it]Running Inference:  63%|██████▎   | 89/142 [02:42<01:46,  2.02s/it]Running Inference:  63%|██████▎   | 90/142 [02:43<01:27,  1.68s/it]Running Inference:  64%|██████▍   | 91/142 [02:44<01:18,  1.53s/it]Running Inference:  65%|██████▍   | 92/142 [02:48<01:53,  2.27s/it]Running Inference:  65%|██████▌   | 93/142 [02:49<01:40,  2.04s/it]Running Inference:  66%|██████▌   | 94/142 [02:50<01:17,  1.62s/it]Running Inference:  67%|██████▋   | 95/142 [02:55<02:02,  2.60s/it]Running Inference:  68%|██████▊   | 96/142 [02:56<01:37,  2.12s/it]Running Inference:  68%|██████▊   | 97/142 [03:00<01:59,  2.65s/it]Running Inference:  69%|██████▉   | 98/142 [03:00<01:31,  2.07s/it]Running Inference:  70%|██████▉   | 99/142 [03:05<01:56,  2.70s/it]Running Inference:  70%|███████   | 100/142 [03:05<01:26,  2.06s/it]Running Inference:  71%|███████   | 101/142 [03:06<01:08,  1.66s/it]Running Inference:  72%|███████▏  | 102/142 [03:07<01:05,  1.63s/it]Running Inference:  73%|███████▎  | 103/142 [03:10<01:15,  1.93s/it]Running Inference:  73%|███████▎  | 104/142 [03:14<01:38,  2.59s/it]Running Inference:  74%|███████▍  | 105/142 [03:19<01:56,  3.15s/it]Running Inference:  75%|███████▍  | 106/142 [03:25<02:26,  4.07s/it]Running Inference:  75%|███████▌  | 107/142 [03:27<02:01,  3.48s/it]Running Inference:  76%|███████▌  | 108/142 [03:27<01:28,  2.59s/it]Running Inference:  77%|███████▋  | 109/142 [03:31<01:38,  2.99s/it]Running Inference:  77%|███████▋  | 110/142 [03:33<01:24,  2.63s/it]Running Inference:  78%|███████▊  | 111/142 [03:34<01:02,  2.03s/it]Running Inference:  79%|███████▉  | 112/142 [03:34<00:47,  1.57s/it]Running Inference:  80%|███████▉  | 113/142 [03:36<00:45,  1.56s/it]Running Inference:  80%|████████  | 114/142 [03:37<00:38,  1.36s/it]Running Inference:  81%|████████  | 115/142 [03:37<00:31,  1.15s/it]Running Inference:  82%|████████▏ | 116/142 [03:41<00:49,  1.90s/it]Running Inference:  82%|████████▏ | 117/142 [03:42<00:41,  1.66s/it]Running Inference:  83%|████████▎ | 118/142 [03:43<00:36,  1.54s/it]Running Inference:  84%|████████▍ | 119/142 [03:44<00:27,  1.18s/it]Running Inference:  85%|████████▍ | 120/142 [03:45<00:26,  1.19s/it]Running Inference:  85%|████████▌ | 121/142 [03:49<00:44,  2.11s/it]Running Inference:  86%|████████▌ | 122/142 [03:50<00:35,  1.79s/it]Running Inference:  87%|████████▋ | 123/142 [03:54<00:45,  2.38s/it]Running Inference:  87%|████████▋ | 124/142 [03:55<00:32,  1.82s/it]Running Inference:  88%|████████▊ | 125/142 [03:56<00:26,  1.58s/it]Running Inference:  89%|████████▊ | 126/142 [03:56<00:20,  1.31s/it]Running Inference:  89%|████████▉ | 127/142 [03:58<00:21,  1.46s/it]Running Inference:  90%|█████████ | 128/142 [03:59<00:17,  1.26s/it]Running Inference:  91%|█████████ | 129/142 [03:59<00:13,  1.05s/it]Running Inference:  92%|█████████▏| 130/142 [04:04<00:25,  2.11s/it]Running Inference:  92%|█████████▏| 131/142 [04:08<00:28,  2.59s/it]Running Inference:  93%|█████████▎| 132/142 [04:08<00:19,  1.97s/it]Running Inference:  94%|█████████▎| 133/142 [04:10<00:18,  2.05s/it]Running Inference:  94%|█████████▍| 134/142 [04:12<00:14,  1.79s/it]Running Inference:  95%|█████████▌| 135/142 [04:12<00:10,  1.48s/it]Running Inference:  96%|█████████▌| 136/142 [04:14<00:09,  1.52s/it]Running Inference:  96%|█████████▋| 137/142 [04:18<00:11,  2.26s/it]Running Inference:  97%|█████████▋| 138/142 [04:18<00:06,  1.72s/it]Running Inference:  98%|█████████▊| 139/142 [04:21<00:05,  1.83s/it]Running Inference:  99%|█████████▊| 140/142 [04:22<00:03,  1.64s/it]Running Inference:  99%|█████████▉| 141/142 [04:26<00:02,  2.43s/it]Running Inference: 100%|██████████| 142/142 [04:28<00:00,  2.22s/it]Running Inference: 100%|██████████| 142/142 [04:28<00:00,  1.89s/it]
2025-12-14 21:40:32,730 - INFO - Inference completed.
2025-12-14 21:40:32,738 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 21:40:32,738 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.657 seconds.
Prefix dict has been built successfully.
2025-12-14 21:40:33,457 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 21:40:33,458 - INFO - Metrics:
39.6
2025-12-14 21:40:33,459 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_zh, ratio=0.3) on GPU 1

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:40:39,968 - INFO - Set deterministic seeds to 42
2025-12-14 21:40:39,968 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:40:39,968 - INFO - Starting evaluation run...
2025-12-14 21:40:39,968 - INFO - Output directory set to: longbenchresult
2025-12-14 21:40:39,968 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 21:40:39,968 - INFO - KV Press 'tova' setup.
2025-12-14 21:40:39,968 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:40:39,968 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.54it/s]
Device set to use cuda:0
2025-12-14 21:40:57,452 - INFO - Model pipeline loaded.
2025-12-14 21:40:57,452 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 21:41:05,607 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:41:05,607 - INFO - Dataset processed with 200 entries.
2025-12-14 21:41:05,617 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:01<04:36,  1.96s/it]Running Inference:   1%|▏         | 2/142 [00:06<08:13,  3.53s/it]Running Inference:   2%|▏         | 3/142 [00:07<05:43,  2.47s/it]Running Inference:   3%|▎         | 4/142 [00:08<04:10,  1.81s/it]Running Inference:   4%|▎         | 5/142 [00:09<03:07,  1.37s/it]Running Inference:   4%|▍         | 6/142 [00:09<02:20,  1.03s/it]Running Inference:   5%|▍         | 7/142 [00:10<02:35,  1.15s/it]Running Inference:   6%|▌         | 8/142 [00:19<07:45,  3.47s/it]Running Inference:   6%|▋         | 9/142 [00:23<08:04,  3.64s/it]Running Inference:   7%|▋         | 10/142 [00:31<11:02,  5.02s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:33<08:54,  4.08s/it]Running Inference:   8%|▊         | 12/142 [00:34<07:08,  3.30s/it]Running Inference:   9%|▉         | 13/142 [00:36<05:37,  2.61s/it]Running Inference:  10%|▉         | 14/142 [00:37<04:51,  2.27s/it]Running Inference:  11%|█         | 15/142 [00:38<03:45,  1.77s/it]Running Inference:  11%|█▏        | 16/142 [00:40<04:13,  2.01s/it]Running Inference:  12%|█▏        | 17/142 [00:45<05:47,  2.78s/it]Running Inference:  13%|█▎        | 18/142 [00:46<04:54,  2.38s/it]Running Inference:  13%|█▎        | 19/142 [00:51<06:16,  3.06s/it]Running Inference:  14%|█▍        | 20/142 [00:52<04:53,  2.41s/it]Running Inference:  15%|█▍        | 21/142 [01:00<08:30,  4.22s/it]Running Inference:  15%|█▌        | 22/142 [01:01<06:38,  3.32s/it]Running Inference:  16%|█▌        | 23/142 [01:02<04:58,  2.51s/it]Running Inference:  17%|█▋        | 24/142 [01:04<04:23,  2.23s/it]Running Inference:  18%|█▊        | 25/142 [01:04<03:26,  1.77s/it]Running Inference:  18%|█▊        | 26/142 [01:08<04:47,  2.48s/it]Running Inference:  19%|█▉        | 27/142 [01:10<04:00,  2.09s/it]Running Inference:  20%|█▉        | 28/142 [01:14<05:19,  2.81s/it]Running Inference:  20%|██        | 29/142 [01:16<04:48,  2.55s/it]Running Inference:  21%|██        | 30/142 [01:17<03:38,  1.95s/it]Running Inference:  22%|██▏       | 31/142 [01:24<06:33,  3.54s/it]Running Inference:  23%|██▎       | 32/142 [01:26<05:53,  3.21s/it]Running Inference:  23%|██▎       | 33/142 [01:27<04:19,  2.38s/it]Running Inference:  24%|██▍       | 34/142 [01:28<03:25,  1.91s/it]Running Inference:  25%|██▍       | 35/142 [01:32<04:34,  2.57s/it]Running Inference:  25%|██▌       | 36/142 [01:33<03:39,  2.08s/it]Running Inference:  26%|██▌       | 37/142 [01:33<02:54,  1.67s/it]Running Inference:  27%|██▋       | 38/142 [01:34<02:30,  1.45s/it]Running Inference:  27%|██▋       | 39/142 [01:38<03:55,  2.29s/it]Running Inference:  28%|██▊       | 40/142 [01:40<03:17,  1.94s/it]Running Inference:  29%|██▉       | 41/142 [01:41<02:53,  1.72s/it]Running Inference:  30%|██▉       | 42/142 [01:42<02:42,  1.62s/it]Running Inference:  30%|███       | 43/142 [01:46<03:51,  2.33s/it]Running Inference:  31%|███       | 44/142 [01:48<03:26,  2.11s/it]Running Inference:  32%|███▏      | 45/142 [01:52<04:35,  2.84s/it]Running Inference:  32%|███▏      | 46/142 [01:54<04:10,  2.61s/it]Running Inference:  33%|███▎      | 47/142 [01:56<03:48,  2.41s/it]Running Inference:  34%|███▍      | 48/142 [01:57<02:53,  1.85s/it]Running Inference:  35%|███▍      | 49/142 [01:58<02:21,  1.52s/it]Running Inference:  35%|███▌      | 50/142 [01:59<02:13,  1.45s/it]Running Inference:  36%|███▌      | 51/142 [02:00<01:55,  1.27s/it]Running Inference:  37%|███▋      | 52/142 [02:04<03:19,  2.22s/it]Running Inference:  37%|███▋      | 53/142 [02:05<02:36,  1.76s/it]Running Inference:  38%|███▊      | 54/142 [02:06<02:11,  1.49s/it]Running Inference:  39%|███▊      | 55/142 [02:08<02:35,  1.78s/it]Running Inference:  39%|███▉      | 56/142 [02:11<02:55,  2.04s/it]Running Inference:  40%|████      | 57/142 [02:15<03:42,  2.62s/it]Running Inference:  41%|████      | 58/142 [02:16<02:52,  2.05s/it]Running Inference:  42%|████▏     | 59/142 [02:17<02:40,  1.93s/it]Running Inference:  42%|████▏     | 60/142 [02:18<02:07,  1.55s/it]Running Inference:  43%|████▎     | 61/142 [02:22<03:14,  2.40s/it]Running Inference:  44%|████▎     | 62/142 [02:23<02:27,  1.84s/it]Running Inference:  44%|████▍     | 63/142 [02:24<02:20,  1.77s/it]Running Inference:  45%|████▌     | 64/142 [02:25<02:00,  1.54s/it]Running Inference:  46%|████▌     | 65/142 [02:27<01:53,  1.47s/it]Running Inference:  46%|████▋     | 66/142 [02:28<01:41,  1.33s/it]Running Inference:  47%|████▋     | 67/142 [02:28<01:26,  1.15s/it]Running Inference:  48%|████▊     | 68/142 [02:29<01:08,  1.08it/s]Running Inference:  49%|████▊     | 69/142 [02:34<02:33,  2.10s/it]Running Inference:  49%|████▉     | 70/142 [02:35<02:14,  1.86s/it]Running Inference:  50%|█████     | 71/142 [02:36<01:47,  1.51s/it]Running Inference:  51%|█████     | 72/142 [02:36<01:30,  1.30s/it]Running Inference:  51%|█████▏    | 73/142 [02:41<02:37,  2.28s/it]Running Inference:  52%|█████▏    | 74/142 [02:42<02:13,  1.96s/it]Running Inference:  53%|█████▎    | 75/142 [02:43<01:39,  1.49s/it]Running Inference:  54%|█████▎    | 76/142 [02:43<01:19,  1.21s/it]Running Inference:  54%|█████▍    | 77/142 [02:45<01:22,  1.28s/it]Running Inference:  55%|█████▍    | 78/142 [02:49<02:14,  2.10s/it]Running Inference:  56%|█████▌    | 79/142 [02:53<03:02,  2.89s/it]Running Inference:  56%|█████▋    | 80/142 [02:54<02:15,  2.18s/it]Running Inference:  57%|█████▋    | 81/142 [02:56<02:04,  2.04s/it]Running Inference:  58%|█████▊    | 82/142 [02:56<01:33,  1.56s/it]Running Inference:  58%|█████▊    | 83/142 [02:57<01:25,  1.44s/it]Running Inference:  59%|█████▉    | 84/142 [02:59<01:30,  1.57s/it]Running Inference:  60%|█████▉    | 85/142 [03:00<01:18,  1.37s/it]Running Inference:  61%|██████    | 86/142 [03:01<01:09,  1.24s/it]Running Inference:  61%|██████▏   | 87/142 [03:03<01:25,  1.55s/it]Running Inference:  62%|██████▏   | 88/142 [03:04<01:17,  1.44s/it]Running Inference:  63%|██████▎   | 89/142 [03:06<01:17,  1.47s/it]Running Inference:  63%|██████▎   | 90/142 [03:07<01:13,  1.41s/it]Running Inference:  64%|██████▍   | 91/142 [03:09<01:10,  1.39s/it]Running Inference:  65%|██████▍   | 92/142 [03:13<01:51,  2.24s/it]Running Inference:  65%|██████▌   | 93/142 [03:15<01:43,  2.10s/it]Running Inference:  66%|██████▌   | 94/142 [03:15<01:19,  1.65s/it]Running Inference:  67%|██████▋   | 95/142 [03:20<01:58,  2.51s/it]Running Inference:  68%|██████▊   | 96/142 [03:21<01:34,  2.06s/it]Running Inference:  68%|██████▊   | 97/142 [03:25<02:00,  2.67s/it]Running Inference:  69%|██████▉   | 98/142 [03:26<01:32,  2.09s/it]Running Inference:  70%|██████▉   | 99/142 [03:30<01:59,  2.78s/it]Running Inference:  70%|███████   | 100/142 [03:30<01:28,  2.12s/it]Running Inference:  71%|███████   | 101/142 [03:31<01:10,  1.72s/it]Running Inference:  72%|███████▏  | 102/142 [03:33<01:07,  1.69s/it]Running Inference:  73%|███████▎  | 103/142 [03:38<01:44,  2.68s/it]Running Inference:  73%|███████▎  | 104/142 [03:42<02:00,  3.18s/it]Running Inference:  74%|███████▍  | 105/142 [03:47<02:14,  3.65s/it]Running Inference:  75%|███████▍  | 106/142 [03:50<02:05,  3.49s/it]Running Inference:  75%|███████▌  | 107/142 [03:52<01:49,  3.13s/it]Running Inference:  76%|███████▌  | 108/142 [03:53<01:19,  2.35s/it]Running Inference:  77%|███████▋  | 109/142 [03:55<01:11,  2.17s/it]Running Inference:  77%|███████▋  | 110/142 [03:56<01:06,  2.07s/it]Running Inference:  78%|███████▊  | 111/142 [03:57<00:50,  1.64s/it]Running Inference:  79%|███████▉  | 112/142 [03:58<00:38,  1.29s/it]Running Inference:  80%|███████▉  | 113/142 [03:59<00:41,  1.43s/it]Running Inference:  80%|████████  | 114/142 [04:00<00:34,  1.23s/it]Running Inference:  81%|████████  | 115/142 [04:01<00:28,  1.07s/it]Running Inference:  82%|████████▏ | 116/142 [04:05<00:49,  1.90s/it]Running Inference:  82%|████████▏ | 117/142 [04:06<00:41,  1.66s/it]Running Inference:  83%|████████▎ | 118/142 [04:07<00:37,  1.58s/it]Running Inference:  84%|████████▍ | 119/142 [04:08<00:27,  1.21s/it]Running Inference:  85%|████████▍ | 120/142 [04:09<00:25,  1.17s/it]Running Inference:  85%|████████▌ | 121/142 [04:13<00:45,  2.16s/it]Running Inference:  86%|████████▌ | 122/142 [04:14<00:36,  1.80s/it]Running Inference:  87%|████████▋ | 123/142 [04:18<00:46,  2.44s/it]Running Inference:  87%|████████▋ | 124/142 [04:18<00:33,  1.85s/it]Running Inference:  88%|████████▊ | 125/142 [04:19<00:27,  1.61s/it]Running Inference:  89%|████████▊ | 126/142 [04:20<00:21,  1.34s/it]Running Inference:  89%|████████▉ | 127/142 [04:22<00:22,  1.49s/it]Running Inference:  90%|█████████ | 128/142 [04:23<00:17,  1.26s/it]Running Inference:  91%|█████████ | 129/142 [04:27<00:27,  2.10s/it]Running Inference:  92%|█████████▏| 130/142 [04:32<00:34,  2.91s/it]Running Inference:  92%|█████████▏| 131/142 [04:36<00:35,  3.21s/it]Running Inference:  93%|█████████▎| 132/142 [04:36<00:24,  2.40s/it]Running Inference:  94%|█████████▎| 133/142 [04:41<00:28,  3.18s/it]Running Inference:  94%|█████████▍| 134/142 [04:42<00:20,  2.59s/it]Running Inference:  95%|█████████▌| 135/142 [04:43<00:14,  2.05s/it]Running Inference:  96%|█████████▌| 136/142 [04:45<00:11,  1.88s/it]Running Inference:  96%|█████████▋| 137/142 [04:46<00:08,  1.69s/it]Running Inference:  97%|█████████▋| 138/142 [04:46<00:05,  1.30s/it]Running Inference:  98%|█████████▊| 139/142 [04:48<00:04,  1.55s/it]Running Inference:  99%|█████████▊| 140/142 [04:50<00:02,  1.45s/it]Running Inference:  99%|█████████▉| 141/142 [04:52<00:01,  1.69s/it]Running Inference: 100%|██████████| 142/142 [04:54<00:00,  1.72s/it]Running Inference: 100%|██████████| 142/142 [04:54<00:00,  2.07s/it]
2025-12-14 21:45:59,672 - INFO - Inference completed.
2025-12-14 21:45:59,680 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 21:45:59,680 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.671 seconds.
Prefix dict has been built successfully.
2025-12-14 21:46:00,412 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 21:46:00,412 - INFO - Metrics:
32.8
2025-12-14 21:46:00,414 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_zh, ratio=0.5) on GPU 1


========================================
LongBench Task: passage_retrieval_zh
========================================
----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:46:07,041 - INFO - Set deterministic seeds to 42
2025-12-14 21:46:07,041 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:46:07,041 - INFO - Starting evaluation run...
2025-12-14 21:46:07,041 - INFO - Output directory set to: longbenchresult
2025-12-14 21:46:07,041 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 21:46:07,041 - INFO - KV Press 'tova' setup.
2025-12-14 21:46:07,041 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:46:07,041 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.35it/s]
Device set to use cuda:0
2025-12-14 21:46:22,876 - INFO - Model pipeline loaded.
2025-12-14 21:46:22,877 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 200 examples [00:00, 3396.45 examples/s]
2025-12-14 21:46:53,310 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:46:53,310 - INFO - Dataset processed with 200 entries.
2025-12-14 21:46:53,323 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:40,  3.52s/it]Running Inference:   1%|          | 2/200 [00:06<10:03,  3.05s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:37,  2.93s/it]Running Inference:   2%|▏         | 4/200 [00:11<09:20,  2.86s/it]Running Inference:   2%|▎         | 5/200 [00:14<09:08,  2.81s/it]Running Inference:   3%|▎         | 6/200 [00:17<09:02,  2.80s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:58,  2.79s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:48,  2.75s/it]Running Inference:   4%|▍         | 9/200 [00:25<08:39,  2.72s/it]Running Inference:   5%|▌         | 10/200 [00:28<08:34,  2.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:30<08:37,  2.74s/it]Running Inference:   6%|▌         | 12/200 [00:33<08:40,  2.77s/it]Running Inference:   6%|▋         | 13/200 [00:36<08:37,  2.77s/it]Running Inference:   7%|▋         | 14/200 [00:39<08:30,  2.75s/it]Running Inference:   8%|▊         | 15/200 [00:41<08:21,  2.71s/it]Running Inference:   8%|▊         | 16/200 [00:44<08:18,  2.71s/it]Running Inference:   8%|▊         | 17/200 [00:47<08:12,  2.69s/it]Running Inference:   9%|▉         | 18/200 [00:49<08:09,  2.69s/it]Running Inference:  10%|▉         | 19/200 [00:52<08:12,  2.72s/it]Running Inference:  10%|█         | 20/200 [00:55<08:07,  2.71s/it]Running Inference:  10%|█         | 21/200 [00:58<08:04,  2.71s/it]Running Inference:  11%|█         | 22/200 [01:00<08:06,  2.74s/it]Running Inference:  12%|█▏        | 23/200 [01:03<08:03,  2.73s/it]Running Inference:  12%|█▏        | 24/200 [01:06<07:55,  2.70s/it]Running Inference:  12%|█▎        | 25/200 [01:08<07:57,  2.73s/it]Running Inference:  13%|█▎        | 26/200 [01:11<07:51,  2.71s/it]Running Inference:  14%|█▎        | 27/200 [01:14<07:45,  2.69s/it]Running Inference:  14%|█▍        | 28/200 [01:17<07:45,  2.71s/it]Running Inference:  14%|█▍        | 29/200 [01:19<07:41,  2.70s/it]Running Inference:  15%|█▌        | 30/200 [01:22<07:39,  2.70s/it]Running Inference:  16%|█▌        | 31/200 [01:25<07:38,  2.71s/it]Running Inference:  16%|█▌        | 32/200 [01:27<07:36,  2.72s/it]Running Inference:  16%|█▋        | 33/200 [01:30<07:32,  2.71s/it]Running Inference:  17%|█▋        | 34/200 [01:33<07:25,  2.68s/it]Running Inference:  18%|█▊        | 35/200 [01:35<07:22,  2.68s/it]Running Inference:  18%|█▊        | 36/200 [01:38<07:27,  2.73s/it]Running Inference:  18%|█▊        | 37/200 [01:41<07:28,  2.75s/it]Running Inference:  19%|█▉        | 38/200 [01:44<07:20,  2.72s/it]Running Inference:  20%|█▉        | 39/200 [01:46<07:15,  2.71s/it]Running Inference:  20%|██        | 40/200 [01:49<07:12,  2.70s/it]Running Inference:  20%|██        | 41/200 [01:52<07:13,  2.73s/it]Running Inference:  21%|██        | 42/200 [01:54<07:09,  2.72s/it]Running Inference:  22%|██▏       | 43/200 [01:57<07:07,  2.72s/it]Running Inference:  22%|██▏       | 44/200 [02:00<07:06,  2.73s/it]Running Inference:  22%|██▎       | 45/200 [02:03<07:00,  2.71s/it]Running Inference:  23%|██▎       | 46/200 [02:05<06:55,  2.70s/it]Running Inference:  24%|██▎       | 47/200 [02:08<06:55,  2.71s/it]Running Inference:  24%|██▍       | 48/200 [02:11<06:57,  2.74s/it]Running Inference:  24%|██▍       | 49/200 [02:14<06:53,  2.74s/it]Running Inference:  25%|██▌       | 50/200 [02:16<06:49,  2.73s/it]Running Inference:  26%|██▌       | 51/200 [02:19<06:46,  2.73s/it]Running Inference:  26%|██▌       | 52/200 [02:22<06:43,  2.73s/it]Running Inference:  26%|██▋       | 53/200 [02:24<06:39,  2.72s/it]Running Inference:  27%|██▋       | 54/200 [02:27<06:38,  2.73s/it]Running Inference:  28%|██▊       | 55/200 [02:30<06:36,  2.74s/it]Running Inference:  28%|██▊       | 56/200 [02:33<06:31,  2.72s/it]Running Inference:  28%|██▊       | 57/200 [02:35<06:26,  2.70s/it]Running Inference:  29%|██▉       | 58/200 [02:38<06:28,  2.74s/it]Running Inference:  30%|██▉       | 59/200 [02:41<06:29,  2.76s/it]Running Inference:  30%|███       | 60/200 [02:44<06:23,  2.74s/it]Running Inference:  30%|███       | 61/200 [02:46<06:16,  2.71s/it]Running Inference:  31%|███       | 62/200 [02:49<06:17,  2.74s/it]Running Inference:  32%|███▏      | 63/200 [02:52<06:13,  2.72s/it]Running Inference:  32%|███▏      | 64/200 [02:55<06:11,  2.73s/it]Running Inference:  32%|███▎      | 65/200 [02:57<06:08,  2.73s/it]Running Inference:  33%|███▎      | 66/200 [03:00<06:05,  2.73s/it]Running Inference:  34%|███▎      | 67/200 [03:03<06:08,  2.77s/it]Running Inference:  34%|███▍      | 68/200 [03:06<06:03,  2.75s/it]Running Inference:  34%|███▍      | 69/200 [03:08<05:56,  2.72s/it]Running Inference:  35%|███▌      | 70/200 [03:11<05:59,  2.77s/it]Running Inference:  36%|███▌      | 71/200 [03:14<05:55,  2.75s/it]Running Inference:  36%|███▌      | 72/200 [03:17<05:54,  2.77s/it]Running Inference:  36%|███▋      | 73/200 [03:19<05:45,  2.72s/it]Running Inference:  37%|███▋      | 74/200 [03:22<05:50,  2.78s/it]Running Inference:  38%|███▊      | 75/200 [03:25<05:45,  2.77s/it]Running Inference:  38%|███▊      | 76/200 [03:28<05:40,  2.75s/it]Running Inference:  38%|███▊      | 77/200 [03:30<05:36,  2.74s/it]Running Inference:  39%|███▉      | 78/200 [03:33<05:30,  2.71s/it]Running Inference:  40%|███▉      | 79/200 [03:36<05:25,  2.69s/it]Running Inference:  40%|████      | 80/200 [03:38<05:24,  2.70s/it]Running Inference:  40%|████      | 81/200 [03:41<05:23,  2.72s/it]Running Inference:  41%|████      | 82/200 [03:44<05:23,  2.74s/it]Running Inference:  42%|████▏     | 83/200 [03:45<04:38,  2.38s/it]Running Inference:  42%|████▏     | 84/200 [03:48<04:45,  2.46s/it]Running Inference:  42%|████▎     | 85/200 [03:51<04:51,  2.53s/it]Running Inference:  43%|████▎     | 86/200 [03:53<04:51,  2.55s/it]Running Inference:  44%|████▎     | 87/200 [03:56<04:51,  2.58s/it]Running Inference:  44%|████▍     | 88/200 [03:59<04:51,  2.60s/it]Running Inference:  44%|████▍     | 89/200 [04:01<04:55,  2.66s/it]Running Inference:  45%|████▌     | 90/200 [04:04<04:56,  2.70s/it]Running Inference:  46%|████▌     | 91/200 [04:07<04:54,  2.70s/it]Running Inference:  46%|████▌     | 92/200 [04:10<04:52,  2.71s/it]Running Inference:  46%|████▋     | 93/200 [04:12<04:49,  2.70s/it]Running Inference:  47%|████▋     | 94/200 [04:15<04:44,  2.68s/it]Running Inference:  48%|████▊     | 95/200 [04:18<04:43,  2.70s/it]Running Inference:  48%|████▊     | 96/200 [04:20<04:40,  2.70s/it]Running Inference:  48%|████▊     | 97/200 [04:23<04:38,  2.70s/it]Running Inference:  49%|████▉     | 98/200 [04:26<04:34,  2.69s/it]Running Inference:  50%|████▉     | 99/200 [04:29<04:33,  2.71s/it]Running Inference:  50%|█████     | 100/200 [04:31<04:33,  2.73s/it]Running Inference:  50%|█████     | 101/200 [04:34<04:27,  2.70s/it]Running Inference:  51%|█████     | 102/200 [04:37<04:28,  2.74s/it]Running Inference:  52%|█████▏    | 103/200 [04:39<04:23,  2.72s/it]Running Inference:  52%|█████▏    | 104/200 [04:42<04:23,  2.74s/it]Running Inference:  52%|█████▎    | 105/200 [04:45<04:20,  2.74s/it]Running Inference:  53%|█████▎    | 106/200 [04:48<04:15,  2.72s/it]Running Inference:  54%|█████▎    | 107/200 [04:50<04:12,  2.71s/it]Running Inference:  54%|█████▍    | 108/200 [04:53<04:10,  2.72s/it]Running Inference:  55%|█████▍    | 109/200 [04:56<04:05,  2.70s/it]Running Inference:  55%|█████▌    | 110/200 [04:59<04:05,  2.72s/it]Running Inference:  56%|█████▌    | 111/200 [05:01<04:03,  2.74s/it]Running Inference:  56%|█████▌    | 112/200 [05:04<04:04,  2.78s/it]Running Inference:  56%|█████▋    | 113/200 [05:07<03:59,  2.75s/it]Running Inference:  57%|█████▋    | 114/200 [05:10<03:57,  2.76s/it]Running Inference:  57%|█████▊    | 115/200 [05:12<03:53,  2.74s/it]Running Inference:  58%|█████▊    | 116/200 [05:15<03:47,  2.71s/it]Running Inference:  58%|█████▊    | 117/200 [05:18<03:45,  2.72s/it]Running Inference:  59%|█████▉    | 118/200 [05:21<03:44,  2.74s/it]Running Inference:  60%|█████▉    | 119/200 [05:23<03:43,  2.75s/it]Running Inference:  60%|██████    | 120/200 [05:26<03:37,  2.72s/it]Running Inference:  60%|██████    | 121/200 [05:29<03:34,  2.71s/it]Running Inference:  61%|██████    | 122/200 [05:31<03:30,  2.69s/it]Running Inference:  62%|██████▏   | 123/200 [05:34<03:25,  2.67s/it]Running Inference:  62%|██████▏   | 124/200 [05:37<03:24,  2.69s/it]Running Inference:  62%|██████▎   | 125/200 [05:39<03:21,  2.69s/it]Running Inference:  63%|██████▎   | 126/200 [05:42<03:19,  2.69s/it]Running Inference:  64%|██████▎   | 127/200 [05:45<03:17,  2.70s/it]Running Inference:  64%|██████▍   | 128/200 [05:47<03:12,  2.68s/it]Running Inference:  64%|██████▍   | 129/200 [05:50<03:11,  2.70s/it]Running Inference:  65%|██████▌   | 130/200 [05:53<03:09,  2.71s/it]Running Inference:  66%|██████▌   | 131/200 [05:56<03:09,  2.74s/it]Running Inference:  66%|██████▌   | 132/200 [05:58<03:07,  2.76s/it]Running Inference:  66%|██████▋   | 133/200 [06:01<03:03,  2.74s/it]Running Inference:  67%|██████▋   | 134/200 [06:04<03:00,  2.73s/it]Running Inference:  68%|██████▊   | 135/200 [06:07<02:58,  2.74s/it]Running Inference:  68%|██████▊   | 136/200 [06:09<02:54,  2.72s/it]Running Inference:  68%|██████▊   | 137/200 [06:12<02:51,  2.72s/it]Running Inference:  69%|██████▉   | 138/200 [06:15<02:47,  2.70s/it]Running Inference:  70%|██████▉   | 139/200 [06:17<02:44,  2.70s/it]Running Inference:  70%|███████   | 140/200 [06:20<02:44,  2.73s/it]Running Inference:  70%|███████   | 141/200 [06:23<02:41,  2.73s/it]Running Inference:  71%|███████   | 142/200 [06:26<02:37,  2.72s/it]Running Inference:  72%|███████▏  | 143/200 [06:28<02:34,  2.71s/it]Running Inference:  72%|███████▏  | 144/200 [06:31<02:30,  2.69s/it]Running Inference:  72%|███████▎  | 145/200 [06:34<02:27,  2.69s/it]Running Inference:  73%|███████▎  | 146/200 [06:36<02:23,  2.65s/it]Running Inference:  74%|███████▎  | 147/200 [06:39<02:21,  2.67s/it]Running Inference:  74%|███████▍  | 148/200 [06:42<02:19,  2.68s/it]Running Inference:  74%|███████▍  | 149/200 [06:44<02:16,  2.67s/it]Running Inference:  75%|███████▌  | 150/200 [06:47<02:14,  2.68s/it]Running Inference:  76%|███████▌  | 151/200 [06:50<02:10,  2.66s/it]Running Inference:  76%|███████▌  | 152/200 [06:52<02:08,  2.68s/it]Running Inference:  76%|███████▋  | 153/200 [06:55<02:05,  2.68s/it]Running Inference:  77%|███████▋  | 154/200 [06:58<02:03,  2.69s/it]Running Inference:  78%|███████▊  | 155/200 [07:01<02:03,  2.74s/it]Running Inference:  78%|███████▊  | 156/200 [07:03<02:00,  2.73s/it]Running Inference:  78%|███████▊  | 157/200 [07:06<01:57,  2.73s/it]Running Inference:  79%|███████▉  | 158/200 [07:09<01:54,  2.72s/it]Running Inference:  80%|███████▉  | 159/200 [07:11<01:51,  2.72s/it]Running Inference:  80%|████████  | 160/200 [07:14<01:48,  2.72s/it]Running Inference:  80%|████████  | 161/200 [07:17<01:45,  2.71s/it]Running Inference:  81%|████████  | 162/200 [07:19<01:42,  2.69s/it]Running Inference:  82%|████████▏ | 163/200 [07:22<01:40,  2.71s/it]Running Inference:  82%|████████▏ | 164/200 [07:25<01:37,  2.71s/it]Running Inference:  82%|████████▎ | 165/200 [07:28<01:34,  2.71s/it]Running Inference:  83%|████████▎ | 166/200 [07:30<01:32,  2.73s/it]Running Inference:  84%|████████▎ | 167/200 [07:33<01:29,  2.72s/it]Running Inference:  84%|████████▍ | 168/200 [07:36<01:27,  2.74s/it]Running Inference:  84%|████████▍ | 169/200 [07:39<01:24,  2.71s/it]Running Inference:  85%|████████▌ | 170/200 [07:41<01:21,  2.72s/it]Running Inference:  86%|████████▌ | 171/200 [07:44<01:18,  2.70s/it]Running Inference:  86%|████████▌ | 172/200 [07:47<01:15,  2.71s/it]Running Inference:  86%|████████▋ | 173/200 [07:49<01:12,  2.67s/it]Running Inference:  87%|████████▋ | 174/200 [07:52<01:09,  2.66s/it]Running Inference:  88%|████████▊ | 175/200 [07:55<01:06,  2.67s/it]Running Inference:  88%|████████▊ | 176/200 [07:57<01:04,  2.69s/it]Running Inference:  88%|████████▊ | 177/200 [08:00<01:02,  2.70s/it]Running Inference:  89%|████████▉ | 178/200 [08:03<01:00,  2.74s/it]Running Inference:  90%|████████▉ | 179/200 [08:06<00:57,  2.74s/it]Running Inference:  90%|█████████ | 180/200 [08:08<00:55,  2.77s/it]Running Inference:  90%|█████████ | 181/200 [08:11<00:52,  2.75s/it]Running Inference:  91%|█████████ | 182/200 [08:14<00:49,  2.77s/it]Running Inference:  92%|█████████▏| 183/200 [08:17<00:46,  2.76s/it]Running Inference:  92%|█████████▏| 184/200 [08:19<00:43,  2.74s/it]Running Inference:  92%|█████████▎| 185/200 [08:22<00:40,  2.72s/it]Running Inference:  93%|█████████▎| 186/200 [08:25<00:37,  2.71s/it]Running Inference:  94%|█████████▎| 187/200 [08:28<00:35,  2.74s/it]Running Inference:  94%|█████████▍| 188/200 [08:30<00:32,  2.71s/it]Running Inference:  94%|█████████▍| 189/200 [08:33<00:29,  2.71s/it]Running Inference:  95%|█████████▌| 190/200 [08:36<00:27,  2.74s/it]Running Inference:  96%|█████████▌| 191/200 [08:39<00:24,  2.76s/it]Running Inference:  96%|█████████▌| 192/200 [08:41<00:22,  2.76s/it]Running Inference:  96%|█████████▋| 193/200 [08:44<00:19,  2.74s/it]Running Inference:  97%|█████████▋| 194/200 [08:47<00:16,  2.78s/it]Running Inference:  98%|█████████▊| 195/200 [08:50<00:13,  2.78s/it]Running Inference:  98%|█████████▊| 196/200 [08:52<00:11,  2.75s/it]Running Inference:  98%|█████████▊| 197/200 [08:55<00:08,  2.79s/it]Running Inference:  99%|█████████▉| 198/200 [08:58<00:05,  2.76s/it]Running Inference: 100%|█████████▉| 199/200 [09:01<00:02,  2.77s/it]Running Inference: 100%|██████████| 200/200 [09:03<00:00,  2.76s/it]Running Inference: 100%|██████████| 200/200 [09:03<00:00,  2.72s/it]
2025-12-14 21:55:57,278 - INFO - Inference completed.
2025-12-14 21:55:57,288 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 21:55:57,288 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:55:57,289 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 21:55:57,289 - INFO - Metrics:
1.19
2025-12-14 21:55:57,291 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_zh, ratio=0.1) on GPU 1

----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:56:03,873 - INFO - Set deterministic seeds to 42
2025-12-14 21:56:03,873 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:56:03,873 - INFO - Starting evaluation run...
2025-12-14 21:56:03,873 - INFO - Output directory set to: longbenchresult
2025-12-14 21:56:03,874 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 21:56:03,874 - INFO - KV Press 'tova' setup.
2025-12-14 21:56:03,874 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:56:03,874 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.37it/s]
Device set to use cuda:0
2025-12-14 21:56:16,877 - INFO - Model pipeline loaded.
2025-12-14 21:56:16,877 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 21:56:24,752 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:56:24,752 - INFO - Dataset processed with 200 entries.
2025-12-14 21:56:24,766 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:45,  3.54s/it]Running Inference:   1%|          | 2/200 [00:06<10:07,  3.07s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:43,  2.96s/it]Running Inference:   2%|▏         | 4/200 [00:11<09:26,  2.89s/it]Running Inference:   2%|▎         | 5/200 [00:14<09:14,  2.85s/it]Running Inference:   3%|▎         | 6/200 [00:17<09:07,  2.82s/it]Running Inference:   4%|▎         | 7/200 [00:20<09:03,  2.82s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:52,  2.77s/it]Running Inference:   4%|▍         | 9/200 [00:25<08:43,  2.74s/it]Running Inference:   5%|▌         | 10/200 [00:28<08:38,  2.73s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:31<08:41,  2.76s/it]Running Inference:   6%|▌         | 12/200 [00:33<08:44,  2.79s/it]Running Inference:   6%|▋         | 13/200 [00:36<08:42,  2.79s/it]Running Inference:   7%|▋         | 14/200 [00:39<08:35,  2.77s/it]Running Inference:   8%|▊         | 15/200 [00:42<08:26,  2.74s/it]Running Inference:   8%|▊         | 16/200 [00:44<08:22,  2.73s/it]Running Inference:   8%|▊         | 17/200 [00:47<08:18,  2.72s/it]Running Inference:   9%|▉         | 18/200 [00:50<08:15,  2.72s/it]Running Inference:  10%|▉         | 19/200 [00:53<08:18,  2.75s/it]Running Inference:  10%|█         | 20/200 [00:55<08:13,  2.74s/it]Running Inference:  10%|█         | 21/200 [00:58<08:10,  2.74s/it]Running Inference:  11%|█         | 22/200 [01:01<08:12,  2.77s/it]Running Inference:  12%|█▏        | 23/200 [01:04<08:08,  2.76s/it]Running Inference:  12%|█▏        | 24/200 [01:06<08:01,  2.73s/it]Running Inference:  12%|█▎        | 25/200 [01:09<08:02,  2.76s/it]Running Inference:  13%|█▎        | 26/200 [01:12<07:56,  2.74s/it]Running Inference:  14%|█▎        | 27/200 [01:15<07:50,  2.72s/it]Running Inference:  14%|█▍        | 28/200 [01:17<07:50,  2.74s/it]Running Inference:  14%|█▍        | 29/200 [01:20<07:47,  2.74s/it]Running Inference:  15%|█▌        | 30/200 [01:23<07:45,  2.74s/it]Running Inference:  16%|█▌        | 31/200 [01:26<07:43,  2.74s/it]Running Inference:  16%|█▌        | 32/200 [01:28<07:41,  2.75s/it]Running Inference:  16%|█▋        | 33/200 [01:31<07:37,  2.74s/it]Running Inference:  17%|█▋        | 34/200 [01:34<07:29,  2.71s/it]Running Inference:  18%|█▊        | 35/200 [01:36<07:27,  2.71s/it]Running Inference:  18%|█▊        | 36/200 [01:39<07:32,  2.76s/it]Running Inference:  18%|█▊        | 37/200 [01:42<07:33,  2.78s/it]Running Inference:  19%|█▉        | 38/200 [01:45<07:25,  2.75s/it]Running Inference:  20%|█▉        | 39/200 [01:47<07:20,  2.74s/it]Running Inference:  20%|██        | 40/200 [01:50<07:16,  2.73s/it]Running Inference:  20%|██        | 41/200 [01:53<07:18,  2.76s/it]Running Inference:  21%|██        | 42/200 [01:56<07:14,  2.75s/it]Running Inference:  22%|██▏       | 43/200 [01:58<07:11,  2.75s/it]Running Inference:  22%|██▏       | 44/200 [02:01<07:10,  2.76s/it]Running Inference:  22%|██▎       | 45/200 [02:04<07:03,  2.73s/it]Running Inference:  23%|██▎       | 46/200 [02:07<06:57,  2.71s/it]Running Inference:  24%|██▎       | 47/200 [02:09<06:57,  2.73s/it]Running Inference:  24%|██▍       | 48/200 [02:12<06:59,  2.76s/it]Running Inference:  24%|██▍       | 49/200 [02:15<06:56,  2.76s/it]Running Inference:  25%|██▌       | 50/200 [02:18<06:52,  2.75s/it]Running Inference:  26%|██▌       | 51/200 [02:20<06:50,  2.76s/it]Running Inference:  26%|██▌       | 52/200 [02:23<06:47,  2.75s/it]Running Inference:  26%|██▋       | 53/200 [02:26<06:42,  2.74s/it]Running Inference:  27%|██▋       | 54/200 [02:29<06:41,  2.75s/it]Running Inference:  28%|██▊       | 55/200 [02:31<06:38,  2.75s/it]Running Inference:  28%|██▊       | 56/200 [02:34<06:35,  2.75s/it]Running Inference:  28%|██▊       | 57/200 [02:37<06:29,  2.72s/it]Running Inference:  29%|██▉       | 58/200 [02:40<06:30,  2.75s/it]Running Inference:  30%|██▉       | 59/200 [02:42<06:31,  2.78s/it]Running Inference:  30%|███       | 60/200 [02:45<06:25,  2.75s/it]Running Inference:  30%|███       | 61/200 [02:48<06:18,  2.72s/it]Running Inference:  31%|███       | 62/200 [02:51<06:19,  2.75s/it]Running Inference:  32%|███▏      | 63/200 [02:53<06:15,  2.74s/it]Running Inference:  32%|███▏      | 64/200 [02:56<06:13,  2.75s/it]Running Inference:  32%|███▎      | 65/200 [02:59<06:10,  2.74s/it]Running Inference:  33%|███▎      | 66/200 [03:02<06:07,  2.74s/it]Running Inference:  34%|███▎      | 67/200 [03:04<06:10,  2.78s/it]Running Inference:  34%|███▍      | 68/200 [03:07<06:05,  2.77s/it]Running Inference:  34%|███▍      | 69/200 [03:10<05:58,  2.74s/it]Running Inference:  35%|███▌      | 70/200 [03:13<06:01,  2.78s/it]Running Inference:  36%|███▌      | 71/200 [03:15<05:56,  2.76s/it]Running Inference:  36%|███▌      | 72/200 [03:18<05:55,  2.78s/it]Running Inference:  36%|███▋      | 73/200 [03:21<05:47,  2.73s/it]Running Inference:  37%|███▋      | 74/200 [03:24<05:52,  2.80s/it]Running Inference:  38%|███▊      | 75/200 [03:27<05:47,  2.78s/it]Running Inference:  38%|███▊      | 76/200 [03:29<05:42,  2.76s/it]Running Inference:  38%|███▊      | 77/200 [03:32<05:38,  2.75s/it]Running Inference:  39%|███▉      | 78/200 [03:35<05:32,  2.72s/it]Running Inference:  40%|███▉      | 79/200 [03:37<05:27,  2.71s/it]Running Inference:  40%|████      | 80/200 [03:40<05:27,  2.73s/it]Running Inference:  40%|████      | 81/200 [03:43<05:26,  2.74s/it]Running Inference:  41%|████      | 82/200 [03:46<05:25,  2.75s/it]Running Inference:  42%|████▏     | 83/200 [03:47<04:38,  2.38s/it]Running Inference:  42%|████▏     | 84/200 [03:50<04:46,  2.47s/it]Running Inference:  42%|████▎     | 85/200 [03:53<04:52,  2.54s/it]Running Inference:  43%|████▎     | 86/200 [03:55<04:50,  2.55s/it]Running Inference:  44%|████▎     | 87/200 [03:58<04:51,  2.58s/it]Running Inference:  44%|████▍     | 88/200 [04:01<04:53,  2.62s/it]Running Inference:  44%|████▍     | 89/200 [04:03<04:57,  2.68s/it]Running Inference:  45%|████▌     | 90/200 [04:06<04:58,  2.72s/it]Running Inference:  46%|████▌     | 91/200 [04:09<04:56,  2.72s/it]Running Inference:  46%|████▌     | 92/200 [04:12<04:56,  2.75s/it]Running Inference:  46%|████▋     | 93/200 [04:14<04:54,  2.75s/it]Running Inference:  47%|████▋     | 94/200 [04:17<04:48,  2.72s/it]Running Inference:  48%|████▊     | 95/200 [04:20<04:45,  2.72s/it]Running Inference:  48%|████▊     | 96/200 [04:23<04:42,  2.72s/it]Running Inference:  48%|████▊     | 97/200 [04:25<04:39,  2.72s/it]Running Inference:  49%|████▉     | 98/200 [04:28<04:36,  2.71s/it]Running Inference:  50%|████▉     | 99/200 [04:30<04:25,  2.63s/it]Running Inference:  50%|█████     | 100/200 [04:33<04:27,  2.68s/it]Running Inference:  50%|█████     | 101/200 [04:36<04:23,  2.67s/it]Running Inference:  51%|█████     | 102/200 [04:39<04:26,  2.72s/it]Running Inference:  52%|█████▏    | 103/200 [04:41<04:21,  2.70s/it]Running Inference:  52%|█████▏    | 104/200 [04:44<04:22,  2.73s/it]Running Inference:  52%|█████▎    | 105/200 [04:47<04:19,  2.73s/it]Running Inference:  53%|█████▎    | 106/200 [04:50<04:15,  2.71s/it]Running Inference:  54%|█████▎    | 107/200 [04:52<04:11,  2.71s/it]Running Inference:  54%|█████▍    | 108/200 [04:55<04:09,  2.71s/it]Running Inference:  55%|█████▍    | 109/200 [04:58<04:05,  2.70s/it]Running Inference:  55%|█████▌    | 110/200 [05:00<04:05,  2.72s/it]Running Inference:  56%|█████▌    | 111/200 [05:03<04:04,  2.74s/it]Running Inference:  56%|█████▌    | 112/200 [05:06<04:04,  2.78s/it]Running Inference:  56%|█████▋    | 113/200 [05:09<03:59,  2.75s/it]Running Inference:  57%|█████▋    | 114/200 [05:12<03:57,  2.76s/it]Running Inference:  57%|█████▊    | 115/200 [05:14<03:53,  2.74s/it]Running Inference:  58%|█████▊    | 116/200 [05:17<03:48,  2.72s/it]Running Inference:  58%|█████▊    | 117/200 [05:20<03:45,  2.72s/it]Running Inference:  59%|█████▉    | 118/200 [05:22<03:44,  2.74s/it]Running Inference:  60%|█████▉    | 119/200 [05:25<03:43,  2.76s/it]Running Inference:  60%|██████    | 120/200 [05:28<03:37,  2.72s/it]Running Inference:  60%|██████    | 121/200 [05:31<03:34,  2.72s/it]Running Inference:  61%|██████    | 122/200 [05:33<03:30,  2.70s/it]Running Inference:  62%|██████▏   | 123/200 [05:36<03:26,  2.68s/it]Running Inference:  62%|██████▏   | 124/200 [05:39<03:25,  2.70s/it]Running Inference:  62%|██████▎   | 125/200 [05:41<03:22,  2.70s/it]Running Inference:  63%|██████▎   | 126/200 [05:44<03:17,  2.66s/it]Running Inference:  64%|██████▎   | 127/200 [05:46<03:00,  2.48s/it]Running Inference:  64%|██████▍   | 128/200 [05:49<03:01,  2.53s/it]Running Inference:  64%|██████▍   | 129/200 [05:51<03:04,  2.59s/it]Running Inference:  65%|██████▌   | 130/200 [05:54<03:04,  2.63s/it]Running Inference:  66%|██████▌   | 131/200 [05:57<03:05,  2.69s/it]Running Inference:  66%|██████▌   | 132/200 [06:00<03:04,  2.72s/it]Running Inference:  66%|██████▋   | 133/200 [06:02<03:02,  2.72s/it]Running Inference:  67%|██████▋   | 134/200 [06:05<02:59,  2.72s/it]Running Inference:  68%|██████▊   | 135/200 [06:08<02:57,  2.73s/it]Running Inference:  68%|██████▊   | 136/200 [06:10<02:53,  2.72s/it]Running Inference:  68%|██████▊   | 137/200 [06:13<02:50,  2.71s/it]Running Inference:  69%|██████▉   | 138/200 [06:16<02:47,  2.70s/it]Running Inference:  70%|██████▉   | 139/200 [06:19<02:44,  2.70s/it]Running Inference:  70%|███████   | 140/200 [06:21<02:44,  2.73s/it]Running Inference:  70%|███████   | 141/200 [06:24<02:41,  2.73s/it]Running Inference:  71%|███████   | 142/200 [06:27<02:38,  2.72s/it]Running Inference:  72%|███████▏  | 143/200 [06:29<02:34,  2.71s/it]Running Inference:  72%|███████▏  | 144/200 [06:32<02:30,  2.69s/it]Running Inference:  72%|███████▎  | 145/200 [06:35<02:28,  2.69s/it]Running Inference:  73%|███████▎  | 146/200 [06:37<02:23,  2.65s/it]Running Inference:  74%|███████▎  | 147/200 [06:40<02:21,  2.68s/it]Running Inference:  74%|███████▍  | 148/200 [06:43<02:19,  2.68s/it]Running Inference:  74%|███████▍  | 149/200 [06:45<02:16,  2.68s/it]Running Inference:  75%|███████▌  | 150/200 [06:48<02:14,  2.69s/it]Running Inference:  76%|███████▌  | 151/200 [06:51<02:10,  2.67s/it]Running Inference:  76%|███████▌  | 152/200 [06:54<02:08,  2.68s/it]Running Inference:  76%|███████▋  | 153/200 [06:56<02:06,  2.68s/it]Running Inference:  77%|███████▋  | 154/200 [06:59<02:03,  2.69s/it]Running Inference:  78%|███████▊  | 155/200 [07:02<02:03,  2.74s/it]Running Inference:  78%|███████▊  | 156/200 [07:04<02:00,  2.73s/it]Running Inference:  78%|███████▊  | 157/200 [07:07<01:57,  2.73s/it]Running Inference:  79%|███████▉  | 158/200 [07:10<01:54,  2.72s/it]Running Inference:  80%|███████▉  | 159/200 [07:13<01:51,  2.72s/it]Running Inference:  80%|████████  | 160/200 [07:15<01:48,  2.72s/it]Running Inference:  80%|████████  | 161/200 [07:18<01:45,  2.72s/it]Running Inference:  81%|████████  | 162/200 [07:21<01:42,  2.69s/it]Running Inference:  82%|████████▏ | 163/200 [07:23<01:40,  2.71s/it]Running Inference:  82%|████████▏ | 164/200 [07:26<01:37,  2.71s/it]Running Inference:  82%|████████▎ | 165/200 [07:29<01:35,  2.72s/it]Running Inference:  83%|████████▎ | 166/200 [07:32<01:32,  2.73s/it]Running Inference:  84%|████████▎ | 167/200 [07:34<01:29,  2.72s/it]Running Inference:  84%|████████▍ | 168/200 [07:37<01:27,  2.75s/it]Running Inference:  84%|████████▍ | 169/200 [07:40<01:24,  2.72s/it]Running Inference:  85%|████████▌ | 170/200 [07:43<01:21,  2.73s/it]Running Inference:  86%|████████▌ | 171/200 [07:45<01:18,  2.70s/it]Running Inference:  86%|████████▌ | 172/200 [07:48<01:16,  2.72s/it]Running Inference:  86%|████████▋ | 173/200 [07:51<01:12,  2.67s/it]Running Inference:  87%|████████▋ | 174/200 [07:53<01:09,  2.66s/it]Running Inference:  88%|████████▊ | 175/200 [07:56<01:06,  2.68s/it]Running Inference:  88%|████████▊ | 176/200 [07:59<01:04,  2.70s/it]Running Inference:  88%|████████▊ | 177/200 [08:01<01:02,  2.71s/it]Running Inference:  89%|████████▉ | 178/200 [08:04<01:00,  2.74s/it]Running Inference:  90%|████████▉ | 179/200 [08:07<00:57,  2.74s/it]Running Inference:  90%|█████████ | 180/200 [08:10<00:55,  2.77s/it]Running Inference:  90%|█████████ | 181/200 [08:12<00:52,  2.76s/it]Running Inference:  91%|█████████ | 182/200 [08:15<00:49,  2.77s/it]Running Inference:  92%|█████████▏| 183/200 [08:18<00:46,  2.76s/it]Running Inference:  92%|█████████▏| 184/200 [08:21<00:43,  2.74s/it]Running Inference:  92%|█████████▎| 185/200 [08:23<00:40,  2.72s/it]Running Inference:  93%|█████████▎| 186/200 [08:26<00:37,  2.70s/it]Running Inference:  94%|█████████▎| 187/200 [08:29<00:35,  2.74s/it]Running Inference:  94%|█████████▍| 188/200 [08:32<00:32,  2.71s/it]Running Inference:  94%|█████████▍| 189/200 [08:34<00:29,  2.71s/it]Running Inference:  95%|█████████▌| 190/200 [08:37<00:27,  2.73s/it]Running Inference:  96%|█████████▌| 191/200 [08:40<00:24,  2.73s/it]Running Inference:  96%|█████████▌| 192/200 [08:42<00:21,  2.72s/it]Running Inference:  96%|█████████▋| 193/200 [08:45<00:18,  2.70s/it]Running Inference:  97%|█████████▋| 194/200 [08:48<00:16,  2.74s/it]Running Inference:  98%|█████████▊| 195/200 [08:51<00:13,  2.74s/it]Running Inference:  98%|█████████▊| 196/200 [08:53<00:10,  2.70s/it]Running Inference:  98%|█████████▊| 197/200 [08:56<00:08,  2.74s/it]Running Inference:  99%|█████████▉| 198/200 [08:59<00:05,  2.71s/it]Running Inference: 100%|█████████▉| 199/200 [09:01<00:02,  2.71s/it]Running Inference: 100%|██████████| 200/200 [09:04<00:00,  2.71s/it]Running Inference: 100%|██████████| 200/200 [09:04<00:00,  2.72s/it]
2025-12-14 22:05:29,447 - INFO - Inference completed.
2025-12-14 22:05:29,457 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 22:05:29,457 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:05:29,458 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 22:05:29,458 - INFO - Metrics:
0.07
2025-12-14 22:05:29,460 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_zh, ratio=0.2) on GPU 1

----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:05:35,972 - INFO - Set deterministic seeds to 42
2025-12-14 22:05:35,972 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:05:35,972 - INFO - Starting evaluation run...
2025-12-14 22:05:35,972 - INFO - Output directory set to: longbenchresult
2025-12-14 22:05:35,973 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 22:05:35,973 - INFO - KV Press 'tova' setup.
2025-12-14 22:05:35,973 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:05:35,973 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.56it/s]
Device set to use cuda:0
2025-12-14 22:05:49,280 - INFO - Model pipeline loaded.
2025-12-14 22:05:49,280 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 22:05:55,747 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:05:55,747 - INFO - Dataset processed with 200 entries.
2025-12-14 22:05:55,759 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:39,  3.51s/it]Running Inference:   1%|          | 2/200 [00:06<10:02,  3.04s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:37,  2.93s/it]Running Inference:   2%|▏         | 4/200 [00:11<09:20,  2.86s/it]Running Inference:   2%|▎         | 5/200 [00:14<09:09,  2.82s/it]Running Inference:   3%|▎         | 6/200 [00:17<09:02,  2.80s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:58,  2.79s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:47,  2.75s/it]Running Inference:   4%|▍         | 9/200 [00:25<08:39,  2.72s/it]Running Inference:   5%|▌         | 10/200 [00:28<08:34,  2.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:30<08:37,  2.74s/it]Running Inference:   6%|▌         | 12/200 [00:33<08:40,  2.77s/it]Running Inference:   6%|▋         | 13/200 [00:36<08:37,  2.77s/it]Running Inference:   7%|▋         | 14/200 [00:39<08:30,  2.75s/it]Running Inference:   8%|▊         | 15/200 [00:41<08:21,  2.71s/it]Running Inference:   8%|▊         | 16/200 [00:44<08:18,  2.71s/it]Running Inference:   8%|▊         | 17/200 [00:47<08:13,  2.70s/it]Running Inference:   9%|▉         | 18/200 [00:49<08:10,  2.69s/it]Running Inference:  10%|▉         | 19/200 [00:52<08:13,  2.72s/it]Running Inference:  10%|█         | 20/200 [00:55<08:08,  2.71s/it]Running Inference:  10%|█         | 21/200 [00:58<08:04,  2.71s/it]Running Inference:  11%|█         | 22/200 [01:00<08:07,  2.74s/it]Running Inference:  12%|█▏        | 23/200 [01:03<08:04,  2.74s/it]Running Inference:  12%|█▏        | 24/200 [01:06<07:56,  2.71s/it]Running Inference:  12%|█▎        | 25/200 [01:08<07:57,  2.73s/it]Running Inference:  13%|█▎        | 26/200 [01:11<07:52,  2.71s/it]Running Inference:  14%|█▎        | 27/200 [01:14<07:46,  2.70s/it]Running Inference:  14%|█▍        | 28/200 [01:17<07:46,  2.71s/it]Running Inference:  14%|█▍        | 29/200 [01:19<07:42,  2.70s/it]Running Inference:  15%|█▌        | 30/200 [01:22<07:39,  2.71s/it]Running Inference:  16%|█▌        | 31/200 [01:25<07:38,  2.71s/it]Running Inference:  16%|█▌        | 32/200 [01:27<07:39,  2.74s/it]Running Inference:  16%|█▋        | 33/200 [01:30<07:34,  2.72s/it]Running Inference:  17%|█▋        | 34/200 [01:33<07:25,  2.69s/it]Running Inference:  18%|█▊        | 35/200 [01:35<06:58,  2.54s/it]Running Inference:  18%|█▊        | 36/200 [01:38<07:09,  2.62s/it]Running Inference:  18%|█▊        | 37/200 [01:41<07:16,  2.68s/it]Running Inference:  19%|█▉        | 38/200 [01:43<07:13,  2.68s/it]Running Inference:  20%|█▉        | 39/200 [01:46<07:11,  2.68s/it]Running Inference:  20%|██        | 40/200 [01:49<07:08,  2.68s/it]Running Inference:  20%|██        | 41/200 [01:51<07:10,  2.71s/it]Running Inference:  21%|██        | 42/200 [01:54<07:05,  2.70s/it]Running Inference:  22%|██▏       | 43/200 [01:57<06:57,  2.66s/it]Running Inference:  22%|██▏       | 44/200 [01:59<06:57,  2.68s/it]Running Inference:  22%|██▎       | 45/200 [02:02<06:52,  2.66s/it]Running Inference:  23%|██▎       | 46/200 [02:05<06:47,  2.65s/it]Running Inference:  24%|██▎       | 47/200 [02:07<06:49,  2.67s/it]Running Inference:  24%|██▍       | 48/200 [02:10<06:51,  2.71s/it]Running Inference:  24%|██▍       | 49/200 [02:13<06:49,  2.71s/it]Running Inference:  25%|██▌       | 50/200 [02:16<06:45,  2.70s/it]Running Inference:  26%|██▌       | 51/200 [02:18<06:43,  2.71s/it]Running Inference:  26%|██▌       | 52/200 [02:21<06:40,  2.71s/it]Running Inference:  26%|██▋       | 53/200 [02:24<06:36,  2.70s/it]Running Inference:  27%|██▋       | 54/200 [02:26<06:35,  2.71s/it]Running Inference:  28%|██▊       | 55/200 [02:29<06:33,  2.72s/it]Running Inference:  28%|██▊       | 56/200 [02:32<06:29,  2.71s/it]Running Inference:  28%|██▊       | 57/200 [02:34<06:23,  2.68s/it]Running Inference:  29%|██▉       | 58/200 [02:37<06:25,  2.72s/it]Running Inference:  30%|██▉       | 59/200 [02:40<06:26,  2.74s/it]Running Inference:  30%|███       | 60/200 [02:43<06:20,  2.72s/it]Running Inference:  30%|███       | 61/200 [02:45<06:13,  2.69s/it]Running Inference:  31%|███       | 62/200 [02:48<06:14,  2.72s/it]Running Inference:  32%|███▏      | 63/200 [02:51<06:10,  2.70s/it]Running Inference:  32%|███▏      | 64/200 [02:53<06:08,  2.71s/it]Running Inference:  32%|███▎      | 65/200 [02:56<06:05,  2.71s/it]Running Inference:  33%|███▎      | 66/200 [02:59<06:02,  2.71s/it]Running Inference:  34%|███▎      | 67/200 [03:02<06:05,  2.75s/it]Running Inference:  34%|███▍      | 68/200 [03:04<06:00,  2.73s/it]Running Inference:  34%|███▍      | 69/200 [03:07<05:53,  2.70s/it]Running Inference:  35%|███▌      | 70/200 [03:10<05:56,  2.74s/it]Running Inference:  36%|███▌      | 71/200 [03:13<05:51,  2.73s/it]Running Inference:  36%|███▌      | 72/200 [03:15<05:51,  2.74s/it]Running Inference:  36%|███▋      | 73/200 [03:18<05:42,  2.70s/it]Running Inference:  37%|███▋      | 74/200 [03:21<05:47,  2.76s/it]Running Inference:  38%|███▊      | 75/200 [03:24<05:42,  2.74s/it]Running Inference:  38%|███▊      | 76/200 [03:26<05:37,  2.72s/it]Running Inference:  38%|███▊      | 77/200 [03:29<05:33,  2.72s/it]Running Inference:  39%|███▉      | 78/200 [03:32<05:27,  2.69s/it]Running Inference:  40%|███▉      | 79/200 [03:34<05:22,  2.67s/it]Running Inference:  40%|████      | 80/200 [03:37<05:22,  2.68s/it]Running Inference:  40%|████      | 81/200 [03:40<05:20,  2.70s/it]Running Inference:  41%|████      | 82/200 [03:42<05:20,  2.71s/it]Running Inference:  42%|████▏     | 83/200 [03:44<04:35,  2.35s/it]Running Inference:  42%|████▏     | 84/200 [03:47<04:43,  2.44s/it]Running Inference:  42%|████▎     | 85/200 [03:49<04:48,  2.51s/it]Running Inference:  43%|████▎     | 86/200 [03:52<04:50,  2.55s/it]Running Inference:  44%|████▎     | 87/200 [03:54<04:49,  2.57s/it]Running Inference:  44%|████▍     | 88/200 [03:57<04:50,  2.59s/it]Running Inference:  44%|████▍     | 89/200 [04:00<04:53,  2.65s/it]Running Inference:  45%|████▌     | 90/200 [04:03<04:54,  2.68s/it]Running Inference:  46%|████▌     | 91/200 [04:05<04:52,  2.68s/it]Running Inference:  46%|████▌     | 92/200 [04:08<04:50,  2.69s/it]Running Inference:  46%|████▋     | 93/200 [04:09<03:50,  2.15s/it]Running Inference:  47%|████▋     | 94/200 [04:12<04:03,  2.29s/it]Running Inference:  48%|████▊     | 95/200 [04:14<04:12,  2.41s/it]Running Inference:  48%|████▊     | 96/200 [04:17<04:19,  2.49s/it]Running Inference:  48%|████▊     | 97/200 [04:20<04:22,  2.55s/it]Running Inference:  49%|████▉     | 98/200 [04:22<04:23,  2.58s/it]Running Inference:  50%|████▉     | 99/200 [04:25<04:25,  2.62s/it]Running Inference:  50%|█████     | 100/200 [04:28<04:26,  2.66s/it]Running Inference:  50%|█████     | 101/200 [04:30<04:22,  2.65s/it]Running Inference:  51%|█████     | 102/200 [04:33<04:24,  2.70s/it]Running Inference:  52%|█████▏    | 103/200 [04:36<04:19,  2.68s/it]Running Inference:  52%|█████▏    | 104/200 [04:39<04:20,  2.71s/it]Running Inference:  52%|█████▎    | 105/200 [04:41<04:16,  2.71s/it]Running Inference:  53%|█████▎    | 106/200 [04:44<04:13,  2.69s/it]Running Inference:  54%|█████▎    | 107/200 [04:47<04:09,  2.68s/it]Running Inference:  54%|█████▍    | 108/200 [04:49<04:07,  2.69s/it]Running Inference:  55%|█████▍    | 109/200 [04:52<04:03,  2.67s/it]Running Inference:  55%|█████▌    | 110/200 [04:55<04:02,  2.70s/it]Running Inference:  56%|█████▌    | 111/200 [04:57<04:01,  2.72s/it]Running Inference:  56%|█████▌    | 112/200 [05:00<04:02,  2.76s/it]Running Inference:  56%|█████▋    | 113/200 [05:03<03:57,  2.73s/it]Running Inference:  57%|█████▋    | 114/200 [05:06<03:55,  2.74s/it]Running Inference:  57%|█████▊    | 115/200 [05:08<03:51,  2.72s/it]Running Inference:  58%|█████▊    | 116/200 [05:09<02:54,  2.08s/it]Running Inference:  58%|█████▊    | 117/200 [05:12<03:08,  2.27s/it]Running Inference:  59%|█████▉    | 118/200 [05:14<03:18,  2.42s/it]Running Inference:  60%|█████▉    | 119/200 [05:17<03:24,  2.52s/it]Running Inference:  60%|██████    | 120/200 [05:20<03:23,  2.55s/it]Running Inference:  60%|██████    | 121/200 [05:23<03:24,  2.59s/it]Running Inference:  61%|██████    | 122/200 [05:25<03:22,  2.60s/it]Running Inference:  62%|██████▏   | 123/200 [05:28<03:20,  2.60s/it]Running Inference:  62%|██████▏   | 124/200 [05:30<03:20,  2.64s/it]Running Inference:  62%|██████▎   | 125/200 [05:33<03:18,  2.65s/it]Running Inference:  63%|██████▎   | 126/200 [05:36<03:16,  2.66s/it]Running Inference:  64%|██████▎   | 127/200 [05:39<03:14,  2.67s/it]Running Inference:  64%|██████▍   | 128/200 [05:41<03:11,  2.65s/it]Running Inference:  64%|██████▍   | 129/200 [05:44<03:09,  2.68s/it]Running Inference:  65%|██████▌   | 130/200 [05:47<03:07,  2.68s/it]Running Inference:  66%|██████▌   | 131/200 [05:49<03:07,  2.72s/it]Running Inference:  66%|██████▌   | 132/200 [05:52<03:05,  2.73s/it]Running Inference:  66%|██████▋   | 133/200 [05:55<03:02,  2.72s/it]Running Inference:  67%|██████▋   | 134/200 [05:58<02:58,  2.71s/it]Running Inference:  68%|██████▊   | 135/200 [06:00<02:56,  2.71s/it]Running Inference:  68%|██████▊   | 136/200 [06:03<02:52,  2.70s/it]Running Inference:  68%|██████▊   | 137/200 [06:06<02:49,  2.70s/it]Running Inference:  69%|██████▉   | 138/200 [06:08<02:46,  2.68s/it]Running Inference:  70%|██████▉   | 139/200 [06:11<02:43,  2.68s/it]Running Inference:  70%|███████   | 140/200 [06:14<02:42,  2.71s/it]Running Inference:  70%|███████   | 141/200 [06:16<02:39,  2.71s/it]Running Inference:  71%|███████   | 142/200 [06:19<02:36,  2.70s/it]Running Inference:  72%|███████▏  | 143/200 [06:22<02:33,  2.69s/it]Running Inference:  72%|███████▏  | 144/200 [06:24<02:29,  2.66s/it]Running Inference:  72%|███████▎  | 145/200 [06:27<02:26,  2.66s/it]Running Inference:  73%|███████▎  | 146/200 [06:30<02:21,  2.63s/it]Running Inference:  74%|███████▎  | 147/200 [06:32<02:20,  2.65s/it]Running Inference:  74%|███████▍  | 148/200 [06:35<02:17,  2.65s/it]Running Inference:  74%|███████▍  | 149/200 [06:38<02:15,  2.65s/it]Running Inference:  75%|███████▌  | 150/200 [06:40<02:13,  2.66s/it]Running Inference:  76%|███████▌  | 151/200 [06:43<02:09,  2.64s/it]Running Inference:  76%|███████▌  | 152/200 [06:46<02:07,  2.66s/it]Running Inference:  76%|███████▋  | 153/200 [06:48<02:04,  2.66s/it]Running Inference:  77%|███████▋  | 154/200 [06:51<02:02,  2.66s/it]Running Inference:  78%|███████▊  | 155/200 [06:54<02:02,  2.72s/it]Running Inference:  78%|███████▊  | 156/200 [06:56<01:59,  2.71s/it]Running Inference:  78%|███████▊  | 157/200 [06:59<01:56,  2.71s/it]Running Inference:  79%|███████▉  | 158/200 [07:02<01:53,  2.70s/it]Running Inference:  80%|███████▉  | 159/200 [07:04<01:50,  2.70s/it]Running Inference:  80%|████████  | 160/200 [07:07<01:47,  2.70s/it]Running Inference:  80%|████████  | 161/200 [07:10<01:44,  2.69s/it]Running Inference:  81%|████████  | 162/200 [07:12<01:41,  2.67s/it]Running Inference:  82%|████████▏ | 163/200 [07:15<01:39,  2.68s/it]Running Inference:  82%|████████▏ | 164/200 [07:18<01:36,  2.69s/it]Running Inference:  82%|████████▎ | 165/200 [07:21<01:34,  2.69s/it]Running Inference:  83%|████████▎ | 166/200 [07:23<01:31,  2.70s/it]Running Inference:  84%|████████▎ | 167/200 [07:26<01:29,  2.70s/it]Running Inference:  84%|████████▍ | 168/200 [07:29<01:27,  2.72s/it]Running Inference:  84%|████████▍ | 169/200 [07:31<01:23,  2.69s/it]Running Inference:  85%|████████▌ | 170/200 [07:34<01:21,  2.70s/it]Running Inference:  86%|████████▌ | 171/200 [07:37<01:17,  2.67s/it]Running Inference:  86%|████████▌ | 172/200 [07:39<01:15,  2.69s/it]Running Inference:  86%|████████▋ | 173/200 [07:42<01:11,  2.65s/it]Running Inference:  87%|████████▋ | 174/200 [07:45<01:08,  2.64s/it]Running Inference:  88%|████████▊ | 175/200 [07:47<01:06,  2.65s/it]Running Inference:  88%|████████▊ | 176/200 [07:50<01:04,  2.67s/it]Running Inference:  88%|████████▊ | 177/200 [07:53<01:01,  2.69s/it]Running Inference:  89%|████████▉ | 178/200 [07:56<00:59,  2.71s/it]Running Inference:  90%|████████▉ | 179/200 [07:58<00:56,  2.71s/it]Running Inference:  90%|█████████ | 180/200 [08:01<00:54,  2.74s/it]Running Inference:  90%|█████████ | 181/200 [08:04<00:51,  2.73s/it]Running Inference:  91%|█████████ | 182/200 [08:07<00:49,  2.75s/it]Running Inference:  92%|█████████▏| 183/200 [08:09<00:46,  2.73s/it]Running Inference:  92%|█████████▏| 184/200 [08:12<00:43,  2.71s/it]Running Inference:  92%|█████████▎| 185/200 [08:15<00:40,  2.70s/it]Running Inference:  93%|█████████▎| 186/200 [08:17<00:37,  2.68s/it]Running Inference:  94%|█████████▎| 187/200 [08:20<00:35,  2.71s/it]Running Inference:  94%|█████████▍| 188/200 [08:23<00:32,  2.69s/it]Running Inference:  94%|█████████▍| 189/200 [08:25<00:29,  2.69s/it]Running Inference:  95%|█████████▌| 190/200 [08:28<00:27,  2.70s/it]Running Inference:  96%|█████████▌| 191/200 [08:31<00:24,  2.71s/it]Running Inference:  96%|█████████▌| 192/200 [08:33<00:21,  2.70s/it]Running Inference:  96%|█████████▋| 193/200 [08:36<00:18,  2.68s/it]Running Inference:  97%|█████████▋| 194/200 [08:39<00:16,  2.71s/it]Running Inference:  98%|█████████▊| 195/200 [08:42<00:13,  2.71s/it]Running Inference:  98%|█████████▊| 196/200 [08:44<00:10,  2.68s/it]Running Inference:  98%|█████████▊| 197/200 [08:47<00:08,  2.71s/it]Running Inference:  99%|█████████▉| 198/200 [08:50<00:05,  2.68s/it]Running Inference: 100%|█████████▉| 199/200 [08:52<00:02,  2.69s/it]Running Inference: 100%|██████████| 200/200 [08:55<00:00,  2.69s/it]Running Inference: 100%|██████████| 200/200 [08:55<00:00,  2.68s/it]
2025-12-14 22:14:51,232 - INFO - Inference completed.
2025-12-14 22:14:51,241 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-14 22:14:51,241 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:14:51,243 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-14 22:14:51,243 - INFO - Metrics:
0.0
2025-12-14 22:14:51,244 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_zh, ratio=0.3) on GPU 1

----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:14:57,704 - INFO - Set deterministic seeds to 42
2025-12-14 22:14:57,704 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:14:57,705 - INFO - Starting evaluation run...
2025-12-14 22:14:57,705 - INFO - Output directory set to: longbenchresult
2025-12-14 22:14:57,705 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-14 22:14:57,705 - INFO - KV Press 'tova' setup.
2025-12-14 22:14:57,705 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:14:57,705 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.45it/s]
Device set to use cuda:0
2025-12-14 22:15:15,087 - INFO - Model pipeline loaded.
2025-12-14 22:15:15,087 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 22:15:22,010 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:15:22,010 - INFO - Dataset processed with 200 entries.
2025-12-14 22:15:22,023 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:25,  2.84s/it]Running Inference:   1%|          | 2/200 [00:05<09:08,  2.77s/it]Running Inference:   2%|▏         | 3/200 [00:08<09:08,  2.78s/it]Running Inference:   2%|▏         | 4/200 [00:11<09:03,  2.77s/it]Running Inference:   2%|▎         | 5/200 [00:13<08:57,  2.76s/it]Running Inference:   3%|▎         | 6/200 [00:16<08:55,  2.76s/it]Running Inference:   4%|▎         | 7/200 [00:19<08:53,  2.76s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:43,  2.73s/it]Running Inference:   4%|▍         | 9/200 [00:24<08:36,  2.70s/it]Running Inference:   5%|▌         | 10/200 [00:27<08:33,  2.70s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:30<08:35,  2.73s/it]Running Inference:   6%|▌         | 12/200 [00:33<08:39,  2.76s/it]Running Inference:   6%|▋         | 13/200 [00:33<06:40,  2.14s/it]Running Inference:   7%|▋         | 14/200 [00:36<07:09,  2.31s/it]Running Inference:   8%|▊         | 15/200 [00:39<07:26,  2.41s/it]Running Inference:   8%|▊         | 16/200 [00:41<07:40,  2.50s/it]Running Inference:   8%|▊         | 17/200 [00:44<07:47,  2.55s/it]Running Inference:   9%|▉         | 18/200 [00:47<07:52,  2.60s/it]Running Inference:  10%|▉         | 19/200 [00:49<08:00,  2.66s/it]Running Inference:  10%|█         | 20/200 [00:52<07:59,  2.66s/it]Running Inference:  10%|█         | 21/200 [00:55<07:58,  2.68s/it]Running Inference:  11%|█         | 22/200 [00:58<08:03,  2.72s/it]Running Inference:  12%|█▏        | 23/200 [01:00<08:01,  2.72s/it]Running Inference:  12%|█▏        | 24/200 [01:03<07:54,  2.70s/it]Running Inference:  12%|█▎        | 25/200 [01:06<07:56,  2.72s/it]Running Inference:  13%|█▎        | 26/200 [01:08<07:51,  2.71s/it]Running Inference:  14%|█▎        | 27/200 [01:11<07:45,  2.69s/it]Running Inference:  14%|█▍        | 28/200 [01:14<07:45,  2.71s/it]Running Inference:  14%|█▍        | 29/200 [01:17<07:42,  2.71s/it]Running Inference:  15%|█▌        | 30/200 [01:19<07:40,  2.71s/it]Running Inference:  16%|█▌        | 31/200 [01:22<07:39,  2.72s/it]Running Inference:  16%|█▌        | 32/200 [01:25<07:36,  2.72s/it]Running Inference:  16%|█▋        | 33/200 [01:27<07:32,  2.71s/it]Running Inference:  17%|█▋        | 34/200 [01:30<07:25,  2.68s/it]Running Inference:  18%|█▊        | 35/200 [01:33<07:23,  2.69s/it]Running Inference:  18%|█▊        | 36/200 [01:36<07:27,  2.73s/it]Running Inference:  18%|█▊        | 37/200 [01:38<07:28,  2.75s/it]Running Inference:  19%|█▉        | 38/200 [01:41<07:20,  2.72s/it]Running Inference:  20%|█▉        | 39/200 [01:44<07:20,  2.74s/it]Running Inference:  20%|██        | 40/200 [01:47<07:16,  2.73s/it]Running Inference:  20%|██        | 41/200 [01:49<07:17,  2.75s/it]Running Inference:  21%|██        | 42/200 [01:52<07:11,  2.73s/it]Running Inference:  22%|██▏       | 43/200 [01:55<07:08,  2.73s/it]Running Inference:  22%|██▏       | 44/200 [01:58<07:08,  2.74s/it]Running Inference:  22%|██▎       | 45/200 [02:00<07:01,  2.72s/it]Running Inference:  23%|██▎       | 46/200 [02:03<06:54,  2.69s/it]Running Inference:  24%|██▎       | 47/200 [02:06<06:53,  2.70s/it]Running Inference:  24%|██▍       | 48/200 [02:08<06:55,  2.73s/it]Running Inference:  24%|██▍       | 49/200 [02:11<06:52,  2.73s/it]Running Inference:  25%|██▌       | 50/200 [02:14<06:49,  2.73s/it]Running Inference:  26%|██▌       | 51/200 [02:16<06:26,  2.60s/it]Running Inference:  26%|██▌       | 52/200 [02:19<06:30,  2.64s/it]Running Inference:  26%|██▋       | 53/200 [02:21<05:53,  2.40s/it]Running Inference:  27%|██▋       | 54/200 [02:23<06:07,  2.52s/it]Running Inference:  28%|██▊       | 55/200 [02:26<06:15,  2.59s/it]Running Inference:  28%|██▊       | 56/200 [02:29<06:18,  2.63s/it]Running Inference:  28%|██▊       | 57/200 [02:32<06:17,  2.64s/it]Running Inference:  29%|██▉       | 58/200 [02:34<06:22,  2.69s/it]Running Inference:  30%|██▉       | 59/200 [02:37<06:26,  2.74s/it]Running Inference:  30%|███       | 60/200 [02:40<06:21,  2.73s/it]Running Inference:  30%|███       | 61/200 [02:43<06:15,  2.70s/it]Running Inference:  31%|███       | 62/200 [02:45<06:18,  2.74s/it]Running Inference:  32%|███▏      | 63/200 [02:48<06:14,  2.73s/it]Running Inference:  32%|███▏      | 64/200 [02:51<06:12,  2.74s/it]Running Inference:  32%|███▎      | 65/200 [02:54<06:09,  2.74s/it]Running Inference:  33%|███▎      | 66/200 [02:56<06:07,  2.74s/it]Running Inference:  34%|███▎      | 67/200 [02:59<06:09,  2.78s/it]Running Inference:  34%|███▍      | 68/200 [03:02<06:04,  2.76s/it]Running Inference:  34%|███▍      | 69/200 [03:05<05:57,  2.73s/it]Running Inference:  35%|███▌      | 70/200 [03:07<06:00,  2.77s/it]Running Inference:  36%|███▌      | 71/200 [03:10<05:55,  2.76s/it]Running Inference:  36%|███▌      | 72/200 [03:13<05:54,  2.77s/it]Running Inference:  36%|███▋      | 73/200 [03:16<05:45,  2.72s/it]Running Inference:  37%|███▋      | 74/200 [03:19<05:50,  2.78s/it]Running Inference:  38%|███▊      | 75/200 [03:21<05:46,  2.77s/it]Running Inference:  38%|███▊      | 76/200 [03:24<05:41,  2.75s/it]Running Inference:  38%|███▊      | 77/200 [03:27<05:38,  2.75s/it]Running Inference:  39%|███▉      | 78/200 [03:29<05:32,  2.72s/it]Running Inference:  40%|███▉      | 79/200 [03:32<05:27,  2.71s/it]Running Inference:  40%|████      | 80/200 [03:35<05:25,  2.72s/it]Running Inference:  40%|████      | 81/200 [03:38<05:25,  2.73s/it]Running Inference:  41%|████      | 82/200 [03:40<05:24,  2.75s/it]Running Inference:  42%|████▏     | 83/200 [03:42<04:41,  2.41s/it]Running Inference:  42%|████▏     | 84/200 [03:45<04:49,  2.49s/it]Running Inference:  42%|████▎     | 85/200 [03:47<04:54,  2.56s/it]Running Inference:  43%|████▎     | 86/200 [03:50<04:55,  2.59s/it]Running Inference:  44%|████▎     | 87/200 [03:53<04:54,  2.60s/it]Running Inference:  44%|████▍     | 88/200 [03:55<04:54,  2.63s/it]Running Inference:  44%|████▍     | 89/200 [03:58<04:57,  2.68s/it]Running Inference:  45%|████▌     | 90/200 [04:01<04:58,  2.72s/it]Running Inference:  46%|████▌     | 91/200 [04:04<04:56,  2.72s/it]Running Inference:  46%|████▌     | 92/200 [04:06<04:53,  2.72s/it]Running Inference:  46%|████▋     | 93/200 [04:07<03:52,  2.18s/it]Running Inference:  47%|████▋     | 94/200 [04:10<04:05,  2.31s/it]Running Inference:  48%|████▊     | 95/200 [04:13<04:15,  2.43s/it]Running Inference:  48%|████▊     | 96/200 [04:15<04:21,  2.52s/it]Running Inference:  48%|████▊     | 97/200 [04:18<04:25,  2.57s/it]Running Inference:  49%|████▉     | 98/200 [04:21<04:25,  2.61s/it]Running Inference:  50%|████▉     | 99/200 [04:24<04:27,  2.65s/it]Running Inference:  50%|█████     | 100/200 [04:26<04:29,  2.69s/it]Running Inference:  50%|█████     | 101/200 [04:29<04:25,  2.68s/it]Running Inference:  51%|█████     | 102/200 [04:32<04:27,  2.73s/it]Running Inference:  52%|█████▏    | 103/200 [04:34<04:22,  2.71s/it]Running Inference:  52%|█████▏    | 104/200 [04:37<04:23,  2.74s/it]Running Inference:  52%|█████▎    | 105/200 [04:40<04:20,  2.74s/it]Running Inference:  53%|█████▎    | 106/200 [04:42<04:04,  2.60s/it]Running Inference:  54%|█████▎    | 107/200 [04:45<04:04,  2.63s/it]Running Inference:  54%|█████▍    | 108/200 [04:48<04:05,  2.66s/it]Running Inference:  55%|█████▍    | 109/200 [04:49<03:12,  2.11s/it]Running Inference:  55%|█████▌    | 110/200 [04:51<03:28,  2.32s/it]Running Inference:  56%|█████▌    | 111/200 [04:54<03:38,  2.46s/it]Running Inference:  56%|█████▌    | 112/200 [04:57<03:47,  2.59s/it]Running Inference:  56%|█████▋    | 113/200 [05:00<03:47,  2.62s/it]Running Inference:  57%|█████▋    | 114/200 [05:03<03:49,  2.67s/it]Running Inference:  57%|█████▊    | 115/200 [05:05<03:47,  2.68s/it]Running Inference:  58%|█████▊    | 116/200 [05:06<02:52,  2.06s/it]Running Inference:  58%|█████▊    | 117/200 [05:09<03:07,  2.26s/it]Running Inference:  59%|█████▉    | 118/200 [05:11<03:18,  2.42s/it]Running Inference:  60%|█████▉    | 119/200 [05:14<03:25,  2.53s/it]Running Inference:  60%|██████    | 120/200 [05:17<03:25,  2.56s/it]Running Inference:  60%|██████    | 121/200 [05:20<03:26,  2.61s/it]Running Inference:  61%|██████    | 122/200 [05:22<03:24,  2.63s/it]Running Inference:  62%|██████▏   | 123/200 [05:25<03:22,  2.63s/it]Running Inference:  62%|██████▏   | 124/200 [05:28<03:22,  2.66s/it]Running Inference:  62%|██████▎   | 125/200 [05:30<03:20,  2.67s/it]Running Inference:  63%|██████▎   | 126/200 [05:33<03:18,  2.69s/it]Running Inference:  64%|██████▎   | 127/200 [05:36<03:17,  2.70s/it]Running Inference:  64%|██████▍   | 128/200 [05:38<03:13,  2.68s/it]Running Inference:  64%|██████▍   | 129/200 [05:41<03:12,  2.71s/it]Running Inference:  65%|██████▌   | 130/200 [05:44<03:10,  2.71s/it]Running Inference:  66%|██████▌   | 131/200 [05:47<03:09,  2.75s/it]Running Inference:  66%|██████▌   | 132/200 [05:49<03:07,  2.76s/it]Running Inference:  66%|██████▋   | 133/200 [05:52<03:04,  2.75s/it]Running Inference:  67%|██████▋   | 134/200 [05:55<03:00,  2.74s/it]Running Inference:  68%|██████▊   | 135/200 [05:58<02:58,  2.75s/it]Running Inference:  68%|██████▊   | 136/200 [06:00<02:54,  2.73s/it]Running Inference:  68%|██████▊   | 137/200 [06:03<02:51,  2.73s/it]Running Inference:  69%|██████▉   | 138/200 [06:06<02:48,  2.71s/it]Running Inference:  70%|██████▉   | 139/200 [06:08<02:45,  2.71s/it]Running Inference:  70%|███████   | 140/200 [06:11<02:44,  2.74s/it]Running Inference:  70%|███████   | 141/200 [06:14<02:41,  2.74s/it]Running Inference:  71%|███████   | 142/200 [06:17<02:38,  2.73s/it]Running Inference:  72%|███████▏  | 143/200 [06:19<02:34,  2.71s/it]Running Inference:  72%|███████▏  | 144/200 [06:22<02:30,  2.69s/it]Running Inference:  72%|███████▎  | 145/200 [06:25<02:28,  2.69s/it]Running Inference:  73%|███████▎  | 146/200 [06:27<02:23,  2.66s/it]Running Inference:  74%|███████▎  | 147/200 [06:30<02:21,  2.68s/it]Running Inference:  74%|███████▍  | 148/200 [06:33<02:19,  2.68s/it]Running Inference:  74%|███████▍  | 149/200 [06:35<02:16,  2.68s/it]Running Inference:  75%|███████▌  | 150/200 [06:38<02:14,  2.69s/it]Running Inference:  76%|███████▌  | 151/200 [06:41<02:10,  2.67s/it]Running Inference:  76%|███████▌  | 152/200 [06:43<02:09,  2.69s/it]Running Inference:  76%|███████▋  | 153/200 [06:46<02:06,  2.69s/it]Running Inference:  77%|███████▋  | 154/200 [06:49<01:59,  2.60s/it]Running Inference:  78%|███████▊  | 155/200 [06:51<02:00,  2.68s/it]Running Inference:  78%|███████▊  | 156/200 [06:54<01:58,  2.69s/it]Running Inference:  78%|███████▊  | 157/200 [06:56<01:51,  2.59s/it]Running Inference:  79%|███████▉  | 158/200 [06:59<01:50,  2.62s/it]Running Inference:  80%|███████▉  | 159/200 [07:02<01:48,  2.66s/it]Running Inference:  80%|████████  | 160/200 [07:05<01:47,  2.68s/it]Running Inference:  80%|████████  | 161/200 [07:07<01:44,  2.69s/it]Running Inference:  81%|████████  | 162/200 [07:10<01:41,  2.67s/it]Running Inference:  82%|████████▏ | 163/200 [07:13<01:39,  2.70s/it]Running Inference:  82%|████████▏ | 164/200 [07:15<01:37,  2.70s/it]Running Inference:  82%|████████▎ | 165/200 [07:18<01:35,  2.71s/it]Running Inference:  83%|████████▎ | 166/200 [07:21<01:32,  2.73s/it]Running Inference:  84%|████████▎ | 167/200 [07:24<01:29,  2.73s/it]Running Inference:  84%|████████▍ | 168/200 [07:26<01:27,  2.75s/it]Running Inference:  84%|████████▍ | 169/200 [07:29<01:24,  2.72s/it]Running Inference:  85%|████████▌ | 170/200 [07:32<01:21,  2.73s/it]Running Inference:  86%|████████▌ | 171/200 [07:35<01:18,  2.70s/it]Running Inference:  86%|████████▌ | 172/200 [07:37<01:16,  2.72s/it]Running Inference:  86%|████████▋ | 173/200 [07:40<01:12,  2.68s/it]Running Inference:  87%|████████▋ | 174/200 [07:43<01:09,  2.67s/it]Running Inference:  88%|████████▊ | 175/200 [07:45<01:06,  2.68s/it]Running Inference:  88%|████████▊ | 176/200 [07:48<01:04,  2.70s/it]Running Inference:  88%|████████▊ | 177/200 [07:51<01:02,  2.73s/it]Running Inference:  89%|████████▉ | 178/200 [07:54<01:00,  2.76s/it]Running Inference:  90%|████████▉ | 179/200 [07:56<00:57,  2.76s/it]Running Inference:  90%|█████████ | 180/200 [07:59<00:55,  2.79s/it]Running Inference:  90%|█████████ | 181/200 [08:02<00:52,  2.78s/it]Running Inference:  91%|█████████ | 182/200 [08:05<00:50,  2.79s/it]Running Inference:  92%|█████████▏| 183/200 [08:08<00:47,  2.78s/it]Running Inference:  92%|█████████▏| 184/200 [08:10<00:44,  2.77s/it]Running Inference:  92%|█████████▎| 185/200 [08:13<00:41,  2.74s/it]Running Inference:  93%|█████████▎| 186/200 [08:16<00:38,  2.72s/it]Running Inference:  94%|█████████▎| 187/200 [08:18<00:35,  2.75s/it]Running Inference:  94%|█████████▍| 188/200 [08:21<00:32,  2.73s/it]Running Inference:  94%|█████████▍| 189/200 [08:24<00:30,  2.73s/it]Running Inference:  95%|█████████▌| 190/200 [08:27<00:27,  2.74s/it]Running Inference:  96%|█████████▌| 191/200 [08:29<00:24,  2.75s/it]Running Inference:  96%|█████████▌| 192/200 [08:32<00:21,  2.73s/it]Running Inference:  96%|█████████▋| 193/200 [08:35<00:18,  2.71s/it]Running Inference:  97%|█████████▋| 194/200 [08:38<00:16,  2.75s/it]Running Inference:  98%|█████████▊| 195/200 [08:40<00:13,  2.74s/it]Running Inference:  98%|█████████▊| 196/200 [08:43<00:10,  2.71s/it]Running Inference:  98%|█████████▊| 197/200 [08:46<00:08,  2.74s/it]Running Inference:  99%|█████████▉| 198/200 [08:48<00:05,  2.71s/it]Running Inference: 100%|█████████▉| 199/200 [08:51<00:02,  2.72s/it]Running Inference: 100%|██████████| 200/200 [08:54<00:00,  2.71s/it]Running Inference: 100%|██████████| 200/200 [08:54<00:00,  2.67s/it]
2025-12-14 22:24:16,356 - INFO - Inference completed.
2025-12-14 22:24:16,365 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-14 22:24:16,365 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:24:16,367 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-14 22:24:16,367 - INFO - Metrics:
0.0
2025-12-14 22:24:16,368 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=passage_retrieval_zh, ratio=0.5) on GPU 1


========================================
LongBench Task: repobench-p
========================================
----------------------------------------
Task: repobench-p | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:24:22,831 - INFO - Set deterministic seeds to 42
2025-12-14 22:24:22,831 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:24:22,831 - INFO - Starting evaluation run...
2025-12-14 22:24:22,831 - INFO - Output directory set to: longbenchresult
2025-12-14 22:24:22,831 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-14 22:24:22,831 - INFO - KV Press 'tova' setup.
2025-12-14 22:24:22,832 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:24:22,832 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.90it/s]
Device set to use cuda:0
2025-12-14 22:24:36,771 - INFO - Model pipeline loaded.
2025-12-14 22:24:36,771 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 500 examples [00:00, 1235.39 examples/s]Generating test split: 500 examples [00:00, 1230.74 examples/s]
2025-12-14 22:24:44,774 - INFO - Dataset loaded with 500 entries.
2025-12-14 22:24:44,774 - INFO - Dataset processed with 500 entries.
2025-12-14 22:24:44,818 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<43:05,  5.18s/it]Running Inference:   0%|          | 2/500 [00:11<48:15,  5.81s/it]Running Inference:   1%|          | 3/500 [00:16<43:25,  5.24s/it]Running Inference:   1%|          | 4/500 [00:20<39:33,  4.79s/it]Running Inference:   1%|          | 5/500 [00:24<38:46,  4.70s/it]Running Inference:   1%|          | 6/500 [00:28<37:19,  4.53s/it]Running Inference:   1%|▏         | 7/500 [00:29<28:06,  3.42s/it]Running Inference:   2%|▏         | 8/500 [00:34<29:39,  3.62s/it]Running Inference:   2%|▏         | 9/500 [00:38<30:42,  3.75s/it]Running Inference:   2%|▏         | 10/500 [00:42<31:42,  3.88s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:45<29:44,  3.65s/it]Running Inference:   2%|▏         | 12/500 [00:49<31:31,  3.88s/it]Running Inference:   3%|▎         | 13/500 [00:55<35:03,  4.32s/it]Running Inference:   3%|▎         | 14/500 [01:05<49:15,  6.08s/it]Running Inference:   3%|▎         | 15/500 [01:09<45:54,  5.68s/it]Running Inference:   3%|▎         | 16/500 [01:14<42:29,  5.27s/it]Running Inference:   3%|▎         | 17/500 [01:19<42:59,  5.34s/it]Running Inference:   4%|▎         | 18/500 [01:23<37:57,  4.72s/it]Running Inference:   4%|▍         | 19/500 [01:27<36:37,  4.57s/it]Running Inference:   4%|▍         | 20/500 [01:31<34:57,  4.37s/it]Running Inference:   4%|▍         | 21/500 [01:35<34:06,  4.27s/it]Running Inference:   4%|▍         | 22/500 [01:40<37:26,  4.70s/it]Running Inference:   5%|▍         | 23/500 [01:43<32:50,  4.13s/it]Running Inference:   5%|▍         | 24/500 [01:49<37:23,  4.71s/it]Running Inference:   5%|▌         | 25/500 [01:55<40:17,  5.09s/it]Running Inference:   5%|▌         | 26/500 [02:00<39:21,  4.98s/it]Running Inference:   5%|▌         | 27/500 [02:04<37:08,  4.71s/it]Running Inference:   6%|▌         | 28/500 [02:08<35:07,  4.46s/it]Running Inference:   6%|▌         | 29/500 [02:12<34:00,  4.33s/it]Running Inference:   6%|▌         | 30/500 [02:17<36:08,  4.61s/it]Running Inference:   6%|▌         | 31/500 [02:23<37:32,  4.80s/it]Running Inference:   6%|▋         | 32/500 [02:27<36:15,  4.65s/it]Running Inference:   7%|▋         | 33/500 [02:31<34:49,  4.47s/it]Running Inference:   7%|▋         | 34/500 [02:35<34:25,  4.43s/it]Running Inference:   7%|▋         | 35/500 [02:40<34:22,  4.44s/it]Running Inference:   7%|▋         | 36/500 [02:42<28:22,  3.67s/it]Running Inference:   7%|▋         | 37/500 [02:46<30:42,  3.98s/it]Running Inference:   8%|▊         | 38/500 [02:51<31:49,  4.13s/it]Running Inference:   8%|▊         | 39/500 [02:55<31:43,  4.13s/it]Running Inference:   8%|▊         | 40/500 [02:59<31:39,  4.13s/it]Running Inference:   8%|▊         | 41/500 [03:00<25:10,  3.29s/it]Running Inference:   8%|▊         | 42/500 [03:04<26:22,  3.45s/it]Running Inference:   9%|▊         | 43/500 [03:07<25:15,  3.32s/it]Running Inference:   9%|▉         | 44/500 [03:13<31:32,  4.15s/it]Running Inference:   9%|▉         | 45/500 [03:19<34:01,  4.49s/it]Running Inference:   9%|▉         | 46/500 [03:22<31:24,  4.15s/it]Running Inference:   9%|▉         | 47/500 [03:27<34:00,  4.50s/it]Running Inference:  10%|▉         | 48/500 [03:32<33:33,  4.46s/it]Running Inference:  10%|▉         | 49/500 [03:36<33:43,  4.49s/it]Running Inference:  10%|█         | 50/500 [03:40<31:27,  4.19s/it]Running Inference:  10%|█         | 51/500 [03:45<33:14,  4.44s/it]Running Inference:  10%|█         | 52/500 [03:47<29:02,  3.89s/it]Running Inference:  11%|█         | 53/500 [03:51<29:25,  3.95s/it]Running Inference:  11%|█         | 54/500 [03:56<29:52,  4.02s/it]Running Inference:  11%|█         | 55/500 [04:02<35:04,  4.73s/it]Running Inference:  11%|█         | 56/500 [04:06<34:40,  4.69s/it]Running Inference:  11%|█▏        | 57/500 [04:10<32:45,  4.44s/it]Running Inference:  12%|█▏        | 58/500 [04:14<31:49,  4.32s/it]Running Inference:  12%|█▏        | 59/500 [04:19<32:03,  4.36s/it]Running Inference:  12%|█▏        | 60/500 [04:23<31:26,  4.29s/it]Running Inference:  12%|█▏        | 61/500 [04:27<30:54,  4.22s/it]Running Inference:  12%|█▏        | 62/500 [04:33<34:32,  4.73s/it]Running Inference:  13%|█▎        | 63/500 [04:39<37:02,  5.08s/it]Running Inference:  13%|█▎        | 64/500 [04:43<35:39,  4.91s/it]Running Inference:  13%|█▎        | 65/500 [04:50<38:17,  5.28s/it]Running Inference:  13%|█▎        | 66/500 [04:53<35:04,  4.85s/it]Running Inference:  13%|█▎        | 67/500 [04:57<32:49,  4.55s/it]Running Inference:  14%|█▎        | 68/500 [05:03<34:47,  4.83s/it]Running Inference:  14%|█▍        | 69/500 [05:08<35:53,  5.00s/it]Running Inference:  14%|█▍        | 70/500 [05:17<43:25,  6.06s/it]Running Inference:  14%|█▍        | 71/500 [05:22<41:04,  5.74s/it]Running Inference:  14%|█▍        | 72/500 [05:27<39:08,  5.49s/it]Running Inference:  15%|█▍        | 73/500 [05:32<39:05,  5.49s/it]Running Inference:  15%|█▍        | 74/500 [05:37<38:14,  5.39s/it]Running Inference:  15%|█▌        | 75/500 [05:42<37:11,  5.25s/it]Running Inference:  15%|█▌        | 76/500 [05:47<36:52,  5.22s/it]Running Inference:  15%|█▌        | 77/500 [05:52<35:18,  5.01s/it]Running Inference:  16%|█▌        | 78/500 [05:58<38:40,  5.50s/it]Running Inference:  16%|█▌        | 79/500 [06:00<30:32,  4.35s/it]Running Inference:  16%|█▌        | 80/500 [06:03<26:39,  3.81s/it]Running Inference:  16%|█▌        | 81/500 [06:08<30:09,  4.32s/it]Running Inference:  16%|█▋        | 82/500 [06:11<26:13,  3.76s/it]Running Inference:  17%|█▋        | 83/500 [06:19<35:10,  5.06s/it]Running Inference:  17%|█▋        | 84/500 [06:24<34:42,  5.01s/it]Running Inference:  17%|█▋        | 85/500 [06:25<27:32,  3.98s/it]Running Inference:  17%|█▋        | 86/500 [06:30<29:17,  4.25s/it]Running Inference:  17%|█▋        | 87/500 [06:34<29:18,  4.26s/it]Running Inference:  18%|█▊        | 88/500 [06:39<29:06,  4.24s/it]Running Inference:  18%|█▊        | 89/500 [06:43<28:58,  4.23s/it]Running Inference:  18%|█▊        | 90/500 [06:47<28:54,  4.23s/it]Running Inference:  18%|█▊        | 91/500 [06:51<28:56,  4.25s/it]Running Inference:  18%|█▊        | 92/500 [06:56<29:49,  4.39s/it]Running Inference:  19%|█▊        | 93/500 [07:01<31:11,  4.60s/it]Running Inference:  19%|█▉        | 94/500 [07:04<28:00,  4.14s/it]Running Inference:  19%|█▉        | 95/500 [07:12<34:57,  5.18s/it]Running Inference:  19%|█▉        | 96/500 [07:17<34:11,  5.08s/it]Running Inference:  19%|█▉        | 97/500 [07:21<33:31,  4.99s/it]Running Inference:  20%|█▉        | 98/500 [07:26<32:02,  4.78s/it]Running Inference:  20%|█▉        | 99/500 [07:28<26:28,  3.96s/it]Running Inference:  20%|██        | 100/500 [07:29<21:49,  3.27s/it]Running Inference:  20%|██        | 101/500 [07:33<21:56,  3.30s/it]Running Inference:  20%|██        | 102/500 [07:35<19:32,  2.95s/it]Running Inference:  21%|██        | 103/500 [07:39<21:54,  3.31s/it]Running Inference:  21%|██        | 104/500 [07:44<25:37,  3.88s/it]Running Inference:  21%|██        | 105/500 [07:46<20:29,  3.11s/it]Running Inference:  21%|██        | 106/500 [07:50<22:34,  3.44s/it]Running Inference:  21%|██▏       | 107/500 [07:54<23:36,  3.60s/it]Running Inference:  22%|██▏       | 108/500 [07:58<25:30,  3.90s/it]Running Inference:  22%|██▏       | 109/500 [08:03<26:39,  4.09s/it]Running Inference:  22%|██▏       | 110/500 [08:09<30:38,  4.71s/it]Running Inference:  22%|██▏       | 111/500 [08:11<25:08,  3.88s/it]Running Inference:  22%|██▏       | 112/500 [08:15<26:09,  4.04s/it]Running Inference:  23%|██▎       | 113/500 [08:20<26:18,  4.08s/it]Running Inference:  23%|██▎       | 114/500 [08:24<26:49,  4.17s/it]Running Inference:  23%|██▎       | 115/500 [08:29<27:41,  4.32s/it]Running Inference:  23%|██▎       | 116/500 [08:33<27:31,  4.30s/it]Running Inference:  23%|██▎       | 117/500 [08:38<28:18,  4.43s/it]Running Inference:  24%|██▎       | 118/500 [08:43<29:22,  4.61s/it]Running Inference:  24%|██▍       | 119/500 [08:47<29:06,  4.58s/it]Running Inference:  24%|██▍       | 120/500 [08:51<28:11,  4.45s/it]Running Inference:  24%|██▍       | 121/500 [08:54<24:49,  3.93s/it]Running Inference:  24%|██▍       | 122/500 [08:58<24:38,  3.91s/it]Running Inference:  25%|██▍       | 123/500 [09:02<24:51,  3.96s/it]Running Inference:  25%|██▍       | 124/500 [09:06<25:00,  3.99s/it]Running Inference:  25%|██▌       | 125/500 [09:11<26:00,  4.16s/it]Running Inference:  25%|██▌       | 126/500 [09:13<21:51,  3.51s/it]Running Inference:  25%|██▌       | 127/500 [09:14<18:50,  3.03s/it]Running Inference:  26%|██▌       | 128/500 [09:18<20:30,  3.31s/it]Running Inference:  26%|██▌       | 129/500 [09:26<28:09,  4.55s/it]Running Inference:  26%|██▌       | 130/500 [09:33<32:59,  5.35s/it]Running Inference:  26%|██▌       | 131/500 [09:35<26:28,  4.31s/it]Running Inference:  26%|██▋       | 132/500 [09:39<25:52,  4.22s/it]Running Inference:  27%|██▋       | 133/500 [09:43<25:46,  4.21s/it]Running Inference:  27%|██▋       | 134/500 [09:48<26:57,  4.42s/it]Running Inference:  27%|██▋       | 135/500 [09:57<34:27,  5.67s/it]Running Inference:  27%|██▋       | 136/500 [10:03<35:52,  5.91s/it]Running Inference:  27%|██▋       | 137/500 [10:09<34:50,  5.76s/it]Running Inference:  28%|██▊       | 138/500 [10:15<36:05,  5.98s/it]Running Inference:  28%|██▊       | 139/500 [10:21<36:35,  6.08s/it]Running Inference:  28%|██▊       | 140/500 [10:27<34:57,  5.83s/it]Running Inference:  28%|██▊       | 141/500 [10:31<31:50,  5.32s/it]Running Inference:  28%|██▊       | 142/500 [10:31<23:22,  3.92s/it]Running Inference:  29%|██▊       | 143/500 [10:40<32:32,  5.47s/it]Running Inference:  29%|██▉       | 144/500 [10:44<29:51,  5.03s/it]Running Inference:  29%|██▉       | 145/500 [10:46<23:24,  3.96s/it]Running Inference:  29%|██▉       | 146/500 [10:49<21:14,  3.60s/it]Running Inference:  29%|██▉       | 147/500 [10:49<16:16,  2.77s/it]Running Inference:  30%|██▉       | 148/500 [10:55<20:56,  3.57s/it]Running Inference:  30%|██▉       | 149/500 [11:01<24:28,  4.18s/it]Running Inference:  30%|███       | 150/500 [11:06<26:46,  4.59s/it]Running Inference:  30%|███       | 151/500 [11:10<25:52,  4.45s/it]Running Inference:  30%|███       | 152/500 [11:14<25:01,  4.31s/it]Running Inference:  31%|███       | 153/500 [11:19<25:07,  4.34s/it]Running Inference:  31%|███       | 154/500 [11:24<27:06,  4.70s/it]Running Inference:  31%|███       | 155/500 [11:29<26:51,  4.67s/it]Running Inference:  31%|███       | 156/500 [11:34<28:29,  4.97s/it]Running Inference:  31%|███▏      | 157/500 [11:38<26:53,  4.70s/it]Running Inference:  32%|███▏      | 158/500 [11:43<26:26,  4.64s/it]Running Inference:  32%|███▏      | 159/500 [11:48<27:24,  4.82s/it]Running Inference:  32%|███▏      | 160/500 [11:53<27:05,  4.78s/it]Running Inference:  32%|███▏      | 161/500 [11:58<27:37,  4.89s/it]Running Inference:  32%|███▏      | 162/500 [12:03<27:15,  4.84s/it]Running Inference:  33%|███▎      | 163/500 [12:07<26:35,  4.74s/it]Running Inference:  33%|███▎      | 164/500 [12:11<24:16,  4.33s/it]Running Inference:  33%|███▎      | 165/500 [12:16<26:01,  4.66s/it]Running Inference:  33%|███▎      | 166/500 [12:21<26:45,  4.81s/it]Running Inference:  33%|███▎      | 167/500 [12:25<25:35,  4.61s/it]Running Inference:  34%|███▎      | 168/500 [12:28<21:57,  3.97s/it]Running Inference:  34%|███▍      | 169/500 [12:34<25:08,  4.56s/it]Running Inference:  34%|███▍      | 170/500 [12:38<24:23,  4.44s/it]Running Inference:  34%|███▍      | 171/500 [12:42<23:53,  4.36s/it]Running Inference:  34%|███▍      | 172/500 [12:46<23:25,  4.29s/it]Running Inference:  35%|███▍      | 173/500 [12:50<23:04,  4.23s/it]Running Inference:  35%|███▍      | 174/500 [12:54<22:44,  4.19s/it]Running Inference:  35%|███▌      | 175/500 [13:00<24:25,  4.51s/it]Running Inference:  35%|███▌      | 176/500 [13:05<25:31,  4.73s/it]Running Inference:  35%|███▌      | 177/500 [13:10<26:15,  4.88s/it]Running Inference:  36%|███▌      | 178/500 [13:16<27:40,  5.16s/it]Running Inference:  36%|███▌      | 179/500 [13:20<25:58,  4.86s/it]Running Inference:  36%|███▌      | 180/500 [13:24<25:04,  4.70s/it]Running Inference:  36%|███▌      | 181/500 [13:29<24:22,  4.58s/it]Running Inference:  36%|███▋      | 182/500 [13:33<23:58,  4.52s/it]Running Inference:  37%|███▋      | 183/500 [13:38<23:38,  4.47s/it]Running Inference:  37%|███▋      | 184/500 [13:42<22:50,  4.34s/it]Running Inference:  37%|███▋      | 185/500 [13:46<22:31,  4.29s/it]Running Inference:  37%|███▋      | 186/500 [13:51<23:53,  4.56s/it]Running Inference:  37%|███▋      | 187/500 [13:55<23:42,  4.55s/it]Running Inference:  38%|███▊      | 188/500 [14:00<23:07,  4.45s/it]Running Inference:  38%|███▊      | 189/500 [14:04<23:30,  4.53s/it]Running Inference:  38%|███▊      | 190/500 [14:13<29:31,  5.71s/it]Running Inference:  38%|███▊      | 191/500 [14:15<24:36,  4.78s/it]Running Inference:  38%|███▊      | 192/500 [14:20<24:24,  4.76s/it]Running Inference:  39%|███▊      | 193/500 [14:25<23:55,  4.68s/it]Running Inference:  39%|███▉      | 194/500 [14:29<23:40,  4.64s/it]Running Inference:  39%|███▉      | 195/500 [14:34<24:02,  4.73s/it]Running Inference:  39%|███▉      | 196/500 [14:38<23:02,  4.55s/it]Running Inference:  39%|███▉      | 197/500 [14:42<22:17,  4.41s/it]Running Inference:  40%|███▉      | 198/500 [14:47<21:50,  4.34s/it]Running Inference:  40%|███▉      | 199/500 [14:51<21:24,  4.27s/it]Running Inference:  40%|████      | 200/500 [14:55<21:11,  4.24s/it]Running Inference:  40%|████      | 201/500 [14:59<20:48,  4.18s/it]Running Inference:  40%|████      | 202/500 [15:05<23:38,  4.76s/it]Running Inference:  41%|████      | 203/500 [15:09<23:14,  4.69s/it]Running Inference:  41%|████      | 204/500 [15:14<22:47,  4.62s/it]Running Inference:  41%|████      | 205/500 [15:18<22:28,  4.57s/it]Running Inference:  41%|████      | 206/500 [15:23<23:04,  4.71s/it]Running Inference:  41%|████▏     | 207/500 [15:28<22:37,  4.63s/it]Running Inference:  42%|████▏     | 208/500 [15:30<18:29,  3.80s/it]Running Inference:  42%|████▏     | 209/500 [15:31<15:04,  3.11s/it]Running Inference:  42%|████▏     | 210/500 [15:35<16:22,  3.39s/it]Running Inference:  42%|████▏     | 211/500 [15:41<19:13,  3.99s/it]Running Inference:  42%|████▏     | 212/500 [15:45<19:39,  4.10s/it]Running Inference:  43%|████▎     | 213/500 [15:50<20:39,  4.32s/it]Running Inference:  43%|████▎     | 214/500 [15:54<19:59,  4.19s/it]Running Inference:  43%|████▎     | 215/500 [15:58<19:44,  4.16s/it]Running Inference:  43%|████▎     | 216/500 [16:02<19:31,  4.13s/it]Running Inference:  43%|████▎     | 217/500 [16:06<19:30,  4.13s/it]Running Inference:  44%|████▎     | 218/500 [16:10<19:26,  4.14s/it]Running Inference:  44%|████▍     | 219/500 [16:14<19:21,  4.13s/it]Running Inference:  44%|████▍     | 220/500 [16:18<19:16,  4.13s/it]Running Inference:  44%|████▍     | 221/500 [16:23<19:11,  4.13s/it]Running Inference:  44%|████▍     | 222/500 [16:27<19:10,  4.14s/it]Running Inference:  45%|████▍     | 223/500 [16:31<19:44,  4.27s/it]Running Inference:  45%|████▍     | 224/500 [16:36<20:19,  4.42s/it]Running Inference:  45%|████▌     | 225/500 [16:40<20:17,  4.43s/it]Running Inference:  45%|████▌     | 226/500 [16:47<22:25,  4.91s/it]Running Inference:  45%|████▌     | 227/500 [16:53<24:02,  5.29s/it]Running Inference:  46%|████▌     | 228/500 [16:55<20:35,  4.54s/it]Running Inference:  46%|████▌     | 229/500 [17:02<22:59,  5.09s/it]Running Inference:  46%|████▌     | 230/500 [17:04<19:18,  4.29s/it]Running Inference:  46%|████▌     | 231/500 [17:07<16:53,  3.77s/it]Running Inference:  46%|████▋     | 232/500 [17:10<16:09,  3.62s/it]Running Inference:  47%|████▋     | 233/500 [17:14<16:28,  3.70s/it]Running Inference:  47%|████▋     | 234/500 [17:18<16:41,  3.77s/it]Running Inference:  47%|████▋     | 235/500 [17:23<18:26,  4.18s/it]Running Inference:  47%|████▋     | 236/500 [17:25<15:29,  3.52s/it]Running Inference:  47%|████▋     | 237/500 [17:29<16:18,  3.72s/it]Running Inference:  48%|████▊     | 238/500 [17:35<18:48,  4.31s/it]Running Inference:  48%|████▊     | 239/500 [17:39<18:32,  4.26s/it]Running Inference:  48%|████▊     | 240/500 [17:44<19:30,  4.50s/it]Running Inference:  48%|████▊     | 241/500 [17:49<19:57,  4.62s/it]Running Inference:  48%|████▊     | 242/500 [17:55<21:08,  4.92s/it]Running Inference:  49%|████▊     | 243/500 [18:00<21:57,  5.13s/it]Running Inference:  49%|████▉     | 244/500 [18:03<18:50,  4.42s/it]Running Inference:  49%|████▉     | 245/500 [18:09<20:18,  4.78s/it]Running Inference:  49%|████▉     | 246/500 [18:10<16:19,  3.85s/it]Running Inference:  49%|████▉     | 247/500 [18:15<16:46,  3.98s/it]Running Inference:  50%|████▉     | 248/500 [18:19<17:51,  4.25s/it]Running Inference:  50%|████▉     | 249/500 [18:24<18:04,  4.32s/it]Running Inference:  50%|█████     | 250/500 [18:28<17:33,  4.22s/it]Running Inference:  50%|█████     | 251/500 [18:34<19:41,  4.74s/it]Running Inference:  50%|█████     | 252/500 [18:39<20:28,  4.95s/it]Running Inference:  51%|█████     | 253/500 [18:43<18:14,  4.43s/it]Running Inference:  51%|█████     | 254/500 [18:47<18:24,  4.49s/it]Running Inference:  51%|█████     | 255/500 [18:52<18:30,  4.53s/it]Running Inference:  51%|█████     | 256/500 [18:56<18:10,  4.47s/it]Running Inference:  51%|█████▏    | 257/500 [19:01<18:06,  4.47s/it]Running Inference:  52%|█████▏    | 258/500 [19:03<15:57,  3.96s/it]Running Inference:  52%|█████▏    | 259/500 [19:07<14:54,  3.71s/it]Running Inference:  52%|█████▏    | 260/500 [19:11<15:11,  3.80s/it]Running Inference:  52%|█████▏    | 261/500 [19:14<15:05,  3.79s/it]Running Inference:  52%|█████▏    | 262/500 [19:17<14:00,  3.53s/it]Running Inference:  53%|█████▎    | 263/500 [19:21<14:33,  3.69s/it]Running Inference:  53%|█████▎    | 264/500 [19:26<15:47,  4.01s/it]Running Inference:  53%|█████▎    | 265/500 [19:31<16:19,  4.17s/it]Running Inference:  53%|█████▎    | 266/500 [19:35<16:43,  4.29s/it]Running Inference:  53%|█████▎    | 267/500 [19:43<20:26,  5.27s/it]Running Inference:  54%|█████▎    | 268/500 [19:47<19:08,  4.95s/it]Running Inference:  54%|█████▍    | 269/500 [19:55<22:38,  5.88s/it]Running Inference:  54%|█████▍    | 270/500 [20:03<25:31,  6.66s/it]Running Inference:  54%|█████▍    | 271/500 [20:10<25:14,  6.62s/it]Running Inference:  54%|█████▍    | 272/500 [20:14<22:33,  5.94s/it]Running Inference:  55%|█████▍    | 273/500 [20:17<18:41,  4.94s/it]Running Inference:  55%|█████▍    | 274/500 [20:23<19:38,  5.22s/it]Running Inference:  55%|█████▌    | 275/500 [20:27<18:26,  4.92s/it]Running Inference:  55%|█████▌    | 276/500 [20:33<19:23,  5.19s/it]Running Inference:  55%|█████▌    | 277/500 [20:38<18:54,  5.09s/it]Running Inference:  56%|█████▌    | 278/500 [20:43<19:34,  5.29s/it]Running Inference:  56%|█████▌    | 279/500 [20:49<20:09,  5.47s/it]Running Inference:  56%|█████▌    | 280/500 [20:52<16:39,  4.54s/it]Running Inference:  56%|█████▌    | 281/500 [20:56<16:17,  4.47s/it]Running Inference:  56%|█████▋    | 282/500 [20:58<13:46,  3.79s/it]Running Inference:  57%|█████▋    | 283/500 [21:03<15:08,  4.19s/it]Running Inference:  57%|█████▋    | 284/500 [21:08<16:05,  4.47s/it]Running Inference:  57%|█████▋    | 285/500 [21:13<15:46,  4.40s/it]Running Inference:  57%|█████▋    | 286/500 [21:17<15:34,  4.37s/it]Running Inference:  57%|█████▋    | 287/500 [21:24<18:05,  5.10s/it]Running Inference:  58%|█████▊    | 288/500 [21:28<16:48,  4.76s/it]Running Inference:  58%|█████▊    | 289/500 [21:32<15:53,  4.52s/it]Running Inference:  58%|█████▊    | 290/500 [21:37<16:10,  4.62s/it]Running Inference:  58%|█████▊    | 291/500 [21:42<17:22,  4.99s/it]Running Inference:  58%|█████▊    | 292/500 [21:47<16:37,  4.79s/it]Running Inference:  59%|█████▊    | 293/500 [21:48<12:29,  3.62s/it]Running Inference:  59%|█████▉    | 294/500 [21:52<13:14,  3.86s/it]Running Inference:  59%|█████▉    | 295/500 [21:57<14:17,  4.18s/it]Running Inference:  59%|█████▉    | 296/500 [21:59<12:30,  3.68s/it]Running Inference:  59%|█████▉    | 297/500 [22:04<13:07,  3.88s/it]Running Inference:  60%|█████▉    | 298/500 [22:08<13:28,  4.00s/it]Running Inference:  60%|█████▉    | 299/500 [22:12<13:45,  4.11s/it]Running Inference:  60%|██████    | 300/500 [22:18<15:04,  4.52s/it]Running Inference:  60%|██████    | 301/500 [22:23<15:15,  4.60s/it]Running Inference:  60%|██████    | 302/500 [22:27<14:40,  4.45s/it]Running Inference:  61%|██████    | 303/500 [22:31<14:29,  4.41s/it]Running Inference:  61%|██████    | 304/500 [22:36<14:20,  4.39s/it]Running Inference:  61%|██████    | 305/500 [22:40<13:52,  4.27s/it]Running Inference:  61%|██████    | 306/500 [22:44<14:13,  4.40s/it]Running Inference:  61%|██████▏   | 307/500 [22:47<12:45,  3.96s/it]Running Inference:  62%|██████▏   | 308/500 [22:50<12:02,  3.76s/it]Running Inference:  62%|██████▏   | 309/500 [22:55<12:20,  3.88s/it]Running Inference:  62%|██████▏   | 310/500 [22:59<12:59,  4.10s/it]Running Inference:  62%|██████▏   | 311/500 [23:03<12:50,  4.08s/it]Running Inference:  62%|██████▏   | 312/500 [23:07<12:52,  4.11s/it]Running Inference:  63%|██████▎   | 313/500 [23:12<13:30,  4.33s/it]Running Inference:  63%|██████▎   | 314/500 [23:16<13:13,  4.27s/it]Running Inference:  63%|██████▎   | 315/500 [23:20<12:44,  4.13s/it]Running Inference:  63%|██████▎   | 316/500 [23:25<13:36,  4.44s/it]Running Inference:  63%|██████▎   | 317/500 [23:31<14:16,  4.68s/it]Running Inference:  64%|██████▎   | 318/500 [23:36<14:45,  4.86s/it]Running Inference:  64%|██████▍   | 319/500 [23:38<12:28,  4.14s/it]Running Inference:  64%|██████▍   | 320/500 [23:42<12:21,  4.12s/it]Running Inference:  64%|██████▍   | 321/500 [23:47<12:30,  4.19s/it]Running Inference:  64%|██████▍   | 322/500 [23:51<12:15,  4.13s/it]Running Inference:  65%|██████▍   | 323/500 [23:55<12:20,  4.18s/it]Running Inference:  65%|██████▍   | 324/500 [24:01<13:25,  4.58s/it]Running Inference:  65%|██████▌   | 325/500 [24:07<14:43,  5.05s/it]Running Inference:  65%|██████▌   | 326/500 [24:08<11:15,  3.88s/it]Running Inference:  65%|██████▌   | 327/500 [24:13<12:15,  4.25s/it]Running Inference:  66%|██████▌   | 328/500 [24:17<12:03,  4.21s/it]Running Inference:  66%|██████▌   | 329/500 [24:22<12:50,  4.50s/it]Running Inference:  66%|██████▌   | 330/500 [24:25<10:57,  3.87s/it]Running Inference:  66%|██████▌   | 331/500 [24:29<11:02,  3.92s/it]Running Inference:  66%|██████▋   | 332/500 [24:31<09:34,  3.42s/it]Running Inference:  67%|██████▋   | 333/500 [24:35<10:09,  3.65s/it]Running Inference:  67%|██████▋   | 334/500 [24:39<10:23,  3.75s/it]Running Inference:  67%|██████▋   | 335/500 [24:48<14:47,  5.38s/it]Running Inference:  67%|██████▋   | 336/500 [24:55<16:02,  5.87s/it]Running Inference:  67%|██████▋   | 337/500 [25:00<15:06,  5.56s/it]Running Inference:  68%|██████▊   | 338/500 [25:04<13:46,  5.10s/it]Running Inference:  68%|██████▊   | 339/500 [25:06<10:40,  3.98s/it]Running Inference:  68%|██████▊   | 340/500 [25:11<11:53,  4.46s/it]Running Inference:  68%|██████▊   | 341/500 [25:13<10:01,  3.78s/it]Running Inference:  68%|██████▊   | 342/500 [25:18<10:52,  4.13s/it]Running Inference:  69%|██████▊   | 343/500 [25:22<10:39,  4.07s/it]Running Inference:  69%|██████▉   | 344/500 [25:27<10:46,  4.15s/it]Running Inference:  69%|██████▉   | 345/500 [25:32<11:25,  4.42s/it]Running Inference:  69%|██████▉   | 346/500 [25:36<11:03,  4.31s/it]Running Inference:  69%|██████▉   | 347/500 [25:40<10:45,  4.22s/it]Running Inference:  70%|██████▉   | 348/500 [25:44<10:29,  4.14s/it]Running Inference:  70%|██████▉   | 349/500 [25:49<11:16,  4.48s/it]Running Inference:  70%|███████   | 350/500 [25:54<11:47,  4.72s/it]Running Inference:  70%|███████   | 351/500 [25:59<12:07,  4.88s/it]Running Inference:  70%|███████   | 352/500 [26:05<12:20,  5.01s/it]Running Inference:  71%|███████   | 353/500 [26:12<14:16,  5.82s/it]Running Inference:  71%|███████   | 354/500 [26:16<12:18,  5.06s/it]Running Inference:  71%|███████   | 355/500 [26:21<12:14,  5.06s/it]Running Inference:  71%|███████   | 356/500 [26:25<11:21,  4.74s/it]Running Inference:  71%|███████▏  | 357/500 [26:29<11:11,  4.69s/it]Running Inference:  72%|███████▏  | 358/500 [26:34<10:41,  4.52s/it]Running Inference:  72%|███████▏  | 359/500 [26:36<09:17,  3.95s/it]Running Inference:  72%|███████▏  | 360/500 [26:38<07:42,  3.31s/it]Running Inference:  72%|███████▏  | 361/500 [26:42<08:19,  3.60s/it]Running Inference:  72%|███████▏  | 362/500 [26:47<08:57,  3.89s/it]Running Inference:  73%|███████▎  | 363/500 [26:52<09:57,  4.36s/it]Running Inference:  73%|███████▎  | 364/500 [26:57<09:57,  4.39s/it]Running Inference:  73%|███████▎  | 365/500 [27:01<09:55,  4.41s/it]Running Inference:  73%|███████▎  | 366/500 [27:05<09:28,  4.24s/it]Running Inference:  73%|███████▎  | 367/500 [27:09<09:10,  4.14s/it]Running Inference:  74%|███████▎  | 368/500 [27:13<09:01,  4.10s/it]Running Inference:  74%|███████▍  | 369/500 [27:17<08:55,  4.09s/it]Running Inference:  74%|███████▍  | 370/500 [27:21<08:58,  4.14s/it]Running Inference:  74%|███████▍  | 371/500 [27:26<09:28,  4.41s/it]Running Inference:  74%|███████▍  | 372/500 [27:31<09:46,  4.58s/it]Running Inference:  75%|███████▍  | 373/500 [27:36<09:30,  4.49s/it]Running Inference:  75%|███████▍  | 374/500 [27:40<09:30,  4.52s/it]Running Inference:  75%|███████▌  | 375/500 [27:46<10:14,  4.92s/it]Running Inference:  75%|███████▌  | 376/500 [27:50<09:45,  4.72s/it]Running Inference:  75%|███████▌  | 377/500 [27:54<08:58,  4.38s/it]Running Inference:  76%|███████▌  | 378/500 [28:04<12:29,  6.14s/it]Running Inference:  76%|███████▌  | 379/500 [28:09<11:30,  5.70s/it]Running Inference:  76%|███████▌  | 380/500 [28:13<10:18,  5.15s/it]Running Inference:  76%|███████▌  | 381/500 [28:17<09:31,  4.80s/it]Running Inference:  76%|███████▋  | 382/500 [28:22<09:37,  4.90s/it]Running Inference:  77%|███████▋  | 383/500 [28:26<08:58,  4.60s/it]Running Inference:  77%|███████▋  | 384/500 [28:27<07:17,  3.77s/it]Running Inference:  77%|███████▋  | 385/500 [28:32<07:22,  3.85s/it]Running Inference:  77%|███████▋  | 386/500 [28:36<07:41,  4.05s/it]Running Inference:  77%|███████▋  | 387/500 [28:41<08:03,  4.28s/it]Running Inference:  78%|███████▊  | 388/500 [28:46<08:45,  4.69s/it]Running Inference:  78%|███████▊  | 389/500 [28:54<10:24,  5.62s/it]Running Inference:  78%|███████▊  | 390/500 [29:03<12:04,  6.58s/it]Running Inference:  78%|███████▊  | 391/500 [29:10<12:13,  6.73s/it]Running Inference:  78%|███████▊  | 392/500 [29:14<10:43,  5.96s/it]Running Inference:  79%|███████▊  | 393/500 [29:19<10:10,  5.70s/it]Running Inference:  79%|███████▉  | 394/500 [29:25<09:45,  5.52s/it]Running Inference:  79%|███████▉  | 395/500 [29:29<09:05,  5.19s/it]Running Inference:  79%|███████▉  | 396/500 [29:35<09:17,  5.36s/it]Running Inference:  79%|███████▉  | 397/500 [29:40<09:20,  5.44s/it]Running Inference:  80%|███████▉  | 398/500 [29:45<08:45,  5.15s/it]Running Inference:  80%|███████▉  | 399/500 [29:46<06:50,  4.06s/it]Running Inference:  80%|████████  | 400/500 [29:50<06:41,  4.02s/it]Running Inference:  80%|████████  | 401/500 [29:54<06:34,  3.99s/it]Running Inference:  80%|████████  | 402/500 [29:58<06:30,  3.98s/it]Running Inference:  81%|████████  | 403/500 [30:02<06:25,  3.97s/it]Running Inference:  81%|████████  | 404/500 [30:07<06:36,  4.13s/it]Running Inference:  81%|████████  | 405/500 [30:13<07:47,  4.92s/it]Running Inference:  81%|████████  | 406/500 [30:20<08:36,  5.49s/it]Running Inference:  81%|████████▏ | 407/500 [30:25<08:13,  5.31s/it]Running Inference:  82%|████████▏ | 408/500 [30:29<07:39,  5.00s/it]Running Inference:  82%|████████▏ | 409/500 [30:35<07:48,  5.15s/it]Running Inference:  82%|████████▏ | 410/500 [30:39<07:28,  4.98s/it]Running Inference:  82%|████████▏ | 411/500 [30:44<07:01,  4.74s/it]Running Inference:  82%|████████▏ | 412/500 [30:49<07:03,  4.81s/it]Running Inference:  83%|████████▎ | 413/500 [30:54<07:10,  4.95s/it]Running Inference:  83%|████████▎ | 414/500 [30:58<06:54,  4.82s/it]Running Inference:  83%|████████▎ | 415/500 [31:04<07:00,  4.95s/it]Running Inference:  83%|████████▎ | 416/500 [31:08<06:35,  4.71s/it]Running Inference:  83%|████████▎ | 417/500 [31:12<06:12,  4.48s/it]Running Inference:  84%|████████▎ | 418/500 [31:16<05:55,  4.33s/it]Running Inference:  84%|████████▍ | 419/500 [31:20<05:45,  4.27s/it]Running Inference:  84%|████████▍ | 420/500 [31:24<05:38,  4.23s/it]Running Inference:  84%|████████▍ | 421/500 [31:26<04:30,  3.43s/it]Running Inference:  84%|████████▍ | 422/500 [31:28<03:57,  3.05s/it]Running Inference:  85%|████████▍ | 423/500 [31:34<05:15,  4.10s/it]Running Inference:  85%|████████▍ | 424/500 [31:39<05:29,  4.34s/it]Running Inference:  85%|████████▌ | 425/500 [31:44<05:37,  4.50s/it]Running Inference:  85%|████████▌ | 426/500 [31:49<05:32,  4.49s/it]Running Inference:  85%|████████▌ | 427/500 [31:55<06:06,  5.03s/it]Running Inference:  86%|████████▌ | 428/500 [31:59<05:49,  4.86s/it]Running Inference:  86%|████████▌ | 429/500 [32:04<05:42,  4.83s/it]Running Inference:  86%|████████▌ | 430/500 [32:09<05:33,  4.76s/it]Running Inference:  86%|████████▌ | 431/500 [32:13<05:11,  4.51s/it]Running Inference:  86%|████████▋ | 432/500 [32:19<05:40,  5.01s/it]Running Inference:  87%|████████▋ | 433/500 [32:25<05:59,  5.37s/it]Running Inference:  87%|████████▋ | 434/500 [32:26<04:22,  3.98s/it]Running Inference:  87%|████████▋ | 435/500 [32:27<03:30,  3.24s/it]Running Inference:  87%|████████▋ | 436/500 [32:28<02:40,  2.50s/it]Running Inference:  87%|████████▋ | 437/500 [32:32<03:12,  3.06s/it]Running Inference:  88%|████████▊ | 438/500 [32:37<03:39,  3.54s/it]Running Inference:  88%|████████▊ | 439/500 [32:44<04:48,  4.72s/it]Running Inference:  88%|████████▊ | 440/500 [32:49<04:44,  4.73s/it]Running Inference:  88%|████████▊ | 441/500 [32:54<04:46,  4.86s/it]Running Inference:  88%|████████▊ | 442/500 [32:59<04:39,  4.83s/it]Running Inference:  89%|████████▊ | 443/500 [33:04<04:43,  4.97s/it]Running Inference:  89%|████████▉ | 444/500 [33:09<04:29,  4.82s/it]Running Inference:  89%|████████▉ | 445/500 [33:14<04:29,  4.91s/it]Running Inference:  89%|████████▉ | 446/500 [33:21<04:57,  5.51s/it]Running Inference:  89%|████████▉ | 447/500 [33:26<04:52,  5.52s/it]Running Inference:  90%|████████▉ | 448/500 [33:32<04:44,  5.46s/it]Running Inference:  90%|████████▉ | 449/500 [33:37<04:34,  5.39s/it]Running Inference:  90%|█████████ | 450/500 [33:41<04:11,  5.02s/it]Running Inference:  90%|█████████ | 451/500 [33:46<04:06,  5.03s/it]Running Inference:  90%|█████████ | 452/500 [33:49<03:22,  4.21s/it]Running Inference:  91%|█████████ | 453/500 [33:54<03:29,  4.46s/it]Running Inference:  91%|█████████ | 454/500 [33:58<03:25,  4.47s/it]Running Inference:  91%|█████████ | 455/500 [34:03<03:32,  4.72s/it]Running Inference:  91%|█████████ | 456/500 [34:08<03:24,  4.64s/it]Running Inference:  91%|█████████▏| 457/500 [34:12<03:11,  4.46s/it]Running Inference:  92%|█████████▏| 458/500 [34:17<03:20,  4.78s/it]Running Inference:  92%|█████████▏| 459/500 [34:23<03:24,  4.99s/it]Running Inference:  92%|█████████▏| 460/500 [34:28<03:26,  5.17s/it]Running Inference:  92%|█████████▏| 461/500 [34:33<03:16,  5.03s/it]Running Inference:  92%|█████████▏| 462/500 [34:38<03:14,  5.11s/it]Running Inference:  93%|█████████▎| 463/500 [34:43<03:03,  4.96s/it]Running Inference:  93%|█████████▎| 464/500 [34:48<02:55,  4.87s/it]Running Inference:  93%|█████████▎| 465/500 [34:52<02:44,  4.71s/it]Running Inference:  93%|█████████▎| 466/500 [34:56<02:36,  4.61s/it]Running Inference:  93%|█████████▎| 467/500 [35:01<02:28,  4.50s/it]Running Inference:  94%|█████████▎| 468/500 [35:05<02:19,  4.35s/it]Running Inference:  94%|█████████▍| 469/500 [35:09<02:10,  4.22s/it]Running Inference:  94%|█████████▍| 470/500 [35:13<02:05,  4.18s/it]Running Inference:  94%|█████████▍| 471/500 [35:17<02:01,  4.18s/it]Running Inference:  94%|█████████▍| 472/500 [35:21<01:57,  4.18s/it]Running Inference:  95%|█████████▍| 473/500 [35:26<02:01,  4.50s/it]Running Inference:  95%|█████████▍| 474/500 [35:28<01:38,  3.78s/it]Running Inference:  95%|█████████▌| 475/500 [35:33<01:44,  4.18s/it]Running Inference:  95%|█████████▌| 476/500 [35:38<01:39,  4.14s/it]Running Inference:  95%|█████████▌| 477/500 [35:41<01:33,  4.08s/it]Running Inference:  96%|█████████▌| 478/500 [35:48<01:43,  4.69s/it]Running Inference:  96%|█████████▌| 479/500 [35:55<01:56,  5.54s/it]Running Inference:  96%|█████████▌| 480/500 [35:59<01:42,  5.10s/it]Running Inference:  96%|█████████▌| 481/500 [36:04<01:36,  5.10s/it]Running Inference:  96%|█████████▋| 482/500 [36:09<01:28,  4.94s/it]Running Inference:  97%|█████████▋| 483/500 [36:13<01:21,  4.78s/it]Running Inference:  97%|█████████▋| 484/500 [36:17<01:12,  4.53s/it]Running Inference:  97%|█████████▋| 485/500 [36:22<01:09,  4.61s/it]Running Inference:  97%|█████████▋| 486/500 [36:26<01:03,  4.52s/it]Running Inference:  97%|█████████▋| 487/500 [36:31<00:59,  4.58s/it]Running Inference:  98%|█████████▊| 488/500 [36:35<00:53,  4.44s/it]Running Inference:  98%|█████████▊| 489/500 [36:40<00:51,  4.69s/it]Running Inference:  98%|█████████▊| 490/500 [36:45<00:47,  4.71s/it]Running Inference:  98%|█████████▊| 491/500 [36:50<00:43,  4.78s/it]Running Inference:  98%|█████████▊| 492/500 [36:56<00:40,  5.09s/it]Running Inference:  99%|█████████▊| 493/500 [37:00<00:33,  4.84s/it]Running Inference:  99%|█████████▉| 494/500 [37:05<00:28,  4.75s/it]Running Inference:  99%|█████████▉| 495/500 [37:09<00:23,  4.68s/it]Running Inference:  99%|█████████▉| 496/500 [37:14<00:18,  4.71s/it]Running Inference:  99%|█████████▉| 497/500 [37:19<00:14,  4.73s/it]Running Inference: 100%|█████████▉| 498/500 [37:23<00:09,  4.65s/it]Running Inference: 100%|█████████▉| 499/500 [37:28<00:04,  4.56s/it]Running Inference: 100%|██████████| 500/500 [37:32<00:00,  4.45s/it]Running Inference: 100%|██████████| 500/500 [37:32<00:00,  4.50s/it]
2025-12-14 23:02:17,153 - INFO - Inference completed.
2025-12-14 23:02:17,205 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-14 23:02:17,205 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:02:17,207 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-14 23:02:17,207 - INFO - Metrics:
9.78
2025-12-14 23:02:17,208 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=repobench-p, ratio=0.1) on GPU 1

----------------------------------------
Task: repobench-p | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:02:23,657 - INFO - Set deterministic seeds to 42
2025-12-14 23:02:23,658 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:02:23,658 - INFO - Starting evaluation run...
2025-12-14 23:02:23,658 - INFO - Output directory set to: longbenchresult
2025-12-14 23:02:23,658 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-14 23:02:23,658 - INFO - KV Press 'tova' setup.
2025-12-14 23:02:23,658 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:02:23,658 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.45it/s]
Device set to use cuda:0
2025-12-14 23:02:37,386 - INFO - Model pipeline loaded.
2025-12-14 23:02:37,387 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
2025-12-14 23:02:45,300 - INFO - Dataset loaded with 500 entries.
2025-12-14 23:02:45,300 - INFO - Dataset processed with 500 entries.
2025-12-14 23:02:45,344 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<42:58,  5.17s/it]Running Inference:   0%|          | 2/500 [00:11<47:52,  5.77s/it]Running Inference:   1%|          | 3/500 [00:15<43:02,  5.20s/it]Running Inference:   1%|          | 4/500 [00:19<39:25,  4.77s/it]Running Inference:   1%|          | 5/500 [00:24<38:37,  4.68s/it]Running Inference:   1%|          | 6/500 [00:28<37:06,  4.51s/it]Running Inference:   1%|▏         | 7/500 [00:29<27:56,  3.40s/it]Running Inference:   2%|▏         | 8/500 [00:33<29:10,  3.56s/it]Running Inference:   2%|▏         | 9/500 [00:37<30:18,  3.70s/it]Running Inference:   2%|▏         | 10/500 [00:45<39:45,  4.87s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:48<35:17,  4.33s/it]Running Inference:   2%|▏         | 12/500 [00:52<35:21,  4.35s/it]Running Inference:   3%|▎         | 13/500 [00:58<37:40,  4.64s/it]Running Inference:   3%|▎         | 14/500 [01:08<50:43,  6.26s/it]Running Inference:   3%|▎         | 15/500 [01:12<46:52,  5.80s/it]Running Inference:   3%|▎         | 16/500 [01:17<43:05,  5.34s/it]Running Inference:   3%|▎         | 17/500 [01:22<43:18,  5.38s/it]Running Inference:   4%|▎         | 18/500 [01:28<44:03,  5.48s/it]Running Inference:   4%|▍         | 19/500 [01:32<40:50,  5.09s/it]Running Inference:   4%|▍         | 20/500 [01:38<44:04,  5.51s/it]Running Inference:   4%|▍         | 21/500 [01:42<40:25,  5.06s/it]Running Inference:   4%|▍         | 22/500 [01:45<34:20,  4.31s/it]Running Inference:   5%|▍         | 23/500 [01:48<30:39,  3.86s/it]Running Inference:   5%|▍         | 24/500 [01:54<35:41,  4.50s/it]Running Inference:   5%|▌         | 25/500 [02:00<38:59,  4.93s/it]Running Inference:   5%|▌         | 26/500 [02:04<38:23,  4.86s/it]Running Inference:   5%|▌         | 27/500 [02:08<36:24,  4.62s/it]Running Inference:   6%|▌         | 28/500 [02:12<34:34,  4.39s/it]Running Inference:   6%|▌         | 29/500 [02:16<33:36,  4.28s/it]Running Inference:   6%|▌         | 30/500 [02:22<35:47,  4.57s/it]Running Inference:   6%|▌         | 31/500 [02:27<37:13,  4.76s/it]Running Inference:   6%|▋         | 32/500 [02:31<36:02,  4.62s/it]Running Inference:   7%|▋         | 33/500 [02:35<34:38,  4.45s/it]Running Inference:   7%|▋         | 34/500 [02:39<34:14,  4.41s/it]Running Inference:   7%|▋         | 35/500 [02:44<34:12,  4.41s/it]Running Inference:   7%|▋         | 36/500 [02:46<28:13,  3.65s/it]Running Inference:   7%|▋         | 37/500 [02:50<30:33,  3.96s/it]Running Inference:   8%|▊         | 38/500 [02:55<31:39,  4.11s/it]Running Inference:   8%|▊         | 39/500 [02:59<31:34,  4.11s/it]Running Inference:   8%|▊         | 40/500 [03:03<31:32,  4.11s/it]Running Inference:   8%|▊         | 41/500 [03:04<25:05,  3.28s/it]Running Inference:   8%|▊         | 42/500 [03:08<26:15,  3.44s/it]Running Inference:   9%|▊         | 43/500 [03:11<25:09,  3.30s/it]Running Inference:   9%|▉         | 44/500 [03:17<31:20,  4.12s/it]Running Inference:   9%|▉         | 45/500 [03:23<33:51,  4.46s/it]Running Inference:   9%|▉         | 46/500 [03:28<35:40,  4.71s/it]Running Inference:   9%|▉         | 47/500 [03:33<36:56,  4.89s/it]Running Inference:  10%|▉         | 48/500 [03:37<35:36,  4.73s/it]Running Inference:  10%|▉         | 49/500 [03:42<35:07,  4.67s/it]Running Inference:  10%|█         | 50/500 [03:45<32:16,  4.30s/it]Running Inference:  10%|█         | 51/500 [03:50<33:45,  4.51s/it]Running Inference:  10%|█         | 52/500 [03:53<29:21,  3.93s/it]Running Inference:  11%|█         | 53/500 [03:57<29:37,  3.98s/it]Running Inference:  11%|█         | 54/500 [04:01<29:55,  4.03s/it]Running Inference:  11%|█         | 55/500 [04:08<34:58,  4.72s/it]Running Inference:  11%|█         | 56/500 [04:12<34:33,  4.67s/it]Running Inference:  11%|█▏        | 57/500 [04:16<32:38,  4.42s/it]Running Inference:  12%|█▏        | 58/500 [04:20<31:41,  4.30s/it]Running Inference:  12%|█▏        | 59/500 [04:24<31:53,  4.34s/it]Running Inference:  12%|█▏        | 60/500 [04:29<31:13,  4.26s/it]Running Inference:  12%|█▏        | 61/500 [04:32<30:23,  4.15s/it]Running Inference:  12%|█▏        | 62/500 [04:38<33:55,  4.65s/it]Running Inference:  13%|█▎        | 63/500 [04:44<35:33,  4.88s/it]Running Inference:  13%|█▎        | 64/500 [04:48<34:27,  4.74s/it]Running Inference:  13%|█▎        | 65/500 [04:54<37:12,  5.13s/it]Running Inference:  13%|█▎        | 66/500 [04:58<34:10,  4.73s/it]Running Inference:  13%|█▎        | 67/500 [05:02<32:04,  4.44s/it]Running Inference:  14%|█▎        | 68/500 [05:06<31:49,  4.42s/it]Running Inference:  14%|█▍        | 69/500 [05:11<33:32,  4.67s/it]Running Inference:  14%|█▍        | 70/500 [05:20<41:27,  5.78s/it]Running Inference:  14%|█▍        | 71/500 [05:25<39:30,  5.53s/it]Running Inference:  14%|█▍        | 72/500 [05:29<36:50,  5.16s/it]Running Inference:  15%|█▍        | 73/500 [05:34<37:18,  5.24s/it]Running Inference:  15%|█▍        | 74/500 [05:39<36:50,  5.19s/it]Running Inference:  15%|█▌        | 75/500 [05:44<36:06,  5.10s/it]Running Inference:  15%|█▌        | 76/500 [05:49<36:00,  5.10s/it]Running Inference:  15%|█▌        | 77/500 [05:54<34:35,  4.91s/it]Running Inference:  16%|█▌        | 78/500 [06:00<37:55,  5.39s/it]Running Inference:  16%|█▌        | 79/500 [06:04<34:45,  4.95s/it]Running Inference:  16%|█▌        | 80/500 [06:09<33:30,  4.79s/it]Running Inference:  16%|█▌        | 81/500 [06:14<34:49,  4.99s/it]Running Inference:  16%|█▋        | 82/500 [06:19<35:21,  5.07s/it]Running Inference:  17%|█▋        | 83/500 [06:27<41:14,  5.93s/it]Running Inference:  17%|█▋        | 84/500 [06:29<31:56,  4.61s/it]Running Inference:  17%|█▋        | 85/500 [06:30<25:35,  3.70s/it]Running Inference:  17%|█▋        | 86/500 [06:35<26:13,  3.80s/it]Running Inference:  17%|█▋        | 87/500 [06:39<27:02,  3.93s/it]Running Inference:  18%|█▊        | 88/500 [06:43<27:20,  3.98s/it]Running Inference:  18%|█▊        | 89/500 [06:47<27:37,  4.03s/it]Running Inference:  18%|█▊        | 90/500 [06:51<27:49,  4.07s/it]Running Inference:  18%|█▊        | 91/500 [06:55<28:02,  4.11s/it]Running Inference:  18%|█▊        | 92/500 [07:00<29:02,  4.27s/it]Running Inference:  19%|█▊        | 93/500 [07:05<30:29,  4.50s/it]Running Inference:  19%|█▉        | 94/500 [07:08<27:25,  4.05s/it]Running Inference:  19%|█▉        | 95/500 [07:16<34:15,  5.08s/it]Running Inference:  19%|█▉        | 96/500 [07:20<33:32,  4.98s/it]Running Inference:  19%|█▉        | 97/500 [07:25<32:57,  4.91s/it]Running Inference:  20%|█▉        | 98/500 [07:29<31:30,  4.70s/it]Running Inference:  20%|█▉        | 99/500 [07:31<26:04,  3.90s/it]Running Inference:  20%|██        | 100/500 [07:33<21:30,  3.23s/it]Running Inference:  20%|██        | 101/500 [07:37<22:36,  3.40s/it]Running Inference:  20%|██        | 102/500 [07:39<19:57,  3.01s/it]Running Inference:  21%|██        | 103/500 [07:43<22:04,  3.34s/it]Running Inference:  21%|██        | 104/500 [07:48<25:32,  3.87s/it]Running Inference:  21%|██        | 105/500 [07:49<20:24,  3.10s/it]Running Inference:  21%|██        | 106/500 [07:53<22:24,  3.41s/it]Running Inference:  21%|██▏       | 107/500 [07:57<23:22,  3.57s/it]Running Inference:  22%|██▏       | 108/500 [08:02<25:08,  3.85s/it]Running Inference:  22%|██▏       | 109/500 [08:06<26:17,  4.03s/it]Running Inference:  22%|██▏       | 110/500 [08:12<30:10,  4.64s/it]Running Inference:  22%|██▏       | 111/500 [08:14<24:47,  3.82s/it]Running Inference:  22%|██▏       | 112/500 [08:19<25:45,  3.98s/it]Running Inference:  23%|██▎       | 113/500 [08:23<25:59,  4.03s/it]Running Inference:  23%|██▎       | 114/500 [08:27<26:31,  4.12s/it]Running Inference:  23%|██▎       | 115/500 [08:32<27:24,  4.27s/it]Running Inference:  23%|██▎       | 116/500 [08:36<27:13,  4.25s/it]Running Inference:  23%|██▎       | 117/500 [08:41<28:02,  4.39s/it]Running Inference:  24%|██▎       | 118/500 [08:46<29:04,  4.57s/it]Running Inference:  24%|██▍       | 119/500 [08:50<28:59,  4.57s/it]Running Inference:  24%|██▍       | 120/500 [08:54<28:09,  4.45s/it]Running Inference:  24%|██▍       | 121/500 [08:57<24:51,  3.94s/it]Running Inference:  24%|██▍       | 122/500 [09:01<24:44,  3.93s/it]Running Inference:  25%|██▍       | 123/500 [09:05<25:00,  3.98s/it]Running Inference:  25%|██▍       | 124/500 [09:09<25:08,  4.01s/it]Running Inference:  25%|██▌       | 125/500 [09:14<26:09,  4.19s/it]Running Inference:  25%|██▌       | 126/500 [09:16<21:30,  3.45s/it]Running Inference:  25%|██▌       | 127/500 [09:20<22:31,  3.62s/it]Running Inference:  26%|██▌       | 128/500 [09:24<23:10,  3.74s/it]Running Inference:  26%|██▌       | 129/500 [09:31<29:56,  4.84s/it]Running Inference:  26%|██▌       | 130/500 [09:38<34:08,  5.54s/it]Running Inference:  26%|██▌       | 131/500 [09:43<32:15,  5.25s/it]Running Inference:  26%|██▋       | 132/500 [09:46<28:55,  4.72s/it]Running Inference:  27%|██▋       | 133/500 [09:51<27:58,  4.57s/it]Running Inference:  27%|██▋       | 134/500 [09:55<28:31,  4.68s/it]Running Inference:  27%|██▋       | 135/500 [10:04<36:03,  5.93s/it]Running Inference:  27%|██▋       | 136/500 [10:11<36:58,  6.10s/it]Running Inference:  27%|██▋       | 137/500 [10:16<35:44,  5.91s/it]Running Inference:  28%|██▊       | 138/500 [10:23<36:42,  6.08s/it]Running Inference:  28%|██▊       | 139/500 [10:29<36:59,  6.15s/it]Running Inference:  28%|██▊       | 140/500 [10:34<35:15,  5.88s/it]Running Inference:  28%|██▊       | 141/500 [10:36<27:03,  4.52s/it]Running Inference:  28%|██▊       | 142/500 [10:36<20:01,  3.36s/it]Running Inference:  29%|██▊       | 143/500 [10:45<30:07,  5.06s/it]Running Inference:  29%|██▉       | 144/500 [10:49<28:16,  4.76s/it]Running Inference:  29%|██▉       | 145/500 [10:51<22:17,  3.77s/it]Running Inference:  29%|██▉       | 146/500 [10:55<22:44,  3.86s/it]Running Inference:  29%|██▉       | 147/500 [10:56<17:20,  2.95s/it]Running Inference:  30%|██▉       | 148/500 [11:01<21:42,  3.70s/it]Running Inference:  30%|██▉       | 149/500 [11:07<24:57,  4.27s/it]Running Inference:  30%|███       | 150/500 [11:12<27:04,  4.64s/it]Running Inference:  30%|███       | 151/500 [11:16<26:11,  4.50s/it]Running Inference:  30%|███       | 152/500 [11:20<25:14,  4.35s/it]Running Inference:  31%|███       | 153/500 [11:25<25:12,  4.36s/it]Running Inference:  31%|███       | 154/500 [11:30<27:10,  4.71s/it]Running Inference:  31%|███       | 155/500 [11:35<26:54,  4.68s/it]Running Inference:  31%|███       | 156/500 [11:41<28:28,  4.97s/it]Running Inference:  31%|███▏      | 157/500 [11:45<26:57,  4.71s/it]Running Inference:  32%|███▏      | 158/500 [11:49<26:35,  4.67s/it]Running Inference:  32%|███▏      | 159/500 [11:55<27:36,  4.86s/it]Running Inference:  32%|███▏      | 160/500 [11:59<27:19,  4.82s/it]Running Inference:  32%|███▏      | 161/500 [12:04<27:44,  4.91s/it]Running Inference:  32%|███▏      | 162/500 [12:09<27:21,  4.86s/it]Running Inference:  33%|███▎      | 163/500 [12:14<26:38,  4.74s/it]Running Inference:  33%|███▎      | 164/500 [12:17<23:28,  4.19s/it]Running Inference:  33%|███▎      | 165/500 [12:22<25:29,  4.57s/it]Running Inference:  33%|███▎      | 166/500 [12:27<26:24,  4.74s/it]Running Inference:  33%|███▎      | 167/500 [12:31<25:22,  4.57s/it]Running Inference:  34%|███▎      | 168/500 [12:34<21:47,  3.94s/it]Running Inference:  34%|███▍      | 169/500 [12:40<24:57,  4.52s/it]Running Inference:  34%|███▍      | 170/500 [12:44<24:17,  4.42s/it]Running Inference:  34%|███▍      | 171/500 [12:48<23:48,  4.34s/it]Running Inference:  34%|███▍      | 172/500 [12:52<23:24,  4.28s/it]Running Inference:  35%|███▍      | 173/500 [12:56<23:08,  4.25s/it]Running Inference:  35%|███▍      | 174/500 [13:00<22:52,  4.21s/it]Running Inference:  35%|███▌      | 175/500 [13:06<24:29,  4.52s/it]Running Inference:  35%|███▌      | 176/500 [13:11<25:38,  4.75s/it]Running Inference:  35%|███▌      | 177/500 [13:16<26:25,  4.91s/it]Running Inference:  36%|███▌      | 178/500 [13:22<27:42,  5.16s/it]Running Inference:  36%|███▌      | 179/500 [13:26<26:01,  4.87s/it]Running Inference:  36%|███▌      | 180/500 [13:31<25:06,  4.71s/it]Running Inference:  36%|███▌      | 181/500 [13:35<24:27,  4.60s/it]Running Inference:  36%|███▋      | 182/500 [13:39<24:06,  4.55s/it]Running Inference:  37%|███▋      | 183/500 [13:44<23:50,  4.51s/it]Running Inference:  37%|███▋      | 184/500 [13:48<23:05,  4.38s/it]Running Inference:  37%|███▋      | 185/500 [13:52<22:44,  4.33s/it]Running Inference:  37%|███▋      | 186/500 [13:57<24:08,  4.61s/it]Running Inference:  37%|███▋      | 187/500 [14:02<23:58,  4.60s/it]Running Inference:  38%|███▊      | 188/500 [14:03<18:33,  3.57s/it]Running Inference:  38%|███▊      | 189/500 [14:08<20:19,  3.92s/it]Running Inference:  38%|███▊      | 190/500 [14:16<27:10,  5.26s/it]Running Inference:  38%|███▊      | 191/500 [14:21<25:51,  5.02s/it]Running Inference:  38%|███▊      | 192/500 [14:25<25:18,  4.93s/it]Running Inference:  39%|███▊      | 193/500 [14:30<24:33,  4.80s/it]Running Inference:  39%|███▉      | 194/500 [14:34<24:17,  4.76s/it]Running Inference:  39%|███▉      | 195/500 [14:39<24:30,  4.82s/it]Running Inference:  39%|███▉      | 196/500 [14:44<23:25,  4.62s/it]Running Inference:  39%|███▉      | 197/500 [14:48<22:34,  4.47s/it]Running Inference:  40%|███▉      | 198/500 [14:50<18:42,  3.72s/it]Running Inference:  40%|███▉      | 199/500 [14:54<19:18,  3.85s/it]Running Inference:  40%|████      | 200/500 [14:58<19:45,  3.95s/it]Running Inference:  40%|████      | 201/500 [15:02<19:50,  3.98s/it]Running Inference:  40%|████      | 202/500 [15:08<22:59,  4.63s/it]Running Inference:  41%|████      | 203/500 [15:13<22:51,  4.62s/it]Running Inference:  41%|████      | 204/500 [15:17<22:33,  4.57s/it]Running Inference:  41%|████      | 205/500 [15:20<19:55,  4.05s/it]Running Inference:  41%|████      | 206/500 [15:25<21:16,  4.34s/it]Running Inference:  41%|████▏     | 207/500 [15:30<21:21,  4.38s/it]Running Inference:  42%|████▏     | 208/500 [15:34<20:41,  4.25s/it]Running Inference:  42%|████▏     | 209/500 [15:35<16:36,  3.43s/it]Running Inference:  42%|████▏     | 210/500 [15:39<17:29,  3.62s/it]Running Inference:  42%|████▏     | 211/500 [15:45<20:04,  4.17s/it]Running Inference:  42%|████▏     | 212/500 [15:49<20:19,  4.23s/it]Running Inference:  43%|████▎     | 213/500 [15:54<21:11,  4.43s/it]Running Inference:  43%|████▎     | 214/500 [15:58<20:22,  4.27s/it]Running Inference:  43%|████▎     | 215/500 [16:02<20:00,  4.21s/it]Running Inference:  43%|████▎     | 216/500 [16:06<19:43,  4.17s/it]Running Inference:  43%|████▎     | 217/500 [16:10<19:39,  4.17s/it]Running Inference:  44%|████▎     | 218/500 [16:13<17:56,  3.82s/it]Running Inference:  44%|████▍     | 219/500 [16:17<18:19,  3.91s/it]Running Inference:  44%|████▍     | 220/500 [16:21<18:33,  3.98s/it]Running Inference:  44%|████▍     | 221/500 [16:25<18:45,  4.03s/it]Running Inference:  44%|████▍     | 222/500 [16:30<18:57,  4.09s/it]Running Inference:  45%|████▍     | 223/500 [16:34<19:38,  4.25s/it]Running Inference:  45%|████▍     | 224/500 [16:39<20:19,  4.42s/it]Running Inference:  45%|████▌     | 225/500 [16:44<20:23,  4.45s/it]Running Inference:  45%|████▌     | 226/500 [16:50<22:30,  4.93s/it]Running Inference:  45%|████▌     | 227/500 [16:56<24:05,  5.29s/it]Running Inference:  46%|████▌     | 228/500 [16:59<20:36,  4.55s/it]Running Inference:  46%|████▌     | 229/500 [17:05<22:58,  5.09s/it]Running Inference:  46%|████▌     | 230/500 [17:07<19:14,  4.28s/it]Running Inference:  46%|████▌     | 231/500 [17:10<16:51,  3.76s/it]Running Inference:  46%|████▋     | 232/500 [17:13<16:06,  3.61s/it]Running Inference:  47%|████▋     | 233/500 [17:17<16:31,  3.71s/it]Running Inference:  47%|████▋     | 234/500 [17:21<16:48,  3.79s/it]Running Inference:  47%|████▋     | 235/500 [17:26<18:31,  4.19s/it]Running Inference:  47%|████▋     | 236/500 [17:28<15:33,  3.54s/it]Running Inference:  47%|████▋     | 237/500 [17:32<16:23,  3.74s/it]Running Inference:  48%|████▊     | 238/500 [17:38<18:50,  4.31s/it]Running Inference:  48%|████▊     | 239/500 [17:42<18:34,  4.27s/it]Running Inference:  48%|████▊     | 240/500 [17:47<19:32,  4.51s/it]Running Inference:  48%|████▊     | 241/500 [17:52<19:59,  4.63s/it]Running Inference:  48%|████▊     | 242/500 [17:58<21:09,  4.92s/it]Running Inference:  49%|████▊     | 243/500 [18:03<21:57,  5.13s/it]Running Inference:  49%|████▉     | 244/500 [18:06<18:49,  4.41s/it]Running Inference:  49%|████▉     | 245/500 [18:12<20:20,  4.78s/it]Running Inference:  49%|████▉     | 246/500 [18:14<16:21,  3.86s/it]Running Inference:  49%|████▉     | 247/500 [18:18<16:52,  4.00s/it]Running Inference:  50%|████▉     | 248/500 [18:23<17:59,  4.28s/it]Running Inference:  50%|████▉     | 249/500 [18:27<18:14,  4.36s/it]Running Inference:  50%|█████     | 250/500 [18:31<17:44,  4.26s/it]Running Inference:  50%|█████     | 251/500 [18:37<19:49,  4.78s/it]Running Inference:  50%|█████     | 252/500 [18:43<20:37,  4.99s/it]Running Inference:  51%|█████     | 253/500 [18:46<18:20,  4.45s/it]Running Inference:  51%|█████     | 254/500 [18:48<15:02,  3.67s/it]Running Inference:  51%|█████     | 255/500 [18:51<13:41,  3.35s/it]Running Inference:  51%|█████     | 256/500 [18:56<16:30,  4.06s/it]Running Inference:  51%|█████▏    | 257/500 [19:01<17:00,  4.20s/it]Running Inference:  52%|█████▏    | 258/500 [19:04<15:11,  3.77s/it]Running Inference:  52%|█████▏    | 259/500 [19:06<14:06,  3.51s/it]Running Inference:  52%|█████▏    | 260/500 [19:11<14:41,  3.67s/it]Running Inference:  52%|█████▏    | 261/500 [19:14<14:45,  3.70s/it]Running Inference:  52%|█████▏    | 262/500 [19:17<13:48,  3.48s/it]Running Inference:  53%|█████▎    | 263/500 [19:21<14:28,  3.67s/it]Running Inference:  53%|█████▎    | 264/500 [19:26<15:48,  4.02s/it]Running Inference:  53%|█████▎    | 265/500 [19:31<16:24,  4.19s/it]Running Inference:  53%|█████▎    | 266/500 [19:35<16:50,  4.32s/it]Running Inference:  53%|█████▎    | 267/500 [19:43<20:29,  5.27s/it]Running Inference:  54%|█████▎    | 268/500 [19:47<19:12,  4.97s/it]Running Inference:  54%|█████▍    | 269/500 [19:55<22:34,  5.87s/it]Running Inference:  54%|█████▍    | 270/500 [20:04<25:25,  6.63s/it]Running Inference:  54%|█████▍    | 271/500 [20:10<25:14,  6.61s/it]Running Inference:  54%|█████▍    | 272/500 [20:15<22:43,  5.98s/it]Running Inference:  55%|█████▍    | 273/500 [20:17<18:47,  4.97s/it]Running Inference:  55%|█████▍    | 274/500 [20:23<19:42,  5.23s/it]Running Inference:  55%|█████▌    | 275/500 [20:27<18:32,  4.95s/it]Running Inference:  55%|█████▌    | 276/500 [20:30<16:04,  4.31s/it]Running Inference:  55%|█████▌    | 277/500 [20:35<16:39,  4.48s/it]Running Inference:  56%|█████▌    | 278/500 [20:41<18:02,  4.87s/it]Running Inference:  56%|█████▌    | 279/500 [20:45<17:10,  4.66s/it]Running Inference:  56%|█████▌    | 280/500 [20:47<14:34,  3.97s/it]Running Inference:  56%|█████▌    | 281/500 [20:52<14:55,  4.09s/it]Running Inference:  56%|█████▋    | 282/500 [20:57<16:23,  4.51s/it]Running Inference:  57%|█████▋    | 283/500 [21:02<17:02,  4.71s/it]Running Inference:  57%|█████▋    | 284/500 [21:08<17:25,  4.84s/it]Running Inference:  57%|█████▋    | 285/500 [21:12<16:42,  4.67s/it]Running Inference:  57%|█████▋    | 286/500 [21:16<16:14,  4.55s/it]Running Inference:  57%|█████▋    | 287/500 [21:23<18:29,  5.21s/it]Running Inference:  58%|█████▊    | 288/500 [21:27<17:05,  4.84s/it]Running Inference:  58%|█████▊    | 289/500 [21:31<16:06,  4.58s/it]Running Inference:  58%|█████▊    | 290/500 [21:36<16:19,  4.67s/it]Running Inference:  58%|█████▊    | 291/500 [21:41<17:26,  5.01s/it]Running Inference:  58%|█████▊    | 292/500 [21:46<16:40,  4.81s/it]Running Inference:  59%|█████▊    | 293/500 [21:50<15:44,  4.56s/it]Running Inference:  59%|█████▉    | 294/500 [21:54<15:30,  4.52s/it]Running Inference:  59%|█████▉    | 295/500 [21:59<15:51,  4.64s/it]Running Inference:  59%|█████▉    | 296/500 [22:02<13:35,  4.00s/it]Running Inference:  59%|█████▉    | 297/500 [22:06<13:53,  4.10s/it]Running Inference:  60%|█████▉    | 298/500 [22:10<14:03,  4.17s/it]Running Inference:  60%|█████▉    | 299/500 [22:15<14:13,  4.24s/it]Running Inference:  60%|██████    | 300/500 [22:20<15:26,  4.63s/it]Running Inference:  60%|██████    | 301/500 [22:22<12:19,  3.71s/it]Running Inference:  60%|██████    | 302/500 [22:26<12:41,  3.85s/it]Running Inference:  61%|██████    | 303/500 [22:30<13:10,  4.01s/it]Running Inference:  61%|██████    | 304/500 [22:35<13:28,  4.12s/it]Running Inference:  61%|██████    | 305/500 [22:39<13:19,  4.10s/it]Running Inference:  61%|██████    | 306/500 [22:44<13:53,  4.29s/it]Running Inference:  61%|██████▏   | 307/500 [22:45<11:11,  3.48s/it]Running Inference:  62%|██████▏   | 308/500 [22:49<11:32,  3.60s/it]Running Inference:  62%|██████▏   | 309/500 [22:53<12:02,  3.79s/it]Running Inference:  62%|██████▏   | 310/500 [22:58<12:50,  4.05s/it]Running Inference:  62%|██████▏   | 311/500 [23:02<12:47,  4.06s/it]Running Inference:  62%|██████▏   | 312/500 [23:06<12:54,  4.12s/it]Running Inference:  63%|██████▎   | 313/500 [23:11<13:33,  4.35s/it]Running Inference:  63%|██████▎   | 314/500 [23:15<13:20,  4.30s/it]Running Inference:  63%|██████▎   | 315/500 [23:20<14:03,  4.56s/it]Running Inference:  63%|██████▎   | 316/500 [23:26<14:31,  4.74s/it]Running Inference:  63%|██████▎   | 317/500 [23:31<14:54,  4.89s/it]Running Inference:  64%|██████▎   | 318/500 [23:36<15:12,  5.01s/it]Running Inference:  64%|██████▍   | 319/500 [23:39<12:47,  4.24s/it]Running Inference:  64%|██████▍   | 320/500 [23:43<12:35,  4.19s/it]Running Inference:  64%|██████▍   | 321/500 [23:47<12:40,  4.25s/it]Running Inference:  64%|██████▍   | 322/500 [23:51<12:23,  4.18s/it]Running Inference:  65%|██████▍   | 323/500 [23:55<12:26,  4.22s/it]Running Inference:  65%|██████▍   | 324/500 [24:01<13:29,  4.60s/it]Running Inference:  65%|██████▌   | 325/500 [24:07<14:44,  5.05s/it]Running Inference:  65%|██████▌   | 326/500 [24:08<11:15,  3.88s/it]Running Inference:  65%|██████▌   | 327/500 [24:12<10:48,  3.75s/it]Running Inference:  66%|██████▌   | 328/500 [24:16<11:03,  3.86s/it]Running Inference:  66%|██████▌   | 329/500 [24:21<12:07,  4.26s/it]Running Inference:  66%|██████▌   | 330/500 [24:23<10:28,  3.70s/it]Running Inference:  66%|██████▌   | 331/500 [24:27<10:43,  3.81s/it]Running Inference:  66%|██████▋   | 332/500 [24:29<09:03,  3.24s/it]Running Inference:  67%|██████▋   | 333/500 [24:33<09:48,  3.52s/it]Running Inference:  67%|██████▋   | 334/500 [24:35<07:43,  2.79s/it]Running Inference:  67%|██████▋   | 335/500 [24:44<12:53,  4.69s/it]Running Inference:  67%|██████▋   | 336/500 [24:51<14:39,  5.36s/it]Running Inference:  67%|██████▋   | 337/500 [24:55<14:09,  5.21s/it]Running Inference:  68%|██████▊   | 338/500 [24:59<13:07,  4.86s/it]Running Inference:  68%|██████▊   | 339/500 [25:01<10:13,  3.81s/it]Running Inference:  68%|██████▊   | 340/500 [25:06<11:36,  4.35s/it]Running Inference:  68%|██████▊   | 341/500 [25:09<09:49,  3.71s/it]Running Inference:  68%|██████▊   | 342/500 [25:14<10:50,  4.12s/it]Running Inference:  69%|██████▊   | 343/500 [25:18<10:40,  4.08s/it]Running Inference:  69%|██████▉   | 344/500 [25:22<10:49,  4.16s/it]Running Inference:  69%|██████▉   | 345/500 [25:27<11:28,  4.44s/it]Running Inference:  69%|██████▉   | 346/500 [25:31<11:11,  4.36s/it]Running Inference:  69%|██████▉   | 347/500 [25:35<10:56,  4.29s/it]Running Inference:  70%|██████▉   | 348/500 [25:40<10:40,  4.21s/it]Running Inference:  70%|██████▉   | 349/500 [25:45<11:27,  4.56s/it]Running Inference:  70%|███████   | 350/500 [25:50<11:58,  4.79s/it]Running Inference:  70%|███████   | 351/500 [25:56<12:16,  4.94s/it]Running Inference:  70%|███████   | 352/500 [26:01<12:28,  5.06s/it]Running Inference:  71%|███████   | 353/500 [26:09<14:20,  5.86s/it]Running Inference:  71%|███████   | 354/500 [26:12<12:21,  5.08s/it]Running Inference:  71%|███████   | 355/500 [26:17<12:10,  5.04s/it]Running Inference:  71%|███████   | 356/500 [26:21<11:18,  4.72s/it]Running Inference:  71%|███████▏  | 357/500 [26:25<11:09,  4.68s/it]Running Inference:  72%|███████▏  | 358/500 [26:29<10:41,  4.52s/it]Running Inference:  72%|███████▏  | 359/500 [26:33<10:07,  4.31s/it]Running Inference:  72%|███████▏  | 360/500 [26:35<08:17,  3.55s/it]Running Inference:  72%|███████▏  | 361/500 [26:38<07:34,  3.27s/it]Running Inference:  72%|███████▏  | 362/500 [26:42<08:26,  3.67s/it]Running Inference:  73%|███████▎  | 363/500 [26:48<09:36,  4.21s/it]Running Inference:  73%|███████▎  | 364/500 [26:52<09:44,  4.30s/it]Running Inference:  73%|███████▎  | 365/500 [26:57<09:49,  4.36s/it]Running Inference:  73%|███████▎  | 366/500 [27:01<09:27,  4.23s/it]Running Inference:  73%|███████▎  | 367/500 [27:05<09:12,  4.15s/it]Running Inference:  74%|███████▎  | 368/500 [27:09<09:05,  4.13s/it]Running Inference:  74%|███████▍  | 369/500 [27:13<09:01,  4.13s/it]Running Inference:  74%|███████▍  | 370/500 [27:17<09:04,  4.19s/it]Running Inference:  74%|███████▍  | 371/500 [27:22<09:34,  4.45s/it]Running Inference:  74%|███████▍  | 372/500 [27:27<09:52,  4.63s/it]Running Inference:  75%|███████▍  | 373/500 [27:32<09:37,  4.55s/it]Running Inference:  75%|███████▍  | 374/500 [27:36<09:37,  4.58s/it]Running Inference:  75%|███████▌  | 375/500 [27:42<10:20,  4.96s/it]Running Inference:  75%|███████▌  | 376/500 [27:47<09:51,  4.77s/it]Running Inference:  75%|███████▌  | 377/500 [27:49<08:29,  4.14s/it]Running Inference:  76%|███████▌  | 378/500 [27:59<12:07,  5.97s/it]Running Inference:  76%|███████▌  | 379/500 [28:04<11:16,  5.59s/it]Running Inference:  76%|███████▌  | 380/500 [28:05<08:26,  4.22s/it]Running Inference:  76%|███████▌  | 381/500 [28:09<08:13,  4.15s/it]Running Inference:  76%|███████▋  | 382/500 [28:14<08:44,  4.45s/it]Running Inference:  77%|███████▋  | 383/500 [28:18<08:21,  4.29s/it]Running Inference:  77%|███████▋  | 384/500 [28:20<06:52,  3.55s/it]Running Inference:  77%|███████▋  | 385/500 [28:24<07:04,  3.70s/it]Running Inference:  77%|███████▋  | 386/500 [28:29<07:32,  3.97s/it]Running Inference:  77%|███████▋  | 387/500 [28:33<07:56,  4.22s/it]Running Inference:  78%|███████▊  | 388/500 [28:39<08:41,  4.66s/it]Running Inference:  78%|███████▊  | 389/500 [28:47<10:20,  5.59s/it]Running Inference:  78%|███████▊  | 390/500 [28:56<12:00,  6.55s/it]Running Inference:  78%|███████▊  | 391/500 [29:03<12:11,  6.71s/it]Running Inference:  78%|███████▊  | 392/500 [29:07<10:43,  5.96s/it]Running Inference:  79%|███████▊  | 393/500 [29:12<10:12,  5.72s/it]Running Inference:  79%|███████▉  | 394/500 [29:17<09:49,  5.56s/it]Running Inference:  79%|███████▉  | 395/500 [29:22<09:09,  5.24s/it]Running Inference:  79%|███████▉  | 396/500 [29:28<09:21,  5.40s/it]Running Inference:  79%|███████▉  | 397/500 [29:33<09:24,  5.48s/it]Running Inference:  80%|███████▉  | 398/500 [29:38<08:50,  5.20s/it]Running Inference:  80%|███████▉  | 399/500 [29:39<06:53,  4.10s/it]Running Inference:  80%|████████  | 400/500 [29:40<05:10,  3.10s/it]Running Inference:  80%|████████  | 401/500 [29:44<05:33,  3.37s/it]Running Inference:  80%|████████  | 402/500 [29:48<05:48,  3.56s/it]Running Inference:  81%|████████  | 403/500 [29:52<05:58,  3.69s/it]Running Inference:  81%|████████  | 404/500 [29:57<06:18,  3.95s/it]Running Inference:  81%|████████  | 405/500 [30:03<07:33,  4.78s/it]Running Inference:  81%|████████  | 406/500 [30:10<08:26,  5.39s/it]Running Inference:  81%|████████▏ | 407/500 [30:15<08:08,  5.26s/it]Running Inference:  82%|████████▏ | 408/500 [30:19<07:37,  4.98s/it]Running Inference:  82%|████████▏ | 409/500 [30:25<07:48,  5.15s/it]Running Inference:  82%|████████▏ | 410/500 [30:30<07:29,  5.00s/it]Running Inference:  82%|████████▏ | 411/500 [30:34<07:04,  4.77s/it]Running Inference:  82%|████████▏ | 412/500 [30:39<07:07,  4.86s/it]Running Inference:  83%|████████▎ | 413/500 [30:44<07:14,  4.99s/it]Running Inference:  83%|████████▎ | 414/500 [30:49<06:57,  4.86s/it]Running Inference:  83%|████████▎ | 415/500 [30:54<07:04,  5.00s/it]Running Inference:  83%|████████▎ | 416/500 [30:58<06:41,  4.78s/it]Running Inference:  83%|████████▎ | 417/500 [31:02<06:17,  4.55s/it]Running Inference:  84%|████████▎ | 418/500 [31:06<05:59,  4.39s/it]Running Inference:  84%|████████▍ | 419/500 [31:11<05:50,  4.33s/it]Running Inference:  84%|████████▍ | 420/500 [31:15<05:44,  4.30s/it]Running Inference:  84%|████████▍ | 421/500 [31:16<04:34,  3.48s/it]Running Inference:  84%|████████▍ | 422/500 [31:19<04:00,  3.09s/it]Running Inference:  85%|████████▍ | 423/500 [31:25<05:16,  4.11s/it]Running Inference:  85%|████████▍ | 424/500 [31:30<05:31,  4.36s/it]Running Inference:  85%|████████▌ | 425/500 [31:35<05:40,  4.53s/it]Running Inference:  85%|████████▌ | 426/500 [31:39<05:34,  4.53s/it]Running Inference:  85%|████████▌ | 427/500 [31:46<06:08,  5.05s/it]Running Inference:  86%|████████▌ | 428/500 [31:50<05:52,  4.89s/it]Running Inference:  86%|████████▌ | 429/500 [31:55<05:45,  4.87s/it]Running Inference:  86%|████████▌ | 430/500 [32:00<05:36,  4.81s/it]Running Inference:  86%|████████▌ | 431/500 [32:04<05:14,  4.56s/it]Running Inference:  86%|████████▋ | 432/500 [32:10<05:42,  5.04s/it]Running Inference:  87%|████████▋ | 433/500 [32:16<06:01,  5.39s/it]Running Inference:  87%|████████▋ | 434/500 [32:17<04:23,  4.00s/it]Running Inference:  87%|████████▋ | 435/500 [32:18<03:31,  3.26s/it]Running Inference:  87%|████████▋ | 436/500 [32:19<02:41,  2.52s/it]Running Inference:  87%|████████▋ | 437/500 [32:24<03:14,  3.09s/it]Running Inference:  88%|████████▊ | 438/500 [32:28<03:42,  3.59s/it]Running Inference:  88%|████████▊ | 439/500 [32:36<04:49,  4.75s/it]Running Inference:  88%|████████▊ | 440/500 [32:41<04:45,  4.77s/it]Running Inference:  88%|████████▊ | 441/500 [32:46<04:48,  4.89s/it]Running Inference:  88%|████████▊ | 442/500 [32:51<04:42,  4.86s/it]Running Inference:  89%|████████▊ | 443/500 [32:56<04:45,  5.01s/it]Running Inference:  89%|████████▉ | 444/500 [33:00<04:32,  4.87s/it]Running Inference:  89%|████████▉ | 445/500 [33:06<04:32,  4.96s/it]Running Inference:  89%|████████▉ | 446/500 [33:13<04:59,  5.55s/it]Running Inference:  89%|████████▉ | 447/500 [33:18<04:54,  5.55s/it]Running Inference:  90%|████████▉ | 448/500 [33:24<04:46,  5.50s/it]Running Inference:  90%|████████▉ | 449/500 [33:29<04:42,  5.53s/it]Running Inference:  90%|█████████ | 450/500 [33:33<04:17,  5.15s/it]Running Inference:  90%|█████████ | 451/500 [33:36<03:30,  4.29s/it]Running Inference:  90%|█████████ | 452/500 [33:38<02:57,  3.69s/it]Running Inference:  91%|█████████ | 453/500 [33:43<03:13,  4.12s/it]Running Inference:  91%|█████████ | 454/500 [33:48<03:15,  4.24s/it]Running Inference:  91%|█████████ | 455/500 [33:53<03:25,  4.57s/it]Running Inference:  91%|█████████ | 456/500 [33:57<03:19,  4.54s/it]Running Inference:  91%|█████████▏| 457/500 [33:58<02:28,  3.44s/it]Running Inference:  92%|█████████▏| 458/500 [34:04<02:50,  4.07s/it]Running Inference:  92%|█████████▏| 459/500 [34:09<03:04,  4.50s/it]Running Inference:  92%|█████████▏| 460/500 [34:15<03:13,  4.84s/it]Running Inference:  92%|█████████▏| 461/500 [34:20<03:07,  4.81s/it]Running Inference:  92%|█████████▏| 462/500 [34:25<03:08,  4.96s/it]Running Inference:  93%|█████████▎| 463/500 [34:30<02:59,  4.85s/it]Running Inference:  93%|█████████▎| 464/500 [34:34<02:52,  4.80s/it]Running Inference:  93%|█████████▎| 465/500 [34:39<02:43,  4.67s/it]Running Inference:  93%|█████████▎| 466/500 [34:43<02:35,  4.59s/it]Running Inference:  93%|█████████▎| 467/500 [34:47<02:28,  4.49s/it]Running Inference:  94%|█████████▎| 468/500 [34:51<02:18,  4.34s/it]Running Inference:  94%|█████████▍| 469/500 [34:55<02:10,  4.21s/it]Running Inference:  94%|█████████▍| 470/500 [34:59<02:05,  4.18s/it]Running Inference:  94%|█████████▍| 471/500 [35:04<02:01,  4.19s/it]Running Inference:  94%|█████████▍| 472/500 [35:08<01:57,  4.20s/it]Running Inference:  95%|█████████▍| 473/500 [35:13<02:02,  4.52s/it]Running Inference:  95%|█████████▍| 474/500 [35:15<01:38,  3.80s/it]Running Inference:  95%|█████████▌| 475/500 [35:20<01:44,  4.18s/it]Running Inference:  95%|█████████▌| 476/500 [35:24<01:39,  4.13s/it]Running Inference:  95%|█████████▌| 477/500 [35:28<01:33,  4.08s/it]Running Inference:  96%|█████████▌| 478/500 [35:34<01:42,  4.67s/it]Running Inference:  96%|█████████▌| 479/500 [35:42<01:55,  5.49s/it]Running Inference:  96%|█████████▌| 480/500 [35:46<01:41,  5.07s/it]Running Inference:  96%|█████████▌| 481/500 [35:51<01:36,  5.08s/it]Running Inference:  96%|█████████▋| 482/500 [35:55<01:28,  4.92s/it]Running Inference:  97%|█████████▋| 483/500 [36:00<01:21,  4.77s/it]Running Inference:  97%|█████████▋| 484/500 [36:04<01:12,  4.54s/it]Running Inference:  97%|█████████▋| 485/500 [36:09<01:09,  4.61s/it]Running Inference:  97%|█████████▋| 486/500 [36:13<01:03,  4.52s/it]Running Inference:  97%|█████████▋| 487/500 [36:18<00:59,  4.58s/it]Running Inference:  98%|█████████▊| 488/500 [36:22<00:53,  4.44s/it]Running Inference:  98%|█████████▊| 489/500 [36:27<00:51,  4.70s/it]Running Inference:  98%|█████████▊| 490/500 [36:32<00:47,  4.72s/it]Running Inference:  98%|█████████▊| 491/500 [36:37<00:43,  4.79s/it]Running Inference:  98%|█████████▊| 492/500 [36:43<00:40,  5.09s/it]Running Inference:  99%|█████████▊| 493/500 [36:47<00:33,  4.84s/it]Running Inference:  99%|█████████▉| 494/500 [36:51<00:28,  4.77s/it]Running Inference:  99%|█████████▉| 495/500 [36:56<00:23,  4.71s/it]Running Inference:  99%|█████████▉| 496/500 [37:01<00:19,  4.75s/it]Running Inference:  99%|█████████▉| 497/500 [37:06<00:14,  4.77s/it]Running Inference: 100%|█████████▉| 498/500 [37:10<00:09,  4.70s/it]Running Inference: 100%|█████████▉| 499/500 [37:15<00:04,  4.62s/it]Running Inference: 100%|██████████| 500/500 [37:19<00:00,  4.51s/it]Running Inference: 100%|██████████| 500/500 [37:19<00:00,  4.48s/it]
2025-12-14 23:40:04,737 - INFO - Inference completed.
2025-12-14 23:40:04,790 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-14 23:40:04,790 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:40:04,792 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-14 23:40:04,792 - INFO - Metrics:
10.29
2025-12-14 23:40:04,793 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=repobench-p, ratio=0.2) on GPU 1

----------------------------------------
Task: repobench-p | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:40:11,361 - INFO - Set deterministic seeds to 42
2025-12-14 23:40:11,361 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:40:11,361 - INFO - Starting evaluation run...
2025-12-14 23:40:11,361 - INFO - Output directory set to: longbenchresult
2025-12-14 23:40:11,362 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-14 23:40:11,362 - INFO - KV Press 'tova' setup.
2025-12-14 23:40:11,362 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:40:11,362 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.55it/s]
Device set to use cuda:0
2025-12-14 23:40:25,636 - INFO - Model pipeline loaded.
2025-12-14 23:40:25,637 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
2025-12-14 23:40:47,243 - INFO - Dataset loaded with 500 entries.
2025-12-14 23:40:47,243 - INFO - Dataset processed with 500 entries.
2025-12-14 23:40:47,289 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<44:21,  5.33s/it]Running Inference:   0%|          | 2/500 [00:11<48:21,  5.83s/it]Running Inference:   1%|          | 3/500 [00:14<39:09,  4.73s/it]Running Inference:   1%|          | 4/500 [00:19<37:15,  4.51s/it]Running Inference:   1%|          | 5/500 [00:23<37:28,  4.54s/it]Running Inference:   1%|          | 6/500 [00:28<36:44,  4.46s/it]Running Inference:   1%|▏         | 7/500 [00:29<27:50,  3.39s/it]Running Inference:   2%|▏         | 8/500 [00:33<29:36,  3.61s/it]Running Inference:   2%|▏         | 9/500 [00:37<30:48,  3.76s/it]Running Inference:   2%|▏         | 10/500 [00:44<40:08,  4.91s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:47<35:12,  4.32s/it]Running Inference:   2%|▏         | 12/500 [00:52<35:29,  4.36s/it]Running Inference:   3%|▎         | 13/500 [00:57<37:56,  4.67s/it]Running Inference:   3%|▎         | 14/500 [01:07<50:54,  6.29s/it]Running Inference:   3%|▎         | 15/500 [01:12<47:09,  5.83s/it]Running Inference:   3%|▎         | 16/500 [01:16<43:23,  5.38s/it]Running Inference:   3%|▎         | 17/500 [01:22<43:38,  5.42s/it]Running Inference:   4%|▎         | 18/500 [01:28<44:30,  5.54s/it]Running Inference:   4%|▍         | 19/500 [01:32<41:27,  5.17s/it]Running Inference:   4%|▍         | 20/500 [01:39<45:41,  5.71s/it]Running Inference:   4%|▍         | 21/500 [01:43<41:41,  5.22s/it]Running Inference:   4%|▍         | 22/500 [01:46<35:14,  4.42s/it]Running Inference:   5%|▍         | 23/500 [01:48<31:17,  3.94s/it]Running Inference:   5%|▍         | 24/500 [01:54<36:26,  4.59s/it]Running Inference:   5%|▌         | 25/500 [02:00<39:42,  5.02s/it]Running Inference:   5%|▌         | 26/500 [02:05<39:19,  4.98s/it]Running Inference:   5%|▌         | 27/500 [02:10<37:22,  4.74s/it]Running Inference:   6%|▌         | 28/500 [02:13<33:33,  4.26s/it]Running Inference:   6%|▌         | 29/500 [02:17<33:09,  4.22s/it]Running Inference:   6%|▌         | 30/500 [02:22<35:41,  4.56s/it]Running Inference:   6%|▌         | 31/500 [02:28<37:24,  4.79s/it]Running Inference:   6%|▋         | 32/500 [02:32<36:21,  4.66s/it]Running Inference:   7%|▋         | 33/500 [02:36<35:06,  4.51s/it]Running Inference:   7%|▋         | 34/500 [02:40<34:43,  4.47s/it]Running Inference:   7%|▋         | 35/500 [02:45<34:37,  4.47s/it]Running Inference:   7%|▋         | 36/500 [02:47<28:33,  3.69s/it]Running Inference:   7%|▋         | 37/500 [02:50<26:56,  3.49s/it]Running Inference:   8%|▊         | 38/500 [02:54<29:23,  3.82s/it]Running Inference:   8%|▊         | 39/500 [02:59<30:09,  3.92s/it]Running Inference:   8%|▊         | 40/500 [03:03<30:37,  4.00s/it]Running Inference:   8%|▊         | 41/500 [03:04<24:28,  3.20s/it]Running Inference:   8%|▊         | 42/500 [03:08<25:57,  3.40s/it]Running Inference:   9%|▊         | 43/500 [03:11<25:02,  3.29s/it]Running Inference:   9%|▉         | 44/500 [03:17<31:15,  4.11s/it]Running Inference:   9%|▉         | 45/500 [03:22<34:01,  4.49s/it]Running Inference:   9%|▉         | 46/500 [03:28<36:00,  4.76s/it]Running Inference:   9%|▉         | 47/500 [03:33<37:18,  4.94s/it]Running Inference:  10%|▉         | 48/500 [03:37<35:57,  4.77s/it]Running Inference:  10%|▉         | 49/500 [03:42<35:29,  4.72s/it]Running Inference:  10%|█         | 50/500 [03:46<32:32,  4.34s/it]Running Inference:  10%|█         | 51/500 [03:51<34:07,  4.56s/it]Running Inference:  10%|█         | 52/500 [03:55<34:13,  4.58s/it]Running Inference:  11%|█         | 53/500 [03:59<33:15,  4.46s/it]Running Inference:  11%|█         | 54/500 [04:04<32:42,  4.40s/it]Running Inference:  11%|█         | 55/500 [04:10<36:59,  4.99s/it]Running Inference:  11%|█         | 56/500 [04:15<36:07,  4.88s/it]Running Inference:  11%|█▏        | 57/500 [04:19<33:50,  4.58s/it]Running Inference:  12%|█▏        | 58/500 [04:23<32:40,  4.43s/it]Running Inference:  12%|█▏        | 59/500 [04:27<33:14,  4.52s/it]Running Inference:  12%|█▏        | 60/500 [04:31<32:18,  4.41s/it]Running Inference:  12%|█▏        | 61/500 [04:35<31:15,  4.27s/it]Running Inference:  12%|█▏        | 62/500 [04:41<34:36,  4.74s/it]Running Inference:  13%|█▎        | 63/500 [04:46<35:27,  4.87s/it]Running Inference:  13%|█▎        | 64/500 [04:51<34:30,  4.75s/it]Running Inference:  13%|█▎        | 65/500 [04:57<37:18,  5.15s/it]Running Inference:  13%|█▎        | 66/500 [05:01<34:21,  4.75s/it]Running Inference:  13%|█▎        | 67/500 [05:04<30:12,  4.19s/it]Running Inference:  14%|█▎        | 68/500 [05:09<32:53,  4.57s/it]Running Inference:  14%|█▍        | 69/500 [05:14<34:24,  4.79s/it]Running Inference:  14%|█▍        | 70/500 [05:22<39:38,  5.53s/it]Running Inference:  14%|█▍        | 71/500 [05:27<38:26,  5.38s/it]Running Inference:  14%|█▍        | 72/500 [05:32<37:23,  5.24s/it]Running Inference:  15%|█▍        | 73/500 [05:37<37:48,  5.31s/it]Running Inference:  15%|█▍        | 74/500 [05:42<37:17,  5.25s/it]Running Inference:  15%|█▌        | 75/500 [05:47<36:29,  5.15s/it]Running Inference:  15%|█▌        | 76/500 [05:52<36:20,  5.14s/it]Running Inference:  15%|█▌        | 77/500 [05:57<34:53,  4.95s/it]Running Inference:  16%|█▌        | 78/500 [06:03<38:08,  5.42s/it]Running Inference:  16%|█▌        | 79/500 [06:07<35:04,  5.00s/it]Running Inference:  16%|█▌        | 80/500 [06:12<33:51,  4.84s/it]Running Inference:  16%|█▌        | 81/500 [06:17<35:14,  5.05s/it]Running Inference:  16%|█▋        | 82/500 [06:23<36:01,  5.17s/it]Running Inference:  17%|█▋        | 83/500 [06:31<41:40,  6.00s/it]Running Inference:  17%|█▋        | 84/500 [06:32<32:14,  4.65s/it]Running Inference:  17%|█▋        | 85/500 [06:34<25:46,  3.73s/it]Running Inference:  17%|█▋        | 86/500 [06:38<26:26,  3.83s/it]Running Inference:  17%|█▋        | 87/500 [06:42<26:03,  3.79s/it]Running Inference:  18%|█▊        | 88/500 [06:43<21:22,  3.11s/it]Running Inference:  18%|█▊        | 89/500 [06:47<23:32,  3.44s/it]Running Inference:  18%|█▊        | 90/500 [06:52<25:08,  3.68s/it]Running Inference:  18%|█▊        | 91/500 [06:56<26:21,  3.87s/it]Running Inference:  18%|█▊        | 92/500 [07:01<28:03,  4.13s/it]Running Inference:  19%|█▊        | 93/500 [07:06<29:55,  4.41s/it]Running Inference:  19%|█▉        | 94/500 [07:09<27:15,  4.03s/it]Running Inference:  19%|█▉        | 95/500 [07:16<34:04,  5.05s/it]Running Inference:  19%|█▉        | 96/500 [07:21<33:31,  4.98s/it]Running Inference:  19%|█▉        | 97/500 [07:26<33:01,  4.92s/it]Running Inference:  20%|█▉        | 98/500 [07:30<31:40,  4.73s/it]Running Inference:  20%|█▉        | 99/500 [07:32<26:11,  3.92s/it]Running Inference:  20%|██        | 100/500 [07:34<21:36,  3.24s/it]Running Inference:  20%|██        | 101/500 [07:38<23:15,  3.50s/it]Running Inference:  20%|██        | 102/500 [07:40<20:29,  3.09s/it]Running Inference:  21%|██        | 103/500 [07:44<22:38,  3.42s/it]Running Inference:  21%|██        | 104/500 [07:49<26:08,  3.96s/it]Running Inference:  21%|██        | 105/500 [07:51<20:51,  3.17s/it]Running Inference:  21%|██        | 106/500 [07:55<22:54,  3.49s/it]Running Inference:  21%|██▏       | 107/500 [07:59<23:54,  3.65s/it]Running Inference:  22%|██▏       | 108/500 [08:00<19:24,  2.97s/it]Running Inference:  22%|██▏       | 109/500 [08:05<22:25,  3.44s/it]Running Inference:  22%|██▏       | 110/500 [08:11<27:28,  4.23s/it]Running Inference:  22%|██▏       | 111/500 [08:13<22:54,  3.53s/it]Running Inference:  22%|██▏       | 112/500 [08:17<24:37,  3.81s/it]Running Inference:  23%|██▎       | 113/500 [08:22<25:17,  3.92s/it]Running Inference:  23%|██▎       | 114/500 [08:26<26:10,  4.07s/it]Running Inference:  23%|██▎       | 115/500 [08:31<27:18,  4.26s/it]Running Inference:  23%|██▎       | 116/500 [08:35<27:21,  4.27s/it]Running Inference:  23%|██▎       | 117/500 [08:40<28:14,  4.42s/it]Running Inference:  24%|██▎       | 118/500 [08:45<29:16,  4.60s/it]Running Inference:  24%|██▍       | 119/500 [08:49<29:00,  4.57s/it]Running Inference:  24%|██▍       | 120/500 [08:53<28:03,  4.43s/it]Running Inference:  24%|██▍       | 121/500 [08:56<24:42,  3.91s/it]Running Inference:  24%|██▍       | 122/500 [09:00<24:32,  3.89s/it]Running Inference:  25%|██▍       | 123/500 [09:01<18:46,  2.99s/it]Running Inference:  25%|██▍       | 124/500 [09:05<20:43,  3.31s/it]Running Inference:  25%|██▌       | 125/500 [09:09<23:00,  3.68s/it]Running Inference:  25%|██▌       | 126/500 [09:11<19:35,  3.14s/it]Running Inference:  25%|██▌       | 127/500 [09:13<15:58,  2.57s/it]Running Inference:  26%|██▌       | 128/500 [09:16<18:32,  2.99s/it]Running Inference:  26%|██▌       | 129/500 [09:24<26:32,  4.29s/it]Running Inference:  26%|██▌       | 130/500 [09:31<31:39,  5.13s/it]Running Inference:  26%|██▌       | 131/500 [09:33<25:38,  4.17s/it]Running Inference:  26%|██▋       | 132/500 [09:37<25:17,  4.12s/it]Running Inference:  27%|██▋       | 133/500 [09:41<25:21,  4.15s/it]Running Inference:  27%|██▋       | 134/500 [09:46<26:36,  4.36s/it]Running Inference:  27%|██▋       | 135/500 [09:55<34:31,  5.67s/it]Running Inference:  27%|██▋       | 136/500 [10:01<35:40,  5.88s/it]Running Inference:  27%|██▋       | 137/500 [10:06<34:40,  5.73s/it]Running Inference:  28%|██▊       | 138/500 [10:10<30:30,  5.06s/it]Running Inference:  28%|██▊       | 139/500 [10:16<32:29,  5.40s/it]Running Inference:  28%|██▊       | 140/500 [10:21<32:03,  5.34s/it]Running Inference:  28%|██▊       | 141/500 [10:23<24:49,  4.15s/it]Running Inference:  28%|██▊       | 142/500 [10:23<18:28,  3.10s/it]Running Inference:  29%|██▊       | 143/500 [10:32<28:44,  4.83s/it]Running Inference:  29%|██▉       | 144/500 [10:36<27:15,  4.59s/it]Running Inference:  29%|██▉       | 145/500 [10:38<21:35,  3.65s/it]Running Inference:  29%|██▉       | 146/500 [10:42<22:12,  3.76s/it]Running Inference:  29%|██▉       | 147/500 [10:42<16:56,  2.88s/it]Running Inference:  30%|██▉       | 148/500 [10:48<21:26,  3.65s/it]Running Inference:  30%|██▉       | 149/500 [10:54<24:48,  4.24s/it]Running Inference:  30%|███       | 150/500 [10:59<27:00,  4.63s/it]Running Inference:  30%|███       | 151/500 [11:03<26:13,  4.51s/it]Running Inference:  30%|███       | 152/500 [11:07<25:18,  4.36s/it]Running Inference:  31%|███       | 153/500 [11:12<25:14,  4.36s/it]Running Inference:  31%|███       | 154/500 [11:17<27:14,  4.72s/it]Running Inference:  31%|███       | 155/500 [11:22<26:58,  4.69s/it]Running Inference:  31%|███       | 156/500 [11:28<28:33,  4.98s/it]Running Inference:  31%|███▏      | 157/500 [11:32<27:00,  4.72s/it]Running Inference:  32%|███▏      | 158/500 [11:36<26:34,  4.66s/it]Running Inference:  32%|███▏      | 159/500 [11:41<27:33,  4.85s/it]Running Inference:  32%|███▏      | 160/500 [11:46<27:11,  4.80s/it]Running Inference:  32%|███▏      | 161/500 [11:51<27:40,  4.90s/it]Running Inference:  32%|███▏      | 162/500 [11:56<27:15,  4.84s/it]Running Inference:  33%|███▎      | 163/500 [12:00<26:31,  4.72s/it]Running Inference:  33%|███▎      | 164/500 [12:03<23:22,  4.17s/it]Running Inference:  33%|███▎      | 165/500 [12:09<25:21,  4.54s/it]Running Inference:  33%|███▎      | 166/500 [12:14<26:15,  4.72s/it]Running Inference:  33%|███▎      | 167/500 [12:18<25:15,  4.55s/it]Running Inference:  34%|███▎      | 168/500 [12:21<21:55,  3.96s/it]Running Inference:  34%|███▍      | 169/500 [12:26<24:59,  4.53s/it]Running Inference:  34%|███▍      | 170/500 [12:31<24:15,  4.41s/it]Running Inference:  34%|███▍      | 171/500 [12:35<23:45,  4.33s/it]Running Inference:  34%|███▍      | 172/500 [12:39<23:20,  4.27s/it]Running Inference:  35%|███▍      | 173/500 [12:43<23:01,  4.22s/it]Running Inference:  35%|███▍      | 174/500 [12:47<22:42,  4.18s/it]Running Inference:  35%|███▌      | 175/500 [12:52<24:15,  4.48s/it]Running Inference:  35%|███▌      | 176/500 [12:57<25:22,  4.70s/it]Running Inference:  35%|███▌      | 177/500 [13:03<26:06,  4.85s/it]Running Inference:  36%|███▌      | 178/500 [13:08<27:24,  5.11s/it]Running Inference:  36%|███▌      | 179/500 [13:13<25:45,  4.82s/it]Running Inference:  36%|███▌      | 180/500 [13:17<24:52,  4.67s/it]Running Inference:  36%|███▌      | 181/500 [13:21<24:12,  4.55s/it]Running Inference:  36%|███▋      | 182/500 [13:25<23:50,  4.50s/it]Running Inference:  37%|███▋      | 183/500 [13:30<23:33,  4.46s/it]Running Inference:  37%|███▋      | 184/500 [13:34<22:48,  4.33s/it]Running Inference:  37%|███▋      | 185/500 [13:38<22:29,  4.28s/it]Running Inference:  37%|███▋      | 186/500 [13:43<23:52,  4.56s/it]Running Inference:  37%|███▋      | 187/500 [13:48<23:44,  4.55s/it]Running Inference:  38%|███▊      | 188/500 [13:49<18:23,  3.54s/it]Running Inference:  38%|███▊      | 189/500 [13:54<20:17,  3.92s/it]Running Inference:  38%|███▊      | 190/500 [14:02<27:06,  5.25s/it]Running Inference:  38%|███▊      | 191/500 [14:07<25:59,  5.05s/it]Running Inference:  38%|███▊      | 192/500 [14:11<25:22,  4.94s/it]Running Inference:  39%|███▊      | 193/500 [14:16<24:35,  4.81s/it]Running Inference:  39%|███▉      | 194/500 [14:17<19:30,  3.82s/it]Running Inference:  39%|███▉      | 195/500 [14:22<21:04,  4.15s/it]Running Inference:  39%|███▉      | 196/500 [14:26<20:54,  4.13s/it]Running Inference:  39%|███▉      | 197/500 [14:30<20:46,  4.11s/it]Running Inference:  40%|███▉      | 198/500 [14:32<17:24,  3.46s/it]Running Inference:  40%|███▉      | 199/500 [14:36<18:15,  3.64s/it]Running Inference:  40%|████      | 200/500 [14:41<18:56,  3.79s/it]Running Inference:  40%|████      | 201/500 [14:44<18:29,  3.71s/it]Running Inference:  40%|████      | 202/500 [14:50<21:49,  4.39s/it]Running Inference:  41%|████      | 203/500 [14:55<21:52,  4.42s/it]Running Inference:  41%|████      | 204/500 [14:59<21:47,  4.42s/it]Running Inference:  41%|████      | 205/500 [15:03<21:42,  4.41s/it]Running Inference:  41%|████      | 206/500 [15:08<22:26,  4.58s/it]Running Inference:  41%|████▏     | 207/500 [15:13<22:07,  4.53s/it]Running Inference:  42%|████▏     | 208/500 [15:17<21:10,  4.35s/it]Running Inference:  42%|████▏     | 209/500 [15:18<16:55,  3.49s/it]Running Inference:  42%|████▏     | 210/500 [15:22<17:36,  3.64s/it]Running Inference:  42%|████▏     | 211/500 [15:28<20:00,  4.15s/it]Running Inference:  42%|████▏     | 212/500 [15:32<20:09,  4.20s/it]Running Inference:  43%|████▎     | 213/500 [15:37<20:55,  4.38s/it]Running Inference:  43%|████▎     | 214/500 [15:41<20:07,  4.22s/it]Running Inference:  43%|████▎     | 215/500 [15:45<19:47,  4.17s/it]Running Inference:  43%|████▎     | 216/500 [15:49<19:31,  4.12s/it]Running Inference:  43%|████▎     | 217/500 [15:53<19:25,  4.12s/it]Running Inference:  44%|████▎     | 218/500 [15:56<17:37,  3.75s/it]Running Inference:  44%|████▍     | 219/500 [16:00<18:02,  3.85s/it]Running Inference:  44%|████▍     | 220/500 [16:04<18:18,  3.92s/it]Running Inference:  44%|████▍     | 221/500 [16:08<18:27,  3.97s/it]Running Inference:  44%|████▍     | 222/500 [16:12<18:37,  4.02s/it]Running Inference:  45%|████▍     | 223/500 [16:17<19:17,  4.18s/it]Running Inference:  45%|████▍     | 224/500 [16:21<19:57,  4.34s/it]Running Inference:  45%|████▌     | 225/500 [16:26<19:59,  4.36s/it]Running Inference:  45%|████▌     | 226/500 [16:32<22:03,  4.83s/it]Running Inference:  45%|████▌     | 227/500 [16:38<23:36,  5.19s/it]Running Inference:  46%|████▌     | 228/500 [16:40<20:15,  4.47s/it]Running Inference:  46%|████▌     | 229/500 [16:47<22:32,  4.99s/it]Running Inference:  46%|████▌     | 230/500 [16:49<18:55,  4.20s/it]Running Inference:  46%|████▌     | 231/500 [16:51<16:35,  3.70s/it]Running Inference:  46%|████▋     | 232/500 [16:55<15:53,  3.56s/it]Running Inference:  47%|████▋     | 233/500 [16:59<16:13,  3.65s/it]Running Inference:  47%|████▋     | 234/500 [17:02<16:28,  3.72s/it]Running Inference:  47%|████▋     | 235/500 [17:07<18:08,  4.11s/it]Running Inference:  47%|████▋     | 236/500 [17:09<15:15,  3.47s/it]Running Inference:  47%|████▋     | 237/500 [17:14<16:06,  3.67s/it]Running Inference:  48%|████▊     | 238/500 [17:19<18:33,  4.25s/it]Running Inference:  48%|████▊     | 239/500 [17:23<18:19,  4.21s/it]Running Inference:  48%|████▊     | 240/500 [17:28<19:17,  4.45s/it]Running Inference:  48%|████▊     | 241/500 [17:33<19:45,  4.58s/it]Running Inference:  48%|████▊     | 242/500 [17:39<20:55,  4.87s/it]Running Inference:  49%|████▊     | 243/500 [17:44<21:03,  4.92s/it]Running Inference:  49%|████▉     | 244/500 [17:47<18:10,  4.26s/it]Running Inference:  49%|████▉     | 245/500 [17:52<19:45,  4.65s/it]Running Inference:  49%|████▉     | 246/500 [17:54<15:54,  3.76s/it]Running Inference:  49%|████▉     | 247/500 [17:58<16:26,  3.90s/it]Running Inference:  50%|████▉     | 248/500 [18:03<17:34,  4.18s/it]Running Inference:  50%|████▉     | 249/500 [18:07<17:49,  4.26s/it]Running Inference:  50%|█████     | 250/500 [18:11<17:21,  4.16s/it]Running Inference:  50%|█████     | 251/500 [18:17<19:24,  4.68s/it]Running Inference:  50%|█████     | 252/500 [18:22<20:12,  4.89s/it]Running Inference:  51%|█████     | 253/500 [18:26<18:01,  4.38s/it]Running Inference:  51%|█████     | 254/500 [18:27<14:48,  3.61s/it]Running Inference:  51%|█████     | 255/500 [18:30<13:27,  3.30s/it]Running Inference:  51%|█████     | 256/500 [18:36<16:13,  3.99s/it]Running Inference:  51%|█████▏    | 257/500 [18:40<16:42,  4.13s/it]Running Inference:  52%|█████▏    | 258/500 [18:43<14:56,  3.70s/it]Running Inference:  52%|█████▏    | 259/500 [18:46<14:11,  3.53s/it]Running Inference:  52%|█████▏    | 260/500 [18:50<14:38,  3.66s/it]Running Inference:  52%|█████▏    | 261/500 [18:54<14:39,  3.68s/it]Running Inference:  52%|█████▏    | 262/500 [18:57<13:40,  3.45s/it]Running Inference:  53%|█████▎    | 263/500 [19:01<14:18,  3.62s/it]Running Inference:  53%|█████▎    | 264/500 [19:05<15:33,  3.95s/it]Running Inference:  53%|█████▎    | 265/500 [19:10<16:07,  4.12s/it]Running Inference:  53%|█████▎    | 266/500 [19:14<16:32,  4.24s/it]Running Inference:  53%|█████▎    | 267/500 [19:22<20:06,  5.18s/it]Running Inference:  54%|█████▎    | 268/500 [19:26<18:51,  4.88s/it]Running Inference:  54%|█████▍    | 269/500 [19:34<22:17,  5.79s/it]Running Inference:  54%|█████▍    | 270/500 [19:42<25:02,  6.53s/it]Running Inference:  54%|█████▍    | 271/500 [19:48<24:43,  6.48s/it]Running Inference:  54%|█████▍    | 272/500 [19:53<22:08,  5.83s/it]Running Inference:  55%|█████▍    | 273/500 [19:55<18:21,  4.85s/it]Running Inference:  55%|█████▍    | 274/500 [20:01<19:16,  5.12s/it]Running Inference:  55%|█████▌    | 275/500 [20:05<18:08,  4.84s/it]Running Inference:  55%|█████▌    | 276/500 [20:08<15:45,  4.22s/it]Running Inference:  55%|█████▌    | 277/500 [20:13<16:22,  4.41s/it]Running Inference:  56%|█████▌    | 278/500 [20:15<14:09,  3.83s/it]Running Inference:  56%|█████▌    | 279/500 [20:19<14:24,  3.91s/it]Running Inference:  56%|█████▌    | 280/500 [20:22<12:37,  3.44s/it]Running Inference:  56%|█████▌    | 281/500 [20:26<13:27,  3.69s/it]Running Inference:  56%|█████▋    | 282/500 [20:31<15:14,  4.19s/it]Running Inference:  57%|█████▋    | 283/500 [20:36<16:08,  4.46s/it]Running Inference:  57%|█████▋    | 284/500 [20:42<16:44,  4.65s/it]Running Inference:  57%|█████▋    | 285/500 [20:46<16:11,  4.52s/it]Running Inference:  57%|█████▋    | 286/500 [20:50<15:48,  4.43s/it]Running Inference:  57%|█████▋    | 287/500 [20:57<18:04,  5.09s/it]Running Inference:  58%|█████▊    | 288/500 [21:01<16:45,  4.74s/it]Running Inference:  58%|█████▊    | 289/500 [21:04<15:49,  4.50s/it]Running Inference:  58%|█████▊    | 290/500 [21:09<16:06,  4.60s/it]Running Inference:  58%|█████▊    | 291/500 [21:15<17:12,  4.94s/it]Running Inference:  58%|█████▊    | 292/500 [21:19<16:28,  4.75s/it]Running Inference:  59%|█████▊    | 293/500 [21:23<15:33,  4.51s/it]Running Inference:  59%|█████▉    | 294/500 [21:28<15:20,  4.47s/it]Running Inference:  59%|█████▉    | 295/500 [21:33<15:41,  4.59s/it]Running Inference:  59%|█████▉    | 296/500 [21:35<13:22,  3.93s/it]Running Inference:  59%|█████▉    | 297/500 [21:39<13:40,  4.04s/it]Running Inference:  60%|█████▉    | 298/500 [21:44<13:49,  4.10s/it]Running Inference:  60%|█████▉    | 299/500 [21:48<13:56,  4.16s/it]Running Inference:  60%|██████    | 300/500 [21:51<12:26,  3.73s/it]Running Inference:  60%|██████    | 301/500 [21:52<10:13,  3.08s/it]Running Inference:  60%|██████    | 302/500 [21:56<11:05,  3.36s/it]Running Inference:  61%|██████    | 303/500 [22:00<11:55,  3.63s/it]Running Inference:  61%|██████    | 304/500 [22:05<12:30,  3.83s/it]Running Inference:  61%|██████    | 305/500 [22:09<12:33,  3.86s/it]Running Inference:  61%|██████    | 306/500 [22:13<13:15,  4.10s/it]Running Inference:  61%|██████▏   | 307/500 [22:17<13:09,  4.09s/it]Running Inference:  62%|██████▏   | 308/500 [22:21<12:49,  4.01s/it]Running Inference:  62%|██████▏   | 309/500 [22:25<12:51,  4.04s/it]Running Inference:  62%|██████▏   | 310/500 [22:30<13:17,  4.20s/it]Running Inference:  62%|██████▏   | 311/500 [22:34<13:00,  4.13s/it]Running Inference:  62%|██████▏   | 312/500 [22:38<12:57,  4.14s/it]Running Inference:  63%|██████▎   | 313/500 [22:43<13:30,  4.34s/it]Running Inference:  63%|██████▎   | 314/500 [22:47<13:11,  4.26s/it]Running Inference:  63%|██████▎   | 315/500 [22:52<13:51,  4.50s/it]Running Inference:  63%|██████▎   | 316/500 [22:54<11:51,  3.87s/it]Running Inference:  63%|██████▎   | 317/500 [22:59<13:00,  4.26s/it]Running Inference:  64%|██████▎   | 318/500 [23:05<13:48,  4.55s/it]Running Inference:  64%|██████▍   | 319/500 [23:07<11:47,  3.91s/it]Running Inference:  64%|██████▍   | 320/500 [23:11<11:50,  3.95s/it]Running Inference:  64%|██████▍   | 321/500 [23:15<12:07,  4.06s/it]Running Inference:  64%|██████▍   | 322/500 [23:19<11:57,  4.03s/it]Running Inference:  65%|██████▍   | 323/500 [23:24<12:05,  4.10s/it]Running Inference:  65%|██████▍   | 324/500 [23:29<13:12,  4.50s/it]Running Inference:  65%|██████▌   | 325/500 [23:35<14:27,  4.96s/it]Running Inference:  65%|██████▌   | 326/500 [23:36<10:58,  3.79s/it]Running Inference:  65%|██████▌   | 327/500 [23:40<10:34,  3.67s/it]Running Inference:  66%|██████▌   | 328/500 [23:44<10:51,  3.79s/it]Running Inference:  66%|██████▌   | 329/500 [23:49<11:57,  4.19s/it]Running Inference:  66%|██████▌   | 330/500 [23:51<10:19,  3.65s/it]Running Inference:  66%|██████▌   | 331/500 [23:55<10:34,  3.75s/it]Running Inference:  66%|██████▋   | 332/500 [23:57<08:56,  3.19s/it]Running Inference:  67%|██████▋   | 333/500 [24:01<09:40,  3.48s/it]Running Inference:  67%|██████▋   | 334/500 [24:02<07:37,  2.75s/it]Running Inference:  67%|██████▋   | 335/500 [24:11<12:41,  4.61s/it]Running Inference:  67%|██████▋   | 336/500 [24:18<14:26,  5.28s/it]Running Inference:  67%|██████▋   | 337/500 [24:23<13:57,  5.14s/it]Running Inference:  68%|██████▊   | 338/500 [24:27<12:57,  4.80s/it]Running Inference:  68%|██████▊   | 339/500 [24:28<10:06,  3.77s/it]Running Inference:  68%|██████▊   | 340/500 [24:34<11:27,  4.29s/it]Running Inference:  68%|██████▊   | 341/500 [24:36<09:41,  3.66s/it]Running Inference:  68%|██████▊   | 342/500 [24:41<10:36,  4.03s/it]Running Inference:  69%|██████▊   | 343/500 [24:45<10:26,  3.99s/it]Running Inference:  69%|██████▉   | 344/500 [24:49<10:35,  4.07s/it]Running Inference:  69%|██████▉   | 345/500 [24:54<11:15,  4.36s/it]Running Inference:  69%|██████▉   | 346/500 [24:58<10:54,  4.25s/it]Running Inference:  69%|██████▉   | 347/500 [25:02<10:38,  4.17s/it]Running Inference:  70%|██████▉   | 348/500 [25:06<10:21,  4.09s/it]Running Inference:  70%|██████▉   | 349/500 [25:11<11:08,  4.43s/it]Running Inference:  70%|███████   | 350/500 [25:16<11:40,  4.67s/it]Running Inference:  70%|███████   | 351/500 [25:22<12:00,  4.83s/it]Running Inference:  70%|███████   | 352/500 [25:27<12:12,  4.95s/it]Running Inference:  71%|███████   | 353/500 [25:34<14:03,  5.74s/it]Running Inference:  71%|███████   | 354/500 [25:38<12:07,  4.99s/it]Running Inference:  71%|███████   | 355/500 [25:42<11:39,  4.83s/it]Running Inference:  71%|███████   | 356/500 [25:46<10:54,  4.55s/it]Running Inference:  71%|███████▏  | 357/500 [25:50<10:49,  4.54s/it]Running Inference:  72%|███████▏  | 358/500 [25:55<10:24,  4.40s/it]Running Inference:  72%|███████▏  | 359/500 [26:00<10:53,  4.64s/it]Running Inference:  72%|███████▏  | 360/500 [26:01<08:48,  3.78s/it]Running Inference:  72%|███████▏  | 361/500 [26:06<09:03,  3.91s/it]Running Inference:  72%|███████▏  | 362/500 [26:10<09:25,  4.10s/it]Running Inference:  73%|███████▎  | 363/500 [26:16<10:14,  4.49s/it]Running Inference:  73%|███████▎  | 364/500 [26:20<10:07,  4.47s/it]Running Inference:  73%|███████▎  | 365/500 [26:24<10:00,  4.45s/it]Running Inference:  73%|███████▎  | 366/500 [26:28<09:31,  4.26s/it]Running Inference:  73%|███████▎  | 367/500 [26:32<09:10,  4.14s/it]Running Inference:  74%|███████▎  | 368/500 [26:36<08:59,  4.09s/it]Running Inference:  74%|███████▍  | 369/500 [26:40<08:53,  4.07s/it]Running Inference:  74%|███████▍  | 370/500 [26:44<08:55,  4.12s/it]Running Inference:  74%|███████▍  | 371/500 [26:49<09:24,  4.38s/it]Running Inference:  74%|███████▍  | 372/500 [26:54<09:41,  4.54s/it]Running Inference:  75%|███████▍  | 373/500 [26:59<09:25,  4.45s/it]Running Inference:  75%|███████▍  | 374/500 [27:03<09:24,  4.48s/it]Running Inference:  75%|███████▌  | 375/500 [27:09<10:07,  4.86s/it]Running Inference:  75%|███████▌  | 376/500 [27:13<09:39,  4.68s/it]Running Inference:  75%|███████▌  | 377/500 [27:16<08:19,  4.06s/it]Running Inference:  76%|███████▌  | 378/500 [27:26<11:51,  5.84s/it]Running Inference:  76%|███████▌  | 379/500 [27:30<11:02,  5.47s/it]Running Inference:  76%|███████▌  | 380/500 [27:31<08:16,  4.14s/it]Running Inference:  76%|███████▌  | 381/500 [27:35<08:04,  4.07s/it]Running Inference:  76%|███████▋  | 382/500 [27:40<08:36,  4.38s/it]Running Inference:  77%|███████▋  | 383/500 [27:44<08:14,  4.22s/it]Running Inference:  77%|███████▋  | 384/500 [27:46<06:45,  3.50s/it]Running Inference:  77%|███████▋  | 385/500 [27:50<06:59,  3.64s/it]Running Inference:  77%|███████▋  | 386/500 [27:54<07:24,  3.90s/it]Running Inference:  77%|███████▋  | 387/500 [27:59<07:49,  4.16s/it]Running Inference:  78%|███████▊  | 388/500 [28:05<08:33,  4.58s/it]Running Inference:  78%|███████▊  | 389/500 [28:12<10:09,  5.49s/it]Running Inference:  78%|███████▊  | 390/500 [28:21<11:45,  6.41s/it]Running Inference:  78%|███████▊  | 391/500 [28:28<11:55,  6.56s/it]Running Inference:  78%|███████▊  | 392/500 [28:32<10:29,  5.83s/it]Running Inference:  79%|███████▊  | 393/500 [28:37<09:58,  5.60s/it]Running Inference:  79%|███████▉  | 394/500 [28:42<09:36,  5.44s/it]Running Inference:  79%|███████▉  | 395/500 [28:47<08:57,  5.12s/it]Running Inference:  79%|███████▉  | 396/500 [28:52<09:09,  5.28s/it]Running Inference:  79%|███████▉  | 397/500 [28:58<09:12,  5.37s/it]Running Inference:  80%|███████▉  | 398/500 [29:02<08:38,  5.09s/it]Running Inference:  80%|███████▉  | 399/500 [29:04<06:44,  4.01s/it]Running Inference:  80%|████████  | 400/500 [29:04<05:03,  3.04s/it]Running Inference:  80%|████████  | 401/500 [29:08<05:25,  3.29s/it]Running Inference:  80%|████████  | 402/500 [29:12<05:40,  3.47s/it]Running Inference:  81%|████████  | 403/500 [29:16<05:49,  3.60s/it]Running Inference:  81%|████████  | 404/500 [29:21<06:10,  3.86s/it]Running Inference:  81%|████████  | 405/500 [29:27<07:24,  4.68s/it]Running Inference:  81%|████████  | 406/500 [29:34<08:15,  5.27s/it]Running Inference:  81%|████████▏ | 407/500 [29:39<07:58,  5.14s/it]Running Inference:  82%|████████▏ | 408/500 [29:43<07:27,  4.87s/it]Running Inference:  82%|████████▏ | 409/500 [29:48<07:38,  5.04s/it]Running Inference:  82%|████████▏ | 410/500 [29:53<07:19,  4.89s/it]Running Inference:  82%|████████▏ | 411/500 [29:57<06:55,  4.67s/it]Running Inference:  82%|████████▏ | 412/500 [30:02<06:57,  4.74s/it]Running Inference:  83%|████████▎ | 413/500 [30:07<07:05,  4.89s/it]Running Inference:  83%|████████▎ | 414/500 [30:12<06:49,  4.77s/it]Running Inference:  83%|████████▎ | 415/500 [30:17<06:55,  4.89s/it]Running Inference:  83%|████████▎ | 416/500 [30:21<06:31,  4.66s/it]Running Inference:  83%|████████▎ | 417/500 [30:25<06:08,  4.44s/it]Running Inference:  84%|████████▎ | 418/500 [30:29<05:51,  4.28s/it]Running Inference:  84%|████████▍ | 419/500 [30:33<05:41,  4.22s/it]Running Inference:  84%|████████▍ | 420/500 [30:37<05:35,  4.19s/it]Running Inference:  84%|████████▍ | 421/500 [30:39<04:27,  3.39s/it]Running Inference:  84%|████████▍ | 422/500 [30:40<03:48,  2.93s/it]Running Inference:  85%|████████▍ | 423/500 [30:47<05:05,  3.96s/it]Running Inference:  85%|████████▍ | 424/500 [30:52<05:21,  4.23s/it]Running Inference:  85%|████████▌ | 425/500 [30:56<05:30,  4.41s/it]Running Inference:  85%|████████▌ | 426/500 [31:01<05:26,  4.41s/it]Running Inference:  85%|████████▌ | 427/500 [31:07<05:59,  4.92s/it]Running Inference:  86%|████████▌ | 428/500 [31:11<05:43,  4.77s/it]Running Inference:  86%|████████▌ | 429/500 [31:16<05:37,  4.75s/it]Running Inference:  86%|████████▌ | 430/500 [31:21<05:28,  4.70s/it]Running Inference:  86%|████████▌ | 431/500 [31:25<05:06,  4.45s/it]Running Inference:  86%|████████▋ | 432/500 [31:31<05:34,  4.92s/it]Running Inference:  87%|████████▋ | 433/500 [31:37<05:53,  5.27s/it]Running Inference:  87%|████████▋ | 434/500 [31:37<04:18,  3.91s/it]Running Inference:  87%|████████▋ | 435/500 [31:39<03:26,  3.18s/it]Running Inference:  87%|████████▋ | 436/500 [31:40<02:37,  2.46s/it]Running Inference:  87%|████████▋ | 437/500 [31:44<03:10,  3.02s/it]Running Inference:  88%|████████▊ | 438/500 [31:49<03:37,  3.50s/it]Running Inference:  88%|████████▊ | 439/500 [31:56<04:42,  4.64s/it]Running Inference:  88%|████████▊ | 440/500 [32:01<04:39,  4.66s/it]Running Inference:  88%|████████▊ | 441/500 [32:06<04:42,  4.78s/it]Running Inference:  88%|████████▊ | 442/500 [32:10<04:36,  4.76s/it]Running Inference:  89%|████████▊ | 443/500 [32:16<04:39,  4.91s/it]Running Inference:  89%|████████▉ | 444/500 [32:20<04:26,  4.76s/it]Running Inference:  89%|████████▉ | 445/500 [32:25<04:26,  4.85s/it]Running Inference:  89%|████████▉ | 446/500 [32:32<04:48,  5.33s/it]Running Inference:  89%|████████▉ | 447/500 [32:37<04:44,  5.37s/it]Running Inference:  90%|████████▉ | 448/500 [32:42<04:38,  5.35s/it]Running Inference:  90%|████████▉ | 449/500 [32:45<03:53,  4.58s/it]Running Inference:  90%|█████████ | 450/500 [32:49<03:41,  4.44s/it]Running Inference:  90%|█████████ | 451/500 [32:51<03:05,  3.78s/it]Running Inference:  90%|█████████ | 452/500 [32:54<02:39,  3.33s/it]Running Inference:  91%|█████████ | 453/500 [32:59<03:00,  3.83s/it]Running Inference:  91%|█████████ | 454/500 [33:03<03:04,  4.01s/it]Running Inference:  91%|█████████ | 455/500 [33:08<03:17,  4.38s/it]Running Inference:  91%|█████████ | 456/500 [33:13<03:13,  4.40s/it]Running Inference:  91%|█████████▏| 457/500 [33:14<02:23,  3.35s/it]Running Inference:  92%|█████████▏| 458/500 [33:19<02:47,  3.98s/it]Running Inference:  92%|█████████▏| 459/500 [33:25<03:01,  4.42s/it]Running Inference:  92%|█████████▏| 460/500 [33:30<03:10,  4.75s/it]Running Inference:  92%|█████████▏| 461/500 [33:35<03:04,  4.73s/it]Running Inference:  92%|█████████▏| 462/500 [33:40<03:05,  4.89s/it]Running Inference:  93%|█████████▎| 463/500 [33:45<02:57,  4.79s/it]Running Inference:  93%|█████████▎| 464/500 [33:49<02:50,  4.74s/it]Running Inference:  93%|█████████▎| 465/500 [33:54<02:41,  4.60s/it]Running Inference:  93%|█████████▎| 466/500 [33:58<02:33,  4.52s/it]Running Inference:  93%|█████████▎| 467/500 [34:02<02:26,  4.43s/it]Running Inference:  94%|█████████▎| 468/500 [34:06<02:16,  4.28s/it]Running Inference:  94%|█████████▍| 469/500 [34:10<02:08,  4.16s/it]Running Inference:  94%|█████████▍| 470/500 [34:14<02:03,  4.13s/it]Running Inference:  94%|█████████▍| 471/500 [34:18<01:59,  4.13s/it]Running Inference:  94%|█████████▍| 472/500 [34:22<01:55,  4.12s/it]Running Inference:  95%|█████████▍| 473/500 [34:27<02:00,  4.45s/it]Running Inference:  95%|█████████▍| 474/500 [34:29<01:37,  3.74s/it]Running Inference:  95%|█████████▌| 475/500 [34:35<01:43,  4.12s/it]Running Inference:  95%|█████████▌| 476/500 [34:38<01:37,  4.07s/it]Running Inference:  95%|█████████▌| 477/500 [34:42<01:32,  4.02s/it]Running Inference:  96%|█████████▌| 478/500 [34:48<01:41,  4.60s/it]Running Inference:  96%|█████████▌| 479/500 [34:56<01:53,  5.42s/it]Running Inference:  96%|█████████▌| 480/500 [35:00<01:40,  5.01s/it]Running Inference:  96%|█████████▌| 481/500 [35:05<01:35,  5.02s/it]Running Inference:  96%|█████████▋| 482/500 [35:09<01:27,  4.86s/it]Running Inference:  97%|█████████▋| 483/500 [35:14<01:20,  4.71s/it]Running Inference:  97%|█████████▋| 484/500 [35:18<01:11,  4.47s/it]Running Inference:  97%|█████████▋| 485/500 [35:22<01:08,  4.55s/it]Running Inference:  97%|█████████▋| 486/500 [35:27<01:02,  4.47s/it]Running Inference:  97%|█████████▋| 487/500 [35:31<00:58,  4.53s/it]Running Inference:  98%|█████████▊| 488/500 [35:35<00:52,  4.39s/it]Running Inference:  98%|█████████▊| 489/500 [35:40<00:51,  4.64s/it]Running Inference:  98%|█████████▊| 490/500 [35:43<00:40,  4.03s/it]Running Inference:  98%|█████████▊| 491/500 [35:48<00:38,  4.29s/it]Running Inference:  98%|█████████▊| 492/500 [35:54<00:37,  4.72s/it]Running Inference:  99%|█████████▊| 493/500 [35:58<00:31,  4.57s/it]Running Inference:  99%|█████████▉| 494/500 [36:02<00:27,  4.54s/it]Running Inference:  99%|█████████▉| 495/500 [36:07<00:22,  4.52s/it]Running Inference:  99%|█████████▉| 496/500 [36:12<00:18,  4.59s/it]Running Inference:  99%|█████████▉| 497/500 [36:16<00:13,  4.63s/it]Running Inference: 100%|█████████▉| 498/500 [36:21<00:09,  4.56s/it]Running Inference: 100%|█████████▉| 499/500 [36:25<00:04,  4.49s/it]Running Inference: 100%|██████████| 500/500 [36:29<00:00,  4.39s/it]Running Inference: 100%|██████████| 500/500 [36:29<00:00,  4.38s/it]
2025-12-15 00:17:17,074 - INFO - Inference completed.
2025-12-15 00:17:17,126 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-15 00:17:17,126 - INFO - Calculating metrics for dataset: longbench
2025-12-15 00:17:17,128 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-15 00:17:17,128 - INFO - Metrics:
10.73
2025-12-15 00:17:17,129 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=repobench-p, ratio=0.3) on GPU 1

----------------------------------------
Task: repobench-p | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 00:17:23,482 - INFO - Set deterministic seeds to 42
2025-12-15 00:17:23,482 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 00:17:23,482 - INFO - Starting evaluation run...
2025-12-15 00:17:23,482 - INFO - Output directory set to: longbenchresult
2025-12-15 00:17:23,482 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-15 00:17:23,482 - INFO - KV Press 'tova' setup.
2025-12-15 00:17:23,482 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 00:17:23,482 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.65it/s]
Device set to use cuda:0
2025-12-15 00:17:37,583 - INFO - Model pipeline loaded.
2025-12-15 00:17:37,584 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
2025-12-15 00:17:42,187 - INFO - Dataset loaded with 500 entries.
2025-12-15 00:17:42,187 - INFO - Dataset processed with 500 entries.
2025-12-15 00:17:42,232 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<43:09,  5.19s/it]Running Inference:   0%|          | 2/500 [00:11<47:49,  5.76s/it]Running Inference:   1%|          | 3/500 [00:14<37:36,  4.54s/it]Running Inference:   1%|          | 4/500 [00:18<36:04,  4.36s/it]Running Inference:   1%|          | 5/500 [00:23<36:33,  4.43s/it]Running Inference:   1%|          | 6/500 [00:27<35:50,  4.35s/it]Running Inference:   1%|▏         | 7/500 [00:28<27:10,  3.31s/it]Running Inference:   2%|▏         | 8/500 [00:32<28:58,  3.53s/it]Running Inference:   2%|▏         | 9/500 [00:36<30:15,  3.70s/it]Running Inference:   2%|▏         | 10/500 [00:43<39:17,  4.81s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:49<42:29,  5.21s/it]Running Inference:   2%|▏         | 12/500 [00:54<40:23,  4.97s/it]Running Inference:   3%|▎         | 13/500 [00:59<41:12,  5.08s/it]Running Inference:   3%|▎         | 14/500 [01:09<52:25,  6.47s/it]Running Inference:   3%|▎         | 15/500 [01:14<48:04,  5.95s/it]Running Inference:   3%|▎         | 16/500 [01:18<43:58,  5.45s/it]Running Inference:   3%|▎         | 17/500 [01:23<43:55,  5.46s/it]Running Inference:   4%|▎         | 18/500 [01:29<44:31,  5.54s/it]Running Inference:   4%|▍         | 19/500 [01:33<41:13,  5.14s/it]Running Inference:   4%|▍         | 20/500 [01:40<45:11,  5.65s/it]Running Inference:   4%|▍         | 21/500 [01:44<41:16,  5.17s/it]Running Inference:   4%|▍         | 22/500 [01:47<34:53,  4.38s/it]Running Inference:   5%|▍         | 23/500 [01:50<31:04,  3.91s/it]Running Inference:   5%|▍         | 24/500 [01:56<35:54,  4.53s/it]Running Inference:   5%|▌         | 25/500 [02:01<39:02,  4.93s/it]Running Inference:   5%|▌         | 26/500 [02:06<38:24,  4.86s/it]Running Inference:   5%|▌         | 27/500 [02:10<36:28,  4.63s/it]Running Inference:   6%|▌         | 28/500 [02:14<34:39,  4.41s/it]Running Inference:   6%|▌         | 29/500 [02:18<33:44,  4.30s/it]Running Inference:   6%|▌         | 30/500 [02:23<35:54,  4.58s/it]Running Inference:   6%|▌         | 31/500 [02:29<37:20,  4.78s/it]Running Inference:   6%|▋         | 32/500 [02:33<36:07,  4.63s/it]Running Inference:   7%|▋         | 33/500 [02:34<27:46,  3.57s/it]Running Inference:   7%|▋         | 34/500 [02:38<29:30,  3.80s/it]Running Inference:   7%|▋         | 35/500 [02:43<30:54,  3.99s/it]Running Inference:   7%|▋         | 36/500 [02:45<25:53,  3.35s/it]Running Inference:   7%|▋         | 37/500 [02:49<28:57,  3.75s/it]Running Inference:   8%|▊         | 38/500 [02:54<30:35,  3.97s/it]Running Inference:   8%|▊         | 39/500 [02:58<30:52,  4.02s/it]Running Inference:   8%|▊         | 40/500 [03:02<31:05,  4.05s/it]Running Inference:   8%|▊         | 41/500 [03:03<24:05,  3.15s/it]Running Inference:   8%|▊         | 42/500 [03:05<21:40,  2.84s/it]Running Inference:   9%|▊         | 43/500 [03:09<24:11,  3.18s/it]Running Inference:   9%|▉         | 44/500 [03:15<30:36,  4.03s/it]Running Inference:   9%|▉         | 45/500 [03:20<33:24,  4.40s/it]Running Inference:   9%|▉         | 46/500 [03:26<35:21,  4.67s/it]Running Inference:   9%|▉         | 47/500 [03:31<36:44,  4.87s/it]Running Inference:  10%|▉         | 48/500 [03:35<35:32,  4.72s/it]Running Inference:  10%|▉         | 49/500 [03:40<35:07,  4.67s/it]Running Inference:  10%|█         | 50/500 [03:43<32:12,  4.29s/it]Running Inference:  10%|█         | 51/500 [03:48<33:42,  4.51s/it]Running Inference:  10%|█         | 52/500 [03:53<33:45,  4.52s/it]Running Inference:  11%|█         | 53/500 [03:57<32:50,  4.41s/it]Running Inference:  11%|█         | 54/500 [04:01<32:14,  4.34s/it]Running Inference:  11%|█         | 55/500 [04:08<36:25,  4.91s/it]Running Inference:  11%|█         | 56/500 [04:12<35:38,  4.82s/it]Running Inference:  11%|█▏        | 57/500 [04:16<33:26,  4.53s/it]Running Inference:  12%|█▏        | 58/500 [04:20<32:19,  4.39s/it]Running Inference:  12%|█▏        | 59/500 [04:24<31:09,  4.24s/it]Running Inference:  12%|█▏        | 60/500 [04:28<30:47,  4.20s/it]Running Inference:  12%|█▏        | 61/500 [04:32<30:08,  4.12s/it]Running Inference:  12%|█▏        | 62/500 [04:38<33:44,  4.62s/it]Running Inference:  13%|█▎        | 63/500 [04:44<36:11,  4.97s/it]Running Inference:  13%|█▎        | 64/500 [04:48<34:57,  4.81s/it]Running Inference:  13%|█▎        | 65/500 [04:54<37:30,  5.17s/it]Running Inference:  13%|█▎        | 66/500 [04:58<34:26,  4.76s/it]Running Inference:  13%|█▎        | 67/500 [05:02<32:19,  4.48s/it]Running Inference:  14%|█▎        | 68/500 [05:07<34:15,  4.76s/it]Running Inference:  14%|█▍        | 69/500 [05:12<35:14,  4.91s/it]Running Inference:  14%|█▍        | 70/500 [05:19<39:50,  5.56s/it]Running Inference:  14%|█▍        | 71/500 [05:24<38:24,  5.37s/it]Running Inference:  14%|█▍        | 72/500 [05:29<37:08,  5.21s/it]Running Inference:  15%|█▍        | 73/500 [05:35<37:32,  5.28s/it]Running Inference:  15%|█▍        | 74/500 [05:39<35:52,  5.05s/it]Running Inference:  15%|█▌        | 75/500 [05:44<35:26,  5.00s/it]Running Inference:  15%|█▌        | 76/500 [05:49<35:33,  5.03s/it]Running Inference:  15%|█▌        | 77/500 [05:54<34:15,  4.86s/it]Running Inference:  16%|█▌        | 78/500 [06:00<37:29,  5.33s/it]Running Inference:  16%|█▌        | 79/500 [06:04<34:32,  4.92s/it]Running Inference:  16%|█▌        | 80/500 [06:08<33:19,  4.76s/it]Running Inference:  16%|█▌        | 81/500 [06:14<34:39,  4.96s/it]Running Inference:  16%|█▋        | 82/500 [06:19<35:12,  5.05s/it]Running Inference:  17%|█▋        | 83/500 [06:27<40:40,  5.85s/it]Running Inference:  17%|█▋        | 84/500 [06:28<31:31,  4.55s/it]Running Inference:  17%|█▋        | 85/500 [06:30<25:14,  3.65s/it]Running Inference:  17%|█▋        | 86/500 [06:35<27:34,  4.00s/it]Running Inference:  17%|█▋        | 87/500 [06:38<25:56,  3.77s/it]Running Inference:  18%|█▊        | 88/500 [06:39<21:16,  3.10s/it]Running Inference:  18%|█▊        | 89/500 [06:44<23:24,  3.42s/it]Running Inference:  18%|█▊        | 90/500 [06:48<24:53,  3.64s/it]Running Inference:  18%|█▊        | 91/500 [06:52<26:00,  3.82s/it]Running Inference:  18%|█▊        | 92/500 [06:57<27:40,  4.07s/it]Running Inference:  19%|█▊        | 93/500 [07:02<29:33,  4.36s/it]Running Inference:  19%|█▉        | 94/500 [07:06<29:00,  4.29s/it]Running Inference:  19%|█▉        | 95/500 [07:13<34:58,  5.18s/it]Running Inference:  19%|█▉        | 96/500 [07:18<34:00,  5.05s/it]Running Inference:  19%|█▉        | 97/500 [07:22<31:51,  4.74s/it]Running Inference:  20%|█▉        | 98/500 [07:26<30:45,  4.59s/it]Running Inference:  20%|█▉        | 99/500 [07:28<26:06,  3.91s/it]Running Inference:  20%|██        | 100/500 [07:30<21:30,  3.23s/it]Running Inference:  20%|██        | 101/500 [07:34<23:03,  3.47s/it]Running Inference:  20%|██        | 102/500 [07:36<19:26,  2.93s/it]Running Inference:  21%|██        | 103/500 [07:40<21:45,  3.29s/it]Running Inference:  21%|██        | 104/500 [07:45<25:20,  3.84s/it]Running Inference:  21%|██        | 105/500 [07:46<20:15,  3.08s/it]Running Inference:  21%|██        | 106/500 [07:50<22:19,  3.40s/it]Running Inference:  21%|██▏       | 107/500 [07:54<23:20,  3.56s/it]Running Inference:  22%|██▏       | 108/500 [07:56<18:58,  2.91s/it]Running Inference:  22%|██▏       | 109/500 [08:00<22:18,  3.42s/it]Running Inference:  22%|██▏       | 110/500 [08:06<27:17,  4.20s/it]Running Inference:  22%|██▏       | 111/500 [08:11<28:33,  4.41s/it]Running Inference:  22%|██▏       | 112/500 [08:16<28:23,  4.39s/it]Running Inference:  23%|██▎       | 113/500 [08:20<27:46,  4.31s/it]Running Inference:  23%|██▎       | 114/500 [08:24<27:45,  4.32s/it]Running Inference:  23%|██▎       | 115/500 [08:29<28:16,  4.41s/it]Running Inference:  23%|██▎       | 116/500 [08:33<27:50,  4.35s/it]Running Inference:  23%|██▎       | 117/500 [08:38<28:25,  4.45s/it]Running Inference:  24%|██▎       | 118/500 [08:43<29:21,  4.61s/it]Running Inference:  24%|██▍       | 119/500 [08:47<29:00,  4.57s/it]Running Inference:  24%|██▍       | 120/500 [08:51<27:59,  4.42s/it]Running Inference:  24%|██▍       | 121/500 [08:54<24:38,  3.90s/it]Running Inference:  24%|██▍       | 122/500 [08:58<24:26,  3.88s/it]Running Inference:  25%|██▍       | 123/500 [08:58<18:41,  2.97s/it]Running Inference:  25%|██▍       | 124/500 [09:02<20:37,  3.29s/it]Running Inference:  25%|██▌       | 125/500 [09:06<20:24,  3.27s/it]Running Inference:  25%|██▌       | 126/500 [09:08<19:13,  3.08s/it]Running Inference:  25%|██▌       | 127/500 [09:10<15:36,  2.51s/it]Running Inference:  26%|██▌       | 128/500 [09:13<18:11,  2.93s/it]Running Inference:  26%|██▌       | 129/500 [09:21<25:55,  4.19s/it]Running Inference:  26%|██▌       | 130/500 [09:27<30:51,  5.00s/it]Running Inference:  26%|██▌       | 131/500 [09:32<29:49,  4.85s/it]Running Inference:  26%|██▋       | 132/500 [09:36<28:08,  4.59s/it]Running Inference:  27%|██▋       | 133/500 [09:40<27:22,  4.48s/it]Running Inference:  27%|██▋       | 134/500 [09:45<27:59,  4.59s/it]Running Inference:  27%|██▋       | 135/500 [09:54<35:18,  5.80s/it]Running Inference:  27%|██▋       | 136/500 [10:00<36:06,  5.95s/it]Running Inference:  27%|██▋       | 137/500 [10:05<34:52,  5.76s/it]Running Inference:  28%|██▊       | 138/500 [10:09<30:34,  5.07s/it]Running Inference:  28%|██▊       | 139/500 [10:15<32:27,  5.40s/it]Running Inference:  28%|██▊       | 140/500 [10:20<31:58,  5.33s/it]Running Inference:  28%|██▊       | 141/500 [10:21<24:44,  4.14s/it]Running Inference:  28%|██▊       | 142/500 [10:22<18:24,  3.09s/it]Running Inference:  29%|██▊       | 143/500 [10:31<28:20,  4.76s/it]Running Inference:  29%|██▉       | 144/500 [10:35<26:56,  4.54s/it]Running Inference:  29%|██▉       | 145/500 [10:38<24:48,  4.19s/it]Running Inference:  29%|██▉       | 146/500 [10:42<24:25,  4.14s/it]Running Inference:  29%|██▉       | 147/500 [10:43<18:29,  3.14s/it]Running Inference:  30%|██▉       | 148/500 [10:48<22:23,  3.82s/it]Running Inference:  30%|██▉       | 149/500 [10:54<25:18,  4.33s/it]Running Inference:  30%|███       | 150/500 [10:59<27:12,  4.66s/it]Running Inference:  30%|███       | 151/500 [11:03<26:14,  4.51s/it]Running Inference:  30%|███       | 152/500 [11:07<25:12,  4.35s/it]Running Inference:  31%|███       | 153/500 [11:12<25:04,  4.34s/it]Running Inference:  31%|███       | 154/500 [11:17<26:58,  4.68s/it]Running Inference:  31%|███       | 155/500 [11:22<26:39,  4.64s/it]Running Inference:  31%|███       | 156/500 [11:27<28:10,  4.92s/it]Running Inference:  31%|███▏      | 157/500 [11:31<26:36,  4.66s/it]Running Inference:  32%|███▏      | 158/500 [11:36<26:09,  4.59s/it]Running Inference:  32%|███▏      | 159/500 [11:41<27:05,  4.77s/it]Running Inference:  32%|███▏      | 160/500 [11:46<26:48,  4.73s/it]Running Inference:  32%|███▏      | 161/500 [11:51<27:19,  4.84s/it]Running Inference:  32%|███▏      | 162/500 [11:55<27:01,  4.80s/it]Running Inference:  33%|███▎      | 163/500 [12:00<26:20,  4.69s/it]Running Inference:  33%|███▎      | 164/500 [12:03<23:11,  4.14s/it]Running Inference:  33%|███▎      | 165/500 [12:08<25:09,  4.51s/it]Running Inference:  33%|███▎      | 166/500 [12:13<26:03,  4.68s/it]Running Inference:  33%|███▎      | 167/500 [12:17<25:02,  4.51s/it]Running Inference:  34%|███▎      | 168/500 [12:20<21:30,  3.89s/it]Running Inference:  34%|███▍      | 169/500 [12:26<24:39,  4.47s/it]Running Inference:  34%|███▍      | 170/500 [12:30<24:00,  4.37s/it]Running Inference:  34%|███▍      | 171/500 [12:34<23:33,  4.30s/it]Running Inference:  34%|███▍      | 172/500 [12:38<23:07,  4.23s/it]Running Inference:  35%|███▍      | 173/500 [12:42<22:48,  4.19s/it]Running Inference:  35%|███▍      | 174/500 [12:46<22:30,  4.14s/it]Running Inference:  35%|███▌      | 175/500 [12:51<24:03,  4.44s/it]Running Inference:  35%|███▌      | 176/500 [12:56<25:10,  4.66s/it]Running Inference:  35%|███▌      | 177/500 [13:01<25:54,  4.81s/it]Running Inference:  36%|███▌      | 178/500 [13:07<27:13,  5.07s/it]Running Inference:  36%|███▌      | 179/500 [13:11<25:34,  4.78s/it]Running Inference:  36%|███▌      | 180/500 [13:16<24:42,  4.63s/it]Running Inference:  36%|███▌      | 181/500 [13:20<24:03,  4.53s/it]Running Inference:  36%|███▋      | 182/500 [13:24<23:40,  4.47s/it]Running Inference:  37%|███▋      | 183/500 [13:28<23:21,  4.42s/it]Running Inference:  37%|███▋      | 184/500 [13:32<22:35,  4.29s/it]Running Inference:  37%|███▋      | 185/500 [13:37<22:15,  4.24s/it]Running Inference:  37%|███▋      | 186/500 [13:42<23:37,  4.52s/it]Running Inference:  37%|███▋      | 187/500 [13:46<23:28,  4.50s/it]Running Inference:  38%|███▊      | 188/500 [13:47<18:09,  3.49s/it]Running Inference:  38%|███▊      | 189/500 [13:52<19:56,  3.85s/it]Running Inference:  38%|███▊      | 190/500 [14:00<26:25,  5.12s/it]Running Inference:  38%|███▊      | 191/500 [14:04<25:11,  4.89s/it]Running Inference:  38%|███▊      | 192/500 [14:09<24:44,  4.82s/it]Running Inference:  39%|███▊      | 193/500 [14:14<24:04,  4.71s/it]Running Inference:  39%|███▉      | 194/500 [14:18<23:41,  4.65s/it]Running Inference:  39%|███▉      | 195/500 [14:23<24:00,  4.72s/it]Running Inference:  39%|███▉      | 196/500 [14:27<22:57,  4.53s/it]Running Inference:  39%|███▉      | 197/500 [14:31<22:11,  4.40s/it]Running Inference:  40%|███▉      | 198/500 [14:35<21:41,  4.31s/it]Running Inference:  40%|███▉      | 199/500 [14:39<21:13,  4.23s/it]Running Inference:  40%|████      | 200/500 [14:43<20:59,  4.20s/it]Running Inference:  40%|████      | 201/500 [14:45<17:13,  3.46s/it]Running Inference:  40%|████      | 202/500 [14:51<20:58,  4.22s/it]Running Inference:  41%|████      | 203/500 [14:56<21:17,  4.30s/it]Running Inference:  41%|████      | 204/500 [15:00<21:21,  4.33s/it]Running Inference:  41%|████      | 205/500 [15:04<21:23,  4.35s/it]Running Inference:  41%|████      | 206/500 [15:09<22:11,  4.53s/it]Running Inference:  41%|████▏     | 207/500 [15:14<22:01,  4.51s/it]Running Inference:  42%|████▏     | 208/500 [15:18<21:07,  4.34s/it]Running Inference:  42%|████▏     | 209/500 [15:19<16:52,  3.48s/it]Running Inference:  42%|████▏     | 210/500 [15:22<15:44,  3.26s/it]Running Inference:  42%|████▏     | 211/500 [15:27<18:40,  3.88s/it]Running Inference:  42%|████▏     | 212/500 [15:32<19:12,  4.00s/it]Running Inference:  43%|████▎     | 213/500 [15:36<20:14,  4.23s/it]Running Inference:  43%|████▎     | 214/500 [15:40<19:37,  4.12s/it]Running Inference:  43%|████▎     | 215/500 [15:41<15:13,  3.21s/it]Running Inference:  43%|████▎     | 216/500 [15:45<16:18,  3.45s/it]Running Inference:  43%|████▎     | 217/500 [15:49<17:11,  3.65s/it]Running Inference:  44%|████▎     | 218/500 [15:54<17:46,  3.78s/it]Running Inference:  44%|████▍     | 219/500 [15:58<18:07,  3.87s/it]Running Inference:  44%|████▍     | 220/500 [16:02<18:20,  3.93s/it]Running Inference:  44%|████▍     | 221/500 [16:06<18:28,  3.97s/it]Running Inference:  44%|████▍     | 222/500 [16:10<18:37,  4.02s/it]Running Inference:  45%|████▍     | 223/500 [16:14<19:15,  4.17s/it]Running Inference:  45%|████▍     | 224/500 [16:19<19:55,  4.33s/it]Running Inference:  45%|████▌     | 225/500 [16:23<19:56,  4.35s/it]Running Inference:  45%|████▌     | 226/500 [16:29<22:00,  4.82s/it]Running Inference:  45%|████▌     | 227/500 [16:35<23:35,  5.19s/it]Running Inference:  46%|████▌     | 228/500 [16:38<20:12,  4.46s/it]Running Inference:  46%|████▌     | 229/500 [16:44<22:28,  4.98s/it]Running Inference:  46%|████▌     | 230/500 [16:50<23:20,  5.19s/it]Running Inference:  46%|████▌     | 231/500 [16:53<19:38,  4.38s/it]Running Inference:  46%|████▋     | 232/500 [16:56<17:58,  4.02s/it]Running Inference:  47%|████▋     | 233/500 [17:00<17:41,  3.97s/it]Running Inference:  47%|████▋     | 234/500 [17:01<13:37,  3.07s/it]Running Inference:  47%|████▋     | 235/500 [17:06<16:07,  3.65s/it]Running Inference:  47%|████▋     | 236/500 [17:08<13:50,  3.15s/it]Running Inference:  47%|████▋     | 237/500 [17:12<15:06,  3.45s/it]Running Inference:  48%|████▊     | 238/500 [17:17<17:50,  4.09s/it]Running Inference:  48%|████▊     | 239/500 [17:21<17:47,  4.09s/it]Running Inference:  48%|████▊     | 240/500 [17:26<18:54,  4.36s/it]Running Inference:  48%|████▊     | 241/500 [17:31<19:28,  4.51s/it]Running Inference:  48%|████▊     | 242/500 [17:37<20:43,  4.82s/it]Running Inference:  49%|████▊     | 243/500 [17:42<20:51,  4.87s/it]Running Inference:  49%|████▉     | 244/500 [17:44<18:00,  4.22s/it]Running Inference:  49%|████▉     | 245/500 [17:50<19:36,  4.62s/it]Running Inference:  49%|████▉     | 246/500 [17:52<15:48,  3.73s/it]Running Inference:  49%|████▉     | 247/500 [17:56<16:21,  3.88s/it]Running Inference:  50%|████▉     | 248/500 [17:58<13:50,  3.30s/it]Running Inference:  50%|████▉     | 249/500 [18:02<15:12,  3.64s/it]Running Inference:  50%|█████     | 250/500 [18:06<15:31,  3.73s/it]Running Inference:  50%|█████     | 251/500 [18:12<18:07,  4.37s/it]Running Inference:  50%|█████     | 252/500 [18:17<19:17,  4.67s/it]Running Inference:  51%|█████     | 253/500 [18:21<17:33,  4.26s/it]Running Inference:  51%|█████     | 254/500 [18:23<14:27,  3.53s/it]Running Inference:  51%|█████     | 255/500 [18:27<15:42,  3.84s/it]Running Inference:  51%|█████     | 256/500 [18:33<17:47,  4.37s/it]Running Inference:  51%|█████▏    | 257/500 [18:37<17:47,  4.39s/it]Running Inference:  52%|█████▏    | 258/500 [18:40<15:25,  3.82s/it]Running Inference:  52%|█████▏    | 259/500 [18:43<14:57,  3.73s/it]Running Inference:  52%|█████▏    | 260/500 [18:47<15:11,  3.80s/it]Running Inference:  52%|█████▏    | 261/500 [18:51<15:10,  3.81s/it]Running Inference:  52%|█████▏    | 262/500 [18:55<15:27,  3.90s/it]Running Inference:  53%|█████▎    | 263/500 [18:59<15:31,  3.93s/it]Running Inference:  53%|█████▎    | 264/500 [19:04<16:23,  4.17s/it]Running Inference:  53%|█████▎    | 265/500 [19:08<16:41,  4.26s/it]Running Inference:  53%|█████▎    | 266/500 [19:13<16:55,  4.34s/it]Running Inference:  53%|█████▎    | 267/500 [19:20<19:42,  5.07s/it]Running Inference:  54%|█████▎    | 268/500 [19:24<18:33,  4.80s/it]Running Inference:  54%|█████▍    | 269/500 [19:31<21:49,  5.67s/it]Running Inference:  54%|█████▍    | 270/500 [19:40<24:31,  6.40s/it]Running Inference:  54%|█████▍    | 271/500 [19:46<24:18,  6.37s/it]Running Inference:  54%|█████▍    | 272/500 [19:50<21:50,  5.75s/it]Running Inference:  55%|█████▍    | 273/500 [19:53<18:06,  4.79s/it]Running Inference:  55%|█████▍    | 274/500 [19:55<15:28,  4.11s/it]Running Inference:  55%|█████▌    | 275/500 [19:58<13:37,  3.63s/it]Running Inference:  55%|█████▌    | 276/500 [20:01<12:58,  3.47s/it]Running Inference:  55%|█████▌    | 277/500 [20:06<14:22,  3.87s/it]Running Inference:  56%|█████▌    | 278/500 [20:08<12:45,  3.45s/it]Running Inference:  56%|█████▌    | 279/500 [20:11<11:43,  3.18s/it]Running Inference:  56%|█████▌    | 280/500 [20:13<10:42,  2.92s/it]Running Inference:  56%|█████▌    | 281/500 [20:17<12:07,  3.32s/it]Running Inference:  56%|█████▋    | 282/500 [20:21<12:02,  3.32s/it]Running Inference:  57%|█████▋    | 283/500 [20:26<13:53,  3.84s/it]Running Inference:  57%|█████▋    | 284/500 [20:31<15:10,  4.21s/it]Running Inference:  57%|█████▋    | 285/500 [20:34<14:16,  3.99s/it]Running Inference:  57%|█████▋    | 286/500 [20:38<14:29,  4.06s/it]Running Inference:  57%|█████▋    | 287/500 [20:45<17:04,  4.81s/it]Running Inference:  58%|█████▊    | 288/500 [20:49<16:03,  4.54s/it]Running Inference:  58%|█████▊    | 289/500 [20:53<15:19,  4.36s/it]Running Inference:  58%|█████▊    | 290/500 [20:58<15:43,  4.49s/it]Running Inference:  58%|█████▊    | 291/500 [21:03<16:55,  4.86s/it]Running Inference:  58%|█████▊    | 292/500 [21:08<16:15,  4.69s/it]Running Inference:  59%|█████▊    | 293/500 [21:12<15:24,  4.47s/it]Running Inference:  59%|█████▉    | 294/500 [21:16<15:13,  4.44s/it]Running Inference:  59%|█████▉    | 295/500 [21:21<15:35,  4.57s/it]Running Inference:  59%|█████▉    | 296/500 [21:23<13:16,  3.91s/it]Running Inference:  59%|█████▉    | 297/500 [21:27<13:37,  4.03s/it]Running Inference:  60%|█████▉    | 298/500 [21:32<13:45,  4.09s/it]Running Inference:  60%|█████▉    | 299/500 [21:36<13:53,  4.15s/it]Running Inference:  60%|██████    | 300/500 [21:39<12:18,  3.69s/it]Running Inference:  60%|██████    | 301/500 [21:43<13:15,  4.00s/it]Running Inference:  60%|██████    | 302/500 [21:47<13:11,  4.00s/it]Running Inference:  61%|██████    | 303/500 [21:52<13:23,  4.08s/it]Running Inference:  61%|██████    | 304/500 [21:56<13:30,  4.14s/it]Running Inference:  61%|██████    | 305/500 [21:57<10:40,  3.28s/it]Running Inference:  61%|██████    | 306/500 [22:02<11:56,  3.69s/it]Running Inference:  61%|██████▏   | 307/500 [22:04<10:18,  3.20s/it]Running Inference:  62%|██████▏   | 308/500 [22:08<10:59,  3.43s/it]Running Inference:  62%|██████▏   | 309/500 [22:12<11:33,  3.63s/it]Running Inference:  62%|██████▏   | 310/500 [22:16<12:23,  3.91s/it]Running Inference:  62%|██████▏   | 311/500 [22:20<12:22,  3.93s/it]Running Inference:  62%|██████▏   | 312/500 [22:22<10:18,  3.29s/it]Running Inference:  63%|██████▎   | 313/500 [22:27<11:39,  3.74s/it]Running Inference:  63%|██████▎   | 314/500 [22:31<11:54,  3.84s/it]Running Inference:  63%|██████▎   | 315/500 [22:36<12:56,  4.20s/it]Running Inference:  63%|██████▎   | 316/500 [22:41<13:41,  4.46s/it]Running Inference:  63%|██████▎   | 317/500 [22:46<14:15,  4.68s/it]Running Inference:  64%|██████▎   | 318/500 [22:49<12:08,  4.00s/it]Running Inference:  64%|██████▍   | 319/500 [22:51<10:37,  3.52s/it]Running Inference:  64%|██████▍   | 320/500 [22:55<11:00,  3.67s/it]Running Inference:  64%|██████▍   | 321/500 [23:00<11:31,  3.86s/it]Running Inference:  64%|██████▍   | 322/500 [23:04<11:32,  3.89s/it]Running Inference:  65%|██████▍   | 323/500 [23:05<09:11,  3.11s/it]Running Inference:  65%|██████▍   | 324/500 [23:10<11:09,  3.80s/it]Running Inference:  65%|██████▌   | 325/500 [23:16<13:00,  4.46s/it]Running Inference:  65%|██████▌   | 326/500 [23:17<10:00,  3.45s/it]Running Inference:  65%|██████▌   | 327/500 [23:22<11:20,  3.93s/it]Running Inference:  66%|██████▌   | 328/500 [23:26<11:22,  3.97s/it]Running Inference:  66%|██████▌   | 329/500 [23:32<12:18,  4.32s/it]Running Inference:  66%|██████▌   | 330/500 [23:34<10:34,  3.73s/it]Running Inference:  66%|██████▌   | 331/500 [23:38<10:43,  3.81s/it]Running Inference:  66%|██████▋   | 332/500 [23:40<09:02,  3.23s/it]Running Inference:  67%|██████▋   | 333/500 [23:44<09:45,  3.51s/it]Running Inference:  67%|██████▋   | 334/500 [23:45<07:40,  2.77s/it]Running Inference:  67%|██████▋   | 335/500 [23:54<12:33,  4.57s/it]Running Inference:  67%|██████▋   | 336/500 [24:00<13:28,  4.93s/it]Running Inference:  67%|██████▋   | 337/500 [24:04<13:16,  4.89s/it]Running Inference:  68%|██████▊   | 338/500 [24:08<12:27,  4.62s/it]Running Inference:  68%|██████▊   | 339/500 [24:10<09:45,  3.63s/it]Running Inference:  68%|██████▊   | 340/500 [24:15<11:10,  4.19s/it]Running Inference:  68%|██████▊   | 341/500 [24:17<09:29,  3.58s/it]Running Inference:  68%|██████▊   | 342/500 [24:22<10:27,  3.97s/it]Running Inference:  69%|██████▊   | 343/500 [24:26<10:19,  3.95s/it]Running Inference:  69%|██████▉   | 344/500 [24:30<10:30,  4.04s/it]Running Inference:  69%|██████▉   | 345/500 [24:35<11:11,  4.33s/it]Running Inference:  69%|██████▉   | 346/500 [24:39<10:51,  4.23s/it]Running Inference:  69%|██████▉   | 347/500 [24:43<10:35,  4.15s/it]Running Inference:  70%|██████▉   | 348/500 [24:47<10:19,  4.08s/it]Running Inference:  70%|██████▉   | 349/500 [24:52<11:06,  4.41s/it]Running Inference:  70%|███████   | 350/500 [24:58<11:37,  4.65s/it]Running Inference:  70%|███████   | 351/500 [25:03<11:59,  4.83s/it]Running Inference:  70%|███████   | 352/500 [25:08<12:17,  4.99s/it]Running Inference:  71%|███████   | 353/500 [25:16<14:01,  5.72s/it]Running Inference:  71%|███████   | 354/500 [25:19<11:59,  4.93s/it]Running Inference:  71%|███████   | 355/500 [25:24<12:23,  5.12s/it]Running Inference:  71%|███████   | 356/500 [25:28<11:25,  4.76s/it]Running Inference:  71%|███████▏  | 357/500 [25:33<11:10,  4.69s/it]Running Inference:  72%|███████▏  | 358/500 [25:37<10:39,  4.50s/it]Running Inference:  72%|███████▏  | 359/500 [25:40<09:55,  4.22s/it]Running Inference:  72%|███████▏  | 360/500 [25:42<08:07,  3.48s/it]Running Inference:  72%|███████▏  | 361/500 [25:46<08:35,  3.71s/it]Running Inference:  72%|███████▏  | 362/500 [25:48<07:04,  3.07s/it]Running Inference:  73%|███████▎  | 363/500 [25:53<08:36,  3.77s/it]Running Inference:  73%|███████▎  | 364/500 [25:58<08:58,  3.96s/it]Running Inference:  73%|███████▎  | 365/500 [26:02<09:12,  4.09s/it]Running Inference:  73%|███████▎  | 366/500 [26:06<08:57,  4.01s/it]Running Inference:  73%|███████▎  | 367/500 [26:10<08:48,  3.97s/it]Running Inference:  74%|███████▎  | 368/500 [26:14<08:44,  3.97s/it]Running Inference:  74%|███████▍  | 369/500 [26:18<08:42,  3.99s/it]Running Inference:  74%|███████▍  | 370/500 [26:22<08:47,  4.06s/it]Running Inference:  74%|███████▍  | 371/500 [26:27<09:18,  4.33s/it]Running Inference:  74%|███████▍  | 372/500 [26:32<09:37,  4.51s/it]Running Inference:  75%|███████▍  | 373/500 [26:36<09:22,  4.43s/it]Running Inference:  75%|███████▍  | 374/500 [26:41<09:22,  4.46s/it]Running Inference:  75%|███████▌  | 375/500 [26:47<10:06,  4.85s/it]Running Inference:  75%|███████▌  | 376/500 [26:51<09:37,  4.66s/it]Running Inference:  75%|███████▌  | 377/500 [26:53<08:17,  4.05s/it]Running Inference:  76%|███████▌  | 378/500 [27:03<11:44,  5.77s/it]Running Inference:  76%|███████▌  | 379/500 [27:08<10:57,  5.43s/it]Running Inference:  76%|███████▌  | 380/500 [27:09<08:13,  4.11s/it]Running Inference:  76%|███████▌  | 381/500 [27:10<06:17,  3.17s/it]Running Inference:  76%|███████▋  | 382/500 [27:12<05:34,  2.84s/it]Running Inference:  77%|███████▋  | 383/500 [27:16<06:07,  3.14s/it]Running Inference:  77%|███████▋  | 384/500 [27:18<05:18,  2.75s/it]Running Inference:  77%|███████▋  | 385/500 [27:19<04:22,  2.29s/it]Running Inference:  77%|███████▋  | 386/500 [27:23<05:36,  2.95s/it]Running Inference:  77%|███████▋  | 387/500 [27:28<06:34,  3.49s/it]Running Inference:  78%|███████▊  | 388/500 [27:34<07:41,  4.12s/it]Running Inference:  78%|███████▊  | 389/500 [27:41<09:28,  5.12s/it]Running Inference:  78%|███████▊  | 390/500 [27:49<11:11,  6.11s/it]Running Inference:  78%|███████▊  | 391/500 [27:54<10:00,  5.51s/it]Running Inference:  78%|███████▊  | 392/500 [27:58<09:10,  5.09s/it]Running Inference:  79%|███████▊  | 393/500 [28:03<09:03,  5.08s/it]Running Inference:  79%|███████▉  | 394/500 [28:08<08:58,  5.08s/it]Running Inference:  79%|███████▉  | 395/500 [28:12<08:31,  4.87s/it]Running Inference:  79%|███████▉  | 396/500 [28:18<08:50,  5.10s/it]Running Inference:  79%|███████▉  | 397/500 [28:23<08:59,  5.24s/it]Running Inference:  80%|███████▉  | 398/500 [28:28<08:29,  4.99s/it]Running Inference:  80%|███████▉  | 399/500 [28:29<06:36,  3.93s/it]Running Inference:  80%|████████  | 400/500 [28:30<04:59,  2.99s/it]Running Inference:  80%|████████  | 401/500 [28:34<05:22,  3.26s/it]Running Inference:  80%|████████  | 402/500 [28:38<05:38,  3.45s/it]Running Inference:  81%|████████  | 403/500 [28:39<04:26,  2.75s/it]Running Inference:  81%|████████  | 404/500 [28:43<05:12,  3.26s/it]Running Inference:  81%|████████  | 405/500 [28:50<06:42,  4.23s/it]Running Inference:  81%|████████  | 406/500 [28:56<07:43,  4.93s/it]Running Inference:  81%|████████▏ | 407/500 [29:01<07:36,  4.90s/it]Running Inference:  82%|████████▏ | 408/500 [29:06<07:12,  4.70s/it]Running Inference:  82%|████████▏ | 409/500 [29:10<06:57,  4.59s/it]Running Inference:  82%|████████▏ | 410/500 [29:14<06:52,  4.58s/it]Running Inference:  82%|████████▏ | 411/500 [29:19<06:35,  4.45s/it]Running Inference:  82%|████████▏ | 412/500 [29:23<06:44,  4.59s/it]Running Inference:  83%|████████▎ | 413/500 [29:29<06:55,  4.78s/it]Running Inference:  83%|████████▎ | 414/500 [29:33<06:43,  4.69s/it]Running Inference:  83%|████████▎ | 415/500 [29:38<06:51,  4.84s/it]Running Inference:  83%|████████▎ | 416/500 [29:42<06:28,  4.62s/it]Running Inference:  83%|████████▎ | 417/500 [29:46<06:06,  4.41s/it]Running Inference:  84%|████████▎ | 418/500 [29:50<05:49,  4.26s/it]Running Inference:  84%|████████▍ | 419/500 [29:54<05:40,  4.20s/it]Running Inference:  84%|████████▍ | 420/500 [29:56<04:26,  3.33s/it]Running Inference:  84%|████████▍ | 421/500 [30:00<04:48,  3.65s/it]Running Inference:  84%|████████▍ | 422/500 [30:04<05:02,  3.88s/it]Running Inference:  85%|████████▍ | 423/500 [30:11<05:55,  4.61s/it]Running Inference:  85%|████████▍ | 424/500 [30:16<05:55,  4.68s/it]Running Inference:  85%|████████▌ | 425/500 [30:20<05:54,  4.73s/it]Running Inference:  85%|████████▌ | 426/500 [30:25<05:42,  4.63s/it]Running Inference:  85%|████████▌ | 427/500 [30:31<06:10,  5.08s/it]Running Inference:  86%|████████▌ | 428/500 [30:35<05:51,  4.88s/it]Running Inference:  86%|████████▌ | 429/500 [30:40<05:42,  4.83s/it]Running Inference:  86%|████████▌ | 430/500 [30:45<05:32,  4.75s/it]Running Inference:  86%|████████▌ | 431/500 [30:49<05:09,  4.48s/it]Running Inference:  86%|████████▋ | 432/500 [30:55<05:36,  4.95s/it]Running Inference:  87%|████████▋ | 433/500 [31:01<05:53,  5.28s/it]Running Inference:  87%|████████▋ | 434/500 [31:05<05:20,  4.86s/it]Running Inference:  87%|████████▋ | 435/500 [31:06<04:10,  3.85s/it]Running Inference:  87%|████████▋ | 436/500 [31:07<03:07,  2.93s/it]Running Inference:  87%|████████▋ | 437/500 [31:11<03:30,  3.34s/it]Running Inference:  88%|████████▊ | 438/500 [31:16<03:50,  3.72s/it]Running Inference:  88%|████████▊ | 439/500 [31:22<04:31,  4.45s/it]Running Inference:  88%|████████▊ | 440/500 [31:27<04:31,  4.52s/it]Running Inference:  88%|████████▊ | 441/500 [31:32<04:36,  4.68s/it]Running Inference:  88%|████████▊ | 442/500 [31:36<04:31,  4.69s/it]Running Inference:  89%|████████▊ | 443/500 [31:42<04:36,  4.85s/it]Running Inference:  89%|████████▉ | 444/500 [31:46<04:24,  4.72s/it]Running Inference:  89%|████████▉ | 445/500 [31:51<04:24,  4.81s/it]Running Inference:  89%|████████▉ | 446/500 [31:58<04:50,  5.37s/it]Running Inference:  89%|████████▉ | 447/500 [32:03<04:45,  5.39s/it]Running Inference:  90%|████████▉ | 448/500 [32:08<04:38,  5.36s/it]Running Inference:  90%|████████▉ | 449/500 [32:11<03:47,  4.47s/it]Running Inference:  90%|█████████ | 450/500 [32:13<03:11,  3.84s/it]Running Inference:  90%|█████████ | 451/500 [32:15<02:44,  3.35s/it]Running Inference:  90%|█████████ | 452/500 [32:21<03:09,  3.95s/it]Running Inference:  91%|█████████ | 453/500 [32:26<03:20,  4.26s/it]Running Inference:  91%|█████████ | 454/500 [32:30<03:18,  4.31s/it]Running Inference:  91%|█████████ | 455/500 [32:35<03:26,  4.59s/it]Running Inference:  91%|█████████ | 456/500 [32:40<03:19,  4.54s/it]Running Inference:  91%|█████████▏| 457/500 [32:44<03:08,  4.37s/it]Running Inference:  92%|█████████▏| 458/500 [32:49<03:17,  4.69s/it]Running Inference:  92%|█████████▏| 459/500 [32:55<03:21,  4.91s/it]Running Inference:  92%|█████████▏| 460/500 [33:00<03:23,  5.09s/it]Running Inference:  92%|█████████▏| 461/500 [33:05<03:13,  4.96s/it]Running Inference:  92%|█████████▏| 462/500 [33:07<02:37,  4.13s/it]Running Inference:  93%|█████████▎| 463/500 [33:12<02:37,  4.26s/it]Running Inference:  93%|█████████▎| 464/500 [33:16<02:37,  4.37s/it]Running Inference:  93%|█████████▎| 465/500 [33:20<02:31,  4.34s/it]Running Inference:  93%|█████████▎| 466/500 [33:25<02:27,  4.34s/it]Running Inference:  93%|█████████▎| 467/500 [33:29<02:21,  4.30s/it]Running Inference:  94%|█████████▎| 468/500 [33:33<02:14,  4.19s/it]Running Inference:  94%|█████████▍| 469/500 [33:37<02:06,  4.10s/it]Running Inference:  94%|█████████▍| 470/500 [33:41<02:02,  4.08s/it]Running Inference:  94%|█████████▍| 471/500 [33:45<01:58,  4.09s/it]Running Inference:  94%|█████████▍| 472/500 [33:49<01:54,  4.09s/it]Running Inference:  95%|█████████▍| 473/500 [33:54<01:59,  4.42s/it]Running Inference:  95%|█████████▍| 474/500 [33:56<01:36,  3.71s/it]Running Inference:  95%|█████████▌| 475/500 [34:01<01:42,  4.10s/it]Running Inference:  95%|█████████▌| 476/500 [34:05<01:37,  4.05s/it]Running Inference:  95%|█████████▌| 477/500 [34:09<01:32,  4.01s/it]Running Inference:  96%|█████████▌| 478/500 [34:15<01:40,  4.58s/it]Running Inference:  96%|█████████▌| 479/500 [34:22<01:52,  5.37s/it]Running Inference:  96%|█████████▌| 480/500 [34:26<01:39,  4.96s/it]Running Inference:  96%|█████████▌| 481/500 [34:31<01:34,  4.98s/it]Running Inference:  96%|█████████▋| 482/500 [34:36<01:26,  4.83s/it]Running Inference:  97%|█████████▋| 483/500 [34:40<01:19,  4.69s/it]Running Inference:  97%|█████████▋| 484/500 [34:44<01:11,  4.46s/it]Running Inference:  97%|█████████▋| 485/500 [34:49<01:07,  4.50s/it]Running Inference:  97%|█████████▋| 486/500 [34:53<01:01,  4.42s/it]Running Inference:  97%|█████████▋| 487/500 [34:58<00:58,  4.49s/it]Running Inference:  98%|█████████▊| 488/500 [35:02<00:52,  4.36s/it]Running Inference:  98%|█████████▊| 489/500 [35:07<00:50,  4.61s/it]Running Inference:  98%|█████████▊| 490/500 [35:09<00:38,  3.87s/it]Running Inference:  98%|█████████▊| 491/500 [35:14<00:37,  4.17s/it]Running Inference:  98%|█████████▊| 492/500 [35:20<00:37,  4.63s/it]Running Inference:  99%|█████████▊| 493/500 [35:24<00:31,  4.50s/it]Running Inference:  99%|█████████▉| 494/500 [35:28<00:26,  4.50s/it]Running Inference:  99%|█████████▉| 495/500 [35:33<00:22,  4.48s/it]Running Inference:  99%|█████████▉| 496/500 [35:37<00:18,  4.56s/it]Running Inference:  99%|█████████▉| 497/500 [35:42<00:13,  4.60s/it]Running Inference: 100%|█████████▉| 498/500 [35:46<00:09,  4.54s/it]Running Inference: 100%|█████████▉| 499/500 [35:51<00:04,  4.47s/it]Running Inference: 100%|██████████| 500/500 [35:55<00:00,  4.37s/it]Running Inference: 100%|██████████| 500/500 [35:55<00:00,  4.31s/it]
2025-12-15 00:53:37,727 - INFO - Inference completed.
2025-12-15 00:53:37,779 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-15 00:53:37,779 - INFO - Calculating metrics for dataset: longbench
2025-12-15 00:53:37,781 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-15 00:53:37,781 - INFO - Metrics:
11.08
2025-12-15 00:53:37,782 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=repobench-p, ratio=0.5) on GPU 1


========================================
LongBench Task: 2wikimqa_e
========================================
----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 00:53:44,180 - INFO - Set deterministic seeds to 42
2025-12-15 00:53:44,180 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 00:53:44,180 - INFO - Starting evaluation run...
2025-12-15 00:53:44,181 - INFO - Output directory set to: longbenchresult
2025-12-15 00:53:44,181 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-15 00:53:44,181 - INFO - KV Press 'tova' setup.
2025-12-15 00:53:44,181 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 00:53:44,181 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.82it/s]
Device set to use cuda:0
2025-12-15 00:53:58,127 - INFO - Model pipeline loaded.
2025-12-15 00:53:58,127 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 300 examples [00:00, 1350.19 examples/s]Generating test split: 300 examples [00:00, 1342.38 examples/s]
2025-12-15 00:54:02,968 - INFO - Dataset loaded with 300 entries.
2025-12-15 00:54:02,968 - INFO - Dataset processed with 300 entries.
2025-12-15 00:54:03,003 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:01<07:36,  1.53s/it]Running Inference:   1%|          | 2/300 [00:02<06:24,  1.29s/it]Running Inference:   1%|          | 3/300 [00:03<05:16,  1.07s/it]Running Inference:   1%|▏         | 4/300 [00:05<06:12,  1.26s/it]Running Inference:   2%|▏         | 5/300 [00:05<05:41,  1.16s/it]Running Inference:   2%|▏         | 6/300 [00:06<05:17,  1.08s/it]Running Inference:   2%|▏         | 7/300 [00:09<08:06,  1.66s/it]Running Inference:   3%|▎         | 8/300 [00:12<09:23,  1.93s/it]Running Inference:   3%|▎         | 9/300 [00:15<10:52,  2.24s/it]Running Inference:   3%|▎         | 10/300 [00:18<11:44,  2.43s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:20<12:13,  2.54s/it]Running Inference:   4%|▍         | 12/300 [00:23<12:52,  2.68s/it]Running Inference:   4%|▍         | 13/300 [00:24<10:30,  2.20s/it]Running Inference:   5%|▍         | 14/300 [00:27<11:20,  2.38s/it]Running Inference:   5%|▌         | 15/300 [00:30<11:31,  2.43s/it]Running Inference:   5%|▌         | 16/300 [00:32<11:23,  2.41s/it]Running Inference:   6%|▌         | 17/300 [00:36<13:11,  2.80s/it]Running Inference:   6%|▌         | 18/300 [00:37<10:15,  2.18s/it]Running Inference:   6%|▋         | 19/300 [00:39<09:53,  2.11s/it]Running Inference:   7%|▋         | 20/300 [00:39<07:30,  1.61s/it]Running Inference:   7%|▋         | 21/300 [00:41<08:07,  1.75s/it]Running Inference:   7%|▋         | 22/300 [00:43<08:12,  1.77s/it]Running Inference:   8%|▊         | 23/300 [00:46<10:04,  2.18s/it]Running Inference:   8%|▊         | 24/300 [00:49<11:10,  2.43s/it]Running Inference:   8%|▊         | 25/300 [00:51<11:07,  2.43s/it]Running Inference:   9%|▊         | 26/300 [00:53<09:15,  2.03s/it]Running Inference:   9%|▉         | 27/300 [00:53<07:43,  1.70s/it]Running Inference:   9%|▉         | 28/300 [00:55<07:11,  1.59s/it]Running Inference:  10%|▉         | 29/300 [00:56<06:42,  1.48s/it]Running Inference:  10%|█         | 30/300 [01:00<09:46,  2.17s/it]Running Inference:  10%|█         | 31/300 [01:01<09:01,  2.01s/it]Running Inference:  11%|█         | 32/300 [01:05<11:35,  2.60s/it]Running Inference:  11%|█         | 33/300 [01:10<13:43,  3.09s/it]Running Inference:  11%|█▏        | 34/300 [01:12<12:29,  2.82s/it]Running Inference:  12%|█▏        | 35/300 [01:12<09:17,  2.10s/it]Running Inference:  12%|█▏        | 36/300 [01:15<10:02,  2.28s/it]Running Inference:  12%|█▏        | 37/300 [01:17<09:43,  2.22s/it]Running Inference:  13%|█▎        | 38/300 [01:19<08:56,  2.05s/it]Running Inference:  13%|█▎        | 39/300 [01:20<07:19,  1.68s/it]Running Inference:  13%|█▎        | 40/300 [01:20<05:53,  1.36s/it]Running Inference:  14%|█▎        | 41/300 [01:24<08:41,  2.01s/it]Running Inference:  14%|█▍        | 42/300 [01:25<07:34,  1.76s/it]Running Inference:  14%|█▍        | 43/300 [01:27<07:55,  1.85s/it]Running Inference:  15%|█▍        | 44/300 [01:28<06:47,  1.59s/it]Running Inference:  15%|█▌        | 45/300 [01:31<08:28,  1.99s/it]Running Inference:  15%|█▌        | 46/300 [01:33<08:25,  1.99s/it]Running Inference:  16%|█▌        | 47/300 [01:33<06:33,  1.55s/it]Running Inference:  16%|█▌        | 48/300 [01:36<08:24,  2.00s/it]Running Inference:  16%|█▋        | 49/300 [01:39<08:54,  2.13s/it]Running Inference:  17%|█▋        | 50/300 [01:40<08:09,  1.96s/it]Running Inference:  17%|█▋        | 51/300 [01:42<07:18,  1.76s/it]Running Inference:  17%|█▋        | 52/300 [01:45<08:48,  2.13s/it]Running Inference:  18%|█▊        | 53/300 [01:46<07:23,  1.79s/it]Running Inference:  18%|█▊        | 54/300 [01:47<06:10,  1.51s/it]Running Inference:  18%|█▊        | 55/300 [01:48<06:13,  1.52s/it]Running Inference:  19%|█▊        | 56/300 [01:50<07:10,  1.76s/it]Running Inference:  19%|█▉        | 57/300 [01:52<06:43,  1.66s/it]Running Inference:  19%|█▉        | 58/300 [01:55<08:29,  2.10s/it]Running Inference:  20%|█▉        | 59/300 [01:56<06:45,  1.68s/it]Running Inference:  20%|██        | 60/300 [01:57<06:05,  1.52s/it]Running Inference:  20%|██        | 61/300 [01:58<05:28,  1.37s/it]Running Inference:  21%|██        | 62/300 [02:01<08:01,  2.02s/it]Running Inference:  21%|██        | 63/300 [02:04<08:54,  2.25s/it]Running Inference:  21%|██▏       | 64/300 [02:05<06:45,  1.72s/it]Running Inference:  22%|██▏       | 65/300 [02:09<09:24,  2.40s/it]Running Inference:  22%|██▏       | 66/300 [02:10<07:55,  2.03s/it]Running Inference:  22%|██▏       | 67/300 [02:13<08:55,  2.30s/it]Running Inference:  23%|██▎       | 68/300 [02:15<09:19,  2.41s/it]Running Inference:  23%|██▎       | 69/300 [02:17<08:17,  2.16s/it]Running Inference:  23%|██▎       | 70/300 [02:18<06:52,  1.80s/it]Running Inference:  24%|██▎       | 71/300 [02:19<06:04,  1.59s/it]Running Inference:  24%|██▍       | 72/300 [02:20<05:31,  1.45s/it]Running Inference:  24%|██▍       | 73/300 [02:21<05:16,  1.39s/it]Running Inference:  25%|██▍       | 74/300 [02:22<04:48,  1.28s/it]Running Inference:  25%|██▌       | 75/300 [02:23<04:18,  1.15s/it]Running Inference:  25%|██▌       | 76/300 [02:24<03:53,  1.04s/it]Running Inference:  26%|██▌       | 77/300 [02:26<05:11,  1.40s/it]Running Inference:  26%|██▌       | 78/300 [02:30<07:27,  2.02s/it]Running Inference:  26%|██▋       | 79/300 [02:33<08:26,  2.29s/it]Running Inference:  27%|██▋       | 80/300 [02:33<06:45,  1.84s/it]Running Inference:  27%|██▋       | 81/300 [02:35<06:26,  1.76s/it]Running Inference:  27%|██▋       | 82/300 [02:36<05:15,  1.45s/it]Running Inference:  28%|██▊       | 83/300 [02:37<05:02,  1.39s/it]Running Inference:  28%|██▊       | 84/300 [02:40<06:18,  1.75s/it]Running Inference:  28%|██▊       | 85/300 [02:42<06:34,  1.84s/it]Running Inference:  29%|██▊       | 86/300 [02:43<05:48,  1.63s/it]Running Inference:  29%|██▉       | 87/300 [02:45<06:33,  1.85s/it]Running Inference:  29%|██▉       | 88/300 [02:49<08:47,  2.49s/it]Running Inference:  30%|██▉       | 89/300 [02:52<08:42,  2.47s/it]Running Inference:  30%|███       | 90/300 [02:53<07:23,  2.11s/it]Running Inference:  30%|███       | 91/300 [02:55<07:23,  2.12s/it]Running Inference:  31%|███       | 92/300 [02:56<05:58,  1.72s/it]Running Inference:  31%|███       | 93/300 [02:58<06:09,  1.78s/it]Running Inference:  31%|███▏      | 94/300 [02:59<05:31,  1.61s/it]Running Inference:  32%|███▏      | 95/300 [03:02<06:36,  1.93s/it]Running Inference:  32%|███▏      | 96/300 [03:03<05:36,  1.65s/it]Running Inference:  32%|███▏      | 97/300 [03:04<04:53,  1.45s/it]Running Inference:  33%|███▎      | 98/300 [03:05<04:34,  1.36s/it]Running Inference:  33%|███▎      | 99/300 [03:07<05:26,  1.62s/it]Running Inference:  33%|███▎      | 100/300 [03:08<04:43,  1.42s/it]Running Inference:  34%|███▎      | 101/300 [03:09<04:08,  1.25s/it]Running Inference:  34%|███▍      | 102/300 [03:10<03:48,  1.15s/it]Running Inference:  34%|███▍      | 103/300 [03:12<05:11,  1.58s/it]Running Inference:  35%|███▍      | 104/300 [03:15<06:10,  1.89s/it]Running Inference:  35%|███▌      | 105/300 [03:16<05:06,  1.57s/it]Running Inference:  35%|███▌      | 106/300 [03:19<06:23,  1.98s/it]Running Inference:  36%|███▌      | 107/300 [03:21<06:59,  2.17s/it]Running Inference:  36%|███▌      | 108/300 [03:24<07:47,  2.44s/it]Running Inference:  36%|███▋      | 109/300 [03:27<08:11,  2.57s/it]Running Inference:  37%|███▋      | 110/300 [03:31<09:21,  2.96s/it]Running Inference:  37%|███▋      | 111/300 [03:35<10:30,  3.34s/it]Running Inference:  37%|███▋      | 112/300 [03:36<08:05,  2.58s/it]Running Inference:  38%|███▊      | 113/300 [03:37<06:14,  2.00s/it]Running Inference:  38%|███▊      | 114/300 [03:38<05:52,  1.90s/it]Running Inference:  38%|███▊      | 115/300 [03:40<06:01,  1.96s/it]Running Inference:  39%|███▊      | 116/300 [03:44<07:42,  2.51s/it]Running Inference:  39%|███▉      | 117/300 [03:47<07:53,  2.59s/it]Running Inference:  39%|███▉      | 118/300 [03:48<06:26,  2.12s/it]Running Inference:  40%|███▉      | 119/300 [03:49<05:15,  1.74s/it]Running Inference:  40%|████      | 120/300 [03:49<04:10,  1.39s/it]Running Inference:  40%|████      | 121/300 [03:51<04:26,  1.49s/it]Running Inference:  41%|████      | 122/300 [03:52<03:48,  1.28s/it]Running Inference:  41%|████      | 123/300 [03:53<03:30,  1.19s/it]Running Inference:  41%|████▏     | 124/300 [03:55<04:36,  1.57s/it]Running Inference:  42%|████▏     | 125/300 [03:57<04:51,  1.66s/it]Running Inference:  42%|████▏     | 126/300 [04:00<05:44,  1.98s/it]Running Inference:  42%|████▏     | 127/300 [04:02<05:57,  2.06s/it]Running Inference:  43%|████▎     | 128/300 [04:03<04:41,  1.63s/it]Running Inference:  43%|████▎     | 129/300 [04:04<04:12,  1.48s/it]Running Inference:  43%|████▎     | 130/300 [04:05<03:57,  1.40s/it]Running Inference:  44%|████▎     | 131/300 [04:06<03:27,  1.23s/it]Running Inference:  44%|████▍     | 132/300 [04:07<03:04,  1.10s/it]Running Inference:  44%|████▍     | 133/300 [04:08<03:23,  1.22s/it]Running Inference:  45%|████▍     | 134/300 [04:11<04:25,  1.60s/it]Running Inference:  45%|████▌     | 135/300 [04:12<03:36,  1.31s/it]Running Inference:  45%|████▌     | 136/300 [04:15<05:12,  1.91s/it]Running Inference:  46%|████▌     | 137/300 [04:18<06:34,  2.42s/it]Running Inference:  46%|████▌     | 138/300 [04:19<05:23,  2.00s/it]Running Inference:  46%|████▋     | 139/300 [04:21<04:40,  1.74s/it]Running Inference:  47%|████▋     | 140/300 [04:22<04:00,  1.51s/it]Running Inference:  47%|████▋     | 141/300 [04:23<04:13,  1.59s/it]Running Inference:  47%|████▋     | 142/300 [04:25<04:07,  1.57s/it]Running Inference:  48%|████▊     | 143/300 [04:26<03:48,  1.45s/it]Running Inference:  48%|████▊     | 144/300 [04:28<04:23,  1.69s/it]Running Inference:  48%|████▊     | 145/300 [04:29<03:58,  1.54s/it]Running Inference:  49%|████▊     | 146/300 [04:31<03:46,  1.47s/it]Running Inference:  49%|████▉     | 147/300 [04:32<03:12,  1.26s/it]Running Inference:  49%|████▉     | 148/300 [04:34<03:57,  1.56s/it]Running Inference:  50%|████▉     | 149/300 [04:36<04:21,  1.73s/it]Running Inference:  50%|█████     | 150/300 [04:37<03:59,  1.60s/it]Running Inference:  50%|█████     | 151/300 [04:38<03:23,  1.37s/it]Running Inference:  51%|█████     | 152/300 [04:39<03:01,  1.23s/it]Running Inference:  51%|█████     | 153/300 [04:40<03:01,  1.23s/it]Running Inference:  51%|█████▏    | 154/300 [04:43<03:49,  1.57s/it]Running Inference:  52%|█████▏    | 155/300 [04:43<03:19,  1.37s/it]Running Inference:  52%|█████▏    | 156/300 [04:44<02:51,  1.19s/it]Running Inference:  52%|█████▏    | 157/300 [04:45<02:39,  1.12s/it]Running Inference:  53%|█████▎    | 158/300 [04:49<04:35,  1.94s/it]Running Inference:  53%|█████▎    | 159/300 [04:50<03:49,  1.63s/it]Running Inference:  53%|█████▎    | 160/300 [04:53<04:31,  1.94s/it]Running Inference:  54%|█████▎    | 161/300 [04:55<04:55,  2.13s/it]Running Inference:  54%|█████▍    | 162/300 [04:59<05:59,  2.60s/it]Running Inference:  54%|█████▍    | 163/300 [05:01<05:30,  2.41s/it]Running Inference:  55%|█████▍    | 164/300 [05:05<06:38,  2.93s/it]Running Inference:  55%|█████▌    | 165/300 [05:06<05:06,  2.27s/it]Running Inference:  55%|█████▌    | 166/300 [05:07<04:14,  1.90s/it]Running Inference:  56%|█████▌    | 167/300 [05:08<03:31,  1.59s/it]Running Inference:  56%|█████▌    | 168/300 [05:09<03:10,  1.45s/it]Running Inference:  56%|█████▋    | 169/300 [05:13<04:52,  2.23s/it]Running Inference:  57%|█████▋    | 170/300 [05:14<04:11,  1.93s/it]Running Inference:  57%|█████▋    | 171/300 [05:18<05:40,  2.64s/it]Running Inference:  57%|█████▋    | 172/300 [05:21<05:22,  2.52s/it]Running Inference:  58%|█████▊    | 173/300 [05:23<05:10,  2.45s/it]Running Inference:  58%|█████▊    | 174/300 [05:24<04:00,  1.91s/it]Running Inference:  58%|█████▊    | 175/300 [05:25<03:24,  1.64s/it]Running Inference:  59%|█████▊    | 176/300 [05:27<03:43,  1.80s/it]Running Inference:  59%|█████▉    | 177/300 [05:31<05:03,  2.46s/it]Running Inference:  59%|█████▉    | 178/300 [05:33<04:37,  2.27s/it]Running Inference:  60%|█████▉    | 179/300 [05:33<03:40,  1.82s/it]Running Inference:  60%|██████    | 180/300 [05:36<04:26,  2.22s/it]Running Inference:  60%|██████    | 181/300 [05:37<03:24,  1.72s/it]Running Inference:  61%|██████    | 182/300 [05:40<04:21,  2.21s/it]Running Inference:  61%|██████    | 183/300 [05:42<03:49,  1.97s/it]Running Inference:  61%|██████▏   | 184/300 [05:46<05:07,  2.65s/it]Running Inference:  62%|██████▏   | 185/300 [05:47<04:19,  2.26s/it]Running Inference:  62%|██████▏   | 186/300 [05:49<03:41,  1.94s/it]Running Inference:  62%|██████▏   | 187/300 [05:49<03:00,  1.60s/it]Running Inference:  63%|██████▎   | 188/300 [05:52<03:41,  1.97s/it]Running Inference:  63%|██████▎   | 189/300 [05:53<02:56,  1.59s/it]Running Inference:  63%|██████▎   | 190/300 [05:54<02:22,  1.30s/it]Running Inference:  64%|██████▎   | 191/300 [05:54<02:07,  1.17s/it]Running Inference:  64%|██████▍   | 192/300 [05:58<03:34,  1.99s/it]Running Inference:  64%|██████▍   | 193/300 [06:00<03:15,  1.83s/it]Running Inference:  65%|██████▍   | 194/300 [06:01<02:54,  1.65s/it]Running Inference:  65%|██████▌   | 195/300 [06:02<02:27,  1.40s/it]Running Inference:  65%|██████▌   | 196/300 [06:02<02:00,  1.16s/it]Running Inference:  66%|██████▌   | 197/300 [06:04<02:12,  1.29s/it]Running Inference:  66%|██████▌   | 198/300 [06:05<01:54,  1.12s/it]Running Inference:  66%|██████▋   | 199/300 [06:07<02:43,  1.62s/it]Running Inference:  67%|██████▋   | 200/300 [06:12<04:01,  2.41s/it]Running Inference:  67%|██████▋   | 201/300 [06:13<03:10,  1.93s/it]Running Inference:  67%|██████▋   | 202/300 [06:15<03:38,  2.23s/it]Running Inference:  68%|██████▊   | 203/300 [06:16<02:58,  1.84s/it]Running Inference:  68%|██████▊   | 204/300 [06:19<03:04,  1.92s/it]Running Inference:  68%|██████▊   | 205/300 [06:20<02:37,  1.65s/it]Running Inference:  69%|██████▊   | 206/300 [06:21<02:42,  1.72s/it]Running Inference:  69%|██████▉   | 207/300 [06:23<02:22,  1.53s/it]Running Inference:  69%|██████▉   | 208/300 [06:25<02:44,  1.79s/it]Running Inference:  70%|██████▉   | 209/300 [06:27<02:39,  1.75s/it]Running Inference:  70%|███████   | 210/300 [06:30<03:16,  2.18s/it]Running Inference:  70%|███████   | 211/300 [06:31<02:56,  1.98s/it]Running Inference:  71%|███████   | 212/300 [06:33<02:35,  1.76s/it]Running Inference:  71%|███████   | 213/300 [06:33<02:05,  1.44s/it]Running Inference:  71%|███████▏  | 214/300 [06:36<02:41,  1.88s/it]Running Inference:  72%|███████▏  | 215/300 [06:38<02:49,  2.00s/it]Running Inference:  72%|███████▏  | 216/300 [06:40<02:39,  1.90s/it]Running Inference:  72%|███████▏  | 217/300 [06:41<02:11,  1.58s/it]Running Inference:  73%|███████▎  | 218/300 [06:42<01:46,  1.30s/it]Running Inference:  73%|███████▎  | 219/300 [06:43<01:41,  1.25s/it]Running Inference:  73%|███████▎  | 220/300 [06:44<01:32,  1.15s/it]Running Inference:  74%|███████▎  | 221/300 [06:45<01:35,  1.21s/it]Running Inference:  74%|███████▍  | 222/300 [06:47<01:49,  1.41s/it]Running Inference:  74%|███████▍  | 223/300 [06:48<01:36,  1.26s/it]Running Inference:  75%|███████▍  | 224/300 [06:49<01:27,  1.15s/it]Running Inference:  75%|███████▌  | 225/300 [06:52<02:07,  1.71s/it]Running Inference:  75%|███████▌  | 226/300 [06:53<02:02,  1.66s/it]Running Inference:  76%|███████▌  | 227/300 [06:54<01:46,  1.46s/it]Running Inference:  76%|███████▌  | 228/300 [06:55<01:22,  1.14s/it]Running Inference:  76%|███████▋  | 229/300 [06:56<01:17,  1.09s/it]Running Inference:  77%|███████▋  | 230/300 [06:59<02:10,  1.87s/it]Running Inference:  77%|███████▋  | 231/300 [07:00<01:42,  1.49s/it]Running Inference:  77%|███████▋  | 232/300 [07:00<01:24,  1.24s/it]Running Inference:  78%|███████▊  | 233/300 [07:02<01:26,  1.28s/it]Running Inference:  78%|███████▊  | 234/300 [07:03<01:12,  1.10s/it]Running Inference:  78%|███████▊  | 235/300 [07:06<01:51,  1.71s/it]Running Inference:  79%|███████▊  | 236/300 [07:09<02:17,  2.14s/it]Running Inference:  79%|███████▉  | 237/300 [07:11<02:10,  2.08s/it]Running Inference:  79%|███████▉  | 238/300 [07:12<02:00,  1.95s/it]Running Inference:  80%|███████▉  | 239/300 [07:14<01:55,  1.89s/it]Running Inference:  80%|████████  | 240/300 [07:16<02:00,  2.02s/it]Running Inference:  80%|████████  | 241/300 [07:19<02:04,  2.11s/it]Running Inference:  81%|████████  | 242/300 [07:20<01:42,  1.77s/it]Running Inference:  81%|████████  | 243/300 [07:22<01:56,  2.04s/it]Running Inference:  81%|████████▏ | 244/300 [07:23<01:28,  1.58s/it]Running Inference:  82%|████████▏ | 245/300 [07:25<01:31,  1.67s/it]Running Inference:  82%|████████▏ | 246/300 [07:26<01:19,  1.47s/it]Running Inference:  82%|████████▏ | 247/300 [07:26<01:02,  1.19s/it]Running Inference:  83%|████████▎ | 248/300 [07:28<01:15,  1.45s/it]Running Inference:  83%|████████▎ | 249/300 [07:30<01:09,  1.35s/it]Running Inference:  83%|████████▎ | 250/300 [07:31<01:10,  1.41s/it]Running Inference:  84%|████████▎ | 251/300 [07:33<01:14,  1.53s/it]Running Inference:  84%|████████▍ | 252/300 [07:33<00:58,  1.21s/it]Running Inference:  84%|████████▍ | 253/300 [07:36<01:13,  1.57s/it]Running Inference:  85%|████████▍ | 254/300 [07:36<01:00,  1.31s/it]Running Inference:  85%|████████▌ | 255/300 [07:37<00:51,  1.15s/it]Running Inference:  85%|████████▌ | 256/300 [07:38<00:46,  1.05s/it]Running Inference:  86%|████████▌ | 257/300 [07:41<01:11,  1.66s/it]Running Inference:  86%|████████▌ | 258/300 [07:43<01:14,  1.78s/it]Running Inference:  86%|████████▋ | 259/300 [07:45<01:08,  1.67s/it]Running Inference:  87%|████████▋ | 260/300 [07:47<01:16,  1.91s/it]Running Inference:  87%|████████▋ | 261/300 [07:48<01:05,  1.67s/it]Running Inference:  87%|████████▋ | 262/300 [07:49<00:51,  1.36s/it]Running Inference:  88%|████████▊ | 263/300 [07:50<00:44,  1.21s/it]Running Inference:  88%|████████▊ | 264/300 [07:50<00:38,  1.06s/it]Running Inference:  88%|████████▊ | 265/300 [07:53<00:56,  1.61s/it]Running Inference:  89%|████████▊ | 266/300 [07:54<00:47,  1.39s/it]Running Inference:  89%|████████▉ | 267/300 [07:55<00:37,  1.15s/it]Running Inference:  89%|████████▉ | 268/300 [07:58<00:57,  1.80s/it]Running Inference:  90%|████████▉ | 269/300 [08:02<01:17,  2.50s/it]Running Inference:  90%|█████████ | 270/300 [08:03<00:59,  1.99s/it]Running Inference:  90%|█████████ | 271/300 [08:05<00:55,  1.92s/it]Running Inference:  91%|█████████ | 272/300 [08:07<00:54,  1.96s/it]Running Inference:  91%|█████████ | 273/300 [08:10<01:01,  2.29s/it]Running Inference:  91%|█████████▏| 274/300 [08:13<01:04,  2.48s/it]Running Inference:  92%|█████████▏| 275/300 [08:14<00:49,  1.97s/it]Running Inference:  92%|█████████▏| 276/300 [08:15<00:46,  1.94s/it]Running Inference:  92%|█████████▏| 277/300 [08:19<00:57,  2.50s/it]Running Inference:  93%|█████████▎| 278/300 [08:20<00:42,  1.95s/it]Running Inference:  93%|█████████▎| 279/300 [08:22<00:44,  2.12s/it]Running Inference:  93%|█████████▎| 280/300 [08:25<00:42,  2.15s/it]Running Inference:  94%|█████████▎| 281/300 [08:27<00:44,  2.33s/it]Running Inference:  94%|█████████▍| 282/300 [08:30<00:41,  2.33s/it]Running Inference:  94%|█████████▍| 283/300 [08:32<00:37,  2.21s/it]Running Inference:  95%|█████████▍| 284/300 [08:36<00:45,  2.82s/it]Running Inference:  95%|█████████▌| 285/300 [08:37<00:35,  2.34s/it]Running Inference:  95%|█████████▌| 286/300 [08:38<00:26,  1.89s/it]Running Inference:  96%|█████████▌| 287/300 [08:42<00:33,  2.60s/it]Running Inference:  96%|█████████▌| 288/300 [08:45<00:31,  2.66s/it]Running Inference:  96%|█████████▋| 289/300 [08:49<00:34,  3.10s/it]Running Inference:  97%|█████████▋| 290/300 [08:51<00:26,  2.69s/it]Running Inference:  97%|█████████▋| 291/300 [08:52<00:18,  2.09s/it]Running Inference:  97%|█████████▋| 292/300 [08:54<00:16,  2.08s/it]Running Inference:  98%|█████████▊| 293/300 [08:57<00:16,  2.42s/it]Running Inference:  98%|█████████▊| 294/300 [08:59<00:14,  2.35s/it]Running Inference:  98%|█████████▊| 295/300 [09:00<00:09,  1.83s/it]Running Inference:  99%|█████████▊| 296/300 [09:01<00:06,  1.55s/it]Running Inference:  99%|█████████▉| 297/300 [09:02<00:04,  1.51s/it]Running Inference:  99%|█████████▉| 298/300 [09:05<00:04,  2.04s/it]Running Inference: 100%|█████████▉| 299/300 [09:07<00:01,  1.86s/it]Running Inference: 100%|██████████| 300/300 [09:09<00:00,  1.90s/it]Running Inference: 100%|██████████| 300/300 [09:09<00:00,  1.83s/it]
2025-12-15 01:03:12,163 - INFO - Inference completed.
2025-12-15 01:03:12,173 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-15 01:03:12,173 - INFO - Calculating metrics for dataset: longbench
2025-12-15 01:03:12,179 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-15 01:03:12,179 - INFO - Metrics:
28.36
2025-12-15 01:03:12,181 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa_e, ratio=0.1) on GPU 1

----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 01:03:18,545 - INFO - Set deterministic seeds to 42
2025-12-15 01:03:18,545 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 01:03:18,545 - INFO - Starting evaluation run...
2025-12-15 01:03:18,545 - INFO - Output directory set to: longbenchresult
2025-12-15 01:03:18,545 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-15 01:03:18,545 - INFO - KV Press 'tova' setup.
2025-12-15 01:03:18,545 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 01:03:18,545 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.90it/s]
Device set to use cuda:0
2025-12-15 01:03:31,570 - INFO - Model pipeline loaded.
2025-12-15 01:03:31,570 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
2025-12-15 01:03:35,303 - INFO - Dataset loaded with 300 entries.
2025-12-15 01:03:35,303 - INFO - Dataset processed with 300 entries.
2025-12-15 01:03:35,339 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:01<07:36,  1.53s/it]Running Inference:   1%|          | 2/300 [00:02<06:24,  1.29s/it]Running Inference:   1%|          | 3/300 [00:03<05:16,  1.07s/it]Running Inference:   1%|▏         | 4/300 [00:05<06:15,  1.27s/it]Running Inference:   2%|▏         | 5/300 [00:06<05:43,  1.16s/it]Running Inference:   2%|▏         | 6/300 [00:06<05:18,  1.08s/it]Running Inference:   2%|▏         | 7/300 [00:08<06:46,  1.39s/it]Running Inference:   3%|▎         | 8/300 [00:11<08:47,  1.81s/it]Running Inference:   3%|▎         | 9/300 [00:14<10:31,  2.17s/it]Running Inference:   3%|▎         | 10/300 [00:17<11:31,  2.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:20<12:07,  2.52s/it]Running Inference:   4%|▍         | 12/300 [00:23<12:50,  2.67s/it]Running Inference:   4%|▍         | 13/300 [00:24<10:28,  2.19s/it]Running Inference:   5%|▍         | 14/300 [00:27<11:19,  2.38s/it]Running Inference:   5%|▌         | 15/300 [00:29<11:32,  2.43s/it]Running Inference:   5%|▌         | 16/300 [00:32<11:23,  2.41s/it]Running Inference:   6%|▌         | 17/300 [00:35<13:13,  2.80s/it]Running Inference:   6%|▌         | 18/300 [00:36<10:17,  2.19s/it]Running Inference:   6%|▋         | 19/300 [00:38<09:55,  2.12s/it]Running Inference:   7%|▋         | 20/300 [00:39<07:31,  1.61s/it]Running Inference:   7%|▋         | 21/300 [00:41<08:08,  1.75s/it]Running Inference:   7%|▋         | 22/300 [00:42<08:13,  1.78s/it]Running Inference:   8%|▊         | 23/300 [00:46<10:07,  2.19s/it]Running Inference:   8%|▊         | 24/300 [00:49<11:15,  2.45s/it]Running Inference:   8%|▊         | 25/300 [00:49<08:41,  1.90s/it]Running Inference:   9%|▊         | 26/300 [00:50<07:34,  1.66s/it]Running Inference:   9%|▉         | 27/300 [00:51<06:33,  1.44s/it]Running Inference:   9%|▉         | 28/300 [00:53<06:22,  1.41s/it]Running Inference:  10%|▉         | 29/300 [00:54<06:08,  1.36s/it]Running Inference:  10%|█         | 30/300 [00:58<09:24,  2.09s/it]Running Inference:  10%|█         | 31/300 [00:59<08:45,  1.95s/it]Running Inference:  11%|█         | 32/300 [01:03<11:26,  2.56s/it]Running Inference:  11%|█         | 33/300 [01:07<13:37,  3.06s/it]Running Inference:  11%|█▏        | 34/300 [01:10<12:24,  2.80s/it]Running Inference:  12%|█▏        | 35/300 [01:10<09:24,  2.13s/it]Running Inference:  12%|█▏        | 36/300 [01:13<10:08,  2.30s/it]Running Inference:  12%|█▏        | 37/300 [01:15<09:48,  2.24s/it]Running Inference:  13%|█▎        | 38/300 [01:17<08:56,  2.05s/it]Running Inference:  13%|█▎        | 39/300 [01:18<07:26,  1.71s/it]Running Inference:  13%|█▎        | 40/300 [01:18<05:58,  1.38s/it]Running Inference:  14%|█▎        | 41/300 [01:20<07:10,  1.66s/it]Running Inference:  14%|█▍        | 42/300 [01:22<06:31,  1.52s/it]Running Inference:  14%|█▍        | 43/300 [01:24<07:12,  1.68s/it]Running Inference:  15%|█▍        | 44/300 [01:25<06:17,  1.48s/it]Running Inference:  15%|█▌        | 45/300 [01:29<09:41,  2.28s/it]Running Inference:  15%|█▌        | 46/300 [01:31<09:16,  2.19s/it]Running Inference:  16%|█▌        | 47/300 [01:31<07:09,  1.70s/it]Running Inference:  16%|█▌        | 48/300 [01:34<08:51,  2.11s/it]Running Inference:  16%|█▋        | 49/300 [01:39<11:38,  2.78s/it]Running Inference:  17%|█▋        | 50/300 [01:40<10:04,  2.42s/it]Running Inference:  17%|█▋        | 51/300 [01:42<08:39,  2.08s/it]Running Inference:  17%|█▋        | 52/300 [01:45<09:46,  2.36s/it]Running Inference:  18%|█▊        | 53/300 [01:46<08:04,  1.96s/it]Running Inference:  18%|█▊        | 54/300 [01:46<06:33,  1.60s/it]Running Inference:  18%|█▊        | 55/300 [01:48<06:28,  1.59s/it]Running Inference:  19%|█▊        | 56/300 [01:50<07:21,  1.81s/it]Running Inference:  19%|█▉        | 57/300 [01:51<06:27,  1.59s/it]Running Inference:  19%|█▉        | 58/300 [01:53<06:07,  1.52s/it]Running Inference:  20%|█▉        | 59/300 [01:54<05:06,  1.27s/it]Running Inference:  20%|██        | 60/300 [01:57<07:20,  1.83s/it]Running Inference:  20%|██        | 61/300 [01:58<06:20,  1.59s/it]Running Inference:  21%|██        | 62/300 [02:00<06:56,  1.75s/it]Running Inference:  21%|██        | 63/300 [02:03<08:10,  2.07s/it]Running Inference:  21%|██▏       | 64/300 [02:03<06:15,  1.59s/it]Running Inference:  22%|██▏       | 65/300 [02:07<09:05,  2.32s/it]Running Inference:  22%|██▏       | 66/300 [02:08<07:42,  1.98s/it]Running Inference:  22%|██▏       | 67/300 [02:11<08:48,  2.27s/it]Running Inference:  23%|██▎       | 68/300 [02:14<09:16,  2.40s/it]Running Inference:  23%|██▎       | 69/300 [02:15<08:09,  2.12s/it]Running Inference:  23%|██▎       | 70/300 [02:16<06:47,  1.77s/it]Running Inference:  24%|██▎       | 71/300 [02:17<06:00,  1.58s/it]Running Inference:  24%|██▍       | 72/300 [02:19<05:28,  1.44s/it]Running Inference:  24%|██▍       | 73/300 [02:20<05:14,  1.39s/it]Running Inference:  25%|██▍       | 74/300 [02:21<04:46,  1.27s/it]Running Inference:  25%|██▌       | 75/300 [02:22<04:17,  1.14s/it]Running Inference:  25%|██▌       | 76/300 [02:23<03:52,  1.04s/it]Running Inference:  26%|██▌       | 77/300 [02:25<05:11,  1.40s/it]Running Inference:  26%|██▌       | 78/300 [02:28<07:29,  2.02s/it]Running Inference:  26%|██▋       | 79/300 [02:31<08:11,  2.23s/it]Running Inference:  27%|██▋       | 80/300 [02:32<06:35,  1.80s/it]Running Inference:  27%|██▋       | 81/300 [02:33<06:19,  1.73s/it]Running Inference:  27%|██▋       | 82/300 [02:34<05:13,  1.44s/it]Running Inference:  28%|██▊       | 83/300 [02:35<05:01,  1.39s/it]Running Inference:  28%|██▊       | 84/300 [02:38<06:23,  1.78s/it]Running Inference:  28%|██▊       | 85/300 [02:40<06:38,  1.85s/it]Running Inference:  29%|██▊       | 86/300 [02:41<05:50,  1.64s/it]Running Inference:  29%|██▉       | 87/300 [02:44<06:34,  1.85s/it]Running Inference:  29%|██▉       | 88/300 [02:48<08:50,  2.50s/it]Running Inference:  30%|██▉       | 89/300 [02:50<08:44,  2.48s/it]Running Inference:  30%|███       | 90/300 [02:51<07:25,  2.12s/it]Running Inference:  30%|███       | 91/300 [02:53<07:25,  2.13s/it]Running Inference:  31%|███       | 92/300 [02:54<05:59,  1.73s/it]Running Inference:  31%|███       | 93/300 [02:56<06:09,  1.79s/it]Running Inference:  31%|███▏      | 94/300 [02:57<05:31,  1.61s/it]Running Inference:  32%|███▏      | 95/300 [03:01<07:36,  2.23s/it]Running Inference:  32%|███▏      | 96/300 [03:02<06:18,  1.86s/it]Running Inference:  32%|███▏      | 97/300 [03:03<05:22,  1.59s/it]Running Inference:  33%|███▎      | 98/300 [03:04<04:54,  1.46s/it]Running Inference:  33%|███▎      | 99/300 [03:06<05:40,  1.69s/it]Running Inference:  33%|███▎      | 100/300 [03:07<04:53,  1.47s/it]Running Inference:  34%|███▎      | 101/300 [03:08<04:15,  1.29s/it]Running Inference:  34%|███▍      | 102/300 [03:09<03:52,  1.18s/it]Running Inference:  34%|███▍      | 103/300 [03:12<05:15,  1.60s/it]Running Inference:  35%|███▍      | 104/300 [03:15<06:48,  2.08s/it]Running Inference:  35%|███▌      | 105/300 [03:16<05:33,  1.71s/it]Running Inference:  35%|███▌      | 106/300 [03:19<06:36,  2.04s/it]Running Inference:  36%|███▌      | 107/300 [03:19<05:16,  1.64s/it]Running Inference:  36%|███▌      | 108/300 [03:22<06:36,  2.07s/it]Running Inference:  36%|███▋      | 109/300 [03:25<07:22,  2.32s/it]Running Inference:  37%|███▋      | 110/300 [03:29<08:49,  2.78s/it]Running Inference:  37%|███▋      | 111/300 [03:33<10:08,  3.22s/it]Running Inference:  37%|███▋      | 112/300 [03:34<07:47,  2.49s/it]Running Inference:  38%|███▊      | 113/300 [03:35<06:01,  1.93s/it]Running Inference:  38%|███▊      | 114/300 [03:36<05:43,  1.85s/it]Running Inference:  38%|███▊      | 115/300 [03:38<05:55,  1.92s/it]Running Inference:  39%|███▊      | 116/300 [03:42<07:39,  2.49s/it]Running Inference:  39%|███▉      | 117/300 [03:45<07:52,  2.58s/it]Running Inference:  39%|███▉      | 118/300 [03:46<06:30,  2.14s/it]Running Inference:  40%|███▉      | 119/300 [03:47<05:18,  1.76s/it]Running Inference:  40%|████      | 120/300 [03:48<04:12,  1.40s/it]Running Inference:  40%|████      | 121/300 [03:49<04:21,  1.46s/it]Running Inference:  41%|████      | 122/300 [03:50<03:45,  1.27s/it]Running Inference:  41%|████      | 123/300 [03:51<03:28,  1.18s/it]Running Inference:  41%|████▏     | 124/300 [03:54<04:36,  1.57s/it]Running Inference:  42%|████▏     | 125/300 [03:55<04:50,  1.66s/it]Running Inference:  42%|████▏     | 126/300 [03:58<05:44,  1.98s/it]Running Inference:  42%|████▏     | 127/300 [04:00<05:57,  2.07s/it]Running Inference:  43%|████▎     | 128/300 [04:01<04:41,  1.64s/it]Running Inference:  43%|████▎     | 129/300 [04:02<04:13,  1.48s/it]Running Inference:  43%|████▎     | 130/300 [04:03<03:57,  1.40s/it]Running Inference:  44%|████▎     | 131/300 [04:04<03:27,  1.23s/it]Running Inference:  44%|████▍     | 132/300 [04:05<03:04,  1.10s/it]Running Inference:  44%|████▍     | 133/300 [04:06<03:23,  1.22s/it]Running Inference:  45%|████▍     | 134/300 [04:09<04:26,  1.60s/it]Running Inference:  45%|████▌     | 135/300 [04:10<03:36,  1.31s/it]Running Inference:  45%|████▌     | 136/300 [04:13<05:14,  1.92s/it]Running Inference:  46%|████▌     | 137/300 [04:17<06:36,  2.43s/it]Running Inference:  46%|████▌     | 138/300 [04:18<05:25,  2.01s/it]Running Inference:  46%|████▋     | 139/300 [04:19<04:41,  1.75s/it]Running Inference:  47%|████▋     | 140/300 [04:20<04:01,  1.51s/it]Running Inference:  47%|████▋     | 141/300 [04:21<04:13,  1.60s/it]Running Inference:  47%|████▋     | 142/300 [04:23<04:08,  1.57s/it]Running Inference:  48%|████▊     | 143/300 [04:24<03:48,  1.46s/it]Running Inference:  48%|████▊     | 144/300 [04:26<04:23,  1.69s/it]Running Inference:  48%|████▊     | 145/300 [04:28<03:58,  1.54s/it]Running Inference:  49%|████▊     | 146/300 [04:29<03:46,  1.47s/it]Running Inference:  49%|████▉     | 147/300 [04:30<03:13,  1.26s/it]Running Inference:  49%|████▉     | 148/300 [04:32<03:56,  1.56s/it]Running Inference:  50%|████▉     | 149/300 [04:34<04:21,  1.73s/it]Running Inference:  50%|█████     | 150/300 [04:35<03:59,  1.60s/it]Running Inference:  50%|█████     | 151/300 [04:36<03:23,  1.37s/it]Running Inference:  51%|█████     | 152/300 [04:37<03:01,  1.23s/it]Running Inference:  51%|█████     | 153/300 [04:40<04:32,  1.86s/it]Running Inference:  51%|█████▏    | 154/300 [04:43<04:53,  2.01s/it]Running Inference:  52%|█████▏    | 155/300 [04:44<04:03,  1.68s/it]Running Inference:  52%|█████▏    | 156/300 [04:44<03:21,  1.40s/it]Running Inference:  52%|█████▏    | 157/300 [04:45<03:01,  1.27s/it]Running Inference:  53%|█████▎    | 158/300 [04:49<04:50,  2.05s/it]Running Inference:  53%|█████▎    | 159/300 [04:50<04:00,  1.70s/it]Running Inference:  53%|█████▎    | 160/300 [04:53<04:40,  2.00s/it]Running Inference:  54%|█████▎    | 161/300 [04:55<05:01,  2.17s/it]Running Inference:  54%|█████▍    | 162/300 [04:59<06:04,  2.64s/it]Running Inference:  54%|█████▍    | 163/300 [05:01<05:33,  2.44s/it]Running Inference:  55%|█████▍    | 164/300 [05:05<06:41,  2.95s/it]Running Inference:  55%|█████▌    | 165/300 [05:06<05:08,  2.28s/it]Running Inference:  55%|█████▌    | 166/300 [05:07<04:11,  1.88s/it]Running Inference:  56%|█████▌    | 167/300 [05:08<03:29,  1.58s/it]Running Inference:  56%|█████▌    | 168/300 [05:09<03:09,  1.44s/it]Running Inference:  56%|█████▋    | 169/300 [05:13<04:51,  2.23s/it]Running Inference:  57%|█████▋    | 170/300 [05:14<04:11,  1.93s/it]Running Inference:  57%|█████▋    | 171/300 [05:19<05:40,  2.64s/it]Running Inference:  57%|█████▋    | 172/300 [05:21<05:22,  2.52s/it]Running Inference:  58%|█████▊    | 173/300 [05:23<05:10,  2.45s/it]Running Inference:  58%|█████▊    | 174/300 [05:24<04:01,  1.92s/it]Running Inference:  58%|█████▊    | 175/300 [05:25<03:25,  1.64s/it]Running Inference:  59%|█████▊    | 176/300 [05:27<03:43,  1.81s/it]Running Inference:  59%|█████▉    | 177/300 [05:31<05:04,  2.47s/it]Running Inference:  59%|█████▉    | 178/300 [05:33<04:37,  2.28s/it]Running Inference:  60%|█████▉    | 179/300 [05:34<03:41,  1.83s/it]Running Inference:  60%|██████    | 180/300 [05:37<04:27,  2.23s/it]Running Inference:  60%|██████    | 181/300 [05:37<03:25,  1.73s/it]Running Inference:  61%|██████    | 182/300 [05:41<04:22,  2.22s/it]Running Inference:  61%|██████    | 183/300 [05:42<03:50,  1.97s/it]Running Inference:  61%|██████▏   | 184/300 [05:46<05:08,  2.66s/it]Running Inference:  62%|██████▏   | 185/300 [05:48<04:19,  2.26s/it]Running Inference:  62%|██████▏   | 186/300 [05:49<03:41,  1.95s/it]Running Inference:  62%|██████▏   | 187/300 [05:50<03:00,  1.60s/it]Running Inference:  63%|██████▎   | 188/300 [05:52<03:42,  1.98s/it]Running Inference:  63%|██████▎   | 189/300 [05:53<02:56,  1.59s/it]Running Inference:  63%|██████▎   | 190/300 [05:54<02:23,  1.30s/it]Running Inference:  64%|██████▎   | 191/300 [05:55<02:08,  1.18s/it]Running Inference:  64%|██████▍   | 192/300 [05:59<03:35,  2.00s/it]Running Inference:  64%|██████▍   | 193/300 [06:00<03:16,  1.84s/it]Running Inference:  65%|██████▍   | 194/300 [06:01<02:55,  1.65s/it]Running Inference:  65%|██████▌   | 195/300 [06:02<02:27,  1.40s/it]Running Inference:  65%|██████▌   | 196/300 [06:03<02:00,  1.16s/it]Running Inference:  66%|██████▌   | 197/300 [06:04<02:12,  1.29s/it]Running Inference:  66%|██████▌   | 198/300 [06:05<01:50,  1.08s/it]Running Inference:  66%|██████▋   | 199/300 [06:08<02:40,  1.59s/it]Running Inference:  67%|██████▋   | 200/300 [06:11<03:31,  2.12s/it]Running Inference:  67%|██████▋   | 201/300 [06:12<02:50,  1.72s/it]Running Inference:  67%|██████▋   | 202/300 [06:15<03:24,  2.09s/it]Running Inference:  68%|██████▊   | 203/300 [06:16<02:48,  1.74s/it]Running Inference:  68%|██████▊   | 204/300 [06:18<02:57,  1.85s/it]Running Inference:  68%|██████▊   | 205/300 [06:19<02:32,  1.60s/it]Running Inference:  69%|██████▊   | 206/300 [06:21<02:38,  1.69s/it]Running Inference:  69%|██████▉   | 207/300 [06:22<02:20,  1.51s/it]Running Inference:  69%|██████▉   | 208/300 [06:25<02:52,  1.87s/it]Running Inference:  70%|██████▉   | 209/300 [06:26<02:44,  1.81s/it]Running Inference:  70%|███████   | 210/300 [06:29<03:20,  2.23s/it]Running Inference:  70%|███████   | 211/300 [06:31<02:59,  2.01s/it]Running Inference:  71%|███████   | 212/300 [06:32<02:37,  1.79s/it]Running Inference:  71%|███████   | 213/300 [06:33<02:07,  1.46s/it]Running Inference:  71%|███████▏  | 214/300 [06:36<02:43,  1.90s/it]Running Inference:  72%|███████▏  | 215/300 [06:38<02:50,  2.01s/it]Running Inference:  72%|███████▏  | 216/300 [06:40<02:40,  1.91s/it]Running Inference:  72%|███████▏  | 217/300 [06:41<02:11,  1.59s/it]Running Inference:  73%|███████▎  | 218/300 [06:41<01:46,  1.30s/it]Running Inference:  73%|███████▎  | 219/300 [06:42<01:41,  1.25s/it]Running Inference:  73%|███████▎  | 220/300 [06:43<01:32,  1.15s/it]Running Inference:  74%|███████▎  | 221/300 [06:45<01:35,  1.21s/it]Running Inference:  74%|███████▍  | 222/300 [06:46<01:50,  1.41s/it]Running Inference:  74%|███████▍  | 223/300 [06:47<01:36,  1.26s/it]Running Inference:  75%|███████▍  | 224/300 [06:50<02:11,  1.72s/it]Running Inference:  75%|███████▌  | 225/300 [06:54<03:04,  2.45s/it]Running Inference:  75%|███████▌  | 226/300 [06:56<02:41,  2.18s/it]Running Inference:  76%|███████▌  | 227/300 [06:57<02:13,  1.83s/it]Running Inference:  76%|███████▌  | 228/300 [06:57<01:40,  1.40s/it]Running Inference:  76%|███████▋  | 229/300 [06:58<01:30,  1.27s/it]Running Inference:  77%|███████▋  | 230/300 [07:01<02:08,  1.83s/it]Running Inference:  77%|███████▋  | 231/300 [07:02<01:40,  1.46s/it]Running Inference:  77%|███████▋  | 232/300 [07:03<01:23,  1.22s/it]Running Inference:  78%|███████▊  | 233/300 [07:04<01:25,  1.27s/it]Running Inference:  78%|███████▊  | 234/300 [07:05<01:11,  1.09s/it]Running Inference:  78%|███████▊  | 235/300 [07:08<01:51,  1.71s/it]Running Inference:  79%|███████▊  | 236/300 [07:11<02:17,  2.15s/it]Running Inference:  79%|███████▉  | 237/300 [07:13<02:11,  2.08s/it]Running Inference:  79%|███████▉  | 238/300 [07:15<02:00,  1.95s/it]Running Inference:  80%|███████▉  | 239/300 [07:16<01:55,  1.89s/it]Running Inference:  80%|████████  | 240/300 [07:19<02:01,  2.02s/it]Running Inference:  80%|████████  | 241/300 [07:21<02:04,  2.12s/it]Running Inference:  81%|████████  | 242/300 [07:22<01:42,  1.77s/it]Running Inference:  81%|████████  | 243/300 [07:25<01:56,  2.05s/it]Running Inference:  81%|████████▏ | 244/300 [07:25<01:28,  1.59s/it]Running Inference:  82%|████████▏ | 245/300 [07:27<01:32,  1.67s/it]Running Inference:  82%|████████▏ | 246/300 [07:28<01:19,  1.47s/it]Running Inference:  82%|████████▏ | 247/300 [07:29<01:03,  1.19s/it]Running Inference:  83%|████████▎ | 248/300 [07:31<01:15,  1.45s/it]Running Inference:  83%|████████▎ | 249/300 [07:32<01:09,  1.36s/it]Running Inference:  83%|████████▎ | 250/300 [07:33<01:10,  1.41s/it]Running Inference:  84%|████████▎ | 251/300 [07:35<01:14,  1.53s/it]Running Inference:  84%|████████▍ | 252/300 [07:36<00:58,  1.22s/it]Running Inference:  84%|████████▍ | 253/300 [07:38<01:14,  1.57s/it]Running Inference:  85%|████████▍ | 254/300 [07:39<01:00,  1.31s/it]Running Inference:  85%|████████▌ | 255/300 [07:40<00:51,  1.15s/it]Running Inference:  85%|████████▌ | 256/300 [07:40<00:46,  1.06s/it]Running Inference:  86%|████████▌ | 257/300 [07:43<01:11,  1.67s/it]Running Inference:  86%|████████▌ | 258/300 [07:46<01:15,  1.80s/it]Running Inference:  86%|████████▋ | 259/300 [07:47<01:09,  1.69s/it]Running Inference:  87%|████████▋ | 260/300 [07:49<01:17,  1.93s/it]Running Inference:  87%|████████▋ | 261/300 [07:50<01:04,  1.65s/it]Running Inference:  87%|████████▋ | 262/300 [07:51<00:50,  1.34s/it]Running Inference:  88%|████████▊ | 263/300 [07:52<00:44,  1.19s/it]Running Inference:  88%|████████▊ | 264/300 [07:53<00:38,  1.06s/it]Running Inference:  88%|████████▊ | 265/300 [07:54<00:36,  1.04s/it]Running Inference:  89%|████████▊ | 266/300 [07:55<00:33,  1.01it/s]Running Inference:  89%|████████▉ | 267/300 [07:55<00:28,  1.15it/s]Running Inference:  89%|████████▉ | 268/300 [07:59<00:51,  1.62s/it]Running Inference:  90%|████████▉ | 269/300 [08:03<01:13,  2.39s/it]Running Inference:  90%|█████████ | 270/300 [08:03<00:57,  1.91s/it]Running Inference:  90%|█████████ | 271/300 [08:05<00:53,  1.86s/it]Running Inference:  91%|█████████ | 272/300 [08:07<00:53,  1.91s/it]Running Inference:  91%|█████████ | 273/300 [08:10<00:57,  2.11s/it]Running Inference:  91%|█████████▏| 274/300 [08:13<01:01,  2.36s/it]Running Inference:  92%|█████████▏| 275/300 [08:14<00:47,  1.89s/it]Running Inference:  92%|█████████▏| 276/300 [08:15<00:45,  1.89s/it]Running Inference:  92%|█████████▏| 277/300 [08:19<00:56,  2.46s/it]Running Inference:  93%|█████████▎| 278/300 [08:20<00:42,  1.93s/it]Running Inference:  93%|█████████▎| 279/300 [08:22<00:44,  2.11s/it]Running Inference:  93%|█████████▎| 280/300 [08:24<00:41,  2.07s/it]Running Inference:  94%|█████████▎| 281/300 [08:27<00:43,  2.28s/it]Running Inference:  94%|█████████▍| 282/300 [08:30<00:41,  2.29s/it]Running Inference:  94%|█████████▍| 283/300 [08:31<00:37,  2.18s/it]Running Inference:  95%|█████████▍| 284/300 [08:34<00:37,  2.36s/it]Running Inference:  95%|█████████▌| 285/300 [08:35<00:30,  2.02s/it]Running Inference:  95%|█████████▌| 286/300 [08:36<00:23,  1.67s/it]Running Inference:  96%|█████████▌| 287/300 [08:41<00:31,  2.44s/it]Running Inference:  96%|█████████▌| 288/300 [08:43<00:30,  2.55s/it]Running Inference:  96%|█████████▋| 289/300 [08:48<00:33,  3.03s/it]Running Inference:  97%|█████████▋| 290/300 [08:49<00:26,  2.64s/it]Running Inference:  97%|█████████▋| 291/300 [08:50<00:18,  2.05s/it]Running Inference:  97%|█████████▋| 292/300 [08:52<00:16,  2.06s/it]Running Inference:  98%|█████████▊| 293/300 [08:55<00:16,  2.41s/it]Running Inference:  98%|█████████▊| 294/300 [08:57<00:13,  2.32s/it]Running Inference:  98%|█████████▊| 295/300 [08:58<00:09,  1.82s/it]Running Inference:  99%|█████████▊| 296/300 [08:59<00:06,  1.54s/it]Running Inference:  99%|█████████▉| 297/300 [09:00<00:04,  1.50s/it]Running Inference:  99%|█████████▉| 298/300 [09:02<00:02,  1.42s/it]Running Inference: 100%|█████████▉| 299/300 [09:03<00:01,  1.36s/it]Running Inference: 100%|██████████| 300/300 [09:05<00:00,  1.55s/it]Running Inference: 100%|██████████| 300/300 [09:05<00:00,  1.82s/it]
2025-12-15 01:12:40,603 - INFO - Inference completed.
2025-12-15 01:12:40,613 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-15 01:12:40,613 - INFO - Calculating metrics for dataset: longbench
2025-12-15 01:12:40,619 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-15 01:12:40,619 - INFO - Metrics:
28.35
2025-12-15 01:12:40,620 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa_e, ratio=0.2) on GPU 1

----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 01:12:47,010 - INFO - Set deterministic seeds to 42
2025-12-15 01:12:47,010 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 01:12:47,010 - INFO - Starting evaluation run...
2025-12-15 01:12:47,010 - INFO - Output directory set to: longbenchresult
2025-12-15 01:12:47,010 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-15 01:12:47,010 - INFO - KV Press 'tova' setup.
2025-12-15 01:12:47,011 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 01:12:47,011 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.01it/s]
Device set to use cuda:0
2025-12-15 01:12:57,706 - INFO - Model pipeline loaded.
2025-12-15 01:12:57,706 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
2025-12-15 01:13:02,722 - INFO - Dataset loaded with 300 entries.
2025-12-15 01:13:02,722 - INFO - Dataset processed with 300 entries.
2025-12-15 01:13:02,761 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:01<07:50,  1.57s/it]Running Inference:   1%|          | 2/300 [00:02<06:27,  1.30s/it]Running Inference:   1%|          | 3/300 [00:03<05:17,  1.07s/it]Running Inference:   1%|▏         | 4/300 [00:05<06:15,  1.27s/it]Running Inference:   2%|▏         | 5/300 [00:06<05:42,  1.16s/it]Running Inference:   2%|▏         | 6/300 [00:06<05:17,  1.08s/it]Running Inference:   2%|▏         | 7/300 [00:08<06:45,  1.38s/it]Running Inference:   3%|▎         | 8/300 [00:09<06:12,  1.28s/it]Running Inference:   3%|▎         | 9/300 [00:11<06:08,  1.27s/it]Running Inference:   3%|▎         | 10/300 [00:14<08:29,  1.76s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:14<07:10,  1.49s/it]Running Inference:   4%|▍         | 12/300 [00:18<09:23,  1.96s/it]Running Inference:   4%|▍         | 13/300 [00:19<08:05,  1.69s/it]Running Inference:   5%|▍         | 14/300 [00:21<09:39,  2.02s/it]Running Inference:   5%|▌         | 15/300 [00:24<10:21,  2.18s/it]Running Inference:   5%|▌         | 16/300 [00:26<10:33,  2.23s/it]Running Inference:   6%|▌         | 17/300 [00:30<12:36,  2.67s/it]Running Inference:   6%|▌         | 18/300 [00:31<09:51,  2.10s/it]Running Inference:   6%|▋         | 19/300 [00:33<09:36,  2.05s/it]Running Inference:   7%|▋         | 20/300 [00:33<07:18,  1.56s/it]Running Inference:   7%|▋         | 21/300 [00:35<07:58,  1.72s/it]Running Inference:   7%|▋         | 22/300 [00:37<08:06,  1.75s/it]Running Inference:   8%|▊         | 23/300 [00:40<10:00,  2.17s/it]Running Inference:   8%|▊         | 24/300 [00:43<11:08,  2.42s/it]Running Inference:   8%|▊         | 25/300 [00:44<08:36,  1.88s/it]Running Inference:   9%|▊         | 26/300 [00:45<07:37,  1.67s/it]Running Inference:   9%|▉         | 27/300 [00:46<06:35,  1.45s/it]Running Inference:   9%|▉         | 28/300 [00:47<06:23,  1.41s/it]Running Inference:  10%|▉         | 29/300 [00:48<06:08,  1.36s/it]Running Inference:  10%|█         | 30/300 [00:51<07:13,  1.61s/it]Running Inference:  10%|█         | 31/300 [00:52<07:07,  1.59s/it]Running Inference:  11%|█         | 32/300 [00:56<10:17,  2.30s/it]Running Inference:  11%|█         | 33/300 [01:00<12:48,  2.88s/it]Running Inference:  11%|█▏        | 34/300 [01:03<11:50,  2.67s/it]Running Inference:  12%|█▏        | 35/300 [01:03<09:03,  2.05s/it]Running Inference:  12%|█▏        | 36/300 [01:07<11:47,  2.68s/it]Running Inference:  12%|█▏        | 37/300 [01:09<10:39,  2.43s/it]Running Inference:  13%|█▎        | 38/300 [01:11<09:32,  2.18s/it]Running Inference:  13%|█▎        | 39/300 [01:12<07:50,  1.80s/it]Running Inference:  13%|█▎        | 40/300 [01:12<06:15,  1.44s/it]Running Inference:  14%|█▎        | 41/300 [01:15<07:18,  1.69s/it]Running Inference:  14%|█▍        | 42/300 [01:16<06:36,  1.54s/it]Running Inference:  14%|█▍        | 43/300 [01:18<07:15,  1.69s/it]Running Inference:  15%|█▍        | 44/300 [01:19<06:20,  1.49s/it]Running Inference:  15%|█▌        | 45/300 [01:21<07:10,  1.69s/it]Running Inference:  15%|█▌        | 46/300 [01:23<07:31,  1.78s/it]Running Inference:  16%|█▌        | 47/300 [01:23<05:55,  1.40s/it]Running Inference:  16%|█▌        | 48/300 [01:27<07:58,  1.90s/it]Running Inference:  16%|█▋        | 49/300 [01:31<10:58,  2.62s/it]Running Inference:  17%|█▋        | 50/300 [01:33<10:13,  2.45s/it]Running Inference:  17%|█▋        | 51/300 [01:34<08:41,  2.10s/it]Running Inference:  17%|█▋        | 52/300 [01:37<09:46,  2.37s/it]Running Inference:  18%|█▊        | 53/300 [01:40<10:30,  2.55s/it]Running Inference:  18%|█▊        | 54/300 [01:41<08:14,  2.01s/it]Running Inference:  18%|█▊        | 55/300 [01:42<07:39,  1.87s/it]Running Inference:  19%|█▊        | 56/300 [01:45<08:09,  2.01s/it]Running Inference:  19%|█▉        | 57/300 [01:46<06:36,  1.63s/it]Running Inference:  19%|█▉        | 58/300 [01:47<06:13,  1.54s/it]Running Inference:  20%|█▉        | 59/300 [01:48<05:10,  1.29s/it]Running Inference:  20%|██        | 60/300 [01:49<04:59,  1.25s/it]Running Inference:  20%|██        | 61/300 [01:50<04:41,  1.18s/it]Running Inference:  21%|██        | 62/300 [01:52<05:47,  1.46s/it]Running Inference:  21%|██        | 63/300 [01:55<07:20,  1.86s/it]Running Inference:  21%|██▏       | 64/300 [01:55<05:40,  1.44s/it]Running Inference:  22%|██▏       | 65/300 [01:58<07:19,  1.87s/it]Running Inference:  22%|██▏       | 66/300 [01:59<06:04,  1.56s/it]Running Inference:  22%|██▏       | 67/300 [02:02<07:39,  1.97s/it]Running Inference:  23%|██▎       | 68/300 [02:04<08:27,  2.19s/it]Running Inference:  23%|██▎       | 69/300 [02:06<07:35,  1.97s/it]Running Inference:  23%|██▎       | 70/300 [02:07<06:23,  1.67s/it]Running Inference:  24%|██▎       | 71/300 [02:08<05:44,  1.50s/it]Running Inference:  24%|██▍       | 72/300 [02:09<05:16,  1.39s/it]Running Inference:  24%|██▍       | 73/300 [02:10<05:06,  1.35s/it]Running Inference:  25%|██▍       | 74/300 [02:11<04:40,  1.24s/it]Running Inference:  25%|██▌       | 75/300 [02:12<04:13,  1.12s/it]Running Inference:  25%|██▌       | 76/300 [02:13<03:49,  1.02s/it]Running Inference:  26%|██▌       | 77/300 [02:15<05:08,  1.39s/it]Running Inference:  26%|██▌       | 78/300 [02:19<07:26,  2.01s/it]Running Inference:  26%|██▋       | 79/300 [02:21<08:08,  2.21s/it]Running Inference:  27%|██▋       | 80/300 [02:22<06:32,  1.79s/it]Running Inference:  27%|██▋       | 81/300 [02:24<06:17,  1.72s/it]Running Inference:  27%|██▋       | 82/300 [02:24<05:11,  1.43s/it]Running Inference:  28%|██▊       | 83/300 [02:26<05:21,  1.48s/it]Running Inference:  28%|██▊       | 84/300 [02:29<06:31,  1.81s/it]Running Inference:  28%|██▊       | 85/300 [02:31<06:44,  1.88s/it]Running Inference:  29%|██▊       | 86/300 [02:32<05:54,  1.66s/it]Running Inference:  29%|██▉       | 87/300 [02:34<06:37,  1.86s/it]Running Inference:  29%|██▉       | 88/300 [02:37<07:22,  2.09s/it]Running Inference:  30%|██▉       | 89/300 [02:39<07:43,  2.19s/it]Running Inference:  30%|███       | 90/300 [02:41<06:42,  1.92s/it]Running Inference:  30%|███       | 91/300 [02:43<06:55,  1.99s/it]Running Inference:  31%|███       | 92/300 [02:44<05:43,  1.65s/it]Running Inference:  31%|███       | 93/300 [02:45<05:58,  1.73s/it]Running Inference:  31%|███▏      | 94/300 [02:47<05:23,  1.57s/it]Running Inference:  32%|███▏      | 95/300 [02:50<06:45,  1.98s/it]Running Inference:  32%|███▏      | 96/300 [02:51<05:45,  1.69s/it]Running Inference:  32%|███▏      | 97/300 [02:52<04:59,  1.48s/it]Running Inference:  33%|███▎      | 98/300 [02:53<04:38,  1.38s/it]Running Inference:  33%|███▎      | 99/300 [02:55<05:29,  1.64s/it]Running Inference:  33%|███▎      | 100/300 [02:56<04:45,  1.43s/it]Running Inference:  34%|███▎      | 101/300 [02:57<04:10,  1.26s/it]Running Inference:  34%|███▍      | 102/300 [02:58<03:48,  1.16s/it]Running Inference:  34%|███▍      | 103/300 [03:00<05:12,  1.59s/it]Running Inference:  35%|███▍      | 104/300 [03:03<06:08,  1.88s/it]Running Inference:  35%|███▌      | 105/300 [03:04<05:05,  1.57s/it]Running Inference:  35%|███▌      | 106/300 [03:07<06:23,  1.98s/it]Running Inference:  36%|███▌      | 107/300 [03:07<05:04,  1.58s/it]Running Inference:  36%|███▌      | 108/300 [03:10<06:27,  2.02s/it]Running Inference:  36%|███▋      | 109/300 [03:13<07:16,  2.28s/it]Running Inference:  37%|███▋      | 110/300 [03:17<08:36,  2.72s/it]Running Inference:  37%|███▋      | 111/300 [03:21<09:58,  3.17s/it]Running Inference:  37%|███▋      | 112/300 [03:22<07:40,  2.45s/it]Running Inference:  38%|███▊      | 113/300 [03:23<05:56,  1.91s/it]Running Inference:  38%|███▊      | 114/300 [03:24<05:40,  1.83s/it]Running Inference:  38%|███▊      | 115/300 [03:26<05:53,  1.91s/it]Running Inference:  39%|███▊      | 116/300 [03:30<07:36,  2.48s/it]Running Inference:  39%|███▉      | 117/300 [03:33<07:49,  2.57s/it]Running Inference:  39%|███▉      | 118/300 [03:34<06:23,  2.11s/it]Running Inference:  40%|███▉      | 119/300 [03:35<05:14,  1.74s/it]Running Inference:  40%|████      | 120/300 [03:35<04:09,  1.38s/it]Running Inference:  40%|████      | 121/300 [03:37<04:19,  1.45s/it]Running Inference:  41%|████      | 122/300 [03:38<03:43,  1.25s/it]Running Inference:  41%|████      | 123/300 [03:39<03:26,  1.17s/it]Running Inference:  41%|████▏     | 124/300 [03:41<04:34,  1.56s/it]Running Inference:  42%|████▏     | 125/300 [03:43<04:49,  1.65s/it]Running Inference:  42%|████▏     | 126/300 [03:44<04:34,  1.58s/it]Running Inference:  42%|████▏     | 127/300 [03:47<05:08,  1.79s/it]Running Inference:  43%|████▎     | 128/300 [03:47<04:07,  1.44s/it]Running Inference:  43%|████▎     | 129/300 [03:48<03:49,  1.34s/it]Running Inference:  43%|████▎     | 130/300 [03:50<03:40,  1.30s/it]Running Inference:  44%|████▎     | 131/300 [03:51<03:16,  1.16s/it]Running Inference:  44%|████▍     | 132/300 [03:51<02:56,  1.05s/it]Running Inference:  44%|████▍     | 133/300 [03:53<03:19,  1.20s/it]Running Inference:  45%|████▍     | 134/300 [03:55<04:23,  1.58s/it]Running Inference:  45%|████▌     | 135/300 [03:56<03:34,  1.30s/it]Running Inference:  45%|████▌     | 136/300 [03:59<05:12,  1.90s/it]Running Inference:  46%|████▌     | 137/300 [04:01<05:11,  1.91s/it]Running Inference:  46%|████▌     | 138/300 [04:02<04:26,  1.65s/it]Running Inference:  46%|████▋     | 139/300 [04:03<04:00,  1.50s/it]Running Inference:  47%|████▋     | 140/300 [04:04<03:33,  1.33s/it]Running Inference:  47%|████▋     | 141/300 [04:06<03:53,  1.47s/it]Running Inference:  47%|████▋     | 142/300 [04:08<03:54,  1.48s/it]Running Inference:  48%|████▊     | 143/300 [04:09<03:38,  1.39s/it]Running Inference:  48%|████▊     | 144/300 [04:11<04:16,  1.65s/it]Running Inference:  48%|████▊     | 145/300 [04:12<03:53,  1.51s/it]Running Inference:  49%|████▊     | 146/300 [04:14<03:42,  1.45s/it]Running Inference:  49%|████▉     | 147/300 [04:14<03:10,  1.24s/it]Running Inference:  49%|████▉     | 148/300 [04:17<03:54,  1.55s/it]Running Inference:  50%|████▉     | 149/300 [04:19<04:19,  1.72s/it]Running Inference:  50%|█████     | 150/300 [04:20<03:58,  1.59s/it]Running Inference:  50%|█████     | 151/300 [04:21<03:22,  1.36s/it]Running Inference:  51%|█████     | 152/300 [04:22<02:59,  1.21s/it]Running Inference:  51%|█████     | 153/300 [04:23<02:59,  1.22s/it]Running Inference:  51%|█████▏    | 154/300 [04:25<03:47,  1.56s/it]Running Inference:  52%|█████▏    | 155/300 [04:26<03:17,  1.37s/it]Running Inference:  52%|█████▏    | 156/300 [04:27<02:49,  1.18s/it]Running Inference:  52%|█████▏    | 157/300 [04:28<02:28,  1.04s/it]Running Inference:  53%|█████▎    | 158/300 [04:31<04:26,  1.88s/it]Running Inference:  53%|█████▎    | 159/300 [04:32<03:43,  1.58s/it]Running Inference:  53%|█████▎    | 160/300 [04:35<04:27,  1.91s/it]Running Inference:  54%|█████▎    | 161/300 [04:38<04:52,  2.11s/it]Running Inference:  54%|█████▍    | 162/300 [04:41<05:57,  2.59s/it]Running Inference:  54%|█████▍    | 163/300 [04:43<05:28,  2.40s/it]Running Inference:  55%|█████▍    | 164/300 [04:47<06:37,  2.93s/it]Running Inference:  55%|█████▌    | 165/300 [04:48<05:05,  2.26s/it]Running Inference:  55%|█████▌    | 166/300 [04:49<04:09,  1.86s/it]Running Inference:  56%|█████▌    | 167/300 [04:50<03:28,  1.56s/it]Running Inference:  56%|█████▌    | 168/300 [04:51<03:08,  1.42s/it]Running Inference:  56%|█████▋    | 169/300 [04:55<04:49,  2.21s/it]Running Inference:  57%|█████▋    | 170/300 [04:56<04:09,  1.92s/it]Running Inference:  57%|█████▋    | 171/300 [05:01<05:38,  2.63s/it]Running Inference:  57%|█████▋    | 172/300 [05:03<05:20,  2.51s/it]Running Inference:  58%|█████▊    | 173/300 [05:05<05:09,  2.44s/it]Running Inference:  58%|█████▊    | 174/300 [05:06<04:00,  1.91s/it]Running Inference:  58%|█████▊    | 175/300 [05:07<03:24,  1.64s/it]Running Inference:  59%|█████▊    | 176/300 [05:09<03:43,  1.80s/it]Running Inference:  59%|█████▉    | 177/300 [05:13<05:02,  2.46s/it]Running Inference:  59%|█████▉    | 178/300 [05:15<04:36,  2.27s/it]Running Inference:  60%|█████▉    | 179/300 [05:16<03:40,  1.82s/it]Running Inference:  60%|██████    | 180/300 [05:19<04:26,  2.22s/it]Running Inference:  60%|██████    | 181/300 [05:19<03:24,  1.72s/it]Running Inference:  61%|██████    | 182/300 [05:23<04:20,  2.21s/it]Running Inference:  61%|██████    | 183/300 [05:24<03:49,  1.96s/it]Running Inference:  61%|██████▏   | 184/300 [05:28<05:06,  2.65s/it]Running Inference:  62%|██████▏   | 185/300 [05:30<04:18,  2.25s/it]Running Inference:  62%|██████▏   | 186/300 [05:31<03:49,  2.01s/it]Running Inference:  62%|██████▏   | 187/300 [05:32<03:05,  1.64s/it]Running Inference:  63%|██████▎   | 188/300 [05:35<03:45,  2.01s/it]Running Inference:  63%|██████▎   | 189/300 [05:35<02:58,  1.61s/it]Running Inference:  63%|██████▎   | 190/300 [05:36<02:24,  1.31s/it]Running Inference:  64%|██████▎   | 191/300 [05:37<02:08,  1.18s/it]Running Inference:  64%|██████▍   | 192/300 [05:41<03:35,  1.99s/it]Running Inference:  64%|██████▍   | 193/300 [05:42<03:16,  1.83s/it]Running Inference:  65%|██████▍   | 194/300 [05:43<02:54,  1.65s/it]Running Inference:  65%|██████▌   | 195/300 [05:44<02:26,  1.40s/it]Running Inference:  65%|██████▌   | 196/300 [05:45<02:00,  1.16s/it]Running Inference:  66%|██████▌   | 197/300 [05:46<02:12,  1.28s/it]Running Inference:  66%|██████▌   | 198/300 [05:47<01:49,  1.08s/it]Running Inference:  66%|██████▋   | 199/300 [05:51<03:25,  2.04s/it]Running Inference:  67%|██████▋   | 200/300 [05:56<04:30,  2.70s/it]Running Inference:  67%|██████▋   | 201/300 [05:56<03:29,  2.12s/it]Running Inference:  67%|██████▋   | 202/300 [05:59<03:51,  2.36s/it]Running Inference:  68%|██████▊   | 203/300 [06:00<03:07,  1.93s/it]Running Inference:  68%|██████▊   | 204/300 [06:04<04:01,  2.51s/it]Running Inference:  68%|██████▊   | 205/300 [06:05<03:16,  2.07s/it]Running Inference:  69%|██████▊   | 206/300 [06:07<03:09,  2.01s/it]Running Inference:  69%|██████▉   | 207/300 [06:08<02:41,  1.73s/it]Running Inference:  69%|██████▉   | 208/300 [06:11<03:06,  2.02s/it]Running Inference:  70%|██████▉   | 209/300 [06:14<03:33,  2.34s/it]Running Inference:  70%|███████   | 210/300 [06:17<03:54,  2.60s/it]Running Inference:  70%|███████   | 211/300 [06:19<03:22,  2.27s/it]Running Inference:  71%|███████   | 212/300 [06:20<02:53,  1.97s/it]Running Inference:  71%|███████   | 213/300 [06:20<02:17,  1.58s/it]Running Inference:  71%|███████▏  | 214/300 [06:23<02:50,  1.98s/it]Running Inference:  72%|███████▏  | 215/300 [06:26<02:55,  2.06s/it]Running Inference:  72%|███████▏  | 216/300 [06:27<02:43,  1.95s/it]Running Inference:  72%|███████▏  | 217/300 [06:28<02:13,  1.61s/it]Running Inference:  73%|███████▎  | 218/300 [06:29<01:48,  1.32s/it]Running Inference:  73%|███████▎  | 219/300 [06:30<01:42,  1.26s/it]Running Inference:  73%|███████▎  | 220/300 [06:31<01:32,  1.16s/it]Running Inference:  74%|███████▎  | 221/300 [06:32<01:36,  1.22s/it]Running Inference:  74%|███████▍  | 222/300 [06:34<01:50,  1.41s/it]Running Inference:  74%|███████▍  | 223/300 [06:35<01:36,  1.26s/it]Running Inference:  75%|███████▍  | 224/300 [06:38<02:10,  1.72s/it]Running Inference:  75%|███████▌  | 225/300 [06:41<02:37,  2.10s/it]Running Inference:  75%|███████▌  | 226/300 [06:43<02:50,  2.30s/it]Running Inference:  76%|███████▌  | 227/300 [06:45<02:26,  2.01s/it]Running Inference:  76%|███████▌  | 228/300 [06:45<01:49,  1.53s/it]Running Inference:  76%|███████▋  | 229/300 [06:46<01:36,  1.35s/it]Running Inference:  77%|███████▋  | 230/300 [06:50<02:32,  2.18s/it]Running Inference:  77%|███████▋  | 231/300 [06:51<01:57,  1.71s/it]Running Inference:  77%|███████▋  | 232/300 [06:52<01:34,  1.39s/it]Running Inference:  78%|███████▊  | 233/300 [06:53<01:33,  1.39s/it]Running Inference:  78%|███████▊  | 234/300 [06:54<01:17,  1.17s/it]Running Inference:  78%|███████▊  | 235/300 [06:55<01:15,  1.16s/it]Running Inference:  79%|███████▊  | 236/300 [06:58<01:52,  1.75s/it]Running Inference:  79%|███████▉  | 237/300 [07:00<01:53,  1.80s/it]Running Inference:  79%|███████▉  | 238/300 [07:01<01:48,  1.75s/it]Running Inference:  80%|███████▉  | 239/300 [07:03<01:46,  1.75s/it]Running Inference:  80%|████████  | 240/300 [07:05<01:55,  1.92s/it]Running Inference:  80%|████████  | 241/300 [07:08<02:00,  2.04s/it]Running Inference:  81%|████████  | 242/300 [07:09<01:39,  1.72s/it]Running Inference:  81%|████████  | 243/300 [07:11<01:54,  2.01s/it]Running Inference:  81%|████████▏ | 244/300 [07:12<01:27,  1.56s/it]Running Inference:  82%|████████▏ | 245/300 [07:16<02:02,  2.22s/it]Running Inference:  82%|████████▏ | 246/300 [07:17<01:40,  1.86s/it]Running Inference:  82%|████████▏ | 247/300 [07:17<01:17,  1.46s/it]Running Inference:  83%|████████▎ | 248/300 [07:19<01:25,  1.64s/it]Running Inference:  83%|████████▎ | 249/300 [07:20<01:15,  1.48s/it]Running Inference:  83%|████████▎ | 250/300 [07:22<01:14,  1.49s/it]Running Inference:  84%|████████▎ | 251/300 [07:24<01:17,  1.59s/it]Running Inference:  84%|████████▍ | 252/300 [07:24<01:00,  1.27s/it]Running Inference:  84%|████████▍ | 253/300 [07:26<01:07,  1.43s/it]Running Inference:  85%|████████▍ | 254/300 [07:27<00:55,  1.21s/it]Running Inference:  85%|████████▌ | 255/300 [07:28<00:48,  1.08s/it]Running Inference:  85%|████████▌ | 256/300 [07:28<00:44,  1.01s/it]Running Inference:  86%|████████▌ | 257/300 [07:31<01:09,  1.62s/it]Running Inference:  86%|████████▌ | 258/300 [07:34<01:14,  1.77s/it]Running Inference:  86%|████████▋ | 259/300 [07:35<01:08,  1.67s/it]Running Inference:  87%|████████▋ | 260/300 [07:37<01:16,  1.90s/it]Running Inference:  87%|████████▋ | 261/300 [07:38<01:03,  1.63s/it]Running Inference:  87%|████████▋ | 262/300 [07:39<00:50,  1.33s/it]Running Inference:  88%|████████▊ | 263/300 [07:40<00:43,  1.19s/it]Running Inference:  88%|████████▊ | 264/300 [07:41<00:37,  1.04s/it]Running Inference:  88%|████████▊ | 265/300 [07:42<00:35,  1.03s/it]Running Inference:  89%|████████▊ | 266/300 [07:42<00:33,  1.02it/s]Running Inference:  89%|████████▉ | 267/300 [07:43<00:28,  1.16it/s]Running Inference:  89%|████████▉ | 268/300 [07:46<00:51,  1.60s/it]Running Inference:  90%|████████▉ | 269/300 [07:51<01:13,  2.36s/it]Running Inference:  90%|█████████ | 270/300 [07:51<00:56,  1.88s/it]Running Inference:  90%|█████████ | 271/300 [07:53<00:53,  1.84s/it]Running Inference:  91%|█████████ | 272/300 [07:55<00:53,  1.90s/it]Running Inference:  91%|█████████ | 273/300 [07:59<01:04,  2.38s/it]Running Inference:  91%|█████████▏| 274/300 [08:01<01:05,  2.54s/it]Running Inference:  92%|█████████▏| 275/300 [08:02<00:50,  2.03s/it]Running Inference:  92%|█████████▏| 276/300 [08:04<00:47,  1.98s/it]Running Inference:  92%|█████████▏| 277/300 [08:08<00:58,  2.53s/it]Running Inference:  93%|█████████▎| 278/300 [08:09<00:43,  1.97s/it]Running Inference:  93%|█████████▎| 279/300 [08:11<00:44,  2.14s/it]Running Inference:  93%|█████████▎| 280/300 [08:13<00:43,  2.15s/it]Running Inference:  94%|█████████▎| 281/300 [08:16<00:44,  2.33s/it]Running Inference:  94%|█████████▍| 282/300 [08:18<00:41,  2.33s/it]Running Inference:  94%|█████████▍| 283/300 [08:20<00:37,  2.21s/it]Running Inference:  95%|█████████▍| 284/300 [08:23<00:38,  2.38s/it]Running Inference:  95%|█████████▌| 285/300 [08:24<00:30,  2.03s/it]Running Inference:  95%|█████████▌| 286/300 [08:25<00:23,  1.68s/it]Running Inference:  96%|█████████▌| 287/300 [08:29<00:31,  2.44s/it]Running Inference:  96%|█████████▌| 288/300 [08:32<00:30,  2.55s/it]Running Inference:  96%|█████████▋| 289/300 [08:36<00:33,  3.03s/it]Running Inference:  97%|█████████▋| 290/300 [08:38<00:26,  2.63s/it]Running Inference:  97%|█████████▋| 291/300 [08:39<00:18,  2.05s/it]Running Inference:  97%|█████████▋| 292/300 [08:41<00:16,  2.06s/it]Running Inference:  98%|█████████▊| 293/300 [08:44<00:16,  2.40s/it]Running Inference:  98%|█████████▊| 294/300 [08:46<00:13,  2.31s/it]Running Inference:  98%|█████████▊| 295/300 [08:47<00:09,  1.81s/it]Running Inference:  99%|█████████▊| 296/300 [08:48<00:06,  1.53s/it]Running Inference:  99%|█████████▉| 297/300 [08:49<00:04,  1.50s/it]Running Inference:  99%|█████████▉| 298/300 [08:50<00:02,  1.41s/it]Running Inference: 100%|█████████▉| 299/300 [08:52<00:01,  1.36s/it]Running Inference: 100%|██████████| 300/300 [08:54<00:00,  1.54s/it]Running Inference: 100%|██████████| 300/300 [08:54<00:00,  1.78s/it]
2025-12-15 01:21:56,800 - INFO - Inference completed.
2025-12-15 01:21:56,810 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-15 01:21:56,810 - INFO - Calculating metrics for dataset: longbench
2025-12-15 01:21:56,816 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-15 01:21:56,816 - INFO - Metrics:
28.67
2025-12-15 01:21:56,817 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa_e, ratio=0.3) on GPU 1

----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 01:22:03,260 - INFO - Set deterministic seeds to 42
2025-12-15 01:22:03,260 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 01:22:03,260 - INFO - Starting evaluation run...
2025-12-15 01:22:03,260 - INFO - Output directory set to: longbenchresult
2025-12-15 01:22:03,261 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-15 01:22:03,261 - INFO - KV Press 'tova' setup.
2025-12-15 01:22:03,261 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 01:22:03,261 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.76it/s]
Device set to use cuda:0
2025-12-15 01:22:15,530 - INFO - Model pipeline loaded.
2025-12-15 01:22:15,530 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
2025-12-15 01:22:19,024 - INFO - Dataset loaded with 300 entries.
2025-12-15 01:22:19,024 - INFO - Dataset processed with 300 entries.
2025-12-15 01:22:19,063 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:01<07:23,  1.48s/it]Running Inference:   1%|          | 2/300 [00:02<06:17,  1.27s/it]Running Inference:   1%|          | 3/300 [00:03<05:11,  1.05s/it]Running Inference:   1%|▏         | 4/300 [00:04<06:11,  1.26s/it]Running Inference:   2%|▏         | 5/300 [00:05<05:40,  1.15s/it]Running Inference:   2%|▏         | 6/300 [00:06<04:38,  1.06it/s]Running Inference:   2%|▏         | 7/300 [00:08<06:18,  1.29s/it]Running Inference:   3%|▎         | 8/300 [00:09<05:55,  1.22s/it]Running Inference:   3%|▎         | 9/300 [00:12<08:32,  1.76s/it]Running Inference:   3%|▎         | 10/300 [00:15<10:09,  2.10s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:16<08:17,  1.72s/it]Running Inference:   4%|▍         | 12/300 [00:19<10:10,  2.12s/it]Running Inference:   4%|▍         | 13/300 [00:20<08:37,  1.80s/it]Running Inference:   5%|▍         | 14/300 [00:22<09:50,  2.07s/it]Running Inference:   5%|▌         | 15/300 [00:24<08:18,  1.75s/it]Running Inference:   5%|▌         | 16/300 [00:26<09:08,  1.93s/it]Running Inference:   6%|▌         | 17/300 [00:30<11:39,  2.47s/it]Running Inference:   6%|▌         | 18/300 [00:30<09:11,  1.95s/it]Running Inference:   6%|▋         | 19/300 [00:32<09:07,  1.95s/it]Running Inference:   7%|▋         | 20/300 [00:33<06:58,  1.49s/it]Running Inference:   7%|▋         | 21/300 [00:35<07:44,  1.67s/it]Running Inference:   7%|▋         | 22/300 [00:38<09:42,  2.10s/it]Running Inference:   8%|▊         | 23/300 [00:41<11:10,  2.42s/it]Running Inference:   8%|▊         | 24/300 [00:44<11:58,  2.60s/it]Running Inference:   8%|▊         | 25/300 [00:45<09:10,  2.00s/it]Running Inference:   9%|▊         | 26/300 [00:46<08:01,  1.76s/it]Running Inference:   9%|▉         | 27/300 [00:47<06:51,  1.51s/it]Running Inference:   9%|▉         | 28/300 [00:48<06:34,  1.45s/it]Running Inference:  10%|▉         | 29/300 [00:49<06:12,  1.38s/it]Running Inference:  10%|█         | 30/300 [00:51<07:05,  1.58s/it]Running Inference:  10%|█         | 31/300 [00:54<08:45,  1.95s/it]Running Inference:  11%|█         | 32/300 [00:56<08:48,  1.97s/it]Running Inference:  11%|█         | 33/300 [01:00<11:48,  2.65s/it]Running Inference:  11%|█▏        | 34/300 [01:03<11:08,  2.51s/it]Running Inference:  12%|█▏        | 35/300 [01:03<08:20,  1.89s/it]Running Inference:  12%|█▏        | 36/300 [01:07<11:18,  2.57s/it]Running Inference:  12%|█▏        | 37/300 [01:09<10:19,  2.35s/it]Running Inference:  13%|█▎        | 38/300 [01:11<09:21,  2.14s/it]Running Inference:  13%|█▎        | 39/300 [01:11<07:26,  1.71s/it]Running Inference:  13%|█▎        | 40/300 [01:12<05:58,  1.38s/it]Running Inference:  14%|█▎        | 41/300 [01:14<07:06,  1.65s/it]Running Inference:  14%|█▍        | 42/300 [01:15<06:28,  1.50s/it]Running Inference:  14%|█▍        | 43/300 [01:18<07:09,  1.67s/it]Running Inference:  15%|█▍        | 44/300 [01:19<06:15,  1.47s/it]Running Inference:  15%|█▌        | 45/300 [01:21<07:07,  1.68s/it]Running Inference:  15%|█▌        | 46/300 [01:23<07:31,  1.78s/it]Running Inference:  16%|█▌        | 47/300 [01:23<05:55,  1.41s/it]Running Inference:  16%|█▌        | 48/300 [01:26<08:00,  1.90s/it]Running Inference:  16%|█▋        | 49/300 [01:29<08:52,  2.12s/it]Running Inference:  17%|█▋        | 50/300 [01:30<08:08,  1.95s/it]Running Inference:  17%|█▋        | 51/300 [01:32<07:14,  1.75s/it]Running Inference:  17%|█▋        | 52/300 [01:35<08:46,  2.12s/it]Running Inference:  18%|█▊        | 53/300 [01:36<07:22,  1.79s/it]Running Inference:  18%|█▊        | 54/300 [01:36<06:00,  1.47s/it]Running Inference:  18%|█▊        | 55/300 [01:38<06:05,  1.49s/it]Running Inference:  19%|█▊        | 56/300 [01:40<07:04,  1.74s/it]Running Inference:  19%|█▉        | 57/300 [01:43<08:13,  2.03s/it]Running Inference:  19%|█▉        | 58/300 [01:44<07:20,  1.82s/it]Running Inference:  20%|█▉        | 59/300 [01:45<05:57,  1.48s/it]Running Inference:  20%|██        | 60/300 [01:48<07:55,  1.98s/it]Running Inference:  20%|██        | 61/300 [01:49<06:45,  1.69s/it]Running Inference:  21%|██        | 62/300 [01:51<07:12,  1.82s/it]Running Inference:  21%|██        | 63/300 [01:54<08:21,  2.12s/it]Running Inference:  21%|██▏       | 64/300 [01:55<06:23,  1.62s/it]Running Inference:  22%|██▏       | 65/300 [01:59<09:09,  2.34s/it]Running Inference:  22%|██▏       | 66/300 [01:59<07:21,  1.89s/it]Running Inference:  22%|██▏       | 67/300 [02:01<06:28,  1.67s/it]Running Inference:  23%|██▎       | 68/300 [02:03<07:39,  1.98s/it]Running Inference:  23%|██▎       | 69/300 [02:05<07:07,  1.85s/it]Running Inference:  23%|██▎       | 70/300 [02:06<06:06,  1.59s/it]Running Inference:  24%|██▎       | 71/300 [02:07<05:29,  1.44s/it]Running Inference:  24%|██▍       | 72/300 [02:08<05:06,  1.34s/it]Running Inference:  24%|██▍       | 73/300 [02:09<04:59,  1.32s/it]Running Inference:  25%|██▍       | 74/300 [02:10<04:36,  1.22s/it]Running Inference:  25%|██▌       | 75/300 [02:11<04:09,  1.11s/it]Running Inference:  25%|██▌       | 76/300 [02:12<03:47,  1.01s/it]Running Inference:  26%|██▌       | 77/300 [02:14<05:07,  1.38s/it]Running Inference:  26%|██▌       | 78/300 [02:18<07:37,  2.06s/it]Running Inference:  26%|██▋       | 79/300 [02:19<06:16,  1.70s/it]Running Inference:  27%|██▋       | 80/300 [02:20<05:14,  1.43s/it]Running Inference:  27%|██▋       | 81/300 [02:21<05:22,  1.47s/it]Running Inference:  27%|██▋       | 82/300 [02:22<04:33,  1.26s/it]Running Inference:  28%|██▊       | 83/300 [02:25<06:40,  1.85s/it]Running Inference:  28%|██▊       | 84/300 [02:28<07:32,  2.09s/it]Running Inference:  28%|██▊       | 85/300 [02:30<07:25,  2.07s/it]Running Inference:  29%|██▊       | 86/300 [02:31<06:23,  1.79s/it]Running Inference:  29%|██▉       | 87/300 [02:33<06:56,  1.96s/it]Running Inference:  29%|██▉       | 88/300 [02:37<08:52,  2.51s/it]Running Inference:  30%|██▉       | 89/300 [02:39<08:45,  2.49s/it]Running Inference:  30%|███       | 90/300 [02:41<07:25,  2.12s/it]Running Inference:  30%|███       | 91/300 [02:44<08:27,  2.43s/it]Running Inference:  31%|███       | 92/300 [02:45<06:47,  1.96s/it]Running Inference:  31%|███       | 93/300 [02:47<06:42,  1.95s/it]Running Inference:  31%|███▏      | 94/300 [02:48<05:54,  1.72s/it]Running Inference:  32%|███▏      | 95/300 [02:52<08:28,  2.48s/it]Running Inference:  32%|███▏      | 96/300 [02:53<06:59,  2.06s/it]Running Inference:  32%|███▏      | 97/300 [02:54<05:51,  1.73s/it]Running Inference:  33%|███▎      | 98/300 [02:57<07:13,  2.15s/it]Running Inference:  33%|███▎      | 99/300 [03:00<07:19,  2.18s/it]Running Inference:  33%|███▎      | 100/300 [03:00<06:01,  1.81s/it]Running Inference:  34%|███▎      | 101/300 [03:01<05:02,  1.52s/it]Running Inference:  34%|███▍      | 102/300 [03:02<04:27,  1.35s/it]Running Inference:  34%|███▍      | 103/300 [03:04<04:27,  1.36s/it]Running Inference:  35%|███▍      | 104/300 [03:06<05:27,  1.67s/it]Running Inference:  35%|███▌      | 105/300 [03:07<04:36,  1.42s/it]Running Inference:  35%|███▌      | 106/300 [03:08<03:59,  1.24s/it]Running Inference:  36%|███▌      | 107/300 [03:10<05:20,  1.66s/it]Running Inference:  36%|███▌      | 108/300 [03:13<06:39,  2.08s/it]Running Inference:  36%|███▋      | 109/300 [03:15<05:44,  1.81s/it]Running Inference:  37%|███▋      | 110/300 [03:18<07:40,  2.42s/it]Running Inference:  37%|███▋      | 111/300 [03:23<09:21,  2.97s/it]Running Inference:  37%|███▋      | 112/300 [03:23<07:14,  2.31s/it]Running Inference:  38%|███▊      | 113/300 [03:24<05:38,  1.81s/it]Running Inference:  38%|███▊      | 114/300 [03:26<05:27,  1.76s/it]Running Inference:  38%|███▊      | 115/300 [03:28<05:43,  1.86s/it]Running Inference:  39%|███▊      | 116/300 [03:32<07:31,  2.45s/it]Running Inference:  39%|███▉      | 117/300 [03:34<07:26,  2.44s/it]Running Inference:  39%|███▉      | 118/300 [03:35<06:02,  1.99s/it]Running Inference:  40%|███▉      | 119/300 [03:36<04:59,  1.65s/it]Running Inference:  40%|████      | 120/300 [03:36<03:58,  1.33s/it]Running Inference:  40%|████      | 121/300 [03:38<04:12,  1.41s/it]Running Inference:  41%|████      | 122/300 [03:39<03:38,  1.23s/it]Running Inference:  41%|████      | 123/300 [03:40<03:21,  1.14s/it]Running Inference:  41%|████▏     | 124/300 [03:42<03:50,  1.31s/it]Running Inference:  42%|████▏     | 125/300 [03:43<04:18,  1.48s/it]Running Inference:  42%|████▏     | 126/300 [03:45<04:11,  1.45s/it]Running Inference:  42%|████▏     | 127/300 [03:47<04:52,  1.69s/it]Running Inference:  43%|████▎     | 128/300 [03:48<04:07,  1.44s/it]Running Inference:  43%|████▎     | 129/300 [03:49<03:49,  1.34s/it]Running Inference:  43%|████▎     | 130/300 [03:50<03:40,  1.30s/it]Running Inference:  44%|████▎     | 131/300 [03:53<04:37,  1.64s/it]Running Inference:  44%|████▍     | 132/300 [03:53<03:53,  1.39s/it]Running Inference:  44%|████▍     | 133/300 [03:55<03:59,  1.43s/it]Running Inference:  45%|████▍     | 134/300 [03:57<04:50,  1.75s/it]Running Inference:  45%|████▌     | 135/300 [03:58<03:53,  1.41s/it]Running Inference:  45%|████▌     | 136/300 [04:01<05:25,  1.99s/it]Running Inference:  46%|████▌     | 137/300 [04:03<05:27,  2.01s/it]Running Inference:  46%|████▌     | 138/300 [04:04<04:37,  1.71s/it]Running Inference:  46%|████▋     | 139/300 [04:06<04:08,  1.54s/it]Running Inference:  47%|████▋     | 140/300 [04:07<03:37,  1.36s/it]Running Inference:  47%|████▋     | 141/300 [04:08<03:56,  1.49s/it]Running Inference:  47%|████▋     | 142/300 [04:10<03:56,  1.50s/it]Running Inference:  48%|████▊     | 143/300 [04:11<03:40,  1.40s/it]Running Inference:  48%|████▊     | 144/300 [04:13<04:17,  1.65s/it]Running Inference:  48%|████▊     | 145/300 [04:14<03:54,  1.51s/it]Running Inference:  49%|████▊     | 146/300 [04:16<03:43,  1.45s/it]Running Inference:  49%|████▉     | 147/300 [04:17<03:10,  1.25s/it]Running Inference:  49%|████▉     | 148/300 [04:21<05:29,  2.17s/it]Running Inference:  50%|████▉     | 149/300 [04:23<05:25,  2.16s/it]Running Inference:  50%|█████     | 150/300 [04:24<04:44,  1.89s/it]Running Inference:  50%|█████     | 151/300 [04:25<03:54,  1.58s/it]Running Inference:  51%|█████     | 152/300 [04:26<03:23,  1.37s/it]Running Inference:  51%|█████     | 153/300 [04:27<03:15,  1.33s/it]Running Inference:  51%|█████▏    | 154/300 [04:30<03:59,  1.64s/it]Running Inference:  52%|█████▏    | 155/300 [04:31<03:25,  1.42s/it]Running Inference:  52%|█████▏    | 156/300 [04:31<02:55,  1.22s/it]Running Inference:  52%|█████▏    | 157/300 [04:32<02:31,  1.06s/it]Running Inference:  53%|█████▎    | 158/300 [04:36<04:30,  1.91s/it]Running Inference:  53%|█████▎    | 159/300 [04:37<03:45,  1.60s/it]Running Inference:  53%|█████▎    | 160/300 [04:39<04:30,  1.93s/it]Running Inference:  54%|█████▎    | 161/300 [04:42<04:54,  2.12s/it]Running Inference:  54%|█████▍    | 162/300 [04:46<05:59,  2.60s/it]Running Inference:  54%|█████▍    | 163/300 [04:48<05:30,  2.41s/it]Running Inference:  55%|█████▍    | 164/300 [04:52<06:39,  2.94s/it]Running Inference:  55%|█████▌    | 165/300 [04:53<05:06,  2.27s/it]Running Inference:  55%|█████▌    | 166/300 [04:53<04:09,  1.86s/it]Running Inference:  56%|█████▌    | 167/300 [04:54<03:28,  1.57s/it]Running Inference:  56%|█████▌    | 168/300 [04:55<03:08,  1.43s/it]Running Inference:  56%|█████▋    | 169/300 [05:00<04:51,  2.22s/it]Running Inference:  57%|█████▋    | 170/300 [05:01<04:10,  1.93s/it]Running Inference:  57%|█████▋    | 171/300 [05:05<05:39,  2.64s/it]Running Inference:  57%|█████▋    | 172/300 [05:07<05:21,  2.51s/it]Running Inference:  58%|█████▊    | 173/300 [05:10<05:13,  2.47s/it]Running Inference:  58%|█████▊    | 174/300 [05:10<04:02,  1.93s/it]Running Inference:  58%|█████▊    | 175/300 [05:11<03:26,  1.65s/it]Running Inference:  59%|█████▊    | 176/300 [05:14<03:55,  1.90s/it]Running Inference:  59%|█████▉    | 177/300 [05:18<05:11,  2.54s/it]Running Inference:  59%|█████▉    | 178/300 [05:20<04:42,  2.32s/it]Running Inference:  60%|█████▉    | 179/300 [05:20<03:44,  1.85s/it]Running Inference:  60%|██████    | 180/300 [05:22<03:27,  1.73s/it]Running Inference:  60%|██████    | 181/300 [05:22<02:43,  1.37s/it]Running Inference:  61%|██████    | 182/300 [05:26<03:53,  1.98s/it]Running Inference:  61%|██████    | 183/300 [05:27<03:30,  1.80s/it]Running Inference:  61%|██████▏   | 184/300 [05:31<04:54,  2.53s/it]Running Inference:  62%|██████▏   | 185/300 [05:33<04:09,  2.17s/it]Running Inference:  62%|██████▏   | 186/300 [05:34<03:34,  1.88s/it]Running Inference:  62%|██████▏   | 187/300 [05:35<02:55,  1.55s/it]Running Inference:  63%|██████▎   | 188/300 [05:38<03:38,  1.95s/it]Running Inference:  63%|██████▎   | 189/300 [05:38<02:54,  1.57s/it]Running Inference:  63%|██████▎   | 190/300 [05:39<02:21,  1.29s/it]Running Inference:  64%|██████▎   | 191/300 [05:40<02:06,  1.16s/it]Running Inference:  64%|██████▍   | 192/300 [05:42<02:40,  1.49s/it]Running Inference:  64%|██████▍   | 193/300 [05:45<03:34,  2.00s/it]Running Inference:  65%|██████▍   | 194/300 [05:46<03:07,  1.77s/it]Running Inference:  65%|██████▌   | 195/300 [05:49<03:35,  2.06s/it]Running Inference:  65%|██████▌   | 196/300 [05:50<02:48,  1.62s/it]Running Inference:  66%|██████▌   | 197/300 [05:51<02:45,  1.61s/it]Running Inference:  66%|██████▌   | 198/300 [05:52<02:13,  1.31s/it]Running Inference:  66%|██████▋   | 199/300 [05:55<02:58,  1.77s/it]Running Inference:  67%|██████▋   | 200/300 [05:59<04:11,  2.52s/it]Running Inference:  67%|██████▋   | 201/300 [06:00<03:18,  2.00s/it]Running Inference:  67%|██████▋   | 202/300 [06:03<03:44,  2.29s/it]Running Inference:  68%|██████▊   | 203/300 [06:04<03:01,  1.88s/it]Running Inference:  68%|██████▊   | 204/300 [06:06<03:07,  1.96s/it]Running Inference:  68%|██████▊   | 205/300 [06:07<02:39,  1.68s/it]Running Inference:  69%|██████▊   | 206/300 [06:09<02:43,  1.74s/it]Running Inference:  69%|██████▉   | 207/300 [06:10<02:23,  1.54s/it]Running Inference:  69%|██████▉   | 208/300 [06:11<02:00,  1.31s/it]Running Inference:  70%|██████▉   | 209/300 [06:12<02:08,  1.41s/it]Running Inference:  70%|███████   | 210/300 [06:16<02:56,  1.96s/it]Running Inference:  70%|███████   | 211/300 [06:17<02:30,  1.69s/it]Running Inference:  71%|███████   | 212/300 [06:18<02:17,  1.56s/it]Running Inference:  71%|███████   | 213/300 [06:19<01:53,  1.30s/it]Running Inference:  71%|███████▏  | 214/300 [06:20<01:54,  1.33s/it]Running Inference:  72%|███████▏  | 215/300 [06:22<02:22,  1.67s/it]Running Inference:  72%|███████▏  | 216/300 [06:24<02:20,  1.67s/it]Running Inference:  72%|███████▏  | 217/300 [06:25<01:57,  1.42s/it]Running Inference:  73%|███████▎  | 218/300 [06:26<01:37,  1.18s/it]Running Inference:  73%|███████▎  | 219/300 [06:27<01:34,  1.17s/it]Running Inference:  73%|███████▎  | 220/300 [06:28<01:27,  1.09s/it]Running Inference:  74%|███████▎  | 221/300 [06:29<01:32,  1.17s/it]Running Inference:  74%|███████▍  | 222/300 [06:31<01:47,  1.38s/it]Running Inference:  74%|███████▍  | 223/300 [06:32<01:35,  1.23s/it]Running Inference:  75%|███████▍  | 224/300 [06:35<02:09,  1.71s/it]Running Inference:  75%|███████▌  | 225/300 [06:38<02:37,  2.10s/it]Running Inference:  75%|███████▌  | 226/300 [06:39<02:23,  1.95s/it]Running Inference:  76%|███████▌  | 227/300 [06:40<02:05,  1.71s/it]Running Inference:  76%|███████▌  | 228/300 [06:41<01:34,  1.32s/it]Running Inference:  76%|███████▋  | 229/300 [06:42<01:25,  1.21s/it]Running Inference:  77%|███████▋  | 230/300 [06:46<02:25,  2.08s/it]Running Inference:  77%|███████▋  | 231/300 [06:46<01:53,  1.64s/it]Running Inference:  77%|███████▋  | 232/300 [06:47<01:31,  1.35s/it]Running Inference:  78%|███████▊  | 233/300 [06:48<01:30,  1.36s/it]Running Inference:  78%|███████▊  | 234/300 [06:49<01:15,  1.15s/it]Running Inference:  78%|███████▊  | 235/300 [06:50<01:17,  1.19s/it]Running Inference:  79%|███████▊  | 236/300 [06:54<01:54,  1.78s/it]Running Inference:  79%|███████▉  | 237/300 [06:55<01:54,  1.82s/it]Running Inference:  79%|███████▉  | 238/300 [06:57<01:49,  1.77s/it]Running Inference:  80%|███████▉  | 239/300 [06:59<01:47,  1.76s/it]Running Inference:  80%|████████  | 240/300 [07:01<01:55,  1.93s/it]Running Inference:  80%|████████  | 241/300 [07:03<02:00,  2.05s/it]Running Inference:  81%|████████  | 242/300 [07:04<01:40,  1.73s/it]Running Inference:  81%|████████  | 243/300 [07:07<01:54,  2.01s/it]Running Inference:  81%|████████▏ | 244/300 [07:08<01:27,  1.56s/it]Running Inference:  82%|████████▏ | 245/300 [07:10<01:30,  1.65s/it]Running Inference:  82%|████████▏ | 246/300 [07:11<01:18,  1.46s/it]Running Inference:  82%|████████▏ | 247/300 [07:13<01:34,  1.79s/it]Running Inference:  83%|████████▎ | 248/300 [07:15<01:37,  1.87s/it]Running Inference:  83%|████████▎ | 249/300 [07:16<01:23,  1.64s/it]Running Inference:  83%|████████▎ | 250/300 [07:18<01:20,  1.61s/it]Running Inference:  84%|████████▎ | 251/300 [07:20<01:21,  1.67s/it]Running Inference:  84%|████████▍ | 252/300 [07:20<01:03,  1.32s/it]Running Inference:  84%|████████▍ | 253/300 [07:22<01:09,  1.47s/it]Running Inference:  85%|████████▍ | 254/300 [07:23<00:56,  1.23s/it]Running Inference:  85%|████████▌ | 255/300 [07:23<00:50,  1.11s/it]Running Inference:  85%|████████▌ | 256/300 [07:24<00:45,  1.03s/it]Running Inference:  86%|████████▌ | 257/300 [07:27<01:10,  1.64s/it]Running Inference:  86%|████████▌ | 258/300 [07:29<01:14,  1.78s/it]Running Inference:  86%|████████▋ | 259/300 [07:31<01:08,  1.68s/it]Running Inference:  87%|████████▋ | 260/300 [07:33<01:16,  1.92s/it]Running Inference:  87%|████████▋ | 261/300 [07:34<01:03,  1.64s/it]Running Inference:  87%|████████▋ | 262/300 [07:35<00:50,  1.34s/it]Running Inference:  88%|████████▊ | 263/300 [07:36<00:44,  1.19s/it]Running Inference:  88%|████████▊ | 264/300 [07:37<00:38,  1.07s/it]Running Inference:  88%|████████▊ | 265/300 [07:38<00:36,  1.04s/it]Running Inference:  89%|████████▊ | 266/300 [07:40<00:53,  1.57s/it]Running Inference:  89%|████████▉ | 267/300 [07:41<00:42,  1.27s/it]Running Inference:  89%|████████▉ | 268/300 [07:44<01:00,  1.89s/it]Running Inference:  90%|████████▉ | 269/300 [07:47<01:03,  2.06s/it]Running Inference:  90%|█████████ | 270/300 [07:49<01:06,  2.21s/it]Running Inference:  90%|█████████ | 271/300 [07:51<01:00,  2.07s/it]Running Inference:  91%|█████████ | 272/300 [07:53<00:57,  2.06s/it]Running Inference:  91%|█████████ | 273/300 [07:55<00:58,  2.16s/it]Running Inference:  91%|█████████▏| 274/300 [07:57<00:48,  1.87s/it]Running Inference:  92%|█████████▏| 275/300 [07:57<00:38,  1.55s/it]Running Inference:  92%|█████████▏| 276/300 [07:59<00:39,  1.64s/it]Running Inference:  92%|█████████▏| 277/300 [08:03<00:52,  2.30s/it]Running Inference:  93%|█████████▎| 278/300 [08:04<00:39,  1.81s/it]Running Inference:  93%|█████████▎| 279/300 [08:06<00:42,  2.03s/it]Running Inference:  93%|█████████▎| 280/300 [08:08<00:37,  1.89s/it]Running Inference:  94%|█████████▎| 281/300 [08:11<00:40,  2.15s/it]Running Inference:  94%|█████████▍| 282/300 [08:13<00:39,  2.20s/it]Running Inference:  94%|█████████▍| 283/300 [08:15<00:36,  2.12s/it]Running Inference:  95%|█████████▍| 284/300 [08:18<00:37,  2.32s/it]Running Inference:  95%|█████████▌| 285/300 [08:19<00:29,  1.99s/it]Running Inference:  95%|█████████▌| 286/300 [08:20<00:23,  1.64s/it]Running Inference:  96%|█████████▌| 287/300 [08:24<00:31,  2.43s/it]Running Inference:  96%|█████████▌| 288/300 [08:27<00:30,  2.55s/it]Running Inference:  96%|█████████▋| 289/300 [08:31<00:33,  3.03s/it]Running Inference:  97%|█████████▋| 290/300 [08:33<00:26,  2.64s/it]Running Inference:  97%|█████████▋| 291/300 [08:33<00:18,  2.05s/it]Running Inference:  97%|█████████▋| 292/300 [08:35<00:16,  2.06s/it]Running Inference:  98%|█████████▊| 293/300 [08:40<00:18,  2.71s/it]Running Inference:  98%|█████████▊| 294/300 [08:43<00:18,  3.01s/it]Running Inference:  98%|█████████▊| 295/300 [08:46<00:14,  2.93s/it]Running Inference:  99%|█████████▊| 296/300 [08:47<00:09,  2.32s/it]Running Inference:  99%|█████████▉| 297/300 [08:48<00:06,  2.04s/it]Running Inference:  99%|█████████▉| 298/300 [08:50<00:03,  1.79s/it]Running Inference: 100%|█████████▉| 299/300 [08:51<00:01,  1.69s/it]Running Inference: 100%|██████████| 300/300 [08:53<00:00,  1.84s/it]Running Inference: 100%|██████████| 300/300 [08:53<00:00,  1.78s/it]
2025-12-15 01:31:12,882 - INFO - Inference completed.
2025-12-15 01:31:12,892 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-15 01:31:12,892 - INFO - Calculating metrics for dataset: longbench
2025-12-15 01:31:12,898 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-15 01:31:12,898 - INFO - Metrics:
28.66
2025-12-15 01:31:12,899 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=2wikimqa_e, ratio=0.5) on GPU 1


========================================
LongBench Task: gov_report_e
========================================
----------------------------------------
Task: gov_report_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 01:31:19,339 - INFO - Set deterministic seeds to 42
2025-12-15 01:31:19,339 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 01:31:19,339 - INFO - Starting evaluation run...
2025-12-15 01:31:19,339 - INFO - Output directory set to: longbenchresult
2025-12-15 01:31:19,340 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-15 01:31:19,340 - INFO - KV Press 'tova' setup.
2025-12-15 01:31:19,340 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 01:31:19,340 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.55it/s]
Device set to use cuda:0
2025-12-15 01:31:31,800 - INFO - Model pipeline loaded.
2025-12-15 01:31:31,800 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report_e)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 300 examples [00:00, 1242.94 examples/s]Generating test split: 300 examples [00:00, 1238.90 examples/s]
2025-12-15 01:31:35,634 - INFO - Dataset loaded with 300 entries.
2025-12-15 01:31:35,634 - INFO - Dataset processed with 300 entries.
2025-12-15 01:31:35,672 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:12<59:48, 12.00s/it]Running Inference:   1%|          | 2/300 [00:39<1:43:34, 20.85s/it]Running Inference:   1%|          | 3/300 [01:01<1:47:38, 21.75s/it]Running Inference:   1%|▏         | 4/300 [01:24<1:48:58, 22.09s/it]Running Inference:   2%|▏         | 5/300 [01:47<1:51:07, 22.60s/it]Running Inference:   2%|▏         | 6/300 [02:11<1:52:47, 23.02s/it]Running Inference:   2%|▏         | 7/300 [02:34<1:51:49, 22.90s/it]Running Inference:   3%|▎         | 8/300 [02:58<1:53:05, 23.24s/it]Running Inference:   3%|▎         | 9/300 [03:21<1:51:56, 23.08s/it]Running Inference:   3%|▎         | 10/300 [03:44<1:51:15, 23.02s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [04:07<1:51:13, 23.09s/it]Running Inference:   4%|▍         | 12/300 [04:30<1:50:17, 22.98s/it]Running Inference:   4%|▍         | 13/300 [04:53<1:51:04, 23.22s/it]Running Inference:   5%|▍         | 14/300 [05:16<1:49:59, 23.07s/it]Running Inference:   5%|▌         | 15/300 [05:38<1:47:58, 22.73s/it]Running Inference:   5%|▌         | 16/300 [06:02<1:48:51, 23.00s/it]Running Inference:   6%|▌         | 17/300 [06:24<1:47:59, 22.90s/it]Running Inference:   6%|▌         | 18/300 [06:48<1:48:11, 23.02s/it]Running Inference:   6%|▋         | 19/300 [07:11<1:47:48, 23.02s/it]Running Inference:   7%|▋         | 20/300 [07:36<1:50:57, 23.78s/it]Running Inference:   7%|▋         | 21/300 [07:59<1:49:05, 23.46s/it]Running Inference:   7%|▋         | 22/300 [08:17<1:41:41, 21.95s/it]Running Inference:   8%|▊         | 23/300 [08:42<1:45:23, 22.83s/it]Running Inference:   8%|▊         | 24/300 [09:05<1:45:15, 22.88s/it]Running Inference:   8%|▊         | 25/300 [09:29<1:46:40, 23.27s/it]Running Inference:   9%|▊         | 26/300 [09:53<1:46:30, 23.32s/it]Running Inference:   9%|▉         | 27/300 [10:16<1:45:22, 23.16s/it]Running Inference:   9%|▉         | 28/300 [10:39<1:44:53, 23.14s/it]Running Inference:  10%|▉         | 29/300 [11:01<1:43:55, 23.01s/it]Running Inference:  10%|█         | 30/300 [11:24<1:43:18, 22.96s/it]Running Inference:  10%|█         | 31/300 [11:49<1:45:46, 23.59s/it]Running Inference:  11%|█         | 32/300 [12:12<1:44:32, 23.41s/it]Running Inference:  11%|█         | 33/300 [12:37<1:45:41, 23.75s/it]Running Inference:  11%|█▏        | 34/300 [13:00<1:44:34, 23.59s/it]Running Inference:  12%|█▏        | 35/300 [13:23<1:43:00, 23.32s/it]Running Inference:  12%|█▏        | 36/300 [13:45<1:41:49, 23.14s/it]Running Inference:  12%|█▏        | 37/300 [14:09<1:41:56, 23.26s/it]Running Inference:  13%|█▎        | 38/300 [14:33<1:42:18, 23.43s/it]Running Inference:  13%|█▎        | 39/300 [14:57<1:42:31, 23.57s/it]Running Inference:  13%|█▎        | 40/300 [15:20<1:42:07, 23.57s/it]Running Inference:  14%|█▎        | 41/300 [15:24<1:16:36, 17.75s/it]Running Inference:  14%|█▍        | 42/300 [15:48<1:23:18, 19.37s/it]Running Inference:  14%|█▍        | 43/300 [16:10<1:27:26, 20.42s/it]Running Inference:  15%|█▍        | 44/300 [16:33<1:30:06, 21.12s/it]Running Inference:  15%|█▌        | 45/300 [16:57<1:32:55, 21.86s/it]Running Inference:  15%|█▌        | 46/300 [17:20<1:34:15, 22.27s/it]Running Inference:  16%|█▌        | 47/300 [17:43<1:34:55, 22.51s/it]Running Inference:  16%|█▌        | 48/300 [18:07<1:36:01, 22.86s/it]Running Inference:  16%|█▋        | 49/300 [18:30<1:35:30, 22.83s/it]Running Inference:  17%|█▋        | 50/300 [18:53<1:36:29, 23.16s/it]Running Inference:  17%|█▋        | 51/300 [19:16<1:35:44, 23.07s/it]Running Inference:  17%|█▋        | 52/300 [19:40<1:35:47, 23.17s/it]Running Inference:  18%|█▊        | 53/300 [20:02<1:34:47, 23.02s/it]Running Inference:  18%|█▊        | 54/300 [20:25<1:34:09, 22.97s/it]Running Inference:  18%|█▊        | 55/300 [20:50<1:35:28, 23.38s/it]Running Inference:  19%|█▊        | 56/300 [21:10<1:31:09, 22.42s/it]Running Inference:  19%|█▉        | 57/300 [21:33<1:31:38, 22.63s/it]Running Inference:  19%|█▉        | 58/300 [21:56<1:31:56, 22.79s/it]Running Inference:  20%|█▉        | 59/300 [22:20<1:32:42, 23.08s/it]Running Inference:  20%|██        | 60/300 [22:43<1:32:23, 23.10s/it]Running Inference:  20%|██        | 61/300 [23:06<1:31:39, 23.01s/it]Running Inference:  21%|██        | 62/300 [23:29<1:31:46, 23.14s/it]Running Inference:  21%|██        | 63/300 [23:52<1:31:09, 23.08s/it]Running Inference:  21%|██▏       | 64/300 [24:15<1:31:00, 23.14s/it]Running Inference:  22%|██▏       | 65/300 [24:39<1:31:22, 23.33s/it]Running Inference:  22%|██▏       | 66/300 [25:02<1:30:53, 23.31s/it]Running Inference:  22%|██▏       | 67/300 [25:26<1:31:10, 23.48s/it]Running Inference:  23%|██▎       | 68/300 [25:50<1:30:50, 23.49s/it]Running Inference:  23%|██▎       | 69/300 [26:14<1:30:59, 23.63s/it]Running Inference:  23%|██▎       | 70/300 [26:42<1:35:53, 25.01s/it]Running Inference:  24%|██▎       | 71/300 [27:05<1:33:05, 24.39s/it]Running Inference:  24%|██▍       | 72/300 [27:29<1:31:51, 24.17s/it]Running Inference:  24%|██▍       | 73/300 [27:52<1:30:29, 23.92s/it]Running Inference:  25%|██▍       | 74/300 [28:16<1:30:01, 23.90s/it]Running Inference:  25%|██▌       | 75/300 [28:39<1:28:21, 23.56s/it]Running Inference:  25%|██▌       | 76/300 [28:53<1:17:36, 20.79s/it]Running Inference:  26%|██▌       | 77/300 [29:17<1:21:15, 21.86s/it]Running Inference:  26%|██▌       | 78/300 [29:40<1:22:02, 22.17s/it]Running Inference:  26%|██▋       | 79/300 [30:03<1:22:26, 22.38s/it]Running Inference:  27%|██▋       | 80/300 [30:27<1:23:26, 22.76s/it]Running Inference:  27%|██▋       | 81/300 [30:51<1:24:53, 23.26s/it]Running Inference:  27%|██▋       | 82/300 [31:14<1:23:58, 23.11s/it]Running Inference:  28%|██▊       | 83/300 [31:37<1:23:18, 23.03s/it]Running Inference:  28%|██▊       | 84/300 [31:56<1:19:07, 21.98s/it]Running Inference:  28%|██▊       | 85/300 [32:16<1:16:06, 21.24s/it]Running Inference:  29%|██▊       | 86/300 [32:40<1:18:44, 22.08s/it]Running Inference:  29%|██▉       | 87/300 [33:07<1:23:27, 23.51s/it]Running Inference:  29%|██▉       | 88/300 [33:30<1:22:59, 23.49s/it]Running Inference:  30%|██▉       | 89/300 [33:53<1:21:57, 23.31s/it]Running Inference:  30%|███       | 90/300 [34:16<1:21:02, 23.15s/it]Running Inference:  30%|███       | 91/300 [34:39<1:20:40, 23.16s/it]Running Inference:  31%|███       | 92/300 [35:02<1:19:55, 23.05s/it]Running Inference:  31%|███       | 93/300 [35:25<1:20:04, 23.21s/it]Running Inference:  31%|███▏      | 94/300 [35:28<58:23, 17.01s/it]  Running Inference:  32%|███▏      | 95/300 [35:51<1:03:55, 18.71s/it]Running Inference:  32%|███▏      | 96/300 [36:14<1:08:06, 20.03s/it]Running Inference:  32%|███▏      | 97/300 [36:37<1:10:46, 20.92s/it]Running Inference:  33%|███▎      | 98/300 [37:00<1:12:37, 21.57s/it]Running Inference:  33%|███▎      | 99/300 [37:25<1:16:03, 22.70s/it]Running Inference:  33%|███▎      | 100/300 [37:50<1:17:59, 23.40s/it]Running Inference:  34%|███▎      | 101/300 [38:14<1:17:41, 23.42s/it]Running Inference:  34%|███▍      | 102/300 [38:36<1:16:47, 23.27s/it]Running Inference:  34%|███▍      | 103/300 [38:46<1:02:47, 19.12s/it]Running Inference:  35%|███▍      | 104/300 [39:09<1:06:18, 20.30s/it]Running Inference:  35%|███▌      | 105/300 [39:33<1:09:14, 21.30s/it]Running Inference:  35%|███▌      | 106/300 [39:55<1:10:24, 21.78s/it]Running Inference:  36%|███▌      | 107/300 [40:19<1:11:32, 22.24s/it]Running Inference:  36%|███▌      | 108/300 [40:42<1:11:40, 22.40s/it]Running Inference:  36%|███▋      | 109/300 [41:05<1:12:32, 22.79s/it]Running Inference:  37%|███▋      | 110/300 [41:29<1:12:42, 22.96s/it]Running Inference:  37%|███▋      | 111/300 [41:52<1:12:28, 23.01s/it]Running Inference:  37%|███▋      | 112/300 [42:18<1:15:05, 23.97s/it]Running Inference:  38%|███▊      | 113/300 [42:41<1:14:01, 23.75s/it]Running Inference:  38%|███▊      | 114/300 [43:05<1:13:25, 23.69s/it]Running Inference:  38%|███▊      | 115/300 [43:28<1:12:23, 23.48s/it]Running Inference:  39%|███▊      | 116/300 [43:50<1:11:18, 23.25s/it]Running Inference:  39%|███▉      | 117/300 [44:13<1:10:29, 23.11s/it]Running Inference:  39%|███▉      | 118/300 [44:15<50:21, 16.60s/it]  Running Inference:  40%|███▉      | 119/300 [44:37<55:42, 18.47s/it]Running Inference:  40%|████      | 120/300 [45:00<59:13, 19.74s/it]Running Inference:  40%|████      | 121/300 [45:24<1:02:24, 20.92s/it]Running Inference:  41%|████      | 122/300 [45:47<1:03:47, 21.50s/it]Running Inference:  41%|████      | 123/300 [46:10<1:05:19, 22.14s/it]Running Inference:  41%|████▏     | 124/300 [46:33<1:05:41, 22.39s/it]Running Inference:  42%|████▏     | 125/300 [46:57<1:06:22, 22.76s/it]Running Inference:  42%|████▏     | 126/300 [46:59<47:45, 16.47s/it]  Running Inference:  42%|████▏     | 127/300 [47:26<57:09, 19.82s/it]Running Inference:  43%|████▎     | 128/300 [47:49<59:27, 20.74s/it]Running Inference:  43%|████▎     | 129/300 [48:12<1:00:56, 21.38s/it]Running Inference:  43%|████▎     | 130/300 [48:32<59:34, 21.03s/it]  Running Inference:  44%|████▎     | 131/300 [48:56<1:01:10, 21.72s/it]Running Inference:  44%|████▍     | 132/300 [49:19<1:02:14, 22.23s/it]Running Inference:  44%|████▍     | 133/300 [49:42<1:02:49, 22.57s/it]Running Inference:  45%|████▍     | 134/300 [50:07<1:03:44, 23.04s/it]Running Inference:  45%|████▌     | 135/300 [50:25<59:50, 21.76s/it]  Running Inference:  45%|████▌     | 136/300 [50:50<1:01:29, 22.50s/it]Running Inference:  46%|████▌     | 137/300 [51:13<1:02:01, 22.83s/it]Running Inference:  46%|████▌     | 138/300 [51:37<1:02:01, 22.97s/it]Running Inference:  46%|████▋     | 139/300 [52:00<1:02:04, 23.14s/it]Running Inference:  47%|████▋     | 140/300 [52:24<1:02:13, 23.34s/it]Running Inference:  47%|████▋     | 141/300 [52:38<54:12, 20.46s/it]  Running Inference:  47%|████▋     | 142/300 [53:01<56:05, 21.30s/it]Running Inference:  48%|████▊     | 143/300 [53:24<56:54, 21.75s/it]Running Inference:  48%|████▊     | 144/300 [53:46<57:20, 22.05s/it]Running Inference:  48%|████▊     | 145/300 [54:09<57:32, 22.27s/it]Running Inference:  49%|████▊     | 146/300 [54:32<57:29, 22.40s/it]Running Inference:  49%|████▉     | 147/300 [54:55<57:37, 22.60s/it]Running Inference:  49%|████▉     | 148/300 [55:21<1:00:13, 23.77s/it]Running Inference:  50%|████▉     | 149/300 [55:44<59:10, 23.52s/it]  Running Inference:  50%|█████     | 150/300 [56:07<58:14, 23.30s/it]Running Inference:  50%|█████     | 151/300 [56:30<57:38, 23.21s/it]Running Inference:  51%|█████     | 152/300 [56:31<40:52, 16.57s/it]Running Inference:  51%|█████     | 153/300 [56:55<45:35, 18.61s/it]Running Inference:  51%|█████▏    | 154/300 [57:18<48:31, 19.94s/it]Running Inference:  52%|█████▏    | 155/300 [57:42<51:10, 21.17s/it]Running Inference:  52%|█████▏    | 156/300 [58:05<52:12, 21.75s/it]Running Inference:  52%|█████▏    | 157/300 [58:28<52:58, 22.23s/it]Running Inference:  53%|█████▎    | 158/300 [58:52<53:34, 22.64s/it]Running Inference:  53%|█████▎    | 159/300 [59:15<53:28, 22.76s/it]Running Inference:  53%|█████▎    | 160/300 [59:38<53:08, 22.77s/it]Running Inference:  54%|█████▎    | 161/300 [1:00:01<52:56, 22.85s/it]Running Inference:  54%|█████▍    | 162/300 [1:00:24<52:55, 23.01s/it]Running Inference:  54%|█████▍    | 163/300 [1:00:47<52:28, 22.98s/it]Running Inference:  55%|█████▍    | 164/300 [1:01:10<52:19, 23.08s/it]Running Inference:  55%|█████▌    | 165/300 [1:01:14<39:06, 17.38s/it]Running Inference:  55%|█████▌    | 166/300 [1:01:17<28:48, 12.90s/it]Running Inference:  56%|█████▌    | 167/300 [1:01:24<24:39, 11.13s/it]Running Inference:  56%|█████▌    | 168/300 [1:01:44<30:16, 13.76s/it]Running Inference:  56%|█████▋    | 169/300 [1:02:07<36:01, 16.50s/it]Running Inference:  57%|█████▋    | 170/300 [1:02:24<36:39, 16.92s/it]Running Inference:  57%|█████▋    | 171/300 [1:02:48<40:32, 18.85s/it]Running Inference:  57%|█████▋    | 172/300 [1:03:11<42:56, 20.13s/it]Running Inference:  58%|█████▊    | 173/300 [1:03:34<44:16, 20.91s/it]Running Inference:  58%|█████▊    | 174/300 [1:03:57<45:34, 21.70s/it]Running Inference:  58%|█████▊    | 175/300 [1:04:21<46:14, 22.20s/it]Running Inference:  59%|█████▊    | 176/300 [1:04:43<46:18, 22.41s/it]Running Inference:  59%|█████▉    | 177/300 [1:05:06<46:14, 22.56s/it]Running Inference:  59%|█████▉    | 178/300 [1:05:30<46:37, 22.93s/it]Running Inference:  60%|█████▉    | 179/300 [1:05:54<46:54, 23.26s/it]Running Inference:  60%|██████    | 180/300 [1:06:17<46:13, 23.11s/it]Running Inference:  60%|██████    | 181/300 [1:06:40<45:43, 23.06s/it]Running Inference:  61%|██████    | 182/300 [1:07:03<45:14, 23.00s/it]Running Inference:  61%|██████    | 183/300 [1:07:26<44:47, 22.97s/it]Running Inference:  61%|██████▏   | 184/300 [1:07:49<44:23, 22.96s/it]Running Inference:  62%|██████▏   | 185/300 [1:08:14<45:27, 23.72s/it]Running Inference:  62%|██████▏   | 186/300 [1:08:37<44:46, 23.57s/it]Running Inference:  62%|██████▏   | 187/300 [1:09:00<43:55, 23.32s/it]Running Inference:  63%|██████▎   | 188/300 [1:09:24<43:44, 23.43s/it]Running Inference:  63%|██████▎   | 189/300 [1:09:48<43:34, 23.55s/it]Running Inference:  63%|██████▎   | 190/300 [1:10:10<42:45, 23.32s/it]Running Inference:  64%|██████▎   | 191/300 [1:10:34<42:36, 23.46s/it]Running Inference:  64%|██████▍   | 192/300 [1:10:57<42:09, 23.43s/it]Running Inference:  64%|██████▍   | 193/300 [1:11:25<44:14, 24.81s/it]Running Inference:  65%|██████▍   | 194/300 [1:11:50<43:40, 24.73s/it]Running Inference:  65%|██████▌   | 195/300 [1:12:15<43:38, 24.94s/it]Running Inference:  65%|██████▌   | 196/300 [1:12:39<42:31, 24.53s/it]Running Inference:  66%|██████▌   | 197/300 [1:13:02<41:14, 24.02s/it]Running Inference:  66%|██████▌   | 198/300 [1:13:25<40:30, 23.83s/it]Running Inference:  66%|██████▋   | 199/300 [1:13:51<40:53, 24.29s/it]Running Inference:  67%|██████▋   | 200/300 [1:14:13<39:45, 23.85s/it]Running Inference:  67%|██████▋   | 201/300 [1:14:37<39:04, 23.68s/it]Running Inference:  67%|██████▋   | 202/300 [1:15:00<38:19, 23.46s/it]Running Inference:  68%|██████▊   | 203/300 [1:15:23<37:48, 23.38s/it]Running Inference:  68%|██████▊   | 204/300 [1:15:47<37:34, 23.48s/it]Running Inference:  68%|██████▊   | 205/300 [1:16:10<37:22, 23.60s/it]Running Inference:  69%|██████▊   | 206/300 [1:16:34<37:07, 23.70s/it]Running Inference:  69%|██████▉   | 207/300 [1:17:00<37:50, 24.41s/it]Running Inference:  69%|██████▉   | 208/300 [1:17:24<36:57, 24.10s/it]Running Inference:  70%|██████▉   | 209/300 [1:17:48<36:41, 24.19s/it]Running Inference:  70%|███████   | 210/300 [1:18:12<35:52, 23.92s/it]Running Inference:  70%|███████   | 211/300 [1:18:34<34:59, 23.59s/it]Running Inference:  71%|███████   | 212/300 [1:19:02<36:14, 24.71s/it]Running Inference:  71%|███████   | 213/300 [1:19:25<35:00, 24.14s/it]Running Inference:  71%|███████▏  | 214/300 [1:19:43<32:18, 22.55s/it]Running Inference:  72%|███████▏  | 215/300 [1:20:06<32:01, 22.61s/it]Running Inference:  72%|███████▏  | 216/300 [1:20:29<31:48, 22.73s/it]Running Inference:  72%|███████▏  | 217/300 [1:20:32<23:20, 16.87s/it]Running Inference:  73%|███████▎  | 218/300 [1:20:57<26:26, 19.34s/it]Running Inference:  73%|███████▎  | 219/300 [1:21:20<27:29, 20.36s/it]Running Inference:  73%|███████▎  | 220/300 [1:21:43<28:04, 21.06s/it]Running Inference:  74%|███████▎  | 221/300 [1:22:06<28:29, 21.64s/it]Running Inference:  74%|███████▍  | 222/300 [1:22:27<28:06, 21.62s/it]Running Inference:  74%|███████▍  | 223/300 [1:22:52<28:43, 22.39s/it]Running Inference:  75%|███████▍  | 224/300 [1:23:15<28:53, 22.81s/it]Running Inference:  75%|███████▌  | 225/300 [1:23:38<28:34, 22.86s/it]Running Inference:  75%|███████▌  | 226/300 [1:24:02<28:19, 22.96s/it]Running Inference:  76%|███████▌  | 227/300 [1:24:25<28:12, 23.18s/it]Running Inference:  76%|███████▌  | 228/300 [1:24:30<21:13, 17.69s/it]Running Inference:  76%|███████▋  | 229/300 [1:24:55<23:34, 19.92s/it]Running Inference:  77%|███████▋  | 230/300 [1:24:58<17:15, 14.80s/it]Running Inference:  77%|███████▋  | 231/300 [1:25:21<19:55, 17.32s/it]Running Inference:  77%|███████▋  | 232/300 [1:25:45<21:49, 19.26s/it]Running Inference:  78%|███████▊  | 233/300 [1:26:13<24:21, 21.82s/it]Running Inference:  78%|███████▊  | 234/300 [1:26:36<24:19, 22.12s/it]Running Inference:  78%|███████▊  | 235/300 [1:26:59<24:22, 22.49s/it]Running Inference:  79%|███████▊  | 236/300 [1:27:22<24:05, 22.59s/it]Running Inference:  79%|███████▉  | 237/300 [1:27:47<24:21, 23.20s/it]Running Inference:  79%|███████▉  | 238/300 [1:28:07<23:16, 22.53s/it]Running Inference:  80%|███████▉  | 239/300 [1:28:31<23:12, 22.82s/it]Running Inference:  80%|████████  | 240/300 [1:28:54<23:00, 23.01s/it]Running Inference:  80%|████████  | 241/300 [1:29:17<22:34, 22.96s/it]Running Inference:  81%|████████  | 242/300 [1:29:40<22:10, 22.94s/it]Running Inference:  81%|████████  | 243/300 [1:30:04<21:55, 23.07s/it]Running Inference:  81%|████████▏ | 244/300 [1:30:29<22:07, 23.70s/it]Running Inference:  82%|████████▏ | 245/300 [1:30:43<19:07, 20.86s/it]Running Inference:  82%|████████▏ | 246/300 [1:31:06<19:19, 21.47s/it]Running Inference:  82%|████████▏ | 247/300 [1:31:30<19:45, 22.37s/it]Running Inference:  83%|████████▎ | 248/300 [1:31:56<20:19, 23.46s/it]Running Inference:  83%|████████▎ | 249/300 [1:32:20<20:02, 23.58s/it]Running Inference:  83%|████████▎ | 250/300 [1:32:44<19:35, 23.51s/it]Running Inference:  84%|████████▎ | 251/300 [1:33:07<19:07, 23.42s/it]Running Inference:  84%|████████▍ | 252/300 [1:33:31<18:52, 23.59s/it]Running Inference:  84%|████████▍ | 253/300 [1:33:54<18:19, 23.38s/it]Running Inference:  85%|████████▍ | 254/300 [1:34:11<16:31, 21.56s/it]Running Inference:  85%|████████▌ | 255/300 [1:34:34<16:26, 21.92s/it]Running Inference:  85%|████████▌ | 256/300 [1:34:47<14:08, 19.27s/it]Running Inference:  86%|████████▌ | 257/300 [1:35:10<14:35, 20.35s/it]Running Inference:  86%|████████▌ | 258/300 [1:35:33<14:48, 21.15s/it]Running Inference:  86%|████████▋ | 259/300 [1:35:58<15:20, 22.44s/it]Running Inference:  87%|████████▋ | 260/300 [1:36:21<15:05, 22.64s/it]Running Inference:  87%|████████▋ | 261/300 [1:36:44<14:47, 22.75s/it]Running Inference:  87%|████████▋ | 262/300 [1:37:03<13:40, 21.58s/it]Running Inference:  88%|████████▊ | 263/300 [1:37:26<13:37, 22.10s/it]Running Inference:  88%|████████▊ | 264/300 [1:37:49<13:20, 22.25s/it]Running Inference:  88%|████████▊ | 265/300 [1:38:12<13:06, 22.46s/it]Running Inference:  89%|████████▊ | 266/300 [1:38:35<12:51, 22.68s/it]Running Inference:  89%|████████▉ | 267/300 [1:38:59<12:37, 22.95s/it]Running Inference:  89%|████████▉ | 268/300 [1:39:22<12:21, 23.18s/it]Running Inference:  90%|████████▉ | 269/300 [1:39:47<12:08, 23.50s/it]Running Inference:  90%|█████████ | 270/300 [1:40:11<11:49, 23.65s/it]Running Inference:  90%|█████████ | 271/300 [1:40:34<11:25, 23.63s/it]Running Inference:  91%|█████████ | 272/300 [1:41:00<11:22, 24.39s/it]Running Inference:  91%|█████████ | 273/300 [1:41:22<10:32, 23.43s/it]Running Inference:  91%|█████████▏| 274/300 [1:41:45<10:12, 23.55s/it]Running Inference:  92%|█████████▏| 275/300 [1:42:09<09:47, 23.50s/it]Running Inference:  92%|█████████▏| 276/300 [1:42:30<09:04, 22.67s/it]Running Inference:  92%|█████████▏| 277/300 [1:42:54<08:51, 23.10s/it]Running Inference:  93%|█████████▎| 278/300 [1:43:17<08:28, 23.11s/it]Running Inference:  93%|█████████▎| 279/300 [1:43:40<08:05, 23.13s/it]Running Inference:  93%|█████████▎| 280/300 [1:44:00<07:25, 22.26s/it]Running Inference:  94%|█████████▎| 281/300 [1:44:24<07:09, 22.60s/it]Running Inference:  94%|█████████▍| 282/300 [1:44:47<06:50, 22.79s/it]Running Inference:  94%|█████████▍| 283/300 [1:45:10<06:30, 22.96s/it]Running Inference:  95%|█████████▍| 284/300 [1:45:33<06:06, 22.93s/it]Running Inference:  95%|█████████▌| 285/300 [1:45:58<05:53, 23.60s/it]Running Inference:  95%|█████████▌| 286/300 [1:46:26<05:48, 24.89s/it]Running Inference:  96%|█████████▌| 287/300 [1:46:49<05:17, 24.43s/it]Running Inference:  96%|█████████▌| 288/300 [1:47:13<04:48, 24.02s/it]Running Inference:  96%|█████████▋| 289/300 [1:47:36<04:20, 23.72s/it]Running Inference:  97%|█████████▋| 290/300 [1:48:00<03:58, 23.86s/it]Running Inference:  97%|█████████▋| 291/300 [1:48:24<03:34, 23.86s/it]Running Inference:  97%|█████████▋| 292/300 [1:48:44<03:02, 22.87s/it]Running Inference:  98%|█████████▊| 293/300 [1:49:07<02:41, 23.00s/it]Running Inference:  98%|█████████▊| 294/300 [1:49:30<02:17, 22.84s/it]Running Inference:  98%|█████████▊| 295/300 [1:49:31<01:21, 16.38s/it]Running Inference:  99%|█████████▊| 296/300 [1:49:55<01:14, 18.65s/it]Running Inference:  99%|█████████▉| 297/300 [1:50:18<00:59, 19.87s/it]Running Inference:  99%|█████████▉| 298/300 [1:50:41<00:41, 20.76s/it]Running Inference: 100%|█████████▉| 299/300 [1:51:04<00:21, 21.52s/it]Running Inference: 100%|██████████| 300/300 [1:51:27<00:00, 21.94s/it]Running Inference: 100%|██████████| 300/300 [1:51:27<00:00, 22.29s/it]
2025-12-15 03:23:03,176 - INFO - Inference completed.
2025-12-15 03:23:03,224 - INFO - Results saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-15 03:23:03,224 - INFO - Calculating metrics for dataset: longbench
2025-12-15 03:23:29,892 - INFO - Metrics saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-15 03:23:29,893 - INFO - Metrics:
14.78
2025-12-15 03:23:29,894 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report_e, ratio=0.1) on GPU 1

----------------------------------------
Task: gov_report_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 03:23:37,121 - INFO - Set deterministic seeds to 42
2025-12-15 03:23:37,121 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 03:23:37,121 - INFO - Starting evaluation run...
2025-12-15 03:23:37,121 - INFO - Output directory set to: longbenchresult
2025-12-15 03:23:37,121 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-15 03:23:37,122 - INFO - KV Press 'tova' setup.
2025-12-15 03:23:37,122 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 03:23:37,122 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.19it/s]
Device set to use cuda:0
2025-12-15 03:23:48,055 - INFO - Model pipeline loaded.
2025-12-15 03:23:48,055 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report_e)
2025-12-15 03:23:50,984 - INFO - Dataset loaded with 300 entries.
2025-12-15 03:23:50,984 - INFO - Dataset processed with 300 entries.
2025-12-15 03:23:51,024 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:03<16:10,  3.25s/it]Running Inference:   1%|          | 2/300 [00:30<1:25:38, 17.24s/it]Running Inference:   1%|          | 3/300 [00:48<1:27:58, 17.77s/it]Running Inference:   1%|▏         | 4/300 [01:11<1:37:53, 19.84s/it]Running Inference:   2%|▏         | 5/300 [01:35<1:44:50, 21.32s/it]Running Inference:   2%|▏         | 6/300 [01:59<1:49:20, 22.32s/it]Running Inference:   2%|▏         | 7/300 [02:22<1:50:13, 22.57s/it]Running Inference:   3%|▎         | 8/300 [02:47<1:52:43, 23.16s/it]Running Inference:   3%|▎         | 9/300 [03:10<1:52:21, 23.17s/it]Running Inference:   3%|▎         | 10/300 [03:18<1:28:40, 18.35s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [03:41<1:36:12, 19.97s/it]Running Inference:   4%|▍         | 12/300 [04:04<1:40:29, 20.93s/it]Running Inference:   4%|▍         | 13/300 [04:29<1:44:56, 21.94s/it]Running Inference:   5%|▍         | 14/300 [04:52<1:46:22, 22.32s/it]Running Inference:   5%|▌         | 15/300 [05:11<1:41:02, 21.27s/it]Running Inference:   5%|▌         | 16/300 [05:35<1:44:37, 22.11s/it]Running Inference:   6%|▌         | 17/300 [05:58<1:45:38, 22.40s/it]Running Inference:   6%|▌         | 18/300 [06:22<1:47:12, 22.81s/it]Running Inference:   6%|▋         | 19/300 [06:45<1:47:45, 23.01s/it]Running Inference:   7%|▋         | 20/300 [07:11<1:51:02, 23.80s/it]Running Inference:   7%|▋         | 21/300 [07:34<1:49:45, 23.60s/it]Running Inference:   7%|▋         | 22/300 [07:59<1:50:51, 23.93s/it]Running Inference:   8%|▊         | 23/300 [08:24<1:52:01, 24.26s/it]Running Inference:   8%|▊         | 24/300 [08:47<1:50:30, 24.02s/it]Running Inference:   8%|▊         | 25/300 [09:12<1:50:57, 24.21s/it]Running Inference:   9%|▊         | 26/300 [09:36<1:50:08, 24.12s/it]Running Inference:   9%|▉         | 27/300 [09:59<1:48:32, 23.85s/it]Running Inference:   9%|▉         | 28/300 [10:22<1:47:43, 23.76s/it]Running Inference:  10%|▉         | 29/300 [10:46<1:46:30, 23.58s/it]Running Inference:  10%|█         | 30/300 [11:05<1:39:59, 22.22s/it]Running Inference:  10%|█         | 31/300 [11:30<1:43:38, 23.12s/it]Running Inference:  11%|█         | 32/300 [11:53<1:43:42, 23.22s/it]Running Inference:  11%|█         | 33/300 [12:18<1:45:35, 23.73s/it]Running Inference:  11%|█▏        | 34/300 [12:42<1:45:05, 23.71s/it]Running Inference:  12%|█▏        | 35/300 [13:05<1:43:59, 23.55s/it]Running Inference:  12%|█▏        | 36/300 [13:28<1:43:05, 23.43s/it]Running Inference:  12%|█▏        | 37/300 [13:52<1:43:27, 23.60s/it]Running Inference:  13%|█▎        | 38/300 [14:17<1:44:03, 23.83s/it]Running Inference:  13%|█▎        | 39/300 [14:41<1:44:23, 24.00s/it]Running Inference:  13%|█▎        | 40/300 [15:05<1:43:59, 24.00s/it]Running Inference:  14%|█▎        | 41/300 [15:29<1:43:09, 23.90s/it]Running Inference:  14%|█▍        | 42/300 [15:52<1:42:29, 23.84s/it]Running Inference:  14%|█▍        | 43/300 [16:16<1:41:28, 23.69s/it]Running Inference:  15%|█▍        | 44/300 [16:39<1:40:33, 23.57s/it]Running Inference:  15%|█▌        | 45/300 [17:03<1:40:19, 23.61s/it]Running Inference:  15%|█▌        | 46/300 [17:26<1:39:59, 23.62s/it]Running Inference:  16%|█▌        | 47/300 [17:50<1:39:34, 23.61s/it]Running Inference:  16%|█▌        | 48/300 [18:14<1:39:52, 23.78s/it]Running Inference:  16%|█▋        | 49/300 [18:37<1:38:34, 23.57s/it]Running Inference:  17%|█▋        | 50/300 [19:01<1:39:01, 23.77s/it]Running Inference:  17%|█▋        | 51/300 [19:25<1:38:01, 23.62s/it]Running Inference:  17%|█▋        | 52/300 [19:48<1:37:55, 23.69s/it]Running Inference:  18%|█▊        | 53/300 [20:12<1:36:48, 23.52s/it]Running Inference:  18%|█▊        | 54/300 [20:35<1:36:04, 23.43s/it]Running Inference:  18%|█▊        | 55/300 [21:00<1:37:18, 23.83s/it]Running Inference:  19%|█▊        | 56/300 [21:23<1:36:17, 23.68s/it]Running Inference:  19%|█▉        | 57/300 [21:42<1:30:13, 22.28s/it]Running Inference:  19%|█▉        | 58/300 [22:05<1:31:25, 22.67s/it]Running Inference:  20%|█▉        | 59/300 [22:30<1:32:49, 23.11s/it]Running Inference:  20%|██        | 60/300 [22:53<1:32:58, 23.25s/it]Running Inference:  20%|██        | 61/300 [23:15<1:30:38, 22.75s/it]Running Inference:  21%|██        | 62/300 [23:39<1:31:34, 23.08s/it]Running Inference:  21%|██        | 63/300 [24:02<1:31:28, 23.16s/it]Running Inference:  21%|██▏       | 64/300 [24:26<1:31:42, 23.31s/it]Running Inference:  22%|██▏       | 65/300 [24:50<1:32:21, 23.58s/it]Running Inference:  22%|██▏       | 66/300 [25:14<1:32:02, 23.60s/it]Running Inference:  22%|██▏       | 67/300 [25:38<1:32:28, 23.81s/it]Running Inference:  23%|██▎       | 68/300 [26:02<1:32:12, 23.85s/it]Running Inference:  23%|██▎       | 69/300 [26:26<1:32:30, 24.03s/it]Running Inference:  23%|██▎       | 70/300 [26:54<1:36:47, 25.25s/it]Running Inference:  24%|██▎       | 71/300 [27:18<1:34:13, 24.69s/it]Running Inference:  24%|██▍       | 72/300 [27:42<1:32:56, 24.46s/it]Running Inference:  24%|██▍       | 73/300 [28:05<1:31:42, 24.24s/it]Running Inference:  25%|██▍       | 74/300 [28:30<1:31:21, 24.25s/it]Running Inference:  25%|██▌       | 75/300 [28:53<1:29:44, 23.93s/it]Running Inference:  25%|██▌       | 76/300 [29:16<1:28:41, 23.76s/it]Running Inference:  26%|██▌       | 77/300 [29:41<1:29:26, 24.06s/it]Running Inference:  26%|██▌       | 78/300 [30:04<1:28:10, 23.83s/it]Running Inference:  26%|██▋       | 79/300 [30:28<1:27:11, 23.67s/it]Running Inference:  27%|██▋       | 80/300 [30:52<1:27:12, 23.79s/it]Running Inference:  27%|██▋       | 81/300 [31:16<1:27:44, 24.04s/it]Running Inference:  27%|██▋       | 82/300 [31:39<1:26:23, 23.78s/it]Running Inference:  28%|██▊       | 83/300 [32:03<1:25:26, 23.62s/it]Running Inference:  28%|██▊       | 84/300 [32:26<1:24:38, 23.51s/it]Running Inference:  28%|██▊       | 85/300 [32:50<1:24:30, 23.58s/it]Running Inference:  29%|██▊       | 86/300 [33:11<1:22:14, 23.06s/it]Running Inference:  29%|██▉       | 87/300 [33:38<1:25:52, 24.19s/it]Running Inference:  29%|██▉       | 88/300 [34:02<1:25:04, 24.08s/it]Running Inference:  30%|██▉       | 89/300 [34:21<1:18:59, 22.46s/it]Running Inference:  30%|███       | 90/300 [34:44<1:19:23, 22.69s/it]Running Inference:  30%|███       | 91/300 [35:08<1:19:55, 22.95s/it]Running Inference:  31%|███       | 92/300 [35:31<1:19:47, 23.02s/it]Running Inference:  31%|███       | 93/300 [35:55<1:20:23, 23.30s/it]Running Inference:  31%|███▏      | 94/300 [36:18<1:20:07, 23.34s/it]Running Inference:  32%|███▏      | 95/300 [36:41<1:19:28, 23.26s/it]Running Inference:  32%|███▏      | 96/300 [37:05<1:19:20, 23.34s/it]Running Inference:  32%|███▏      | 97/300 [37:28<1:19:00, 23.35s/it]Running Inference:  33%|███▎      | 98/300 [37:52<1:18:45, 23.40s/it]Running Inference:  33%|███▎      | 99/300 [38:17<1:20:26, 24.01s/it]Running Inference:  33%|███▎      | 100/300 [38:42<1:21:08, 24.34s/it]Running Inference:  34%|███▎      | 101/300 [39:06<1:20:14, 24.19s/it]Running Inference:  34%|███▍      | 102/300 [39:29<1:18:56, 23.92s/it]Running Inference:  34%|███▍      | 103/300 [39:39<1:04:11, 19.55s/it]Running Inference:  35%|███▍      | 104/300 [40:02<1:07:43, 20.73s/it]Running Inference:  35%|███▌      | 105/300 [40:23<1:07:39, 20.82s/it]Running Inference:  35%|███▌      | 106/300 [40:46<1:09:44, 21.57s/it]Running Inference:  36%|███▌      | 107/300 [41:10<1:11:29, 22.23s/it]Running Inference:  36%|███▌      | 108/300 [41:33<1:12:04, 22.52s/it]Running Inference:  36%|███▋      | 109/300 [41:58<1:13:10, 22.99s/it]Running Inference:  37%|███▋      | 110/300 [42:21<1:13:32, 23.22s/it]Running Inference:  37%|███▋      | 111/300 [42:45<1:13:25, 23.31s/it]Running Inference:  37%|███▋      | 112/300 [43:11<1:15:46, 24.18s/it]Running Inference:  38%|███▊      | 113/300 [43:35<1:14:48, 24.00s/it]Running Inference:  38%|███▊      | 114/300 [43:58<1:14:15, 23.95s/it]Running Inference:  38%|███▊      | 115/300 [44:21<1:12:37, 23.55s/it]Running Inference:  39%|███▊      | 116/300 [44:44<1:11:48, 23.42s/it]Running Inference:  39%|███▉      | 117/300 [45:07<1:11:11, 23.34s/it]Running Inference:  39%|███▉      | 118/300 [45:31<1:10:38, 23.29s/it]Running Inference:  40%|███▉      | 119/300 [45:54<1:10:12, 23.27s/it]Running Inference:  40%|████      | 120/300 [46:17<1:09:40, 23.22s/it]Running Inference:  40%|████      | 121/300 [46:41<1:10:00, 23.47s/it]Running Inference:  41%|████      | 122/300 [46:59<1:04:53, 21.88s/it]Running Inference:  41%|████      | 123/300 [47:04<49:13, 16.68s/it]  Running Inference:  41%|████▏     | 124/300 [47:27<54:48, 18.69s/it]Running Inference:  42%|████▏     | 125/300 [47:51<59:09, 20.28s/it]Running Inference:  42%|████▏     | 126/300 [47:53<42:45, 14.74s/it]Running Inference:  42%|████▏     | 127/300 [48:20<53:35, 18.59s/it]Running Inference:  43%|████▎     | 128/300 [48:44<57:18, 19.99s/it]Running Inference:  43%|████▎     | 129/300 [49:07<59:49, 20.99s/it]Running Inference:  43%|████▎     | 130/300 [49:31<1:02:07, 21.93s/it]Running Inference:  44%|████▎     | 131/300 [49:55<1:03:18, 22.47s/it]Running Inference:  44%|████▍     | 132/300 [50:19<1:04:05, 22.89s/it]Running Inference:  44%|████▍     | 133/300 [50:42<1:04:02, 23.01s/it]Running Inference:  45%|████▍     | 134/300 [51:06<1:04:55, 23.47s/it]Running Inference:  45%|████▌     | 135/300 [51:30<1:04:29, 23.45s/it]Running Inference:  45%|████▌     | 136/300 [51:55<1:05:08, 23.83s/it]Running Inference:  46%|████▌     | 137/300 [52:18<1:04:13, 23.64s/it]Running Inference:  46%|████▌     | 138/300 [52:42<1:03:54, 23.67s/it]Running Inference:  46%|████▋     | 139/300 [53:05<1:03:42, 23.74s/it]Running Inference:  47%|████▋     | 140/300 [53:30<1:03:41, 23.89s/it]Running Inference:  47%|████▋     | 141/300 [53:36<49:40, 18.74s/it]  Running Inference:  47%|████▋     | 142/300 [54:00<53:13, 20.21s/it]Running Inference:  48%|████▊     | 143/300 [54:19<51:55, 19.84s/it]Running Inference:  48%|████▊     | 144/300 [54:42<54:12, 20.85s/it]Running Inference:  48%|████▊     | 145/300 [55:05<55:39, 21.54s/it]Running Inference:  49%|████▊     | 146/300 [55:29<56:29, 22.01s/it]Running Inference:  49%|████▉     | 147/300 [55:52<57:08, 22.41s/it]Running Inference:  49%|████▉     | 148/300 [56:18<59:51, 23.63s/it]Running Inference:  50%|████▉     | 149/300 [56:42<59:15, 23.55s/it]Running Inference:  50%|█████     | 150/300 [57:05<58:37, 23.45s/it]Running Inference:  50%|█████     | 151/300 [57:28<58:13, 23.45s/it]Running Inference:  51%|█████     | 152/300 [57:29<41:17, 16.74s/it]Running Inference:  51%|█████     | 153/300 [57:53<46:08, 18.84s/it]Running Inference:  51%|█████▏    | 154/300 [58:17<49:12, 20.22s/it]Running Inference:  52%|█████▏    | 155/300 [58:41<51:56, 21.49s/it]Running Inference:  52%|█████▏    | 156/300 [59:05<53:01, 22.10s/it]Running Inference:  52%|█████▏    | 157/300 [59:28<53:50, 22.59s/it]Running Inference:  53%|█████▎    | 158/300 [59:52<54:26, 23.00s/it]Running Inference:  53%|█████▎    | 159/300 [1:00:16<54:22, 23.14s/it]Running Inference:  53%|█████▎    | 160/300 [1:00:39<54:02, 23.16s/it]Running Inference:  54%|█████▎    | 161/300 [1:01:02<53:50, 23.24s/it]Running Inference:  54%|█████▍    | 162/300 [1:01:26<53:50, 23.41s/it]Running Inference:  54%|█████▍    | 163/300 [1:01:50<53:23, 23.38s/it]Running Inference:  55%|█████▍    | 164/300 [1:02:13<53:13, 23.48s/it]Running Inference:  55%|█████▌    | 165/300 [1:02:36<52:39, 23.40s/it]Running Inference:  55%|█████▌    | 166/300 [1:02:39<38:19, 17.16s/it]Running Inference:  56%|█████▌    | 167/300 [1:03:02<42:03, 18.98s/it]Running Inference:  56%|█████▌    | 168/300 [1:03:26<44:37, 20.28s/it]Running Inference:  56%|█████▋    | 169/300 [1:03:49<46:15, 21.18s/it]Running Inference:  57%|█████▋    | 170/300 [1:04:11<46:29, 21.46s/it]Running Inference:  57%|█████▋    | 171/300 [1:04:35<47:37, 22.15s/it]Running Inference:  57%|█████▋    | 172/300 [1:04:58<48:06, 22.55s/it]Running Inference:  58%|█████▊    | 173/300 [1:05:21<48:06, 22.73s/it]Running Inference:  58%|█████▊    | 174/300 [1:05:45<48:27, 23.08s/it]Running Inference:  58%|█████▊    | 175/300 [1:06:09<48:28, 23.27s/it]Running Inference:  59%|█████▊    | 176/300 [1:06:32<48:06, 23.28s/it]Running Inference:  59%|█████▉    | 177/300 [1:06:50<44:19, 21.62s/it]Running Inference:  59%|█████▉    | 178/300 [1:07:14<45:31, 22.39s/it]Running Inference:  60%|█████▉    | 179/300 [1:07:39<46:22, 23.00s/it]Running Inference:  60%|██████    | 180/300 [1:08:02<46:05, 23.05s/it]Running Inference:  60%|██████    | 181/300 [1:08:25<45:49, 23.10s/it]Running Inference:  61%|██████    | 182/300 [1:08:48<45:28, 23.12s/it]Running Inference:  61%|██████    | 183/300 [1:09:10<44:30, 22.82s/it]Running Inference:  61%|██████▏   | 184/300 [1:09:34<44:25, 22.98s/it]Running Inference:  62%|██████▏   | 185/300 [1:09:59<45:31, 23.75s/it]Running Inference:  62%|██████▏   | 186/300 [1:10:23<45:01, 23.70s/it]Running Inference:  62%|██████▏   | 187/300 [1:10:46<44:18, 23.53s/it]Running Inference:  63%|██████▎   | 188/300 [1:10:57<36:42, 19.66s/it]Running Inference:  63%|██████▎   | 189/300 [1:11:21<38:55, 21.04s/it]Running Inference:  63%|██████▎   | 190/300 [1:11:44<39:45, 21.69s/it]Running Inference:  64%|██████▎   | 191/300 [1:12:08<40:45, 22.43s/it]Running Inference:  64%|██████▍   | 192/300 [1:12:32<41:05, 22.83s/it]Running Inference:  64%|██████▍   | 193/300 [1:13:00<43:26, 24.36s/it]Running Inference:  65%|██████▍   | 194/300 [1:13:25<43:18, 24.51s/it]Running Inference:  65%|██████▌   | 195/300 [1:13:50<43:25, 24.81s/it]Running Inference:  65%|██████▌   | 196/300 [1:14:14<42:33, 24.55s/it]Running Inference:  66%|██████▌   | 197/300 [1:14:37<41:27, 24.15s/it]Running Inference:  66%|██████▌   | 198/300 [1:15:01<40:50, 24.03s/it]Running Inference:  66%|██████▋   | 199/300 [1:15:27<41:08, 24.44s/it]Running Inference:  67%|██████▋   | 200/300 [1:15:50<40:06, 24.07s/it]Running Inference:  67%|██████▋   | 201/300 [1:16:13<39:29, 23.94s/it]Running Inference:  67%|██████▋   | 202/300 [1:16:37<38:49, 23.77s/it]Running Inference:  68%|██████▊   | 203/300 [1:17:00<38:19, 23.70s/it]Running Inference:  68%|██████▊   | 204/300 [1:17:24<38:06, 23.81s/it]Running Inference:  68%|██████▊   | 205/300 [1:17:49<37:54, 23.94s/it]Running Inference:  69%|██████▊   | 206/300 [1:18:13<37:40, 24.04s/it]Running Inference:  69%|██████▉   | 207/300 [1:18:39<38:12, 24.65s/it]Running Inference:  69%|██████▉   | 208/300 [1:19:03<37:23, 24.38s/it]Running Inference:  70%|██████▉   | 209/300 [1:19:28<37:09, 24.50s/it]Running Inference:  70%|███████   | 210/300 [1:19:51<36:22, 24.25s/it]Running Inference:  70%|███████   | 211/300 [1:20:14<35:30, 23.93s/it]Running Inference:  71%|███████   | 212/300 [1:20:42<36:34, 24.94s/it]Running Inference:  71%|███████   | 213/300 [1:21:05<35:24, 24.42s/it]Running Inference:  71%|███████▏  | 214/300 [1:21:28<34:37, 24.16s/it]Running Inference:  72%|███████▏  | 215/300 [1:21:52<33:47, 23.86s/it]Running Inference:  72%|███████▏  | 216/300 [1:22:15<33:12, 23.72s/it]Running Inference:  72%|███████▏  | 217/300 [1:22:18<24:08, 17.45s/it]Running Inference:  73%|███████▎  | 218/300 [1:22:43<27:01, 19.77s/it]Running Inference:  73%|███████▎  | 219/300 [1:23:06<28:02, 20.77s/it]Running Inference:  73%|███████▎  | 220/300 [1:23:29<28:36, 21.45s/it]Running Inference:  74%|███████▎  | 221/300 [1:23:52<28:59, 22.02s/it]Running Inference:  74%|███████▍  | 222/300 [1:24:17<29:45, 22.89s/it]Running Inference:  74%|███████▍  | 223/300 [1:24:42<30:01, 23.39s/it]Running Inference:  75%|███████▍  | 224/300 [1:25:06<29:55, 23.62s/it]Running Inference:  75%|███████▌  | 225/300 [1:25:29<29:25, 23.54s/it]Running Inference:  75%|███████▌  | 226/300 [1:25:53<29:03, 23.56s/it]Running Inference:  76%|███████▌  | 227/300 [1:26:17<28:50, 23.70s/it]Running Inference:  76%|███████▌  | 228/300 [1:26:22<21:41, 18.08s/it]Running Inference:  76%|███████▋  | 229/300 [1:26:47<23:55, 20.22s/it]Running Inference:  77%|███████▋  | 230/300 [1:26:50<17:31, 15.02s/it]Running Inference:  77%|███████▋  | 231/300 [1:27:14<20:12, 17.58s/it]Running Inference:  77%|███████▋  | 232/300 [1:27:38<22:09, 19.55s/it]Running Inference:  78%|███████▊  | 233/300 [1:28:06<24:33, 21.99s/it]Running Inference:  78%|███████▊  | 234/300 [1:28:29<24:35, 22.35s/it]Running Inference:  78%|███████▊  | 235/300 [1:28:52<24:39, 22.77s/it]Running Inference:  79%|███████▊  | 236/300 [1:29:16<24:24, 22.89s/it]Running Inference:  79%|███████▉  | 237/300 [1:29:41<24:39, 23.49s/it]Running Inference:  79%|███████▉  | 238/300 [1:30:04<24:12, 23.43s/it]Running Inference:  80%|███████▉  | 239/300 [1:30:28<23:58, 23.57s/it]Running Inference:  80%|████████  | 240/300 [1:30:43<21:12, 21.20s/it]Running Inference:  80%|████████  | 241/300 [1:31:04<20:46, 21.12s/it]Running Inference:  81%|████████  | 242/300 [1:31:28<21:01, 21.76s/it]Running Inference:  81%|████████  | 243/300 [1:31:51<21:14, 22.36s/it]Running Inference:  81%|████████▏ | 244/300 [1:32:17<21:39, 23.21s/it]Running Inference:  82%|████████▏ | 245/300 [1:32:41<21:42, 23.68s/it]Running Inference:  82%|████████▏ | 246/300 [1:33:05<21:12, 23.57s/it]Running Inference:  82%|████████▏ | 247/300 [1:33:29<21:08, 23.94s/it]Running Inference:  83%|████████▎ | 248/300 [1:33:55<21:16, 24.56s/it]Running Inference:  83%|████████▎ | 249/300 [1:34:20<20:48, 24.47s/it]Running Inference:  83%|████████▎ | 250/300 [1:34:43<20:11, 24.23s/it]Running Inference:  84%|████████▎ | 251/300 [1:35:07<19:37, 24.03s/it]Running Inference:  84%|████████▍ | 252/300 [1:35:31<19:18, 24.14s/it]Running Inference:  84%|████████▍ | 253/300 [1:35:55<18:42, 23.88s/it]Running Inference:  85%|████████▍ | 254/300 [1:36:18<18:08, 23.66s/it]Running Inference:  85%|████████▌ | 255/300 [1:36:41<17:37, 23.50s/it]Running Inference:  85%|████████▌ | 256/300 [1:37:04<17:13, 23.50s/it]Running Inference:  86%|████████▌ | 257/300 [1:37:25<16:09, 22.54s/it]Running Inference:  86%|████████▌ | 258/300 [1:37:48<15:57, 22.79s/it]Running Inference:  86%|████████▋ | 259/300 [1:38:14<16:07, 23.60s/it]Running Inference:  87%|████████▋ | 260/300 [1:38:37<15:43, 23.58s/it]Running Inference:  87%|████████▋ | 261/300 [1:39:00<15:17, 23.52s/it]Running Inference:  87%|████████▋ | 262/300 [1:39:22<14:34, 23.01s/it]Running Inference:  88%|████████▊ | 263/300 [1:39:46<14:18, 23.21s/it]Running Inference:  88%|████████▊ | 264/300 [1:39:47<09:54, 16.52s/it]Running Inference:  88%|████████▊ | 265/300 [1:40:10<10:49, 18.56s/it]Running Inference:  89%|████████▊ | 266/300 [1:40:34<11:21, 20.06s/it]Running Inference:  89%|████████▉ | 267/300 [1:40:58<11:40, 21.22s/it]Running Inference:  89%|████████▉ | 268/300 [1:41:22<11:46, 22.08s/it]Running Inference:  90%|████████▉ | 269/300 [1:41:46<11:48, 22.86s/it]Running Inference:  90%|█████████ | 270/300 [1:42:11<11:39, 23.31s/it]Running Inference:  90%|█████████ | 271/300 [1:42:34<11:13, 23.23s/it]Running Inference:  91%|█████████ | 272/300 [1:43:00<11:14, 24.10s/it]Running Inference:  91%|█████████ | 273/300 [1:43:25<10:58, 24.39s/it]Running Inference:  91%|█████████▏| 274/300 [1:43:49<10:32, 24.33s/it]Running Inference:  92%|█████████▏| 275/300 [1:44:13<10:04, 24.16s/it]Running Inference:  92%|█████████▏| 276/300 [1:44:36<09:34, 23.92s/it]Running Inference:  92%|█████████▏| 277/300 [1:45:01<09:13, 24.08s/it]Running Inference:  93%|█████████▎| 278/300 [1:45:24<08:46, 23.91s/it]Running Inference:  93%|█████████▎| 279/300 [1:45:48<08:19, 23.80s/it]Running Inference:  93%|█████████▎| 280/300 [1:46:08<07:33, 22.67s/it]Running Inference:  94%|█████████▎| 281/300 [1:46:17<05:55, 18.73s/it]Running Inference:  94%|█████████▍| 282/300 [1:46:19<04:05, 13.66s/it]Running Inference:  94%|█████████▍| 283/300 [1:46:43<04:43, 16.69s/it]Running Inference:  95%|█████████▍| 284/300 [1:47:06<04:58, 18.65s/it]Running Inference:  95%|█████████▌| 285/300 [1:47:31<05:09, 20.62s/it]Running Inference:  95%|█████████▌| 286/300 [1:47:59<05:18, 22.77s/it]Running Inference:  96%|█████████▌| 287/300 [1:48:23<04:59, 23.06s/it]Running Inference:  96%|█████████▌| 288/300 [1:48:46<04:38, 23.18s/it]Running Inference:  96%|█████████▋| 289/300 [1:49:10<04:15, 23.24s/it]Running Inference:  97%|█████████▋| 290/300 [1:49:34<03:56, 23.64s/it]Running Inference:  97%|█████████▋| 291/300 [1:49:59<03:34, 23.81s/it]Running Inference:  97%|█████████▋| 292/300 [1:50:22<03:09, 23.68s/it]Running Inference:  98%|█████████▊| 293/300 [1:50:46<02:45, 23.68s/it]Running Inference:  98%|█████████▊| 294/300 [1:51:09<02:21, 23.61s/it]Running Inference:  98%|█████████▊| 295/300 [1:51:33<01:57, 23.55s/it]Running Inference:  99%|█████████▊| 296/300 [1:51:57<01:35, 23.78s/it]Running Inference:  99%|█████████▉| 297/300 [1:52:20<01:10, 23.59s/it]Running Inference:  99%|█████████▉| 298/300 [1:52:43<00:46, 23.47s/it]Running Inference: 100%|█████████▉| 299/300 [1:53:07<00:23, 23.53s/it]Running Inference: 100%|██████████| 300/300 [1:53:30<00:00, 23.47s/it]Running Inference: 100%|██████████| 300/300 [1:53:30<00:00, 22.70s/it]
2025-12-15 05:17:21,704 - INFO - Inference completed.
2025-12-15 05:17:21,751 - INFO - Results saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-15 05:17:21,751 - INFO - Calculating metrics for dataset: longbench
2025-12-15 05:17:48,729 - INFO - Metrics saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-15 05:17:48,729 - INFO - Metrics:
14.66
2025-12-15 05:17:48,731 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report_e, ratio=0.2) on GPU 1

----------------------------------------
Task: gov_report_e | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 05:17:55,951 - INFO - Set deterministic seeds to 42
2025-12-15 05:17:55,951 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 05:17:55,951 - INFO - Starting evaluation run...
2025-12-15 05:17:55,951 - INFO - Output directory set to: longbenchresult
2025-12-15 05:17:55,952 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-15 05:17:55,952 - INFO - KV Press 'tova' setup.
2025-12-15 05:17:55,952 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 05:17:55,952 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.61it/s]
Device set to use cuda:0
2025-12-15 05:18:06,792 - INFO - Model pipeline loaded.
2025-12-15 05:18:06,792 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report_e)
2025-12-15 05:18:10,085 - INFO - Dataset loaded with 300 entries.
2025-12-15 05:18:10,085 - INFO - Dataset processed with 300 entries.
2025-12-15 05:18:10,126 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:23<1:59:14, 23.93s/it]Running Inference:   1%|          | 2/300 [00:50<2:06:47, 25.53s/it]Running Inference:   1%|          | 3/300 [01:13<2:01:06, 24.47s/it]Running Inference:   1%|▏         | 4/300 [01:32<1:48:59, 22.09s/it]Running Inference:   2%|▏         | 5/300 [01:56<1:51:52, 22.75s/it]Running Inference:   2%|▏         | 6/300 [02:20<1:53:52, 23.24s/it]Running Inference:   2%|▏         | 7/300 [02:43<1:53:11, 23.18s/it]Running Inference:   3%|▎         | 8/300 [03:07<1:54:35, 23.55s/it]Running Inference:   3%|▎         | 9/300 [03:30<1:53:36, 23.43s/it]Running Inference:   3%|▎         | 10/300 [03:54<1:53:00, 23.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [04:17<1:52:57, 23.45s/it]Running Inference:   4%|▍         | 12/300 [04:40<1:52:00, 23.34s/it]Running Inference:   4%|▍         | 13/300 [05:05<1:52:53, 23.60s/it]Running Inference:   5%|▍         | 14/300 [05:28<1:51:46, 23.45s/it]Running Inference:   5%|▌         | 15/300 [05:44<1:41:29, 21.37s/it]Running Inference:   5%|▌         | 16/300 [06:08<1:44:52, 22.16s/it]Running Inference:   6%|▌         | 17/300 [06:29<1:43:15, 21.89s/it]Running Inference:   6%|▌         | 18/300 [06:35<1:19:43, 16.96s/it]Running Inference:   6%|▋         | 19/300 [06:58<1:28:28, 18.89s/it]Running Inference:   7%|▋         | 20/300 [07:24<1:37:11, 20.83s/it]Running Inference:   7%|▋         | 21/300 [07:47<1:40:01, 21.51s/it]Running Inference:   7%|▋         | 22/300 [08:11<1:43:57, 22.44s/it]Running Inference:   8%|▊         | 23/300 [08:36<1:47:09, 23.21s/it]Running Inference:   8%|▊         | 24/300 [09:00<1:47:01, 23.26s/it]Running Inference:   8%|▊         | 25/300 [09:06<1:23:34, 18.23s/it]Running Inference:   9%|▊         | 26/300 [09:30<1:30:57, 19.92s/it]Running Inference:   9%|▉         | 27/300 [09:53<1:35:08, 20.91s/it]Running Inference:   9%|▉         | 28/300 [10:17<1:38:16, 21.68s/it]Running Inference:  10%|▉         | 29/300 [10:40<1:39:50, 22.11s/it]Running Inference:  10%|█         | 30/300 [11:04<1:41:45, 22.61s/it]Running Inference:  10%|█         | 31/300 [11:29<1:44:43, 23.36s/it]Running Inference:  11%|█         | 32/300 [11:52<1:44:21, 23.36s/it]Running Inference:  11%|█         | 33/300 [12:17<1:45:59, 23.82s/it]Running Inference:  11%|█▏        | 34/300 [12:41<1:45:18, 23.76s/it]Running Inference:  12%|█▏        | 35/300 [13:04<1:44:09, 23.58s/it]Running Inference:  12%|█▏        | 36/300 [13:27<1:43:12, 23.45s/it]Running Inference:  12%|█▏        | 37/300 [13:51<1:43:28, 23.61s/it]Running Inference:  13%|█▎        | 38/300 [14:15<1:43:58, 23.81s/it]Running Inference:  13%|█▎        | 39/300 [14:39<1:43:26, 23.78s/it]Running Inference:  13%|█▎        | 40/300 [15:03<1:43:16, 23.83s/it]Running Inference:  14%|█▎        | 41/300 [15:24<1:38:55, 22.92s/it]Running Inference:  14%|█▍        | 42/300 [15:47<1:39:26, 23.12s/it]Running Inference:  14%|█▍        | 43/300 [16:11<1:39:14, 23.17s/it]Running Inference:  15%|█▍        | 44/300 [16:34<1:38:52, 23.17s/it]Running Inference:  15%|█▌        | 45/300 [16:58<1:39:33, 23.43s/it]Running Inference:  15%|█▌        | 46/300 [17:21<1:39:27, 23.49s/it]Running Inference:  16%|█▌        | 47/300 [17:45<1:39:04, 23.50s/it]Running Inference:  16%|█▌        | 48/300 [18:09<1:39:25, 23.67s/it]Running Inference:  16%|█▋        | 49/300 [18:32<1:38:14, 23.48s/it]Running Inference:  17%|█▋        | 50/300 [18:56<1:38:39, 23.68s/it]Running Inference:  17%|█▋        | 51/300 [19:19<1:37:40, 23.54s/it]Running Inference:  17%|█▋        | 52/300 [19:43<1:37:36, 23.61s/it]Running Inference:  18%|█▊        | 53/300 [20:06<1:36:29, 23.44s/it]Running Inference:  18%|█▊        | 54/300 [20:21<1:25:17, 20.80s/it]Running Inference:  18%|█▊        | 55/300 [20:46<1:29:44, 21.98s/it]Running Inference:  19%|█▊        | 56/300 [21:01<1:21:02, 19.93s/it]Running Inference:  19%|█▉        | 57/300 [21:24<1:25:02, 21.00s/it]Running Inference:  19%|█▉        | 58/300 [21:48<1:27:47, 21.76s/it]Running Inference:  20%|█▉        | 59/300 [22:12<1:30:17, 22.48s/it]Running Inference:  20%|██        | 60/300 [22:27<1:21:22, 20.34s/it]Running Inference:  20%|██        | 61/300 [22:51<1:24:25, 21.20s/it]Running Inference:  21%|██        | 62/300 [23:14<1:27:06, 21.96s/it]Running Inference:  21%|██        | 63/300 [23:38<1:28:18, 22.35s/it]Running Inference:  21%|██▏       | 64/300 [24:01<1:29:25, 22.74s/it]Running Inference:  22%|██▏       | 65/300 [24:25<1:30:40, 23.15s/it]Running Inference:  22%|██▏       | 66/300 [24:49<1:30:48, 23.28s/it]Running Inference:  22%|██▏       | 67/300 [25:13<1:31:31, 23.57s/it]Running Inference:  23%|██▎       | 68/300 [25:37<1:31:30, 23.67s/it]Running Inference:  23%|██▎       | 69/300 [26:01<1:31:53, 23.87s/it]Running Inference:  23%|██▎       | 70/300 [26:29<1:35:51, 25.01s/it]Running Inference:  24%|██▎       | 71/300 [26:52<1:33:29, 24.49s/it]Running Inference:  24%|██▍       | 72/300 [27:12<1:27:45, 23.10s/it]Running Inference:  24%|██▍       | 73/300 [27:36<1:28:01, 23.27s/it]Running Inference:  25%|██▍       | 74/300 [28:00<1:28:44, 23.56s/it]Running Inference:  25%|██▌       | 75/300 [28:23<1:27:51, 23.43s/it]Running Inference:  25%|██▌       | 76/300 [28:46<1:27:18, 23.39s/it]Running Inference:  26%|██▌       | 77/300 [29:11<1:28:25, 23.79s/it]Running Inference:  26%|██▌       | 78/300 [29:29<1:21:52, 22.13s/it]Running Inference:  26%|██▋       | 79/300 [29:53<1:22:45, 22.47s/it]Running Inference:  27%|██▋       | 80/300 [30:17<1:24:01, 22.92s/it]Running Inference:  27%|██▋       | 81/300 [30:41<1:25:27, 23.41s/it]Running Inference:  27%|██▋       | 82/300 [31:04<1:24:42, 23.31s/it]Running Inference:  28%|██▊       | 83/300 [31:28<1:24:12, 23.28s/it]Running Inference:  28%|██▊       | 84/300 [31:51<1:23:44, 23.26s/it]Running Inference:  28%|██▊       | 85/300 [32:14<1:23:05, 23.19s/it]Running Inference:  29%|██▊       | 86/300 [32:34<1:19:31, 22.30s/it]Running Inference:  29%|██▉       | 87/300 [33:00<1:23:31, 23.53s/it]Running Inference:  29%|██▉       | 88/300 [33:24<1:23:19, 23.58s/it]Running Inference:  30%|██▉       | 89/300 [33:42<1:17:18, 21.98s/it]Running Inference:  30%|███       | 90/300 [34:05<1:17:09, 22.04s/it]Running Inference:  30%|███       | 91/300 [34:28<1:18:18, 22.48s/it]Running Inference:  31%|███       | 92/300 [34:51<1:18:38, 22.69s/it]Running Inference:  31%|███       | 93/300 [35:15<1:19:31, 23.05s/it]Running Inference:  31%|███▏      | 94/300 [35:18<58:02, 16.90s/it]  Running Inference:  32%|███▏      | 95/300 [35:41<1:04:01, 18.74s/it]Running Inference:  32%|███▏      | 96/300 [36:04<1:08:31, 20.15s/it]Running Inference:  32%|███▏      | 97/300 [36:27<1:11:25, 21.11s/it]Running Inference:  33%|███▎      | 98/300 [36:51<1:13:26, 21.81s/it]Running Inference:  33%|███▎      | 99/300 [37:16<1:16:31, 22.84s/it]Running Inference:  33%|███▎      | 100/300 [37:41<1:18:19, 23.50s/it]Running Inference:  34%|███▎      | 101/300 [38:05<1:18:16, 23.60s/it]Running Inference:  34%|███▍      | 102/300 [38:28<1:17:31, 23.49s/it]Running Inference:  34%|███▍      | 103/300 [38:53<1:18:06, 23.79s/it]Running Inference:  35%|███▍      | 104/300 [39:16<1:17:22, 23.69s/it]Running Inference:  35%|███▌      | 105/300 [39:36<1:13:22, 22.58s/it]Running Inference:  35%|███▌      | 106/300 [39:59<1:13:37, 22.77s/it]Running Inference:  36%|███▌      | 107/300 [40:23<1:14:10, 23.06s/it]Running Inference:  36%|███▌      | 108/300 [40:46<1:13:52, 23.08s/it]Running Inference:  36%|███▋      | 109/300 [41:04<1:08:16, 21.45s/it]Running Inference:  37%|███▋      | 110/300 [41:28<1:10:03, 22.13s/it]Running Inference:  37%|███▋      | 111/300 [41:51<1:10:58, 22.53s/it]Running Inference:  37%|███▋      | 112/300 [42:17<1:13:45, 23.54s/it]Running Inference:  38%|███▊      | 113/300 [42:41<1:13:22, 23.54s/it]Running Inference:  38%|███▊      | 114/300 [43:04<1:13:13, 23.62s/it]Running Inference:  38%|███▊      | 115/300 [43:28<1:12:34, 23.54s/it]Running Inference:  39%|███▊      | 116/300 [43:51<1:11:42, 23.39s/it]Running Inference:  39%|███▉      | 117/300 [44:09<1:06:41, 21.87s/it]Running Inference:  39%|███▉      | 118/300 [44:32<1:07:26, 22.23s/it]Running Inference:  40%|███▉      | 119/300 [44:55<1:07:53, 22.50s/it]Running Inference:  40%|████      | 120/300 [45:18<1:07:59, 22.66s/it]Running Inference:  40%|████      | 121/300 [45:42<1:08:48, 23.06s/it]Running Inference:  41%|████      | 122/300 [46:02<1:05:26, 22.06s/it]Running Inference:  41%|████      | 123/300 [46:26<1:06:48, 22.64s/it]Running Inference:  41%|████▏     | 124/300 [46:49<1:07:01, 22.85s/it]Running Inference:  42%|████▏     | 125/300 [47:13<1:07:37, 23.19s/it]Running Inference:  42%|████▏     | 126/300 [47:15<48:38, 16.77s/it]  Running Inference:  42%|████▏     | 127/300 [47:42<57:18, 19.88s/it]Running Inference:  43%|████▎     | 128/300 [48:05<59:50, 20.88s/it]Running Inference:  43%|████▎     | 129/300 [48:29<1:01:33, 21.60s/it]Running Inference:  43%|████▎     | 130/300 [48:50<1:00:37, 21.40s/it]Running Inference:  44%|████▎     | 131/300 [49:13<1:02:13, 22.09s/it]Running Inference:  44%|████▍     | 132/300 [49:37<1:03:15, 22.60s/it]Running Inference:  44%|████▍     | 133/300 [50:01<1:03:50, 22.94s/it]Running Inference:  45%|████▍     | 134/300 [50:06<48:27, 17.52s/it]  Running Inference:  45%|████▌     | 135/300 [50:29<52:58, 19.27s/it]Running Inference:  45%|████▌     | 136/300 [50:54<57:06, 20.89s/it]Running Inference:  46%|████▌     | 137/300 [51:09<52:20, 19.26s/it]Running Inference:  46%|████▌     | 138/300 [51:33<55:37, 20.60s/it]Running Inference:  46%|████▋     | 139/300 [51:57<57:54, 21.58s/it]Running Inference:  47%|████▋     | 140/300 [52:21<59:38, 22.37s/it]Running Inference:  47%|████▋     | 141/300 [52:39<55:31, 20.95s/it]Running Inference:  47%|████▋     | 142/300 [53:02<57:17, 21.76s/it]Running Inference:  48%|████▊     | 143/300 [53:26<58:02, 22.18s/it]Running Inference:  48%|████▊     | 144/300 [53:49<58:26, 22.48s/it]Running Inference:  48%|████▊     | 145/300 [53:51<42:32, 16.47s/it]Running Inference:  49%|████▊     | 146/300 [54:14<47:20, 18.45s/it]Running Inference:  49%|████▉     | 147/300 [54:37<50:45, 19.90s/it]Running Inference:  49%|████▉     | 148/300 [55:04<55:08, 21.77s/it]Running Inference:  50%|████▉     | 149/300 [55:27<55:56, 22.23s/it]Running Inference:  50%|█████     | 150/300 [55:50<56:17, 22.52s/it]Running Inference:  50%|█████     | 151/300 [56:14<56:35, 22.79s/it]Running Inference:  51%|█████     | 152/300 [56:37<56:26, 22.88s/it]Running Inference:  51%|█████     | 153/300 [57:00<56:41, 23.14s/it]Running Inference:  51%|█████▏    | 154/300 [57:24<56:30, 23.22s/it]Running Inference:  52%|█████▏    | 155/300 [57:48<56:59, 23.58s/it]Running Inference:  52%|█████▏    | 156/300 [58:12<56:31, 23.55s/it]Running Inference:  52%|█████▏    | 157/300 [58:35<56:15, 23.61s/it]Running Inference:  53%|█████▎    | 158/300 [58:59<56:06, 23.70s/it]Running Inference:  53%|█████▎    | 159/300 [59:23<55:29, 23.61s/it]Running Inference:  53%|█████▎    | 160/300 [59:46<54:46, 23.48s/it]Running Inference:  54%|█████▎    | 161/300 [1:00:09<54:20, 23.46s/it]Running Inference:  54%|█████▍    | 162/300 [1:00:33<54:06, 23.53s/it]Running Inference:  54%|█████▍    | 163/300 [1:00:56<53:33, 23.45s/it]Running Inference:  55%|█████▍    | 164/300 [1:01:19<52:53, 23.34s/it]Running Inference:  55%|█████▌    | 165/300 [1:01:42<52:23, 23.29s/it]Running Inference:  55%|█████▌    | 166/300 [1:01:45<38:03, 17.04s/it]Running Inference:  56%|█████▌    | 167/300 [1:02:08<41:50, 18.88s/it]Running Inference:  56%|█████▌    | 168/300 [1:02:31<44:27, 20.21s/it]Running Inference:  56%|█████▋    | 169/300 [1:02:55<46:07, 21.13s/it]Running Inference:  57%|█████▋    | 170/300 [1:03:15<45:28, 20.99s/it]Running Inference:  57%|█████▋    | 171/300 [1:03:39<46:53, 21.81s/it]Running Inference:  57%|█████▋    | 172/300 [1:04:03<47:36, 22.32s/it]Running Inference:  58%|█████▊    | 173/300 [1:04:26<47:43, 22.55s/it]Running Inference:  58%|█████▊    | 174/300 [1:04:50<48:11, 22.95s/it]Running Inference:  58%|█████▊    | 175/300 [1:05:13<48:16, 23.18s/it]Running Inference:  59%|█████▊    | 176/300 [1:05:37<47:56, 23.20s/it]Running Inference:  59%|█████▉    | 177/300 [1:06:00<47:36, 23.22s/it]Running Inference:  59%|█████▉    | 178/300 [1:06:22<46:29, 22.87s/it]Running Inference:  60%|█████▉    | 179/300 [1:06:28<35:55, 17.82s/it]Running Inference:  60%|██████    | 180/300 [1:06:51<38:50, 19.42s/it]Running Inference:  60%|██████    | 181/300 [1:07:14<40:48, 20.57s/it]Running Inference:  61%|██████    | 182/300 [1:07:37<41:59, 21.35s/it]Running Inference:  61%|██████    | 183/300 [1:08:01<42:45, 21.92s/it]Running Inference:  61%|██████▏   | 184/300 [1:08:24<43:11, 22.34s/it]Running Inference:  62%|██████▏   | 185/300 [1:08:49<44:32, 23.24s/it]Running Inference:  62%|██████▏   | 186/300 [1:09:13<44:18, 23.32s/it]Running Inference:  62%|██████▏   | 187/300 [1:09:36<43:49, 23.27s/it]Running Inference:  63%|██████▎   | 188/300 [1:10:00<43:50, 23.49s/it]Running Inference:  63%|██████▎   | 189/300 [1:10:22<42:52, 23.17s/it]Running Inference:  63%|██████▎   | 190/300 [1:10:46<42:29, 23.17s/it]Running Inference:  64%|██████▎   | 191/300 [1:11:10<42:37, 23.46s/it]Running Inference:  64%|██████▍   | 192/300 [1:11:29<40:11, 22.33s/it]Running Inference:  64%|██████▍   | 193/300 [1:11:57<42:36, 23.89s/it]Running Inference:  65%|██████▍   | 194/300 [1:12:22<42:43, 24.18s/it]Running Inference:  65%|██████▌   | 195/300 [1:12:47<42:55, 24.53s/it]Running Inference:  65%|██████▌   | 196/300 [1:13:07<40:12, 23.20s/it]Running Inference:  66%|██████▌   | 197/300 [1:13:31<39:49, 23.20s/it]Running Inference:  66%|██████▌   | 198/300 [1:13:54<39:42, 23.36s/it]Running Inference:  66%|██████▋   | 199/300 [1:14:20<40:16, 23.93s/it]Running Inference:  67%|██████▋   | 200/300 [1:14:43<39:31, 23.71s/it]Running Inference:  67%|██████▋   | 201/300 [1:15:06<39:04, 23.68s/it]Running Inference:  67%|██████▋   | 202/300 [1:15:30<38:29, 23.57s/it]Running Inference:  68%|██████▊   | 203/300 [1:15:53<38:05, 23.56s/it]Running Inference:  68%|██████▊   | 204/300 [1:16:17<37:55, 23.70s/it]Running Inference:  68%|██████▊   | 205/300 [1:16:41<37:45, 23.84s/it]Running Inference:  69%|██████▊   | 206/300 [1:17:01<35:17, 22.53s/it]Running Inference:  69%|██████▉   | 207/300 [1:17:27<36:23, 23.48s/it]Running Inference:  69%|██████▉   | 208/300 [1:17:50<36:06, 23.55s/it]Running Inference:  70%|██████▉   | 209/300 [1:18:15<36:15, 23.90s/it]Running Inference:  70%|███████   | 210/300 [1:18:39<35:42, 23.81s/it]Running Inference:  70%|███████   | 211/300 [1:19:01<34:47, 23.45s/it]Running Inference:  71%|███████   | 212/300 [1:19:28<35:52, 24.46s/it]Running Inference:  71%|███████   | 213/300 [1:19:51<34:54, 24.08s/it]Running Inference:  71%|███████▏  | 214/300 [1:20:15<34:15, 23.90s/it]Running Inference:  72%|███████▏  | 215/300 [1:20:38<33:31, 23.66s/it]Running Inference:  72%|███████▏  | 216/300 [1:21:01<32:59, 23.56s/it]Running Inference:  72%|███████▏  | 217/300 [1:21:04<23:59, 17.34s/it]Running Inference:  73%|███████▎  | 218/300 [1:21:29<26:53, 19.68s/it]Running Inference:  73%|███████▎  | 219/300 [1:21:52<27:57, 20.70s/it]Running Inference:  73%|███████▎  | 220/300 [1:22:15<28:32, 21.40s/it]Running Inference:  74%|███████▎  | 221/300 [1:22:39<28:56, 21.98s/it]Running Inference:  74%|███████▍  | 222/300 [1:23:03<29:41, 22.83s/it]Running Inference:  74%|███████▍  | 223/300 [1:23:28<29:57, 23.34s/it]Running Inference:  75%|███████▍  | 224/300 [1:23:52<29:51, 23.57s/it]Running Inference:  75%|███████▌  | 225/300 [1:24:15<29:21, 23.49s/it]Running Inference:  75%|███████▌  | 226/300 [1:24:39<28:59, 23.51s/it]Running Inference:  76%|███████▌  | 227/300 [1:25:03<28:46, 23.65s/it]Running Inference:  76%|███████▌  | 228/300 [1:25:20<26:05, 21.74s/it]Running Inference:  76%|███████▋  | 229/300 [1:25:45<26:55, 22.75s/it]Running Inference:  77%|███████▋  | 230/300 [1:25:48<19:35, 16.79s/it]Running Inference:  77%|███████▋  | 231/300 [1:26:12<21:37, 18.81s/it]Running Inference:  77%|███████▋  | 232/300 [1:26:36<23:06, 20.39s/it]Running Inference:  78%|███████▊  | 233/300 [1:27:03<25:04, 22.45s/it]Running Inference:  78%|███████▊  | 234/300 [1:27:26<24:55, 22.65s/it]Running Inference:  78%|███████▊  | 235/300 [1:27:50<24:52, 22.97s/it]Running Inference:  79%|███████▊  | 236/300 [1:28:13<24:33, 23.03s/it]Running Inference:  79%|███████▉  | 237/300 [1:28:38<24:45, 23.58s/it]Running Inference:  79%|███████▉  | 238/300 [1:29:01<24:15, 23.48s/it]Running Inference:  80%|███████▉  | 239/300 [1:29:25<23:58, 23.58s/it]Running Inference:  80%|████████  | 240/300 [1:29:49<23:37, 23.63s/it]Running Inference:  80%|████████  | 241/300 [1:30:12<23:06, 23.50s/it]Running Inference:  81%|████████  | 242/300 [1:30:35<22:37, 23.41s/it]Running Inference:  81%|████████  | 243/300 [1:30:59<22:19, 23.49s/it]Running Inference:  81%|████████▏ | 244/300 [1:31:24<22:22, 23.98s/it]Running Inference:  82%|████████▏ | 245/300 [1:31:49<22:10, 24.20s/it]Running Inference:  82%|████████▏ | 246/300 [1:32:12<21:32, 23.93s/it]Running Inference:  82%|████████▏ | 247/300 [1:32:37<21:22, 24.19s/it]Running Inference:  83%|████████▎ | 248/300 [1:33:02<21:20, 24.62s/it]Running Inference:  83%|████████▎ | 249/300 [1:33:26<20:49, 24.50s/it]Running Inference:  83%|████████▎ | 250/300 [1:33:50<20:11, 24.23s/it]Running Inference:  84%|████████▎ | 251/300 [1:34:14<19:36, 24.02s/it]Running Inference:  84%|████████▍ | 252/300 [1:34:38<19:17, 24.11s/it]Running Inference:  84%|████████▍ | 253/300 [1:35:01<18:40, 23.84s/it]Running Inference:  85%|████████▍ | 254/300 [1:35:23<17:47, 23.21s/it]Running Inference:  85%|████████▌ | 255/300 [1:35:46<17:22, 23.17s/it]Running Inference:  85%|████████▌ | 256/300 [1:35:49<12:30, 17.06s/it]Running Inference:  86%|████████▌ | 257/300 [1:36:07<12:29, 17.43s/it]Running Inference:  86%|████████▌ | 258/300 [1:36:30<13:26, 19.21s/it]Running Inference:  86%|████████▋ | 259/300 [1:36:56<14:22, 21.03s/it]Running Inference:  87%|████████▋ | 260/300 [1:37:19<14:30, 21.77s/it]Running Inference:  87%|████████▋ | 261/300 [1:37:43<14:27, 22.24s/it]Running Inference:  87%|████████▋ | 262/300 [1:37:50<11:17, 17.84s/it]Running Inference:  88%|████████▊ | 263/300 [1:38:14<12:04, 19.57s/it]Running Inference:  88%|████████▊ | 264/300 [1:38:15<08:23, 13.98s/it]Running Inference:  88%|████████▊ | 265/300 [1:38:38<09:46, 16.77s/it]Running Inference:  89%|████████▊ | 266/300 [1:38:47<08:09, 14.39s/it]Running Inference:  89%|████████▉ | 267/300 [1:39:11<09:28, 17.24s/it]Running Inference:  89%|████████▉ | 268/300 [1:39:13<06:53, 12.91s/it]Running Inference:  90%|████████▉ | 269/300 [1:39:38<08:28, 16.42s/it]Running Inference:  90%|█████████ | 270/300 [1:40:02<09:23, 18.78s/it]Running Inference:  90%|█████████ | 271/300 [1:40:26<09:49, 20.33s/it]Running Inference:  91%|█████████ | 272/300 [1:40:52<10:15, 21.97s/it]Running Inference:  91%|█████████ | 273/300 [1:41:17<10:17, 22.89s/it]Running Inference:  91%|█████████▏| 274/300 [1:41:41<10:04, 23.25s/it]Running Inference:  92%|█████████▏| 275/300 [1:42:05<09:45, 23.42s/it]Running Inference:  92%|█████████▏| 276/300 [1:42:28<09:21, 23.38s/it]Running Inference:  92%|█████████▏| 277/300 [1:42:53<09:04, 23.69s/it]Running Inference:  93%|█████████▎| 278/300 [1:43:16<08:39, 23.62s/it]Running Inference:  93%|█████████▎| 279/300 [1:43:40<08:15, 23.59s/it]Running Inference:  93%|█████████▎| 280/300 [1:44:03<07:52, 23.63s/it]Running Inference:  94%|█████████▎| 281/300 [1:44:27<07:29, 23.67s/it]Running Inference:  94%|█████████▍| 282/300 [1:44:29<05:07, 17.11s/it]Running Inference:  94%|█████████▍| 283/300 [1:44:53<05:24, 19.09s/it]Running Inference:  95%|█████████▍| 284/300 [1:45:16<05:25, 20.32s/it]Running Inference:  95%|█████████▌| 285/300 [1:45:41<05:26, 21.77s/it]Running Inference:  95%|█████████▌| 286/300 [1:46:08<05:28, 23.44s/it]Running Inference:  96%|█████████▌| 287/300 [1:46:32<05:05, 23.52s/it]Running Inference:  96%|█████████▌| 288/300 [1:46:55<04:41, 23.48s/it]Running Inference:  96%|█████████▋| 289/300 [1:47:19<04:17, 23.44s/it]Running Inference:  97%|█████████▋| 290/300 [1:47:43<03:57, 23.77s/it]Running Inference:  97%|█████████▋| 291/300 [1:48:08<03:35, 23.89s/it]Running Inference:  97%|█████████▋| 292/300 [1:48:31<03:09, 23.71s/it]Running Inference:  98%|█████████▊| 293/300 [1:48:54<02:45, 23.69s/it]Running Inference:  98%|█████████▊| 294/300 [1:49:17<02:19, 23.32s/it]Running Inference:  98%|█████████▊| 295/300 [1:49:40<01:56, 23.32s/it]Running Inference:  99%|█████████▊| 296/300 [1:50:05<01:34, 23.61s/it]Running Inference:  99%|█████████▉| 297/300 [1:50:28<01:10, 23.44s/it]Running Inference:  99%|█████████▉| 298/300 [1:50:51<00:46, 23.35s/it]Running Inference: 100%|█████████▉| 299/300 [1:51:14<00:23, 23.43s/it]Running Inference: 100%|██████████| 300/300 [1:51:38<00:00, 23.38s/it]Running Inference: 100%|██████████| 300/300 [1:51:38<00:00, 22.33s/it]
2025-12-15 07:09:48,246 - INFO - Inference completed.
2025-12-15 07:09:48,293 - INFO - Results saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-15 07:09:48,293 - INFO - Calculating metrics for dataset: longbench
2025-12-15 07:10:14,508 - INFO - Metrics saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-15 07:10:14,508 - INFO - Metrics:
14.34
2025-12-15 07:10:14,509 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report_e, ratio=0.3) on GPU 1

----------------------------------------
Task: gov_report_e | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 07:10:21,845 - INFO - Set deterministic seeds to 42
2025-12-15 07:10:21,845 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 07:10:21,845 - INFO - Starting evaluation run...
2025-12-15 07:10:21,845 - INFO - Output directory set to: longbenchresult
2025-12-15 07:10:21,845 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-15 07:10:21,845 - INFO - KV Press 'tova' setup.
2025-12-15 07:10:21,845 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 07:10:21,845 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.23it/s]
Device set to use cuda:0
2025-12-15 07:10:32,885 - INFO - Model pipeline loaded.
2025-12-15 07:10:32,885 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report_e)
2025-12-15 07:10:36,970 - INFO - Dataset loaded with 300 entries.
2025-12-15 07:10:36,970 - INFO - Dataset processed with 300 entries.
2025-12-15 07:10:37,008 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:23<1:57:14, 23.53s/it]Running Inference:   1%|          | 2/300 [00:49<2:03:38, 24.89s/it]Running Inference:   1%|          | 3/300 [01:12<1:58:37, 23.96s/it]Running Inference:   1%|▏         | 4/300 [01:34<1:55:42, 23.46s/it]Running Inference:   2%|▏         | 5/300 [01:58<1:55:29, 23.49s/it]Running Inference:   2%|▏         | 6/300 [02:22<1:55:41, 23.61s/it]Running Inference:   2%|▏         | 7/300 [02:45<1:53:51, 23.32s/it]Running Inference:   3%|▎         | 8/300 [03:08<1:54:29, 23.52s/it]Running Inference:   3%|▎         | 9/300 [03:31<1:52:57, 23.29s/it]Running Inference:   3%|▎         | 10/300 [03:54<1:52:01, 23.18s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [04:17<1:51:45, 23.20s/it]Running Inference:   4%|▍         | 12/300 [04:40<1:50:46, 23.08s/it]Running Inference:   4%|▍         | 13/300 [05:04<1:51:28, 23.31s/it]Running Inference:   5%|▍         | 14/300 [05:27<1:50:17, 23.14s/it]Running Inference:   5%|▌         | 15/300 [05:50<1:49:21, 23.02s/it]Running Inference:   5%|▌         | 16/300 [06:13<1:49:52, 23.21s/it]Running Inference:   6%|▌         | 17/300 [06:36<1:48:47, 23.07s/it]Running Inference:   6%|▌         | 18/300 [06:55<1:42:58, 21.91s/it]Running Inference:   6%|▋         | 19/300 [07:18<1:44:15, 22.26s/it]Running Inference:   7%|▋         | 20/300 [07:43<1:47:46, 23.09s/it]Running Inference:   7%|▋         | 21/300 [08:06<1:46:55, 23.00s/it]Running Inference:   7%|▋         | 22/300 [08:30<1:48:19, 23.38s/it]Running Inference:   8%|▊         | 23/300 [08:55<1:49:41, 23.76s/it]Running Inference:   8%|▊         | 24/300 [09:18<1:48:22, 23.56s/it]Running Inference:   8%|▊         | 25/300 [09:42<1:48:55, 23.77s/it]Running Inference:   9%|▊         | 26/300 [10:06<1:48:11, 23.69s/it]Running Inference:   9%|▉         | 27/300 [10:29<1:46:37, 23.43s/it]Running Inference:   9%|▉         | 28/300 [10:52<1:45:49, 23.34s/it]Running Inference:  10%|▉         | 29/300 [11:15<1:44:37, 23.17s/it]Running Inference:  10%|█         | 30/300 [11:38<1:44:37, 23.25s/it]Running Inference:  10%|█         | 31/300 [11:56<1:37:08, 21.67s/it]Running Inference:  11%|█         | 32/300 [12:19<1:38:38, 22.09s/it]Running Inference:  11%|█         | 33/300 [12:44<1:41:30, 22.81s/it]Running Inference:  11%|█▏        | 34/300 [13:07<1:41:45, 22.95s/it]Running Inference:  12%|█▏        | 35/300 [13:30<1:41:09, 22.90s/it]Running Inference:  12%|█▏        | 36/300 [13:52<1:40:34, 22.86s/it]Running Inference:  12%|█▏        | 37/300 [14:16<1:41:13, 23.09s/it]Running Inference:  13%|█▎        | 38/300 [14:40<1:41:57, 23.35s/it]Running Inference:  13%|█▎        | 39/300 [15:04<1:42:23, 23.54s/it]Running Inference:  13%|█▎        | 40/300 [15:28<1:42:07, 23.57s/it]Running Inference:  14%|█▎        | 41/300 [15:51<1:41:20, 23.48s/it]Running Inference:  14%|█▍        | 42/300 [16:14<1:40:39, 23.41s/it]Running Inference:  14%|█▍        | 43/300 [16:37<1:39:41, 23.27s/it]Running Inference:  15%|█▍        | 44/300 [17:00<1:38:48, 23.16s/it]Running Inference:  15%|█▌        | 45/300 [17:24<1:39:06, 23.32s/it]Running Inference:  15%|█▌        | 46/300 [17:47<1:38:41, 23.31s/it]Running Inference:  16%|█▌        | 47/300 [18:10<1:38:08, 23.27s/it]Running Inference:  16%|█▌        | 48/300 [18:34<1:38:22, 23.42s/it]Running Inference:  16%|█▋        | 49/300 [18:57<1:37:05, 23.21s/it]Running Inference:  17%|█▋        | 50/300 [19:16<1:32:20, 22.16s/it]Running Inference:  17%|█▋        | 51/300 [19:39<1:32:53, 22.38s/it]Running Inference:  17%|█▋        | 52/300 [20:03<1:33:50, 22.70s/it]Running Inference:  18%|█▊        | 53/300 [20:25<1:33:28, 22.71s/it]Running Inference:  18%|█▊        | 54/300 [20:48<1:33:14, 22.74s/it]Running Inference:  18%|█▊        | 55/300 [21:13<1:34:53, 23.24s/it]Running Inference:  19%|█▊        | 56/300 [21:35<1:34:06, 23.14s/it]Running Inference:  19%|█▉        | 57/300 [21:59<1:33:43, 23.14s/it]Running Inference:  19%|█▉        | 58/300 [22:22<1:33:22, 23.15s/it]Running Inference:  20%|█▉        | 59/300 [22:46<1:33:43, 23.33s/it]Running Inference:  20%|██        | 60/300 [23:09<1:33:07, 23.28s/it]Running Inference:  20%|██        | 61/300 [23:32<1:32:09, 23.13s/it]Running Inference:  21%|██        | 62/300 [23:55<1:32:08, 23.23s/it]Running Inference:  21%|██        | 63/300 [24:18<1:31:23, 23.14s/it]Running Inference:  21%|██▏       | 64/300 [24:41<1:31:11, 23.18s/it]Running Inference:  22%|██▏       | 65/300 [25:05<1:31:28, 23.35s/it]Running Inference:  22%|██▏       | 66/300 [25:28<1:30:56, 23.32s/it]Running Inference:  22%|██▏       | 67/300 [25:52<1:31:12, 23.49s/it]Running Inference:  23%|██▎       | 68/300 [26:16<1:30:52, 23.50s/it]Running Inference:  23%|██▎       | 69/300 [26:40<1:31:02, 23.65s/it]Running Inference:  23%|██▎       | 70/300 [27:05<1:32:36, 24.16s/it]Running Inference:  24%|██▎       | 71/300 [27:28<1:30:50, 23.80s/it]Running Inference:  24%|██▍       | 72/300 [27:52<1:30:14, 23.75s/it]Running Inference:  24%|██▍       | 73/300 [28:15<1:29:20, 23.61s/it]Running Inference:  25%|██▍       | 74/300 [28:39<1:29:13, 23.69s/it]Running Inference:  25%|██▌       | 75/300 [29:01<1:27:44, 23.40s/it]Running Inference:  25%|██▌       | 76/300 [29:24<1:26:49, 23.26s/it]Running Inference:  26%|██▌       | 77/300 [29:49<1:27:41, 23.60s/it]Running Inference:  26%|██▌       | 78/300 [30:12<1:26:31, 23.38s/it]Running Inference:  26%|██▋       | 79/300 [30:34<1:25:35, 23.24s/it]Running Inference:  27%|██▋       | 80/300 [30:58<1:25:35, 23.34s/it]Running Inference:  27%|██▋       | 81/300 [31:22<1:26:07, 23.60s/it]Running Inference:  27%|██▋       | 82/300 [31:45<1:24:46, 23.33s/it]Running Inference:  28%|██▊       | 83/300 [32:08<1:23:52, 23.19s/it]Running Inference:  28%|██▊       | 84/300 [32:31<1:23:04, 23.08s/it]Running Inference:  28%|██▊       | 85/300 [32:54<1:22:56, 23.15s/it]Running Inference:  29%|██▊       | 86/300 [33:18<1:23:28, 23.40s/it]Running Inference:  29%|██▉       | 87/300 [33:44<1:25:31, 24.09s/it]Running Inference:  29%|██▉       | 88/300 [34:07<1:24:23, 23.88s/it]Running Inference:  30%|██▉       | 89/300 [34:25<1:18:08, 22.22s/it]Running Inference:  30%|███       | 90/300 [34:48<1:18:21, 22.39s/it]Running Inference:  30%|███       | 91/300 [35:11<1:18:46, 22.62s/it]Running Inference:  31%|███       | 92/300 [35:34<1:18:34, 22.67s/it]Running Inference:  31%|███       | 93/300 [35:58<1:19:06, 22.93s/it]Running Inference:  31%|███▏      | 94/300 [36:00<57:42, 16.81s/it]  Running Inference:  32%|███▏      | 95/300 [36:17<57:27, 16.82s/it]Running Inference:  32%|███▏      | 96/300 [36:40<1:03:33, 18.69s/it]Running Inference:  32%|███▏      | 97/300 [37:03<1:07:34, 19.97s/it]Running Inference:  33%|███▎      | 98/300 [37:26<1:10:22, 20.90s/it]Running Inference:  33%|███▎      | 99/300 [37:51<1:14:01, 22.09s/it]Running Inference:  33%|███▎      | 100/300 [38:16<1:16:12, 22.86s/it]Running Inference:  34%|███▎      | 101/300 [38:38<1:15:41, 22.82s/it]Running Inference:  34%|███▍      | 102/300 [39:01<1:15:19, 22.83s/it]Running Inference:  34%|███▍      | 103/300 [39:10<1:01:33, 18.75s/it]Running Inference:  35%|███▍      | 104/300 [39:34<1:05:27, 20.04s/it]Running Inference:  35%|███▌      | 105/300 [39:57<1:08:36, 21.11s/it]Running Inference:  35%|███▌      | 106/300 [40:20<1:09:56, 21.63s/it]Running Inference:  36%|███▌      | 107/300 [40:43<1:11:13, 22.14s/it]Running Inference:  36%|███▌      | 108/300 [41:06<1:11:25, 22.32s/it]Running Inference:  36%|███▋      | 109/300 [41:28<1:11:04, 22.33s/it]Running Inference:  37%|███▋      | 110/300 [41:52<1:11:38, 22.62s/it]Running Inference:  37%|███▋      | 111/300 [42:15<1:11:43, 22.77s/it]Running Inference:  37%|███▋      | 112/300 [42:40<1:13:44, 23.54s/it]Running Inference:  38%|███▊      | 113/300 [43:03<1:13:00, 23.42s/it]Running Inference:  38%|███▊      | 114/300 [43:21<1:07:41, 21.84s/it]Running Inference:  38%|███▊      | 115/300 [43:44<1:08:22, 22.18s/it]Running Inference:  39%|███▊      | 116/300 [44:07<1:08:29, 22.33s/it]Running Inference:  39%|███▉      | 117/300 [44:30<1:08:29, 22.45s/it]Running Inference:  39%|███▉      | 118/300 [44:53<1:08:22, 22.54s/it]Running Inference:  40%|███▉      | 119/300 [45:15<1:08:11, 22.61s/it]Running Inference:  40%|████      | 120/300 [45:38<1:07:53, 22.63s/it]Running Inference:  40%|████      | 121/300 [46:02<1:08:24, 22.93s/it]Running Inference:  41%|████      | 122/300 [46:24<1:07:56, 22.90s/it]Running Inference:  41%|████      | 123/300 [46:48<1:08:10, 23.11s/it]Running Inference:  41%|████▏     | 124/300 [47:11<1:07:40, 23.07s/it]Running Inference:  42%|████▏     | 125/300 [47:35<1:07:43, 23.22s/it]Running Inference:  42%|████▏     | 126/300 [47:58<1:07:44, 23.36s/it]Running Inference:  42%|████▏     | 127/300 [48:24<1:09:45, 24.19s/it]Running Inference:  43%|████▎     | 128/300 [48:47<1:08:11, 23.79s/it]Running Inference:  43%|████▎     | 129/300 [49:10<1:07:02, 23.53s/it]Running Inference:  43%|████▎     | 130/300 [49:35<1:08:05, 24.03s/it]Running Inference:  44%|████▎     | 131/300 [49:56<1:04:51, 23.03s/it]Running Inference:  44%|████▍     | 132/300 [50:20<1:04:49, 23.15s/it]Running Inference:  44%|████▍     | 133/300 [50:43<1:04:36, 23.22s/it]Running Inference:  45%|████▍     | 134/300 [50:48<48:59, 17.71s/it]  Running Inference:  45%|████▌     | 135/300 [51:11<53:02, 19.29s/it]Running Inference:  45%|████▌     | 136/300 [51:35<56:51, 20.80s/it]Running Inference:  46%|████▌     | 137/300 [51:59<58:46, 21.64s/it]Running Inference:  46%|████▌     | 138/300 [52:22<59:45, 22.14s/it]Running Inference:  46%|████▋     | 139/300 [52:45<1:00:28, 22.54s/it]Running Inference:  47%|████▋     | 140/300 [53:09<1:01:06, 22.91s/it]Running Inference:  47%|████▋     | 141/300 [53:28<57:27, 21.69s/it]  Running Inference:  47%|████▋     | 142/300 [53:51<58:19, 22.15s/it]Running Inference:  48%|████▊     | 143/300 [54:14<58:24, 22.32s/it]Running Inference:  48%|████▊     | 144/300 [54:28<51:37, 19.85s/it]Running Inference:  48%|████▊     | 145/300 [54:51<53:32, 20.73s/it]Running Inference:  49%|████▊     | 146/300 [55:14<54:42, 21.32s/it]Running Inference:  49%|████▉     | 147/300 [55:36<55:06, 21.61s/it]Running Inference:  49%|████▉     | 148/300 [55:50<49:17, 19.46s/it]Running Inference:  50%|████▉     | 149/300 [56:13<51:34, 20.50s/it]Running Inference:  50%|█████     | 150/300 [56:36<52:56, 21.18s/it]Running Inference:  50%|█████     | 151/300 [56:59<53:58, 21.74s/it]Running Inference:  51%|█████     | 152/300 [57:02<39:49, 16.14s/it]Running Inference:  51%|█████     | 153/300 [57:25<44:49, 18.30s/it]Running Inference:  51%|█████▏    | 154/300 [57:49<47:59, 19.72s/it]Running Inference:  52%|█████▏    | 155/300 [58:13<50:46, 21.01s/it]Running Inference:  52%|█████▏    | 156/300 [58:33<49:54, 20.80s/it]Running Inference:  52%|█████▏    | 157/300 [58:56<51:23, 21.56s/it]Running Inference:  53%|█████▎    | 158/300 [59:20<52:26, 22.16s/it]Running Inference:  53%|█████▎    | 159/300 [59:43<52:43, 22.43s/it]Running Inference:  53%|█████▎    | 160/300 [1:00:06<52:35, 22.54s/it]Running Inference:  54%|█████▎    | 161/300 [1:00:29<52:34, 22.69s/it]Running Inference:  54%|█████▍    | 162/300 [1:00:52<52:38, 22.89s/it]Running Inference:  54%|█████▍    | 163/300 [1:01:15<52:17, 22.90s/it]Running Inference:  55%|█████▍    | 164/300 [1:01:38<52:10, 23.02s/it]Running Inference:  55%|█████▌    | 165/300 [1:02:01<51:36, 22.94s/it]Running Inference:  55%|█████▌    | 166/300 [1:02:24<51:19, 22.98s/it]Running Inference:  56%|█████▌    | 167/300 [1:02:47<50:47, 22.92s/it]Running Inference:  56%|█████▌    | 168/300 [1:03:06<47:50, 21.75s/it]Running Inference:  56%|█████▋    | 169/300 [1:03:29<48:14, 22.09s/it]Running Inference:  57%|█████▋    | 170/300 [1:03:50<47:06, 21.74s/it]Running Inference:  57%|█████▋    | 171/300 [1:04:13<47:45, 22.22s/it]Running Inference:  57%|█████▋    | 172/300 [1:04:36<47:59, 22.49s/it]Running Inference:  58%|█████▊    | 173/300 [1:04:57<46:33, 22.00s/it]Running Inference:  58%|█████▊    | 174/300 [1:05:20<47:07, 22.44s/it]Running Inference:  58%|█████▊    | 175/300 [1:05:44<47:19, 22.72s/it]Running Inference:  59%|█████▊    | 176/300 [1:06:07<47:03, 22.77s/it]Running Inference:  59%|█████▉    | 177/300 [1:06:30<46:45, 22.81s/it]Running Inference:  59%|█████▉    | 178/300 [1:06:53<46:59, 23.11s/it]Running Inference:  60%|█████▉    | 179/300 [1:07:17<47:09, 23.38s/it]Running Inference:  60%|██████    | 180/300 [1:07:40<46:23, 23.20s/it]Running Inference:  60%|██████    | 181/300 [1:08:03<45:47, 23.09s/it]Running Inference:  61%|██████    | 182/300 [1:08:26<45:12, 22.99s/it]Running Inference:  61%|██████    | 183/300 [1:08:49<44:47, 22.97s/it]Running Inference:  61%|██████▏   | 184/300 [1:09:12<44:23, 22.96s/it]Running Inference:  62%|██████▏   | 185/300 [1:09:37<45:11, 23.57s/it]Running Inference:  62%|██████▏   | 186/300 [1:10:00<44:34, 23.46s/it]Running Inference:  62%|██████▏   | 187/300 [1:10:23<43:47, 23.25s/it]Running Inference:  63%|██████▎   | 188/300 [1:10:46<43:37, 23.37s/it]Running Inference:  63%|██████▎   | 189/300 [1:11:09<43:01, 23.26s/it]Running Inference:  63%|██████▎   | 190/300 [1:11:32<42:22, 23.11s/it]Running Inference:  64%|██████▎   | 191/300 [1:11:56<42:21, 23.31s/it]Running Inference:  64%|██████▍   | 192/300 [1:12:19<41:58, 23.32s/it]Running Inference:  64%|██████▍   | 193/300 [1:12:42<41:31, 23.28s/it]Running Inference:  65%|██████▍   | 194/300 [1:13:07<41:45, 23.64s/it]Running Inference:  65%|██████▌   | 195/300 [1:13:32<42:04, 24.04s/it]Running Inference:  65%|██████▌   | 196/300 [1:13:55<41:24, 23.89s/it]Running Inference:  66%|██████▌   | 197/300 [1:14:18<40:28, 23.58s/it]Running Inference:  66%|██████▌   | 198/300 [1:14:42<39:58, 23.52s/it]Running Inference:  66%|██████▋   | 199/300 [1:15:06<40:16, 23.93s/it]Running Inference:  67%|██████▋   | 200/300 [1:15:29<39:19, 23.59s/it]Running Inference:  67%|██████▋   | 201/300 [1:15:52<38:44, 23.48s/it]Running Inference:  67%|██████▋   | 202/300 [1:16:15<38:05, 23.32s/it]Running Inference:  68%|██████▊   | 203/300 [1:16:39<37:37, 23.27s/it]Running Inference:  68%|██████▊   | 204/300 [1:17:02<37:25, 23.39s/it]Running Inference:  68%|██████▊   | 205/300 [1:17:26<37:14, 23.52s/it]Running Inference:  69%|██████▊   | 206/300 [1:17:46<35:22, 22.58s/it]Running Inference:  69%|██████▉   | 207/300 [1:18:12<36:14, 23.38s/it]Running Inference:  69%|██████▉   | 208/300 [1:18:35<35:50, 23.38s/it]Running Inference:  70%|██████▉   | 209/300 [1:18:59<35:53, 23.66s/it]Running Inference:  70%|███████   | 210/300 [1:19:23<35:18, 23.54s/it]Running Inference:  70%|███████   | 211/300 [1:19:45<34:35, 23.32s/it]Running Inference:  71%|███████   | 212/300 [1:20:11<35:21, 24.11s/it]Running Inference:  71%|███████   | 213/300 [1:20:34<34:22, 23.71s/it]Running Inference:  71%|███████▏  | 214/300 [1:20:57<33:44, 23.54s/it]Running Inference:  72%|███████▏  | 215/300 [1:21:20<33:00, 23.30s/it]Running Inference:  72%|███████▏  | 216/300 [1:21:43<32:29, 23.21s/it]Running Inference:  72%|███████▏  | 217/300 [1:22:06<32:02, 23.16s/it]Running Inference:  73%|███████▎  | 218/300 [1:22:31<32:18, 23.64s/it]Running Inference:  73%|███████▎  | 219/300 [1:22:54<31:32, 23.36s/it]Running Inference:  73%|███████▎  | 220/300 [1:23:15<30:27, 22.85s/it]Running Inference:  74%|███████▎  | 221/300 [1:23:38<30:07, 22.88s/it]Running Inference:  74%|███████▍  | 222/300 [1:24:03<30:22, 23.37s/it]Running Inference:  74%|███████▍  | 223/300 [1:24:27<30:17, 23.60s/it]Running Inference:  75%|███████▍  | 224/300 [1:24:51<29:56, 23.64s/it]Running Inference:  75%|███████▌  | 225/300 [1:25:14<29:17, 23.43s/it]Running Inference:  75%|███████▌  | 226/300 [1:25:37<28:47, 23.35s/it]Running Inference:  76%|███████▌  | 227/300 [1:26:00<28:30, 23.44s/it]Running Inference:  76%|███████▌  | 228/300 [1:26:23<27:55, 23.27s/it]Running Inference:  76%|███████▋  | 229/300 [1:26:48<28:03, 23.71s/it]Running Inference:  77%|███████▋  | 230/300 [1:26:51<20:21, 17.45s/it]Running Inference:  77%|███████▋  | 231/300 [1:27:14<22:02, 19.16s/it]Running Inference:  77%|███████▋  | 232/300 [1:27:38<23:16, 20.54s/it]Running Inference:  78%|███████▊  | 233/300 [1:27:56<22:10, 19.85s/it]Running Inference:  78%|███████▊  | 234/300 [1:28:19<22:47, 20.72s/it]Running Inference:  78%|███████▊  | 235/300 [1:28:42<23:17, 21.50s/it]Running Inference:  79%|███████▊  | 236/300 [1:29:05<23:19, 21.87s/it]Running Inference:  79%|███████▉  | 237/300 [1:29:29<23:47, 22.66s/it]Running Inference:  79%|███████▉  | 238/300 [1:29:49<22:28, 21.75s/it]Running Inference:  80%|███████▉  | 239/300 [1:30:12<22:37, 22.25s/it]Running Inference:  80%|████████  | 240/300 [1:30:36<22:35, 22.59s/it]Running Inference:  80%|████████  | 241/300 [1:30:58<22:15, 22.64s/it]Running Inference:  81%|████████  | 242/300 [1:31:21<21:57, 22.71s/it]Running Inference:  81%|████████  | 243/300 [1:31:45<21:45, 22.90s/it]Running Inference:  81%|████████▏ | 244/300 [1:32:09<21:53, 23.45s/it]Running Inference:  82%|████████▏ | 245/300 [1:32:34<21:44, 23.71s/it]Running Inference:  82%|████████▏ | 246/300 [1:32:57<21:07, 23.47s/it]Running Inference:  82%|████████▏ | 247/300 [1:33:21<20:58, 23.74s/it]Running Inference:  83%|████████▎ | 248/300 [1:33:46<20:57, 24.18s/it]Running Inference:  83%|████████▎ | 249/300 [1:34:10<20:27, 24.07s/it]Running Inference:  83%|████████▎ | 250/300 [1:34:33<19:50, 23.82s/it]Running Inference:  84%|████████▎ | 251/300 [1:34:56<19:17, 23.62s/it]Running Inference:  84%|████████▍ | 252/300 [1:35:20<18:58, 23.71s/it]Running Inference:  84%|████████▍ | 253/300 [1:35:43<18:22, 23.45s/it]Running Inference:  85%|████████▍ | 254/300 [1:35:59<16:18, 21.28s/it]Running Inference:  85%|████████▌ | 255/300 [1:36:22<16:16, 21.70s/it]Running Inference:  85%|████████▌ | 256/300 [1:36:29<12:36, 17.19s/it]Running Inference:  86%|████████▌ | 257/300 [1:36:52<13:31, 18.88s/it]Running Inference:  86%|████████▌ | 258/300 [1:37:14<14:04, 20.10s/it]Running Inference:  86%|████████▋ | 259/300 [1:37:39<14:43, 21.55s/it]Running Inference:  87%|████████▋ | 260/300 [1:38:02<14:40, 22.01s/it]Running Inference:  87%|████████▋ | 261/300 [1:38:25<14:29, 22.30s/it]Running Inference:  87%|████████▋ | 262/300 [1:38:49<14:22, 22.69s/it]Running Inference:  88%|████████▊ | 263/300 [1:39:12<14:05, 22.85s/it]Running Inference:  88%|████████▊ | 264/300 [1:39:13<09:45, 16.27s/it]Running Inference:  88%|████████▊ | 265/300 [1:39:36<10:39, 18.27s/it]Running Inference:  89%|████████▊ | 266/300 [1:39:59<11:10, 19.73s/it]Running Inference:  89%|████████▉ | 267/300 [1:40:23<11:29, 20.88s/it]Running Inference:  89%|████████▉ | 268/300 [1:40:46<11:34, 21.71s/it]Running Inference:  90%|████████▉ | 269/300 [1:41:11<11:36, 22.47s/it]Running Inference:  90%|█████████ | 270/300 [1:41:35<11:27, 22.91s/it]Running Inference:  90%|█████████ | 271/300 [1:41:58<11:09, 23.10s/it]Running Inference:  91%|█████████ | 272/300 [1:42:23<11:05, 23.76s/it]Running Inference:  91%|█████████ | 273/300 [1:42:37<09:18, 20.70s/it]Running Inference:  91%|█████████▏| 274/300 [1:43:01<09:21, 21.61s/it]Running Inference:  92%|█████████▏| 275/300 [1:43:24<09:13, 22.14s/it]Running Inference:  92%|█████████▏| 276/300 [1:43:47<08:56, 22.37s/it]Running Inference:  92%|█████████▏| 277/300 [1:44:08<08:26, 22.01s/it]Running Inference:  93%|█████████▎| 278/300 [1:44:31<08:11, 22.35s/it]Running Inference:  93%|█████████▎| 279/300 [1:44:48<07:12, 20.57s/it]Running Inference:  93%|█████████▎| 280/300 [1:45:11<07:08, 21.41s/it]Running Inference:  94%|█████████▎| 281/300 [1:45:35<06:57, 22.00s/it]Running Inference:  94%|█████████▍| 282/300 [1:45:58<06:42, 22.36s/it]Running Inference:  94%|█████████▍| 283/300 [1:46:21<06:25, 22.65s/it]Running Inference:  95%|█████████▍| 284/300 [1:46:44<06:03, 22.70s/it]Running Inference:  95%|█████████▌| 285/300 [1:47:09<05:49, 23.32s/it]Running Inference:  95%|█████████▌| 286/300 [1:47:35<05:38, 24.21s/it]Running Inference:  96%|█████████▌| 287/300 [1:47:58<05:11, 23.95s/it]Running Inference:  96%|█████████▌| 288/300 [1:48:21<04:44, 23.68s/it]Running Inference:  96%|█████████▋| 289/300 [1:48:44<04:18, 23.48s/it]Running Inference:  97%|█████████▋| 290/300 [1:49:09<03:56, 23.70s/it]Running Inference:  97%|█████████▋| 291/300 [1:49:32<03:33, 23.73s/it]Running Inference:  97%|█████████▋| 292/300 [1:49:55<03:08, 23.50s/it]Running Inference:  98%|█████████▊| 293/300 [1:50:19<02:44, 23.43s/it]Running Inference:  98%|█████████▊| 294/300 [1:50:40<02:17, 22.88s/it]Running Inference:  98%|█████████▊| 295/300 [1:51:03<01:54, 22.91s/it]Running Inference:  99%|█████████▊| 296/300 [1:51:27<01:32, 23.22s/it]Running Inference:  99%|█████████▉| 297/300 [1:51:50<01:09, 23.07s/it]Running Inference:  99%|█████████▉| 298/300 [1:52:13<00:45, 22.98s/it]Running Inference: 100%|█████████▉| 299/300 [1:52:36<00:23, 23.07s/it]Running Inference: 100%|██████████| 300/300 [1:52:59<00:00, 23.01s/it]Running Inference: 100%|██████████| 300/300 [1:52:59<00:00, 22.60s/it]
2025-12-15 09:03:36,319 - INFO - Inference completed.
2025-12-15 09:03:36,366 - INFO - Results saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-15 09:03:36,366 - INFO - Calculating metrics for dataset: longbench
2025-12-15 09:04:03,497 - INFO - Metrics saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-15 09:04:03,497 - INFO - Metrics:
14.01
2025-12-15 09:04:03,499 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=gov_report_e, ratio=0.5) on GPU 1


========================================
LongBench Task: hotpotqa_e
========================================
----------------------------------------
Task: hotpotqa_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 09:04:10,652 - INFO - Set deterministic seeds to 42
2025-12-15 09:04:10,652 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 09:04:10,652 - INFO - Starting evaluation run...
2025-12-15 09:04:10,652 - INFO - Output directory set to: longbenchresult
2025-12-15 09:04:10,652 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-15 09:04:10,653 - INFO - KV Press 'tova' setup.
2025-12-15 09:04:10,653 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 09:04:10,653 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 100.58it/s]
Device set to use cuda:0
2025-12-15 09:04:21,700 - INFO - Model pipeline loaded.
2025-12-15 09:04:21,700 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa_e)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 300 examples [00:00, 1580.78 examples/s]Generating test split: 300 examples [00:00, 1572.12 examples/s]
2025-12-15 09:04:27,255 - INFO - Dataset loaded with 300 entries.
2025-12-15 09:04:27,255 - INFO - Dataset processed with 300 entries.
2025-12-15 09:04:27,294 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:05<25:08,  5.04s/it]Running Inference:   1%|          | 2/300 [00:07<16:23,  3.30s/it]Running Inference:   1%|          | 3/300 [00:07<10:26,  2.11s/it]Running Inference:   1%|▏         | 4/300 [00:08<07:13,  1.46s/it]Running Inference:   2%|▏         | 5/300 [00:09<07:04,  1.44s/it]Running Inference:   2%|▏         | 6/300 [00:10<05:50,  1.19s/it]Running Inference:   2%|▏         | 7/300 [00:11<05:42,  1.17s/it]Running Inference:   3%|▎         | 8/300 [00:12<04:53,  1.01s/it]Running Inference:   3%|▎         | 9/300 [00:14<06:53,  1.42s/it]Running Inference:   3%|▎         | 10/300 [00:16<07:07,  1.47s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:18<08:43,  1.81s/it]Running Inference:   4%|▍         | 12/300 [00:20<09:14,  1.93s/it]Running Inference:   4%|▍         | 13/300 [00:21<07:25,  1.55s/it]Running Inference:   5%|▍         | 14/300 [00:22<06:06,  1.28s/it]Running Inference:   5%|▌         | 15/300 [00:24<07:54,  1.67s/it]Running Inference:   5%|▌         | 16/300 [00:26<07:26,  1.57s/it]Running Inference:   6%|▌         | 17/300 [00:28<08:41,  1.84s/it]Running Inference:   6%|▌         | 18/300 [00:29<06:52,  1.46s/it]Running Inference:   6%|▋         | 19/300 [00:30<06:40,  1.42s/it]Running Inference:   7%|▋         | 20/300 [00:31<05:50,  1.25s/it]Running Inference:   7%|▋         | 21/300 [00:34<08:47,  1.89s/it]Running Inference:   7%|▋         | 22/300 [00:37<09:57,  2.15s/it]Running Inference:   8%|▊         | 23/300 [00:40<10:57,  2.37s/it]Running Inference:   8%|▊         | 24/300 [00:44<12:52,  2.80s/it]Running Inference:   8%|▊         | 25/300 [00:45<10:32,  2.30s/it]Running Inference:   9%|▊         | 26/300 [00:48<12:02,  2.64s/it]Running Inference:   9%|▉         | 27/300 [00:50<10:26,  2.29s/it]Running Inference:   9%|▉         | 28/300 [00:53<11:51,  2.61s/it]Running Inference:  10%|▉         | 29/300 [00:54<09:12,  2.04s/it]Running Inference:  10%|█         | 30/300 [00:55<08:28,  1.88s/it]Running Inference:  10%|█         | 31/300 [00:56<07:09,  1.60s/it]Running Inference:  11%|█         | 32/300 [00:59<08:44,  1.96s/it]Running Inference:  11%|█         | 33/300 [01:00<07:50,  1.76s/it]Running Inference:  11%|█▏        | 34/300 [01:03<08:37,  1.94s/it]Running Inference:  12%|█▏        | 35/300 [01:05<09:09,  2.07s/it]Running Inference:  12%|█▏        | 36/300 [01:06<07:32,  1.71s/it]Running Inference:  12%|█▏        | 37/300 [01:09<08:35,  1.96s/it]Running Inference:  13%|█▎        | 38/300 [01:11<09:25,  2.16s/it]Running Inference:  13%|█▎        | 39/300 [01:12<08:07,  1.87s/it]Running Inference:  13%|█▎        | 40/300 [01:14<07:21,  1.70s/it]Running Inference:  14%|█▎        | 41/300 [01:14<05:54,  1.37s/it]Running Inference:  14%|█▍        | 42/300 [01:16<07:03,  1.64s/it]Running Inference:  14%|█▍        | 43/300 [01:21<10:19,  2.41s/it]Running Inference:  15%|█▍        | 44/300 [01:21<08:12,  1.92s/it]Running Inference:  15%|█▌        | 45/300 [01:23<07:21,  1.73s/it]Running Inference:  15%|█▌        | 46/300 [01:24<06:40,  1.58s/it]Running Inference:  16%|█▌        | 47/300 [01:25<05:41,  1.35s/it]Running Inference:  16%|█▌        | 48/300 [01:27<06:56,  1.65s/it]Running Inference:  16%|█▋        | 49/300 [01:29<06:54,  1.65s/it]Running Inference:  17%|█▋        | 50/300 [01:30<06:52,  1.65s/it]Running Inference:  17%|█▋        | 51/300 [01:35<10:08,  2.44s/it]Running Inference:  17%|█▋        | 52/300 [01:35<07:46,  1.88s/it]Running Inference:  18%|█▊        | 53/300 [01:36<06:36,  1.61s/it]Running Inference:  18%|█▊        | 54/300 [01:38<06:16,  1.53s/it]Running Inference:  18%|█▊        | 55/300 [01:39<05:44,  1.41s/it]Running Inference:  19%|█▊        | 56/300 [01:42<07:29,  1.84s/it]Running Inference:  19%|█▉        | 57/300 [01:43<07:15,  1.79s/it]Running Inference:  19%|█▉        | 58/300 [01:45<06:53,  1.71s/it]Running Inference:  20%|█▉        | 59/300 [01:49<09:24,  2.34s/it]Running Inference:  20%|██        | 60/300 [01:52<10:44,  2.69s/it]Running Inference:  20%|██        | 61/300 [01:54<10:05,  2.53s/it]Running Inference:  21%|██        | 62/300 [01:55<07:45,  1.96s/it]Running Inference:  21%|██        | 63/300 [01:57<07:35,  1.92s/it]Running Inference:  21%|██▏       | 64/300 [01:58<06:42,  1.70s/it]Running Inference:  22%|██▏       | 65/300 [01:59<05:41,  1.45s/it]Running Inference:  22%|██▏       | 66/300 [01:59<04:36,  1.18s/it]Running Inference:  22%|██▏       | 67/300 [02:03<07:05,  1.83s/it]Running Inference:  23%|██▎       | 68/300 [02:04<06:22,  1.65s/it]Running Inference:  23%|██▎       | 69/300 [02:06<07:21,  1.91s/it]Running Inference:  23%|██▎       | 70/300 [02:10<08:44,  2.28s/it]Running Inference:  24%|██▎       | 71/300 [02:11<07:17,  1.91s/it]Running Inference:  24%|██▍       | 72/300 [02:12<06:37,  1.74s/it]Running Inference:  24%|██▍       | 73/300 [02:13<05:22,  1.42s/it]Running Inference:  25%|██▍       | 74/300 [02:15<06:07,  1.63s/it]Running Inference:  25%|██▌       | 75/300 [02:16<05:52,  1.57s/it]Running Inference:  25%|██▌       | 76/300 [02:18<05:51,  1.57s/it]Running Inference:  26%|██▌       | 77/300 [02:20<06:24,  1.72s/it]Running Inference:  26%|██▌       | 78/300 [02:22<06:35,  1.78s/it]Running Inference:  26%|██▋       | 79/300 [02:23<05:26,  1.48s/it]Running Inference:  27%|██▋       | 80/300 [02:26<07:23,  2.02s/it]Running Inference:  27%|██▋       | 81/300 [02:28<07:53,  2.16s/it]Running Inference:  27%|██▋       | 82/300 [02:30<07:18,  2.01s/it]Running Inference:  28%|██▊       | 83/300 [02:31<06:17,  1.74s/it]Running Inference:  28%|██▊       | 84/300 [02:32<05:46,  1.60s/it]Running Inference:  28%|██▊       | 85/300 [02:34<06:18,  1.76s/it]Running Inference:  29%|██▊       | 86/300 [02:35<04:48,  1.35s/it]Running Inference:  29%|██▉       | 87/300 [02:36<04:38,  1.31s/it]Running Inference:  29%|██▉       | 88/300 [02:39<06:35,  1.86s/it]Running Inference:  30%|██▉       | 89/300 [02:41<06:48,  1.94s/it]Running Inference:  30%|███       | 90/300 [02:44<07:24,  2.12s/it]Running Inference:  30%|███       | 91/300 [02:47<08:41,  2.50s/it]Running Inference:  31%|███       | 92/300 [02:49<07:43,  2.23s/it]Running Inference:  31%|███       | 93/300 [02:51<07:23,  2.14s/it]Running Inference:  31%|███▏      | 94/300 [02:52<06:06,  1.78s/it]Running Inference:  32%|███▏      | 95/300 [02:53<05:03,  1.48s/it]Running Inference:  32%|███▏      | 96/300 [02:55<06:24,  1.88s/it]Running Inference:  32%|███▏      | 97/300 [02:56<05:05,  1.50s/it]Running Inference:  33%|███▎      | 98/300 [02:59<06:09,  1.83s/it]Running Inference:  33%|███▎      | 99/300 [02:59<04:46,  1.43s/it]Running Inference:  33%|███▎      | 100/300 [03:00<04:21,  1.31s/it]Running Inference:  34%|███▎      | 101/300 [03:01<04:04,  1.23s/it]Running Inference:  34%|███▍      | 102/300 [03:05<06:50,  2.07s/it]Running Inference:  34%|███▍      | 103/300 [03:07<06:48,  2.07s/it]Running Inference:  35%|███▍      | 104/300 [03:10<07:00,  2.15s/it]Running Inference:  35%|███▌      | 105/300 [03:11<06:18,  1.94s/it]Running Inference:  35%|███▌      | 106/300 [03:13<05:56,  1.84s/it]Running Inference:  36%|███▌      | 107/300 [03:15<05:57,  1.85s/it]Running Inference:  36%|███▌      | 108/300 [03:16<05:24,  1.69s/it]Running Inference:  36%|███▋      | 109/300 [03:19<06:36,  2.08s/it]Running Inference:  37%|███▋      | 110/300 [03:22<07:18,  2.31s/it]Running Inference:  37%|███▋      | 111/300 [03:24<07:12,  2.29s/it]Running Inference:  37%|███▋      | 112/300 [03:26<07:07,  2.27s/it]Running Inference:  38%|███▊      | 113/300 [03:27<05:42,  1.83s/it]Running Inference:  38%|███▊      | 114/300 [03:29<05:40,  1.83s/it]Running Inference:  38%|███▊      | 115/300 [03:30<05:06,  1.66s/it]Running Inference:  39%|███▊      | 116/300 [03:31<04:43,  1.54s/it]Running Inference:  39%|███▉      | 117/300 [03:32<03:59,  1.31s/it]Running Inference:  39%|███▉      | 118/300 [03:35<05:10,  1.71s/it]Running Inference:  40%|███▉      | 119/300 [03:37<05:36,  1.86s/it]Running Inference:  40%|████      | 120/300 [03:38<04:30,  1.50s/it]Running Inference:  40%|████      | 121/300 [03:41<06:30,  2.18s/it]Running Inference:  41%|████      | 122/300 [03:45<07:49,  2.63s/it]Running Inference:  41%|████      | 123/300 [03:46<06:42,  2.28s/it]Running Inference:  41%|████▏     | 124/300 [03:47<05:28,  1.87s/it]Running Inference:  42%|████▏     | 125/300 [03:49<05:17,  1.81s/it]Running Inference:  42%|████▏     | 126/300 [03:50<04:28,  1.54s/it]Running Inference:  42%|████▏     | 127/300 [03:51<04:10,  1.45s/it]Running Inference:  43%|████▎     | 128/300 [03:51<03:08,  1.10s/it]Running Inference:  43%|████▎     | 129/300 [03:53<03:32,  1.24s/it]Running Inference:  43%|████▎     | 130/300 [03:54<02:59,  1.06s/it]Running Inference:  44%|████▎     | 131/300 [03:54<02:42,  1.04it/s]Running Inference:  44%|████▍     | 132/300 [03:56<02:51,  1.02s/it]Running Inference:  44%|████▍     | 133/300 [03:56<02:38,  1.05it/s]Running Inference:  45%|████▍     | 134/300 [04:00<04:58,  1.80s/it]Running Inference:  45%|████▌     | 135/300 [04:02<05:04,  1.85s/it]Running Inference:  45%|████▌     | 136/300 [04:03<04:18,  1.57s/it]Running Inference:  46%|████▌     | 137/300 [04:04<03:59,  1.47s/it]Running Inference:  46%|████▌     | 138/300 [04:07<04:44,  1.76s/it]Running Inference:  46%|████▋     | 139/300 [04:11<06:25,  2.39s/it]Running Inference:  47%|████▋     | 140/300 [04:13<06:10,  2.32s/it]Running Inference:  47%|████▋     | 141/300 [04:13<04:47,  1.81s/it]Running Inference:  47%|████▋     | 142/300 [04:14<03:55,  1.49s/it]Running Inference:  48%|████▊     | 143/300 [04:16<04:29,  1.72s/it]Running Inference:  48%|████▊     | 144/300 [04:19<05:26,  2.09s/it]Running Inference:  48%|████▊     | 145/300 [04:21<05:09,  2.00s/it]Running Inference:  49%|████▊     | 146/300 [04:22<04:31,  1.76s/it]Running Inference:  49%|████▉     | 147/300 [04:26<05:58,  2.35s/it]Running Inference:  49%|████▉     | 148/300 [04:27<04:46,  1.88s/it]Running Inference:  50%|████▉     | 149/300 [04:27<03:49,  1.52s/it]Running Inference:  50%|█████     | 150/300 [04:28<03:19,  1.33s/it]Running Inference:  50%|█████     | 151/300 [04:31<04:19,  1.74s/it]Running Inference:  51%|█████     | 152/300 [04:35<05:35,  2.27s/it]Running Inference:  51%|█████     | 153/300 [04:39<07:00,  2.86s/it]Running Inference:  51%|█████▏    | 154/300 [04:42<06:57,  2.86s/it]Running Inference:  52%|█████▏    | 155/300 [04:44<06:28,  2.68s/it]Running Inference:  52%|█████▏    | 156/300 [04:45<05:36,  2.34s/it]Running Inference:  52%|█████▏    | 157/300 [04:46<04:26,  1.87s/it]Running Inference:  53%|█████▎    | 158/300 [04:48<04:31,  1.91s/it]Running Inference:  53%|█████▎    | 159/300 [04:49<03:36,  1.54s/it]Running Inference:  53%|█████▎    | 160/300 [04:50<03:02,  1.31s/it]Running Inference:  54%|█████▎    | 161/300 [04:53<04:32,  1.96s/it]Running Inference:  54%|█████▍    | 162/300 [04:54<03:30,  1.52s/it]Running Inference:  54%|█████▍    | 163/300 [04:55<03:09,  1.39s/it]Running Inference:  55%|█████▍    | 164/300 [04:55<02:35,  1.14s/it]Running Inference:  55%|█████▌    | 165/300 [04:56<02:08,  1.05it/s]Running Inference:  55%|█████▌    | 166/300 [04:57<02:09,  1.03it/s]Running Inference:  56%|█████▌    | 167/300 [04:59<02:41,  1.22s/it]Running Inference:  56%|█████▌    | 168/300 [05:01<03:12,  1.46s/it]Running Inference:  56%|█████▋    | 169/300 [05:03<03:53,  1.78s/it]Running Inference:  57%|█████▋    | 170/300 [05:05<03:34,  1.65s/it]Running Inference:  57%|█████▋    | 171/300 [05:06<03:11,  1.49s/it]Running Inference:  57%|█████▋    | 172/300 [05:07<02:58,  1.39s/it]Running Inference:  58%|█████▊    | 173/300 [05:11<04:35,  2.17s/it]Running Inference:  58%|█████▊    | 174/300 [05:14<05:03,  2.40s/it]Running Inference:  58%|█████▊    | 175/300 [05:15<04:28,  2.15s/it]Running Inference:  59%|█████▊    | 176/300 [05:16<03:44,  1.81s/it]Running Inference:  59%|█████▉    | 177/300 [05:17<03:13,  1.57s/it]Running Inference:  59%|█████▉    | 178/300 [05:19<03:29,  1.72s/it]Running Inference:  60%|█████▉    | 179/300 [05:22<04:08,  2.05s/it]Running Inference:  60%|██████    | 180/300 [05:24<03:39,  1.83s/it]Running Inference:  60%|██████    | 181/300 [05:26<04:02,  2.04s/it]Running Inference:  61%|██████    | 182/300 [05:27<03:38,  1.85s/it]Running Inference:  61%|██████    | 183/300 [05:32<05:00,  2.57s/it]Running Inference:  61%|██████▏   | 184/300 [05:35<05:20,  2.76s/it]Running Inference:  62%|██████▏   | 185/300 [05:37<05:00,  2.61s/it]Running Inference:  62%|██████▏   | 186/300 [05:39<04:44,  2.50s/it]Running Inference:  62%|██████▏   | 187/300 [05:41<04:26,  2.36s/it]Running Inference:  63%|██████▎   | 188/300 [05:43<04:01,  2.16s/it]Running Inference:  63%|██████▎   | 189/300 [05:45<04:04,  2.20s/it]Running Inference:  63%|██████▎   | 190/300 [05:49<04:37,  2.52s/it]Running Inference:  64%|██████▎   | 191/300 [05:53<05:32,  3.05s/it]Running Inference:  64%|██████▍   | 192/300 [05:55<04:58,  2.77s/it]Running Inference:  64%|██████▍   | 193/300 [05:57<04:37,  2.60s/it]Running Inference:  65%|██████▍   | 194/300 [06:00<04:51,  2.75s/it]Running Inference:  65%|██████▌   | 195/300 [06:01<03:44,  2.14s/it]Running Inference:  65%|██████▌   | 196/300 [06:02<03:01,  1.74s/it]Running Inference:  66%|██████▌   | 197/300 [06:04<03:20,  1.94s/it]Running Inference:  66%|██████▌   | 198/300 [06:08<03:57,  2.33s/it]Running Inference:  66%|██████▋   | 199/300 [06:11<04:30,  2.68s/it]Running Inference:  67%|██████▋   | 200/300 [06:12<03:44,  2.24s/it]Running Inference:  67%|██████▋   | 201/300 [06:15<04:01,  2.44s/it]Running Inference:  67%|██████▋   | 202/300 [06:16<03:11,  1.96s/it]Running Inference:  68%|██████▊   | 203/300 [06:18<03:19,  2.05s/it]Running Inference:  68%|██████▊   | 204/300 [06:21<03:29,  2.19s/it]Running Inference:  68%|██████▊   | 205/300 [06:23<03:36,  2.28s/it]Running Inference:  69%|██████▊   | 206/300 [06:27<04:06,  2.63s/it]Running Inference:  69%|██████▉   | 207/300 [06:28<03:15,  2.11s/it]Running Inference:  69%|██████▉   | 208/300 [06:30<03:14,  2.12s/it]Running Inference:  70%|██████▉   | 209/300 [06:32<03:24,  2.25s/it]Running Inference:  70%|███████   | 210/300 [06:33<02:40,  1.78s/it]Running Inference:  70%|███████   | 211/300 [06:36<03:03,  2.06s/it]Running Inference:  71%|███████   | 212/300 [06:36<02:24,  1.64s/it]Running Inference:  71%|███████   | 213/300 [06:38<02:11,  1.52s/it]Running Inference:  71%|███████▏  | 214/300 [06:39<01:55,  1.34s/it]Running Inference:  72%|███████▏  | 215/300 [06:40<02:08,  1.51s/it]Running Inference:  72%|███████▏  | 216/300 [06:41<01:50,  1.32s/it]Running Inference:  72%|███████▏  | 217/300 [06:42<01:42,  1.23s/it]Running Inference:  73%|███████▎  | 218/300 [06:44<01:41,  1.24s/it]Running Inference:  73%|███████▎  | 219/300 [06:45<01:33,  1.16s/it]Running Inference:  73%|███████▎  | 220/300 [06:47<01:53,  1.42s/it]Running Inference:  74%|███████▎  | 221/300 [06:47<01:30,  1.15s/it]Running Inference:  74%|███████▍  | 222/300 [06:49<01:57,  1.50s/it]Running Inference:  74%|███████▍  | 223/300 [06:50<01:39,  1.30s/it]Running Inference:  75%|███████▍  | 224/300 [06:52<01:47,  1.41s/it]Running Inference:  75%|███████▌  | 225/300 [06:54<01:56,  1.55s/it]Running Inference:  75%|███████▌  | 226/300 [06:56<02:04,  1.69s/it]Running Inference:  76%|███████▌  | 227/300 [06:57<01:57,  1.61s/it]Running Inference:  76%|███████▌  | 228/300 [06:59<01:52,  1.56s/it]Running Inference:  76%|███████▋  | 229/300 [06:59<01:27,  1.23s/it]Running Inference:  77%|███████▋  | 230/300 [07:01<01:37,  1.39s/it]Running Inference:  77%|███████▋  | 231/300 [07:03<01:47,  1.56s/it]Running Inference:  77%|███████▋  | 232/300 [07:06<02:23,  2.12s/it]Running Inference:  78%|███████▊  | 233/300 [07:08<02:18,  2.06s/it]Running Inference:  78%|███████▊  | 234/300 [07:10<02:07,  1.93s/it]Running Inference:  78%|███████▊  | 235/300 [07:11<01:50,  1.69s/it]Running Inference:  79%|███████▊  | 236/300 [07:14<02:19,  2.18s/it]Running Inference:  79%|███████▉  | 237/300 [07:15<01:55,  1.84s/it]Running Inference:  79%|███████▉  | 238/300 [07:19<02:34,  2.50s/it]Running Inference:  80%|███████▉  | 239/300 [07:21<02:18,  2.26s/it]Running Inference:  80%|████████  | 240/300 [07:22<01:53,  1.89s/it]Running Inference:  80%|████████  | 241/300 [07:24<01:55,  1.95s/it]Running Inference:  81%|████████  | 242/300 [07:26<01:47,  1.85s/it]Running Inference:  81%|████████  | 243/300 [07:28<01:46,  1.86s/it]Running Inference:  81%|████████▏ | 244/300 [07:29<01:34,  1.69s/it]Running Inference:  82%|████████▏ | 245/300 [07:32<02:02,  2.23s/it]Running Inference:  82%|████████▏ | 246/300 [07:33<01:35,  1.76s/it]Running Inference:  82%|████████▏ | 247/300 [07:35<01:27,  1.66s/it]Running Inference:  83%|████████▎ | 248/300 [07:35<01:13,  1.42s/it]Running Inference:  83%|████████▎ | 249/300 [07:36<01:03,  1.25s/it]Running Inference:  83%|████████▎ | 250/300 [07:38<01:06,  1.34s/it]Running Inference:  84%|████████▎ | 251/300 [07:40<01:15,  1.54s/it]Running Inference:  84%|████████▍ | 252/300 [07:42<01:22,  1.72s/it]Running Inference:  84%|████████▍ | 253/300 [07:44<01:23,  1.79s/it]Running Inference:  85%|████████▍ | 254/300 [07:46<01:24,  1.85s/it]Running Inference:  85%|████████▌ | 255/300 [07:49<01:45,  2.34s/it]Running Inference:  85%|████████▌ | 256/300 [07:54<02:09,  2.94s/it]Running Inference:  86%|████████▌ | 257/300 [07:54<01:37,  2.27s/it]Running Inference:  86%|████████▌ | 258/300 [07:57<01:34,  2.25s/it]Running Inference:  86%|████████▋ | 259/300 [07:57<01:09,  1.70s/it]Running Inference:  87%|████████▋ | 260/300 [07:58<01:01,  1.55s/it]Running Inference:  87%|████████▋ | 261/300 [08:00<01:07,  1.73s/it]Running Inference:  87%|████████▋ | 262/300 [08:02<00:59,  1.58s/it]Running Inference:  88%|████████▊ | 263/300 [08:04<01:04,  1.75s/it]Running Inference:  88%|████████▊ | 264/300 [08:06<01:04,  1.78s/it]Running Inference:  88%|████████▊ | 265/300 [08:08<01:07,  1.92s/it]Running Inference:  89%|████████▊ | 266/300 [08:11<01:14,  2.21s/it]Running Inference:  89%|████████▉ | 267/300 [08:12<01:00,  1.84s/it]Running Inference:  89%|████████▉ | 268/300 [08:13<00:49,  1.56s/it]Running Inference:  90%|████████▉ | 269/300 [08:13<00:40,  1.29s/it]Running Inference:  90%|█████████ | 270/300 [08:14<00:35,  1.20s/it]Running Inference:  90%|█████████ | 271/300 [08:15<00:31,  1.08s/it]Running Inference:  91%|█████████ | 272/300 [08:16<00:27,  1.01it/s]Running Inference:  91%|█████████ | 273/300 [08:20<00:52,  1.93s/it]Running Inference:  91%|█████████▏| 274/300 [08:23<01:01,  2.37s/it]Running Inference:  92%|█████████▏| 275/300 [08:25<00:52,  2.09s/it]Running Inference:  92%|█████████▏| 276/300 [08:28<00:57,  2.41s/it]Running Inference:  92%|█████████▏| 277/300 [08:30<00:55,  2.42s/it]Running Inference:  93%|█████████▎| 278/300 [08:31<00:41,  1.88s/it]Running Inference:  93%|█████████▎| 279/300 [08:35<00:50,  2.41s/it]Running Inference:  93%|█████████▎| 280/300 [08:36<00:43,  2.15s/it]Running Inference:  94%|█████████▎| 281/300 [08:38<00:40,  2.11s/it]Running Inference:  94%|█████████▍| 282/300 [08:43<00:50,  2.78s/it]Running Inference:  94%|█████████▍| 283/300 [08:47<00:55,  3.24s/it]Running Inference:  95%|█████████▍| 284/300 [08:49<00:47,  2.94s/it]Running Inference:  95%|█████████▌| 285/300 [08:50<00:34,  2.29s/it]Running Inference:  95%|█████████▌| 286/300 [08:51<00:25,  1.82s/it]Running Inference:  96%|█████████▌| 287/300 [08:52<00:21,  1.67s/it]Running Inference:  96%|█████████▌| 288/300 [08:54<00:20,  1.69s/it]Running Inference:  96%|█████████▋| 289/300 [08:54<00:13,  1.26s/it]Running Inference:  97%|█████████▋| 290/300 [08:56<00:13,  1.36s/it]Running Inference:  97%|█████████▋| 291/300 [08:57<00:12,  1.41s/it]Running Inference:  97%|█████████▋| 292/300 [08:58<00:10,  1.27s/it]Running Inference:  98%|█████████▊| 293/300 [09:00<00:09,  1.36s/it]Running Inference:  98%|█████████▊| 294/300 [09:01<00:08,  1.39s/it]Running Inference:  98%|█████████▊| 295/300 [09:03<00:08,  1.67s/it]Running Inference:  99%|█████████▊| 296/300 [09:06<00:07,  1.84s/it]Running Inference:  99%|█████████▉| 297/300 [09:07<00:05,  1.83s/it]Running Inference:  99%|█████████▉| 298/300 [09:10<00:03,  1.99s/it]Running Inference: 100%|█████████▉| 299/300 [09:10<00:01,  1.54s/it]Running Inference: 100%|██████████| 300/300 [09:14<00:00,  2.16s/it]Running Inference: 100%|██████████| 300/300 [09:14<00:00,  1.85s/it]
2025-12-15 09:13:41,682 - INFO - Inference completed.
2025-12-15 09:13:41,692 - INFO - Results saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-15 09:13:41,692 - INFO - Calculating metrics for dataset: longbench
2025-12-15 09:13:41,698 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-15 09:13:41,698 - INFO - Metrics:
41.49
2025-12-15 09:13:41,699 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa_e, ratio=0.1) on GPU 1

----------------------------------------
Task: hotpotqa_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 09:13:48,096 - INFO - Set deterministic seeds to 42
2025-12-15 09:13:48,097 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 09:13:48,097 - INFO - Starting evaluation run...
2025-12-15 09:13:48,097 - INFO - Output directory set to: longbenchresult
2025-12-15 09:13:48,097 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-15 09:13:48,097 - INFO - KV Press 'tova' setup.
2025-12-15 09:13:48,097 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 09:13:48,097 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.06it/s]
Device set to use cuda:0
2025-12-15 09:13:59,450 - INFO - Model pipeline loaded.
2025-12-15 09:13:59,450 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa_e)
2025-12-15 09:14:02,937 - INFO - Dataset loaded with 300 entries.
2025-12-15 09:14:02,937 - INFO - Dataset processed with 300 entries.
2025-12-15 09:14:02,973 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:04<24:44,  4.97s/it]Running Inference:   1%|          | 2/300 [00:07<16:15,  3.27s/it]Running Inference:   1%|          | 3/300 [00:07<10:21,  2.09s/it]Running Inference:   1%|▏         | 4/300 [00:08<07:09,  1.45s/it]Running Inference:   2%|▏         | 5/300 [00:09<07:01,  1.43s/it]Running Inference:   2%|▏         | 6/300 [00:10<05:48,  1.18s/it]Running Inference:   2%|▏         | 7/300 [00:11<05:39,  1.16s/it]Running Inference:   3%|▎         | 8/300 [00:12<04:51,  1.00it/s]Running Inference:   3%|▎         | 9/300 [00:14<06:51,  1.41s/it]Running Inference:   3%|▎         | 10/300 [00:15<07:05,  1.47s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:18<08:41,  1.81s/it]Running Inference:   4%|▍         | 12/300 [00:20<09:13,  1.92s/it]Running Inference:   4%|▍         | 13/300 [00:21<07:24,  1.55s/it]Running Inference:   5%|▍         | 14/300 [00:22<06:05,  1.28s/it]Running Inference:   5%|▌         | 15/300 [00:24<07:53,  1.66s/it]Running Inference:   5%|▌         | 16/300 [00:25<07:24,  1.57s/it]Running Inference:   6%|▌         | 17/300 [00:28<08:38,  1.83s/it]Running Inference:   6%|▌         | 18/300 [00:29<06:49,  1.45s/it]Running Inference:   6%|▋         | 19/300 [00:30<06:37,  1.42s/it]Running Inference:   7%|▋         | 20/300 [00:31<05:47,  1.24s/it]Running Inference:   7%|▋         | 21/300 [00:34<08:40,  1.86s/it]Running Inference:   7%|▋         | 22/300 [00:37<09:47,  2.11s/it]Running Inference:   8%|▊         | 23/300 [00:40<10:44,  2.33s/it]Running Inference:   8%|▊         | 24/300 [00:43<12:38,  2.75s/it]Running Inference:   8%|▊         | 25/300 [00:44<10:22,  2.26s/it]Running Inference:   9%|▊         | 26/300 [00:48<11:49,  2.59s/it]Running Inference:   9%|▉         | 27/300 [00:49<10:16,  2.26s/it]Running Inference:   9%|▉         | 28/300 [00:53<11:40,  2.58s/it]Running Inference:  10%|▉         | 29/300 [00:53<09:05,  2.01s/it]Running Inference:  10%|█         | 30/300 [00:55<08:22,  1.86s/it]Running Inference:  10%|█         | 31/300 [00:56<07:04,  1.58s/it]Running Inference:  11%|█         | 32/300 [00:58<08:35,  1.93s/it]Running Inference:  11%|█         | 33/300 [01:00<07:42,  1.73s/it]Running Inference:  11%|█▏        | 34/300 [01:02<08:30,  1.92s/it]Running Inference:  12%|█▏        | 35/300 [01:04<09:04,  2.05s/it]Running Inference:  12%|█▏        | 36/300 [01:05<07:27,  1.69s/it]Running Inference:  12%|█▏        | 37/300 [01:08<08:27,  1.93s/it]Running Inference:  13%|█▎        | 38/300 [01:08<06:34,  1.51s/it]Running Inference:  13%|█▎        | 39/300 [01:09<06:08,  1.41s/it]Running Inference:  13%|█▎        | 40/300 [01:11<05:58,  1.38s/it]Running Inference:  14%|█▎        | 41/300 [01:11<04:56,  1.14s/it]Running Inference:  14%|█▍        | 42/300 [01:14<06:22,  1.48s/it]Running Inference:  14%|█▍        | 43/300 [01:18<09:46,  2.28s/it]Running Inference:  15%|█▍        | 44/300 [01:19<07:49,  1.83s/it]Running Inference:  15%|█▌        | 45/300 [01:20<07:04,  1.67s/it]Running Inference:  15%|█▌        | 46/300 [01:21<06:29,  1.53s/it]Running Inference:  16%|█▌        | 47/300 [01:22<05:33,  1.32s/it]Running Inference:  16%|█▌        | 48/300 [01:24<06:50,  1.63s/it]Running Inference:  16%|█▋        | 49/300 [01:26<06:48,  1.63s/it]Running Inference:  17%|█▋        | 50/300 [01:27<06:47,  1.63s/it]Running Inference:  17%|█▋        | 51/300 [01:32<10:00,  2.41s/it]Running Inference:  17%|█▋        | 52/300 [01:32<07:40,  1.86s/it]Running Inference:  18%|█▊        | 53/300 [01:33<06:31,  1.59s/it]Running Inference:  18%|█▊        | 54/300 [01:35<06:12,  1.51s/it]Running Inference:  18%|█▊        | 55/300 [01:36<05:40,  1.39s/it]Running Inference:  19%|█▊        | 56/300 [01:38<07:23,  1.82s/it]Running Inference:  19%|█▉        | 57/300 [01:40<07:25,  1.83s/it]Running Inference:  19%|█▉        | 58/300 [01:42<06:59,  1.73s/it]Running Inference:  20%|█▉        | 59/300 [01:46<09:23,  2.34s/it]Running Inference:  20%|██        | 60/300 [01:49<10:39,  2.66s/it]Running Inference:  20%|██        | 61/300 [01:51<10:02,  2.52s/it]Running Inference:  21%|██        | 62/300 [01:52<07:45,  1.96s/it]Running Inference:  21%|██        | 63/300 [01:54<07:35,  1.92s/it]Running Inference:  21%|██▏       | 64/300 [01:55<06:41,  1.70s/it]Running Inference:  22%|██▏       | 65/300 [01:56<05:40,  1.45s/it]Running Inference:  22%|██▏       | 66/300 [01:56<04:35,  1.18s/it]Running Inference:  22%|██▏       | 67/300 [02:00<07:02,  1.81s/it]Running Inference:  23%|██▎       | 68/300 [02:01<06:13,  1.61s/it]Running Inference:  23%|██▎       | 69/300 [02:03<07:13,  1.88s/it]Running Inference:  23%|██▎       | 70/300 [02:07<08:59,  2.34s/it]Running Inference:  24%|██▎       | 71/300 [02:08<07:27,  1.95s/it]Running Inference:  24%|██▍       | 72/300 [02:09<06:43,  1.77s/it]Running Inference:  24%|██▍       | 73/300 [02:10<05:26,  1.44s/it]Running Inference:  25%|██▍       | 74/300 [02:12<06:09,  1.64s/it]Running Inference:  25%|██▌       | 75/300 [02:13<05:53,  1.57s/it]Running Inference:  25%|██▌       | 76/300 [02:15<05:51,  1.57s/it]Running Inference:  26%|██▌       | 77/300 [02:17<06:23,  1.72s/it]Running Inference:  26%|██▌       | 78/300 [02:19<06:35,  1.78s/it]Running Inference:  26%|██▋       | 79/300 [02:20<05:25,  1.47s/it]Running Inference:  27%|██▋       | 80/300 [02:23<07:18,  1.99s/it]Running Inference:  27%|██▋       | 81/300 [02:25<07:50,  2.15s/it]Running Inference:  27%|██▋       | 82/300 [02:27<07:16,  2.00s/it]Running Inference:  28%|██▊       | 83/300 [02:28<06:15,  1.73s/it]Running Inference:  28%|██▊       | 84/300 [02:29<05:43,  1.59s/it]Running Inference:  28%|██▊       | 85/300 [02:31<06:15,  1.75s/it]Running Inference:  29%|██▊       | 86/300 [02:32<04:45,  1.34s/it]Running Inference:  29%|██▉       | 87/300 [02:33<04:36,  1.30s/it]Running Inference:  29%|██▉       | 88/300 [02:36<06:29,  1.84s/it]Running Inference:  30%|██▉       | 89/300 [02:38<06:44,  1.92s/it]Running Inference:  30%|███       | 90/300 [02:41<07:17,  2.08s/it]Running Inference:  30%|███       | 91/300 [02:44<08:32,  2.45s/it]Running Inference:  31%|███       | 92/300 [02:46<07:35,  2.19s/it]Running Inference:  31%|███       | 93/300 [02:47<07:17,  2.11s/it]Running Inference:  31%|███▏      | 94/300 [02:48<06:02,  1.76s/it]Running Inference:  32%|███▏      | 95/300 [02:49<05:00,  1.46s/it]Running Inference:  32%|███▏      | 96/300 [02:52<06:16,  1.85s/it]Running Inference:  32%|███▏      | 97/300 [02:53<04:59,  1.48s/it]Running Inference:  33%|███▎      | 98/300 [02:55<06:02,  1.79s/it]Running Inference:  33%|███▎      | 99/300 [02:56<04:41,  1.40s/it]Running Inference:  33%|███▎      | 100/300 [02:57<04:16,  1.28s/it]Running Inference:  34%|███▎      | 101/300 [02:58<04:00,  1.21s/it]Running Inference:  34%|███▍      | 102/300 [03:02<06:43,  2.04s/it]Running Inference:  34%|███▍      | 103/300 [03:04<06:43,  2.05s/it]Running Inference:  35%|███▍      | 104/300 [03:06<06:54,  2.12s/it]Running Inference:  35%|███▌      | 105/300 [03:07<06:13,  1.92s/it]Running Inference:  35%|███▌      | 106/300 [03:10<07:08,  2.21s/it]Running Inference:  36%|███▌      | 107/300 [03:12<06:34,  2.04s/it]Running Inference:  36%|███▌      | 108/300 [03:13<05:49,  1.82s/it]Running Inference:  36%|███▋      | 109/300 [03:16<06:50,  2.15s/it]Running Inference:  37%|███▋      | 110/300 [03:19<07:23,  2.33s/it]Running Inference:  37%|███▋      | 111/300 [03:21<07:15,  2.31s/it]Running Inference:  37%|███▋      | 112/300 [03:23<07:08,  2.28s/it]Running Inference:  38%|███▊      | 113/300 [03:24<05:43,  1.83s/it]Running Inference:  38%|███▊      | 114/300 [03:27<06:19,  2.04s/it]Running Inference:  38%|███▊      | 115/300 [03:28<05:33,  1.80s/it]Running Inference:  39%|███▊      | 116/300 [03:29<05:01,  1.64s/it]Running Inference:  39%|███▉      | 117/300 [03:30<04:11,  1.37s/it]Running Inference:  39%|███▉      | 118/300 [03:32<05:15,  1.74s/it]Running Inference:  40%|███▉      | 119/300 [03:35<05:39,  1.87s/it]Running Inference:  40%|████      | 120/300 [03:35<04:32,  1.51s/it]Running Inference:  40%|████      | 121/300 [03:39<06:27,  2.16s/it]Running Inference:  41%|████      | 122/300 [03:43<07:42,  2.60s/it]Running Inference:  41%|████      | 123/300 [03:44<06:38,  2.25s/it]Running Inference:  41%|████▏     | 124/300 [03:45<05:24,  1.85s/it]Running Inference:  42%|████▏     | 125/300 [03:47<05:14,  1.80s/it]Running Inference:  42%|████▏     | 126/300 [03:48<04:26,  1.53s/it]Running Inference:  42%|████▏     | 127/300 [03:49<04:08,  1.44s/it]Running Inference:  43%|████▎     | 128/300 [03:49<03:07,  1.09s/it]Running Inference:  43%|████▎     | 129/300 [03:51<03:26,  1.21s/it]Running Inference:  43%|████▎     | 130/300 [03:51<02:55,  1.03s/it]Running Inference:  44%|████▎     | 131/300 [03:52<02:39,  1.06it/s]Running Inference:  44%|████▍     | 132/300 [03:53<02:49,  1.01s/it]Running Inference:  44%|████▍     | 133/300 [03:54<02:36,  1.06it/s]Running Inference:  45%|████▍     | 134/300 [03:58<04:53,  1.77s/it]Running Inference:  45%|████▌     | 135/300 [04:00<05:01,  1.83s/it]Running Inference:  45%|████▌     | 136/300 [04:00<04:15,  1.56s/it]Running Inference:  46%|████▌     | 137/300 [04:02<03:57,  1.46s/it]Running Inference:  46%|████▌     | 138/300 [04:04<04:42,  1.74s/it]Running Inference:  46%|████▋     | 139/300 [04:08<06:20,  2.36s/it]Running Inference:  47%|████▋     | 140/300 [04:10<06:07,  2.30s/it]Running Inference:  47%|████▋     | 141/300 [04:11<04:44,  1.79s/it]Running Inference:  47%|████▋     | 142/300 [04:11<03:53,  1.48s/it]Running Inference:  48%|████▊     | 143/300 [04:14<04:28,  1.71s/it]Running Inference:  48%|████▊     | 144/300 [04:17<05:22,  2.07s/it]Running Inference:  48%|████▊     | 145/300 [04:18<05:04,  1.96s/it]Running Inference:  49%|████▊     | 146/300 [04:19<04:27,  1.74s/it]Running Inference:  49%|████▉     | 147/300 [04:23<05:52,  2.31s/it]Running Inference:  49%|████▉     | 148/300 [04:24<04:41,  1.85s/it]Running Inference:  50%|████▉     | 149/300 [04:25<03:46,  1.50s/it]Running Inference:  50%|█████     | 150/300 [04:25<03:16,  1.31s/it]Running Inference:  50%|█████     | 151/300 [04:28<04:16,  1.72s/it]Running Inference:  51%|█████     | 152/300 [04:32<05:31,  2.24s/it]Running Inference:  51%|█████     | 153/300 [04:36<06:54,  2.82s/it]Running Inference:  51%|█████▏    | 154/300 [04:39<06:50,  2.81s/it]Running Inference:  52%|█████▏    | 155/300 [04:41<06:23,  2.65s/it]Running Inference:  52%|█████▏    | 156/300 [04:42<05:32,  2.31s/it]Running Inference:  52%|█████▏    | 157/300 [04:43<04:23,  1.85s/it]Running Inference:  53%|█████▎    | 158/300 [04:45<04:28,  1.89s/it]Running Inference:  53%|█████▎    | 159/300 [04:46<03:34,  1.52s/it]Running Inference:  53%|█████▎    | 160/300 [04:47<03:01,  1.29s/it]Running Inference:  54%|█████▎    | 161/300 [04:49<03:41,  1.59s/it]Running Inference:  54%|█████▍    | 162/300 [04:49<02:54,  1.26s/it]Running Inference:  54%|█████▍    | 163/300 [04:50<02:37,  1.15s/it]Running Inference:  55%|█████▍    | 164/300 [04:51<02:12,  1.03it/s]Running Inference:  55%|█████▌    | 165/300 [04:51<01:52,  1.20it/s]Running Inference:  55%|█████▌    | 166/300 [04:52<01:58,  1.13it/s]Running Inference:  56%|█████▌    | 167/300 [04:54<02:33,  1.15s/it]Running Inference:  56%|█████▌    | 168/300 [04:56<03:06,  1.41s/it]Running Inference:  56%|█████▋    | 169/300 [04:59<03:48,  1.74s/it]Running Inference:  57%|█████▋    | 170/300 [05:00<03:31,  1.63s/it]Running Inference:  57%|█████▋    | 171/300 [05:01<03:09,  1.47s/it]Running Inference:  57%|█████▋    | 172/300 [05:02<02:56,  1.38s/it]Running Inference:  58%|█████▊    | 173/300 [05:06<04:32,  2.14s/it]Running Inference:  58%|█████▊    | 174/300 [05:09<04:57,  2.36s/it]Running Inference:  58%|█████▊    | 175/300 [05:11<04:26,  2.13s/it]Running Inference:  59%|█████▊    | 176/300 [05:12<03:42,  1.80s/it]Running Inference:  59%|█████▉    | 177/300 [05:13<03:11,  1.55s/it]Running Inference:  59%|█████▉    | 178/300 [05:15<03:28,  1.71s/it]Running Inference:  60%|█████▉    | 179/300 [05:17<03:55,  1.95s/it]Running Inference:  60%|██████    | 180/300 [05:18<03:30,  1.75s/it]Running Inference:  60%|██████    | 181/300 [05:21<03:55,  1.98s/it]Running Inference:  61%|██████    | 182/300 [05:22<03:33,  1.81s/it]Running Inference:  61%|██████    | 183/300 [05:27<04:54,  2.52s/it]Running Inference:  61%|██████▏   | 184/300 [05:29<05:00,  2.59s/it]Running Inference:  62%|██████▏   | 185/300 [05:32<04:59,  2.60s/it]Running Inference:  62%|██████▏   | 186/300 [05:34<04:47,  2.52s/it]Running Inference:  62%|██████▏   | 187/300 [05:36<04:27,  2.37s/it]Running Inference:  63%|██████▎   | 188/300 [05:38<04:07,  2.21s/it]Running Inference:  63%|██████▎   | 189/300 [05:42<05:13,  2.82s/it]Running Inference:  63%|██████▎   | 190/300 [05:46<05:46,  3.15s/it]Running Inference:  64%|██████▎   | 191/300 [05:51<06:18,  3.47s/it]Running Inference:  64%|██████▍   | 192/300 [05:53<05:30,  3.06s/it]Running Inference:  64%|██████▍   | 193/300 [05:55<04:59,  2.80s/it]Running Inference:  65%|██████▍   | 194/300 [05:58<05:04,  2.87s/it]Running Inference:  65%|██████▌   | 195/300 [05:59<03:53,  2.22s/it]Running Inference:  65%|██████▌   | 196/300 [05:59<03:05,  1.79s/it]Running Inference:  66%|██████▌   | 197/300 [06:02<03:23,  1.97s/it]Running Inference:  66%|██████▌   | 198/300 [06:05<03:58,  2.33s/it]Running Inference:  66%|██████▋   | 199/300 [06:08<04:29,  2.66s/it]Running Inference:  67%|██████▋   | 200/300 [06:10<03:42,  2.23s/it]Running Inference:  67%|██████▋   | 201/300 [06:12<03:58,  2.41s/it]Running Inference:  67%|██████▋   | 202/300 [06:13<03:09,  1.94s/it]Running Inference:  68%|██████▊   | 203/300 [06:15<03:17,  2.04s/it]Running Inference:  68%|██████▊   | 204/300 [06:18<03:28,  2.17s/it]Running Inference:  68%|██████▊   | 205/300 [06:20<03:34,  2.26s/it]Running Inference:  69%|██████▊   | 206/300 [06:24<04:04,  2.60s/it]Running Inference:  69%|██████▉   | 207/300 [06:25<03:13,  2.09s/it]Running Inference:  69%|██████▉   | 208/300 [06:27<03:13,  2.10s/it]Running Inference:  70%|██████▉   | 209/300 [06:31<04:08,  2.73s/it]Running Inference:  70%|███████   | 210/300 [06:32<03:10,  2.12s/it]Running Inference:  70%|███████   | 211/300 [06:34<03:22,  2.28s/it]Running Inference:  71%|███████   | 212/300 [06:35<02:37,  1.78s/it]Running Inference:  71%|███████   | 213/300 [06:36<02:20,  1.62s/it]Running Inference:  71%|███████▏  | 214/300 [06:37<02:00,  1.41s/it]Running Inference:  72%|███████▏  | 215/300 [06:39<02:12,  1.56s/it]Running Inference:  72%|███████▏  | 216/300 [06:40<01:53,  1.35s/it]Running Inference:  72%|███████▏  | 217/300 [06:41<01:43,  1.25s/it]Running Inference:  73%|███████▎  | 218/300 [06:42<01:42,  1.25s/it]Running Inference:  73%|███████▎  | 219/300 [06:43<01:34,  1.16s/it]Running Inference:  73%|███████▎  | 220/300 [06:45<01:53,  1.42s/it]Running Inference:  74%|███████▎  | 221/300 [06:46<01:30,  1.15s/it]Running Inference:  74%|███████▍  | 222/300 [06:48<01:56,  1.49s/it]Running Inference:  74%|███████▍  | 223/300 [06:49<01:39,  1.29s/it]Running Inference:  75%|███████▍  | 224/300 [06:51<01:46,  1.40s/it]Running Inference:  75%|███████▌  | 225/300 [06:52<01:55,  1.54s/it]Running Inference:  75%|███████▌  | 226/300 [06:54<02:04,  1.68s/it]Running Inference:  76%|███████▌  | 227/300 [06:56<01:57,  1.61s/it]Running Inference:  76%|███████▌  | 228/300 [06:57<01:51,  1.56s/it]Running Inference:  76%|███████▋  | 229/300 [06:58<01:27,  1.23s/it]Running Inference:  77%|███████▋  | 230/300 [06:59<01:36,  1.38s/it]Running Inference:  77%|███████▋  | 231/300 [07:01<01:47,  1.55s/it]Running Inference:  77%|███████▋  | 232/300 [07:05<02:22,  2.09s/it]Running Inference:  78%|███████▊  | 233/300 [07:07<02:16,  2.04s/it]Running Inference:  78%|███████▊  | 234/300 [07:08<02:06,  1.91s/it]Running Inference:  78%|███████▊  | 235/300 [07:09<01:49,  1.68s/it]Running Inference:  79%|███████▊  | 236/300 [07:13<02:17,  2.15s/it]Running Inference:  79%|███████▉  | 237/300 [07:14<01:54,  1.82s/it]Running Inference:  79%|███████▉  | 238/300 [07:18<02:32,  2.47s/it]Running Inference:  80%|███████▉  | 239/300 [07:19<02:16,  2.24s/it]Running Inference:  80%|████████  | 240/300 [07:20<01:52,  1.87s/it]Running Inference:  80%|████████  | 241/300 [07:22<01:54,  1.94s/it]Running Inference:  81%|████████  | 242/300 [07:24<01:46,  1.84s/it]Running Inference:  81%|████████  | 243/300 [07:26<01:44,  1.84s/it]Running Inference:  81%|████████▏ | 244/300 [07:27<01:33,  1.67s/it]Running Inference:  82%|████████▏ | 245/300 [07:31<02:00,  2.20s/it]Running Inference:  82%|████████▏ | 246/300 [07:31<01:33,  1.74s/it]Running Inference:  82%|████████▏ | 247/300 [07:33<01:26,  1.64s/it]Running Inference:  83%|████████▎ | 248/300 [07:34<01:12,  1.40s/it]Running Inference:  83%|████████▎ | 249/300 [07:34<01:03,  1.24s/it]Running Inference:  83%|████████▎ | 250/300 [07:36<01:06,  1.33s/it]Running Inference:  84%|████████▎ | 251/300 [07:38<01:14,  1.52s/it]Running Inference:  84%|████████▍ | 252/300 [07:40<01:22,  1.71s/it]Running Inference:  84%|████████▍ | 253/300 [07:42<01:23,  1.78s/it]Running Inference:  85%|████████▍ | 254/300 [07:44<01:24,  1.84s/it]Running Inference:  85%|████████▌ | 255/300 [07:47<01:43,  2.31s/it]Running Inference:  85%|████████▌ | 256/300 [07:52<02:07,  2.90s/it]Running Inference:  86%|████████▌ | 257/300 [07:52<01:36,  2.24s/it]Running Inference:  86%|████████▌ | 258/300 [07:55<01:33,  2.22s/it]Running Inference:  86%|████████▋ | 259/300 [07:57<01:32,  2.25s/it]Running Inference:  87%|████████▋ | 260/300 [07:58<01:17,  1.93s/it]Running Inference:  87%|████████▋ | 261/300 [08:02<01:33,  2.40s/it]Running Inference:  87%|████████▋ | 262/300 [08:03<01:17,  2.04s/it]Running Inference:  88%|████████▊ | 263/300 [08:05<01:16,  2.07s/it]Running Inference:  88%|████████▊ | 264/300 [08:07<01:12,  2.00s/it]Running Inference:  88%|████████▊ | 265/300 [08:09<01:12,  2.07s/it]Running Inference:  89%|████████▊ | 266/300 [08:10<00:57,  1.68s/it]Running Inference:  89%|████████▉ | 267/300 [08:11<00:48,  1.47s/it]Running Inference:  89%|████████▉ | 268/300 [08:12<00:41,  1.29s/it]Running Inference:  90%|████████▉ | 269/300 [08:12<00:34,  1.11s/it]Running Inference:  90%|█████████ | 270/300 [08:13<00:31,  1.06s/it]Running Inference:  90%|█████████ | 271/300 [08:14<00:28,  1.02it/s]Running Inference:  91%|█████████ | 272/300 [08:15<00:25,  1.08it/s]Running Inference:  91%|█████████ | 273/300 [08:19<00:50,  1.86s/it]Running Inference:  91%|█████████▏| 274/300 [08:20<00:45,  1.74s/it]Running Inference:  92%|█████████▏| 275/300 [08:22<00:41,  1.65s/it]Running Inference:  92%|█████████▏| 276/300 [08:25<00:49,  2.08s/it]Running Inference:  92%|█████████▏| 277/300 [08:27<00:49,  2.16s/it]Running Inference:  93%|█████████▎| 278/300 [08:28<00:37,  1.70s/it]Running Inference:  93%|█████████▎| 279/300 [08:31<00:47,  2.26s/it]Running Inference:  93%|█████████▎| 280/300 [08:33<00:40,  2.05s/it]Running Inference:  94%|█████████▎| 281/300 [08:35<00:38,  2.04s/it]Running Inference:  94%|█████████▍| 282/300 [08:39<00:48,  2.70s/it]Running Inference:  94%|█████████▍| 283/300 [08:43<00:53,  3.17s/it]Running Inference:  95%|█████████▍| 284/300 [08:46<00:46,  2.89s/it]Running Inference:  95%|█████████▌| 285/300 [08:46<00:33,  2.25s/it]Running Inference:  95%|█████████▌| 286/300 [08:47<00:25,  1.79s/it]Running Inference:  96%|█████████▌| 287/300 [08:48<00:21,  1.64s/it]Running Inference:  96%|█████████▌| 288/300 [08:52<00:26,  2.22s/it]Running Inference:  96%|█████████▋| 289/300 [08:52<00:17,  1.63s/it]Running Inference:  97%|█████████▋| 290/300 [08:54<00:16,  1.62s/it]Running Inference:  97%|█████████▋| 291/300 [08:55<00:14,  1.59s/it]Running Inference:  97%|█████████▋| 292/300 [08:56<00:11,  1.39s/it]Running Inference:  98%|█████████▊| 293/300 [08:58<00:10,  1.44s/it]Running Inference:  98%|█████████▊| 294/300 [08:59<00:08,  1.44s/it]Running Inference:  98%|█████████▊| 295/300 [09:02<00:08,  1.70s/it]Running Inference:  99%|█████████▊| 296/300 [09:04<00:07,  1.86s/it]Running Inference:  99%|█████████▉| 297/300 [09:06<00:05,  1.84s/it]Running Inference:  99%|█████████▉| 298/300 [09:08<00:03,  1.99s/it]Running Inference: 100%|█████████▉| 299/300 [09:08<00:01,  1.54s/it]Running Inference: 100%|██████████| 300/300 [09:12<00:00,  2.14s/it]Running Inference: 100%|██████████| 300/300 [09:12<00:00,  1.84s/it]
2025-12-15 09:23:15,522 - INFO - Inference completed.
2025-12-15 09:23:15,532 - INFO - Results saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-15 09:23:15,532 - INFO - Calculating metrics for dataset: longbench
2025-12-15 09:23:15,537 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-15 09:23:15,537 - INFO - Metrics:
41.9
2025-12-15 09:23:15,538 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa_e, ratio=0.2) on GPU 1

----------------------------------------
Task: hotpotqa_e | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 09:23:21,960 - INFO - Set deterministic seeds to 42
2025-12-15 09:23:21,960 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 09:23:21,960 - INFO - Starting evaluation run...
2025-12-15 09:23:21,960 - INFO - Output directory set to: longbenchresult
2025-12-15 09:23:21,960 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-15 09:23:21,960 - INFO - KV Press 'tova' setup.
2025-12-15 09:23:21,961 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 09:23:21,961 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.07it/s]
Device set to use cuda:0
2025-12-15 09:23:32,917 - INFO - Model pipeline loaded.
2025-12-15 09:23:32,917 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa_e)
2025-12-15 09:23:35,955 - INFO - Dataset loaded with 300 entries.
2025-12-15 09:23:35,955 - INFO - Dataset processed with 300 entries.
2025-12-15 09:23:35,991 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:05<25:00,  5.02s/it]Running Inference:   1%|          | 2/300 [00:07<16:21,  3.29s/it]Running Inference:   1%|          | 3/300 [00:07<10:28,  2.12s/it]Running Inference:   1%|▏         | 4/300 [00:08<07:05,  1.44s/it]Running Inference:   2%|▏         | 5/300 [00:09<06:58,  1.42s/it]Running Inference:   2%|▏         | 6/300 [00:10<05:45,  1.18s/it]Running Inference:   2%|▏         | 7/300 [00:13<08:24,  1.72s/it]Running Inference:   3%|▎         | 8/300 [00:13<06:43,  1.38s/it]Running Inference:   3%|▎         | 9/300 [00:16<08:07,  1.68s/it]Running Inference:   3%|▎         | 10/300 [00:17<07:57,  1.65s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:20<09:17,  1.93s/it]Running Inference:   4%|▍         | 12/300 [00:22<09:37,  2.01s/it]Running Inference:   4%|▍         | 13/300 [00:23<07:40,  1.61s/it]Running Inference:   5%|▍         | 14/300 [00:23<06:16,  1.32s/it]Running Inference:   5%|▌         | 15/300 [00:26<08:00,  1.69s/it]Running Inference:   5%|▌         | 16/300 [00:27<07:29,  1.58s/it]Running Inference:   6%|▌         | 17/300 [00:30<08:41,  1.84s/it]Running Inference:   6%|▌         | 18/300 [00:30<06:51,  1.46s/it]Running Inference:   6%|▋         | 19/300 [00:32<06:38,  1.42s/it]Running Inference:   7%|▋         | 20/300 [00:32<05:41,  1.22s/it]Running Inference:   7%|▋         | 21/300 [00:36<08:34,  1.85s/it]Running Inference:   7%|▋         | 22/300 [00:36<06:57,  1.50s/it]Running Inference:   8%|▊         | 23/300 [00:39<08:45,  1.90s/it]Running Inference:   8%|▊         | 24/300 [00:43<11:15,  2.45s/it]Running Inference:   8%|▊         | 25/300 [00:44<09:24,  2.05s/it]Running Inference:   9%|▊         | 26/300 [00:47<11:08,  2.44s/it]Running Inference:   9%|▉         | 27/300 [00:49<09:47,  2.15s/it]Running Inference:   9%|▉         | 28/300 [00:53<12:40,  2.79s/it]Running Inference:  10%|▉         | 29/300 [00:54<09:46,  2.16s/it]Running Inference:  10%|█         | 30/300 [00:55<08:50,  1.97s/it]Running Inference:  10%|█         | 31/300 [00:56<07:30,  1.67s/it]Running Inference:  11%|█         | 32/300 [00:59<08:53,  1.99s/it]Running Inference:  11%|█         | 33/300 [01:01<08:22,  1.88s/it]Running Inference:  11%|█▏        | 34/300 [01:03<08:57,  2.02s/it]Running Inference:  12%|█▏        | 35/300 [01:05<09:22,  2.12s/it]Running Inference:  12%|█▏        | 36/300 [01:06<07:30,  1.71s/it]Running Inference:  12%|█▏        | 37/300 [01:07<06:42,  1.53s/it]Running Inference:  13%|█▎        | 38/300 [01:08<05:21,  1.23s/it]Running Inference:  13%|█▎        | 39/300 [01:09<05:17,  1.22s/it]Running Inference:  13%|█▎        | 40/300 [01:10<05:21,  1.24s/it]Running Inference:  14%|█▎        | 41/300 [01:11<04:30,  1.04s/it]Running Inference:  14%|█▍        | 42/300 [01:13<06:04,  1.41s/it]Running Inference:  14%|█▍        | 43/300 [01:17<09:32,  2.23s/it]Running Inference:  15%|█▍        | 44/300 [01:18<07:38,  1.79s/it]Running Inference:  15%|█▌        | 45/300 [01:20<07:23,  1.74s/it]Running Inference:  15%|█▌        | 46/300 [01:21<06:42,  1.58s/it]Running Inference:  16%|█▌        | 47/300 [01:22<05:38,  1.34s/it]Running Inference:  16%|█▌        | 48/300 [01:24<06:53,  1.64s/it]Running Inference:  16%|█▋        | 49/300 [01:26<06:50,  1.64s/it]Running Inference:  17%|█▋        | 50/300 [01:27<06:48,  1.64s/it]Running Inference:  17%|█▋        | 51/300 [01:31<10:00,  2.41s/it]Running Inference:  17%|█▋        | 52/300 [01:32<07:40,  1.86s/it]Running Inference:  18%|█▊        | 53/300 [01:33<06:30,  1.58s/it]Running Inference:  18%|█▊        | 54/300 [01:34<06:11,  1.51s/it]Running Inference:  18%|█▊        | 55/300 [01:37<08:00,  1.96s/it]Running Inference:  19%|█▊        | 56/300 [01:40<08:58,  2.21s/it]Running Inference:  19%|█▉        | 57/300 [01:42<08:31,  2.11s/it]Running Inference:  19%|█▉        | 58/300 [01:43<07:45,  1.92s/it]Running Inference:  20%|█▉        | 59/300 [01:47<09:54,  2.47s/it]Running Inference:  20%|██        | 60/300 [01:51<11:00,  2.75s/it]Running Inference:  20%|██        | 61/300 [01:53<10:16,  2.58s/it]Running Inference:  21%|██        | 62/300 [01:53<07:52,  1.99s/it]Running Inference:  21%|██        | 63/300 [01:55<07:39,  1.94s/it]Running Inference:  21%|██▏       | 64/300 [01:56<06:44,  1.71s/it]Running Inference:  22%|██▏       | 65/300 [01:57<05:41,  1.45s/it]Running Inference:  22%|██▏       | 66/300 [01:58<04:36,  1.18s/it]Running Inference:  22%|██▏       | 67/300 [02:01<07:01,  1.81s/it]Running Inference:  23%|██▎       | 68/300 [02:02<06:13,  1.61s/it]Running Inference:  23%|██▎       | 69/300 [02:05<07:13,  1.88s/it]Running Inference:  23%|██▎       | 70/300 [02:08<08:58,  2.34s/it]Running Inference:  24%|██▎       | 71/300 [02:09<07:26,  1.95s/it]Running Inference:  24%|██▍       | 72/300 [02:10<06:42,  1.76s/it]Running Inference:  24%|██▍       | 73/300 [02:11<05:25,  1.44s/it]Running Inference:  25%|██▍       | 74/300 [02:13<06:08,  1.63s/it]Running Inference:  25%|██▌       | 75/300 [02:15<05:52,  1.57s/it]Running Inference:  25%|██▌       | 76/300 [02:16<05:50,  1.57s/it]Running Inference:  26%|██▌       | 77/300 [02:18<06:23,  1.72s/it]Running Inference:  26%|██▌       | 78/300 [02:20<06:34,  1.78s/it]Running Inference:  26%|██▋       | 79/300 [02:21<05:25,  1.47s/it]Running Inference:  27%|██▋       | 80/300 [02:24<07:17,  1.99s/it]Running Inference:  27%|██▋       | 81/300 [02:27<07:49,  2.14s/it]Running Inference:  27%|██▋       | 82/300 [02:28<07:15,  2.00s/it]Running Inference:  28%|██▊       | 83/300 [02:29<06:14,  1.72s/it]Running Inference:  28%|██▊       | 84/300 [02:31<05:42,  1.58s/it]Running Inference:  28%|██▊       | 85/300 [02:33<06:14,  1.74s/it]Running Inference:  29%|██▊       | 86/300 [02:33<04:45,  1.33s/it]Running Inference:  29%|██▉       | 87/300 [02:34<04:36,  1.30s/it]Running Inference:  29%|██▉       | 88/300 [02:37<06:29,  1.84s/it]Running Inference:  30%|██▉       | 89/300 [02:40<06:44,  1.92s/it]Running Inference:  30%|███       | 90/300 [02:40<05:30,  1.57s/it]Running Inference:  30%|███       | 91/300 [02:44<07:17,  2.09s/it]Running Inference:  31%|███       | 92/300 [02:45<06:43,  1.94s/it]Running Inference:  31%|███       | 93/300 [02:47<06:40,  1.93s/it]Running Inference:  31%|███▏      | 94/300 [02:48<05:36,  1.64s/it]Running Inference:  32%|███▏      | 95/300 [02:49<04:42,  1.38s/it]Running Inference:  32%|███▏      | 96/300 [02:52<06:04,  1.78s/it]Running Inference:  32%|███▏      | 97/300 [02:52<04:50,  1.43s/it]Running Inference:  33%|███▎      | 98/300 [02:53<03:52,  1.15s/it]Running Inference:  33%|███▎      | 99/300 [02:53<03:11,  1.05it/s]Running Inference:  33%|███▎      | 100/300 [02:54<03:13,  1.03it/s]Running Inference:  34%|███▎      | 101/300 [02:55<03:16,  1.01it/s]Running Inference:  34%|███▍      | 102/300 [02:59<06:12,  1.88s/it]Running Inference:  34%|███▍      | 103/300 [03:01<06:22,  1.94s/it]Running Inference:  35%|███▍      | 104/300 [03:04<06:39,  2.04s/it]Running Inference:  35%|███▌      | 105/300 [03:05<06:02,  1.86s/it]Running Inference:  35%|███▌      | 106/300 [03:08<07:00,  2.17s/it]Running Inference:  36%|███▌      | 107/300 [03:09<06:26,  2.00s/it]Running Inference:  36%|███▌      | 108/300 [03:11<05:43,  1.79s/it]Running Inference:  36%|███▋      | 109/300 [03:14<06:46,  2.13s/it]Running Inference:  37%|███▋      | 110/300 [03:16<07:19,  2.32s/it]Running Inference:  37%|███▋      | 111/300 [03:19<07:13,  2.29s/it]Running Inference:  37%|███▋      | 112/300 [03:21<07:06,  2.27s/it]Running Inference:  38%|███▊      | 113/300 [03:22<05:41,  1.83s/it]Running Inference:  38%|███▊      | 114/300 [03:24<06:18,  2.03s/it]Running Inference:  38%|███▊      | 115/300 [03:25<05:32,  1.80s/it]Running Inference:  39%|███▊      | 116/300 [03:27<05:00,  1.63s/it]Running Inference:  39%|███▉      | 117/300 [03:27<04:10,  1.37s/it]Running Inference:  39%|███▉      | 118/300 [03:30<05:14,  1.73s/it]Running Inference:  40%|███▉      | 119/300 [03:32<05:38,  1.87s/it]Running Inference:  40%|████      | 120/300 [03:33<04:31,  1.51s/it]Running Inference:  40%|████      | 121/300 [03:37<06:27,  2.16s/it]Running Inference:  41%|████      | 122/300 [03:40<07:42,  2.60s/it]Running Inference:  41%|████      | 123/300 [03:42<06:37,  2.25s/it]Running Inference:  41%|████▏     | 124/300 [03:42<05:24,  1.84s/it]Running Inference:  42%|████▏     | 125/300 [03:44<05:14,  1.80s/it]Running Inference:  42%|████▏     | 126/300 [03:45<04:25,  1.53s/it]Running Inference:  42%|████▏     | 127/300 [03:46<04:08,  1.43s/it]Running Inference:  43%|████▎     | 128/300 [03:47<03:06,  1.09s/it]Running Inference:  43%|████▎     | 129/300 [03:48<03:26,  1.21s/it]Running Inference:  43%|████▎     | 130/300 [03:49<02:54,  1.03s/it]Running Inference:  44%|████▎     | 131/300 [03:49<02:38,  1.07it/s]Running Inference:  44%|████▍     | 132/300 [03:51<02:49,  1.01s/it]Running Inference:  44%|████▍     | 133/300 [03:51<02:36,  1.07it/s]Running Inference:  45%|████▍     | 134/300 [03:55<04:52,  1.76s/it]Running Inference:  45%|████▌     | 135/300 [03:57<05:00,  1.82s/it]Running Inference:  45%|████▌     | 136/300 [03:58<04:14,  1.55s/it]Running Inference:  46%|████▌     | 137/300 [03:59<03:56,  1.45s/it]Running Inference:  46%|████▌     | 138/300 [04:02<04:41,  1.74s/it]Running Inference:  46%|████▋     | 139/300 [04:05<06:19,  2.36s/it]Running Inference:  47%|████▋     | 140/300 [04:07<06:06,  2.29s/it]Running Inference:  47%|████▋     | 141/300 [04:08<04:43,  1.79s/it]Running Inference:  47%|████▋     | 142/300 [04:09<03:53,  1.48s/it]Running Inference:  48%|████▊     | 143/300 [04:11<04:27,  1.70s/it]Running Inference:  48%|████▊     | 144/300 [04:13<04:34,  1.76s/it]Running Inference:  48%|████▊     | 145/300 [04:15<04:30,  1.74s/it]Running Inference:  49%|████▊     | 146/300 [04:16<04:03,  1.58s/it]Running Inference:  49%|████▉     | 147/300 [04:20<05:36,  2.20s/it]Running Inference:  49%|████▉     | 148/300 [04:20<04:29,  1.78s/it]Running Inference:  50%|████▉     | 149/300 [04:21<03:37,  1.44s/it]Running Inference:  50%|█████     | 150/300 [04:22<03:10,  1.27s/it]Running Inference:  50%|█████     | 151/300 [04:25<04:12,  1.69s/it]Running Inference:  51%|█████     | 152/300 [04:28<05:28,  2.22s/it]Running Inference:  51%|█████     | 153/300 [04:32<06:51,  2.80s/it]Running Inference:  51%|█████▏    | 154/300 [04:35<06:48,  2.80s/it]Running Inference:  52%|█████▏    | 155/300 [04:37<06:22,  2.64s/it]Running Inference:  52%|█████▏    | 156/300 [04:39<05:31,  2.31s/it]Running Inference:  52%|█████▏    | 157/300 [04:39<04:23,  1.84s/it]Running Inference:  53%|█████▎    | 158/300 [04:41<04:28,  1.89s/it]Running Inference:  53%|█████▎    | 159/300 [04:42<03:34,  1.52s/it]Running Inference:  53%|█████▎    | 160/300 [04:43<03:00,  1.29s/it]Running Inference:  54%|█████▎    | 161/300 [04:45<03:41,  1.59s/it]Running Inference:  54%|█████▍    | 162/300 [04:46<03:22,  1.47s/it]Running Inference:  54%|█████▍    | 163/300 [04:47<02:56,  1.29s/it]Running Inference:  55%|█████▍    | 164/300 [04:48<02:25,  1.07s/it]Running Inference:  55%|█████▌    | 165/300 [04:48<02:02,  1.11it/s]Running Inference:  55%|█████▌    | 166/300 [04:49<02:04,  1.07it/s]Running Inference:  56%|█████▌    | 167/300 [04:53<03:41,  1.67s/it]Running Inference:  56%|█████▌    | 168/300 [04:55<03:53,  1.77s/it]Running Inference:  56%|█████▋    | 169/300 [04:57<04:19,  1.98s/it]Running Inference:  57%|█████▋    | 170/300 [04:59<03:52,  1.79s/it]Running Inference:  57%|█████▋    | 171/300 [05:00<03:23,  1.58s/it]Running Inference:  57%|█████▋    | 172/300 [05:01<03:06,  1.46s/it]Running Inference:  58%|█████▊    | 173/300 [05:05<04:39,  2.20s/it]Running Inference:  58%|█████▊    | 174/300 [05:08<05:02,  2.40s/it]Running Inference:  58%|█████▊    | 175/300 [05:09<04:29,  2.15s/it]Running Inference:  59%|█████▊    | 176/300 [05:10<03:44,  1.81s/it]Running Inference:  59%|█████▉    | 177/300 [05:11<03:12,  1.56s/it]Running Inference:  59%|█████▉    | 178/300 [05:13<03:29,  1.71s/it]Running Inference:  60%|█████▉    | 179/300 [05:16<03:56,  1.95s/it]Running Inference:  60%|██████    | 180/300 [05:17<03:30,  1.76s/it]Running Inference:  60%|██████    | 181/300 [05:20<03:55,  1.98s/it]Running Inference:  61%|██████    | 182/300 [05:21<03:33,  1.81s/it]Running Inference:  61%|██████    | 183/300 [05:25<04:54,  2.51s/it]Running Inference:  61%|██████▏   | 184/300 [05:27<04:41,  2.43s/it]Running Inference:  62%|██████▏   | 185/300 [05:30<04:49,  2.52s/it]Running Inference:  62%|██████▏   | 186/300 [05:32<04:36,  2.43s/it]Running Inference:  62%|██████▏   | 187/300 [05:34<04:20,  2.31s/it]Running Inference:  63%|██████▎   | 188/300 [05:36<04:02,  2.17s/it]Running Inference:  63%|██████▎   | 189/300 [05:38<04:04,  2.20s/it]Running Inference:  63%|██████▎   | 190/300 [05:42<04:41,  2.56s/it]Running Inference:  64%|██████▎   | 191/300 [05:46<05:32,  3.05s/it]Running Inference:  64%|██████▍   | 192/300 [05:48<04:58,  2.77s/it]Running Inference:  64%|██████▍   | 193/300 [05:50<04:37,  2.59s/it]Running Inference:  65%|██████▍   | 194/300 [05:53<04:49,  2.73s/it]Running Inference:  65%|██████▌   | 195/300 [05:54<03:42,  2.12s/it]Running Inference:  65%|██████▌   | 196/300 [05:55<02:59,  1.73s/it]Running Inference:  66%|██████▌   | 197/300 [05:57<03:18,  1.93s/it]Running Inference:  66%|██████▌   | 198/300 [06:00<03:54,  2.30s/it]Running Inference:  66%|██████▋   | 199/300 [06:04<04:26,  2.64s/it]Running Inference:  67%|██████▋   | 200/300 [06:05<03:40,  2.20s/it]Running Inference:  67%|██████▋   | 201/300 [06:08<03:56,  2.39s/it]Running Inference:  67%|██████▋   | 202/300 [06:09<03:08,  1.92s/it]Running Inference:  68%|██████▊   | 203/300 [06:11<03:16,  2.02s/it]Running Inference:  68%|██████▊   | 204/300 [06:13<03:27,  2.16s/it]Running Inference:  68%|██████▊   | 205/300 [06:15<03:20,  2.11s/it]Running Inference:  69%|██████▊   | 206/300 [06:19<03:54,  2.49s/it]Running Inference:  69%|██████▉   | 207/300 [06:20<03:06,  2.01s/it]Running Inference:  69%|██████▉   | 208/300 [06:22<03:08,  2.05s/it]Running Inference:  70%|██████▉   | 209/300 [06:25<03:40,  2.42s/it]Running Inference:  70%|███████   | 210/300 [06:26<02:51,  1.90s/it]Running Inference:  70%|███████   | 211/300 [06:28<03:09,  2.12s/it]Running Inference:  71%|███████   | 212/300 [06:29<02:27,  1.68s/it]Running Inference:  71%|███████   | 213/300 [06:30<02:14,  1.54s/it]Running Inference:  71%|███████▏  | 214/300 [06:31<01:56,  1.35s/it]Running Inference:  72%|███████▏  | 215/300 [06:33<02:09,  1.52s/it]Running Inference:  72%|███████▏  | 216/300 [06:34<01:54,  1.36s/it]Running Inference:  72%|███████▏  | 217/300 [06:35<01:44,  1.26s/it]Running Inference:  73%|███████▎  | 218/300 [06:36<01:43,  1.26s/it]Running Inference:  73%|███████▎  | 219/300 [06:37<01:34,  1.16s/it]Running Inference:  73%|███████▎  | 220/300 [06:39<01:54,  1.43s/it]Running Inference:  74%|███████▎  | 221/300 [06:40<01:30,  1.15s/it]Running Inference:  74%|███████▍  | 222/300 [06:42<01:56,  1.50s/it]Running Inference:  74%|███████▍  | 223/300 [06:43<01:39,  1.29s/it]Running Inference:  75%|███████▍  | 224/300 [06:45<01:46,  1.40s/it]Running Inference:  75%|███████▌  | 225/300 [06:47<01:55,  1.54s/it]Running Inference:  75%|███████▌  | 226/300 [06:49<02:03,  1.67s/it]Running Inference:  76%|███████▌  | 227/300 [06:50<01:57,  1.60s/it]Running Inference:  76%|███████▌  | 228/300 [06:51<01:51,  1.55s/it]Running Inference:  76%|███████▋  | 229/300 [06:52<01:27,  1.23s/it]Running Inference:  77%|███████▋  | 230/300 [06:54<01:36,  1.38s/it]Running Inference:  77%|███████▋  | 231/300 [06:56<01:46,  1.55s/it]Running Inference:  77%|███████▋  | 232/300 [06:59<02:21,  2.09s/it]Running Inference:  78%|███████▊  | 233/300 [07:01<02:16,  2.03s/it]Running Inference:  78%|███████▊  | 234/300 [07:02<02:05,  1.90s/it]Running Inference:  78%|███████▊  | 235/300 [07:04<01:48,  1.67s/it]Running Inference:  79%|███████▊  | 236/300 [07:06<01:54,  1.79s/it]Running Inference:  79%|███████▉  | 237/300 [07:07<01:38,  1.56s/it]Running Inference:  79%|███████▉  | 238/300 [07:11<02:21,  2.28s/it]Running Inference:  80%|███████▉  | 239/300 [07:12<02:08,  2.11s/it]Running Inference:  80%|████████  | 240/300 [07:13<01:46,  1.78s/it]Running Inference:  80%|████████  | 241/300 [07:17<02:23,  2.43s/it]Running Inference:  81%|████████  | 242/300 [07:19<02:06,  2.18s/it]Running Inference:  81%|████████  | 243/300 [07:19<01:36,  1.69s/it]Running Inference:  81%|████████▏ | 244/300 [07:21<01:27,  1.57s/it]Running Inference:  82%|████████▏ | 245/300 [07:24<01:56,  2.12s/it]Running Inference:  82%|████████▏ | 246/300 [07:25<01:30,  1.68s/it]Running Inference:  82%|████████▏ | 247/300 [07:26<01:24,  1.60s/it]Running Inference:  83%|████████▎ | 248/300 [07:27<01:11,  1.37s/it]Running Inference:  83%|████████▎ | 249/300 [07:28<01:02,  1.22s/it]Running Inference:  83%|████████▎ | 250/300 [07:29<01:05,  1.31s/it]Running Inference:  84%|████████▎ | 251/300 [07:31<01:14,  1.51s/it]Running Inference:  84%|████████▍ | 252/300 [07:34<01:21,  1.71s/it]Running Inference:  84%|████████▍ | 253/300 [07:35<01:23,  1.77s/it]Running Inference:  85%|████████▍ | 254/300 [07:37<01:24,  1.83s/it]Running Inference:  85%|████████▌ | 255/300 [07:41<01:43,  2.31s/it]Running Inference:  85%|████████▌ | 256/300 [07:45<02:07,  2.90s/it]Running Inference:  86%|████████▌ | 257/300 [07:46<01:36,  2.24s/it]Running Inference:  86%|████████▌ | 258/300 [07:48<01:33,  2.23s/it]Running Inference:  86%|████████▋ | 259/300 [07:50<01:32,  2.25s/it]Running Inference:  87%|████████▋ | 260/300 [07:51<01:17,  1.93s/it]Running Inference:  87%|████████▋ | 261/300 [07:55<01:33,  2.39s/it]Running Inference:  87%|████████▋ | 262/300 [07:56<01:17,  2.04s/it]Running Inference:  88%|████████▊ | 263/300 [07:58<01:16,  2.07s/it]Running Inference:  88%|████████▊ | 264/300 [08:00<01:11,  2.00s/it]Running Inference:  88%|████████▊ | 265/300 [08:02<01:12,  2.07s/it]Running Inference:  89%|████████▊ | 266/300 [08:05<01:17,  2.29s/it]Running Inference:  89%|████████▉ | 267/300 [08:06<01:02,  1.89s/it]Running Inference:  89%|████████▉ | 268/300 [08:07<00:50,  1.59s/it]Running Inference:  90%|████████▉ | 269/300 [08:08<00:40,  1.31s/it]Running Inference:  90%|█████████ | 270/300 [08:09<00:36,  1.21s/it]Running Inference:  90%|█████████ | 271/300 [08:09<00:31,  1.08s/it]Running Inference:  91%|█████████ | 272/300 [08:10<00:27,  1.01it/s]Running Inference:  91%|█████████ | 273/300 [08:14<00:51,  1.91s/it]Running Inference:  91%|█████████▏| 274/300 [08:18<01:00,  2.33s/it]Running Inference:  92%|█████████▏| 275/300 [08:19<00:51,  2.06s/it]Running Inference:  92%|█████████▏| 276/300 [08:22<00:56,  2.36s/it]Running Inference:  92%|█████████▏| 277/300 [08:24<00:54,  2.36s/it]Running Inference:  93%|█████████▎| 278/300 [08:25<00:40,  1.84s/it]Running Inference:  93%|█████████▎| 279/300 [08:29<00:49,  2.36s/it]Running Inference:  93%|█████████▎| 280/300 [08:30<00:42,  2.12s/it]Running Inference:  94%|█████████▎| 281/300 [08:32<00:39,  2.09s/it]Running Inference:  94%|█████████▍| 282/300 [08:36<00:49,  2.74s/it]Running Inference:  94%|█████████▍| 283/300 [08:41<00:54,  3.19s/it]Running Inference:  95%|█████████▍| 284/300 [08:43<00:46,  2.90s/it]Running Inference:  95%|█████████▌| 285/300 [08:44<00:33,  2.26s/it]Running Inference:  95%|█████████▌| 286/300 [08:44<00:25,  1.80s/it]Running Inference:  96%|█████████▌| 287/300 [08:46<00:21,  1.65s/it]Running Inference:  96%|█████████▌| 288/300 [08:49<00:25,  2.17s/it]Running Inference:  96%|█████████▋| 289/300 [08:49<00:17,  1.59s/it]Running Inference:  97%|█████████▋| 290/300 [08:51<00:15,  1.59s/it]Running Inference:  97%|█████████▋| 291/300 [08:52<00:14,  1.57s/it]Running Inference:  97%|█████████▋| 292/300 [08:53<00:11,  1.38s/it]Running Inference:  98%|█████████▊| 293/300 [08:55<00:09,  1.43s/it]Running Inference:  98%|█████████▊| 294/300 [08:56<00:08,  1.44s/it]Running Inference:  98%|█████████▊| 295/300 [08:59<00:08,  1.70s/it]Running Inference:  99%|█████████▊| 296/300 [09:01<00:07,  1.85s/it]Running Inference:  99%|█████████▉| 297/300 [09:03<00:05,  1.83s/it]Running Inference:  99%|█████████▉| 298/300 [09:05<00:03,  1.99s/it]Running Inference: 100%|█████████▉| 299/300 [09:06<00:01,  1.54s/it]Running Inference: 100%|██████████| 300/300 [09:09<00:00,  2.13s/it]Running Inference: 100%|██████████| 300/300 [09:09<00:00,  1.83s/it]
2025-12-15 09:32:45,556 - INFO - Inference completed.
2025-12-15 09:32:45,566 - INFO - Results saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-15 09:32:45,566 - INFO - Calculating metrics for dataset: longbench
2025-12-15 09:32:45,571 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-15 09:32:45,572 - INFO - Metrics:
40.94
2025-12-15 09:32:45,573 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa_e, ratio=0.3) on GPU 1

----------------------------------------
Task: hotpotqa_e | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 09:32:51,924 - INFO - Set deterministic seeds to 42
2025-12-15 09:32:51,924 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 09:32:51,924 - INFO - Starting evaluation run...
2025-12-15 09:32:51,924 - INFO - Output directory set to: longbenchresult
2025-12-15 09:32:51,925 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-15 09:32:51,925 - INFO - KV Press 'tova' setup.
2025-12-15 09:32:51,925 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 09:32:51,925 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.72it/s]
Device set to use cuda:0
2025-12-15 09:33:02,963 - INFO - Model pipeline loaded.
2025-12-15 09:33:02,964 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa_e)
2025-12-15 09:33:06,037 - INFO - Dataset loaded with 300 entries.
2025-12-15 09:33:06,037 - INFO - Dataset processed with 300 entries.
2025-12-15 09:33:06,076 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:05<24:55,  5.00s/it]Running Inference:   1%|          | 2/300 [00:07<16:40,  3.36s/it]Running Inference:   1%|          | 3/300 [00:07<10:38,  2.15s/it]Running Inference:   1%|▏         | 4/300 [00:08<07:11,  1.46s/it]Running Inference:   2%|▏         | 5/300 [00:09<07:03,  1.43s/it]Running Inference:   2%|▏         | 6/300 [00:10<05:48,  1.19s/it]Running Inference:   2%|▏         | 7/300 [00:11<05:40,  1.16s/it]Running Inference:   3%|▎         | 8/300 [00:12<04:51,  1.00it/s]Running Inference:   3%|▎         | 9/300 [00:14<06:51,  1.41s/it]Running Inference:   3%|▎         | 10/300 [00:16<07:04,  1.46s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:18<08:40,  1.80s/it]Running Inference:   4%|▍         | 12/300 [00:20<09:12,  1.92s/it]Running Inference:   4%|▍         | 13/300 [00:21<07:23,  1.54s/it]Running Inference:   5%|▍         | 14/300 [00:22<06:04,  1.27s/it]Running Inference:   5%|▌         | 15/300 [00:24<07:51,  1.66s/it]Running Inference:   5%|▌         | 16/300 [00:26<07:23,  1.56s/it]Running Inference:   6%|▌         | 17/300 [00:28<08:37,  1.83s/it]Running Inference:   6%|▌         | 18/300 [00:29<06:48,  1.45s/it]Running Inference:   6%|▋         | 19/300 [00:30<06:36,  1.41s/it]Running Inference:   7%|▋         | 20/300 [00:31<05:39,  1.21s/it]Running Inference:   7%|▋         | 21/300 [00:34<08:34,  1.84s/it]Running Inference:   7%|▋         | 22/300 [00:35<06:57,  1.50s/it]Running Inference:   8%|▊         | 23/300 [00:37<08:46,  1.90s/it]Running Inference:   8%|▊         | 24/300 [00:40<09:08,  1.99s/it]Running Inference:   8%|▊         | 25/300 [00:41<07:56,  1.73s/it]Running Inference:   9%|▊         | 26/300 [00:44<10:07,  2.22s/it]Running Inference:   9%|▉         | 27/300 [00:46<09:05,  2.00s/it]Running Inference:   9%|▉         | 28/300 [00:50<12:10,  2.69s/it]Running Inference:  10%|▉         | 29/300 [00:51<09:25,  2.09s/it]Running Inference:  10%|█         | 30/300 [00:52<08:35,  1.91s/it]Running Inference:  10%|█         | 31/300 [00:53<07:20,  1.64s/it]Running Inference:  11%|█         | 32/300 [00:56<08:48,  1.97s/it]Running Inference:  11%|█         | 33/300 [00:57<07:54,  1.78s/it]Running Inference:  11%|█▏        | 34/300 [01:00<08:37,  1.95s/it]Running Inference:  12%|█▏        | 35/300 [01:02<09:08,  2.07s/it]Running Inference:  12%|█▏        | 36/300 [01:03<07:33,  1.72s/it]Running Inference:  12%|█▏        | 37/300 [01:06<08:49,  2.01s/it]Running Inference:  13%|█▎        | 38/300 [01:06<06:47,  1.55s/it]Running Inference:  13%|█▎        | 39/300 [01:07<06:16,  1.44s/it]Running Inference:  13%|█▎        | 40/300 [01:08<06:03,  1.40s/it]Running Inference:  14%|█▎        | 41/300 [01:09<04:59,  1.16s/it]Running Inference:  14%|█▍        | 42/300 [01:11<06:24,  1.49s/it]Running Inference:  14%|█▍        | 43/300 [01:15<09:48,  2.29s/it]Running Inference:  15%|█▍        | 44/300 [01:16<07:49,  1.83s/it]Running Inference:  15%|█▌        | 45/300 [01:18<07:30,  1.77s/it]Running Inference:  15%|█▌        | 46/300 [01:21<09:29,  2.24s/it]Running Inference:  16%|█▌        | 47/300 [01:22<07:34,  1.80s/it]Running Inference:  16%|█▌        | 48/300 [01:24<08:11,  1.95s/it]Running Inference:  16%|█▋        | 49/300 [01:26<07:44,  1.85s/it]Running Inference:  17%|█▋        | 50/300 [01:28<07:26,  1.78s/it]Running Inference:  17%|█▋        | 51/300 [01:32<10:27,  2.52s/it]Running Inference:  17%|█▋        | 52/300 [01:32<07:55,  1.92s/it]Running Inference:  18%|█▊        | 53/300 [01:33<06:41,  1.63s/it]Running Inference:  18%|█▊        | 54/300 [01:35<06:19,  1.54s/it]Running Inference:  18%|█▊        | 55/300 [01:36<05:57,  1.46s/it]Running Inference:  19%|█▊        | 56/300 [01:37<05:10,  1.27s/it]Running Inference:  19%|█▉        | 57/300 [01:38<05:46,  1.43s/it]Running Inference:  19%|█▉        | 58/300 [01:40<05:49,  1.45s/it]Running Inference:  20%|█▉        | 59/300 [01:44<08:34,  2.14s/it]Running Inference:  20%|██        | 60/300 [01:47<10:05,  2.52s/it]Running Inference:  20%|██        | 61/300 [01:49<09:37,  2.42s/it]Running Inference:  21%|██        | 62/300 [01:50<07:25,  1.87s/it]Running Inference:  21%|██        | 63/300 [01:52<07:21,  1.86s/it]Running Inference:  21%|██▏       | 64/300 [01:53<06:28,  1.65s/it]Running Inference:  22%|██▏       | 65/300 [01:54<05:30,  1.41s/it]Running Inference:  22%|██▏       | 66/300 [01:54<04:28,  1.15s/it]Running Inference:  22%|██▏       | 67/300 [01:56<05:24,  1.39s/it]Running Inference:  23%|██▎       | 68/300 [01:57<05:06,  1.32s/it]Running Inference:  23%|██▎       | 69/300 [02:00<06:26,  1.67s/it]Running Inference:  23%|██▎       | 70/300 [02:03<08:26,  2.20s/it]Running Inference:  24%|██▎       | 71/300 [02:04<07:03,  1.85s/it]Running Inference:  24%|██▍       | 72/300 [02:06<06:26,  1.70s/it]Running Inference:  24%|██▍       | 73/300 [02:06<05:15,  1.39s/it]Running Inference:  25%|██▍       | 74/300 [02:08<06:02,  1.60s/it]Running Inference:  25%|██▌       | 75/300 [02:10<05:48,  1.55s/it]Running Inference:  25%|██▌       | 76/300 [02:11<05:47,  1.55s/it]Running Inference:  26%|██▌       | 77/300 [02:14<06:21,  1.71s/it]Running Inference:  26%|██▌       | 78/300 [02:15<06:33,  1.77s/it]Running Inference:  26%|██▋       | 79/300 [02:16<05:26,  1.48s/it]Running Inference:  27%|██▋       | 80/300 [02:19<07:19,  2.00s/it]Running Inference:  27%|██▋       | 81/300 [02:23<08:50,  2.42s/it]Running Inference:  27%|██▋       | 82/300 [02:25<08:00,  2.20s/it]Running Inference:  28%|██▊       | 83/300 [02:26<06:45,  1.87s/it]Running Inference:  28%|██▊       | 84/300 [02:27<06:04,  1.69s/it]Running Inference:  28%|██▊       | 85/300 [02:29<06:29,  1.81s/it]Running Inference:  29%|██▊       | 86/300 [02:29<04:55,  1.38s/it]Running Inference:  29%|██▉       | 87/300 [02:31<04:43,  1.33s/it]Running Inference:  29%|██▉       | 88/300 [02:34<06:35,  1.86s/it]Running Inference:  30%|██▉       | 89/300 [02:36<06:48,  1.94s/it]Running Inference:  30%|███       | 90/300 [02:36<05:20,  1.52s/it]Running Inference:  30%|███       | 91/300 [02:40<07:11,  2.06s/it]Running Inference:  31%|███       | 92/300 [02:41<06:39,  1.92s/it]Running Inference:  31%|███       | 93/300 [02:43<06:37,  1.92s/it]Running Inference:  31%|███▏      | 94/300 [02:44<05:34,  1.62s/it]Running Inference:  32%|███▏      | 95/300 [02:45<04:40,  1.37s/it]Running Inference:  32%|███▏      | 96/300 [02:47<05:37,  1.66s/it]Running Inference:  32%|███▏      | 97/300 [02:48<04:32,  1.34s/it]Running Inference:  33%|███▎      | 98/300 [02:48<03:39,  1.09s/it]Running Inference:  33%|███▎      | 99/300 [02:49<03:01,  1.10it/s]Running Inference:  33%|███▎      | 100/300 [02:50<03:07,  1.07it/s]Running Inference:  34%|███▎      | 101/300 [02:51<03:07,  1.06it/s]Running Inference:  34%|███▍      | 102/300 [02:55<06:06,  1.85s/it]Running Inference:  34%|███▍      | 103/300 [02:57<06:17,  1.92s/it]Running Inference:  35%|███▍      | 104/300 [02:59<06:37,  2.03s/it]Running Inference:  35%|███▌      | 105/300 [03:01<06:01,  1.85s/it]Running Inference:  35%|███▌      | 106/300 [03:03<06:59,  2.16s/it]Running Inference:  36%|███▌      | 107/300 [03:05<06:21,  1.97s/it]Running Inference:  36%|███▌      | 108/300 [03:06<05:39,  1.77s/it]Running Inference:  36%|███▋      | 109/300 [03:09<06:44,  2.12s/it]Running Inference:  37%|███▋      | 110/300 [03:12<07:19,  2.31s/it]Running Inference:  37%|███▋      | 111/300 [03:14<07:13,  2.29s/it]Running Inference:  37%|███▋      | 112/300 [03:16<07:06,  2.27s/it]Running Inference:  38%|███▊      | 113/300 [03:17<05:41,  1.83s/it]Running Inference:  38%|███▊      | 114/300 [03:19<05:38,  1.82s/it]Running Inference:  38%|███▊      | 115/300 [03:20<05:04,  1.65s/it]Running Inference:  39%|███▊      | 116/300 [03:21<04:41,  1.53s/it]Running Inference:  39%|███▉      | 117/300 [03:22<03:57,  1.30s/it]Running Inference:  39%|███▉      | 118/300 [03:25<05:06,  1.68s/it]Running Inference:  40%|███▉      | 119/300 [03:27<05:32,  1.84s/it]Running Inference:  40%|████      | 120/300 [03:28<04:27,  1.49s/it]Running Inference:  40%|████      | 121/300 [03:31<06:23,  2.15s/it]Running Inference:  41%|████      | 122/300 [03:35<07:40,  2.59s/it]Running Inference:  41%|████      | 123/300 [03:36<06:36,  2.24s/it]Running Inference:  41%|████▏     | 124/300 [03:37<05:25,  1.85s/it]Running Inference:  42%|████▏     | 125/300 [03:39<05:14,  1.80s/it]Running Inference:  42%|████▏     | 126/300 [03:40<04:25,  1.53s/it]Running Inference:  42%|████▏     | 127/300 [03:41<04:08,  1.44s/it]Running Inference:  43%|████▎     | 128/300 [03:41<03:06,  1.09s/it]Running Inference:  43%|████▎     | 129/300 [03:43<03:26,  1.21s/it]Running Inference:  43%|████▎     | 130/300 [03:44<02:56,  1.04s/it]Running Inference:  44%|████▎     | 131/300 [03:44<02:39,  1.06it/s]Running Inference:  44%|████▍     | 132/300 [03:45<02:50,  1.01s/it]Running Inference:  44%|████▍     | 133/300 [03:46<02:37,  1.06it/s]Running Inference:  45%|████▍     | 134/300 [03:50<04:53,  1.77s/it]Running Inference:  45%|████▌     | 135/300 [03:52<05:01,  1.83s/it]Running Inference:  45%|████▌     | 136/300 [03:53<04:15,  1.56s/it]Running Inference:  46%|████▌     | 137/300 [03:54<03:57,  1.45s/it]Running Inference:  46%|████▌     | 138/300 [03:58<06:04,  2.25s/it]Running Inference:  46%|████▋     | 139/300 [04:00<05:55,  2.21s/it]Running Inference:  47%|████▋     | 140/300 [04:02<05:49,  2.19s/it]Running Inference:  47%|████▋     | 141/300 [04:03<04:31,  1.71s/it]Running Inference:  47%|████▋     | 142/300 [04:04<03:44,  1.42s/it]Running Inference:  48%|████▊     | 143/300 [04:06<04:21,  1.67s/it]Running Inference:  48%|████▊     | 144/300 [04:07<04:10,  1.60s/it]Running Inference:  48%|████▊     | 145/300 [04:09<04:25,  1.71s/it]Running Inference:  49%|████▊     | 146/300 [04:11<04:00,  1.56s/it]Running Inference:  49%|████▉     | 147/300 [04:14<05:34,  2.19s/it]Running Inference:  49%|████▉     | 148/300 [04:15<04:28,  1.77s/it]Running Inference:  50%|████▉     | 149/300 [04:16<03:37,  1.44s/it]Running Inference:  50%|█████     | 150/300 [04:19<04:38,  1.86s/it]Running Inference:  50%|█████     | 151/300 [04:21<04:58,  2.00s/it]Running Inference:  51%|█████     | 152/300 [04:24<06:00,  2.43s/it]Running Inference:  51%|█████     | 153/300 [04:29<07:14,  2.96s/it]Running Inference:  51%|█████▏    | 154/300 [04:31<07:04,  2.91s/it]Running Inference:  52%|█████▏    | 155/300 [04:34<06:33,  2.71s/it]Running Inference:  52%|█████▏    | 156/300 [04:35<05:39,  2.35s/it]Running Inference:  52%|█████▏    | 157/300 [04:36<04:28,  1.88s/it]Running Inference:  53%|█████▎    | 158/300 [04:38<04:31,  1.91s/it]Running Inference:  53%|█████▎    | 159/300 [04:39<03:36,  1.54s/it]Running Inference:  53%|█████▎    | 160/300 [04:39<03:02,  1.30s/it]Running Inference:  54%|█████▎    | 161/300 [04:42<03:42,  1.60s/it]Running Inference:  54%|█████▍    | 162/300 [04:43<03:23,  1.47s/it]Running Inference:  54%|█████▍    | 163/300 [04:44<02:57,  1.29s/it]Running Inference:  55%|█████▍    | 164/300 [04:44<02:26,  1.07s/it]Running Inference:  55%|█████▌    | 165/300 [04:45<02:14,  1.01it/s]Running Inference:  55%|█████▌    | 166/300 [04:46<02:13,  1.01it/s]Running Inference:  56%|█████▌    | 167/300 [04:49<03:47,  1.71s/it]Running Inference:  56%|█████▌    | 168/300 [04:51<03:57,  1.80s/it]Running Inference:  56%|█████▋    | 169/300 [04:54<04:20,  1.99s/it]Running Inference:  57%|█████▋    | 170/300 [04:55<03:53,  1.80s/it]Running Inference:  57%|█████▋    | 171/300 [04:56<03:24,  1.58s/it]Running Inference:  57%|█████▋    | 172/300 [04:57<03:06,  1.46s/it]Running Inference:  58%|█████▊    | 173/300 [05:00<03:52,  1.83s/it]Running Inference:  58%|█████▊    | 174/300 [05:03<04:30,  2.14s/it]Running Inference:  58%|█████▊    | 175/300 [05:05<04:06,  1.98s/it]Running Inference:  59%|█████▊    | 176/300 [05:06<03:29,  1.69s/it]Running Inference:  59%|█████▉    | 177/300 [05:07<03:01,  1.48s/it]Running Inference:  59%|█████▉    | 178/300 [05:09<03:21,  1.65s/it]Running Inference:  60%|█████▉    | 179/300 [05:11<03:50,  1.91s/it]Running Inference:  60%|██████    | 180/300 [05:12<03:27,  1.73s/it]Running Inference:  60%|██████    | 181/300 [05:17<04:50,  2.44s/it]Running Inference:  61%|██████    | 182/300 [05:18<04:11,  2.14s/it]Running Inference:  61%|██████    | 183/300 [05:22<05:20,  2.74s/it]Running Inference:  61%|██████▏   | 184/300 [05:24<05:00,  2.59s/it]Running Inference:  62%|██████▏   | 185/300 [05:27<04:53,  2.56s/it]Running Inference:  62%|██████▏   | 186/300 [05:29<04:39,  2.46s/it]Running Inference:  62%|██████▏   | 187/300 [05:31<04:22,  2.32s/it]Running Inference:  63%|██████▎   | 188/300 [05:35<05:04,  2.72s/it]Running Inference:  63%|██████▎   | 189/300 [05:37<04:46,  2.58s/it]Running Inference:  63%|██████▎   | 190/300 [05:40<05:10,  2.82s/it]Running Inference:  64%|██████▎   | 191/300 [05:45<05:53,  3.24s/it]Running Inference:  64%|██████▍   | 192/300 [05:50<06:47,  3.77s/it]Running Inference:  64%|██████▍   | 193/300 [05:52<05:54,  3.31s/it]Running Inference:  65%|██████▍   | 194/300 [05:55<05:42,  3.23s/it]Running Inference:  65%|██████▌   | 195/300 [05:56<04:19,  2.47s/it]Running Inference:  65%|██████▌   | 196/300 [05:56<03:25,  1.97s/it]Running Inference:  66%|██████▌   | 197/300 [05:59<03:36,  2.10s/it]Running Inference:  66%|██████▌   | 198/300 [06:02<04:07,  2.42s/it]Running Inference:  66%|██████▋   | 199/300 [06:04<03:50,  2.28s/it]Running Inference:  67%|██████▋   | 200/300 [06:05<03:15,  1.96s/it]Running Inference:  67%|██████▋   | 201/300 [06:06<02:39,  1.61s/it]Running Inference:  67%|██████▋   | 202/300 [06:07<02:15,  1.39s/it]Running Inference:  68%|██████▊   | 203/300 [06:09<02:39,  1.65s/it]Running Inference:  68%|██████▊   | 204/300 [06:11<03:02,  1.90s/it]Running Inference:  68%|██████▊   | 205/300 [06:16<04:02,  2.55s/it]Running Inference:  69%|██████▊   | 206/300 [06:19<04:23,  2.80s/it]Running Inference:  69%|██████▉   | 207/300 [06:20<03:26,  2.22s/it]Running Inference:  69%|██████▉   | 208/300 [06:22<03:22,  2.20s/it]Running Inference:  70%|██████▉   | 209/300 [06:24<03:23,  2.23s/it]Running Inference:  70%|███████   | 210/300 [06:27<03:33,  2.37s/it]Running Inference:  70%|███████   | 211/300 [06:30<03:38,  2.45s/it]Running Inference:  71%|███████   | 212/300 [06:30<02:47,  1.91s/it]Running Inference:  71%|███████   | 213/300 [06:31<02:28,  1.70s/it]Running Inference:  71%|███████▏  | 214/300 [06:32<02:06,  1.47s/it]Running Inference:  72%|███████▏  | 215/300 [06:34<02:16,  1.60s/it]Running Inference:  72%|███████▏  | 216/300 [06:35<01:55,  1.38s/it]Running Inference:  72%|███████▏  | 217/300 [06:36<01:45,  1.27s/it]Running Inference:  73%|███████▎  | 218/300 [06:37<01:43,  1.26s/it]Running Inference:  73%|███████▎  | 219/300 [06:38<01:37,  1.21s/it]Running Inference:  73%|███████▎  | 220/300 [06:41<01:56,  1.45s/it]Running Inference:  74%|███████▎  | 221/300 [06:41<01:31,  1.15s/it]Running Inference:  74%|███████▍  | 222/300 [06:43<01:56,  1.50s/it]Running Inference:  74%|███████▍  | 223/300 [06:44<01:39,  1.29s/it]Running Inference:  75%|███████▍  | 224/300 [06:46<01:46,  1.40s/it]Running Inference:  75%|███████▌  | 225/300 [06:48<01:55,  1.54s/it]Running Inference:  75%|███████▌  | 226/300 [06:50<02:04,  1.68s/it]Running Inference:  76%|███████▌  | 227/300 [06:51<01:57,  1.60s/it]Running Inference:  76%|███████▌  | 228/300 [06:52<01:51,  1.55s/it]Running Inference:  76%|███████▋  | 229/300 [06:53<01:27,  1.23s/it]Running Inference:  77%|███████▋  | 230/300 [06:55<01:36,  1.38s/it]Running Inference:  77%|███████▋  | 231/300 [06:57<01:46,  1.55s/it]Running Inference:  77%|███████▋  | 232/300 [07:00<02:22,  2.09s/it]Running Inference:  78%|███████▊  | 233/300 [07:02<02:16,  2.04s/it]Running Inference:  78%|███████▊  | 234/300 [07:03<02:05,  1.91s/it]Running Inference:  78%|███████▊  | 235/300 [07:05<01:48,  1.68s/it]Running Inference:  79%|███████▊  | 236/300 [07:06<01:39,  1.55s/it]Running Inference:  79%|███████▉  | 237/300 [07:07<01:28,  1.40s/it]Running Inference:  79%|███████▉  | 238/300 [07:11<02:14,  2.17s/it]Running Inference:  80%|███████▉  | 239/300 [07:13<02:03,  2.03s/it]Running Inference:  80%|████████  | 240/300 [07:14<01:43,  1.72s/it]Running Inference:  80%|████████  | 241/300 [07:18<02:21,  2.39s/it]Running Inference:  81%|████████  | 242/300 [07:19<02:05,  2.16s/it]Running Inference:  81%|████████  | 243/300 [07:20<01:35,  1.67s/it]Running Inference:  81%|████████▏ | 244/300 [07:21<01:27,  1.56s/it]Running Inference:  82%|████████▏ | 245/300 [07:24<01:56,  2.11s/it]Running Inference:  82%|████████▏ | 246/300 [07:25<01:30,  1.68s/it]Running Inference:  82%|████████▏ | 247/300 [07:26<01:24,  1.60s/it]Running Inference:  83%|████████▎ | 248/300 [07:27<01:11,  1.37s/it]Running Inference:  83%|████████▎ | 249/300 [07:28<01:02,  1.23s/it]Running Inference:  83%|████████▎ | 250/300 [07:30<01:05,  1.32s/it]Running Inference:  84%|████████▎ | 251/300 [07:32<01:14,  1.52s/it]Running Inference:  84%|████████▍ | 252/300 [07:34<01:22,  1.71s/it]Running Inference:  84%|████████▍ | 253/300 [07:36<01:23,  1.77s/it]Running Inference:  85%|████████▍ | 254/300 [07:38<01:24,  1.83s/it]Running Inference:  85%|████████▌ | 255/300 [07:41<01:43,  2.31s/it]Running Inference:  85%|████████▌ | 256/300 [07:45<02:07,  2.91s/it]Running Inference:  86%|████████▌ | 257/300 [07:46<01:36,  2.24s/it]Running Inference:  86%|████████▌ | 258/300 [07:48<01:33,  2.23s/it]Running Inference:  86%|████████▋ | 259/300 [07:49<01:07,  1.65s/it]Running Inference:  87%|████████▋ | 260/300 [07:50<01:00,  1.51s/it]Running Inference:  87%|████████▋ | 261/300 [07:53<01:22,  2.11s/it]Running Inference:  87%|████████▋ | 262/300 [07:55<01:09,  1.84s/it]Running Inference:  88%|████████▊ | 263/300 [07:57<01:11,  1.92s/it]Running Inference:  88%|████████▊ | 264/300 [07:59<01:08,  1.90s/it]Running Inference:  88%|████████▊ | 265/300 [08:01<01:10,  2.00s/it]Running Inference:  89%|████████▊ | 266/300 [08:02<00:55,  1.63s/it]Running Inference:  89%|████████▉ | 267/300 [08:03<00:47,  1.44s/it]Running Inference:  89%|████████▉ | 268/300 [08:03<00:40,  1.27s/it]Running Inference:  90%|████████▉ | 269/300 [08:04<00:33,  1.09s/it]Running Inference:  90%|█████████ | 270/300 [08:05<00:31,  1.05s/it]Running Inference:  90%|█████████ | 271/300 [08:06<00:28,  1.03it/s]Running Inference:  91%|█████████ | 272/300 [08:07<00:25,  1.09it/s]Running Inference:  91%|█████████ | 273/300 [08:11<00:50,  1.86s/it]Running Inference:  91%|█████████▏| 274/300 [08:12<00:45,  1.74s/it]Running Inference:  92%|█████████▏| 275/300 [08:14<00:41,  1.65s/it]Running Inference:  92%|█████████▏| 276/300 [08:17<00:49,  2.07s/it]Running Inference:  92%|█████████▏| 277/300 [08:19<00:49,  2.17s/it]Running Inference:  93%|█████████▎| 278/300 [08:20<00:37,  1.70s/it]Running Inference:  93%|█████████▎| 279/300 [08:21<00:35,  1.70s/it]Running Inference:  93%|█████████▎| 280/300 [08:23<00:33,  1.65s/it]Running Inference:  94%|█████████▎| 281/300 [08:25<00:33,  1.77s/it]Running Inference:  94%|█████████▍| 282/300 [08:29<00:45,  2.52s/it]Running Inference:  94%|█████████▍| 283/300 [08:33<00:51,  3.04s/it]Running Inference:  95%|█████████▍| 284/300 [08:36<00:44,  2.80s/it]Running Inference:  95%|█████████▌| 285/300 [08:37<00:33,  2.26s/it]Running Inference:  95%|█████████▌| 286/300 [08:37<00:25,  1.80s/it]Running Inference:  96%|█████████▌| 287/300 [08:39<00:21,  1.65s/it]Running Inference:  96%|█████████▌| 288/300 [08:40<00:19,  1.66s/it]Running Inference:  96%|█████████▋| 289/300 [08:41<00:14,  1.31s/it]Running Inference:  97%|█████████▋| 290/300 [08:42<00:13,  1.39s/it]Running Inference:  97%|█████████▋| 291/300 [08:44<00:12,  1.43s/it]Running Inference:  97%|█████████▋| 292/300 [08:45<00:10,  1.28s/it]Running Inference:  98%|█████████▊| 293/300 [08:46<00:09,  1.36s/it]Running Inference:  98%|█████████▊| 294/300 [08:48<00:08,  1.38s/it]Running Inference:  98%|█████████▊| 295/300 [08:50<00:08,  1.65s/it]Running Inference:  99%|█████████▊| 296/300 [08:52<00:07,  1.83s/it]Running Inference:  99%|█████████▉| 297/300 [08:54<00:05,  1.81s/it]Running Inference:  99%|█████████▉| 298/300 [08:57<00:03,  1.97s/it]Running Inference: 100%|█████████▉| 299/300 [08:57<00:01,  1.54s/it]Running Inference: 100%|██████████| 300/300 [08:59<00:00,  1.66s/it]Running Inference: 100%|██████████| 300/300 [08:59<00:00,  1.80s/it]
2025-12-15 09:42:05,560 - INFO - Inference completed.
2025-12-15 09:42:05,570 - INFO - Results saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-15 09:42:05,570 - INFO - Calculating metrics for dataset: longbench
2025-12-15 09:42:05,575 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa_e__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-15 09:42:05,575 - INFO - Metrics:
41.67
2025-12-15 09:42:05,577 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=hotpotqa_e, ratio=0.5) on GPU 1


========================================
LongBench Task: lcc_e
========================================
----------------------------------------
Task: lcc_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 09:42:11,943 - INFO - Set deterministic seeds to 42
2025-12-15 09:42:11,943 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 09:42:11,943 - INFO - Starting evaluation run...
2025-12-15 09:42:11,943 - INFO - Output directory set to: longbenchresult
2025-12-15 09:42:11,943 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-15 09:42:11,943 - INFO - KV Press 'tova' setup.
2025-12-15 09:42:11,943 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 09:42:11,943 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.31it/s]
Device set to use cuda:0
2025-12-15 09:42:22,618 - INFO - Model pipeline loaded.
2025-12-15 09:42:22,618 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc_e)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 300 examples [00:00, 1958.19 examples/s]Generating test split: 300 examples [00:00, 1932.62 examples/s]
2025-12-15 09:42:25,937 - INFO - Dataset loaded with 300 entries.
2025-12-15 09:42:25,937 - INFO - Dataset processed with 300 entries.
2025-12-15 09:42:25,970 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:07<35:06,  7.04s/it]Running Inference:   1%|          | 2/300 [00:12<29:09,  5.87s/it]Running Inference:   1%|          | 3/300 [00:16<25:31,  5.16s/it]Running Inference:   1%|▏         | 4/300 [00:18<20:07,  4.08s/it]Running Inference:   2%|▏         | 5/300 [00:23<21:28,  4.37s/it]Running Inference:   2%|▏         | 6/300 [00:27<20:20,  4.15s/it]Running Inference:   2%|▏         | 7/300 [00:32<21:04,  4.31s/it]Running Inference:   3%|▎         | 8/300 [00:38<23:49,  4.90s/it]Running Inference:   3%|▎         | 9/300 [00:41<21:53,  4.51s/it]Running Inference:   3%|▎         | 10/300 [00:47<23:15,  4.81s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [01:00<35:16,  7.33s/it]Running Inference:   4%|▍         | 12/300 [01:04<30:16,  6.31s/it]Running Inference:   4%|▍         | 13/300 [01:10<29:42,  6.21s/it]Running Inference:   5%|▍         | 14/300 [01:14<26:13,  5.50s/it]Running Inference:   5%|▌         | 15/300 [01:20<26:31,  5.58s/it]Running Inference:   5%|▌         | 16/300 [01:23<23:47,  5.03s/it]Running Inference:   6%|▌         | 17/300 [01:29<24:39,  5.23s/it]Running Inference:   6%|▌         | 18/300 [01:33<22:40,  4.82s/it]Running Inference:   6%|▋         | 19/300 [01:37<21:04,  4.50s/it]Running Inference:   7%|▋         | 20/300 [01:43<23:22,  5.01s/it]Running Inference:   7%|▋         | 21/300 [01:47<21:32,  4.63s/it]Running Inference:   7%|▋         | 22/300 [01:55<26:26,  5.71s/it]Running Inference:   8%|▊         | 23/300 [02:00<25:32,  5.53s/it]Running Inference:   8%|▊         | 24/300 [02:06<26:35,  5.78s/it]Running Inference:   8%|▊         | 25/300 [02:13<28:06,  6.13s/it]Running Inference:   9%|▊         | 26/300 [02:17<25:13,  5.52s/it]Running Inference:   9%|▉         | 27/300 [02:23<25:30,  5.61s/it]Running Inference:   9%|▉         | 28/300 [02:27<22:53,  5.05s/it]Running Inference:  10%|▉         | 29/300 [02:31<21:02,  4.66s/it]Running Inference:  10%|█         | 30/300 [02:38<25:04,  5.57s/it]Running Inference:  10%|█         | 31/300 [02:46<27:19,  6.10s/it]Running Inference:  11%|█         | 32/300 [02:52<28:09,  6.30s/it]Running Inference:  11%|█         | 33/300 [02:56<24:35,  5.53s/it]Running Inference:  11%|█▏        | 34/300 [03:02<24:54,  5.62s/it]Running Inference:  12%|█▏        | 35/300 [03:07<23:53,  5.41s/it]Running Inference:  12%|█▏        | 36/300 [03:12<23:08,  5.26s/it]Running Inference:  12%|█▏        | 37/300 [03:16<21:07,  4.82s/it]Running Inference:  13%|█▎        | 38/300 [03:19<19:52,  4.55s/it]Running Inference:  13%|█▎        | 39/300 [03:26<21:52,  5.03s/it]Running Inference:  13%|█▎        | 40/300 [03:31<22:22,  5.16s/it]Running Inference:  14%|█▎        | 41/300 [03:36<21:53,  5.07s/it]Running Inference:  14%|█▍        | 42/300 [03:41<21:26,  4.99s/it]Running Inference:  14%|█▍        | 43/300 [03:48<23:42,  5.53s/it]Running Inference:  15%|█▍        | 44/300 [03:53<23:00,  5.39s/it]Running Inference:  15%|█▌        | 45/300 [03:56<20:49,  4.90s/it]Running Inference:  15%|█▌        | 46/300 [04:01<20:48,  4.92s/it]Running Inference:  16%|█▌        | 47/300 [04:08<22:35,  5.36s/it]Running Inference:  16%|█▌        | 48/300 [04:14<23:35,  5.62s/it]Running Inference:  16%|█▋        | 49/300 [04:23<27:50,  6.65s/it]Running Inference:  17%|█▋        | 50/300 [04:29<27:26,  6.59s/it]Running Inference:  17%|█▋        | 51/300 [04:37<28:33,  6.88s/it]Running Inference:  17%|█▋        | 52/300 [04:41<24:41,  5.97s/it]Running Inference:  18%|█▊        | 53/300 [04:45<22:00,  5.35s/it]Running Inference:  18%|█▊        | 54/300 [04:51<23:38,  5.77s/it]Running Inference:  18%|█▊        | 55/300 [04:55<21:06,  5.17s/it]Running Inference:  19%|█▊        | 56/300 [05:00<20:51,  5.13s/it]Running Inference:  19%|█▉        | 57/300 [05:07<22:49,  5.64s/it]Running Inference:  19%|█▉        | 58/300 [05:14<24:10,  5.99s/it]Running Inference:  20%|█▉        | 59/300 [05:19<22:46,  5.67s/it]Running Inference:  20%|██        | 60/300 [05:27<25:53,  6.47s/it]Running Inference:  20%|██        | 61/300 [05:33<24:34,  6.17s/it]Running Inference:  21%|██        | 62/300 [05:41<27:29,  6.93s/it]Running Inference:  21%|██        | 63/300 [05:45<23:38,  5.98s/it]Running Inference:  21%|██▏       | 64/300 [05:51<23:55,  6.08s/it]Running Inference:  22%|██▏       | 65/300 [05:56<22:05,  5.64s/it]Running Inference:  22%|██▏       | 66/300 [06:00<19:46,  5.07s/it]Running Inference:  22%|██▏       | 67/300 [06:09<24:08,  6.22s/it]Running Inference:  23%|██▎       | 68/300 [06:14<23:10,  6.00s/it]Running Inference:  23%|██▎       | 69/300 [06:19<21:53,  5.69s/it]Running Inference:  23%|██▎       | 70/300 [06:26<23:42,  6.19s/it]Running Inference:  24%|██▎       | 71/300 [06:32<23:12,  6.08s/it]Running Inference:  24%|██▍       | 72/300 [06:37<21:23,  5.63s/it]Running Inference:  24%|██▍       | 73/300 [06:41<19:10,  5.07s/it]Running Inference:  25%|██▍       | 74/300 [06:47<20:19,  5.39s/it]Running Inference:  25%|██▌       | 75/300 [06:53<20:59,  5.60s/it]Running Inference:  25%|██▌       | 76/300 [07:04<26:55,  7.21s/it]Running Inference:  26%|██▌       | 77/300 [07:11<26:19,  7.08s/it]Running Inference:  26%|██▌       | 78/300 [07:17<25:11,  6.81s/it]Running Inference:  26%|██▋       | 79/300 [07:23<24:43,  6.71s/it]Running Inference:  27%|██▋       | 80/300 [07:27<21:20,  5.82s/it]Running Inference:  27%|██▋       | 81/300 [07:31<18:58,  5.20s/it]Running Inference:  27%|██▋       | 82/300 [07:35<17:32,  4.83s/it]Running Inference:  28%|██▊       | 83/300 [07:41<19:19,  5.34s/it]Running Inference:  28%|██▊       | 84/300 [07:45<17:50,  4.96s/it]Running Inference:  28%|██▊       | 85/300 [07:52<19:27,  5.43s/it]Running Inference:  29%|██▊       | 86/300 [07:59<21:10,  5.94s/it]Running Inference:  29%|██▉       | 87/300 [08:05<21:31,  6.07s/it]Running Inference:  29%|██▉       | 88/300 [08:09<19:17,  5.46s/it]Running Inference:  30%|██▉       | 89/300 [08:13<17:28,  4.97s/it]Running Inference:  30%|███       | 90/300 [08:17<16:09,  4.62s/it]Running Inference:  30%|███       | 91/300 [08:23<17:54,  5.14s/it]Running Inference:  31%|███       | 92/300 [08:29<18:32,  5.35s/it]Running Inference:  31%|███       | 93/300 [08:37<20:50,  6.04s/it]Running Inference:  31%|███▏      | 94/300 [08:46<23:24,  6.82s/it]Running Inference:  32%|███▏      | 95/300 [08:53<24:12,  7.09s/it]Running Inference:  32%|███▏      | 96/300 [08:57<20:41,  6.09s/it]Running Inference:  32%|███▏      | 97/300 [09:04<21:53,  6.47s/it]Running Inference:  33%|███▎      | 98/300 [09:09<19:58,  5.93s/it]Running Inference:  33%|███▎      | 99/300 [09:15<20:01,  5.98s/it]Running Inference:  33%|███▎      | 100/300 [09:21<19:47,  5.94s/it]Running Inference:  34%|███▎      | 101/300 [09:25<17:30,  5.28s/it]Running Inference:  34%|███▍      | 102/300 [09:28<15:50,  4.80s/it]Running Inference:  34%|███▍      | 103/300 [09:34<16:12,  4.94s/it]Running Inference:  35%|███▍      | 104/300 [09:41<18:10,  5.56s/it]Running Inference:  35%|███▌      | 105/300 [09:44<16:20,  5.03s/it]Running Inference:  35%|███▌      | 106/300 [09:45<12:21,  3.82s/it]Running Inference:  36%|███▌      | 107/300 [09:50<12:58,  4.03s/it]Running Inference:  36%|███▌      | 108/300 [09:54<12:35,  3.94s/it]Running Inference:  36%|███▋      | 109/300 [09:59<13:51,  4.35s/it]Running Inference:  37%|███▋      | 110/300 [10:05<15:12,  4.80s/it]Running Inference:  37%|███▋      | 111/300 [10:10<15:33,  4.94s/it]Running Inference:  37%|███▋      | 112/300 [10:14<14:21,  4.58s/it]Running Inference:  38%|███▊      | 113/300 [10:20<15:18,  4.91s/it]Running Inference:  38%|███▊      | 114/300 [10:23<14:06,  4.55s/it]Running Inference:  38%|███▊      | 115/300 [10:30<15:36,  5.06s/it]Running Inference:  39%|███▊      | 116/300 [10:33<14:20,  4.67s/it]Running Inference:  39%|███▉      | 117/300 [10:38<14:37,  4.80s/it]Running Inference:  39%|███▉      | 118/300 [10:42<13:43,  4.52s/it]Running Inference:  40%|███▉      | 119/300 [10:46<12:56,  4.29s/it]Running Inference:  40%|████      | 120/300 [10:50<12:18,  4.11s/it]Running Inference:  40%|████      | 121/300 [10:54<12:32,  4.20s/it]Running Inference:  41%|████      | 122/300 [10:59<13:26,  4.53s/it]Running Inference:  41%|████      | 123/300 [11:05<14:25,  4.89s/it]Running Inference:  41%|████▏     | 124/300 [11:12<15:57,  5.44s/it]Running Inference:  42%|████▏     | 125/300 [11:18<16:43,  5.73s/it]Running Inference:  42%|████▏     | 126/300 [11:26<18:31,  6.39s/it]Running Inference:  42%|████▏     | 127/300 [11:31<16:42,  5.79s/it]Running Inference:  43%|████▎     | 128/300 [11:36<16:32,  5.77s/it]Running Inference:  43%|████▎     | 129/300 [11:41<15:17,  5.37s/it]Running Inference:  43%|████▎     | 130/300 [11:46<15:09,  5.35s/it]Running Inference:  44%|████▎     | 131/300 [11:51<14:38,  5.20s/it]Running Inference:  44%|████▍     | 132/300 [11:58<15:56,  5.69s/it]Running Inference:  44%|████▍     | 133/300 [12:03<15:07,  5.44s/it]Running Inference:  45%|████▍     | 134/300 [12:07<14:23,  5.20s/it]Running Inference:  45%|████▌     | 135/300 [12:08<10:34,  3.85s/it]Running Inference:  45%|████▌     | 136/300 [12:14<12:10,  4.45s/it]Running Inference:  46%|████▌     | 137/300 [12:17<11:27,  4.22s/it]Running Inference:  46%|████▌     | 138/300 [12:23<12:26,  4.61s/it]Running Inference:  46%|████▋     | 139/300 [12:27<12:04,  4.50s/it]Running Inference:  47%|████▋     | 140/300 [12:32<12:02,  4.52s/it]Running Inference:  47%|████▋     | 141/300 [12:38<12:56,  4.88s/it]Running Inference:  47%|████▋     | 142/300 [12:44<14:22,  5.46s/it]Running Inference:  48%|████▊     | 143/300 [12:48<13:03,  4.99s/it]Running Inference:  48%|████▊     | 144/300 [12:55<14:35,  5.61s/it]Running Inference:  48%|████▊     | 145/300 [13:00<14:06,  5.46s/it]Running Inference:  49%|████▊     | 146/300 [13:04<12:46,  4.98s/it]Running Inference:  49%|████▉     | 147/300 [13:10<13:16,  5.21s/it]Running Inference:  49%|████▉     | 148/300 [13:15<13:05,  5.17s/it]Running Inference:  50%|████▉     | 149/300 [13:19<12:04,  4.80s/it]Running Inference:  50%|█████     | 150/300 [13:23<11:10,  4.47s/it]Running Inference:  50%|█████     | 151/300 [13:28<11:44,  4.73s/it]Running Inference:  51%|█████     | 152/300 [13:34<12:12,  4.95s/it]Running Inference:  51%|█████     | 153/300 [13:39<12:13,  4.99s/it]Running Inference:  51%|█████▏    | 154/300 [13:43<11:25,  4.70s/it]Running Inference:  52%|█████▏    | 155/300 [13:49<12:20,  5.11s/it]Running Inference:  52%|█████▏    | 156/300 [13:54<12:12,  5.09s/it]Running Inference:  52%|█████▏    | 157/300 [13:55<09:03,  3.80s/it]Running Inference:  53%|█████▎    | 158/300 [14:10<17:08,  7.25s/it]Running Inference:  53%|█████▎    | 159/300 [14:14<15:07,  6.43s/it]Running Inference:  53%|█████▎    | 160/300 [14:24<17:08,  7.35s/it]Running Inference:  54%|█████▎    | 161/300 [14:30<16:13,  7.00s/it]Running Inference:  54%|█████▍    | 162/300 [14:39<17:38,  7.67s/it]Running Inference:  54%|█████▍    | 163/300 [14:45<16:14,  7.12s/it]Running Inference:  55%|█████▍    | 164/300 [14:51<15:15,  6.74s/it]Running Inference:  55%|█████▌    | 165/300 [15:00<16:49,  7.47s/it]Running Inference:  55%|█████▌    | 166/300 [15:05<14:52,  6.66s/it]Running Inference:  56%|█████▌    | 167/300 [15:09<12:49,  5.79s/it]Running Inference:  56%|█████▌    | 168/300 [15:15<13:21,  6.07s/it]Running Inference:  56%|█████▋    | 169/300 [15:19<11:45,  5.39s/it]Running Inference:  57%|█████▋    | 170/300 [15:25<12:03,  5.57s/it]Running Inference:  57%|█████▋    | 171/300 [15:30<11:16,  5.24s/it]Running Inference:  57%|█████▋    | 172/300 [15:33<10:18,  4.83s/it]Running Inference:  58%|█████▊    | 173/300 [15:40<11:20,  5.36s/it]Running Inference:  58%|█████▊    | 174/300 [15:45<10:59,  5.24s/it]Running Inference:  58%|█████▊    | 175/300 [15:50<10:44,  5.15s/it]Running Inference:  59%|█████▊    | 176/300 [15:54<09:45,  4.72s/it]Running Inference:  59%|█████▉    | 177/300 [15:58<09:09,  4.47s/it]Running Inference:  59%|█████▉    | 178/300 [16:02<08:53,  4.37s/it]Running Inference:  60%|█████▉    | 179/300 [16:09<10:34,  5.24s/it]Running Inference:  60%|██████    | 180/300 [16:14<10:11,  5.09s/it]Running Inference:  60%|██████    | 181/300 [16:25<13:54,  7.01s/it]Running Inference:  61%|██████    | 182/300 [16:34<14:33,  7.40s/it]Running Inference:  61%|██████    | 183/300 [16:37<12:18,  6.31s/it]Running Inference:  61%|██████▏   | 184/300 [16:45<13:10,  6.81s/it]Running Inference:  62%|██████▏   | 185/300 [16:49<11:19,  5.91s/it]Running Inference:  62%|██████▏   | 186/300 [16:55<11:27,  6.03s/it]Running Inference:  62%|██████▏   | 187/300 [16:59<10:08,  5.38s/it]Running Inference:  63%|██████▎   | 188/300 [17:04<09:51,  5.28s/it]Running Inference:  63%|██████▎   | 189/300 [17:10<10:12,  5.52s/it]Running Inference:  63%|██████▎   | 190/300 [17:17<10:51,  5.92s/it]Running Inference:  64%|██████▎   | 191/300 [17:24<11:27,  6.31s/it]Running Inference:  64%|██████▍   | 192/300 [17:30<10:59,  6.10s/it]Running Inference:  64%|██████▍   | 193/300 [17:34<09:43,  5.45s/it]Running Inference:  65%|██████▍   | 194/300 [17:39<09:31,  5.40s/it]Running Inference:  65%|██████▌   | 195/300 [17:45<09:35,  5.49s/it]Running Inference:  65%|██████▌   | 196/300 [17:50<09:05,  5.25s/it]Running Inference:  66%|██████▌   | 197/300 [17:54<08:17,  4.83s/it]Running Inference:  66%|██████▌   | 198/300 [17:58<08:12,  4.83s/it]Running Inference:  66%|██████▋   | 199/300 [18:05<08:49,  5.24s/it]Running Inference:  67%|██████▋   | 200/300 [18:10<08:49,  5.29s/it]Running Inference:  67%|██████▋   | 201/300 [18:16<09:11,  5.57s/it]Running Inference:  67%|██████▋   | 202/300 [18:20<08:11,  5.02s/it]Running Inference:  68%|██████▊   | 203/300 [18:21<06:12,  3.84s/it]Running Inference:  68%|██████▊   | 204/300 [18:26<06:47,  4.25s/it]Running Inference:  68%|██████▊   | 205/300 [18:31<07:12,  4.55s/it]Running Inference:  69%|██████▊   | 206/300 [18:38<07:58,  5.09s/it]Running Inference:  69%|██████▉   | 207/300 [18:42<07:39,  4.94s/it]Running Inference:  69%|██████▉   | 208/300 [18:47<07:37,  4.97s/it]Running Inference:  70%|██████▉   | 209/300 [18:54<08:05,  5.33s/it]Running Inference:  70%|███████   | 210/300 [19:00<08:38,  5.76s/it]Running Inference:  70%|███████   | 211/300 [19:07<08:44,  5.89s/it]Running Inference:  71%|███████   | 212/300 [19:13<08:45,  5.97s/it]Running Inference:  71%|███████   | 213/300 [19:21<09:46,  6.74s/it]Running Inference:  71%|███████▏  | 214/300 [19:26<08:54,  6.21s/it]Running Inference:  72%|███████▏  | 215/300 [19:32<08:24,  5.94s/it]Running Inference:  72%|███████▏  | 216/300 [19:38<08:24,  6.01s/it]Running Inference:  72%|███████▏  | 217/300 [19:42<07:25,  5.36s/it]Running Inference:  73%|███████▎  | 218/300 [19:45<06:40,  4.88s/it]Running Inference:  73%|███████▎  | 219/300 [19:52<07:11,  5.32s/it]Running Inference:  73%|███████▎  | 220/300 [19:59<07:41,  5.77s/it]Running Inference:  74%|███████▎  | 221/300 [20:05<07:49,  5.95s/it]Running Inference:  74%|███████▍  | 222/300 [20:10<07:16,  5.59s/it]Running Inference:  74%|███████▍  | 223/300 [20:14<06:32,  5.10s/it]Running Inference:  75%|███████▍  | 224/300 [20:19<06:44,  5.33s/it]Running Inference:  75%|███████▌  | 225/300 [20:23<06:01,  4.82s/it]Running Inference:  75%|███████▌  | 226/300 [20:30<06:42,  5.43s/it]Running Inference:  76%|███████▌  | 227/300 [20:34<05:59,  4.93s/it]Running Inference:  76%|███████▌  | 228/300 [20:38<05:51,  4.88s/it]Running Inference:  76%|███████▋  | 229/300 [20:45<06:11,  5.23s/it]Running Inference:  77%|███████▋  | 230/300 [20:49<05:50,  5.01s/it]Running Inference:  77%|███████▋  | 231/300 [20:56<06:20,  5.52s/it]Running Inference:  77%|███████▋  | 232/300 [21:02<06:39,  5.87s/it]Running Inference:  78%|███████▊  | 233/300 [21:07<06:08,  5.50s/it]Running Inference:  78%|███████▊  | 234/300 [21:11<05:28,  4.98s/it]Running Inference:  78%|███████▊  | 235/300 [21:15<04:59,  4.60s/it]Running Inference:  79%|███████▊  | 236/300 [21:18<04:41,  4.39s/it]Running Inference:  79%|███████▉  | 237/300 [21:31<07:14,  6.89s/it]Running Inference:  79%|███████▉  | 238/300 [21:35<06:09,  5.96s/it]Running Inference:  80%|███████▉  | 239/300 [21:36<04:28,  4.40s/it]Running Inference:  80%|████████  | 240/300 [21:40<04:15,  4.26s/it]Running Inference:  80%|████████  | 241/300 [21:43<04:03,  4.13s/it]Running Inference:  81%|████████  | 242/300 [21:48<04:11,  4.33s/it]Running Inference:  81%|████████  | 243/300 [21:54<04:23,  4.61s/it]Running Inference:  81%|████████▏ | 244/300 [21:57<04:03,  4.35s/it]Running Inference:  82%|████████▏ | 245/300 [22:01<03:48,  4.15s/it]Running Inference:  82%|████████▏ | 246/300 [22:08<04:34,  5.09s/it]Running Inference:  82%|████████▏ | 247/300 [22:15<04:57,  5.61s/it]Running Inference:  83%|████████▎ | 248/300 [22:21<04:53,  5.65s/it]Running Inference:  83%|████████▎ | 249/300 [22:25<04:23,  5.17s/it]Running Inference:  83%|████████▎ | 250/300 [22:29<03:59,  4.79s/it]Running Inference:  84%|████████▎ | 251/300 [22:35<04:15,  5.21s/it]Running Inference:  84%|████████▍ | 252/300 [22:39<03:49,  4.78s/it]Running Inference:  84%|████████▍ | 253/300 [22:41<03:03,  3.90s/it]Running Inference:  85%|████████▍ | 254/300 [22:44<02:59,  3.89s/it]Running Inference:  85%|████████▌ | 255/300 [22:48<02:52,  3.84s/it]Running Inference:  85%|████████▌ | 256/300 [22:53<03:04,  4.20s/it]Running Inference:  86%|████████▌ | 257/300 [23:00<03:27,  4.82s/it]Running Inference:  86%|████████▌ | 258/300 [23:05<03:30,  5.02s/it]Running Inference:  86%|████████▋ | 259/300 [23:09<03:10,  4.64s/it]Running Inference:  87%|████████▋ | 260/300 [23:13<02:55,  4.39s/it]Running Inference:  87%|████████▋ | 261/300 [23:17<02:48,  4.32s/it]Running Inference:  87%|████████▋ | 262/300 [23:21<02:39,  4.20s/it]Running Inference:  88%|████████▊ | 263/300 [23:24<02:30,  4.06s/it]Running Inference:  88%|████████▊ | 264/300 [23:29<02:35,  4.31s/it]Running Inference:  88%|████████▊ | 265/300 [23:33<02:27,  4.20s/it]Running Inference:  89%|████████▊ | 266/300 [23:39<02:43,  4.80s/it]Running Inference:  89%|████████▉ | 267/300 [23:46<02:53,  5.27s/it]Running Inference:  89%|████████▉ | 268/300 [23:51<02:45,  5.18s/it]Running Inference:  90%|████████▉ | 269/300 [23:58<02:58,  5.75s/it]Running Inference:  90%|█████████ | 270/300 [24:02<02:35,  5.17s/it]Running Inference:  90%|█████████ | 271/300 [24:05<02:17,  4.75s/it]Running Inference:  91%|█████████ | 272/300 [24:11<02:17,  4.91s/it]Running Inference:  91%|█████████ | 273/300 [24:14<02:03,  4.57s/it]Running Inference:  91%|█████████▏| 274/300 [24:20<02:09,  5.00s/it]Running Inference:  92%|█████████▏| 275/300 [24:27<02:16,  5.47s/it]Running Inference:  92%|█████████▏| 276/300 [24:33<02:11,  5.47s/it]Running Inference:  92%|█████████▏| 277/300 [24:38<02:03,  5.38s/it]Running Inference:  93%|█████████▎| 278/300 [24:43<02:00,  5.46s/it]Running Inference:  93%|█████████▎| 279/300 [24:50<01:59,  5.69s/it]Running Inference:  93%|█████████▎| 280/300 [24:53<01:41,  5.10s/it]Running Inference:  94%|█████████▎| 281/300 [24:58<01:36,  5.06s/it]Running Inference:  94%|█████████▍| 282/300 [25:00<01:11,  3.96s/it]Running Inference:  94%|█████████▍| 283/300 [25:06<01:20,  4.72s/it]Running Inference:  95%|█████████▍| 284/300 [25:11<01:17,  4.82s/it]Running Inference:  95%|█████████▌| 285/300 [25:18<01:20,  5.35s/it]Running Inference:  95%|█████████▌| 286/300 [25:22<01:08,  4.91s/it]Running Inference:  96%|█████████▌| 287/300 [25:26<01:02,  4.78s/it]Running Inference:  96%|█████████▌| 288/300 [25:31<00:56,  4.70s/it]Running Inference:  96%|█████████▋| 289/300 [25:36<00:54,  4.95s/it]Running Inference:  97%|█████████▋| 290/300 [25:40<00:45,  4.57s/it]Running Inference:  97%|█████████▋| 291/300 [25:44<00:38,  4.30s/it]Running Inference:  97%|█████████▋| 292/300 [25:47<00:32,  4.12s/it]Running Inference:  98%|█████████▊| 293/300 [25:51<00:28,  4.02s/it]Running Inference:  98%|█████████▊| 294/300 [25:56<00:25,  4.19s/it]Running Inference:  98%|█████████▊| 295/300 [26:01<00:22,  4.44s/it]Running Inference:  99%|█████████▊| 296/300 [26:06<00:18,  4.66s/it]Running Inference:  99%|█████████▉| 297/300 [26:10<00:13,  4.67s/it]Running Inference:  99%|█████████▉| 298/300 [26:13<00:07,  3.98s/it]Running Inference: 100%|█████████▉| 299/300 [26:20<00:04,  4.82s/it]Running Inference: 100%|██████████| 300/300 [26:26<00:00,  5.20s/it]Running Inference: 100%|██████████| 300/300 [26:26<00:00,  5.29s/it]
2025-12-15 10:08:52,222 - INFO - Inference completed.
2025-12-15 10:08:52,233 - INFO - Results saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-15 10:08:52,233 - INFO - Calculating metrics for dataset: longbench
2025-12-15 10:08:52,234 - INFO - Metrics saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-15 10:08:52,234 - INFO - Metrics:
20.64
2025-12-15 10:08:52,236 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc_e, ratio=0.1) on GPU 1

----------------------------------------
Task: lcc_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 10:08:58,684 - INFO - Set deterministic seeds to 42
2025-12-15 10:08:58,684 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 10:08:58,684 - INFO - Starting evaluation run...
2025-12-15 10:08:58,684 - INFO - Output directory set to: longbenchresult
2025-12-15 10:08:58,684 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-15 10:08:58,684 - INFO - KV Press 'tova' setup.
2025-12-15 10:08:58,684 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 10:08:58,684 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.02it/s]
Device set to use cuda:0
2025-12-15 10:09:10,941 - INFO - Model pipeline loaded.
2025-12-15 10:09:10,941 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc_e)
2025-12-15 10:09:14,642 - INFO - Dataset loaded with 300 entries.
2025-12-15 10:09:14,642 - INFO - Dataset processed with 300 entries.
2025-12-15 10:09:14,675 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:07<35:24,  7.11s/it]Running Inference:   1%|          | 2/300 [00:12<29:27,  5.93s/it]Running Inference:   1%|          | 3/300 [00:16<25:50,  5.22s/it]Running Inference:   1%|▏         | 4/300 [00:19<20:20,  4.12s/it]Running Inference:   2%|▏         | 5/300 [00:23<21:44,  4.42s/it]Running Inference:   2%|▏         | 6/300 [00:27<20:39,  4.21s/it]Running Inference:   2%|▏         | 7/300 [00:32<21:23,  4.38s/it]Running Inference:   3%|▎         | 8/300 [00:38<24:03,  4.94s/it]Running Inference:   3%|▎         | 9/300 [00:42<22:09,  4.57s/it]Running Inference:   3%|▎         | 10/300 [00:47<23:27,  4.85s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [01:00<35:19,  7.33s/it]Running Inference:   4%|▍         | 12/300 [01:04<30:24,  6.33s/it]Running Inference:   4%|▍         | 13/300 [01:10<29:49,  6.24s/it]Running Inference:   5%|▍         | 14/300 [01:14<26:24,  5.54s/it]Running Inference:   5%|▌         | 15/300 [01:20<26:40,  5.62s/it]Running Inference:   5%|▌         | 16/300 [01:24<24:01,  5.07s/it]Running Inference:   6%|▌         | 17/300 [01:30<24:51,  5.27s/it]Running Inference:   6%|▌         | 18/300 [01:34<22:53,  4.87s/it]Running Inference:   6%|▋         | 19/300 [01:37<21:19,  4.55s/it]Running Inference:   7%|▋         | 20/300 [01:44<23:32,  5.05s/it]Running Inference:   7%|▋         | 21/300 [01:47<21:45,  4.68s/it]Running Inference:   7%|▋         | 22/300 [01:54<24:51,  5.36s/it]Running Inference:   8%|▊         | 23/300 [02:00<24:31,  5.31s/it]Running Inference:   8%|▊         | 24/300 [02:06<25:53,  5.63s/it]Running Inference:   8%|▊         | 25/300 [02:13<27:37,  6.03s/it]Running Inference:   9%|▊         | 26/300 [02:17<24:52,  5.45s/it]Running Inference:   9%|▉         | 27/300 [02:23<25:17,  5.56s/it]Running Inference:   9%|▉         | 28/300 [02:27<22:49,  5.04s/it]Running Inference:  10%|▉         | 29/300 [02:30<21:06,  4.67s/it]Running Inference:  10%|█         | 30/300 [02:38<25:05,  5.58s/it]Running Inference:  10%|█         | 31/300 [02:45<27:18,  6.09s/it]Running Inference:  11%|█         | 32/300 [02:52<28:07,  6.30s/it]Running Inference:  11%|█         | 33/300 [02:59<28:55,  6.50s/it]Running Inference:  11%|█▏        | 34/300 [03:05<27:57,  6.31s/it]Running Inference:  12%|█▏        | 35/300 [03:10<26:06,  5.91s/it]Running Inference:  12%|█▏        | 36/300 [03:15<24:46,  5.63s/it]Running Inference:  12%|█▏        | 37/300 [03:19<22:20,  5.10s/it]Running Inference:  13%|█▎        | 38/300 [03:23<20:48,  4.77s/it]Running Inference:  13%|█▎        | 39/300 [03:29<22:31,  5.18s/it]Running Inference:  13%|█▎        | 40/300 [03:35<22:54,  5.29s/it]Running Inference:  14%|█▎        | 41/300 [03:39<22:20,  5.17s/it]Running Inference:  14%|█▍        | 42/300 [03:44<21:49,  5.08s/it]Running Inference:  14%|█▍        | 43/300 [03:51<23:57,  5.59s/it]Running Inference:  15%|█▍        | 44/300 [03:56<23:15,  5.45s/it]Running Inference:  15%|█▌        | 45/300 [04:00<21:05,  4.96s/it]Running Inference:  15%|█▌        | 46/300 [04:05<21:04,  4.98s/it]Running Inference:  16%|█▌        | 47/300 [04:11<22:47,  5.40s/it]Running Inference:  16%|█▌        | 48/300 [04:18<23:43,  5.65s/it]Running Inference:  16%|█▋        | 49/300 [04:27<27:53,  6.67s/it]Running Inference:  17%|█▋        | 50/300 [04:33<27:27,  6.59s/it]Running Inference:  17%|█▋        | 51/300 [04:41<28:32,  6.88s/it]Running Inference:  17%|█▋        | 52/300 [04:45<24:45,  5.99s/it]Running Inference:  18%|█▊        | 53/300 [04:49<22:08,  5.38s/it]Running Inference:  18%|█▊        | 54/300 [04:55<23:43,  5.79s/it]Running Inference:  18%|█▊        | 55/300 [04:59<21:15,  5.20s/it]Running Inference:  19%|█▊        | 56/300 [05:04<21:02,  5.17s/it]Running Inference:  19%|█▉        | 57/300 [05:11<22:57,  5.67s/it]Running Inference:  19%|█▉        | 58/300 [05:18<24:15,  6.01s/it]Running Inference:  20%|█▉        | 59/300 [05:23<22:53,  5.70s/it]Running Inference:  20%|██        | 60/300 [05:28<22:18,  5.58s/it]Running Inference:  20%|██        | 61/300 [05:34<22:09,  5.56s/it]Running Inference:  21%|██        | 62/300 [05:42<25:45,  6.49s/it]Running Inference:  21%|██        | 63/300 [05:46<22:30,  5.70s/it]Running Inference:  21%|██▏       | 64/300 [05:53<23:08,  5.88s/it]Running Inference:  22%|██▏       | 65/300 [05:57<21:38,  5.52s/it]Running Inference:  22%|██▏       | 66/300 [06:01<19:33,  5.01s/it]Running Inference:  22%|██▏       | 67/300 [06:10<23:56,  6.17s/it]Running Inference:  23%|██▎       | 68/300 [06:15<23:06,  5.98s/it]Running Inference:  23%|██▎       | 69/300 [06:20<21:55,  5.69s/it]Running Inference:  23%|██▎       | 70/300 [06:28<23:42,  6.19s/it]Running Inference:  24%|██▎       | 71/300 [06:34<23:13,  6.08s/it]Running Inference:  24%|██▍       | 72/300 [06:38<21:28,  5.65s/it]Running Inference:  24%|██▍       | 73/300 [06:42<19:18,  5.10s/it]Running Inference:  25%|██▍       | 74/300 [06:48<20:24,  5.42s/it]Running Inference:  25%|██▌       | 75/300 [06:54<21:03,  5.62s/it]Running Inference:  25%|██▌       | 76/300 [07:05<26:54,  7.21s/it]Running Inference:  26%|██▌       | 77/300 [07:12<26:19,  7.08s/it]Running Inference:  26%|██▌       | 78/300 [07:18<25:13,  6.82s/it]Running Inference:  26%|██▋       | 79/300 [07:25<24:44,  6.72s/it]Running Inference:  27%|██▋       | 80/300 [07:29<21:26,  5.85s/it]Running Inference:  27%|██▋       | 81/300 [07:32<19:06,  5.23s/it]Running Inference:  27%|██▋       | 82/300 [07:36<17:42,  4.87s/it]Running Inference:  28%|██▊       | 83/300 [07:40<16:31,  4.57s/it]Running Inference:  28%|██▊       | 84/300 [07:44<15:57,  4.43s/it]Running Inference:  28%|██▊       | 85/300 [07:51<18:09,  5.07s/it]Running Inference:  29%|██▊       | 86/300 [07:58<20:15,  5.68s/it]Running Inference:  29%|██▉       | 87/300 [08:04<20:53,  5.89s/it]Running Inference:  29%|██▉       | 88/300 [08:09<18:55,  5.36s/it]Running Inference:  30%|██▉       | 89/300 [08:12<17:17,  4.92s/it]Running Inference:  30%|███       | 90/300 [08:16<16:05,  4.60s/it]Running Inference:  30%|███       | 91/300 [08:23<17:53,  5.13s/it]Running Inference:  31%|███       | 92/300 [08:28<18:32,  5.35s/it]Running Inference:  31%|███       | 93/300 [08:36<20:48,  6.03s/it]Running Inference:  31%|███▏      | 94/300 [08:45<23:22,  6.81s/it]Running Inference:  32%|███▏      | 95/300 [08:52<24:10,  7.07s/it]Running Inference:  32%|███▏      | 96/300 [08:56<20:44,  6.10s/it]Running Inference:  32%|███▏      | 97/300 [09:04<21:55,  6.48s/it]Running Inference:  33%|███▎      | 98/300 [09:08<20:03,  5.96s/it]Running Inference:  33%|███▎      | 99/300 [09:14<20:05,  6.00s/it]Running Inference:  33%|███▎      | 100/300 [09:20<19:50,  5.95s/it]Running Inference:  34%|███▎      | 101/300 [09:24<17:37,  5.31s/it]Running Inference:  34%|███▍      | 102/300 [09:25<13:12,  4.00s/it]Running Inference:  34%|███▍      | 103/300 [09:30<14:26,  4.40s/it]Running Inference:  35%|███▍      | 104/300 [09:37<16:56,  5.18s/it]Running Inference:  35%|███▌      | 105/300 [09:41<15:32,  4.78s/it]Running Inference:  35%|███▌      | 106/300 [09:45<14:33,  4.50s/it]Running Inference:  36%|███▌      | 107/300 [09:50<14:33,  4.53s/it]Running Inference:  36%|███▌      | 108/300 [09:53<13:46,  4.30s/it]Running Inference:  36%|███▋      | 109/300 [09:59<14:44,  4.63s/it]Running Inference:  37%|███▋      | 110/300 [10:05<15:50,  5.00s/it]Running Inference:  37%|███▋      | 111/300 [10:10<16:04,  5.10s/it]Running Inference:  37%|███▋      | 112/300 [10:14<14:46,  4.72s/it]Running Inference:  38%|███▊      | 113/300 [10:20<15:37,  5.01s/it]Running Inference:  38%|███▊      | 114/300 [10:23<14:23,  4.64s/it]Running Inference:  38%|███▊      | 115/300 [10:30<15:48,  5.13s/it]Running Inference:  39%|███▊      | 116/300 [10:33<14:32,  4.74s/it]Running Inference:  39%|███▉      | 117/300 [10:39<14:50,  4.86s/it]Running Inference:  39%|███▉      | 118/300 [10:43<13:55,  4.59s/it]Running Inference:  40%|███▉      | 119/300 [10:46<13:08,  4.36s/it]Running Inference:  40%|████      | 120/300 [10:50<12:31,  4.18s/it]Running Inference:  40%|████      | 121/300 [10:55<12:44,  4.27s/it]Running Inference:  41%|████      | 122/300 [11:00<13:39,  4.61s/it]Running Inference:  41%|████      | 123/300 [11:06<14:36,  4.95s/it]Running Inference:  41%|████▏     | 124/300 [11:12<16:04,  5.48s/it]Running Inference:  42%|████▏     | 125/300 [11:19<16:48,  5.76s/it]Running Inference:  42%|████▏     | 126/300 [11:27<18:34,  6.40s/it]Running Inference:  42%|████▏     | 127/300 [11:31<16:47,  5.82s/it]Running Inference:  43%|████▎     | 128/300 [11:34<13:54,  4.85s/it]Running Inference:  43%|████▎     | 129/300 [11:38<13:31,  4.74s/it]Running Inference:  43%|████▎     | 130/300 [11:44<13:58,  4.93s/it]Running Inference:  44%|████▎     | 131/300 [11:49<13:52,  4.93s/it]Running Inference:  44%|████▍     | 132/300 [11:55<15:24,  5.50s/it]Running Inference:  44%|████▍     | 133/300 [12:00<14:48,  5.32s/it]Running Inference:  45%|████▍     | 134/300 [12:05<14:13,  5.14s/it]Running Inference:  45%|████▌     | 135/300 [12:06<10:27,  3.81s/it]Running Inference:  45%|████▌     | 136/300 [12:12<12:07,  4.44s/it]Running Inference:  46%|████▌     | 137/300 [12:15<11:30,  4.23s/it]Running Inference:  46%|████▌     | 138/300 [12:21<12:31,  4.64s/it]Running Inference:  46%|████▋     | 139/300 [12:25<12:14,  4.56s/it]Running Inference:  47%|████▋     | 140/300 [12:30<12:13,  4.58s/it]Running Inference:  47%|████▋     | 141/300 [12:36<13:09,  4.96s/it]Running Inference:  47%|████▋     | 142/300 [12:43<14:33,  5.53s/it]Running Inference:  48%|████▊     | 143/300 [12:47<13:15,  5.07s/it]Running Inference:  48%|████▊     | 144/300 [12:54<14:44,  5.67s/it]Running Inference:  48%|████▊     | 145/300 [12:59<14:18,  5.54s/it]Running Inference:  49%|████▊     | 146/300 [13:03<12:59,  5.06s/it]Running Inference:  49%|████▉     | 147/300 [13:09<13:28,  5.28s/it]Running Inference:  49%|████▉     | 148/300 [13:14<13:17,  5.25s/it]Running Inference:  50%|████▉     | 149/300 [13:18<12:17,  4.88s/it]Running Inference:  50%|█████     | 150/300 [13:22<11:23,  4.56s/it]Running Inference:  50%|█████     | 151/300 [13:27<11:58,  4.82s/it]Running Inference:  51%|█████     | 152/300 [13:33<12:25,  5.03s/it]Running Inference:  51%|█████     | 153/300 [13:38<12:25,  5.07s/it]Running Inference:  51%|█████▏    | 154/300 [13:42<11:38,  4.78s/it]Running Inference:  52%|█████▏    | 155/300 [13:48<12:30,  5.18s/it]Running Inference:  52%|█████▏    | 156/300 [13:53<12:23,  5.16s/it]Running Inference:  52%|█████▏    | 157/300 [13:54<09:11,  3.86s/it]Running Inference:  53%|█████▎    | 158/300 [14:09<17:11,  7.27s/it]Running Inference:  53%|█████▎    | 159/300 [14:14<15:12,  6.47s/it]Running Inference:  53%|█████▎    | 160/300 [14:23<17:11,  7.37s/it]Running Inference:  54%|█████▎    | 161/300 [14:30<16:16,  7.03s/it]Running Inference:  54%|█████▍    | 162/300 [14:39<17:40,  7.68s/it]Running Inference:  54%|█████▍    | 163/300 [14:45<16:17,  7.13s/it]Running Inference:  55%|█████▍    | 164/300 [14:51<15:18,  6.75s/it]Running Inference:  55%|█████▌    | 165/300 [15:00<16:50,  7.48s/it]Running Inference:  55%|█████▌    | 166/300 [15:05<14:56,  6.69s/it]Running Inference:  56%|█████▌    | 167/300 [15:08<12:55,  5.83s/it]Running Inference:  56%|█████▌    | 168/300 [15:15<13:25,  6.11s/it]Running Inference:  56%|█████▋    | 169/300 [15:19<11:52,  5.44s/it]Running Inference:  57%|█████▋    | 170/300 [15:25<12:09,  5.61s/it]Running Inference:  57%|█████▋    | 171/300 [15:30<11:24,  5.30s/it]Running Inference:  57%|█████▋    | 172/300 [15:34<10:26,  4.89s/it]Running Inference:  58%|█████▊    | 173/300 [15:40<11:26,  5.41s/it]Running Inference:  58%|█████▊    | 174/300 [15:45<11:07,  5.30s/it]Running Inference:  58%|█████▊    | 175/300 [15:50<10:52,  5.22s/it]Running Inference:  59%|█████▊    | 176/300 [15:54<09:54,  4.80s/it]Running Inference:  59%|█████▉    | 177/300 [15:58<09:19,  4.55s/it]Running Inference:  59%|█████▉    | 178/300 [16:02<09:03,  4.46s/it]Running Inference:  60%|█████▉    | 179/300 [16:10<10:41,  5.30s/it]Running Inference:  60%|██████    | 180/300 [16:14<10:19,  5.16s/it]Running Inference:  60%|██████    | 181/300 [16:26<13:57,  7.04s/it]Running Inference:  61%|██████    | 182/300 [16:34<14:35,  7.42s/it]Running Inference:  61%|██████    | 183/300 [16:38<12:23,  6.36s/it]Running Inference:  61%|██████▏   | 184/300 [16:46<13:13,  6.84s/it]Running Inference:  62%|██████▏   | 185/300 [16:50<11:24,  5.96s/it]Running Inference:  62%|██████▏   | 186/300 [16:56<11:31,  6.07s/it]Running Inference:  62%|██████▏   | 187/300 [17:00<10:13,  5.43s/it]Running Inference:  63%|██████▎   | 188/300 [17:05<09:58,  5.35s/it]Running Inference:  63%|██████▎   | 189/300 [17:11<10:21,  5.60s/it]Running Inference:  63%|██████▎   | 190/300 [17:18<10:59,  5.99s/it]Running Inference:  64%|██████▎   | 191/300 [17:26<11:33,  6.36s/it]Running Inference:  64%|██████▍   | 192/300 [17:31<11:05,  6.16s/it]Running Inference:  64%|██████▍   | 193/300 [17:32<08:11,  4.59s/it]Running Inference:  65%|██████▍   | 194/300 [17:35<06:54,  3.91s/it]Running Inference:  65%|██████▌   | 195/300 [17:40<07:48,  4.46s/it]Running Inference:  65%|██████▌   | 196/300 [17:45<07:54,  4.56s/it]Running Inference:  66%|██████▌   | 197/300 [17:46<05:49,  3.39s/it]Running Inference:  66%|██████▌   | 198/300 [17:51<06:32,  3.85s/it]Running Inference:  66%|██████▋   | 199/300 [17:57<07:40,  4.56s/it]Running Inference:  67%|██████▋   | 200/300 [18:02<08:04,  4.84s/it]Running Inference:  67%|██████▋   | 201/300 [18:09<08:41,  5.27s/it]Running Inference:  67%|██████▋   | 202/300 [18:12<07:54,  4.84s/it]Running Inference:  68%|██████▊   | 203/300 [18:14<06:00,  3.72s/it]Running Inference:  68%|██████▊   | 204/300 [18:19<06:41,  4.18s/it]Running Inference:  68%|██████▊   | 205/300 [18:24<07:10,  4.53s/it]Running Inference:  69%|██████▊   | 206/300 [18:31<07:57,  5.08s/it]Running Inference:  69%|██████▉   | 207/300 [18:35<07:42,  4.97s/it]Running Inference:  69%|██████▉   | 208/300 [18:40<07:40,  5.01s/it]Running Inference:  70%|██████▉   | 209/300 [18:47<08:08,  5.37s/it]Running Inference:  70%|███████   | 210/300 [18:53<08:41,  5.79s/it]Running Inference:  70%|███████   | 211/300 [19:00<08:48,  5.93s/it]Running Inference:  71%|███████   | 212/300 [19:06<08:49,  6.02s/it]Running Inference:  71%|███████   | 213/300 [19:14<09:49,  6.77s/it]Running Inference:  71%|███████▏  | 214/300 [19:19<08:59,  6.27s/it]Running Inference:  72%|███████▏  | 215/300 [19:24<08:09,  5.76s/it]Running Inference:  72%|███████▏  | 216/300 [19:30<08:14,  5.89s/it]Running Inference:  72%|███████▏  | 217/300 [19:34<07:21,  5.32s/it]Running Inference:  73%|███████▎  | 218/300 [19:38<06:39,  4.87s/it]Running Inference:  73%|███████▎  | 219/300 [19:44<07:11,  5.32s/it]Running Inference:  73%|███████▎  | 220/300 [19:51<07:41,  5.77s/it]Running Inference:  74%|███████▎  | 221/300 [19:58<07:50,  5.95s/it]Running Inference:  74%|███████▍  | 222/300 [20:02<07:18,  5.62s/it]Running Inference:  74%|███████▍  | 223/300 [20:06<06:35,  5.14s/it]Running Inference:  75%|███████▍  | 224/300 [20:12<06:47,  5.37s/it]Running Inference:  75%|███████▌  | 225/300 [20:16<06:08,  4.91s/it]Running Inference:  75%|███████▌  | 226/300 [20:23<06:46,  5.50s/it]Running Inference:  76%|███████▌  | 227/300 [20:27<06:04,  5.00s/it]Running Inference:  76%|███████▌  | 228/300 [20:32<05:56,  4.96s/it]Running Inference:  76%|███████▋  | 229/300 [20:38<06:15,  5.28s/it]Running Inference:  77%|███████▋  | 230/300 [20:42<05:55,  5.07s/it]Running Inference:  77%|███████▋  | 231/300 [20:49<06:23,  5.56s/it]Running Inference:  77%|███████▋  | 232/300 [20:53<05:40,  5.01s/it]Running Inference:  78%|███████▊  | 233/300 [20:58<05:30,  4.93s/it]Running Inference:  78%|███████▊  | 234/300 [21:01<05:04,  4.61s/it]Running Inference:  78%|███████▊  | 235/300 [21:05<04:44,  4.37s/it]Running Inference:  79%|███████▊  | 236/300 [21:09<04:32,  4.26s/it]Running Inference:  79%|███████▉  | 237/300 [21:22<07:07,  6.78s/it]Running Inference:  79%|███████▉  | 238/300 [21:26<06:06,  5.91s/it]Running Inference:  80%|███████▉  | 239/300 [21:27<04:26,  4.37s/it]Running Inference:  80%|████████  | 240/300 [21:31<04:15,  4.26s/it]Running Inference:  80%|████████  | 241/300 [21:34<04:05,  4.16s/it]Running Inference:  81%|████████  | 242/300 [21:39<04:14,  4.38s/it]Running Inference:  81%|████████  | 243/300 [21:45<04:27,  4.69s/it]Running Inference:  81%|████████▏ | 244/300 [21:49<04:08,  4.44s/it]Running Inference:  82%|████████▏ | 245/300 [21:53<03:54,  4.26s/it]Running Inference:  82%|████████▏ | 246/300 [22:00<04:39,  5.17s/it]Running Inference:  82%|████████▏ | 247/300 [22:03<04:10,  4.73s/it]Running Inference:  83%|████████▎ | 248/300 [22:09<04:22,  5.05s/it]Running Inference:  83%|████████▎ | 249/300 [22:13<04:03,  4.77s/it]Running Inference:  83%|████████▎ | 250/300 [22:17<03:46,  4.53s/it]Running Inference:  84%|████████▎ | 251/300 [22:24<04:06,  5.04s/it]Running Inference:  84%|████████▍ | 252/300 [22:27<03:44,  4.68s/it]Running Inference:  84%|████████▍ | 253/300 [22:29<03:01,  3.87s/it]Running Inference:  85%|████████▍ | 254/300 [22:33<02:59,  3.89s/it]Running Inference:  85%|████████▌ | 255/300 [22:37<02:53,  3.87s/it]Running Inference:  85%|████████▌ | 256/300 [22:42<03:06,  4.24s/it]Running Inference:  86%|████████▌ | 257/300 [22:49<03:28,  4.85s/it]Running Inference:  86%|████████▌ | 258/300 [22:54<03:32,  5.06s/it]Running Inference:  86%|████████▋ | 259/300 [22:58<03:12,  4.70s/it]Running Inference:  87%|████████▋ | 260/300 [23:02<02:58,  4.45s/it]Running Inference:  87%|████████▋ | 261/300 [23:06<02:50,  4.38s/it]Running Inference:  87%|████████▋ | 262/300 [23:10<02:42,  4.27s/it]Running Inference:  88%|████████▊ | 263/300 [23:14<02:32,  4.12s/it]Running Inference:  88%|████████▊ | 264/300 [23:19<02:37,  4.38s/it]Running Inference:  88%|████████▊ | 265/300 [23:23<02:29,  4.28s/it]Running Inference:  89%|████████▊ | 266/300 [23:29<02:45,  4.87s/it]Running Inference:  89%|████████▉ | 267/300 [23:36<02:55,  5.33s/it]Running Inference:  89%|████████▉ | 268/300 [23:41<02:47,  5.25s/it]Running Inference:  90%|████████▉ | 269/300 [23:48<02:59,  5.80s/it]Running Inference:  90%|█████████ | 270/300 [23:52<02:37,  5.23s/it]Running Inference:  90%|█████████ | 271/300 [23:55<02:19,  4.81s/it]Running Inference:  91%|█████████ | 272/300 [24:01<02:19,  4.98s/it]Running Inference:  91%|█████████ | 273/300 [24:05<02:05,  4.64s/it]Running Inference:  91%|█████████▏| 274/300 [24:11<02:11,  5.05s/it]Running Inference:  92%|█████████▏| 275/300 [24:17<02:17,  5.51s/it]Running Inference:  92%|█████████▏| 276/300 [24:23<02:12,  5.52s/it]Running Inference:  92%|█████████▏| 277/300 [24:28<02:04,  5.43s/it]Running Inference:  93%|█████████▎| 278/300 [24:34<02:01,  5.51s/it]Running Inference:  93%|█████████▎| 279/300 [24:40<02:00,  5.73s/it]Running Inference:  93%|█████████▎| 280/300 [24:44<01:43,  5.15s/it]Running Inference:  94%|█████████▎| 281/300 [24:49<01:37,  5.12s/it]Running Inference:  94%|█████████▍| 282/300 [24:50<01:12,  4.01s/it]Running Inference:  94%|█████████▍| 283/300 [24:57<01:20,  4.75s/it]Running Inference:  95%|█████████▍| 284/300 [25:02<01:17,  4.87s/it]Running Inference:  95%|█████████▌| 285/300 [25:08<01:21,  5.41s/it]Running Inference:  95%|█████████▌| 286/300 [25:12<01:09,  4.99s/it]Running Inference:  96%|█████████▌| 287/300 [25:17<01:03,  4.86s/it]Running Inference:  96%|█████████▌| 288/300 [25:22<00:57,  4.78s/it]Running Inference:  96%|█████████▋| 289/300 [25:27<00:55,  5.03s/it]Running Inference:  97%|█████████▋| 290/300 [25:31<00:46,  4.66s/it]Running Inference:  97%|█████████▋| 291/300 [25:35<00:39,  4.39s/it]Running Inference:  97%|█████████▋| 292/300 [25:39<00:33,  4.21s/it]Running Inference:  98%|█████████▊| 293/300 [25:42<00:28,  4.10s/it]Running Inference:  98%|█████████▊| 294/300 [25:47<00:25,  4.27s/it]Running Inference:  98%|█████████▊| 295/300 [25:52<00:22,  4.51s/it]Running Inference:  99%|█████████▊| 296/300 [25:57<00:18,  4.74s/it]Running Inference:  99%|█████████▉| 297/300 [26:02<00:14,  4.74s/it]Running Inference:  99%|█████████▉| 298/300 [26:05<00:08,  4.04s/it]Running Inference: 100%|█████████▉| 299/300 [26:11<00:04,  4.87s/it]Running Inference: 100%|██████████| 300/300 [26:18<00:00,  5.24s/it]Running Inference: 100%|██████████| 300/300 [26:18<00:00,  5.26s/it]
2025-12-15 10:35:32,710 - INFO - Inference completed.
2025-12-15 10:35:32,721 - INFO - Results saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-15 10:35:32,721 - INFO - Calculating metrics for dataset: longbench
2025-12-15 10:35:32,722 - INFO - Metrics saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-15 10:35:32,722 - INFO - Metrics:
21.21
2025-12-15 10:35:32,724 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc_e, ratio=0.2) on GPU 1

----------------------------------------
Task: lcc_e | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 10:35:39,133 - INFO - Set deterministic seeds to 42
2025-12-15 10:35:39,133 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 10:35:39,133 - INFO - Starting evaluation run...
2025-12-15 10:35:39,133 - INFO - Output directory set to: longbenchresult
2025-12-15 10:35:39,133 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-15 10:35:39,133 - INFO - KV Press 'tova' setup.
2025-12-15 10:35:39,133 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 10:35:39,133 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.23it/s]
Device set to use cuda:0
2025-12-15 10:35:50,498 - INFO - Model pipeline loaded.
2025-12-15 10:35:50,498 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc_e)
2025-12-15 10:35:55,809 - INFO - Dataset loaded with 300 entries.
2025-12-15 10:35:55,809 - INFO - Dataset processed with 300 entries.
2025-12-15 10:35:55,842 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:06<34:43,  6.97s/it]Running Inference:   1%|          | 2/300 [00:12<29:03,  5.85s/it]Running Inference:   1%|          | 3/300 [00:16<25:32,  5.16s/it]Running Inference:   1%|▏         | 4/300 [00:18<20:08,  4.08s/it]Running Inference:   2%|▏         | 5/300 [00:23<21:31,  4.38s/it]Running Inference:   2%|▏         | 6/300 [00:27<20:25,  4.17s/it]Running Inference:   2%|▏         | 7/300 [00:32<21:10,  4.34s/it]Running Inference:   3%|▎         | 8/300 [00:38<23:47,  4.89s/it]Running Inference:   3%|▎         | 9/300 [00:41<21:56,  4.52s/it]Running Inference:   3%|▎         | 10/300 [00:47<24:01,  4.97s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [01:00<35:29,  7.37s/it]Running Inference:   4%|▍         | 12/300 [01:04<30:28,  6.35s/it]Running Inference:   4%|▍         | 13/300 [01:10<29:48,  6.23s/it]Running Inference:   5%|▍         | 14/300 [01:14<26:21,  5.53s/it]Running Inference:   5%|▌         | 15/300 [01:20<26:36,  5.60s/it]Running Inference:   5%|▌         | 16/300 [01:24<23:55,  5.05s/it]Running Inference:   6%|▌         | 17/300 [01:29<24:43,  5.24s/it]Running Inference:   6%|▌         | 18/300 [01:33<22:45,  4.84s/it]Running Inference:   6%|▋         | 19/300 [01:37<21:11,  4.52s/it]Running Inference:   7%|▋         | 20/300 [01:42<22:16,  4.77s/it]Running Inference:   7%|▋         | 21/300 [01:46<20:49,  4.48s/it]Running Inference:   7%|▋         | 22/300 [01:54<25:46,  5.56s/it]Running Inference:   8%|▊         | 23/300 [01:59<25:08,  5.44s/it]Running Inference:   8%|▊         | 24/300 [02:06<26:13,  5.70s/it]Running Inference:   8%|▊         | 25/300 [02:13<27:44,  6.05s/it]Running Inference:   9%|▊         | 26/300 [02:17<24:56,  5.46s/it]Running Inference:   9%|▉         | 27/300 [02:22<25:17,  5.56s/it]Running Inference:   9%|▉         | 28/300 [02:26<22:47,  5.03s/it]Running Inference:  10%|▉         | 29/300 [02:30<21:01,  4.65s/it]Running Inference:  10%|█         | 30/300 [02:38<24:53,  5.53s/it]Running Inference:  10%|█         | 31/300 [02:45<27:01,  6.03s/it]Running Inference:  11%|█         | 32/300 [02:52<27:49,  6.23s/it]Running Inference:  11%|█         | 33/300 [02:58<28:35,  6.42s/it]Running Inference:  11%|█▏        | 34/300 [03:04<27:39,  6.24s/it]Running Inference:  12%|█▏        | 35/300 [03:09<25:50,  5.85s/it]Running Inference:  12%|█▏        | 36/300 [03:14<24:32,  5.58s/it]Running Inference:  12%|█▏        | 37/300 [03:18<22:07,  5.05s/it]Running Inference:  13%|█▎        | 38/300 [03:22<20:36,  4.72s/it]Running Inference:  13%|█▎        | 39/300 [03:28<22:18,  5.13s/it]Running Inference:  13%|█▎        | 40/300 [03:33<22:41,  5.24s/it]Running Inference:  14%|█▎        | 41/300 [03:38<22:08,  5.13s/it]Running Inference:  14%|█▍        | 42/300 [03:43<21:38,  5.03s/it]Running Inference:  14%|█▍        | 43/300 [03:50<23:43,  5.54s/it]Running Inference:  15%|█▍        | 44/300 [03:55<23:01,  5.40s/it]Running Inference:  15%|█▌        | 45/300 [03:59<20:52,  4.91s/it]Running Inference:  15%|█▌        | 46/300 [04:04<20:52,  4.93s/it]Running Inference:  16%|█▌        | 47/300 [04:10<22:32,  5.35s/it]Running Inference:  16%|█▌        | 48/300 [04:16<23:28,  5.59s/it]Running Inference:  16%|█▋        | 49/300 [04:25<27:33,  6.59s/it]Running Inference:  17%|█▋        | 50/300 [04:31<27:08,  6.51s/it]Running Inference:  17%|█▋        | 51/300 [04:39<28:11,  6.79s/it]Running Inference:  17%|█▋        | 52/300 [04:43<24:27,  5.92s/it]Running Inference:  18%|█▊        | 53/300 [04:47<21:52,  5.31s/it]Running Inference:  18%|█▊        | 54/300 [04:53<23:25,  5.71s/it]Running Inference:  18%|█▊        | 55/300 [04:57<20:59,  5.14s/it]Running Inference:  19%|█▊        | 56/300 [05:02<20:48,  5.12s/it]Running Inference:  19%|█▉        | 57/300 [05:08<22:15,  5.50s/it]Running Inference:  19%|█▉        | 58/300 [05:15<23:39,  5.87s/it]Running Inference:  20%|█▉        | 59/300 [05:20<22:26,  5.59s/it]Running Inference:  20%|██        | 60/300 [05:28<25:30,  6.38s/it]Running Inference:  20%|██        | 61/300 [05:34<24:20,  6.11s/it]Running Inference:  21%|██        | 62/300 [05:42<27:08,  6.84s/it]Running Inference:  21%|██        | 63/300 [05:46<23:26,  5.93s/it]Running Inference:  21%|██▏       | 64/300 [05:52<23:41,  6.02s/it]Running Inference:  22%|██▏       | 65/300 [05:57<21:58,  5.61s/it]Running Inference:  22%|██▏       | 66/300 [06:01<19:43,  5.06s/it]Running Inference:  22%|██▏       | 67/300 [06:10<23:55,  6.16s/it]Running Inference:  23%|██▎       | 68/300 [06:15<23:04,  5.97s/it]Running Inference:  23%|██▎       | 69/300 [06:20<21:50,  5.67s/it]Running Inference:  23%|██▎       | 70/300 [06:27<23:32,  6.14s/it]Running Inference:  24%|██▎       | 71/300 [06:33<23:04,  6.05s/it]Running Inference:  24%|██▍       | 72/300 [06:38<21:19,  5.61s/it]Running Inference:  24%|██▍       | 73/300 [06:42<19:12,  5.08s/it]Running Inference:  25%|██▍       | 74/300 [06:48<20:18,  5.39s/it]Running Inference:  25%|██▌       | 75/300 [06:54<20:54,  5.58s/it]Running Inference:  25%|██▌       | 76/300 [07:05<26:39,  7.14s/it]Running Inference:  26%|██▌       | 77/300 [07:11<26:03,  7.01s/it]Running Inference:  26%|██▌       | 78/300 [07:17<24:56,  6.74s/it]Running Inference:  26%|██▋       | 79/300 [07:24<24:27,  6.64s/it]Running Inference:  27%|██▋       | 80/300 [07:28<21:12,  5.78s/it]Running Inference:  27%|██▋       | 81/300 [07:31<18:55,  5.18s/it]Running Inference:  27%|██▋       | 82/300 [07:35<17:32,  4.83s/it]Running Inference:  28%|██▊       | 83/300 [07:39<16:24,  4.54s/it]Running Inference:  28%|██▊       | 84/300 [07:43<15:50,  4.40s/it]Running Inference:  28%|██▊       | 85/300 [07:50<17:58,  5.02s/it]Running Inference:  29%|██▊       | 86/300 [07:57<20:02,  5.62s/it]Running Inference:  29%|██▉       | 87/300 [08:03<20:39,  5.82s/it]Running Inference:  29%|██▉       | 88/300 [08:07<18:43,  5.30s/it]Running Inference:  30%|██▉       | 89/300 [08:11<17:06,  4.87s/it]Running Inference:  30%|███       | 90/300 [08:15<15:56,  4.55s/it]Running Inference:  30%|███       | 91/300 [08:21<17:40,  5.07s/it]Running Inference:  31%|███       | 92/300 [08:27<18:21,  5.30s/it]Running Inference:  31%|███       | 93/300 [08:34<20:34,  5.96s/it]Running Inference:  31%|███▏      | 94/300 [08:43<23:04,  6.72s/it]Running Inference:  32%|███▏      | 95/300 [08:51<23:51,  6.99s/it]Running Inference:  32%|███▏      | 96/300 [08:54<20:29,  6.03s/it]Running Inference:  32%|███▏      | 97/300 [09:02<21:38,  6.39s/it]Running Inference:  33%|███▎      | 98/300 [09:06<19:48,  5.89s/it]Running Inference:  33%|███▎      | 99/300 [09:12<19:52,  5.93s/it]Running Inference:  33%|███▎      | 100/300 [09:18<19:41,  5.91s/it]Running Inference:  34%|███▎      | 101/300 [09:22<17:28,  5.27s/it]Running Inference:  34%|███▍      | 102/300 [09:23<13:06,  3.97s/it]Running Inference:  34%|███▍      | 103/300 [09:28<14:21,  4.37s/it]Running Inference:  35%|███▍      | 104/300 [09:35<16:46,  5.14s/it]Running Inference:  35%|███▌      | 105/300 [09:39<15:23,  4.74s/it]Running Inference:  35%|███▌      | 106/300 [09:43<14:24,  4.46s/it]Running Inference:  36%|███▌      | 107/300 [09:47<14:25,  4.49s/it]Running Inference:  36%|███▌      | 108/300 [09:51<13:38,  4.27s/it]Running Inference:  36%|███▋      | 109/300 [09:56<14:37,  4.59s/it]Running Inference:  37%|███▋      | 110/300 [10:02<15:43,  4.96s/it]Running Inference:  37%|███▋      | 111/300 [10:08<15:58,  5.07s/it]Running Inference:  37%|███▋      | 112/300 [10:11<14:41,  4.69s/it]Running Inference:  38%|███▊      | 113/300 [10:17<15:34,  5.00s/it]Running Inference:  38%|███▊      | 114/300 [10:21<14:20,  4.63s/it]Running Inference:  38%|███▊      | 115/300 [10:27<15:42,  5.09s/it]Running Inference:  39%|███▊      | 116/300 [10:31<14:26,  4.71s/it]Running Inference:  39%|███▉      | 117/300 [10:36<14:43,  4.83s/it]Running Inference:  39%|███▉      | 118/300 [10:40<13:48,  4.55s/it]Running Inference:  40%|███▉      | 119/300 [10:44<13:01,  4.32s/it]Running Inference:  40%|████      | 120/300 [10:47<12:24,  4.14s/it]Running Inference:  40%|████      | 121/300 [10:52<12:37,  4.23s/it]Running Inference:  41%|████      | 122/300 [10:57<13:32,  4.56s/it]Running Inference:  41%|████      | 123/300 [11:03<14:29,  4.91s/it]Running Inference:  41%|████▏     | 124/300 [11:09<15:55,  5.43s/it]Running Inference:  42%|████▏     | 125/300 [11:16<16:37,  5.70s/it]Running Inference:  42%|████▏     | 126/300 [11:24<18:20,  6.33s/it]Running Inference:  42%|████▏     | 127/300 [11:28<16:37,  5.76s/it]Running Inference:  43%|████▎     | 128/300 [11:31<13:46,  4.81s/it]Running Inference:  43%|████▎     | 129/300 [11:35<13:23,  4.70s/it]Running Inference:  43%|████▎     | 130/300 [11:40<13:51,  4.89s/it]Running Inference:  44%|████▎     | 131/300 [11:45<13:46,  4.89s/it]Running Inference:  44%|████▍     | 132/300 [11:52<15:15,  5.45s/it]Running Inference:  44%|████▍     | 133/300 [11:57<14:40,  5.27s/it]Running Inference:  45%|████▍     | 134/300 [12:02<14:05,  5.10s/it]Running Inference:  45%|████▌     | 135/300 [12:05<12:56,  4.71s/it]Running Inference:  45%|████▌     | 136/300 [12:11<13:47,  5.05s/it]Running Inference:  46%|████▌     | 137/300 [12:15<12:37,  4.65s/it]Running Inference:  46%|████▌     | 138/300 [12:20<13:17,  4.92s/it]Running Inference:  46%|████▋     | 139/300 [12:25<12:42,  4.74s/it]Running Inference:  47%|████▋     | 140/300 [12:29<12:31,  4.70s/it]Running Inference:  47%|████▋     | 141/300 [12:35<13:22,  5.04s/it]Running Inference:  47%|████▋     | 142/300 [12:42<14:35,  5.54s/it]Running Inference:  48%|████▊     | 143/300 [12:46<13:13,  5.06s/it]Running Inference:  48%|████▊     | 144/300 [12:53<14:38,  5.63s/it]Running Inference:  48%|████▊     | 145/300 [12:58<14:09,  5.48s/it]Running Inference:  49%|████▊     | 146/300 [13:02<12:50,  5.00s/it]Running Inference:  49%|████▉     | 147/300 [13:08<13:19,  5.22s/it]Running Inference:  49%|████▉     | 148/300 [13:13<13:09,  5.19s/it]Running Inference:  50%|████▉     | 149/300 [13:17<12:08,  4.82s/it]Running Inference:  50%|█████     | 150/300 [13:20<11:14,  4.50s/it]Running Inference:  50%|█████     | 151/300 [13:26<11:48,  4.76s/it]Running Inference:  51%|█████     | 152/300 [13:31<12:15,  4.97s/it]Running Inference:  51%|█████     | 153/300 [13:36<12:16,  5.01s/it]Running Inference:  51%|█████▏    | 154/300 [13:40<11:29,  4.72s/it]Running Inference:  52%|█████▏    | 155/300 [13:46<12:20,  5.11s/it]Running Inference:  52%|█████▏    | 156/300 [13:51<12:13,  5.10s/it]Running Inference:  52%|█████▏    | 157/300 [13:52<09:04,  3.81s/it]Running Inference:  53%|█████▎    | 158/300 [14:07<16:58,  7.17s/it]Running Inference:  53%|█████▎    | 159/300 [14:12<15:01,  6.39s/it]Running Inference:  53%|█████▎    | 160/300 [14:21<16:57,  7.27s/it]Running Inference:  54%|█████▎    | 161/300 [14:27<16:04,  6.94s/it]Running Inference:  54%|█████▍    | 162/300 [14:36<17:26,  7.59s/it]Running Inference:  54%|█████▍    | 163/300 [14:42<16:08,  7.07s/it]Running Inference:  55%|█████▍    | 164/300 [14:48<15:10,  6.69s/it]Running Inference:  55%|█████▌    | 165/300 [14:57<16:38,  7.40s/it]Running Inference:  55%|█████▌    | 166/300 [15:02<14:46,  6.62s/it]Running Inference:  56%|█████▌    | 167/300 [15:06<12:47,  5.77s/it]Running Inference:  56%|█████▌    | 168/300 [15:12<13:16,  6.03s/it]Running Inference:  56%|█████▋    | 169/300 [15:16<11:43,  5.37s/it]Running Inference:  57%|█████▋    | 170/300 [15:22<12:01,  5.55s/it]Running Inference:  57%|█████▋    | 171/300 [15:27<11:15,  5.24s/it]Running Inference:  57%|█████▋    | 172/300 [15:31<10:18,  4.83s/it]Running Inference:  58%|█████▊    | 173/300 [15:37<11:17,  5.34s/it]Running Inference:  58%|█████▊    | 174/300 [15:42<10:59,  5.23s/it]Running Inference:  58%|█████▊    | 175/300 [15:47<10:44,  5.16s/it]Running Inference:  59%|█████▊    | 176/300 [15:51<09:47,  4.74s/it]Running Inference:  59%|█████▉    | 177/300 [15:55<09:12,  4.49s/it]Running Inference:  59%|█████▉    | 178/300 [15:59<08:56,  4.39s/it]Running Inference:  60%|█████▉    | 179/300 [16:06<10:32,  5.22s/it]Running Inference:  60%|██████    | 180/300 [16:11<10:10,  5.09s/it]Running Inference:  60%|██████    | 181/300 [16:22<13:46,  6.94s/it]Running Inference:  61%|██████    | 182/300 [16:30<14:22,  7.31s/it]Running Inference:  61%|██████    | 183/300 [16:34<12:11,  6.26s/it]Running Inference:  61%|██████▏   | 184/300 [16:42<13:00,  6.73s/it]Running Inference:  62%|██████▏   | 185/300 [16:46<11:13,  5.86s/it]Running Inference:  62%|██████▏   | 186/300 [16:52<11:20,  5.97s/it]Running Inference:  62%|██████▏   | 187/300 [16:56<10:03,  5.34s/it]Running Inference:  63%|██████▎   | 188/300 [17:01<09:49,  5.26s/it]Running Inference:  63%|██████▎   | 189/300 [17:07<10:10,  5.50s/it]Running Inference:  63%|██████▎   | 190/300 [17:14<10:47,  5.89s/it]Running Inference:  64%|██████▎   | 191/300 [17:21<11:22,  6.26s/it]Running Inference:  64%|██████▍   | 192/300 [17:27<10:55,  6.07s/it]Running Inference:  64%|██████▍   | 193/300 [17:30<09:41,  5.44s/it]Running Inference:  65%|██████▍   | 194/300 [17:34<08:27,  4.79s/it]Running Inference:  65%|██████▌   | 195/300 [17:39<08:51,  5.06s/it]Running Inference:  65%|██████▌   | 196/300 [17:44<08:34,  4.95s/it]Running Inference:  66%|██████▌   | 197/300 [17:45<06:17,  3.66s/it]Running Inference:  66%|██████▌   | 198/300 [17:50<06:49,  4.02s/it]Running Inference:  66%|██████▋   | 199/300 [17:56<07:49,  4.65s/it]Running Inference:  67%|██████▋   | 200/300 [18:01<08:08,  4.89s/it]Running Inference:  67%|██████▋   | 201/300 [18:07<08:41,  5.26s/it]Running Inference:  67%|██████▋   | 202/300 [18:11<07:52,  4.82s/it]Running Inference:  68%|██████▊   | 203/300 [18:12<05:58,  3.70s/it]Running Inference:  68%|██████▊   | 204/300 [18:17<06:39,  4.16s/it]Running Inference:  68%|██████▊   | 205/300 [18:23<07:07,  4.50s/it]Running Inference:  69%|██████▊   | 206/300 [18:29<07:52,  5.02s/it]Running Inference:  69%|██████▉   | 207/300 [18:34<07:36,  4.91s/it]Running Inference:  69%|██████▉   | 208/300 [18:39<07:35,  4.95s/it]Running Inference:  70%|██████▉   | 209/300 [18:45<08:02,  5.30s/it]Running Inference:  70%|███████   | 210/300 [18:51<08:33,  5.70s/it]Running Inference:  70%|███████   | 211/300 [18:58<08:38,  5.83s/it]Running Inference:  71%|███████   | 212/300 [19:04<08:39,  5.91s/it]Running Inference:  71%|███████   | 213/300 [19:12<09:38,  6.65s/it]Running Inference:  71%|███████▏  | 214/300 [19:17<08:50,  6.16s/it]Running Inference:  72%|███████▏  | 215/300 [19:22<08:02,  5.68s/it]Running Inference:  72%|███████▏  | 216/300 [19:28<08:07,  5.80s/it]Running Inference:  72%|███████▏  | 217/300 [19:32<07:14,  5.23s/it]Running Inference:  73%|███████▎  | 218/300 [19:35<06:33,  4.80s/it]Running Inference:  73%|███████▎  | 219/300 [19:42<07:04,  5.24s/it]Running Inference:  73%|███████▎  | 220/300 [19:48<07:34,  5.68s/it]Running Inference:  74%|███████▎  | 221/300 [19:55<07:42,  5.86s/it]Running Inference:  74%|███████▍  | 222/300 [19:59<07:12,  5.54s/it]Running Inference:  74%|███████▍  | 223/300 [20:03<06:30,  5.07s/it]Running Inference:  75%|███████▍  | 224/300 [20:09<06:42,  5.30s/it]Running Inference:  75%|███████▌  | 225/300 [20:13<06:03,  4.85s/it]Running Inference:  75%|███████▌  | 226/300 [20:20<06:41,  5.42s/it]Running Inference:  76%|███████▌  | 227/300 [20:24<05:59,  4.93s/it]Running Inference:  76%|███████▌  | 228/300 [20:28<05:52,  4.89s/it]Running Inference:  76%|███████▋  | 229/300 [20:34<06:10,  5.22s/it]Running Inference:  77%|███████▋  | 230/300 [20:39<05:50,  5.01s/it]Running Inference:  77%|███████▋  | 231/300 [20:46<06:18,  5.49s/it]Running Inference:  77%|███████▋  | 232/300 [20:52<06:35,  5.82s/it]Running Inference:  78%|███████▊  | 233/300 [20:57<06:06,  5.47s/it]Running Inference:  78%|███████▊  | 234/300 [21:01<05:28,  4.97s/it]Running Inference:  78%|███████▊  | 235/300 [21:04<04:59,  4.60s/it]Running Inference:  79%|███████▊  | 236/300 [21:08<04:42,  4.41s/it]Running Inference:  79%|███████▉  | 237/300 [21:21<07:10,  6.83s/it]Running Inference:  79%|███████▉  | 238/300 [21:25<06:07,  5.93s/it]Running Inference:  80%|███████▉  | 239/300 [21:25<04:27,  4.38s/it]Running Inference:  80%|████████  | 240/300 [21:29<04:14,  4.25s/it]Running Inference:  80%|████████  | 241/300 [21:33<04:03,  4.13s/it]Running Inference:  81%|████████  | 242/300 [21:38<04:11,  4.34s/it]Running Inference:  81%|████████  | 243/300 [21:43<04:23,  4.62s/it]Running Inference:  81%|████████▏ | 244/300 [21:47<04:04,  4.36s/it]Running Inference:  82%|████████▏ | 245/300 [21:51<03:49,  4.17s/it]Running Inference:  82%|████████▏ | 246/300 [21:58<04:33,  5.07s/it]Running Inference:  82%|████████▏ | 247/300 [22:02<04:06,  4.65s/it]Running Inference:  83%|████████▎ | 248/300 [22:05<03:41,  4.26s/it]Running Inference:  83%|████████▎ | 249/300 [22:09<03:34,  4.21s/it]Running Inference:  83%|████████▎ | 250/300 [22:13<03:26,  4.12s/it]Running Inference:  84%|████████▎ | 251/300 [22:19<03:51,  4.73s/it]Running Inference:  84%|████████▍ | 252/300 [22:23<03:33,  4.45s/it]Running Inference:  84%|████████▍ | 253/300 [22:27<03:18,  4.23s/it]Running Inference:  85%|████████▍ | 254/300 [22:31<03:10,  4.14s/it]Running Inference:  85%|████████▌ | 255/300 [22:34<03:01,  4.02s/it]Running Inference:  85%|████████▌ | 256/300 [22:39<03:10,  4.33s/it]Running Inference:  86%|████████▌ | 257/300 [22:46<03:30,  4.89s/it]Running Inference:  86%|████████▌ | 258/300 [22:51<03:33,  5.07s/it]Running Inference:  86%|████████▋ | 259/300 [22:55<03:12,  4.69s/it]Running Inference:  87%|████████▋ | 260/300 [22:59<02:57,  4.43s/it]Running Inference:  87%|████████▋ | 261/300 [23:03<02:49,  4.36s/it]Running Inference:  87%|████████▋ | 262/300 [23:07<02:41,  4.24s/it]Running Inference:  88%|████████▊ | 263/300 [23:11<02:31,  4.09s/it]Running Inference:  88%|████████▊ | 264/300 [23:15<02:36,  4.34s/it]Running Inference:  88%|████████▊ | 265/300 [23:19<02:28,  4.23s/it]Running Inference:  89%|████████▊ | 266/300 [23:26<02:43,  4.81s/it]Running Inference:  89%|████████▉ | 267/300 [23:32<02:53,  5.25s/it]Running Inference:  89%|████████▉ | 268/300 [23:37<02:45,  5.17s/it]Running Inference:  90%|████████▉ | 269/300 [23:41<02:33,  4.96s/it]Running Inference:  90%|█████████ | 270/300 [23:45<02:18,  4.63s/it]Running Inference:  90%|█████████ | 271/300 [23:49<02:06,  4.38s/it]Running Inference:  91%|█████████ | 272/300 [23:54<02:10,  4.66s/it]Running Inference:  91%|█████████ | 273/300 [23:58<01:58,  4.41s/it]Running Inference:  91%|█████████▏| 274/300 [24:04<02:06,  4.87s/it]Running Inference:  92%|█████████▏| 275/300 [24:11<02:14,  5.36s/it]Running Inference:  92%|█████████▏| 276/300 [24:16<02:09,  5.40s/it]Running Inference:  92%|█████████▏| 277/300 [24:21<02:02,  5.34s/it]Running Inference:  93%|█████████▎| 278/300 [24:27<01:59,  5.44s/it]Running Inference:  93%|█████████▎| 279/300 [24:33<01:58,  5.65s/it]Running Inference:  93%|█████████▎| 280/300 [24:37<01:41,  5.08s/it]Running Inference:  94%|█████████▎| 281/300 [24:42<01:36,  5.06s/it]Running Inference:  94%|█████████▍| 282/300 [24:46<01:23,  4.66s/it]Running Inference:  94%|█████████▍| 283/300 [24:52<01:28,  5.18s/it]Running Inference:  95%|█████████▍| 284/300 [24:57<01:22,  5.15s/it]Running Inference:  95%|█████████▌| 285/300 [25:04<01:23,  5.56s/it]Running Inference:  95%|█████████▌| 286/300 [25:07<01:10,  5.07s/it]Running Inference:  96%|█████████▌| 287/300 [25:12<01:03,  4.90s/it]Running Inference:  96%|█████████▌| 288/300 [25:16<00:57,  4.79s/it]Running Inference:  96%|█████████▋| 289/300 [25:22<00:55,  5.01s/it]Running Inference:  97%|█████████▋| 290/300 [25:26<00:46,  4.63s/it]Running Inference:  97%|█████████▋| 291/300 [25:29<00:39,  4.35s/it]Running Inference:  97%|█████████▋| 292/300 [25:33<00:33,  4.16s/it]Running Inference:  98%|█████████▊| 293/300 [25:37<00:28,  4.06s/it]Running Inference:  98%|█████████▊| 294/300 [25:42<00:25,  4.23s/it]Running Inference:  98%|█████████▊| 295/300 [25:47<00:22,  4.48s/it]Running Inference:  99%|█████████▊| 296/300 [25:52<00:18,  4.71s/it]Running Inference:  99%|█████████▉| 297/300 [25:57<00:14,  4.71s/it]Running Inference:  99%|█████████▉| 298/300 [26:01<00:09,  4.75s/it]Running Inference: 100%|█████████▉| 299/300 [26:08<00:05,  5.34s/it]Running Inference: 100%|██████████| 300/300 [26:14<00:00,  5.54s/it]Running Inference: 100%|██████████| 300/300 [26:14<00:00,  5.25s/it]
2025-12-15 11:02:10,593 - INFO - Inference completed.
2025-12-15 11:02:10,604 - INFO - Results saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-15 11:02:10,604 - INFO - Calculating metrics for dataset: longbench
2025-12-15 11:02:10,605 - INFO - Metrics saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-15 11:02:10,606 - INFO - Metrics:
21.95
2025-12-15 11:02:10,607 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc_e, ratio=0.3) on GPU 1

----------------------------------------
Task: lcc_e | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 11:02:16,961 - INFO - Set deterministic seeds to 42
2025-12-15 11:02:16,962 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 11:02:16,962 - INFO - Starting evaluation run...
2025-12-15 11:02:16,962 - INFO - Output directory set to: longbenchresult
2025-12-15 11:02:16,962 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-15 11:02:16,962 - INFO - KV Press 'tova' setup.
2025-12-15 11:02:16,962 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 11:02:16,962 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.19it/s]
Device set to use cuda:0
2025-12-15 11:02:30,219 - INFO - Model pipeline loaded.
2025-12-15 11:02:30,219 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc_e)
2025-12-15 11:02:34,797 - INFO - Dataset loaded with 300 entries.
2025-12-15 11:02:34,797 - INFO - Dataset processed with 300 entries.
2025-12-15 11:02:34,830 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:07<35:39,  7.16s/it]Running Inference:   1%|          | 2/300 [00:12<29:31,  5.95s/it]Running Inference:   1%|          | 3/300 [00:16<25:56,  5.24s/it]Running Inference:   1%|▏         | 4/300 [00:19<20:25,  4.14s/it]Running Inference:   2%|▏         | 5/300 [00:24<21:47,  4.43s/it]Running Inference:   2%|▏         | 6/300 [00:27<20:38,  4.21s/it]Running Inference:   2%|▏         | 7/300 [00:32<21:23,  4.38s/it]Running Inference:   3%|▎         | 8/300 [00:38<23:59,  4.93s/it]Running Inference:   3%|▎         | 9/300 [00:42<22:06,  4.56s/it]Running Inference:   3%|▎         | 10/300 [00:48<24:07,  4.99s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [01:00<35:14,  7.32s/it]Running Inference:   4%|▍         | 12/300 [01:05<30:20,  6.32s/it]Running Inference:   4%|▍         | 13/300 [01:11<29:45,  6.22s/it]Running Inference:   5%|▍         | 14/300 [01:14<26:20,  5.53s/it]Running Inference:   5%|▌         | 15/300 [01:20<26:40,  5.62s/it]Running Inference:   5%|▌         | 16/300 [01:24<24:00,  5.07s/it]Running Inference:   6%|▌         | 17/300 [01:30<24:52,  5.27s/it]Running Inference:   6%|▌         | 18/300 [01:34<22:54,  4.87s/it]Running Inference:   6%|▋         | 19/300 [01:38<21:19,  4.55s/it]Running Inference:   7%|▋         | 20/300 [01:41<19:31,  4.19s/it]Running Inference:   7%|▋         | 21/300 [01:45<18:58,  4.08s/it]Running Inference:   7%|▋         | 22/300 [01:53<24:21,  5.26s/it]Running Inference:   8%|▊         | 23/300 [01:58<24:10,  5.24s/it]Running Inference:   8%|▊         | 24/300 [02:04<25:34,  5.56s/it]Running Inference:   8%|▊         | 25/300 [02:11<27:14,  5.95s/it]Running Inference:   9%|▊         | 26/300 [02:18<28:51,  6.32s/it]Running Inference:   9%|▉         | 27/300 [02:21<23:42,  5.21s/it]Running Inference:   9%|▉         | 28/300 [02:25<21:43,  4.79s/it]Running Inference:  10%|▉         | 29/300 [02:29<20:20,  4.50s/it]Running Inference:  10%|█         | 30/300 [02:35<23:23,  5.20s/it]Running Inference:  10%|█         | 31/300 [02:42<25:56,  5.79s/it]Running Inference:  11%|█         | 32/300 [02:49<27:03,  6.06s/it]Running Inference:  11%|█         | 33/300 [02:56<28:02,  6.30s/it]Running Inference:  11%|█▏        | 34/300 [03:02<27:20,  6.17s/it]Running Inference:  12%|█▏        | 35/300 [03:07<25:40,  5.81s/it]Running Inference:  12%|█▏        | 36/300 [03:12<24:27,  5.56s/it]Running Inference:  12%|█▏        | 37/300 [03:16<22:11,  5.06s/it]Running Inference:  13%|█▎        | 38/300 [03:20<20:43,  4.74s/it]Running Inference:  13%|█▎        | 39/300 [03:25<21:32,  4.95s/it]Running Inference:  13%|█▎        | 40/300 [03:31<22:12,  5.13s/it]Running Inference:  14%|█▎        | 41/300 [03:36<21:51,  5.06s/it]Running Inference:  14%|█▍        | 42/300 [03:41<21:29,  5.00s/it]Running Inference:  14%|█▍        | 43/300 [03:47<23:37,  5.52s/it]Running Inference:  15%|█▍        | 44/300 [03:52<23:02,  5.40s/it]Running Inference:  15%|█▌        | 45/300 [03:56<20:56,  4.93s/it]Running Inference:  15%|█▌        | 46/300 [04:01<20:58,  4.95s/it]Running Inference:  16%|█▌        | 47/300 [04:08<22:40,  5.38s/it]Running Inference:  16%|█▌        | 48/300 [04:14<23:36,  5.62s/it]Running Inference:  16%|█▋        | 49/300 [04:23<27:28,  6.57s/it]Running Inference:  17%|█▋        | 50/300 [04:26<24:03,  5.78s/it]Running Inference:  17%|█▋        | 51/300 [04:34<25:59,  6.26s/it]Running Inference:  17%|█▋        | 52/300 [04:38<22:58,  5.56s/it]Running Inference:  18%|█▊        | 53/300 [04:42<20:54,  5.08s/it]Running Inference:  18%|█▊        | 54/300 [04:48<22:44,  5.55s/it]Running Inference:  18%|█▊        | 55/300 [04:52<20:35,  5.04s/it]Running Inference:  19%|█▊        | 56/300 [04:57<20:35,  5.06s/it]Running Inference:  19%|█▉        | 57/300 [05:04<22:01,  5.44s/it]Running Inference:  19%|█▉        | 58/300 [05:10<23:29,  5.82s/it]Running Inference:  20%|█▉        | 59/300 [05:15<22:23,  5.57s/it]Running Inference:  20%|██        | 60/300 [05:23<25:19,  6.33s/it]Running Inference:  20%|██        | 61/300 [05:29<24:14,  6.09s/it]Running Inference:  21%|██        | 62/300 [05:37<26:56,  6.79s/it]Running Inference:  21%|██        | 63/300 [05:41<23:20,  5.91s/it]Running Inference:  21%|██▏       | 64/300 [05:48<23:40,  6.02s/it]Running Inference:  22%|██▏       | 65/300 [05:50<18:52,  4.82s/it]Running Inference:  22%|██▏       | 66/300 [05:53<17:36,  4.52s/it]Running Inference:  22%|██▏       | 67/300 [06:02<22:18,  5.75s/it]Running Inference:  23%|██▎       | 68/300 [06:08<21:58,  5.68s/it]Running Inference:  23%|██▎       | 69/300 [06:13<21:07,  5.49s/it]Running Inference:  23%|██▎       | 70/300 [06:20<22:58,  5.99s/it]Running Inference:  24%|██▎       | 71/300 [06:26<22:43,  5.95s/it]Running Inference:  24%|██▍       | 72/300 [06:30<21:07,  5.56s/it]Running Inference:  24%|██▍       | 73/300 [06:34<19:06,  5.05s/it]Running Inference:  25%|██▍       | 74/300 [06:40<20:14,  5.38s/it]Running Inference:  25%|██▌       | 75/300 [06:46<20:53,  5.57s/it]Running Inference:  25%|██▌       | 76/300 [06:57<26:28,  7.09s/it]Running Inference:  26%|██▌       | 77/300 [07:04<25:54,  6.97s/it]Running Inference:  26%|██▌       | 78/300 [07:10<24:52,  6.72s/it]Running Inference:  26%|██▋       | 79/300 [07:16<24:25,  6.63s/it]Running Inference:  27%|██▋       | 80/300 [07:20<21:13,  5.79s/it]Running Inference:  27%|██▋       | 81/300 [07:24<18:57,  5.19s/it]Running Inference:  27%|██▋       | 82/300 [07:28<17:36,  4.85s/it]Running Inference:  28%|██▊       | 83/300 [07:34<19:16,  5.33s/it]Running Inference:  28%|██▊       | 84/300 [07:38<17:53,  4.97s/it]Running Inference:  28%|██▊       | 85/300 [07:45<19:25,  5.42s/it]Running Inference:  29%|██▊       | 86/300 [07:52<21:00,  5.89s/it]Running Inference:  29%|██▉       | 87/300 [07:58<21:21,  6.01s/it]Running Inference:  29%|██▉       | 88/300 [08:02<19:14,  5.45s/it]Running Inference:  30%|██▉       | 89/300 [08:06<17:30,  4.98s/it]Running Inference:  30%|███       | 90/300 [08:10<16:16,  4.65s/it]Running Inference:  30%|███       | 91/300 [08:16<17:56,  5.15s/it]Running Inference:  31%|███       | 92/300 [08:22<18:34,  5.36s/it]Running Inference:  31%|███       | 93/300 [08:30<20:38,  5.98s/it]Running Inference:  31%|███▏      | 94/300 [08:38<22:59,  6.70s/it]Running Inference:  32%|███▏      | 95/300 [08:46<23:42,  6.94s/it]Running Inference:  32%|███▏      | 96/300 [08:49<20:23,  6.00s/it]Running Inference:  32%|███▏      | 97/300 [08:57<21:30,  6.36s/it]Running Inference:  33%|███▎      | 98/300 [09:01<19:45,  5.87s/it]Running Inference:  33%|███▎      | 99/300 [09:05<17:49,  5.32s/it]Running Inference:  33%|███▎      | 100/300 [09:08<15:24,  4.62s/it]Running Inference:  34%|███▎      | 101/300 [09:12<14:30,  4.38s/it]Running Inference:  34%|███▍      | 102/300 [09:16<13:50,  4.19s/it]Running Inference:  34%|███▍      | 103/300 [09:21<14:51,  4.53s/it]Running Inference:  35%|███▍      | 104/300 [09:28<17:07,  5.24s/it]Running Inference:  35%|███▌      | 105/300 [09:32<15:40,  4.82s/it]Running Inference:  35%|███▌      | 106/300 [09:36<14:38,  4.53s/it]Running Inference:  36%|███▌      | 107/300 [09:40<14:36,  4.54s/it]Running Inference:  36%|███▌      | 108/300 [09:44<13:47,  4.31s/it]Running Inference:  36%|███▋      | 109/300 [09:49<14:44,  4.63s/it]Running Inference:  37%|███▋      | 110/300 [09:55<15:50,  5.00s/it]Running Inference:  37%|███▋      | 111/300 [10:01<16:03,  5.10s/it]Running Inference:  37%|███▋      | 112/300 [10:04<14:45,  4.71s/it]Running Inference:  38%|███▊      | 113/300 [10:10<15:37,  5.02s/it]Running Inference:  38%|███▊      | 114/300 [10:14<14:23,  4.64s/it]Running Inference:  38%|███▊      | 115/300 [10:20<15:45,  5.11s/it]Running Inference:  39%|███▊      | 116/300 [10:24<14:31,  4.73s/it]Running Inference:  39%|███▉      | 117/300 [10:29<14:48,  4.86s/it]Running Inference:  39%|███▉      | 118/300 [10:33<13:53,  4.58s/it]Running Inference:  40%|███▉      | 119/300 [10:37<13:06,  4.35s/it]Running Inference:  40%|████      | 120/300 [10:39<11:18,  3.77s/it]Running Inference:  40%|████      | 121/300 [10:44<11:53,  3.99s/it]Running Inference:  41%|████      | 122/300 [10:49<13:03,  4.40s/it]Running Inference:  41%|████      | 123/300 [10:55<14:10,  4.81s/it]Running Inference:  41%|████▏     | 124/300 [11:02<15:41,  5.35s/it]Running Inference:  42%|████▏     | 125/300 [11:08<16:29,  5.65s/it]Running Inference:  42%|████▏     | 126/300 [11:16<18:09,  6.26s/it]Running Inference:  42%|████▏     | 127/300 [11:20<16:29,  5.72s/it]Running Inference:  43%|████▎     | 128/300 [11:23<13:43,  4.78s/it]Running Inference:  43%|████▎     | 129/300 [11:27<13:23,  4.70s/it]Running Inference:  43%|████▎     | 130/300 [11:33<13:52,  4.90s/it]Running Inference:  44%|████▎     | 131/300 [11:37<13:48,  4.90s/it]Running Inference:  44%|████▍     | 132/300 [11:44<15:15,  5.45s/it]Running Inference:  44%|████▍     | 133/300 [11:49<14:42,  5.28s/it]Running Inference:  45%|████▍     | 134/300 [11:54<14:08,  5.11s/it]Running Inference:  45%|████▌     | 135/300 [11:58<12:59,  4.72s/it]Running Inference:  45%|████▌     | 136/300 [12:03<13:50,  5.06s/it]Running Inference:  46%|████▌     | 137/300 [12:07<12:41,  4.67s/it]Running Inference:  46%|████▌     | 138/300 [12:13<13:19,  4.94s/it]Running Inference:  46%|████▋     | 139/300 [12:17<12:44,  4.75s/it]Running Inference:  47%|████▋     | 140/300 [12:22<12:32,  4.70s/it]Running Inference:  47%|████▋     | 141/300 [12:24<10:47,  4.07s/it]Running Inference:  47%|████▋     | 142/300 [12:31<12:48,  4.86s/it]Running Inference:  48%|████▊     | 143/300 [12:35<12:00,  4.59s/it]Running Inference:  48%|████▊     | 144/300 [12:42<13:46,  5.30s/it]Running Inference:  48%|████▊     | 145/300 [12:47<13:35,  5.26s/it]Running Inference:  49%|████▊     | 146/300 [12:51<12:28,  4.86s/it]Running Inference:  49%|████▉     | 147/300 [12:57<13:06,  5.14s/it]Running Inference:  49%|████▉     | 148/300 [13:02<13:02,  5.15s/it]Running Inference:  50%|████▉     | 149/300 [13:06<12:04,  4.80s/it]Running Inference:  50%|█████     | 150/300 [13:10<11:13,  4.49s/it]Running Inference:  50%|█████     | 151/300 [13:15<11:49,  4.76s/it]Running Inference:  51%|█████     | 152/300 [13:21<12:18,  4.99s/it]Running Inference:  51%|█████     | 153/300 [13:26<12:20,  5.04s/it]Running Inference:  51%|█████▏    | 154/300 [13:30<11:33,  4.75s/it]Running Inference:  52%|█████▏    | 155/300 [13:36<12:25,  5.14s/it]Running Inference:  52%|█████▏    | 156/300 [13:41<12:18,  5.13s/it]Running Inference:  52%|█████▏    | 157/300 [13:45<11:20,  4.76s/it]Running Inference:  53%|█████▎    | 158/300 [14:00<18:23,  7.77s/it]Running Inference:  53%|█████▎    | 159/300 [14:04<16:01,  6.82s/it]Running Inference:  53%|█████▎    | 160/300 [14:13<17:33,  7.52s/it]Running Inference:  54%|█████▎    | 161/300 [14:20<16:29,  7.12s/it]Running Inference:  54%|█████▍    | 162/300 [14:29<17:37,  7.66s/it]Running Inference:  54%|█████▍    | 163/300 [14:34<16:14,  7.12s/it]Running Inference:  55%|█████▍    | 164/300 [14:40<15:15,  6.73s/it]Running Inference:  55%|█████▌    | 165/300 [14:49<16:36,  7.38s/it]Running Inference:  55%|█████▌    | 166/300 [14:54<14:45,  6.61s/it]Running Inference:  56%|█████▌    | 167/300 [14:58<12:47,  5.77s/it]Running Inference:  56%|█████▌    | 168/300 [15:04<13:15,  6.03s/it]Running Inference:  56%|█████▋    | 169/300 [15:08<11:44,  5.38s/it]Running Inference:  57%|█████▋    | 170/300 [15:14<12:02,  5.56s/it]Running Inference:  57%|█████▋    | 171/300 [15:19<11:19,  5.27s/it]Running Inference:  57%|█████▋    | 172/300 [15:23<10:22,  4.87s/it]Running Inference:  58%|█████▊    | 173/300 [15:29<11:20,  5.36s/it]Running Inference:  58%|█████▊    | 174/300 [15:34<11:01,  5.25s/it]Running Inference:  58%|█████▊    | 175/300 [15:39<10:47,  5.18s/it]Running Inference:  59%|█████▊    | 176/300 [15:43<09:49,  4.76s/it]Running Inference:  59%|█████▉    | 177/300 [15:47<09:15,  4.51s/it]Running Inference:  59%|█████▉    | 178/300 [15:51<09:00,  4.43s/it]Running Inference:  60%|█████▉    | 179/300 [15:58<10:33,  5.24s/it]Running Inference:  60%|██████    | 180/300 [16:03<10:12,  5.11s/it]Running Inference:  60%|██████    | 181/300 [16:14<13:41,  6.91s/it]Running Inference:  61%|██████    | 182/300 [16:19<12:20,  6.28s/it]Running Inference:  61%|██████    | 183/300 [16:23<10:48,  5.54s/it]Running Inference:  61%|██████▏   | 184/300 [16:31<11:59,  6.20s/it]Running Inference:  62%|██████▏   | 185/300 [16:34<10:32,  5.50s/it]Running Inference:  62%|██████▏   | 186/300 [16:41<10:52,  5.73s/it]Running Inference:  62%|██████▏   | 187/300 [16:45<09:46,  5.19s/it]Running Inference:  63%|██████▎   | 188/300 [16:50<09:38,  5.16s/it]Running Inference:  63%|██████▎   | 189/300 [16:56<10:03,  5.43s/it]Running Inference:  63%|██████▎   | 190/300 [17:03<10:41,  5.83s/it]Running Inference:  64%|██████▎   | 191/300 [17:10<11:15,  6.20s/it]Running Inference:  64%|██████▍   | 192/300 [17:15<10:51,  6.03s/it]Running Inference:  64%|██████▍   | 193/300 [17:16<08:01,  4.50s/it]Running Inference:  65%|██████▍   | 194/300 [17:20<07:31,  4.26s/it]Running Inference:  65%|██████▌   | 195/300 [17:26<08:15,  4.72s/it]Running Inference:  65%|██████▌   | 196/300 [17:30<08:11,  4.73s/it]Running Inference:  66%|██████▌   | 197/300 [17:31<06:01,  3.51s/it]Running Inference:  66%|██████▌   | 198/300 [17:36<06:40,  3.93s/it]Running Inference:  66%|██████▋   | 199/300 [17:42<07:44,  4.60s/it]Running Inference:  67%|██████▋   | 200/300 [17:48<08:06,  4.86s/it]Running Inference:  67%|██████▋   | 201/300 [17:54<08:41,  5.26s/it]Running Inference:  67%|██████▋   | 202/300 [17:58<07:53,  4.83s/it]Running Inference:  68%|██████▊   | 203/300 [17:59<05:59,  3.71s/it]Running Inference:  68%|██████▊   | 204/300 [18:02<05:31,  3.45s/it]Running Inference:  68%|██████▊   | 205/300 [18:07<06:21,  4.01s/it]Running Inference:  69%|██████▊   | 206/300 [18:13<07:21,  4.69s/it]Running Inference:  69%|██████▉   | 207/300 [18:18<07:15,  4.69s/it]Running Inference:  69%|██████▉   | 208/300 [18:23<07:22,  4.81s/it]Running Inference:  70%|██████▉   | 209/300 [18:29<07:53,  5.21s/it]Running Inference:  70%|███████   | 210/300 [18:36<08:27,  5.64s/it]Running Inference:  70%|███████   | 211/300 [18:42<08:36,  5.80s/it]Running Inference:  71%|███████   | 212/300 [18:48<08:39,  5.90s/it]Running Inference:  71%|███████   | 213/300 [18:56<09:34,  6.61s/it]Running Inference:  71%|███████▏  | 214/300 [19:01<08:47,  6.14s/it]Running Inference:  72%|███████▏  | 215/300 [19:07<08:21,  5.90s/it]Running Inference:  72%|███████▏  | 216/300 [19:13<08:21,  5.97s/it]Running Inference:  72%|███████▏  | 217/300 [19:17<07:24,  5.36s/it]Running Inference:  73%|███████▎  | 218/300 [19:21<06:41,  4.89s/it]Running Inference:  73%|███████▎  | 219/300 [19:27<07:10,  5.32s/it]Running Inference:  73%|███████▎  | 220/300 [19:34<07:39,  5.75s/it]Running Inference:  74%|███████▎  | 221/300 [19:40<07:46,  5.91s/it]Running Inference:  74%|███████▍  | 222/300 [19:45<07:16,  5.60s/it]Running Inference:  74%|███████▍  | 223/300 [19:49<06:34,  5.12s/it]Running Inference:  75%|███████▍  | 224/300 [19:55<06:46,  5.34s/it]Running Inference:  75%|███████▌  | 225/300 [19:59<06:06,  4.89s/it]Running Inference:  75%|███████▌  | 226/300 [20:05<06:42,  5.44s/it]Running Inference:  76%|███████▌  | 227/300 [20:09<06:01,  4.95s/it]Running Inference:  76%|███████▌  | 228/300 [20:13<05:29,  4.57s/it]Running Inference:  76%|███████▋  | 229/300 [20:19<05:55,  5.01s/it]Running Inference:  77%|███████▋  | 230/300 [20:23<05:41,  4.87s/it]Running Inference:  77%|███████▋  | 231/300 [20:30<06:12,  5.39s/it]Running Inference:  77%|███████▋  | 232/300 [20:37<06:30,  5.75s/it]Running Inference:  78%|███████▊  | 233/300 [20:41<06:03,  5.43s/it]Running Inference:  78%|███████▊  | 234/300 [20:45<05:26,  4.95s/it]Running Inference:  78%|███████▊  | 235/300 [20:49<04:58,  4.60s/it]Running Inference:  79%|███████▊  | 236/300 [20:53<04:42,  4.41s/it]Running Inference:  79%|███████▉  | 237/300 [21:05<07:06,  6.77s/it]Running Inference:  79%|███████▉  | 238/300 [21:09<06:05,  5.90s/it]Running Inference:  80%|███████▉  | 239/300 [21:13<05:22,  5.29s/it]Running Inference:  80%|████████  | 240/300 [21:17<04:53,  4.90s/it]Running Inference:  80%|████████  | 241/300 [21:21<04:30,  4.59s/it]Running Inference:  81%|████████  | 242/300 [21:26<04:30,  4.67s/it]Running Inference:  81%|████████  | 243/300 [21:31<04:37,  4.86s/it]Running Inference:  81%|████████▏ | 244/300 [21:35<04:13,  4.53s/it]Running Inference:  82%|████████▏ | 245/300 [21:38<03:56,  4.30s/it]Running Inference:  82%|████████▏ | 246/300 [21:45<04:37,  5.14s/it]Running Inference:  82%|████████▏ | 247/300 [21:49<04:09,  4.71s/it]Running Inference:  83%|████████▎ | 248/300 [21:55<04:21,  5.03s/it]Running Inference:  83%|████████▎ | 249/300 [21:59<04:02,  4.75s/it]Running Inference:  83%|████████▎ | 250/300 [22:03<03:45,  4.51s/it]Running Inference:  84%|████████▎ | 251/300 [22:09<04:05,  5.01s/it]Running Inference:  84%|████████▍ | 252/300 [22:13<03:43,  4.66s/it]Running Inference:  84%|████████▍ | 253/300 [22:17<03:26,  4.39s/it]Running Inference:  85%|████████▍ | 254/300 [22:21<03:15,  4.25s/it]Running Inference:  85%|████████▌ | 255/300 [22:24<03:04,  4.11s/it]Running Inference:  85%|████████▌ | 256/300 [22:30<03:13,  4.40s/it]Running Inference:  86%|████████▌ | 257/300 [22:36<03:32,  4.95s/it]Running Inference:  86%|████████▌ | 258/300 [22:41<03:35,  5.12s/it]Running Inference:  86%|████████▋ | 259/300 [22:45<03:14,  4.73s/it]Running Inference:  87%|████████▋ | 260/300 [22:49<02:58,  4.47s/it]Running Inference:  87%|████████▋ | 261/300 [22:53<02:50,  4.38s/it]Running Inference:  87%|████████▋ | 262/300 [22:57<02:42,  4.27s/it]Running Inference:  88%|████████▊ | 263/300 [23:01<02:32,  4.13s/it]Running Inference:  88%|████████▊ | 264/300 [23:06<02:37,  4.38s/it]Running Inference:  88%|████████▊ | 265/300 [23:10<02:29,  4.27s/it]Running Inference:  89%|████████▊ | 266/300 [23:16<02:44,  4.84s/it]Running Inference:  89%|████████▉ | 267/300 [23:22<02:54,  5.28s/it]Running Inference:  89%|████████▉ | 268/300 [23:27<02:46,  5.20s/it]Running Inference:  90%|████████▉ | 269/300 [23:32<02:33,  4.94s/it]Running Inference:  90%|█████████ | 270/300 [23:36<02:18,  4.63s/it]Running Inference:  90%|█████████ | 271/300 [23:39<02:06,  4.38s/it]Running Inference:  91%|█████████ | 272/300 [23:45<02:10,  4.67s/it]Running Inference:  91%|█████████ | 273/300 [23:49<01:59,  4.42s/it]Running Inference:  91%|█████████▏| 274/300 [23:55<02:07,  4.89s/it]Running Inference:  92%|█████████▏| 275/300 [24:01<02:14,  5.37s/it]Running Inference:  92%|█████████▏| 276/300 [24:07<02:09,  5.41s/it]Running Inference:  92%|█████████▏| 277/300 [24:12<02:03,  5.36s/it]Running Inference:  93%|█████████▎| 278/300 [24:18<02:00,  5.47s/it]Running Inference:  93%|█████████▎| 279/300 [24:24<01:59,  5.68s/it]Running Inference:  93%|█████████▎| 280/300 [24:28<01:42,  5.11s/it]Running Inference:  94%|█████████▎| 281/300 [24:33<01:36,  5.09s/it]Running Inference:  94%|█████████▍| 282/300 [24:36<01:24,  4.70s/it]Running Inference:  94%|█████████▍| 283/300 [24:41<01:17,  4.55s/it]Running Inference:  95%|█████████▍| 284/300 [24:46<01:15,  4.72s/it]Running Inference:  95%|█████████▌| 285/300 [24:52<01:18,  5.26s/it]Running Inference:  95%|█████████▌| 286/300 [24:56<01:08,  4.87s/it]Running Inference:  96%|█████████▌| 287/300 [25:01<01:01,  4.77s/it]Running Inference:  96%|█████████▌| 288/300 [25:05<00:56,  4.71s/it]Running Inference:  96%|█████████▋| 289/300 [25:11<00:54,  4.97s/it]Running Inference:  97%|█████████▋| 290/300 [25:15<00:46,  4.61s/it]Running Inference:  97%|█████████▋| 291/300 [25:18<00:39,  4.35s/it]Running Inference:  97%|█████████▋| 292/300 [25:22<00:33,  4.18s/it]Running Inference:  98%|█████████▊| 293/300 [25:26<00:28,  4.09s/it]Running Inference:  98%|█████████▊| 294/300 [25:31<00:25,  4.26s/it]Running Inference:  98%|█████████▊| 295/300 [25:36<00:22,  4.52s/it]Running Inference:  99%|█████████▊| 296/300 [25:41<00:18,  4.74s/it]Running Inference:  99%|█████████▉| 297/300 [25:46<00:14,  4.74s/it]Running Inference:  99%|█████████▉| 298/300 [25:51<00:09,  4.78s/it]Running Inference: 100%|█████████▉| 299/300 [25:57<00:05,  5.35s/it]Running Inference: 100%|██████████| 300/300 [26:03<00:00,  5.57s/it]Running Inference: 100%|██████████| 300/300 [26:03<00:00,  5.21s/it]
2025-12-15 11:28:38,766 - INFO - Inference completed.
2025-12-15 11:28:38,777 - INFO - Results saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-15 11:28:38,777 - INFO - Calculating metrics for dataset: longbench
2025-12-15 11:28:38,778 - INFO - Metrics saved to longbenchresult/longbench__lcc_e__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-15 11:28:38,778 - INFO - Metrics:
24.21
2025-12-15 11:28:38,780 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=lcc_e, ratio=0.5) on GPU 1


========================================
LongBench Task: multi_news_e
========================================
----------------------------------------
Task: multi_news_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 11:28:45,078 - INFO - Set deterministic seeds to 42
2025-12-15 11:28:45,079 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 11:28:45,079 - INFO - Starting evaluation run...
2025-12-15 11:28:45,079 - INFO - Output directory set to: longbenchresult
2025-12-15 11:28:45,079 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-15 11:28:45,079 - INFO - KV Press 'tova' setup.
2025-12-15 11:28:45,079 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 11:28:45,079 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 107.02it/s]
Device set to use cuda:0
2025-12-15 11:29:01,709 - INFO - Model pipeline loaded.
2025-12-15 11:29:01,709 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news_e)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 294 examples [00:00, 2505.38 examples/s]Generating test split: 294 examples [00:00, 2483.09 examples/s]
2025-12-15 11:29:11,393 - INFO - Dataset loaded with 294 entries.
2025-12-15 11:29:11,393 - INFO - Dataset processed with 294 entries.
2025-12-15 11:29:11,427 - INFO - Starting inference...
Running Inference:   0%|          | 0/294 [00:00<?, ?it/s]Running Inference:   0%|          | 1/294 [00:23<1:55:57, 23.75s/it]Running Inference:   1%|          | 2/294 [00:47<1:55:45, 23.79s/it]Running Inference:   1%|          | 3/294 [01:10<1:53:08, 23.33s/it]Running Inference:   1%|▏         | 4/294 [01:35<1:55:32, 23.90s/it]Running Inference:   2%|▏         | 5/294 [01:58<1:53:27, 23.56s/it]Running Inference:   2%|▏         | 6/294 [02:20<1:51:47, 23.29s/it]Running Inference:   2%|▏         | 7/294 [02:43<1:50:43, 23.15s/it]Running Inference:   3%|▎         | 8/294 [03:07<1:51:57, 23.49s/it]Running Inference:   3%|▎         | 9/294 [03:31<1:52:12, 23.62s/it]Running Inference:   3%|▎         | 10/294 [03:54<1:51:05, 23.47s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/294 [04:25<2:01:28, 25.76s/it]Running Inference:   4%|▍         | 12/294 [04:49<1:57:29, 25.00s/it]Running Inference:   4%|▍         | 13/294 [05:11<1:53:40, 24.27s/it]Running Inference:   5%|▍         | 14/294 [05:34<1:50:54, 23.77s/it]Running Inference:   5%|▌         | 15/294 [05:57<1:49:37, 23.58s/it]Running Inference:   5%|▌         | 16/294 [06:20<1:48:37, 23.44s/it]Running Inference:   6%|▌         | 17/294 [06:42<1:45:22, 22.83s/it]Running Inference:   6%|▌         | 18/294 [07:05<1:45:37, 22.96s/it]Running Inference:   6%|▋         | 19/294 [07:28<1:44:55, 22.89s/it]Running Inference:   7%|▋         | 20/294 [07:51<1:44:45, 22.94s/it]Running Inference:   7%|▋         | 21/294 [08:13<1:44:08, 22.89s/it]Running Inference:   7%|▋         | 22/294 [08:36<1:43:58, 22.93s/it]Running Inference:   8%|▊         | 23/294 [09:00<1:43:56, 23.01s/it]Running Inference:   8%|▊         | 24/294 [09:23<1:43:27, 22.99s/it]Running Inference:   9%|▊         | 25/294 [09:46<1:43:36, 23.11s/it]Running Inference:   9%|▉         | 26/294 [10:08<1:42:26, 22.94s/it]Running Inference:   9%|▉         | 27/294 [10:32<1:42:57, 23.14s/it]Running Inference:  10%|▉         | 28/294 [10:55<1:42:30, 23.12s/it]Running Inference:  10%|▉         | 29/294 [11:19<1:43:10, 23.36s/it]Running Inference:  10%|█         | 30/294 [11:42<1:42:17, 23.25s/it]Running Inference:  11%|█         | 31/294 [12:07<1:44:02, 23.74s/it]Running Inference:  11%|█         | 32/294 [12:30<1:42:37, 23.50s/it]Running Inference:  11%|█         | 33/294 [12:52<1:40:56, 23.21s/it]Running Inference:  12%|█▏        | 34/294 [13:15<1:40:13, 23.13s/it]Running Inference:  12%|█▏        | 35/294 [13:38<1:39:09, 22.97s/it]Running Inference:  12%|█▏        | 36/294 [14:01<1:38:49, 22.98s/it]Running Inference:  13%|█▎        | 37/294 [14:24<1:39:10, 23.15s/it]Running Inference:  13%|█▎        | 38/294 [14:47<1:38:30, 23.09s/it]Running Inference:  13%|█▎        | 39/294 [15:14<1:43:07, 24.26s/it]Running Inference:  14%|█▎        | 40/294 [15:38<1:42:26, 24.20s/it]Running Inference:  14%|█▍        | 41/294 [16:03<1:42:16, 24.26s/it]Running Inference:  14%|█▍        | 42/294 [16:27<1:41:17, 24.12s/it]Running Inference:  15%|█▍        | 43/294 [16:50<1:39:20, 23.75s/it]Running Inference:  15%|█▍        | 44/294 [17:12<1:37:22, 23.37s/it]Running Inference:  15%|█▌        | 45/294 [17:35<1:35:53, 23.11s/it]Running Inference:  16%|█▌        | 46/294 [17:58<1:35:26, 23.09s/it]Running Inference:  16%|█▌        | 47/294 [18:20<1:34:12, 22.89s/it]Running Inference:  16%|█▋        | 48/294 [18:45<1:36:03, 23.43s/it]Running Inference:  17%|█▋        | 49/294 [19:09<1:36:11, 23.56s/it]Running Inference:  17%|█▋        | 50/294 [19:31<1:34:25, 23.22s/it]Running Inference:  17%|█▋        | 51/294 [19:55<1:34:24, 23.31s/it]Running Inference:  18%|█▊        | 52/294 [20:19<1:36:03, 23.82s/it]Running Inference:  18%|█▊        | 53/294 [20:42<1:34:14, 23.46s/it]Running Inference:  18%|█▊        | 54/294 [21:05<1:33:42, 23.43s/it]Running Inference:  19%|█▊        | 55/294 [21:28<1:32:11, 23.14s/it]Running Inference:  19%|█▉        | 56/294 [21:50<1:30:57, 22.93s/it]Running Inference:  19%|█▉        | 57/294 [22:14<1:30:52, 23.00s/it]Running Inference:  20%|█▉        | 58/294 [22:36<1:30:11, 22.93s/it]Running Inference:  20%|██        | 59/294 [23:00<1:30:40, 23.15s/it]Running Inference:  20%|██        | 60/294 [23:24<1:31:13, 23.39s/it]Running Inference:  21%|██        | 61/294 [23:48<1:31:10, 23.48s/it]Running Inference:  21%|██        | 62/294 [24:10<1:29:56, 23.26s/it]Running Inference:  21%|██▏       | 63/294 [24:18<1:11:29, 18.57s/it]Running Inference:  22%|██▏       | 64/294 [24:41<1:16:14, 19.89s/it]Running Inference:  22%|██▏       | 65/294 [24:58<1:12:55, 19.11s/it]Running Inference:  22%|██▏       | 66/294 [25:22<1:18:20, 20.61s/it]Running Inference:  23%|██▎       | 67/294 [25:45<1:20:39, 21.32s/it]Running Inference:  23%|██▎       | 68/294 [26:08<1:21:38, 21.67s/it]Running Inference:  23%|██▎       | 69/294 [26:31<1:22:39, 22.04s/it]Running Inference:  24%|██▍       | 70/294 [26:53<1:22:39, 22.14s/it]Running Inference:  24%|██▍       | 71/294 [27:16<1:23:26, 22.45s/it]Running Inference:  24%|██▍       | 72/294 [27:39<1:23:10, 22.48s/it]Running Inference:  25%|██▍       | 73/294 [28:03<1:24:18, 22.89s/it]Running Inference:  25%|██▌       | 74/294 [28:25<1:23:32, 22.78s/it]Running Inference:  26%|██▌       | 75/294 [28:48<1:22:52, 22.71s/it]Running Inference:  26%|██▌       | 76/294 [29:10<1:22:25, 22.69s/it]Running Inference:  26%|██▌       | 77/294 [29:33<1:22:15, 22.74s/it]Running Inference:  27%|██▋       | 78/294 [29:56<1:22:18, 22.86s/it]Running Inference:  27%|██▋       | 79/294 [30:20<1:22:13, 22.94s/it]Running Inference:  27%|██▋       | 80/294 [30:43<1:22:36, 23.16s/it]Running Inference:  28%|██▊       | 81/294 [31:07<1:23:24, 23.49s/it]Running Inference:  28%|██▊       | 82/294 [31:30<1:22:25, 23.33s/it]Running Inference:  28%|██▊       | 83/294 [31:54<1:22:22, 23.43s/it]Running Inference:  29%|██▊       | 84/294 [32:16<1:20:55, 23.12s/it]Running Inference:  29%|██▉       | 85/294 [32:40<1:21:12, 23.31s/it]Running Inference:  29%|██▉       | 86/294 [33:04<1:21:13, 23.43s/it]Running Inference:  30%|██▉       | 87/294 [33:27<1:19:59, 23.19s/it]Running Inference:  30%|██▉       | 88/294 [33:50<1:19:25, 23.13s/it]Running Inference:  30%|███       | 89/294 [34:13<1:19:40, 23.32s/it]Running Inference:  31%|███       | 90/294 [34:37<1:19:10, 23.28s/it]Running Inference:  31%|███       | 91/294 [35:03<1:21:45, 24.16s/it]Running Inference:  31%|███▏      | 92/294 [35:06<1:00:34, 17.99s/it]Running Inference:  32%|███▏      | 93/294 [35:29<1:05:22, 19.52s/it]Running Inference:  32%|███▏      | 94/294 [35:55<1:10:52, 21.26s/it]Running Inference:  32%|███▏      | 95/294 [36:17<1:11:48, 21.65s/it]Running Inference:  33%|███▎      | 96/294 [36:48<1:20:18, 24.34s/it]Running Inference:  33%|███▎      | 97/294 [37:11<1:18:25, 23.88s/it]Running Inference:  33%|███▎      | 98/294 [37:34<1:17:17, 23.66s/it]Running Inference:  34%|███▎      | 99/294 [37:58<1:17:47, 23.94s/it]Running Inference:  34%|███▍      | 100/294 [38:22<1:16:44, 23.73s/it]Running Inference:  34%|███▍      | 101/294 [38:45<1:16:16, 23.71s/it]Running Inference:  35%|███▍      | 102/294 [39:09<1:15:45, 23.67s/it]Running Inference:  35%|███▌      | 103/294 [39:32<1:14:49, 23.51s/it]Running Inference:  35%|███▌      | 104/294 [39:56<1:14:27, 23.51s/it]Running Inference:  36%|███▌      | 105/294 [40:19<1:13:48, 23.43s/it]Running Inference:  36%|███▌      | 106/294 [40:48<1:18:33, 25.07s/it]Running Inference:  36%|███▋      | 107/294 [41:11<1:16:24, 24.51s/it]Running Inference:  37%|███▋      | 108/294 [41:37<1:17:47, 25.09s/it]Running Inference:  37%|███▋      | 109/294 [42:02<1:16:43, 24.88s/it]Running Inference:  37%|███▋      | 110/294 [42:25<1:15:02, 24.47s/it]Running Inference:  38%|███▊      | 111/294 [42:49<1:13:33, 24.12s/it]Running Inference:  38%|███▊      | 112/294 [43:15<1:15:17, 24.82s/it]Running Inference:  38%|███▊      | 113/294 [43:39<1:14:24, 24.67s/it]Running Inference:  39%|███▉      | 114/294 [44:02<1:12:26, 24.15s/it]Running Inference:  39%|███▉      | 115/294 [44:25<1:11:01, 23.81s/it]Running Inference:  39%|███▉      | 116/294 [44:49<1:10:46, 23.86s/it]Running Inference:  40%|███▉      | 117/294 [45:13<1:09:48, 23.67s/it]Running Inference:  40%|████      | 118/294 [45:36<1:08:56, 23.50s/it]Running Inference:  40%|████      | 119/294 [45:59<1:08:42, 23.55s/it]Running Inference:  41%|████      | 120/294 [46:23<1:08:04, 23.47s/it]Running Inference:  41%|████      | 121/294 [46:51<1:11:38, 24.85s/it]Running Inference:  41%|████▏     | 122/294 [47:15<1:10:50, 24.71s/it]Running Inference:  42%|████▏     | 123/294 [47:39<1:09:50, 24.50s/it]Running Inference:  42%|████▏     | 124/294 [48:02<1:08:02, 24.01s/it]Running Inference:  43%|████▎     | 125/294 [48:25<1:07:07, 23.83s/it]Running Inference:  43%|████▎     | 126/294 [48:49<1:06:25, 23.72s/it]Running Inference:  43%|████▎     | 127/294 [49:13<1:06:26, 23.87s/it]Running Inference:  44%|████▎     | 128/294 [49:37<1:06:27, 24.02s/it]Running Inference:  44%|████▍     | 129/294 [50:01<1:05:31, 23.83s/it]Running Inference:  44%|████▍     | 130/294 [50:24<1:04:22, 23.55s/it]Running Inference:  45%|████▍     | 131/294 [50:47<1:03:55, 23.53s/it]Running Inference:  45%|████▍     | 132/294 [51:10<1:03:08, 23.38s/it]Running Inference:  45%|████▌     | 133/294 [51:33<1:02:28, 23.28s/it]Running Inference:  46%|████▌     | 134/294 [51:56<1:01:33, 23.09s/it]Running Inference:  46%|████▌     | 135/294 [52:19<1:01:26, 23.18s/it]Running Inference:  46%|████▋     | 136/294 [52:42<1:00:49, 23.10s/it]Running Inference:  47%|████▋     | 137/294 [53:05<1:00:15, 23.03s/it]Running Inference:  47%|████▋     | 138/294 [53:28<1:00:10, 23.14s/it]Running Inference:  47%|████▋     | 139/294 [53:51<59:05, 22.87s/it]  Running Inference:  48%|████▊     | 140/294 [54:16<1:00:14, 23.47s/it]Running Inference:  48%|████▊     | 141/294 [54:40<1:00:16, 23.64s/it]Running Inference:  48%|████▊     | 142/294 [55:03<59:29, 23.48s/it]  Running Inference:  49%|████▊     | 143/294 [55:26<59:02, 23.46s/it]Running Inference:  49%|████▉     | 144/294 [55:50<58:38, 23.46s/it]Running Inference:  49%|████▉     | 145/294 [56:13<58:05, 23.40s/it]Running Inference:  50%|████▉     | 146/294 [56:36<57:16, 23.22s/it]Running Inference:  50%|█████     | 147/294 [57:00<57:33, 23.49s/it]Running Inference:  50%|█████     | 148/294 [57:23<56:42, 23.31s/it]Running Inference:  51%|█████     | 149/294 [57:46<56:06, 23.22s/it]Running Inference:  51%|█████     | 150/294 [58:09<56:05, 23.37s/it]Running Inference:  51%|█████▏    | 151/294 [58:33<55:40, 23.36s/it]Running Inference:  52%|█████▏    | 152/294 [58:56<55:09, 23.31s/it]Running Inference:  52%|█████▏    | 153/294 [59:20<55:24, 23.58s/it]Running Inference:  52%|█████▏    | 154/294 [59:44<55:08, 23.63s/it]Running Inference:  53%|█████▎    | 155/294 [1:00:08<55:18, 23.88s/it]Running Inference:  53%|█████▎    | 156/294 [1:00:33<55:23, 24.08s/it]Running Inference:  53%|█████▎    | 157/294 [1:00:56<54:38, 23.93s/it]Running Inference:  54%|█████▎    | 158/294 [1:01:21<54:50, 24.19s/it]Running Inference:  54%|█████▍    | 159/294 [1:01:44<53:45, 23.89s/it]Running Inference:  54%|█████▍    | 160/294 [1:02:08<52:58, 23.72s/it]Running Inference:  55%|█████▍    | 161/294 [1:02:31<52:00, 23.47s/it]Running Inference:  55%|█████▌    | 162/294 [1:02:55<52:11, 23.73s/it]Running Inference:  55%|█████▌    | 163/294 [1:03:00<39:51, 18.25s/it]Running Inference:  56%|█████▌    | 164/294 [1:03:24<42:55, 19.81s/it]Running Inference:  56%|█████▌    | 165/294 [1:03:48<45:06, 20.98s/it]Running Inference:  56%|█████▋    | 166/294 [1:04:11<46:26, 21.77s/it]Running Inference:  57%|█████▋    | 167/294 [1:04:36<47:58, 22.67s/it]Running Inference:  57%|█████▋    | 168/294 [1:04:59<47:50, 22.78s/it]Running Inference:  57%|█████▋    | 169/294 [1:05:23<48:22, 23.22s/it]Running Inference:  58%|█████▊    | 170/294 [1:05:48<48:50, 23.63s/it]Running Inference:  58%|█████▊    | 171/294 [1:06:11<47:58, 23.41s/it]Running Inference:  59%|█████▊    | 172/294 [1:06:34<47:47, 23.50s/it]Running Inference:  59%|█████▉    | 173/294 [1:06:59<47:59, 23.80s/it]Running Inference:  59%|█████▉    | 174/294 [1:07:22<47:01, 23.51s/it]Running Inference:  60%|█████▉    | 175/294 [1:07:46<46:45, 23.57s/it]Running Inference:  60%|█████▉    | 176/294 [1:08:08<45:57, 23.37s/it]Running Inference:  60%|██████    | 177/294 [1:08:33<46:01, 23.60s/it]Running Inference:  61%|██████    | 178/294 [1:08:57<46:10, 23.88s/it]Running Inference:  61%|██████    | 179/294 [1:09:20<45:19, 23.65s/it]Running Inference:  61%|██████    | 180/294 [1:09:46<45:55, 24.17s/it]Running Inference:  62%|██████▏   | 181/294 [1:10:09<45:11, 24.00s/it]Running Inference:  62%|██████▏   | 182/294 [1:10:34<45:00, 24.11s/it]Running Inference:  62%|██████▏   | 183/294 [1:10:58<44:50, 24.24s/it]Running Inference:  63%|██████▎   | 184/294 [1:11:23<45:00, 24.55s/it]Running Inference:  63%|██████▎   | 185/294 [1:11:47<43:55, 24.18s/it]Running Inference:  63%|██████▎   | 186/294 [1:12:11<43:41, 24.27s/it]Running Inference:  64%|██████▎   | 187/294 [1:12:35<42:59, 24.10s/it]Running Inference:  64%|██████▍   | 188/294 [1:12:58<41:55, 23.73s/it]Running Inference:  64%|██████▍   | 189/294 [1:13:21<41:10, 23.53s/it]Running Inference:  65%|██████▍   | 190/294 [1:13:44<40:24, 23.31s/it]Running Inference:  65%|██████▍   | 191/294 [1:14:07<39:50, 23.21s/it]Running Inference:  65%|██████▌   | 192/294 [1:14:30<39:20, 23.14s/it]Running Inference:  66%|██████▌   | 193/294 [1:14:54<39:29, 23.46s/it]Running Inference:  66%|██████▌   | 194/294 [1:15:18<39:27, 23.67s/it]Running Inference:  66%|██████▋   | 195/294 [1:15:42<39:24, 23.88s/it]Running Inference:  67%|██████▋   | 196/294 [1:16:06<39:09, 23.97s/it]Running Inference:  67%|██████▋   | 197/294 [1:16:32<39:26, 24.40s/it]Running Inference:  67%|██████▋   | 198/294 [1:16:55<38:17, 23.93s/it]Running Inference:  68%|██████▊   | 199/294 [1:17:19<37:52, 23.92s/it]Running Inference:  68%|██████▊   | 200/294 [1:17:41<36:59, 23.61s/it]Running Inference:  68%|██████▊   | 201/294 [1:18:00<34:05, 22.00s/it]Running Inference:  69%|██████▊   | 202/294 [1:18:23<34:10, 22.28s/it]Running Inference:  69%|██████▉   | 203/294 [1:18:47<34:36, 22.82s/it]Running Inference:  69%|██████▉   | 204/294 [1:19:10<34:34, 23.05s/it]Running Inference:  70%|██████▉   | 205/294 [1:19:35<34:41, 23.38s/it]Running Inference:  70%|███████   | 206/294 [1:19:58<34:27, 23.49s/it]Running Inference:  70%|███████   | 207/294 [1:20:22<34:12, 23.59s/it]Running Inference:  71%|███████   | 208/294 [1:20:46<33:53, 23.65s/it]Running Inference:  71%|███████   | 209/294 [1:21:09<33:25, 23.59s/it]Running Inference:  71%|███████▏  | 210/294 [1:21:34<33:22, 23.84s/it]Running Inference:  72%|███████▏  | 211/294 [1:21:57<32:43, 23.66s/it]Running Inference:  72%|███████▏  | 212/294 [1:22:20<31:59, 23.40s/it]Running Inference:  72%|███████▏  | 213/294 [1:22:44<31:55, 23.65s/it]Running Inference:  73%|███████▎  | 214/294 [1:23:07<31:17, 23.47s/it]Running Inference:  73%|███████▎  | 215/294 [1:23:31<31:07, 23.64s/it]Running Inference:  73%|███████▎  | 216/294 [1:23:55<30:40, 23.60s/it]Running Inference:  74%|███████▍  | 217/294 [1:24:18<30:17, 23.60s/it]Running Inference:  74%|███████▍  | 218/294 [1:24:41<29:36, 23.37s/it]Running Inference:  74%|███████▍  | 219/294 [1:25:04<29:04, 23.25s/it]Running Inference:  75%|███████▍  | 220/294 [1:25:25<27:59, 22.70s/it]Running Inference:  75%|███████▌  | 221/294 [1:25:48<27:43, 22.79s/it]Running Inference:  76%|███████▌  | 222/294 [1:26:12<27:39, 23.04s/it]Running Inference:  76%|███████▌  | 223/294 [1:26:36<27:36, 23.33s/it]Running Inference:  76%|███████▌  | 224/294 [1:26:59<27:05, 23.22s/it]Running Inference:  77%|███████▋  | 225/294 [1:27:22<26:45, 23.27s/it]Running Inference:  77%|███████▋  | 226/294 [1:27:46<26:28, 23.35s/it]Running Inference:  77%|███████▋  | 227/294 [1:28:09<25:59, 23.27s/it]Running Inference:  78%|███████▊  | 228/294 [1:28:33<25:43, 23.39s/it]Running Inference:  78%|███████▊  | 229/294 [1:28:57<25:29, 23.53s/it]Running Inference:  78%|███████▊  | 230/294 [1:29:20<24:56, 23.38s/it]Running Inference:  79%|███████▊  | 231/294 [1:29:43<24:36, 23.44s/it]Running Inference:  79%|███████▉  | 232/294 [1:30:06<24:08, 23.36s/it]Running Inference:  79%|███████▉  | 233/294 [1:30:31<24:00, 23.61s/it]Running Inference:  80%|███████▉  | 234/294 [1:30:54<23:37, 23.63s/it]Running Inference:  80%|███████▉  | 235/294 [1:31:18<23:18, 23.71s/it]Running Inference:  80%|████████  | 236/294 [1:31:41<22:48, 23.59s/it]Running Inference:  81%|████████  | 237/294 [1:32:05<22:19, 23.50s/it]Running Inference:  81%|████████  | 238/294 [1:32:29<22:09, 23.73s/it]Running Inference:  81%|████████▏ | 239/294 [1:32:53<21:44, 23.72s/it]Running Inference:  82%|████████▏ | 240/294 [1:33:17<21:34, 23.97s/it]Running Inference:  82%|████████▏ | 241/294 [1:33:41<21:05, 23.87s/it]Running Inference:  82%|████████▏ | 242/294 [1:34:05<20:50, 24.06s/it]Running Inference:  83%|████████▎ | 243/294 [1:34:29<20:17, 23.87s/it]Running Inference:  83%|████████▎ | 244/294 [1:34:52<19:41, 23.62s/it]Running Inference:  83%|████████▎ | 245/294 [1:34:53<13:42, 16.78s/it]Running Inference:  84%|████████▎ | 246/294 [1:35:18<15:27, 19.32s/it]Running Inference:  84%|████████▍ | 247/294 [1:35:42<16:15, 20.75s/it]Running Inference:  84%|████████▍ | 248/294 [1:36:05<16:30, 21.52s/it]Running Inference:  85%|████████▍ | 249/294 [1:36:30<16:55, 22.57s/it]Running Inference:  85%|████████▌ | 250/294 [1:36:55<16:58, 23.14s/it]Running Inference:  85%|████████▌ | 251/294 [1:37:19<16:54, 23.59s/it]Running Inference:  86%|████████▌ | 252/294 [1:37:43<16:33, 23.66s/it]Running Inference:  86%|████████▌ | 253/294 [1:38:07<16:09, 23.64s/it]Running Inference:  86%|████████▋ | 254/294 [1:38:30<15:42, 23.56s/it]Running Inference:  87%|████████▋ | 255/294 [1:38:54<15:24, 23.71s/it]Running Inference:  87%|████████▋ | 256/294 [1:39:19<15:10, 23.96s/it]Running Inference:  87%|████████▋ | 257/294 [1:39:42<14:35, 23.67s/it]Running Inference:  88%|████████▊ | 258/294 [1:40:05<14:08, 23.57s/it]Running Inference:  88%|████████▊ | 259/294 [1:40:28<13:37, 23.35s/it]Running Inference:  88%|████████▊ | 260/294 [1:40:51<13:10, 23.26s/it]Running Inference:  89%|████████▉ | 261/294 [1:41:15<12:49, 23.33s/it]Running Inference:  89%|████████▉ | 262/294 [1:41:37<12:21, 23.18s/it]Running Inference:  89%|████████▉ | 263/294 [1:42:00<11:56, 23.10s/it]Running Inference:  90%|████████▉ | 264/294 [1:42:23<11:31, 23.04s/it]Running Inference:  90%|█████████ | 265/294 [1:42:47<11:12, 23.20s/it]Running Inference:  90%|█████████ | 266/294 [1:43:10<10:52, 23.30s/it]Running Inference:  91%|█████████ | 267/294 [1:43:34<10:35, 23.55s/it]Running Inference:  91%|█████████ | 268/294 [1:44:07<11:23, 26.30s/it]Running Inference:  91%|█████████▏| 269/294 [1:44:30<10:35, 25.40s/it]Running Inference:  92%|█████████▏| 270/294 [1:44:55<10:00, 25.00s/it]Running Inference:  92%|█████████▏| 271/294 [1:45:17<09:19, 24.34s/it]Running Inference:  93%|█████████▎| 272/294 [1:45:20<06:30, 17.75s/it]Running Inference:  93%|█████████▎| 273/294 [1:45:43<06:45, 19.32s/it]Running Inference:  93%|█████████▎| 274/294 [1:46:07<06:55, 20.78s/it]Running Inference:  94%|█████████▎| 275/294 [1:46:30<06:49, 21.53s/it]Running Inference:  94%|█████████▍| 276/294 [1:46:53<06:35, 21.96s/it]Running Inference:  94%|█████████▍| 277/294 [1:47:17<06:21, 22.46s/it]Running Inference:  95%|█████████▍| 278/294 [1:47:41<06:05, 22.87s/it]Running Inference:  95%|█████████▍| 279/294 [1:48:05<05:50, 23.37s/it]Running Inference:  95%|█████████▌| 280/294 [1:48:32<05:42, 24.49s/it]Running Inference:  96%|█████████▌| 281/294 [1:48:56<05:16, 24.36s/it]Running Inference:  96%|█████████▌| 282/294 [1:49:20<04:51, 24.31s/it]Running Inference:  96%|█████████▋| 283/294 [1:49:52<04:50, 26.38s/it]Running Inference:  97%|█████████▋| 284/294 [1:50:16<04:17, 25.79s/it]Running Inference:  97%|█████████▋| 285/294 [1:50:40<03:46, 25.17s/it]Running Inference:  97%|█████████▋| 286/294 [1:51:04<03:20, 25.02s/it]Running Inference:  98%|█████████▊| 287/294 [1:51:28<02:52, 24.57s/it]Running Inference:  98%|█████████▊| 288/294 [1:51:52<02:26, 24.35s/it]Running Inference:  98%|█████████▊| 289/294 [1:52:11<01:53, 22.66s/it]Running Inference:  99%|█████████▊| 290/294 [1:52:34<01:31, 22.75s/it]Running Inference:  99%|█████████▉| 291/294 [1:52:56<01:08, 22.79s/it]Running Inference:  99%|█████████▉| 292/294 [1:53:20<00:45, 22.95s/it]Running Inference: 100%|█████████▉| 293/294 [1:53:43<00:23, 23.05s/it]Running Inference: 100%|██████████| 294/294 [1:54:06<00:00, 23.03s/it]Running Inference: 100%|██████████| 294/294 [1:54:06<00:00, 23.29s/it]
2025-12-15 13:23:17,939 - INFO - Inference completed.
2025-12-15 13:23:17,971 - INFO - Results saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-15 13:23:17,971 - INFO - Calculating metrics for dataset: longbench
2025-12-15 13:23:30,855 - INFO - Metrics saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-15 13:23:30,855 - INFO - Metrics:
11.53
2025-12-15 13:23:30,857 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news_e, ratio=0.1) on GPU 1

----------------------------------------
Task: multi_news_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 13:23:37,574 - INFO - Set deterministic seeds to 42
2025-12-15 13:23:37,574 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 13:23:37,574 - INFO - Starting evaluation run...
2025-12-15 13:23:37,574 - INFO - Output directory set to: longbenchresult
2025-12-15 13:23:37,574 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-15 13:23:37,574 - INFO - KV Press 'tova' setup.
2025-12-15 13:23:37,574 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 13:23:37,574 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.04it/s]
Device set to use cuda:0
2025-12-15 13:23:49,672 - INFO - Model pipeline loaded.
2025-12-15 13:23:49,673 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news_e)
2025-12-15 13:23:53,691 - INFO - Dataset loaded with 294 entries.
2025-12-15 13:23:53,692 - INFO - Dataset processed with 294 entries.
2025-12-15 13:23:53,724 - INFO - Starting inference...
Running Inference:   0%|          | 0/294 [00:00<?, ?it/s]Running Inference:   0%|          | 1/294 [00:24<1:57:26, 24.05s/it]Running Inference:   1%|          | 2/294 [00:48<1:57:27, 24.14s/it]Running Inference:   1%|          | 3/294 [01:11<1:54:43, 23.66s/it]Running Inference:   1%|▏         | 4/294 [01:36<1:56:56, 24.19s/it]Running Inference:   2%|▏         | 5/294 [01:59<1:54:59, 23.87s/it]Running Inference:   2%|▏         | 6/294 [02:22<1:53:19, 23.61s/it]Running Inference:   2%|▏         | 7/294 [02:46<1:52:24, 23.50s/it]Running Inference:   3%|▎         | 8/294 [03:10<1:54:03, 23.93s/it]Running Inference:   3%|▎         | 9/294 [03:35<1:54:27, 24.10s/it]Running Inference:   3%|▎         | 10/294 [03:58<1:53:20, 23.95s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/294 [04:29<2:02:50, 26.05s/it]Running Inference:   4%|▍         | 12/294 [04:53<1:59:16, 25.38s/it]Running Inference:   4%|▍         | 13/294 [05:16<1:55:43, 24.71s/it]Running Inference:   5%|▍         | 14/294 [05:39<1:53:07, 24.24s/it]Running Inference:   5%|▌         | 15/294 [06:03<1:51:57, 24.08s/it]Running Inference:   5%|▌         | 16/294 [06:27<1:51:15, 24.01s/it]Running Inference:   6%|▌         | 17/294 [06:49<1:47:39, 23.32s/it]Running Inference:   6%|▌         | 18/294 [07:13<1:48:03, 23.49s/it]Running Inference:   6%|▋         | 19/294 [07:31<1:40:13, 21.87s/it]Running Inference:   7%|▋         | 20/294 [07:54<1:42:15, 22.39s/it]Running Inference:   7%|▋         | 21/294 [08:18<1:43:12, 22.68s/it]Running Inference:   7%|▋         | 22/294 [08:41<1:44:07, 22.97s/it]Running Inference:   8%|▊         | 23/294 [09:05<1:44:40, 23.17s/it]Running Inference:   8%|▊         | 24/294 [09:28<1:44:43, 23.27s/it]Running Inference:   9%|▊         | 25/294 [09:52<1:45:16, 23.48s/it]Running Inference:   9%|▉         | 26/294 [10:16<1:44:24, 23.38s/it]Running Inference:   9%|▉         | 27/294 [10:40<1:45:05, 23.61s/it]Running Inference:  10%|▉         | 28/294 [11:03<1:44:48, 23.64s/it]Running Inference:  10%|▉         | 29/294 [11:28<1:45:34, 23.90s/it]Running Inference:  10%|█         | 30/294 [11:52<1:44:45, 23.81s/it]Running Inference:  11%|█         | 31/294 [12:17<1:46:09, 24.22s/it]Running Inference:  11%|█         | 32/294 [12:40<1:44:55, 24.03s/it]Running Inference:  11%|█         | 33/294 [13:03<1:43:18, 23.75s/it]Running Inference:  12%|█▏        | 34/294 [13:27<1:42:39, 23.69s/it]Running Inference:  12%|█▏        | 35/294 [13:50<1:41:39, 23.55s/it]Running Inference:  12%|█▏        | 36/294 [14:14<1:41:23, 23.58s/it]Running Inference:  13%|█▎        | 37/294 [14:38<1:41:45, 23.76s/it]Running Inference:  13%|█▎        | 38/294 [15:02<1:41:11, 23.72s/it]Running Inference:  13%|█▎        | 39/294 [15:29<1:45:13, 24.76s/it]Running Inference:  14%|█▎        | 40/294 [15:54<1:44:48, 24.76s/it]Running Inference:  14%|█▍        | 41/294 [16:18<1:44:37, 24.81s/it]Running Inference:  14%|█▍        | 42/294 [16:43<1:43:42, 24.69s/it]Running Inference:  15%|█▍        | 43/294 [17:06<1:41:52, 24.35s/it]Running Inference:  15%|█▍        | 44/294 [17:30<1:40:01, 24.01s/it]Running Inference:  15%|█▌        | 45/294 [17:53<1:38:36, 23.76s/it]Running Inference:  16%|█▌        | 46/294 [18:17<1:38:11, 23.76s/it]Running Inference:  16%|█▌        | 47/294 [18:40<1:36:58, 23.56s/it]Running Inference:  16%|█▋        | 48/294 [19:05<1:38:28, 24.02s/it]Running Inference:  17%|█▋        | 49/294 [19:29<1:38:38, 24.16s/it]Running Inference:  17%|█▋        | 50/294 [19:52<1:36:56, 23.84s/it]Running Inference:  17%|█▋        | 51/294 [20:17<1:36:58, 23.94s/it]Running Inference:  18%|█▊        | 52/294 [20:42<1:38:13, 24.35s/it]Running Inference:  18%|█▊        | 53/294 [21:05<1:36:31, 24.03s/it]Running Inference:  18%|█▊        | 54/294 [21:29<1:36:01, 24.01s/it]Running Inference:  19%|█▊        | 55/294 [21:52<1:34:37, 23.75s/it]Running Inference:  19%|█▉        | 56/294 [22:15<1:33:28, 23.56s/it]Running Inference:  19%|█▉        | 57/294 [22:39<1:33:28, 23.67s/it]Running Inference:  20%|█▉        | 58/294 [23:03<1:32:48, 23.60s/it]Running Inference:  20%|██        | 59/294 [23:27<1:33:21, 23.84s/it]Running Inference:  20%|██        | 60/294 [23:52<1:33:57, 24.09s/it]Running Inference:  21%|██        | 61/294 [24:16<1:33:52, 24.18s/it]Running Inference:  21%|██        | 62/294 [24:39<1:32:28, 23.92s/it]Running Inference:  21%|██▏       | 63/294 [25:03<1:31:08, 23.67s/it]Running Inference:  22%|██▏       | 64/294 [25:26<1:30:43, 23.67s/it]Running Inference:  22%|██▏       | 65/294 [25:49<1:29:42, 23.50s/it]Running Inference:  22%|██▏       | 66/294 [26:14<1:30:50, 23.91s/it]Running Inference:  23%|██▎       | 67/294 [26:38<1:30:13, 23.85s/it]Running Inference:  23%|██▎       | 68/294 [27:01<1:29:02, 23.64s/it]Running Inference:  23%|██▎       | 69/294 [27:25<1:28:36, 23.63s/it]Running Inference:  24%|██▍       | 70/294 [27:48<1:27:37, 23.47s/it]Running Inference:  24%|██▍       | 71/294 [28:12<1:27:41, 23.59s/it]Running Inference:  24%|██▍       | 72/294 [28:35<1:26:52, 23.48s/it]Running Inference:  25%|██▍       | 73/294 [28:59<1:27:38, 23.79s/it]Running Inference:  25%|██▌       | 74/294 [29:23<1:26:34, 23.61s/it]Running Inference:  26%|██▌       | 75/294 [29:46<1:25:42, 23.48s/it]Running Inference:  26%|██▌       | 76/294 [30:09<1:25:07, 23.43s/it]Running Inference:  26%|██▌       | 77/294 [30:33<1:24:58, 23.49s/it]Running Inference:  27%|██▋       | 78/294 [30:57<1:24:56, 23.60s/it]Running Inference:  27%|██▋       | 79/294 [31:20<1:24:49, 23.67s/it]Running Inference:  27%|██▋       | 80/294 [31:45<1:25:16, 23.91s/it]Running Inference:  28%|██▊       | 81/294 [32:10<1:25:55, 24.21s/it]Running Inference:  28%|██▊       | 82/294 [32:33<1:24:53, 24.03s/it]Running Inference:  28%|██▊       | 83/294 [32:58<1:24:50, 24.13s/it]Running Inference:  29%|██▊       | 84/294 [33:21<1:23:25, 23.83s/it]Running Inference:  29%|██▉       | 85/294 [33:45<1:23:41, 24.03s/it]Running Inference:  29%|██▉       | 86/294 [34:10<1:23:36, 24.12s/it]Running Inference:  30%|██▉       | 87/294 [34:33<1:22:23, 23.88s/it]Running Inference:  30%|██▉       | 88/294 [34:57<1:21:50, 23.83s/it]Running Inference:  30%|███       | 89/294 [35:21<1:22:04, 24.02s/it]Running Inference:  31%|███       | 90/294 [35:45<1:21:34, 23.99s/it]Running Inference:  31%|███       | 91/294 [36:12<1:23:47, 24.77s/it]Running Inference:  31%|███▏      | 92/294 [36:35<1:22:09, 24.41s/it]Running Inference:  32%|███▏      | 93/294 [36:59<1:21:10, 24.23s/it]Running Inference:  32%|███▏      | 94/294 [37:25<1:22:02, 24.61s/it]Running Inference:  32%|███▏      | 95/294 [37:48<1:20:13, 24.19s/it]Running Inference:  33%|███▎      | 96/294 [38:18<1:26:12, 26.12s/it]Running Inference:  33%|███▎      | 97/294 [38:42<1:22:53, 25.24s/it]Running Inference:  33%|███▎      | 98/294 [38:58<1:14:10, 22.71s/it]Running Inference:  34%|███▎      | 99/294 [39:23<1:15:38, 23.28s/it]Running Inference:  34%|███▍      | 100/294 [39:46<1:15:21, 23.31s/it]Running Inference:  34%|███▍      | 101/294 [40:10<1:15:20, 23.42s/it]Running Inference:  35%|███▍      | 102/294 [40:34<1:15:08, 23.48s/it]Running Inference:  35%|███▌      | 103/294 [40:57<1:14:34, 23.42s/it]Running Inference:  35%|███▌      | 104/294 [41:21<1:14:27, 23.51s/it]Running Inference:  36%|███▌      | 105/294 [41:44<1:13:43, 23.41s/it]Running Inference:  36%|███▌      | 106/294 [42:12<1:18:12, 24.96s/it]Running Inference:  36%|███▋      | 107/294 [42:36<1:16:14, 24.46s/it]Running Inference:  37%|███▋      | 108/294 [43:02<1:17:26, 24.98s/it]Running Inference:  37%|███▋      | 109/294 [43:27<1:16:41, 24.87s/it]Running Inference:  37%|███▋      | 110/294 [43:50<1:15:09, 24.51s/it]Running Inference:  38%|███▊      | 111/294 [44:14<1:13:51, 24.22s/it]Running Inference:  38%|███▊      | 112/294 [44:40<1:15:23, 24.85s/it]Running Inference:  38%|███▊      | 113/294 [45:05<1:14:39, 24.75s/it]Running Inference:  39%|███▉      | 114/294 [45:28<1:12:48, 24.27s/it]Running Inference:  39%|███▉      | 115/294 [45:51<1:11:28, 23.96s/it]Running Inference:  39%|███▉      | 116/294 [46:15<1:11:19, 24.04s/it]Running Inference:  40%|███▉      | 117/294 [46:39<1:10:27, 23.88s/it]Running Inference:  40%|████      | 118/294 [47:02<1:09:35, 23.72s/it]Running Inference:  40%|████      | 119/294 [47:26<1:09:20, 23.78s/it]Running Inference:  41%|████      | 120/294 [47:50<1:08:47, 23.72s/it]Running Inference:  41%|████      | 121/294 [48:17<1:12:00, 24.98s/it]Running Inference:  41%|████▏     | 122/294 [48:42<1:11:23, 24.90s/it]Running Inference:  42%|████▏     | 123/294 [49:06<1:10:28, 24.73s/it]Running Inference:  42%|████▏     | 124/294 [49:08<50:27, 17.81s/it]  Running Inference:  43%|████▎     | 125/294 [49:32<55:07, 19.57s/it]Running Inference:  43%|████▎     | 126/294 [49:56<58:15, 20.81s/it]Running Inference:  43%|████▎     | 127/294 [50:20<1:00:50, 21.86s/it]Running Inference:  44%|████▎     | 128/294 [50:44<1:02:45, 22.68s/it]Running Inference:  44%|████▍     | 129/294 [51:08<1:02:55, 22.88s/it]Running Inference:  44%|████▍     | 130/294 [51:31<1:02:43, 22.95s/it]Running Inference:  45%|████▍     | 131/294 [51:55<1:03:00, 23.19s/it]Running Inference:  45%|████▍     | 132/294 [52:18<1:02:35, 23.18s/it]Running Inference:  45%|████▌     | 133/294 [52:41<1:02:13, 23.19s/it]Running Inference:  46%|████▌     | 134/294 [53:04<1:01:48, 23.18s/it]Running Inference:  46%|████▌     | 135/294 [53:28<1:01:46, 23.31s/it]Running Inference:  46%|████▋     | 136/294 [53:51<1:01:14, 23.26s/it]Running Inference:  47%|████▋     | 137/294 [54:14<1:00:50, 23.25s/it]Running Inference:  47%|████▋     | 138/294 [54:38<1:00:48, 23.39s/it]Running Inference:  47%|████▋     | 139/294 [55:01<1:00:17, 23.34s/it]Running Inference:  48%|████▊     | 140/294 [55:26<1:01:14, 23.86s/it]Running Inference:  48%|████▊     | 141/294 [55:50<1:01:07, 23.97s/it]Running Inference:  48%|████▊     | 142/294 [56:14<1:00:17, 23.80s/it]Running Inference:  49%|████▊     | 143/294 [56:37<59:47, 23.76s/it]  Running Inference:  49%|████▉     | 144/294 [57:01<59:19, 23.73s/it]Running Inference:  49%|████▉     | 145/294 [57:25<58:47, 23.67s/it]Running Inference:  50%|████▉     | 146/294 [57:48<58:25, 23.69s/it]Running Inference:  50%|█████     | 147/294 [58:14<59:08, 24.14s/it]Running Inference:  50%|█████     | 148/294 [58:38<58:40, 24.11s/it]Running Inference:  51%|█████     | 149/294 [59:02<58:08, 24.06s/it]Running Inference:  51%|█████     | 150/294 [59:26<58:10, 24.24s/it]Running Inference:  51%|█████▏    | 151/294 [59:50<57:39, 24.19s/it]Running Inference:  52%|█████▏    | 152/294 [1:00:14<57:01, 24.09s/it]Running Inference:  52%|█████▏    | 153/294 [1:00:39<57:21, 24.41s/it]Running Inference:  52%|█████▏    | 154/294 [1:01:04<57:00, 24.44s/it]Running Inference:  53%|█████▎    | 155/294 [1:01:29<57:04, 24.64s/it]Running Inference:  53%|█████▎    | 156/294 [1:01:53<56:03, 24.38s/it]Running Inference:  53%|█████▎    | 157/294 [1:02:16<55:10, 24.17s/it]Running Inference:  54%|█████▎    | 158/294 [1:02:42<55:26, 24.46s/it]Running Inference:  54%|█████▍    | 159/294 [1:03:05<54:27, 24.20s/it]Running Inference:  54%|█████▍    | 160/294 [1:03:29<53:42, 24.05s/it]Running Inference:  55%|█████▍    | 161/294 [1:03:52<52:48, 23.82s/it]Running Inference:  55%|█████▌    | 162/294 [1:04:17<53:01, 24.10s/it]Running Inference:  55%|█████▌    | 163/294 [1:04:22<40:25, 18.51s/it]Running Inference:  56%|█████▌    | 164/294 [1:04:46<43:33, 20.11s/it]Running Inference:  56%|█████▌    | 165/294 [1:05:10<45:49, 21.31s/it]Running Inference:  56%|█████▋    | 166/294 [1:05:34<47:09, 22.10s/it]Running Inference:  57%|█████▋    | 167/294 [1:05:59<48:40, 22.99s/it]Running Inference:  57%|█████▋    | 168/294 [1:06:23<48:35, 23.14s/it]Running Inference:  57%|█████▋    | 169/294 [1:06:47<49:07, 23.58s/it]Running Inference:  58%|█████▊    | 170/294 [1:07:12<49:37, 24.01s/it]Running Inference:  58%|█████▊    | 171/294 [1:07:36<48:46, 23.80s/it]Running Inference:  59%|█████▊    | 172/294 [1:08:00<48:34, 23.89s/it]Running Inference:  59%|█████▉    | 173/294 [1:08:25<48:50, 24.22s/it]Running Inference:  59%|█████▉    | 174/294 [1:08:48<47:51, 23.93s/it]Running Inference:  60%|█████▉    | 175/294 [1:09:12<47:15, 23.83s/it]Running Inference:  60%|█████▉    | 176/294 [1:09:35<46:33, 23.68s/it]Running Inference:  60%|██████    | 177/294 [1:10:00<46:41, 23.94s/it]Running Inference:  61%|██████    | 178/294 [1:10:24<46:51, 24.24s/it]Running Inference:  61%|██████    | 179/294 [1:10:48<46:02, 24.03s/it]Running Inference:  61%|██████    | 180/294 [1:11:14<46:29, 24.47s/it]Running Inference:  62%|██████▏   | 181/294 [1:11:38<45:59, 24.42s/it]Running Inference:  62%|██████▏   | 182/294 [1:12:03<45:56, 24.61s/it]Running Inference:  62%|██████▏   | 183/294 [1:12:28<45:35, 24.65s/it]Running Inference:  63%|██████▎   | 184/294 [1:12:53<45:25, 24.78s/it]Running Inference:  63%|██████▎   | 185/294 [1:13:16<44:20, 24.41s/it]Running Inference:  63%|██████▎   | 186/294 [1:13:41<44:06, 24.51s/it]Running Inference:  64%|██████▎   | 187/294 [1:14:05<43:25, 24.35s/it]Running Inference:  64%|██████▍   | 188/294 [1:14:28<42:22, 23.98s/it]Running Inference:  64%|██████▍   | 189/294 [1:14:51<41:34, 23.75s/it]Running Inference:  65%|██████▍   | 190/294 [1:15:14<40:49, 23.55s/it]Running Inference:  65%|██████▍   | 191/294 [1:15:37<40:10, 23.40s/it]Running Inference:  65%|██████▌   | 192/294 [1:16:01<39:39, 23.33s/it]Running Inference:  66%|██████▌   | 193/294 [1:16:24<39:33, 23.50s/it]Running Inference:  66%|██████▌   | 194/294 [1:16:49<39:35, 23.76s/it]Running Inference:  66%|██████▋   | 195/294 [1:17:13<39:37, 24.02s/it]Running Inference:  67%|██████▋   | 196/294 [1:17:38<39:21, 24.10s/it]Running Inference:  67%|██████▋   | 197/294 [1:18:03<39:29, 24.42s/it]Running Inference:  67%|██████▋   | 198/294 [1:18:26<38:24, 24.01s/it]Running Inference:  68%|██████▊   | 199/294 [1:18:50<37:56, 23.97s/it]Running Inference:  68%|██████▊   | 200/294 [1:19:13<37:04, 23.66s/it]Running Inference:  68%|██████▊   | 201/294 [1:19:35<36:09, 23.33s/it]Running Inference:  69%|██████▊   | 202/294 [1:19:58<35:35, 23.22s/it]Running Inference:  69%|██████▉   | 203/294 [1:20:22<35:35, 23.47s/it]Running Inference:  69%|██████▉   | 204/294 [1:20:46<35:13, 23.48s/it]Running Inference:  70%|██████▉   | 205/294 [1:21:10<35:08, 23.69s/it]Running Inference:  70%|███████   | 206/294 [1:21:34<34:47, 23.72s/it]Running Inference:  70%|███████   | 207/294 [1:21:58<34:26, 23.75s/it]Running Inference:  71%|███████   | 208/294 [1:22:21<34:03, 23.76s/it]Running Inference:  71%|███████   | 209/294 [1:22:45<33:31, 23.67s/it]Running Inference:  71%|███████▏  | 210/294 [1:23:09<33:28, 23.92s/it]Running Inference:  72%|███████▏  | 211/294 [1:23:33<32:48, 23.71s/it]Running Inference:  72%|███████▏  | 212/294 [1:23:56<32:04, 23.47s/it]Running Inference:  72%|███████▏  | 213/294 [1:24:20<32:00, 23.71s/it]Running Inference:  73%|███████▎  | 214/294 [1:24:43<31:20, 23.50s/it]Running Inference:  73%|███████▎  | 215/294 [1:25:07<31:11, 23.69s/it]Running Inference:  73%|███████▎  | 216/294 [1:25:30<30:42, 23.63s/it]Running Inference:  74%|███████▍  | 217/294 [1:25:54<30:19, 23.63s/it]Running Inference:  74%|███████▍  | 218/294 [1:26:17<29:37, 23.39s/it]Running Inference:  74%|███████▍  | 219/294 [1:26:40<29:02, 23.24s/it]Running Inference:  75%|███████▍  | 220/294 [1:26:59<27:14, 22.08s/it]Running Inference:  75%|███████▌  | 221/294 [1:27:22<27:11, 22.35s/it]Running Inference:  76%|███████▌  | 222/294 [1:27:46<27:16, 22.73s/it]Running Inference:  76%|███████▌  | 223/294 [1:28:10<27:22, 23.13s/it]Running Inference:  76%|███████▌  | 224/294 [1:28:33<26:54, 23.07s/it]Running Inference:  77%|███████▋  | 225/294 [1:28:56<26:38, 23.16s/it]Running Inference:  77%|███████▋  | 226/294 [1:29:20<26:23, 23.28s/it]Running Inference:  77%|███████▋  | 227/294 [1:29:29<21:17, 19.06s/it]Running Inference:  78%|███████▊  | 228/294 [1:29:53<22:29, 20.44s/it]Running Inference:  78%|███████▊  | 229/294 [1:30:16<23:15, 21.47s/it]Running Inference:  78%|███████▊  | 230/294 [1:30:39<23:23, 21.92s/it]Running Inference:  79%|███████▊  | 231/294 [1:31:03<23:33, 22.44s/it]Running Inference:  79%|███████▉  | 232/294 [1:31:26<23:25, 22.66s/it]Running Inference:  79%|███████▉  | 233/294 [1:31:35<18:52, 18.57s/it]Running Inference:  80%|███████▉  | 234/294 [1:31:59<20:05, 20.09s/it]Running Inference:  80%|███████▉  | 235/294 [1:32:23<20:52, 21.22s/it]Running Inference:  80%|████████  | 236/294 [1:32:46<21:08, 21.87s/it]Running Inference:  81%|████████  | 237/294 [1:32:49<15:26, 16.26s/it]Running Inference:  81%|████████  | 238/294 [1:33:14<17:25, 18.66s/it]Running Inference:  81%|████████▏ | 239/294 [1:33:37<18:27, 20.14s/it]Running Inference:  82%|████████▏ | 240/294 [1:34:02<19:18, 21.46s/it]Running Inference:  82%|████████▏ | 241/294 [1:34:25<19:33, 22.14s/it]Running Inference:  82%|████████▏ | 242/294 [1:34:50<19:48, 22.85s/it]Running Inference:  83%|████████▎ | 243/294 [1:35:13<19:35, 23.05s/it]Running Inference:  83%|████████▎ | 244/294 [1:35:26<16:33, 19.87s/it]Running Inference:  83%|████████▎ | 245/294 [1:35:27<11:33, 14.15s/it]Running Inference:  84%|████████▎ | 246/294 [1:35:52<13:55, 17.40s/it]Running Inference:  84%|████████▍ | 247/294 [1:36:16<15:14, 19.45s/it]Running Inference:  84%|████████▍ | 248/294 [1:36:39<15:51, 20.68s/it]Running Inference:  85%|████████▍ | 249/294 [1:37:04<16:29, 21.98s/it]Running Inference:  85%|████████▌ | 250/294 [1:37:29<16:43, 22.80s/it]Running Inference:  85%|████████▌ | 251/294 [1:37:53<16:38, 23.21s/it]Running Inference:  86%|████████▌ | 252/294 [1:38:16<16:12, 23.16s/it]Running Inference:  86%|████████▌ | 253/294 [1:38:40<15:54, 23.28s/it]Running Inference:  86%|████████▋ | 254/294 [1:39:03<15:34, 23.36s/it]Running Inference:  87%|████████▋ | 255/294 [1:39:28<15:20, 23.60s/it]Running Inference:  87%|████████▋ | 256/294 [1:39:52<15:08, 23.91s/it]Running Inference:  87%|████████▋ | 257/294 [1:40:15<14:34, 23.64s/it]Running Inference:  88%|████████▊ | 258/294 [1:40:39<14:08, 23.58s/it]Running Inference:  88%|████████▊ | 259/294 [1:41:02<13:39, 23.40s/it]Running Inference:  88%|████████▊ | 260/294 [1:41:25<13:12, 23.29s/it]Running Inference:  89%|████████▉ | 261/294 [1:41:48<12:51, 23.39s/it]Running Inference:  89%|████████▉ | 262/294 [1:42:11<12:25, 23.28s/it]Running Inference:  89%|████████▉ | 263/294 [1:42:32<11:34, 22.39s/it]Running Inference:  90%|████████▉ | 264/294 [1:42:55<11:16, 22.56s/it]Running Inference:  90%|█████████ | 265/294 [1:43:18<11:03, 22.87s/it]Running Inference:  90%|█████████ | 266/294 [1:43:42<10:46, 23.10s/it]Running Inference:  91%|█████████ | 267/294 [1:44:06<10:33, 23.45s/it]Running Inference:  91%|█████████ | 268/294 [1:44:38<11:17, 26.07s/it]Running Inference:  91%|█████████▏| 269/294 [1:45:02<10:31, 25.28s/it]Running Inference:  92%|█████████▏| 270/294 [1:45:26<09:58, 24.94s/it]Running Inference:  92%|█████████▏| 271/294 [1:45:49<09:20, 24.36s/it]Running Inference:  93%|█████████▎| 272/294 [1:45:51<06:30, 17.76s/it]Running Inference:  93%|█████████▎| 273/294 [1:46:15<06:47, 19.40s/it]Running Inference:  93%|█████████▎| 274/294 [1:46:39<06:57, 20.89s/it]Running Inference:  94%|█████████▎| 275/294 [1:47:02<06:51, 21.67s/it]Running Inference:  94%|█████████▍| 276/294 [1:47:25<06:37, 22.08s/it]Running Inference:  94%|█████████▍| 277/294 [1:47:49<06:24, 22.59s/it]Running Inference:  95%|█████████▍| 278/294 [1:48:13<06:07, 22.99s/it]Running Inference:  95%|█████████▍| 279/294 [1:48:38<05:51, 23.46s/it]Running Inference:  95%|█████████▌| 280/294 [1:49:05<05:43, 24.52s/it]Running Inference:  96%|█████████▌| 281/294 [1:49:29<05:17, 24.41s/it]Running Inference:  96%|█████████▌| 282/294 [1:49:53<04:52, 24.36s/it]Running Inference:  96%|█████████▋| 283/294 [1:50:24<04:48, 26.27s/it]Running Inference:  97%|█████████▋| 284/294 [1:50:48<04:16, 25.68s/it]Running Inference:  97%|█████████▋| 285/294 [1:51:12<03:45, 25.10s/it]Running Inference:  97%|█████████▋| 286/294 [1:51:37<03:19, 24.97s/it]Running Inference:  98%|█████████▊| 287/294 [1:52:00<02:51, 24.53s/it]Running Inference:  98%|█████████▊| 288/294 [1:52:24<02:25, 24.30s/it]Running Inference:  98%|█████████▊| 289/294 [1:52:44<01:54, 22.93s/it]Running Inference:  99%|█████████▊| 290/294 [1:53:07<01:31, 22.94s/it]Running Inference:  99%|█████████▉| 291/294 [1:53:29<01:08, 22.93s/it]Running Inference:  99%|█████████▉| 292/294 [1:53:34<00:34, 17.43s/it]Running Inference: 100%|█████████▉| 293/294 [1:53:57<00:19, 19.20s/it]Running Inference: 100%|██████████| 294/294 [1:54:20<00:00, 20.32s/it]Running Inference: 100%|██████████| 294/294 [1:54:20<00:00, 23.34s/it]
2025-12-15 15:18:14,539 - INFO - Inference completed.
2025-12-15 15:18:14,570 - INFO - Results saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-15 15:18:14,570 - INFO - Calculating metrics for dataset: longbench
2025-12-15 15:18:27,207 - INFO - Metrics saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-15 15:18:27,207 - INFO - Metrics:
11.1
2025-12-15 15:18:27,209 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news_e, ratio=0.2) on GPU 1

----------------------------------------
Task: multi_news_e | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 15:18:34,079 - INFO - Set deterministic seeds to 42
2025-12-15 15:18:34,079 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 15:18:34,079 - INFO - Starting evaluation run...
2025-12-15 15:18:34,079 - INFO - Output directory set to: longbenchresult
2025-12-15 15:18:34,079 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-15 15:18:34,079 - INFO - KV Press 'tova' setup.
2025-12-15 15:18:34,079 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 15:18:34,079 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.71it/s]
Device set to use cuda:0
2025-12-15 15:18:55,667 - INFO - Model pipeline loaded.
2025-12-15 15:18:55,667 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news_e)
2025-12-15 15:19:00,744 - INFO - Dataset loaded with 294 entries.
2025-12-15 15:19:00,744 - INFO - Dataset processed with 294 entries.
2025-12-15 15:19:00,779 - INFO - Starting inference...
Running Inference:   0%|          | 0/294 [00:00<?, ?it/s]Running Inference:   0%|          | 1/294 [00:23<1:55:14, 23.60s/it]Running Inference:   1%|          | 2/294 [00:47<1:55:04, 23.64s/it]Running Inference:   1%|          | 3/294 [01:09<1:52:15, 23.14s/it]Running Inference:   1%|▏         | 4/294 [01:34<1:54:27, 23.68s/it]Running Inference:   2%|▏         | 5/294 [01:57<1:52:26, 23.34s/it]Running Inference:   2%|▏         | 6/294 [02:19<1:50:44, 23.07s/it]Running Inference:   2%|▏         | 7/294 [02:42<1:49:48, 22.96s/it]Running Inference:   3%|▎         | 8/294 [03:06<1:51:22, 23.37s/it]Running Inference:   3%|▎         | 9/294 [03:30<1:51:50, 23.55s/it]Running Inference:   3%|▎         | 10/294 [03:53<1:50:47, 23.41s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/294 [04:23<1:59:39, 25.37s/it]Running Inference:   4%|▍         | 12/294 [04:46<1:56:17, 24.74s/it]Running Inference:   4%|▍         | 13/294 [04:48<1:23:30, 17.83s/it]Running Inference:   5%|▍         | 14/294 [05:11<1:29:59, 19.28s/it]Running Inference:   5%|▌         | 15/294 [05:34<1:35:05, 20.45s/it]Running Inference:   5%|▌         | 16/294 [05:57<1:38:33, 21.27s/it]Running Inference:   6%|▌         | 17/294 [06:20<1:40:19, 21.73s/it]Running Inference:   6%|▌         | 18/294 [06:43<1:42:10, 22.21s/it]Running Inference:   6%|▋         | 19/294 [07:06<1:42:36, 22.39s/it]Running Inference:   7%|▋         | 20/294 [07:29<1:43:11, 22.60s/it]Running Inference:   7%|▋         | 21/294 [07:52<1:43:08, 22.67s/it]Running Inference:   7%|▋         | 22/294 [08:15<1:43:17, 22.79s/it]Running Inference:   8%|▊         | 23/294 [08:38<1:43:24, 22.90s/it]Running Inference:   8%|▊         | 24/294 [09:01<1:43:11, 22.93s/it]Running Inference:   9%|▊         | 25/294 [09:25<1:43:38, 23.12s/it]Running Inference:   9%|▉         | 26/294 [09:47<1:42:33, 22.96s/it]Running Inference:   9%|▉         | 27/294 [10:11<1:43:08, 23.18s/it]Running Inference:  10%|▉         | 28/294 [10:34<1:42:45, 23.18s/it]Running Inference:  10%|▉         | 29/294 [10:58<1:43:28, 23.43s/it]Running Inference:  10%|█         | 30/294 [11:21<1:42:38, 23.33s/it]Running Inference:  11%|█         | 31/294 [11:46<1:44:04, 23.74s/it]Running Inference:  11%|█         | 32/294 [12:09<1:42:51, 23.55s/it]Running Inference:  11%|█         | 33/294 [12:32<1:41:14, 23.27s/it]Running Inference:  12%|█▏        | 34/294 [12:55<1:40:36, 23.22s/it]Running Inference:  12%|█▏        | 35/294 [13:18<1:39:34, 23.07s/it]Running Inference:  12%|█▏        | 36/294 [13:41<1:39:29, 23.14s/it]Running Inference:  13%|█▎        | 37/294 [14:05<1:39:50, 23.31s/it]Running Inference:  13%|█▎        | 38/294 [14:28<1:39:17, 23.27s/it]Running Inference:  13%|█▎        | 39/294 [14:54<1:42:57, 24.23s/it]Running Inference:  14%|█▎        | 40/294 [15:19<1:42:42, 24.26s/it]Running Inference:  14%|█▍        | 41/294 [15:43<1:42:39, 24.35s/it]Running Inference:  14%|█▍        | 42/294 [16:07<1:41:50, 24.25s/it]Running Inference:  15%|█▍        | 43/294 [16:30<1:40:04, 23.92s/it]Running Inference:  15%|█▍        | 44/294 [16:53<1:38:14, 23.58s/it]Running Inference:  15%|█▌        | 45/294 [17:16<1:36:50, 23.34s/it]Running Inference:  16%|█▌        | 46/294 [17:39<1:36:24, 23.33s/it]Running Inference:  16%|█▌        | 47/294 [18:02<1:35:15, 23.14s/it]Running Inference:  16%|█▋        | 48/294 [18:27<1:36:43, 23.59s/it]Running Inference:  17%|█▋        | 49/294 [18:51<1:36:59, 23.75s/it]Running Inference:  17%|█▋        | 50/294 [19:13<1:35:16, 23.43s/it]Running Inference:  17%|█▋        | 51/294 [19:37<1:35:18, 23.53s/it]Running Inference:  18%|█▊        | 52/294 [20:02<1:36:35, 23.95s/it]Running Inference:  18%|█▊        | 53/294 [20:25<1:34:55, 23.63s/it]Running Inference:  18%|█▊        | 54/294 [20:48<1:34:22, 23.59s/it]Running Inference:  19%|█▊        | 55/294 [21:11<1:32:57, 23.33s/it]Running Inference:  19%|█▉        | 56/294 [21:34<1:31:47, 23.14s/it]Running Inference:  19%|█▉        | 57/294 [21:57<1:31:44, 23.23s/it]Running Inference:  20%|█▉        | 58/294 [22:20<1:31:05, 23.16s/it]Running Inference:  20%|██        | 59/294 [22:44<1:31:34, 23.38s/it]Running Inference:  20%|██        | 60/294 [23:08<1:32:12, 23.64s/it]Running Inference:  21%|██        | 61/294 [23:32<1:32:07, 23.72s/it]Running Inference:  21%|██        | 62/294 [23:55<1:30:45, 23.47s/it]Running Inference:  21%|██▏       | 63/294 [24:18<1:29:25, 23.23s/it]Running Inference:  22%|██▏       | 64/294 [24:41<1:28:57, 23.20s/it]Running Inference:  22%|██▏       | 65/294 [25:04<1:27:52, 23.02s/it]Running Inference:  22%|██▏       | 66/294 [25:28<1:29:00, 23.42s/it]Running Inference:  23%|██▎       | 67/294 [25:51<1:28:26, 23.38s/it]Running Inference:  23%|██▎       | 68/294 [26:14<1:27:15, 23.17s/it]Running Inference:  23%|██▎       | 69/294 [26:37<1:26:52, 23.17s/it]Running Inference:  24%|██▍       | 70/294 [27:00<1:25:54, 23.01s/it]Running Inference:  24%|██▍       | 71/294 [27:23<1:25:59, 23.14s/it]Running Inference:  24%|██▍       | 72/294 [27:46<1:25:11, 23.02s/it]Running Inference:  25%|██▍       | 73/294 [28:09<1:24:56, 23.06s/it]Running Inference:  25%|██▌       | 74/294 [28:32<1:24:09, 22.95s/it]Running Inference:  26%|██▌       | 75/294 [28:54<1:23:31, 22.89s/it]Running Inference:  26%|██▌       | 76/294 [29:17<1:23:06, 22.87s/it]Running Inference:  26%|██▌       | 77/294 [29:40<1:22:54, 22.93s/it]Running Inference:  27%|██▋       | 78/294 [30:04<1:22:58, 23.05s/it]Running Inference:  27%|██▋       | 79/294 [30:27<1:22:56, 23.15s/it]Running Inference:  27%|██▋       | 80/294 [30:51<1:23:19, 23.36s/it]Running Inference:  28%|██▊       | 81/294 [31:15<1:24:07, 23.70s/it]Running Inference:  28%|██▊       | 82/294 [31:38<1:22:58, 23.48s/it]Running Inference:  28%|██▊       | 83/294 [32:02<1:22:44, 23.53s/it]Running Inference:  29%|██▊       | 84/294 [32:24<1:21:11, 23.20s/it]Running Inference:  29%|██▉       | 85/294 [32:48<1:21:21, 23.36s/it]Running Inference:  29%|██▉       | 86/294 [33:12<1:21:13, 23.43s/it]Running Inference:  30%|██▉       | 87/294 [33:34<1:19:49, 23.14s/it]Running Inference:  30%|██▉       | 88/294 [33:57<1:19:07, 23.04s/it]Running Inference:  30%|███       | 89/294 [34:21<1:19:24, 23.24s/it]Running Inference:  31%|███       | 90/294 [34:44<1:19:00, 23.24s/it]Running Inference:  31%|███       | 91/294 [35:10<1:21:00, 23.95s/it]Running Inference:  31%|███▏      | 92/294 [35:32<1:19:27, 23.60s/it]Running Inference:  32%|███▏      | 93/294 [35:55<1:18:32, 23.44s/it]Running Inference:  32%|███▏      | 94/294 [36:20<1:19:20, 23.80s/it]Running Inference:  32%|███▏      | 95/294 [36:43<1:17:36, 23.40s/it]Running Inference:  33%|███▎      | 96/294 [37:12<1:23:14, 25.23s/it]Running Inference:  33%|███▎      | 97/294 [37:35<1:20:13, 24.43s/it]Running Inference:  33%|███▎      | 98/294 [37:57<1:17:31, 23.73s/it]Running Inference:  34%|███▎      | 99/294 [38:21<1:17:24, 23.82s/it]Running Inference:  34%|███▍      | 100/294 [38:43<1:15:52, 23.47s/it]Running Inference:  34%|███▍      | 101/294 [38:54<1:03:03, 19.60s/it]Running Inference:  35%|███▍      | 102/294 [39:17<1:05:55, 20.60s/it]Running Inference:  35%|███▌      | 103/294 [39:40<1:07:32, 21.22s/it]Running Inference:  35%|███▌      | 104/294 [40:03<1:08:59, 21.79s/it]Running Inference:  36%|███▌      | 105/294 [40:25<1:09:18, 22.00s/it]Running Inference:  36%|███▌      | 106/294 [40:53<1:14:05, 23.65s/it]Running Inference:  36%|███▋      | 107/294 [41:15<1:12:48, 23.36s/it]Running Inference:  37%|███▋      | 108/294 [41:41<1:14:16, 23.96s/it]Running Inference:  37%|███▋      | 109/294 [42:05<1:13:53, 23.96s/it]Running Inference:  37%|███▋      | 110/294 [42:28<1:12:38, 23.69s/it]Running Inference:  38%|███▊      | 111/294 [42:51<1:11:26, 23.42s/it]Running Inference:  38%|███▊      | 112/294 [43:16<1:12:52, 24.02s/it]Running Inference:  38%|███▊      | 113/294 [43:40<1:12:14, 23.95s/it]Running Inference:  39%|███▉      | 114/294 [44:02<1:10:28, 23.49s/it]Running Inference:  39%|███▉      | 115/294 [44:25<1:09:16, 23.22s/it]Running Inference:  39%|███▉      | 116/294 [44:48<1:09:08, 23.30s/it]Running Inference:  40%|███▉      | 117/294 [45:11<1:08:16, 23.14s/it]Running Inference:  40%|████      | 118/294 [45:34<1:07:29, 23.01s/it]Running Inference:  40%|████      | 119/294 [45:57<1:07:16, 23.07s/it]Running Inference:  41%|████      | 120/294 [46:20<1:06:46, 23.03s/it]Running Inference:  41%|████      | 121/294 [46:47<1:09:45, 24.19s/it]Running Inference:  41%|████▏     | 122/294 [47:11<1:09:12, 24.14s/it]Running Inference:  42%|████▏     | 123/294 [47:34<1:08:22, 23.99s/it]Running Inference:  42%|████▏     | 124/294 [47:36<48:57, 17.28s/it]  Running Inference:  43%|████▎     | 125/294 [47:59<53:28, 18.99s/it]Running Inference:  43%|████▎     | 126/294 [48:22<56:33, 20.20s/it]Running Inference:  43%|████▎     | 127/294 [48:46<59:06, 21.24s/it]Running Inference:  44%|████▎     | 128/294 [49:10<1:01:01, 22.05s/it]Running Inference:  44%|████▍     | 129/294 [49:32<1:01:08, 22.23s/it]Running Inference:  44%|████▍     | 130/294 [49:55<1:00:57, 22.30s/it]Running Inference:  45%|████▍     | 131/294 [50:18<1:01:12, 22.53s/it]Running Inference:  45%|████▍     | 132/294 [50:40<1:00:49, 22.53s/it]Running Inference:  45%|████▌     | 133/294 [51:03<1:00:28, 22.54s/it]Running Inference:  46%|████▌     | 134/294 [51:25<1:00:03, 22.52s/it]Running Inference:  46%|████▌     | 135/294 [51:48<59:59, 22.64s/it]  Running Inference:  46%|████▋     | 136/294 [52:11<59:27, 22.58s/it]Running Inference:  47%|████▋     | 137/294 [52:33<59:00, 22.55s/it]Running Inference:  47%|████▋     | 138/294 [52:56<59:00, 22.70s/it]Running Inference:  47%|████▋     | 139/294 [53:19<58:29, 22.64s/it]Running Inference:  48%|████▊     | 140/294 [53:43<59:26, 23.16s/it]Running Inference:  48%|████▊     | 141/294 [54:07<59:21, 23.28s/it]Running Inference:  48%|████▊     | 142/294 [54:29<58:31, 23.10s/it]Running Inference:  49%|████▊     | 143/294 [54:52<58:00, 23.05s/it]Running Inference:  49%|████▉     | 144/294 [55:15<57:28, 22.99s/it]Running Inference:  49%|████▉     | 145/294 [55:38<56:58, 22.94s/it]Running Inference:  50%|████▉     | 146/294 [56:00<56:11, 22.78s/it]Running Inference:  50%|█████     | 147/294 [56:24<56:26, 23.04s/it]Running Inference:  50%|█████     | 148/294 [56:47<55:39, 22.87s/it]Running Inference:  51%|█████     | 149/294 [57:09<54:54, 22.72s/it]Running Inference:  51%|█████     | 150/294 [57:32<54:52, 22.87s/it]Running Inference:  51%|█████▏    | 151/294 [57:55<54:27, 22.85s/it]Running Inference:  52%|█████▏    | 152/294 [58:18<53:52, 22.76s/it]Running Inference:  52%|█████▏    | 153/294 [58:41<54:07, 23.03s/it]Running Inference:  52%|█████▏    | 154/294 [59:04<53:49, 23.07s/it]Running Inference:  53%|█████▎    | 155/294 [59:28<54:01, 23.32s/it]Running Inference:  53%|█████▎    | 156/294 [59:52<54:08, 23.54s/it]Running Inference:  53%|█████▎    | 157/294 [1:00:15<53:24, 23.39s/it]Running Inference:  54%|█████▎    | 158/294 [1:00:40<53:33, 23.63s/it]Running Inference:  54%|█████▍    | 159/294 [1:01:02<52:31, 23.34s/it]Running Inference:  54%|█████▍    | 160/294 [1:01:25<51:45, 23.18s/it]Running Inference:  55%|█████▍    | 161/294 [1:01:47<50:52, 22.95s/it]Running Inference:  55%|█████▌    | 162/294 [1:02:11<51:03, 23.21s/it]Running Inference:  55%|█████▌    | 163/294 [1:02:36<51:44, 23.70s/it]Running Inference:  56%|█████▌    | 164/294 [1:02:39<37:41, 17.40s/it]Running Inference:  56%|█████▌    | 165/294 [1:03:02<41:09, 19.14s/it]Running Inference:  56%|█████▋    | 166/294 [1:03:25<43:18, 20.30s/it]Running Inference:  57%|█████▋    | 167/294 [1:03:49<45:26, 21.47s/it]Running Inference:  57%|█████▋    | 168/294 [1:04:12<45:46, 21.80s/it]Running Inference:  57%|█████▋    | 169/294 [1:04:35<46:35, 22.37s/it]Running Inference:  58%|█████▊    | 170/294 [1:05:00<47:21, 22.92s/it]Running Inference:  58%|█████▊    | 171/294 [1:05:22<46:42, 22.78s/it]Running Inference:  59%|█████▊    | 172/294 [1:05:45<46:38, 22.94s/it]Running Inference:  59%|█████▉    | 173/294 [1:06:09<46:56, 23.28s/it]Running Inference:  59%|█████▉    | 174/294 [1:06:32<45:59, 22.99s/it]Running Inference:  60%|█████▉    | 175/294 [1:06:52<43:54, 22.14s/it]Running Inference:  60%|█████▉    | 176/294 [1:07:14<43:43, 22.24s/it]Running Inference:  60%|██████    | 177/294 [1:07:38<44:09, 22.65s/it]Running Inference:  61%|██████    | 178/294 [1:08:02<44:35, 23.07s/it]Running Inference:  61%|██████    | 179/294 [1:08:25<43:57, 22.93s/it]Running Inference:  61%|██████    | 180/294 [1:08:49<44:27, 23.40s/it]Running Inference:  62%|██████▏   | 181/294 [1:09:12<43:55, 23.32s/it]Running Inference:  62%|██████▏   | 182/294 [1:09:36<43:53, 23.51s/it]Running Inference:  62%|██████▏   | 183/294 [1:10:00<43:48, 23.68s/it]Running Inference:  63%|██████▎   | 184/294 [1:10:25<43:52, 23.93s/it]Running Inference:  63%|██████▎   | 185/294 [1:10:48<42:52, 23.60s/it]Running Inference:  63%|██████▎   | 186/294 [1:11:12<42:45, 23.75s/it]Running Inference:  64%|██████▎   | 187/294 [1:11:35<42:07, 23.62s/it]Running Inference:  64%|██████▍   | 188/294 [1:11:58<41:09, 23.30s/it]Running Inference:  64%|██████▍   | 189/294 [1:12:20<40:23, 23.08s/it]Running Inference:  65%|██████▍   | 190/294 [1:12:43<39:43, 22.92s/it]Running Inference:  65%|██████▍   | 191/294 [1:13:05<39:04, 22.76s/it]Running Inference:  65%|██████▌   | 192/294 [1:13:28<38:34, 22.69s/it]Running Inference:  66%|██████▌   | 193/294 [1:13:51<38:42, 22.99s/it]Running Inference:  66%|██████▌   | 194/294 [1:14:15<38:40, 23.21s/it]Running Inference:  66%|██████▋   | 195/294 [1:14:39<38:40, 23.44s/it]Running Inference:  67%|██████▋   | 196/294 [1:15:03<38:25, 23.52s/it]Running Inference:  67%|██████▋   | 197/294 [1:15:27<38:33, 23.85s/it]Running Inference:  67%|██████▋   | 198/294 [1:15:50<37:26, 23.40s/it]Running Inference:  68%|██████▊   | 199/294 [1:16:13<37:01, 23.38s/it]Running Inference:  68%|██████▊   | 200/294 [1:16:35<36:09, 23.08s/it]Running Inference:  68%|██████▊   | 201/294 [1:16:59<36:12, 23.36s/it]Running Inference:  69%|██████▊   | 202/294 [1:17:22<35:25, 23.10s/it]Running Inference:  69%|██████▉   | 203/294 [1:17:46<35:16, 23.26s/it]Running Inference:  69%|██████▉   | 204/294 [1:18:09<34:49, 23.22s/it]Running Inference:  70%|██████▉   | 205/294 [1:18:32<34:37, 23.35s/it]Running Inference:  70%|███████   | 206/294 [1:18:56<34:14, 23.35s/it]Running Inference:  70%|███████   | 207/294 [1:19:19<33:52, 23.36s/it]Running Inference:  71%|███████   | 208/294 [1:19:42<33:29, 23.37s/it]Running Inference:  71%|███████   | 209/294 [1:20:06<32:59, 23.29s/it]Running Inference:  71%|███████▏  | 210/294 [1:20:30<32:59, 23.56s/it]Running Inference:  72%|███████▏  | 211/294 [1:20:53<32:16, 23.33s/it]Running Inference:  72%|███████▏  | 212/294 [1:21:15<31:33, 23.09s/it]Running Inference:  72%|███████▏  | 213/294 [1:21:39<31:29, 23.33s/it]Running Inference:  73%|███████▎  | 214/294 [1:22:02<30:57, 23.22s/it]Running Inference:  73%|███████▎  | 215/294 [1:22:26<30:51, 23.43s/it]Running Inference:  73%|███████▎  | 216/294 [1:22:49<30:26, 23.41s/it]Running Inference:  74%|███████▍  | 217/294 [1:23:13<30:03, 23.42s/it]Running Inference:  74%|███████▍  | 218/294 [1:23:35<29:23, 23.21s/it]Running Inference:  74%|███████▍  | 219/294 [1:23:58<28:47, 23.03s/it]Running Inference:  75%|███████▍  | 220/294 [1:24:21<28:20, 22.97s/it]Running Inference:  75%|███████▌  | 221/294 [1:24:44<27:52, 22.91s/it]Running Inference:  76%|███████▌  | 222/294 [1:25:07<27:38, 23.04s/it]Running Inference:  76%|███████▌  | 223/294 [1:25:31<27:33, 23.29s/it]Running Inference:  76%|███████▌  | 224/294 [1:25:54<27:02, 23.18s/it]Running Inference:  77%|███████▋  | 225/294 [1:26:17<26:46, 23.28s/it]Running Inference:  77%|███████▋  | 226/294 [1:26:41<26:29, 23.38s/it]Running Inference:  77%|███████▋  | 227/294 [1:26:50<21:22, 19.14s/it]Running Inference:  78%|███████▊  | 228/294 [1:27:14<22:34, 20.52s/it]Running Inference:  78%|███████▊  | 229/294 [1:27:38<23:18, 21.52s/it]Running Inference:  78%|███████▊  | 230/294 [1:28:01<23:25, 21.96s/it]Running Inference:  79%|███████▊  | 231/294 [1:28:24<23:34, 22.45s/it]Running Inference:  79%|███████▉  | 232/294 [1:28:47<23:21, 22.61s/it]Running Inference:  79%|███████▉  | 233/294 [1:29:11<23:23, 23.01s/it]Running Inference:  80%|███████▉  | 234/294 [1:29:35<23:07, 23.12s/it]Running Inference:  80%|███████▉  | 235/294 [1:29:58<22:55, 23.31s/it]Running Inference:  80%|████████  | 236/294 [1:30:22<22:37, 23.41s/it]Running Inference:  81%|████████  | 237/294 [1:30:45<22:08, 23.31s/it]Running Inference:  81%|████████  | 238/294 [1:31:09<21:57, 23.52s/it]Running Inference:  81%|████████▏ | 239/294 [1:31:32<21:31, 23.48s/it]Running Inference:  82%|████████▏ | 240/294 [1:31:57<21:21, 23.73s/it]Running Inference:  82%|████████▏ | 241/294 [1:32:20<20:53, 23.65s/it]Running Inference:  82%|████████▏ | 242/294 [1:32:45<20:40, 23.85s/it]Running Inference:  83%|████████▎ | 243/294 [1:33:08<20:10, 23.73s/it]Running Inference:  83%|████████▎ | 244/294 [1:33:31<19:36, 23.52s/it]Running Inference:  83%|████████▎ | 245/294 [1:33:32<13:38, 16.70s/it]Running Inference:  84%|████████▎ | 246/294 [1:33:56<15:15, 19.08s/it]Running Inference:  84%|████████▍ | 247/294 [1:33:59<11:00, 14.06s/it]Running Inference:  84%|████████▍ | 248/294 [1:34:22<12:51, 16.77s/it]Running Inference:  85%|████████▍ | 249/294 [1:34:47<14:20, 19.13s/it]Running Inference:  85%|████████▌ | 250/294 [1:35:10<15:05, 20.58s/it]Running Inference:  85%|████████▌ | 251/294 [1:35:34<15:26, 21.53s/it]Running Inference:  86%|████████▌ | 252/294 [1:35:57<15:18, 21.88s/it]Running Inference:  86%|████████▌ | 253/294 [1:36:20<15:13, 22.28s/it]Running Inference:  86%|████████▋ | 254/294 [1:36:43<15:01, 22.54s/it]Running Inference:  87%|████████▋ | 255/294 [1:37:07<14:54, 22.93s/it]Running Inference:  87%|████████▋ | 256/294 [1:37:32<14:47, 23.36s/it]Running Inference:  87%|████████▋ | 257/294 [1:37:54<14:17, 23.19s/it]Running Inference:  88%|████████▊ | 258/294 [1:38:17<13:54, 23.17s/it]Running Inference:  88%|████████▊ | 259/294 [1:38:40<13:25, 23.01s/it]Running Inference:  88%|████████▊ | 260/294 [1:39:03<13:01, 22.98s/it]Running Inference:  89%|████████▉ | 261/294 [1:39:26<12:43, 23.15s/it]Running Inference:  89%|████████▉ | 262/294 [1:39:50<12:19, 23.12s/it]Running Inference:  89%|████████▉ | 263/294 [1:40:08<11:15, 21.79s/it]Running Inference:  90%|████████▉ | 264/294 [1:40:18<09:09, 18.32s/it]Running Inference:  90%|█████████ | 265/294 [1:40:42<09:34, 19.81s/it]Running Inference:  90%|█████████ | 266/294 [1:41:05<09:44, 20.86s/it]Running Inference:  91%|█████████ | 267/294 [1:41:29<09:48, 21.79s/it]Running Inference:  91%|█████████ | 268/294 [1:42:00<10:40, 24.64s/it]Running Inference:  91%|█████████▏| 269/294 [1:42:23<10:04, 24.19s/it]Running Inference:  92%|█████████▏| 270/294 [1:42:47<09:38, 24.11s/it]Running Inference:  92%|█████████▏| 271/294 [1:43:10<09:04, 23.67s/it]Running Inference:  93%|█████████▎| 272/294 [1:43:12<06:20, 17.28s/it]Running Inference:  93%|█████████▎| 273/294 [1:43:35<06:37, 18.95s/it]Running Inference:  93%|█████████▎| 274/294 [1:43:59<06:49, 20.45s/it]Running Inference:  94%|█████████▎| 275/294 [1:44:22<06:44, 21.30s/it]Running Inference:  94%|█████████▍| 276/294 [1:44:45<06:31, 21.72s/it]Running Inference:  94%|█████████▍| 277/294 [1:45:09<06:18, 22.24s/it]Running Inference:  95%|█████████▍| 278/294 [1:45:32<06:02, 22.66s/it]Running Inference:  95%|█████████▍| 279/294 [1:45:56<05:46, 23.13s/it]Running Inference:  95%|█████████▌| 280/294 [1:46:23<05:36, 24.06s/it]Running Inference:  96%|█████████▌| 281/294 [1:46:47<05:11, 23.99s/it]Running Inference:  96%|█████████▌| 282/294 [1:47:11<04:47, 23.98s/it]Running Inference:  96%|█████████▋| 283/294 [1:47:40<04:43, 25.77s/it]Running Inference:  97%|█████████▋| 284/294 [1:48:05<04:12, 25.28s/it]Running Inference:  97%|█████████▋| 285/294 [1:48:28<03:42, 24.77s/it]Running Inference:  97%|█████████▋| 286/294 [1:48:53<03:17, 24.67s/it]Running Inference:  98%|█████████▊| 287/294 [1:49:16<02:49, 24.27s/it]Running Inference:  98%|█████████▊| 288/294 [1:49:39<02:24, 24.04s/it]Running Inference:  98%|█████████▊| 289/294 [1:50:03<01:59, 23.81s/it]Running Inference:  99%|█████████▊| 290/294 [1:50:25<01:34, 23.50s/it]Running Inference:  99%|█████████▉| 291/294 [1:50:48<01:09, 23.27s/it]Running Inference:  99%|█████████▉| 292/294 [1:51:11<00:46, 23.19s/it]Running Inference: 100%|█████████▉| 293/294 [1:51:34<00:23, 23.15s/it]Running Inference: 100%|██████████| 294/294 [1:51:57<00:00, 23.01s/it]Running Inference: 100%|██████████| 294/294 [1:51:57<00:00, 22.85s/it]
2025-12-15 17:10:58,276 - INFO - Inference completed.
2025-12-15 17:10:58,308 - INFO - Results saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-15 17:10:58,308 - INFO - Calculating metrics for dataset: longbench
2025-12-15 17:11:11,051 - INFO - Metrics saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-15 17:11:11,051 - INFO - Metrics:
11.1
2025-12-15 17:11:11,053 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news_e, ratio=0.3) on GPU 1

----------------------------------------
Task: multi_news_e | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 17:11:20,218 - INFO - Set deterministic seeds to 42
2025-12-15 17:11:20,218 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 17:11:20,218 - INFO - Starting evaluation run...
2025-12-15 17:11:20,218 - INFO - Output directory set to: longbenchresult
2025-12-15 17:11:20,218 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-15 17:11:20,218 - INFO - KV Press 'tova' setup.
2025-12-15 17:11:20,218 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 17:11:20,219 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.24it/s]
Device set to use cuda:0
2025-12-15 17:11:34,639 - INFO - Model pipeline loaded.
2025-12-15 17:11:34,639 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news_e)
2025-12-15 17:11:38,570 - INFO - Dataset loaded with 294 entries.
2025-12-15 17:11:38,570 - INFO - Dataset processed with 294 entries.
2025-12-15 17:11:38,607 - INFO - Starting inference...
Running Inference:   0%|          | 0/294 [00:00<?, ?it/s]Running Inference:   0%|          | 1/294 [00:23<1:54:44, 23.50s/it]Running Inference:   1%|          | 2/294 [00:47<1:54:48, 23.59s/it]Running Inference:   1%|          | 3/294 [01:09<1:52:01, 23.10s/it]Running Inference:   1%|▏         | 4/294 [01:34<1:54:11, 23.63s/it]Running Inference:   2%|▏         | 5/294 [01:56<1:52:12, 23.30s/it]Running Inference:   2%|▏         | 6/294 [02:19<1:50:33, 23.03s/it]Running Inference:   2%|▏         | 7/294 [02:42<1:49:39, 22.92s/it]Running Inference:   3%|▎         | 8/294 [03:06<1:51:09, 23.32s/it]Running Inference:   3%|▎         | 9/294 [03:30<1:51:36, 23.50s/it]Running Inference:   3%|▎         | 10/294 [03:53<1:50:35, 23.36s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/294 [04:21<1:57:52, 24.99s/it]Running Inference:   4%|▍         | 12/294 [04:45<1:54:54, 24.45s/it]Running Inference:   4%|▍         | 13/294 [05:07<1:51:51, 23.89s/it]Running Inference:   5%|▍         | 14/294 [05:30<1:49:43, 23.51s/it]Running Inference:   5%|▌         | 15/294 [05:53<1:48:47, 23.40s/it]Running Inference:   5%|▌         | 16/294 [06:16<1:48:03, 23.32s/it]Running Inference:   6%|▌         | 17/294 [06:39<1:46:53, 23.15s/it]Running Inference:   6%|▌         | 18/294 [07:02<1:46:40, 23.19s/it]Running Inference:   6%|▋         | 19/294 [07:25<1:45:40, 23.05s/it]Running Inference:   7%|▋         | 20/294 [07:48<1:45:18, 23.06s/it]Running Inference:   7%|▋         | 21/294 [08:11<1:44:33, 22.98s/it]Running Inference:   7%|▋         | 22/294 [08:34<1:44:19, 23.01s/it]Running Inference:   8%|▊         | 23/294 [08:57<1:44:05, 23.05s/it]Running Inference:   8%|▊         | 24/294 [09:20<1:43:35, 23.02s/it]Running Inference:   9%|▊         | 25/294 [09:43<1:43:46, 23.15s/it]Running Inference:   9%|▉         | 26/294 [10:06<1:42:34, 22.97s/it]Running Inference:   9%|▉         | 27/294 [10:29<1:43:03, 23.16s/it]Running Inference:  10%|▉         | 28/294 [10:53<1:42:36, 23.14s/it]Running Inference:  10%|▉         | 29/294 [11:16<1:43:14, 23.38s/it]Running Inference:  10%|█         | 30/294 [11:39<1:42:22, 23.27s/it]Running Inference:  11%|█         | 31/294 [12:04<1:43:43, 23.66s/it]Running Inference:  11%|█         | 32/294 [12:27<1:42:27, 23.46s/it]Running Inference:  11%|█         | 33/294 [12:50<1:41:03, 23.23s/it]Running Inference:  12%|█▏        | 34/294 [13:13<1:40:23, 23.17s/it]Running Inference:  12%|█▏        | 35/294 [13:35<1:39:20, 23.02s/it]Running Inference:  12%|█▏        | 36/294 [13:59<1:39:01, 23.03s/it]Running Inference:  13%|█▎        | 37/294 [14:22<1:39:23, 23.20s/it]Running Inference:  13%|█▎        | 38/294 [14:45<1:38:47, 23.16s/it]Running Inference:  13%|█▎        | 39/294 [15:11<1:41:44, 23.94s/it]Running Inference:  14%|█▎        | 40/294 [15:35<1:41:35, 24.00s/it]Running Inference:  14%|█▍        | 41/294 [15:59<1:41:37, 24.10s/it]Running Inference:  14%|█▍        | 42/294 [16:23<1:40:53, 24.02s/it]Running Inference:  15%|█▍        | 43/294 [16:46<1:39:09, 23.70s/it]Running Inference:  15%|█▍        | 44/294 [17:09<1:37:20, 23.36s/it]Running Inference:  15%|█▌        | 45/294 [17:31<1:36:01, 23.14s/it]Running Inference:  16%|█▌        | 46/294 [17:55<1:35:39, 23.14s/it]Running Inference:  16%|█▌        | 47/294 [18:17<1:34:37, 22.99s/it]Running Inference:  16%|█▋        | 48/294 [18:42<1:36:08, 23.45s/it]Running Inference:  17%|█▋        | 49/294 [19:06<1:36:20, 23.59s/it]Running Inference:  17%|█▋        | 50/294 [19:28<1:34:52, 23.33s/it]Running Inference:  17%|█▋        | 51/294 [19:52<1:34:46, 23.40s/it]Running Inference:  18%|█▊        | 52/294 [20:17<1:35:55, 23.78s/it]Running Inference:  18%|█▊        | 53/294 [20:39<1:34:14, 23.46s/it]Running Inference:  18%|█▊        | 54/294 [21:03<1:33:42, 23.43s/it]Running Inference:  19%|█▊        | 55/294 [21:25<1:32:21, 23.19s/it]Running Inference:  19%|█▉        | 56/294 [21:48<1:31:14, 23.00s/it]Running Inference:  19%|█▉        | 57/294 [22:11<1:31:06, 23.07s/it]Running Inference:  20%|█▉        | 58/294 [22:34<1:30:26, 22.99s/it]Running Inference:  20%|██        | 59/294 [22:58<1:30:57, 23.22s/it]Running Inference:  20%|██        | 60/294 [23:22<1:31:32, 23.47s/it]Running Inference:  21%|██        | 61/294 [23:46<1:31:38, 23.60s/it]Running Inference:  21%|██        | 62/294 [24:09<1:30:38, 23.44s/it]Running Inference:  21%|██▏       | 63/294 [24:31<1:29:13, 23.18s/it]Running Inference:  22%|██▏       | 64/294 [24:54<1:28:43, 23.15s/it]Running Inference:  22%|██▏       | 65/294 [25:17<1:27:54, 23.03s/it]Running Inference:  22%|██▏       | 66/294 [25:41<1:29:06, 23.45s/it]Running Inference:  23%|██▎       | 67/294 [26:05<1:28:17, 23.34s/it]Running Inference:  23%|██▎       | 68/294 [26:27<1:26:58, 23.09s/it]Running Inference:  23%|██▎       | 69/294 [26:50<1:26:28, 23.06s/it]Running Inference:  24%|██▍       | 70/294 [27:13<1:25:40, 22.95s/it]Running Inference:  24%|██▍       | 71/294 [27:36<1:25:39, 23.04s/it]Running Inference:  24%|██▍       | 72/294 [27:59<1:24:48, 22.92s/it]Running Inference:  25%|██▍       | 73/294 [28:23<1:25:31, 23.22s/it]Running Inference:  25%|██▌       | 74/294 [28:45<1:24:29, 23.04s/it]Running Inference:  26%|██▌       | 75/294 [29:08<1:23:36, 22.91s/it]Running Inference:  26%|██▌       | 76/294 [29:30<1:22:59, 22.84s/it]Running Inference:  26%|██▌       | 77/294 [29:53<1:22:43, 22.87s/it]Running Inference:  27%|██▋       | 78/294 [30:17<1:22:40, 22.97s/it]Running Inference:  27%|██▋       | 79/294 [30:40<1:22:32, 23.04s/it]Running Inference:  27%|██▋       | 80/294 [31:04<1:22:54, 23.25s/it]Running Inference:  28%|██▊       | 81/294 [31:28<1:23:37, 23.56s/it]Running Inference:  28%|██▊       | 82/294 [31:51<1:22:38, 23.39s/it]Running Inference:  28%|██▊       | 83/294 [32:15<1:22:36, 23.49s/it]Running Inference:  29%|██▊       | 84/294 [32:37<1:21:13, 23.21s/it]Running Inference:  29%|██▉       | 85/294 [33:01<1:21:28, 23.39s/it]Running Inference:  29%|██▉       | 86/294 [33:25<1:21:25, 23.49s/it]Running Inference:  30%|██▉       | 87/294 [33:47<1:20:10, 23.24s/it]Running Inference:  30%|██▉       | 88/294 [34:10<1:19:33, 23.17s/it]Running Inference:  30%|███       | 89/294 [34:34<1:19:50, 23.37s/it]Running Inference:  31%|███       | 90/294 [34:57<1:19:22, 23.35s/it]Running Inference:  31%|███       | 91/294 [35:23<1:21:03, 23.96s/it]Running Inference:  31%|███▏      | 92/294 [35:46<1:19:35, 23.64s/it]Running Inference:  32%|███▏      | 93/294 [36:09<1:18:44, 23.51s/it]Running Inference:  32%|███▏      | 94/294 [36:34<1:19:39, 23.90s/it]Running Inference:  32%|███▏      | 95/294 [36:56<1:17:59, 23.51s/it]Running Inference:  33%|███▎      | 96/294 [37:25<1:22:29, 25.00s/it]Running Inference:  33%|███▎      | 97/294 [37:47<1:19:42, 24.28s/it]Running Inference:  33%|███▎      | 98/294 [38:10<1:17:35, 23.75s/it]Running Inference:  34%|███▎      | 99/294 [38:34<1:17:26, 23.83s/it]Running Inference:  34%|███▍      | 100/294 [38:57<1:15:52, 23.46s/it]Running Inference:  34%|███▍      | 101/294 [39:20<1:15:07, 23.36s/it]Running Inference:  35%|███▍      | 102/294 [39:43<1:14:27, 23.27s/it]Running Inference:  35%|███▌      | 103/294 [40:05<1:13:34, 23.11s/it]Running Inference:  35%|███▌      | 104/294 [40:29<1:13:16, 23.14s/it]Running Inference:  36%|███▌      | 105/294 [40:51<1:12:27, 23.00s/it]Running Inference:  36%|███▌      | 106/294 [41:18<1:15:37, 24.13s/it]Running Inference:  36%|███▋      | 107/294 [41:41<1:13:57, 23.73s/it]Running Inference:  37%|███▋      | 108/294 [42:06<1:15:00, 24.20s/it]Running Inference:  37%|███▋      | 109/294 [42:17<1:02:18, 20.21s/it]Running Inference:  37%|███▋      | 110/294 [42:40<1:04:38, 21.08s/it]Running Inference:  38%|███▊      | 111/294 [43:03<1:05:55, 21.61s/it]Running Inference:  38%|███▊      | 112/294 [43:28<1:08:52, 22.71s/it]Running Inference:  38%|███▊      | 113/294 [43:52<1:09:34, 23.06s/it]Running Inference:  39%|███▉      | 114/294 [44:15<1:08:45, 22.92s/it]Running Inference:  39%|███▉      | 115/294 [44:38<1:08:10, 22.85s/it]Running Inference:  39%|███▉      | 116/294 [45:01<1:08:27, 23.08s/it]Running Inference:  40%|███▉      | 117/294 [45:24<1:07:52, 23.01s/it]Running Inference:  40%|████      | 118/294 [45:47<1:07:13, 22.92s/it]Running Inference:  40%|████      | 119/294 [46:10<1:07:14, 23.06s/it]Running Inference:  41%|████      | 120/294 [46:33<1:06:50, 23.05s/it]Running Inference:  41%|████      | 121/294 [46:59<1:09:08, 23.98s/it]Running Inference:  41%|████▏     | 122/294 [47:23<1:08:51, 24.02s/it]Running Inference:  42%|████▏     | 123/294 [47:47<1:08:11, 23.93s/it]Running Inference:  42%|████▏     | 124/294 [47:49<48:50, 17.24s/it]  Running Inference:  43%|████▎     | 125/294 [48:12<53:34, 19.02s/it]Running Inference:  43%|████▎     | 126/294 [48:35<56:45, 20.27s/it]Running Inference:  43%|████▎     | 127/294 [48:59<59:23, 21.34s/it]Running Inference:  44%|████▎     | 128/294 [49:23<1:01:17, 22.15s/it]Running Inference:  44%|████▍     | 129/294 [49:46<1:01:29, 22.36s/it]Running Inference:  44%|████▍     | 130/294 [50:09<1:01:27, 22.48s/it]Running Inference:  45%|████▍     | 131/294 [50:32<1:01:38, 22.69s/it]Running Inference:  45%|████▍     | 132/294 [50:54<1:01:14, 22.68s/it]Running Inference:  45%|████▌     | 133/294 [51:17<1:00:58, 22.72s/it]Running Inference:  46%|████▌     | 134/294 [51:40<1:00:30, 22.69s/it]Running Inference:  46%|████▌     | 135/294 [52:03<1:00:26, 22.81s/it]Running Inference:  46%|████▋     | 136/294 [52:25<59:50, 22.73s/it]  Running Inference:  47%|████▋     | 137/294 [52:48<59:23, 22.70s/it]Running Inference:  47%|████▋     | 138/294 [53:11<59:19, 22.82s/it]Running Inference:  47%|████▋     | 139/294 [53:34<58:47, 22.76s/it]Running Inference:  48%|████▊     | 140/294 [53:58<59:42, 23.27s/it]Running Inference:  48%|████▊     | 141/294 [54:22<59:36, 23.38s/it]Running Inference:  48%|████▊     | 142/294 [54:45<58:45, 23.20s/it]Running Inference:  49%|████▊     | 143/294 [55:08<58:16, 23.16s/it]Running Inference:  49%|████▉     | 144/294 [55:31<57:45, 23.10s/it]Running Inference:  49%|████▉     | 145/294 [55:54<57:16, 23.06s/it]Running Inference:  50%|████▉     | 146/294 [56:16<56:28, 22.89s/it]Running Inference:  50%|█████     | 147/294 [56:40<56:42, 23.15s/it]Running Inference:  50%|█████     | 148/294 [57:03<56:09, 23.08s/it]Running Inference:  51%|█████     | 149/294 [57:25<55:29, 22.96s/it]Running Inference:  51%|█████     | 150/294 [57:49<55:25, 23.09s/it]Running Inference:  51%|█████▏    | 151/294 [58:12<55:00, 23.08s/it]Running Inference:  52%|█████▏    | 152/294 [58:35<54:24, 22.99s/it]Running Inference:  52%|█████▏    | 153/294 [58:59<54:37, 23.25s/it]Running Inference:  52%|█████▏    | 154/294 [59:22<54:20, 23.29s/it]Running Inference:  53%|█████▎    | 155/294 [59:46<54:31, 23.54s/it]Running Inference:  53%|█████▎    | 156/294 [1:00:10<54:37, 23.75s/it]Running Inference:  53%|█████▎    | 157/294 [1:00:34<53:52, 23.60s/it]Running Inference:  54%|█████▎    | 158/294 [1:00:56<52:56, 23.36s/it]Running Inference:  54%|█████▍    | 159/294 [1:01:19<52:12, 23.21s/it]Running Inference:  54%|█████▍    | 160/294 [1:01:42<51:38, 23.12s/it]Running Inference:  55%|█████▍    | 161/294 [1:02:05<50:55, 22.98s/it]Running Inference:  55%|█████▌    | 162/294 [1:02:29<51:11, 23.27s/it]Running Inference:  55%|█████▌    | 163/294 [1:02:54<51:54, 23.78s/it]Running Inference:  56%|█████▌    | 164/294 [1:03:12<47:43, 22.03s/it]Running Inference:  56%|█████▌    | 165/294 [1:03:35<48:12, 22.42s/it]Running Inference:  56%|█████▋    | 166/294 [1:03:58<48:19, 22.65s/it]Running Inference:  57%|█████▋    | 167/294 [1:04:22<48:58, 23.14s/it]Running Inference:  57%|█████▋    | 168/294 [1:04:45<48:18, 23.00s/it]Running Inference:  57%|█████▋    | 169/294 [1:05:09<48:28, 23.26s/it]Running Inference:  58%|█████▊    | 170/294 [1:05:33<48:41, 23.56s/it]Running Inference:  58%|█████▊    | 171/294 [1:05:56<47:44, 23.29s/it]Running Inference:  59%|█████▊    | 172/294 [1:06:19<47:23, 23.31s/it]Running Inference:  59%|█████▉    | 173/294 [1:06:43<47:32, 23.57s/it]Running Inference:  59%|█████▉    | 174/294 [1:07:06<46:37, 23.31s/it]Running Inference:  60%|█████▉    | 175/294 [1:07:28<45:17, 22.84s/it]Running Inference:  60%|█████▉    | 176/294 [1:07:50<44:43, 22.74s/it]Running Inference:  60%|██████    | 177/294 [1:08:14<44:54, 23.03s/it]Running Inference:  61%|██████    | 178/294 [1:08:38<45:11, 23.37s/it]Running Inference:  61%|██████    | 179/294 [1:09:01<44:26, 23.18s/it]Running Inference:  61%|██████    | 180/294 [1:09:26<44:52, 23.62s/it]Running Inference:  62%|██████▏   | 181/294 [1:09:49<44:15, 23.50s/it]Running Inference:  62%|██████▏   | 182/294 [1:10:13<44:10, 23.66s/it]Running Inference:  62%|██████▏   | 183/294 [1:10:37<44:04, 23.82s/it]Running Inference:  63%|██████▎   | 184/294 [1:11:02<44:05, 24.05s/it]Running Inference:  63%|██████▎   | 185/294 [1:11:25<43:05, 23.72s/it]Running Inference:  63%|██████▎   | 186/294 [1:11:49<42:56, 23.86s/it]Running Inference:  64%|██████▎   | 187/294 [1:12:12<42:18, 23.72s/it]Running Inference:  64%|██████▍   | 188/294 [1:12:35<41:26, 23.46s/it]Running Inference:  64%|██████▍   | 189/294 [1:12:58<40:46, 23.30s/it]Running Inference:  65%|██████▍   | 190/294 [1:13:21<40:10, 23.18s/it]Running Inference:  65%|██████▍   | 191/294 [1:13:44<39:35, 23.06s/it]Running Inference:  65%|██████▌   | 192/294 [1:14:06<39:03, 22.97s/it]Running Inference:  66%|██████▌   | 193/294 [1:14:31<39:12, 23.29s/it]Running Inference:  66%|██████▌   | 194/294 [1:14:54<39:10, 23.50s/it]Running Inference:  66%|██████▋   | 195/294 [1:15:19<39:08, 23.72s/it]Running Inference:  67%|██████▋   | 196/294 [1:15:43<38:52, 23.80s/it]Running Inference:  67%|██████▋   | 197/294 [1:16:08<39:00, 24.13s/it]Running Inference:  67%|██████▋   | 198/294 [1:16:30<37:59, 23.74s/it]Running Inference:  68%|██████▊   | 199/294 [1:16:54<37:32, 23.71s/it]Running Inference:  68%|██████▊   | 200/294 [1:17:17<36:41, 23.42s/it]Running Inference:  68%|██████▊   | 201/294 [1:17:37<35:00, 22.58s/it]Running Inference:  69%|██████▊   | 202/294 [1:18:00<34:42, 22.64s/it]Running Inference:  69%|██████▉   | 203/294 [1:18:24<34:52, 22.99s/it]Running Inference:  69%|██████▉   | 204/294 [1:18:47<34:38, 23.09s/it]Running Inference:  70%|██████▉   | 205/294 [1:19:11<34:34, 23.31s/it]Running Inference:  70%|███████   | 206/294 [1:19:35<34:18, 23.39s/it]Running Inference:  70%|███████   | 207/294 [1:19:58<33:59, 23.44s/it]Running Inference:  71%|███████   | 208/294 [1:20:22<33:39, 23.48s/it]Running Inference:  71%|███████   | 209/294 [1:20:45<33:08, 23.40s/it]Running Inference:  71%|███████▏  | 210/294 [1:21:09<33:06, 23.64s/it]Running Inference:  72%|███████▏  | 211/294 [1:21:32<32:27, 23.47s/it]Running Inference:  72%|███████▏  | 212/294 [1:21:55<31:46, 23.25s/it]Running Inference:  72%|███████▏  | 213/294 [1:22:19<31:41, 23.48s/it]Running Inference:  73%|███████▎  | 214/294 [1:22:42<31:02, 23.28s/it]Running Inference:  73%|███████▎  | 215/294 [1:23:06<30:53, 23.46s/it]Running Inference:  73%|███████▎  | 216/294 [1:23:29<30:25, 23.40s/it]Running Inference:  74%|███████▍  | 217/294 [1:23:53<30:03, 23.42s/it]Running Inference:  74%|███████▍  | 218/294 [1:24:15<29:25, 23.24s/it]Running Inference:  74%|███████▍  | 219/294 [1:24:38<28:51, 23.09s/it]Running Inference:  75%|███████▍  | 220/294 [1:25:01<28:23, 23.02s/it]Running Inference:  75%|███████▌  | 221/294 [1:25:24<27:56, 22.96s/it]Running Inference:  76%|███████▌  | 222/294 [1:25:47<27:43, 23.10s/it]Running Inference:  76%|███████▌  | 223/294 [1:26:11<27:36, 23.33s/it]Running Inference:  76%|███████▌  | 224/294 [1:26:34<27:01, 23.16s/it]Running Inference:  77%|███████▋  | 225/294 [1:26:57<26:38, 23.16s/it]Running Inference:  77%|███████▋  | 226/294 [1:27:20<26:19, 23.23s/it]Running Inference:  77%|███████▋  | 227/294 [1:27:43<25:49, 23.12s/it]Running Inference:  78%|███████▊  | 228/294 [1:28:07<25:33, 23.24s/it]Running Inference:  78%|███████▊  | 229/294 [1:28:30<25:18, 23.36s/it]Running Inference:  78%|███████▊  | 230/294 [1:28:53<24:44, 23.20s/it]Running Inference:  79%|███████▊  | 231/294 [1:29:17<24:25, 23.27s/it]Running Inference:  79%|███████▉  | 232/294 [1:29:40<23:56, 23.18s/it]Running Inference:  79%|███████▉  | 233/294 [1:30:04<23:48, 23.42s/it]Running Inference:  80%|███████▉  | 234/294 [1:30:27<23:25, 23.42s/it]Running Inference:  80%|███████▉  | 235/294 [1:30:51<23:07, 23.51s/it]Running Inference:  80%|████████  | 236/294 [1:31:14<22:37, 23.40s/it]Running Inference:  81%|████████  | 237/294 [1:31:37<22:09, 23.33s/it]Running Inference:  81%|████████  | 238/294 [1:32:01<21:58, 23.54s/it]Running Inference:  81%|████████▏ | 239/294 [1:32:24<21:31, 23.49s/it]Running Inference:  82%|████████▏ | 240/294 [1:32:49<21:22, 23.75s/it]Running Inference:  82%|████████▏ | 241/294 [1:33:12<20:54, 23.67s/it]Running Inference:  82%|████████▏ | 242/294 [1:33:37<20:41, 23.88s/it]Running Inference:  83%|████████▎ | 243/294 [1:34:00<20:08, 23.71s/it]Running Inference:  83%|████████▎ | 244/294 [1:34:23<19:31, 23.44s/it]Running Inference:  83%|████████▎ | 245/294 [1:34:24<13:35, 16.64s/it]Running Inference:  84%|████████▎ | 246/294 [1:34:48<15:14, 19.06s/it]Running Inference:  84%|████████▍ | 247/294 [1:35:12<16:01, 20.45s/it]Running Inference:  84%|████████▍ | 248/294 [1:35:35<16:18, 21.28s/it]Running Inference:  85%|████████▍ | 249/294 [1:36:00<16:43, 22.30s/it]Running Inference:  85%|████████▌ | 250/294 [1:36:24<16:44, 22.83s/it]Running Inference:  85%|████████▌ | 251/294 [1:36:48<16:34, 23.13s/it]Running Inference:  86%|████████▌ | 252/294 [1:37:11<16:06, 23.02s/it]Running Inference:  86%|████████▌ | 253/294 [1:37:34<15:46, 23.08s/it]Running Inference:  86%|████████▋ | 254/294 [1:37:57<15:25, 23.13s/it]Running Inference:  87%|████████▋ | 255/294 [1:38:21<15:11, 23.37s/it]Running Inference:  87%|████████▋ | 256/294 [1:38:45<14:59, 23.68s/it]Running Inference:  87%|████████▋ | 257/294 [1:39:08<14:27, 23.43s/it]Running Inference:  88%|████████▊ | 258/294 [1:39:31<14:00, 23.36s/it]Running Inference:  88%|████████▊ | 259/294 [1:39:54<13:30, 23.16s/it]Running Inference:  88%|████████▊ | 260/294 [1:40:17<13:04, 23.06s/it]Running Inference:  89%|████████▉ | 261/294 [1:40:40<12:43, 23.14s/it]Running Inference:  89%|████████▉ | 262/294 [1:41:03<12:16, 23.01s/it]Running Inference:  89%|████████▉ | 263/294 [1:41:26<11:51, 22.94s/it]Running Inference:  90%|████████▉ | 264/294 [1:41:49<11:27, 22.91s/it]Running Inference:  90%|█████████ | 265/294 [1:42:12<11:07, 23.03s/it]Running Inference:  90%|█████████ | 266/294 [1:42:35<10:47, 23.13s/it]Running Inference:  91%|█████████ | 267/294 [1:42:59<10:31, 23.39s/it]Running Inference:  91%|█████████ | 268/294 [1:43:29<11:00, 25.39s/it]Running Inference:  91%|█████████▏| 269/294 [1:43:52<10:18, 24.74s/it]Running Inference:  92%|█████████▏| 270/294 [1:44:16<09:48, 24.50s/it]Running Inference:  92%|█████████▏| 271/294 [1:44:39<09:11, 23.96s/it]Running Inference:  93%|█████████▎| 272/294 [1:45:03<08:47, 23.98s/it]Running Inference:  93%|█████████▎| 273/294 [1:45:26<08:16, 23.65s/it]Running Inference:  93%|█████████▎| 274/294 [1:45:50<07:55, 23.76s/it]Running Inference:  94%|█████████▎| 275/294 [1:46:13<07:28, 23.60s/it]Running Inference:  94%|█████████▍| 276/294 [1:46:36<07:00, 23.35s/it]Running Inference:  94%|█████████▍| 277/294 [1:47:00<06:37, 23.39s/it]Running Inference:  95%|█████████▍| 278/294 [1:47:23<06:15, 23.49s/it]Running Inference:  95%|█████████▍| 279/294 [1:47:48<05:55, 23.72s/it]Running Inference:  95%|█████████▌| 280/294 [1:48:13<05:40, 24.35s/it]Running Inference:  96%|█████████▌| 281/294 [1:48:37<05:15, 24.24s/it]Running Inference:  96%|█████████▌| 282/294 [1:49:01<04:50, 24.18s/it]Running Inference:  96%|█████████▋| 283/294 [1:49:30<04:41, 25.59s/it]Running Inference:  97%|█████████▋| 284/294 [1:49:54<04:11, 25.16s/it]Running Inference:  97%|█████████▋| 285/294 [1:50:18<03:42, 24.69s/it]Running Inference:  97%|█████████▋| 286/294 [1:50:43<03:17, 24.65s/it]Running Inference:  98%|█████████▊| 287/294 [1:51:06<02:49, 24.27s/it]Running Inference:  98%|█████████▊| 288/294 [1:51:30<02:24, 24.07s/it]Running Inference:  98%|█████████▊| 289/294 [1:51:53<01:59, 23.85s/it]Running Inference:  99%|█████████▊| 290/294 [1:52:10<01:27, 21.96s/it]Running Inference:  99%|█████████▉| 291/294 [1:52:33<01:06, 22.21s/it]Running Inference:  99%|█████████▉| 292/294 [1:52:56<00:44, 22.47s/it]Running Inference: 100%|█████████▉| 293/294 [1:53:19<00:22, 22.66s/it]Running Inference: 100%|██████████| 294/294 [1:53:42<00:00, 22.70s/it]Running Inference: 100%|██████████| 294/294 [1:53:42<00:00, 23.21s/it]
2025-12-15 19:05:21,324 - INFO - Inference completed.
2025-12-15 19:05:21,355 - INFO - Results saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.50/predictions.csv
2025-12-15 19:05:21,355 - INFO - Calculating metrics for dataset: longbench
2025-12-15 19:05:34,153 - INFO - Metrics saved to longbenchresult/longbench__multi_news_e__Qwen--Qwen3-8B__tova__0.50/metrics.json
2025-12-15 19:05:34,153 - INFO - Metrics:
10.93
2025-12-15 19:05:34,154 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multi_news_e, ratio=0.5) on GPU 1


========================================
LongBench Task: multifieldqa_en_e
========================================
----------------------------------------
Task: multifieldqa_en_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 19:05:40,916 - INFO - Set deterministic seeds to 42
2025-12-15 19:05:40,916 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 19:05:40,916 - INFO - Starting evaluation run...
2025-12-15 19:05:40,916 - INFO - Output directory set to: longbenchresult
2025-12-15 19:05:40,916 - INFO - Set TOVAPress compression_ratio to 0.1
2025-12-15 19:05:40,916 - INFO - KV Press 'tova' setup.
2025-12-15 19:05:40,916 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 19:05:40,916 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.97it/s]
Device set to use cuda:0
2025-12-15 19:05:56,352 - INFO - Model pipeline loaded.
2025-12-15 19:05:56,353 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en_e)
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 150 examples [00:00, 4526.16 examples/s]
2025-12-15 19:06:31,876 - INFO - Dataset loaded with 150 entries.
2025-12-15 19:06:31,876 - INFO - Dataset processed with 150 entries.
2025-12-15 19:06:31,888 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:03<05:42,  3.08s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:04,  1.68s/it]Running Inference:   3%|▎         | 3/112 [00:05<02:50,  1.56s/it]Running Inference:   4%|▎         | 4/112 [00:07<03:38,  2.02s/it]Running Inference:   4%|▍         | 5/112 [00:11<04:54,  2.75s/it]Running Inference:   5%|▌         | 6/112 [00:13<04:08,  2.34s/it]Running Inference:   6%|▋         | 7/112 [00:17<05:03,  2.89s/it]Running Inference:   7%|▋         | 8/112 [00:19<04:23,  2.53s/it]Running Inference:   8%|▊         | 9/112 [00:20<03:32,  2.06s/it]Running Inference:   9%|▉         | 10/112 [00:25<05:03,  2.98s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:27<04:23,  2.61s/it]Running Inference:  11%|█         | 12/112 [00:29<04:00,  2.41s/it]Running Inference:  12%|█▏        | 13/112 [00:30<03:28,  2.10s/it]Running Inference:  12%|█▎        | 14/112 [00:32<03:15,  2.00s/it]Running Inference:  13%|█▎        | 15/112 [00:32<02:37,  1.62s/it]Running Inference:  14%|█▍        | 16/112 [00:33<02:11,  1.37s/it]Running Inference:  15%|█▌        | 17/112 [00:34<01:44,  1.09s/it]Running Inference:  16%|█▌        | 18/112 [00:35<01:53,  1.21s/it]Running Inference:  17%|█▋        | 19/112 [00:40<03:28,  2.24s/it]Running Inference:  18%|█▊        | 20/112 [00:42<03:18,  2.15s/it]Running Inference:  19%|█▉        | 21/112 [00:44<03:11,  2.11s/it]Running Inference:  20%|█▉        | 22/112 [00:46<03:06,  2.07s/it]Running Inference:  21%|██        | 23/112 [00:47<02:49,  1.91s/it]Running Inference:  21%|██▏       | 24/112 [00:49<02:34,  1.76s/it]Running Inference:  22%|██▏       | 25/112 [00:50<02:13,  1.53s/it]Running Inference:  23%|██▎       | 26/112 [00:54<03:29,  2.44s/it]Running Inference:  24%|██▍       | 27/112 [00:57<03:22,  2.39s/it]Running Inference:  25%|██▌       | 28/112 [00:58<02:55,  2.09s/it]Running Inference:  26%|██▌       | 29/112 [00:59<02:33,  1.85s/it]Running Inference:  27%|██▋       | 30/112 [01:02<02:58,  2.18s/it]Running Inference:  28%|██▊       | 31/112 [01:03<02:29,  1.85s/it]Running Inference:  29%|██▊       | 32/112 [01:05<02:35,  1.94s/it]Running Inference:  29%|██▉       | 33/112 [01:08<02:42,  2.06s/it]Running Inference:  30%|███       | 34/112 [01:10<02:42,  2.09s/it]Running Inference:  31%|███▏      | 35/112 [01:14<03:16,  2.55s/it]Running Inference:  32%|███▏      | 36/112 [01:14<02:29,  1.96s/it]Running Inference:  33%|███▎      | 37/112 [01:15<01:53,  1.51s/it]Running Inference:  34%|███▍      | 38/112 [01:16<01:39,  1.35s/it]Running Inference:  35%|███▍      | 39/112 [01:20<02:50,  2.33s/it]Running Inference:  36%|███▌      | 40/112 [01:21<02:24,  2.01s/it]Running Inference:  37%|███▋      | 41/112 [01:23<02:04,  1.75s/it]Running Inference:  38%|███▊      | 42/112 [01:26<02:38,  2.26s/it]Running Inference:  38%|███▊      | 43/112 [01:29<03:00,  2.62s/it]Running Inference:  39%|███▉      | 44/112 [01:33<03:16,  2.90s/it]Running Inference:  40%|████      | 45/112 [01:37<03:41,  3.30s/it]Running Inference:  41%|████      | 46/112 [01:38<02:56,  2.68s/it]Running Inference:  42%|████▏     | 47/112 [01:40<02:23,  2.21s/it]Running Inference:  43%|████▎     | 48/112 [01:41<02:15,  2.11s/it]Running Inference:  44%|████▍     | 49/112 [01:45<02:39,  2.53s/it]Running Inference:  45%|████▍     | 50/112 [01:46<02:17,  2.21s/it]Running Inference:  46%|████▌     | 51/112 [01:47<01:52,  1.85s/it]Running Inference:  46%|████▋     | 52/112 [01:49<01:52,  1.87s/it]Running Inference:  47%|████▋     | 53/112 [01:52<01:57,  1.99s/it]Running Inference:  48%|████▊     | 54/112 [01:54<01:52,  1.94s/it]Running Inference:  49%|████▉     | 55/112 [01:58<02:40,  2.81s/it]Running Inference:  50%|█████     | 56/112 [01:59<02:02,  2.19s/it]Running Inference:  51%|█████     | 57/112 [02:01<01:54,  2.07s/it]Running Inference:  52%|█████▏    | 58/112 [02:04<02:12,  2.45s/it]Running Inference:  53%|█████▎    | 59/112 [02:09<02:40,  3.02s/it]Running Inference:  54%|█████▎    | 60/112 [02:09<02:02,  2.36s/it]Running Inference:  54%|█████▍    | 61/112 [02:12<02:04,  2.45s/it]Running Inference:  55%|█████▌    | 62/112 [02:14<01:52,  2.25s/it]Running Inference:  56%|█████▋    | 63/112 [02:15<01:34,  1.93s/it]Running Inference:  57%|█████▋    | 64/112 [02:18<01:48,  2.25s/it]Running Inference:  58%|█████▊    | 65/112 [02:20<01:39,  2.13s/it]Running Inference:  59%|█████▉    | 66/112 [02:25<02:15,  2.94s/it]Running Inference:  60%|█████▉    | 67/112 [02:27<02:04,  2.78s/it]Running Inference:  61%|██████    | 68/112 [02:28<01:43,  2.34s/it]Running Inference:  62%|██████▏   | 69/112 [02:29<01:24,  1.96s/it]Running Inference:  62%|██████▎   | 70/112 [02:30<01:07,  1.60s/it]Running Inference:  63%|██████▎   | 71/112 [02:31<00:56,  1.37s/it]Running Inference:  64%|██████▍   | 72/112 [02:32<00:51,  1.28s/it]Running Inference:  65%|██████▌   | 73/112 [02:34<00:59,  1.51s/it]Running Inference:  66%|██████▌   | 74/112 [02:38<01:20,  2.12s/it]Running Inference:  67%|██████▋   | 75/112 [02:43<01:57,  3.17s/it]Running Inference:  68%|██████▊   | 76/112 [02:44<01:25,  2.36s/it]Running Inference:  69%|██████▉   | 77/112 [02:45<01:09,  1.99s/it]Running Inference:  70%|██████▉   | 78/112 [02:49<01:28,  2.59s/it]Running Inference:  71%|███████   | 79/112 [02:50<01:12,  2.21s/it]Running Inference:  71%|███████▏  | 80/112 [02:52<01:04,  2.02s/it]Running Inference:  72%|███████▏  | 81/112 [02:53<00:57,  1.86s/it]Running Inference:  73%|███████▎  | 82/112 [02:56<01:07,  2.24s/it]Running Inference:  74%|███████▍  | 83/112 [03:04<01:49,  3.78s/it]Running Inference:  75%|███████▌  | 84/112 [03:09<01:56,  4.17s/it]Running Inference:  76%|███████▌  | 85/112 [03:09<01:21,  3.02s/it]Running Inference:  77%|███████▋  | 86/112 [03:10<01:04,  2.47s/it]Running Inference:  78%|███████▊  | 87/112 [03:12<00:52,  2.08s/it]Running Inference:  79%|███████▊  | 88/112 [03:14<00:55,  2.30s/it]Running Inference:  79%|███████▉  | 89/112 [03:21<01:19,  3.44s/it]Running Inference:  80%|████████  | 90/112 [03:22<01:05,  2.99s/it]Running Inference:  81%|████████▏ | 91/112 [03:27<01:12,  3.46s/it]Running Inference:  82%|████████▏ | 92/112 [03:29<01:01,  3.09s/it]Running Inference:  83%|████████▎ | 93/112 [03:33<01:03,  3.36s/it]Running Inference:  84%|████████▍ | 94/112 [03:34<00:48,  2.71s/it]Running Inference:  85%|████████▍ | 95/112 [03:36<00:42,  2.48s/it]Running Inference:  86%|████████▌ | 96/112 [03:37<00:32,  2.03s/it]Running Inference:  87%|████████▋ | 97/112 [03:40<00:33,  2.23s/it]Running Inference:  88%|████████▊ | 98/112 [03:45<00:41,  2.99s/it]Running Inference:  88%|████████▊ | 99/112 [03:47<00:34,  2.68s/it]Running Inference:  89%|████████▉ | 100/112 [03:54<00:48,  4.04s/it]Running Inference:  90%|█████████ | 101/112 [03:56<00:36,  3.35s/it]Running Inference:  91%|█████████ | 102/112 [04:00<00:37,  3.71s/it]Running Inference:  92%|█████████▏| 103/112 [04:02<00:27,  3.01s/it]Running Inference:  93%|█████████▎| 104/112 [04:03<00:19,  2.43s/it]Running Inference:  94%|█████████▍| 105/112 [04:04<00:14,  2.00s/it]Running Inference:  95%|█████████▍| 106/112 [04:07<00:14,  2.42s/it]Running Inference:  96%|█████████▌| 107/112 [04:10<00:12,  2.50s/it]Running Inference:  96%|█████████▋| 108/112 [04:18<00:16,  4.07s/it]Running Inference:  97%|█████████▋| 109/112 [04:19<00:10,  3.43s/it]Running Inference:  98%|█████████▊| 110/112 [04:21<00:05,  2.85s/it]Running Inference:  99%|█████████▉| 111/112 [04:25<00:03,  3.13s/it]Running Inference: 100%|██████████| 112/112 [04:29<00:00,  3.43s/it]Running Inference: 100%|██████████| 112/112 [04:29<00:00,  2.41s/it]
2025-12-15 19:11:01,263 - INFO - Inference completed.
2025-12-15 19:11:01,271 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en_e__Qwen--Qwen3-8B__tova__0.10/predictions.csv
2025-12-15 19:11:01,271 - INFO - Calculating metrics for dataset: longbench
2025-12-15 19:11:01,276 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en_e__Qwen--Qwen3-8B__tova__0.10/metrics.json
2025-12-15 19:11:01,276 - INFO - Metrics:
33.31
2025-12-15 19:11:01,277 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_en_e, ratio=0.1) on GPU 1

----------------------------------------
Task: multifieldqa_en_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 19:11:07,587 - INFO - Set deterministic seeds to 42
2025-12-15 19:11:07,587 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 19:11:07,587 - INFO - Starting evaluation run...
2025-12-15 19:11:07,588 - INFO - Output directory set to: longbenchresult
2025-12-15 19:11:07,588 - INFO - Set TOVAPress compression_ratio to 0.2
2025-12-15 19:11:07,588 - INFO - KV Press 'tova' setup.
2025-12-15 19:11:07,588 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 19:11:07,588 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.62it/s]
Device set to use cuda:0
2025-12-15 19:11:18,538 - INFO - Model pipeline loaded.
2025-12-15 19:11:18,539 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en_e)
2025-12-15 19:11:27,544 - INFO - Dataset loaded with 150 entries.
2025-12-15 19:11:27,544 - INFO - Dataset processed with 150 entries.
2025-12-15 19:11:27,556 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:03<05:44,  3.10s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:06,  1.69s/it]Running Inference:   3%|▎         | 3/112 [00:05<02:51,  1.57s/it]Running Inference:   4%|▎         | 4/112 [00:07<03:36,  2.01s/it]Running Inference:   4%|▍         | 5/112 [00:11<04:53,  2.74s/it]Running Inference:   5%|▌         | 6/112 [00:13<04:07,  2.34s/it]Running Inference:   6%|▋         | 7/112 [00:16<04:40,  2.67s/it]Running Inference:   7%|▋         | 8/112 [00:18<04:07,  2.38s/it]Running Inference:   8%|▊         | 9/112 [00:19<03:21,  1.96s/it]Running Inference:   9%|▉         | 10/112 [00:23<04:34,  2.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:25<04:03,  2.41s/it]Running Inference:  11%|█         | 12/112 [00:27<03:49,  2.29s/it]Running Inference:  12%|█▏        | 13/112 [00:29<03:18,  2.01s/it]Running Inference:  12%|█▎        | 14/112 [00:30<03:09,  1.93s/it]Running Inference:  13%|█▎        | 15/112 [00:31<02:26,  1.51s/it]Running Inference:  14%|█▍        | 16/112 [00:32<02:04,  1.29s/it]Running Inference:  15%|█▌        | 17/112 [00:32<01:39,  1.05s/it]Running Inference:  16%|█▌        | 18/112 [00:34<01:50,  1.18s/it]Running Inference:  17%|█▋        | 19/112 [00:38<03:25,  2.21s/it]Running Inference:  18%|█▊        | 20/112 [00:40<03:16,  2.14s/it]Running Inference:  19%|█▉        | 21/112 [00:42<03:14,  2.13s/it]Running Inference:  20%|█▉        | 22/112 [00:44<03:08,  2.09s/it]Running Inference:  21%|██        | 23/112 [00:46<02:50,  1.92s/it]Running Inference:  21%|██▏       | 24/112 [00:47<02:34,  1.76s/it]Running Inference:  22%|██▏       | 25/112 [00:48<02:13,  1.54s/it]Running Inference:  23%|██▎       | 26/112 [00:53<03:29,  2.44s/it]Running Inference:  24%|██▍       | 27/112 [00:55<03:26,  2.42s/it]Running Inference:  25%|██▌       | 28/112 [00:57<02:57,  2.11s/it]Running Inference:  26%|██▌       | 29/112 [00:58<02:34,  1.86s/it]Running Inference:  27%|██▋       | 30/112 [01:01<02:59,  2.19s/it]Running Inference:  28%|██▊       | 31/112 [01:02<02:30,  1.86s/it]Running Inference:  29%|██▊       | 32/112 [01:04<02:31,  1.90s/it]Running Inference:  29%|██▉       | 33/112 [01:06<02:39,  2.02s/it]Running Inference:  30%|███       | 34/112 [01:09<02:44,  2.11s/it]Running Inference:  31%|███▏      | 35/112 [01:12<03:10,  2.47s/it]Running Inference:  32%|███▏      | 36/112 [01:12<02:24,  1.90s/it]Running Inference:  33%|███▎      | 37/112 [01:13<01:51,  1.48s/it]Running Inference:  34%|███▍      | 38/112 [01:14<01:38,  1.33s/it]Running Inference:  35%|███▍      | 39/112 [01:18<02:48,  2.30s/it]Running Inference:  36%|███▌      | 40/112 [01:20<02:22,  1.98s/it]Running Inference:  37%|███▋      | 41/112 [01:21<02:03,  1.74s/it]Running Inference:  38%|███▊      | 42/112 [01:27<03:38,  3.12s/it]Running Inference:  38%|███▊      | 43/112 [01:29<03:12,  2.79s/it]Running Inference:  39%|███▉      | 44/112 [01:33<03:25,  3.01s/it]Running Inference:  40%|████      | 45/112 [01:37<03:47,  3.39s/it]Running Inference:  41%|████      | 46/112 [01:38<03:00,  2.74s/it]Running Inference:  42%|████▏     | 47/112 [01:39<02:25,  2.24s/it]Running Inference:  43%|████▎     | 48/112 [01:41<02:16,  2.13s/it]Running Inference:  44%|████▍     | 49/112 [01:45<02:40,  2.55s/it]Running Inference:  45%|████▍     | 50/112 [01:46<02:17,  2.22s/it]Running Inference:  46%|████▌     | 51/112 [01:47<01:53,  1.85s/it]Running Inference:  46%|████▋     | 52/112 [01:49<01:55,  1.92s/it]Running Inference:  47%|████▋     | 53/112 [01:52<01:59,  2.03s/it]Running Inference:  48%|████▊     | 54/112 [01:53<01:53,  1.96s/it]Running Inference:  49%|████▉     | 55/112 [01:58<02:40,  2.82s/it]Running Inference:  50%|█████     | 56/112 [01:59<02:03,  2.20s/it]Running Inference:  51%|█████     | 57/112 [02:01<01:56,  2.12s/it]Running Inference:  52%|█████▏    | 58/112 [02:04<02:08,  2.38s/it]Running Inference:  53%|█████▎    | 59/112 [02:08<02:37,  2.98s/it]Running Inference:  54%|█████▎    | 60/112 [02:09<02:00,  2.33s/it]Running Inference:  54%|█████▍    | 61/112 [02:11<02:00,  2.36s/it]Running Inference:  55%|█████▌    | 62/112 [02:13<01:43,  2.07s/it]Running Inference:  56%|█████▋    | 63/112 [02:14<01:28,  1.81s/it]Running Inference:  57%|█████▋    | 64/112 [02:18<01:53,  2.36s/it]Running Inference:  58%|█████▊    | 65/112 [02:19<01:38,  2.09s/it]Running Inference:  59%|█████▉    | 66/112 [02:22<01:53,  2.46s/it]Running Inference:  60%|█████▉    | 67/112 [02:25<01:48,  2.41s/it]Running Inference:  61%|██████    | 68/112 [02:27<01:37,  2.21s/it]Running Inference:  62%|██████▏   | 69/112 [02:28<01:20,  1.87s/it]Running Inference:  62%|██████▎   | 70/112 [02:28<01:04,  1.54s/it]Running Inference:  63%|██████▎   | 71/112 [02:29<00:54,  1.33s/it]Running Inference:  64%|██████▍   | 72/112 [02:30<00:49,  1.24s/it]Running Inference:  65%|██████▌   | 73/112 [02:32<00:58,  1.50s/it]Running Inference:  66%|██████▌   | 74/112 [02:36<01:22,  2.18s/it]Running Inference:  67%|██████▋   | 75/112 [02:42<01:58,  3.21s/it]Running Inference:  68%|██████▊   | 76/112 [02:42<01:26,  2.39s/it]Running Inference:  69%|██████▉   | 77/112 [02:43<01:10,  2.01s/it]Running Inference:  70%|██████▉   | 78/112 [02:49<01:43,  3.06s/it]Running Inference:  71%|███████   | 79/112 [02:50<01:23,  2.53s/it]Running Inference:  71%|███████▏  | 80/112 [02:52<01:11,  2.25s/it]Running Inference:  72%|███████▏  | 81/112 [02:53<01:02,  2.02s/it]Running Inference:  73%|███████▎  | 82/112 [02:56<01:10,  2.35s/it]Running Inference:  74%|███████▍  | 83/112 [03:06<02:12,  4.55s/it]Running Inference:  75%|███████▌  | 84/112 [03:11<02:11,  4.71s/it]Running Inference:  76%|███████▌  | 85/112 [03:11<01:31,  3.41s/it]Running Inference:  77%|███████▋  | 86/112 [03:12<01:08,  2.63s/it]Running Inference:  78%|███████▊  | 87/112 [03:13<00:54,  2.20s/it]Running Inference:  79%|███████▊  | 88/112 [03:15<00:49,  2.07s/it]Running Inference:  79%|███████▉  | 89/112 [03:21<01:11,  3.10s/it]Running Inference:  80%|████████  | 90/112 [03:23<01:00,  2.74s/it]Running Inference:  81%|████████▏ | 91/112 [03:27<01:09,  3.29s/it]Running Inference:  82%|████████▏ | 92/112 [03:29<00:59,  2.97s/it]Running Inference:  83%|████████▎ | 93/112 [03:33<01:02,  3.27s/it]Running Inference:  84%|████████▍ | 94/112 [03:35<00:47,  2.65s/it]Running Inference:  85%|████████▍ | 95/112 [03:36<00:40,  2.37s/it]Running Inference:  86%|████████▌ | 96/112 [03:37<00:31,  1.95s/it]Running Inference:  87%|████████▋ | 97/112 [03:43<00:44,  2.99s/it]Running Inference:  88%|████████▊ | 98/112 [03:48<00:49,  3.54s/it]Running Inference:  88%|████████▊ | 99/112 [03:50<00:39,  3.06s/it]Running Inference:  89%|████████▉ | 100/112 [03:57<00:51,  4.27s/it]Running Inference:  90%|█████████ | 101/112 [03:58<00:38,  3.51s/it]Running Inference:  91%|█████████ | 102/112 [04:03<00:38,  3.82s/it]Running Inference:  92%|█████████▏| 103/112 [04:04<00:27,  3.08s/it]Running Inference:  93%|█████████▎| 104/112 [04:05<00:19,  2.46s/it]Running Inference:  94%|█████████▍| 105/112 [04:06<00:14,  2.03s/it]Running Inference:  95%|█████████▍| 106/112 [04:10<00:14,  2.46s/it]Running Inference:  96%|█████████▌| 107/112 [04:12<00:12,  2.53s/it]Running Inference:  96%|█████████▋| 108/112 [04:20<00:16,  4.09s/it]Running Inference:  97%|█████████▋| 109/112 [04:22<00:10,  3.44s/it]Running Inference:  98%|█████████▊| 110/112 [04:24<00:05,  2.87s/it]Running Inference:  99%|█████████▉| 111/112 [04:26<00:02,  2.76s/it]Running Inference: 100%|██████████| 112/112 [04:30<00:00,  3.16s/it]Running Inference: 100%|██████████| 112/112 [04:30<00:00,  2.42s/it]
2025-12-15 19:15:58,284 - INFO - Inference completed.
2025-12-15 19:15:58,292 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en_e__Qwen--Qwen3-8B__tova__0.20/predictions.csv
2025-12-15 19:15:58,292 - INFO - Calculating metrics for dataset: longbench
2025-12-15 19:15:58,296 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en_e__Qwen--Qwen3-8B__tova__0.20/metrics.json
2025-12-15 19:15:58,296 - INFO - Metrics:
32.91
2025-12-15 19:15:58,298 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_en_e, ratio=0.2) on GPU 1

----------------------------------------
Task: multifieldqa_en_e | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 19:16:04,554 - INFO - Set deterministic seeds to 42
2025-12-15 19:16:04,554 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 19:16:04,554 - INFO - Starting evaluation run...
2025-12-15 19:16:04,554 - INFO - Output directory set to: longbenchresult
2025-12-15 19:16:04,554 - INFO - Set TOVAPress compression_ratio to 0.3
2025-12-15 19:16:04,554 - INFO - KV Press 'tova' setup.
2025-12-15 19:16:04,554 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 19:16:04,554 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.39it/s]
Device set to use cuda:0
2025-12-15 19:16:16,583 - INFO - Model pipeline loaded.
2025-12-15 19:16:16,583 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en_e)
2025-12-15 19:16:20,963 - INFO - Dataset loaded with 150 entries.
2025-12-15 19:16:20,964 - INFO - Dataset processed with 150 entries.
2025-12-15 19:16:20,975 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:03<05:51,  3.17s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:06,  1.70s/it]Running Inference:   3%|▎         | 3/112 [00:05<02:46,  1.53s/it]Running Inference:   4%|▎         | 4/112 [00:07<03:37,  2.01s/it]Running Inference:   4%|▍         | 5/112 [00:11<04:52,  2.74s/it]Running Inference:   5%|▌         | 6/112 [00:13<04:07,  2.33s/it]Running Inference:   6%|▋         | 7/112 [00:16<04:29,  2.56s/it]Running Inference:   7%|▋         | 8/112 [00:18<04:00,  2.31s/it]Running Inference:   8%|▊         | 9/112 [00:19<03:16,  1.91s/it]Running Inference:   9%|▉         | 10/112 [00:24<04:49,  2.84s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:26<04:13,  2.51s/it]Running Inference:  11%|█         | 12/112 [00:27<03:49,  2.30s/it]Running Inference:  12%|█▏        | 13/112 [00:29<03:19,  2.01s/it]Running Inference:  12%|█▎        | 14/112 [00:30<03:09,  1.93s/it]Running Inference:  13%|█▎        | 15/112 [00:31<02:26,  1.51s/it]Running Inference:  14%|█▍        | 16/112 [00:32<02:04,  1.29s/it]Running Inference:  15%|█▌        | 17/112 [00:32<01:38,  1.04s/it]Running Inference:  16%|█▌        | 18/112 [00:37<03:16,  2.09s/it]Running Inference:  17%|█▋        | 19/112 [00:41<04:24,  2.85s/it]Running Inference:  18%|█▊        | 20/112 [00:43<03:57,  2.58s/it]Running Inference:  19%|█▉        | 21/112 [00:47<04:16,  2.82s/it]Running Inference:  20%|█▉        | 22/112 [00:49<03:52,  2.59s/it]Running Inference:  21%|██        | 23/112 [00:50<03:21,  2.27s/it]Running Inference:  21%|██▏       | 24/112 [00:52<02:55,  1.99s/it]Running Inference:  22%|██▏       | 25/112 [00:53<02:27,  1.70s/it]Running Inference:  23%|██▎       | 26/112 [00:55<02:46,  1.93s/it]Running Inference:  24%|██▍       | 27/112 [00:57<02:56,  2.08s/it]Running Inference:  25%|██▌       | 28/112 [00:59<02:37,  1.87s/it]Running Inference:  26%|██▌       | 29/112 [01:00<02:20,  1.70s/it]Running Inference:  27%|██▋       | 30/112 [01:03<02:44,  2.01s/it]Running Inference:  28%|██▊       | 31/112 [01:04<02:21,  1.75s/it]Running Inference:  29%|██▊       | 32/112 [01:07<02:49,  2.12s/it]Running Inference:  29%|██▉       | 33/112 [01:09<02:52,  2.18s/it]Running Inference:  30%|███       | 34/112 [01:11<02:39,  2.05s/it]Running Inference:  31%|███▏      | 35/112 [01:15<03:11,  2.49s/it]Running Inference:  32%|███▏      | 36/112 [01:15<02:25,  1.92s/it]Running Inference:  33%|███▎      | 37/112 [01:16<01:51,  1.49s/it]Running Inference:  34%|███▍      | 38/112 [01:17<01:38,  1.33s/it]Running Inference:  35%|███▍      | 39/112 [01:19<02:03,  1.70s/it]Running Inference:  36%|███▌      | 40/112 [01:20<01:52,  1.56s/it]Running Inference:  37%|███▋      | 41/112 [01:22<01:42,  1.44s/it]Running Inference:  38%|███▊      | 42/112 [01:25<02:19,  1.99s/it]Running Inference:  38%|███▊      | 43/112 [01:27<02:18,  2.01s/it]Running Inference:  39%|███▉      | 44/112 [01:30<02:43,  2.41s/it]Running Inference:  40%|████      | 45/112 [01:34<03:18,  2.96s/it]Running Inference:  41%|████      | 46/112 [01:36<02:41,  2.45s/it]Running Inference:  42%|████▏     | 47/112 [01:37<02:22,  2.20s/it]Running Inference:  43%|████▎     | 48/112 [01:39<02:14,  2.10s/it]Running Inference:  44%|████▍     | 49/112 [01:43<02:39,  2.53s/it]Running Inference:  45%|████▍     | 50/112 [01:44<02:18,  2.24s/it]Running Inference:  46%|████▌     | 51/112 [01:45<01:54,  1.87s/it]Running Inference:  46%|████▋     | 52/112 [01:47<01:56,  1.93s/it]Running Inference:  47%|████▋     | 53/112 [01:50<01:58,  2.00s/it]Running Inference:  48%|████▊     | 54/112 [01:51<01:52,  1.94s/it]Running Inference:  49%|████▉     | 55/112 [01:56<02:40,  2.81s/it]Running Inference:  50%|█████     | 56/112 [01:57<02:02,  2.19s/it]Running Inference:  51%|█████     | 57/112 [02:00<02:14,  2.45s/it]Running Inference:  52%|█████▏    | 58/112 [02:03<02:26,  2.72s/it]Running Inference:  53%|█████▎    | 59/112 [02:06<02:15,  2.55s/it]Running Inference:  54%|█████▎    | 60/112 [02:06<01:45,  2.03s/it]Running Inference:  54%|█████▍    | 61/112 [02:09<01:45,  2.07s/it]Running Inference:  55%|█████▌    | 62/112 [02:10<01:33,  1.86s/it]Running Inference:  56%|█████▋    | 63/112 [02:11<01:21,  1.66s/it]Running Inference:  57%|█████▋    | 64/112 [02:15<01:48,  2.25s/it]Running Inference:  58%|█████▊    | 65/112 [02:16<01:33,  1.98s/it]Running Inference:  59%|█████▉    | 66/112 [02:19<01:50,  2.41s/it]Running Inference:  60%|█████▉    | 67/112 [02:22<01:44,  2.33s/it]Running Inference:  61%|██████    | 68/112 [02:23<01:25,  1.94s/it]Running Inference:  62%|██████▏   | 69/112 [02:24<01:12,  1.68s/it]Running Inference:  62%|██████▎   | 70/112 [02:24<00:58,  1.40s/it]Running Inference:  63%|██████▎   | 71/112 [02:25<00:50,  1.23s/it]Running Inference:  64%|██████▍   | 72/112 [02:33<02:05,  3.14s/it]Running Inference:  65%|██████▌   | 73/112 [02:35<01:50,  2.83s/it]Running Inference:  66%|██████▌   | 74/112 [02:39<01:56,  3.08s/it]Running Inference:  67%|██████▋   | 75/112 [02:44<02:19,  3.77s/it]Running Inference:  68%|██████▊   | 76/112 [02:45<01:40,  2.78s/it]Running Inference:  69%|██████▉   | 77/112 [02:46<01:19,  2.28s/it]Running Inference:  70%|██████▉   | 78/112 [02:51<01:50,  3.24s/it]Running Inference:  71%|███████   | 79/112 [02:52<01:27,  2.66s/it]Running Inference:  71%|███████▏  | 80/112 [02:54<01:14,  2.34s/it]Running Inference:  72%|███████▏  | 81/112 [02:55<01:02,  2.01s/it]Running Inference:  73%|███████▎  | 82/112 [02:58<01:10,  2.35s/it]Running Inference:  74%|███████▍  | 83/112 [03:05<01:45,  3.65s/it]Running Inference:  75%|███████▌  | 84/112 [03:10<01:54,  4.07s/it]Running Inference:  76%|███████▌  | 85/112 [03:11<01:20,  2.97s/it]Running Inference:  77%|███████▋  | 86/112 [03:15<01:25,  3.29s/it]Running Inference:  78%|███████▊  | 87/112 [03:16<01:06,  2.66s/it]Running Inference:  79%|███████▊  | 88/112 [03:18<00:57,  2.39s/it]Running Inference:  79%|███████▉  | 89/112 [03:23<01:18,  3.41s/it]Running Inference:  80%|████████  | 90/112 [03:25<01:05,  2.96s/it]Running Inference:  81%|████████▏ | 91/112 [03:30<01:12,  3.44s/it]Running Inference:  82%|████████▏ | 92/112 [03:32<01:01,  3.07s/it]Running Inference:  83%|████████▎ | 93/112 [03:36<01:03,  3.34s/it]Running Inference:  84%|████████▍ | 94/112 [03:37<00:48,  2.70s/it]Running Inference:  85%|████████▍ | 95/112 [03:39<00:40,  2.36s/it]Running Inference:  86%|████████▌ | 96/112 [03:40<00:30,  1.94s/it]Running Inference:  87%|████████▋ | 97/112 [03:45<00:44,  2.98s/it]Running Inference:  88%|████████▊ | 98/112 [03:50<00:49,  3.52s/it]Running Inference:  88%|████████▊ | 99/112 [03:52<00:39,  3.05s/it]Running Inference:  89%|████████▉ | 100/112 [03:59<00:51,  4.25s/it]Running Inference:  90%|█████████ | 101/112 [04:00<00:38,  3.46s/it]Running Inference:  91%|█████████ | 102/112 [04:05<00:38,  3.80s/it]Running Inference:  92%|█████████▏| 103/112 [04:06<00:27,  3.07s/it]Running Inference:  93%|█████████▎| 104/112 [04:07<00:19,  2.45s/it]Running Inference:  94%|█████████▍| 105/112 [04:08<00:14,  2.02s/it]Running Inference:  95%|█████████▍| 106/112 [04:12<00:14,  2.48s/it]Running Inference:  96%|█████████▌| 107/112 [04:15<00:12,  2.54s/it]Running Inference:  96%|█████████▋| 108/112 [04:22<00:16,  4.10s/it]Running Inference:  97%|█████████▋| 109/112 [04:24<00:10,  3.36s/it]Running Inference:  98%|█████████▊| 110/112 [04:25<00:05,  2.73s/it]Running Inference:  99%|█████████▉| 111/112 [04:29<00:03,  3.04s/it]Running Inference: 100%|██████████| 112/112 [04:33<00:00,  3.36s/it]Running Inference: 100%|██████████| 112/112 [04:33<00:00,  2.44s/it]
2025-12-15 19:20:54,683 - INFO - Inference completed.
2025-12-15 19:20:54,690 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en_e__Qwen--Qwen3-8B__tova__0.30/predictions.csv
2025-12-15 19:20:54,690 - INFO - Calculating metrics for dataset: longbench
2025-12-15 19:20:54,695 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en_e__Qwen--Qwen3-8B__tova__0.30/metrics.json
2025-12-15 19:20:54,695 - INFO - Metrics:
32.38
2025-12-15 19:20:54,696 - INFO - Evaluation run completed successfully.
✓ Completed: tova (task=multifieldqa_en_e, ratio=0.3) on GPU 1

----------------------------------------
Task: multifieldqa_en_e | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 19:21:00,914 - INFO - Set deterministic seeds to 42
2025-12-15 19:21:00,914 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "tova",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 19:21:00,915 - INFO - Starting evaluation run...
2025-12-15 19:21:00,915 - INFO - Output directory set to: longbenchresult
2025-12-15 19:21:00,915 - INFO - Set TOVAPress compression_ratio to 0.5
2025-12-15 19:21:00,915 - INFO - KV Press 'tova' setup.
2025-12-15 19:21:00,915 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 19:21:00,915 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.82it/s]
Device set to use cuda:0
2025-12-15 19:21:14,366 - INFO - Model pipeline loaded.
2025-12-15 19:21:14,366 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en_e)
Using the latest cached version of the dataset since Xnhyacinth/LongBench couldn't be found on the Hugging Face Hub
Traceback (most recent call last):
  File "/data/kvpress-main1/evaluation/evaluate.py", line 685, in <module>
    Fire(CliEntryPoint)
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 568, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/evaluation/evaluate.py", line 681, in __call__
    runner.run_evaluation()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 631, in run_evaluation
    self._load_and_prepare_dataset()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 411, in _load_and_prepare_dataset
    df = load_dataset(DATASET_REGISTRY[dataset_name], data_dir=data_dir, split="test").to_pandas()
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2606, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2314, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 140, in __init__
    config_name, version, hash = _find_hash_in_cache(
                                 ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 65, in _find_hash_in_cache
    raise ValueError(
ValueError: Couldn't find cache for Xnhyacinth/LongBench for config 'default-data_dir=multifieldqa_en_e'
Available configs in the cache: ['2wikimqa', '2wikimqa_e', 'default-0065b3a180de55c0', 'default-098a7eb91a184203', 'default-0b7076831c003298', 'default-1d60c6d61b7edecf', 'default-24f61eabf2a87b99', 'default-24fadcfe8fa8f3a5', 'default-4b2a5b07ba27b9a4', 'default-4e16e465b7b19fe5', 'default-515182422f5c01c8', 'default-531e246ba9fed12a', 'default-5984d2c88b13ebba', 'default-5e4ea58ffb882d49', 'default-65a1875e7b162d76', 'default-7a8b9ba40e031066', 'default-8d4d4887aac16609', 'default-988b39b640b83efb', 'default-9aad90b968e8d825', 'default-9d6e0826941c237c', 'default-a17dcc93f20cb815', 'default-a3f4b3bd9beabe7a', 'default-a9c51f35339386df', 'default-afe1d98c024b9964', 'default-b1151d9592e7a1a3', 'default-dce78539c6759811', 'default-ddd7dcf150b1db88', 'default-f744668059827d3f', 'default-f9b1acc752fbb504', 'dureader', 'gov_report', 'gov_report_e', 'hotpotqa', 'hotpotqa_e', 'lcc', 'lcc_e', 'lsht', 'multi_news', 'multi_news_e', 'multifieldqa_en', 'multifieldqa_en_e', 'multifieldqa_zh', 'musique', 'narrativeqa', 'passage_count', 'passage_count_e', 'passage_retrieval_en', 'passage_retrieval_en_e', 'passage_retrieval_zh', 'qasper', 'qasper_e', 'qmsum', 'repobench-p', 'repobench-p_e', 'samsum', 'samsum_e', 'trec', 'trec_e', 'triviaqa', 'triviaqa_e', 'vcsum']
