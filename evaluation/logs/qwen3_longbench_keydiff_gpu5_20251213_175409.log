==========================================
Method: keydiff
GPU: 5
Start Time: Sat Dec 13 05:54:15 PM CST 2025
==========================================

========================================
LongBench Task: 2wikimqa
========================================
----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 17:54:20,661 - INFO - Set deterministic seeds to 42
2025-12-13 17:54:20,662 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "keydiff",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 17:54:20,662 - INFO - Starting evaluation run...
2025-12-13 17:54:20,662 - INFO - Output directory set to: longbenchresult
2025-12-13 17:54:20,662 - INFO - Set KeyDiffPress compression_ratio to 0.1
2025-12-13 17:54:20,662 - INFO - KV Press 'keydiff' setup.
2025-12-13 17:54:20,662 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 17:54:20,662 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.83it/s]
Device set to use cuda:0
2025-12-13 17:54:32,153 - INFO - Model pipeline loaded.
2025-12-13 17:54:32,154 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 17:54:36,321 - INFO - Dataset loaded with 200 entries.
2025-12-13 17:54:36,322 - INFO - Dataset processed with 200 entries.
2025-12-13 17:54:36,342 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:13,  4.29s/it]Running Inference:   1%|          | 2/200 [00:06<09:25,  2.85s/it]Running Inference:   2%|▏         | 3/200 [00:07<06:49,  2.08s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:58,  2.44s/it]Running Inference:   2%|▎         | 5/200 [00:14<09:57,  3.07s/it]Running Inference:   3%|▎         | 6/200 [00:15<07:53,  2.44s/it]Running Inference:   4%|▎         | 7/200 [00:17<06:50,  2.13s/it]Running Inference:   4%|▍         | 8/200 [00:20<07:52,  2.46s/it]Running Inference:   4%|▍         | 9/200 [00:21<06:35,  2.07s/it]Running Inference:   5%|▌         | 10/200 [00:22<05:04,  1.60s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<04:54,  1.56s/it]Running Inference:   6%|▌         | 12/200 [00:25<05:18,  1.70s/it]Running Inference:   6%|▋         | 13/200 [00:27<05:55,  1.90s/it]Running Inference:   7%|▋         | 14/200 [00:28<04:40,  1.51s/it]Running Inference:   8%|▊         | 15/200 [00:29<04:06,  1.33s/it]Running Inference:   8%|▊         | 16/200 [00:30<03:42,  1.21s/it]Running Inference:   8%|▊         | 17/200 [00:32<04:35,  1.50s/it]Running Inference:   9%|▉         | 18/200 [00:33<04:05,  1.35s/it]Running Inference:  10%|▉         | 19/200 [00:34<03:28,  1.15s/it]Running Inference:  10%|█         | 20/200 [00:34<02:40,  1.12it/s]Running Inference:  10%|█         | 21/200 [00:36<03:45,  1.26s/it]Running Inference:  11%|█         | 22/200 [00:39<05:08,  1.73s/it]Running Inference:  12%|█▏        | 23/200 [00:41<05:09,  1.75s/it]Running Inference:  12%|█▏        | 24/200 [00:44<06:05,  2.07s/it]Running Inference:  12%|█▎        | 25/200 [00:45<05:10,  1.78s/it]Running Inference:  13%|█▎        | 26/200 [00:48<06:07,  2.11s/it]Running Inference:  14%|█▎        | 27/200 [00:48<04:48,  1.67s/it]Running Inference:  14%|█▍        | 28/200 [00:49<03:55,  1.37s/it]Running Inference:  14%|█▍        | 29/200 [00:52<05:04,  1.78s/it]Running Inference:  15%|█▌        | 30/200 [00:52<03:45,  1.33s/it]Running Inference:  16%|█▌        | 31/200 [00:56<05:56,  2.11s/it]Running Inference:  16%|█▌        | 32/200 [00:57<04:57,  1.77s/it]Running Inference:  16%|█▋        | 33/200 [00:58<04:14,  1.53s/it]Running Inference:  17%|█▋        | 34/200 [01:01<05:18,  1.92s/it]Running Inference:  18%|█▊        | 35/200 [01:01<04:21,  1.58s/it]Running Inference:  18%|█▊        | 36/200 [01:02<03:45,  1.38s/it]Running Inference:  18%|█▊        | 37/200 [01:04<03:37,  1.34s/it]Running Inference:  19%|█▉        | 38/200 [01:05<03:45,  1.40s/it]Running Inference:  20%|█▉        | 39/200 [01:07<03:51,  1.44s/it]Running Inference:  20%|██        | 40/200 [01:10<05:00,  1.88s/it]Running Inference:  20%|██        | 41/200 [01:12<05:47,  2.18s/it]Running Inference:  21%|██        | 42/200 [01:13<04:47,  1.82s/it]Running Inference:  22%|██▏       | 43/200 [01:14<04:01,  1.54s/it]Running Inference:  22%|██▏       | 44/200 [01:15<03:17,  1.26s/it]Running Inference:  22%|██▎       | 45/200 [01:16<02:54,  1.13s/it]Running Inference:  23%|██▎       | 46/200 [01:19<04:19,  1.68s/it]Running Inference:  24%|██▎       | 47/200 [01:22<05:22,  2.11s/it]Running Inference:  24%|██▍       | 48/200 [01:25<05:56,  2.34s/it]Running Inference:  24%|██▍       | 49/200 [01:26<05:15,  2.09s/it]Running Inference:  25%|██▌       | 50/200 [01:28<04:58,  1.99s/it]Running Inference:  26%|██▌       | 51/200 [01:30<05:03,  2.04s/it]Running Inference:  26%|██▌       | 52/200 [01:33<05:18,  2.15s/it]Running Inference:  26%|██▋       | 53/200 [01:35<05:40,  2.31s/it]Running Inference:  27%|██▋       | 54/200 [01:37<04:59,  2.05s/it]Running Inference:  28%|██▊       | 55/200 [01:38<04:05,  1.69s/it]Running Inference:  28%|██▊       | 56/200 [01:41<05:18,  2.21s/it]Running Inference:  28%|██▊       | 57/200 [01:42<04:19,  1.82s/it]Running Inference:  29%|██▉       | 58/200 [01:45<05:12,  2.20s/it]Running Inference:  30%|██▉       | 59/200 [01:48<05:33,  2.36s/it]Running Inference:  30%|███       | 60/200 [01:49<04:37,  1.98s/it]Running Inference:  30%|███       | 61/200 [01:49<03:40,  1.58s/it]Running Inference:  31%|███       | 62/200 [01:50<03:14,  1.41s/it]Running Inference:  32%|███▏      | 63/200 [01:51<02:25,  1.06s/it]Running Inference:  32%|███▏      | 64/200 [01:55<04:22,  1.93s/it]Running Inference:  32%|███▎      | 65/200 [01:55<03:21,  1.49s/it]Running Inference:  33%|███▎      | 66/200 [01:56<03:08,  1.40s/it]Running Inference:  34%|███▎      | 67/200 [01:59<04:16,  1.93s/it]Running Inference:  34%|███▍      | 68/200 [02:03<05:10,  2.35s/it]Running Inference:  34%|███▍      | 69/200 [02:06<05:24,  2.47s/it]Running Inference:  35%|███▌      | 70/200 [02:07<04:31,  2.09s/it]Running Inference:  36%|███▌      | 71/200 [02:07<03:31,  1.64s/it]Running Inference:  36%|███▌      | 72/200 [02:11<04:29,  2.11s/it]Running Inference:  36%|███▋      | 73/200 [02:13<04:59,  2.36s/it]Running Inference:  37%|███▋      | 74/200 [02:16<04:47,  2.28s/it]Running Inference:  38%|███▊      | 75/200 [02:17<04:01,  1.93s/it]Running Inference:  38%|███▊      | 76/200 [02:18<03:31,  1.71s/it]Running Inference:  38%|███▊      | 77/200 [02:19<03:06,  1.52s/it]Running Inference:  39%|███▉      | 78/200 [02:22<04:10,  2.05s/it]Running Inference:  40%|███▉      | 79/200 [02:25<04:43,  2.34s/it]Running Inference:  40%|████      | 80/200 [02:29<05:22,  2.68s/it]Running Inference:  40%|████      | 81/200 [02:34<06:47,  3.42s/it]Running Inference:  41%|████      | 82/200 [02:37<06:30,  3.31s/it]Running Inference:  42%|████▏     | 83/200 [02:41<06:38,  3.41s/it]Running Inference:  42%|████▏     | 84/200 [02:42<05:13,  2.70s/it]Running Inference:  42%|████▎     | 85/200 [02:43<04:14,  2.22s/it]Running Inference:  43%|████▎     | 86/200 [02:47<05:07,  2.70s/it]Running Inference:  44%|████▎     | 87/200 [02:47<03:54,  2.07s/it]Running Inference:  44%|████▍     | 88/200 [02:50<04:21,  2.33s/it]Running Inference:  44%|████▍     | 89/200 [02:53<04:35,  2.49s/it]Running Inference:  45%|████▌     | 90/200 [02:56<04:51,  2.65s/it]Running Inference:  46%|████▌     | 91/200 [02:59<04:52,  2.68s/it]Running Inference:  46%|████▌     | 92/200 [03:02<04:59,  2.77s/it]Running Inference:  46%|████▋     | 93/200 [03:03<04:00,  2.25s/it]Running Inference:  47%|████▋     | 94/200 [03:03<03:00,  1.70s/it]Running Inference:  48%|████▊     | 95/200 [03:04<02:27,  1.41s/it]Running Inference:  48%|████▊     | 96/200 [03:05<02:13,  1.29s/it]Running Inference:  48%|████▊     | 97/200 [03:07<02:32,  1.48s/it]Running Inference:  49%|████▉     | 98/200 [03:08<02:16,  1.34s/it]Running Inference:  50%|████▉     | 99/200 [03:11<02:59,  1.77s/it]Running Inference:  50%|█████     | 100/200 [03:11<02:25,  1.46s/it]Running Inference:  50%|█████     | 101/200 [03:14<03:02,  1.84s/it]Running Inference:  51%|█████     | 102/200 [03:15<02:29,  1.53s/it]Running Inference:  52%|█████▏    | 103/200 [03:18<03:03,  1.89s/it]Running Inference:  52%|█████▏    | 104/200 [03:18<02:29,  1.56s/it]Running Inference:  52%|█████▎    | 105/200 [03:20<02:19,  1.47s/it]Running Inference:  53%|█████▎    | 106/200 [03:21<02:03,  1.31s/it]Running Inference:  54%|█████▎    | 107/200 [03:24<03:03,  1.98s/it]Running Inference:  54%|█████▍    | 108/200 [03:27<03:26,  2.25s/it]Running Inference:  55%|█████▍    | 109/200 [03:28<02:47,  1.84s/it]Running Inference:  55%|█████▌    | 110/200 [03:29<02:38,  1.76s/it]Running Inference:  56%|█████▌    | 111/200 [03:30<02:07,  1.44s/it]Running Inference:  56%|█████▌    | 112/200 [03:33<02:45,  1.89s/it]Running Inference:  56%|█████▋    | 113/200 [03:34<02:21,  1.63s/it]Running Inference:  57%|█████▋    | 114/200 [03:37<02:51,  1.99s/it]Running Inference:  57%|█████▊    | 115/200 [03:39<02:47,  1.97s/it]Running Inference:  58%|█████▊    | 116/200 [03:43<03:35,  2.57s/it]Running Inference:  58%|█████▊    | 117/200 [03:45<03:34,  2.58s/it]Running Inference:  59%|█████▉    | 118/200 [03:46<02:44,  2.01s/it]Running Inference:  60%|█████▉    | 119/200 [03:49<03:10,  2.36s/it]Running Inference:  60%|██████    | 120/200 [03:50<02:32,  1.90s/it]Running Inference:  60%|██████    | 121/200 [03:51<02:04,  1.57s/it]Running Inference:  61%|██████    | 122/200 [03:53<02:25,  1.87s/it]Running Inference:  62%|██████▏   | 123/200 [03:54<02:00,  1.57s/it]Running Inference:  62%|██████▏   | 124/200 [03:57<02:31,  2.00s/it]Running Inference:  62%|██████▎   | 125/200 [03:58<02:01,  1.61s/it]Running Inference:  63%|██████▎   | 126/200 [04:01<02:23,  1.94s/it]Running Inference:  64%|██████▎   | 127/200 [04:01<01:47,  1.47s/it]Running Inference:  64%|██████▍   | 128/200 [04:02<01:33,  1.29s/it]Running Inference:  64%|██████▍   | 129/200 [04:04<01:36,  1.35s/it]Running Inference:  65%|██████▌   | 130/200 [04:06<02:05,  1.80s/it]Running Inference:  66%|██████▌   | 131/200 [04:09<02:27,  2.13s/it]Running Inference:  66%|██████▌   | 132/200 [04:14<03:09,  2.78s/it]Running Inference:  66%|██████▋   | 133/200 [04:15<02:40,  2.40s/it]Running Inference:  67%|██████▋   | 134/200 [04:18<02:51,  2.60s/it]Running Inference:  68%|██████▊   | 135/200 [04:21<02:45,  2.55s/it]Running Inference:  68%|██████▊   | 136/200 [04:21<02:08,  2.01s/it]Running Inference:  68%|██████▊   | 137/200 [04:24<02:23,  2.28s/it]Running Inference:  69%|██████▉   | 138/200 [04:25<01:58,  1.91s/it]Running Inference:  70%|██████▉   | 139/200 [04:27<01:57,  1.92s/it]Running Inference:  70%|███████   | 140/200 [04:29<01:49,  1.82s/it]Running Inference:  70%|███████   | 141/200 [04:33<02:21,  2.40s/it]Running Inference:  71%|███████   | 142/200 [04:36<02:28,  2.56s/it]Running Inference:  72%|███████▏  | 143/200 [04:38<02:29,  2.61s/it]Running Inference:  72%|███████▏  | 144/200 [04:40<02:06,  2.26s/it]Running Inference:  72%|███████▎  | 145/200 [04:42<01:57,  2.13s/it]Running Inference:  73%|███████▎  | 146/200 [04:45<02:09,  2.40s/it]Running Inference:  74%|███████▎  | 147/200 [04:47<02:14,  2.53s/it]Running Inference:  74%|███████▍  | 148/200 [04:50<02:15,  2.61s/it]Running Inference:  74%|███████▍  | 149/200 [04:51<01:48,  2.13s/it]Running Inference:  75%|███████▌  | 150/200 [04:53<01:48,  2.17s/it]Running Inference:  76%|███████▌  | 151/200 [04:54<01:18,  1.60s/it]Running Inference:  76%|███████▌  | 152/200 [04:54<01:02,  1.30s/it]Running Inference:  76%|███████▋  | 153/200 [04:55<00:52,  1.11s/it]Running Inference:  77%|███████▋  | 154/200 [04:58<01:14,  1.62s/it]Running Inference:  78%|███████▊  | 155/200 [05:00<01:16,  1.71s/it]Running Inference:  78%|███████▊  | 156/200 [05:01<01:11,  1.63s/it]Running Inference:  78%|███████▊  | 157/200 [05:02<00:54,  1.28s/it]Running Inference:  79%|███████▉  | 158/200 [05:04<01:12,  1.74s/it]Running Inference:  80%|███████▉  | 159/200 [05:07<01:20,  1.95s/it]Running Inference:  80%|████████  | 160/200 [05:11<01:38,  2.47s/it]Running Inference:  80%|████████  | 161/200 [05:12<01:26,  2.22s/it]Running Inference:  81%|████████  | 162/200 [05:15<01:26,  2.28s/it]Running Inference:  82%|████████▏ | 163/200 [05:16<01:11,  1.92s/it]Running Inference:  82%|████████▏ | 164/200 [05:18<01:17,  2.16s/it]Running Inference:  82%|████████▎ | 165/200 [05:19<01:01,  1.75s/it]Running Inference:  83%|████████▎ | 166/200 [05:23<01:22,  2.43s/it]Running Inference:  84%|████████▎ | 167/200 [05:25<01:15,  2.28s/it]Running Inference:  84%|████████▍ | 168/200 [05:26<00:56,  1.76s/it]Running Inference:  84%|████████▍ | 169/200 [05:26<00:41,  1.33s/it]Running Inference:  85%|████████▌ | 170/200 [05:29<00:54,  1.83s/it]Running Inference:  86%|████████▌ | 171/200 [05:31<00:55,  1.93s/it]Running Inference:  86%|████████▌ | 172/200 [05:32<00:46,  1.65s/it]Running Inference:  86%|████████▋ | 173/200 [05:33<00:39,  1.46s/it]Running Inference:  87%|████████▋ | 174/200 [05:36<00:48,  1.87s/it]Running Inference:  88%|████████▊ | 175/200 [05:40<01:02,  2.51s/it]Running Inference:  88%|████████▊ | 176/200 [05:43<01:03,  2.64s/it]Running Inference:  88%|████████▊ | 177/200 [05:46<01:01,  2.65s/it]Running Inference:  89%|████████▉ | 178/200 [05:48<00:55,  2.50s/it]Running Inference:  90%|████████▉ | 179/200 [05:52<01:00,  2.89s/it]Running Inference:  90%|█████████ | 180/200 [05:53<00:48,  2.42s/it]Running Inference:  90%|█████████ | 181/200 [05:54<00:36,  1.90s/it]Running Inference:  91%|█████████ | 182/200 [05:55<00:33,  1.86s/it]Running Inference:  92%|█████████▏| 183/200 [05:57<00:31,  1.85s/it]Running Inference:  92%|█████████▏| 184/200 [05:58<00:24,  1.52s/it]Running Inference:  92%|█████████▎| 185/200 [06:00<00:26,  1.77s/it]Running Inference:  93%|█████████▎| 186/200 [06:01<00:20,  1.45s/it]Running Inference:  94%|█████████▎| 187/200 [06:02<00:17,  1.35s/it]Running Inference:  94%|█████████▍| 188/200 [06:03<00:14,  1.19s/it]Running Inference:  94%|█████████▍| 189/200 [06:06<00:18,  1.70s/it]Running Inference:  95%|█████████▌| 190/200 [06:06<00:12,  1.29s/it]Running Inference:  96%|█████████▌| 191/200 [06:10<00:18,  2.02s/it]Running Inference:  96%|█████████▌| 192/200 [06:11<00:14,  1.76s/it]Running Inference:  96%|█████████▋| 193/200 [06:15<00:16,  2.36s/it]Running Inference:  97%|█████████▋| 194/200 [06:17<00:14,  2.41s/it]Running Inference:  98%|█████████▊| 195/200 [06:20<00:13,  2.63s/it]Running Inference:  98%|█████████▊| 196/200 [06:22<00:09,  2.41s/it]Running Inference:  98%|█████████▊| 197/200 [06:23<00:06,  2.03s/it]Running Inference:  99%|█████████▉| 198/200 [06:24<00:03,  1.61s/it]Running Inference: 100%|█████████▉| 199/200 [06:25<00:01,  1.37s/it]Running Inference: 100%|██████████| 200/200 [06:25<00:00,  1.13s/it]Running Inference: 100%|██████████| 200/200 [06:25<00:00,  1.93s/it]
2025-12-13 18:01:02,336 - INFO - Inference completed.
2025-12-13 18:01:02,345 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.10/predictions.csv
2025-12-13 18:01:02,346 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:01:02,350 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.10/metrics.json
2025-12-13 18:01:02,350 - INFO - Metrics:
24.56
2025-12-13 18:01:02,351 - INFO - Evaluation run completed successfully.
✓ Completed: keydiff (task=2wikimqa, ratio=0.1) on GPU 5

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:01:08,792 - INFO - Set deterministic seeds to 42
2025-12-13 18:01:08,793 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "keydiff",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:01:08,793 - INFO - Starting evaluation run...
2025-12-13 18:01:08,793 - INFO - Output directory set to: longbenchresult
2025-12-13 18:01:08,793 - INFO - Set KeyDiffPress compression_ratio to 0.2
2025-12-13 18:01:08,793 - INFO - KV Press 'keydiff' setup.
2025-12-13 18:01:08,793 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:01:08,793 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.10it/s]
Device set to use cuda:0
2025-12-13 18:01:21,939 - INFO - Model pipeline loaded.
2025-12-13 18:01:21,939 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:01:30,817 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:01:30,817 - INFO - Dataset processed with 200 entries.
2025-12-13 18:01:30,837 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:04,  4.24s/it]Running Inference:   1%|          | 2/200 [00:07<11:22,  3.44s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:47,  3.28s/it]Running Inference:   2%|▏         | 4/200 [00:13<10:20,  3.16s/it]Running Inference:   2%|▎         | 5/200 [00:17<11:29,  3.54s/it]Running Inference:   3%|▎         | 6/200 [00:20<10:52,  3.36s/it]Running Inference:   4%|▎         | 7/200 [00:21<08:25,  2.62s/it]Running Inference:   4%|▍         | 8/200 [00:24<08:56,  2.79s/it]Running Inference:   4%|▍         | 9/200 [00:25<07:18,  2.30s/it]Running Inference:   5%|▌         | 10/200 [00:26<05:34,  1.76s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:27<05:14,  1.66s/it]Running Inference:   6%|▌         | 12/200 [00:29<05:07,  1.63s/it]Running Inference:   6%|▋         | 13/200 [00:32<06:52,  2.21s/it]Running Inference:   7%|▋         | 14/200 [00:33<05:19,  1.72s/it]Running Inference:   8%|▊         | 15/200 [00:34<04:33,  1.48s/it]Running Inference:   8%|▊         | 16/200 [00:35<04:01,  1.31s/it]Running Inference:   8%|▊         | 17/200 [00:37<04:15,  1.40s/it]Running Inference:   9%|▉         | 18/200 [00:37<03:50,  1.27s/it]Running Inference:  10%|▉         | 19/200 [00:38<03:18,  1.09s/it]Running Inference:  10%|█         | 20/200 [00:38<02:33,  1.17it/s]Running Inference:  10%|█         | 21/200 [00:41<04:28,  1.50s/it]Running Inference:  11%|█         | 22/200 [00:44<05:36,  1.89s/it]Running Inference:  12%|█▏        | 23/200 [00:46<05:06,  1.73s/it]Running Inference:  12%|█▏        | 24/200 [00:48<06:01,  2.05s/it]Running Inference:  12%|█▎        | 25/200 [00:49<05:06,  1.75s/it]Running Inference:  13%|█▎        | 26/200 [00:51<04:41,  1.62s/it]Running Inference:  14%|█▎        | 27/200 [00:52<04:19,  1.50s/it]Running Inference:  14%|█▍        | 28/200 [00:53<03:36,  1.26s/it]Running Inference:  14%|█▍        | 29/200 [00:53<03:07,  1.10s/it]Running Inference:  15%|█▌        | 30/200 [00:54<02:24,  1.18it/s]Running Inference:  16%|█▌        | 31/200 [00:56<03:32,  1.26s/it]Running Inference:  16%|█▌        | 32/200 [00:57<03:16,  1.17s/it]Running Inference:  16%|█▋        | 33/200 [00:59<03:51,  1.39s/it]Running Inference:  17%|█▋        | 34/200 [01:02<04:59,  1.81s/it]Running Inference:  18%|█▊        | 35/200 [01:02<04:08,  1.51s/it]Running Inference:  18%|█▊        | 36/200 [01:03<03:36,  1.32s/it]Running Inference:  18%|█▊        | 37/200 [01:05<03:31,  1.30s/it]Running Inference:  19%|█▉        | 38/200 [01:06<03:41,  1.37s/it]Running Inference:  20%|█▉        | 39/200 [01:09<04:52,  1.82s/it]Running Inference:  20%|██        | 40/200 [01:12<05:42,  2.14s/it]Running Inference:  20%|██        | 41/200 [01:15<06:14,  2.36s/it]Running Inference:  21%|██        | 42/200 [01:16<05:06,  1.94s/it]Running Inference:  22%|██▏       | 43/200 [01:16<04:14,  1.62s/it]Running Inference:  22%|██▏       | 44/200 [01:17<03:27,  1.33s/it]Running Inference:  22%|██▎       | 45/200 [01:18<03:00,  1.16s/it]Running Inference:  23%|██▎       | 46/200 [01:20<04:04,  1.59s/it]Running Inference:  24%|██▎       | 47/200 [01:22<04:20,  1.70s/it]Running Inference:  24%|██▍       | 48/200 [01:25<05:12,  2.05s/it]Running Inference:  24%|██▍       | 49/200 [01:27<04:46,  1.90s/it]Running Inference:  25%|██▌       | 50/200 [01:30<05:25,  2.17s/it]Running Inference:  26%|██▌       | 51/200 [01:32<05:21,  2.16s/it]Running Inference:  26%|██▌       | 52/200 [01:32<04:04,  1.65s/it]Running Inference:  26%|██▋       | 53/200 [01:33<03:21,  1.37s/it]Running Inference:  27%|██▋       | 54/200 [01:34<03:23,  1.39s/it]Running Inference:  28%|██▊       | 55/200 [01:35<02:58,  1.23s/it]Running Inference:  28%|██▊       | 56/200 [01:37<02:57,  1.23s/it]Running Inference:  28%|██▊       | 57/200 [01:37<02:38,  1.11s/it]Running Inference:  29%|██▉       | 58/200 [01:39<02:54,  1.23s/it]Running Inference:  30%|██▉       | 59/200 [01:42<03:56,  1.67s/it]Running Inference:  30%|███       | 60/200 [01:43<03:29,  1.50s/it]Running Inference:  30%|███       | 61/200 [01:43<02:51,  1.23s/it]Running Inference:  31%|███       | 62/200 [01:46<04:06,  1.79s/it]Running Inference:  32%|███▏      | 63/200 [01:47<03:01,  1.33s/it]Running Inference:  32%|███▏      | 64/200 [01:49<03:49,  1.69s/it]Running Inference:  32%|███▎      | 65/200 [01:50<02:58,  1.32s/it]Running Inference:  33%|███▎      | 66/200 [01:51<02:51,  1.28s/it]Running Inference:  34%|███▎      | 67/200 [01:54<04:04,  1.84s/it]Running Inference:  34%|███▍      | 68/200 [01:57<05:00,  2.28s/it]Running Inference:  34%|███▍      | 69/200 [02:00<05:15,  2.41s/it]Running Inference:  35%|███▌      | 70/200 [02:01<04:18,  1.99s/it]Running Inference:  36%|███▌      | 71/200 [02:02<03:22,  1.57s/it]Running Inference:  36%|███▌      | 72/200 [02:03<03:07,  1.47s/it]Running Inference:  36%|███▋      | 73/200 [02:06<04:01,  1.90s/it]Running Inference:  37%|███▋      | 74/200 [02:08<04:07,  1.96s/it]Running Inference:  38%|███▊      | 75/200 [02:09<03:39,  1.75s/it]Running Inference:  38%|███▊      | 76/200 [02:10<03:13,  1.56s/it]Running Inference:  38%|███▊      | 77/200 [02:11<02:53,  1.41s/it]Running Inference:  39%|███▉      | 78/200 [02:13<02:58,  1.47s/it]Running Inference:  40%|███▉      | 79/200 [02:16<03:52,  1.92s/it]Running Inference:  40%|████      | 80/200 [02:19<04:45,  2.38s/it]Running Inference:  40%|████      | 81/200 [02:20<04:01,  2.03s/it]Running Inference:  41%|████      | 82/200 [02:23<04:34,  2.32s/it]Running Inference:  42%|████▏     | 83/200 [02:25<04:15,  2.18s/it]Running Inference:  42%|████▏     | 84/200 [02:26<03:33,  1.84s/it]Running Inference:  42%|████▎     | 85/200 [02:29<04:11,  2.19s/it]Running Inference:  43%|████▎     | 86/200 [02:33<05:04,  2.67s/it]Running Inference:  44%|████▎     | 87/200 [02:34<03:56,  2.09s/it]Running Inference:  44%|████▍     | 88/200 [02:37<04:21,  2.34s/it]Running Inference:  44%|████▍     | 89/200 [02:40<04:35,  2.48s/it]Running Inference:  45%|████▌     | 90/200 [02:43<04:50,  2.64s/it]Running Inference:  46%|████▌     | 91/200 [02:45<04:50,  2.66s/it]Running Inference:  46%|████▌     | 92/200 [02:48<04:56,  2.75s/it]Running Inference:  46%|████▋     | 93/200 [02:51<04:38,  2.60s/it]Running Inference:  47%|████▋     | 94/200 [02:51<03:34,  2.03s/it]Running Inference:  48%|████▊     | 95/200 [02:52<02:51,  1.63s/it]Running Inference:  48%|████▊     | 96/200 [02:53<02:28,  1.43s/it]Running Inference:  48%|████▊     | 97/200 [02:55<02:57,  1.72s/it]Running Inference:  49%|████▉     | 98/200 [02:56<02:34,  1.51s/it]Running Inference:  50%|████▉     | 99/200 [02:57<02:13,  1.32s/it]Running Inference:  50%|█████     | 100/200 [02:58<01:53,  1.14s/it]Running Inference:  50%|█████     | 101/200 [03:01<02:39,  1.61s/it]Running Inference:  51%|█████     | 102/200 [03:03<03:11,  1.96s/it]Running Inference:  52%|█████▏    | 103/200 [03:06<03:31,  2.18s/it]Running Inference:  52%|█████▏    | 104/200 [03:09<03:49,  2.39s/it]Running Inference:  52%|█████▎    | 105/200 [03:10<03:05,  1.96s/it]Running Inference:  53%|█████▎    | 106/200 [03:11<02:35,  1.65s/it]Running Inference:  54%|█████▎    | 107/200 [03:13<02:48,  1.81s/it]Running Inference:  54%|█████▍    | 108/200 [03:16<03:15,  2.12s/it]Running Inference:  55%|█████▍    | 109/200 [03:17<02:39,  1.75s/it]Running Inference:  55%|█████▌    | 110/200 [03:18<02:32,  1.70s/it]Running Inference:  56%|█████▌    | 111/200 [03:19<02:03,  1.39s/it]Running Inference:  56%|█████▌    | 112/200 [03:22<02:42,  1.84s/it]Running Inference:  56%|█████▋    | 113/200 [03:23<02:19,  1.60s/it]Running Inference:  57%|█████▋    | 114/200 [03:26<02:48,  1.96s/it]Running Inference:  57%|█████▊    | 115/200 [03:30<03:37,  2.56s/it]Running Inference:  58%|█████▊    | 116/200 [03:32<03:36,  2.58s/it]Running Inference:  58%|█████▊    | 117/200 [03:36<04:09,  3.00s/it]Running Inference:  59%|█████▉    | 118/200 [03:37<03:08,  2.30s/it]Running Inference:  60%|█████▉    | 119/200 [03:40<03:27,  2.56s/it]Running Inference:  60%|██████    | 120/200 [03:41<02:43,  2.04s/it]Running Inference:  60%|██████    | 121/200 [03:42<02:13,  1.69s/it]Running Inference:  61%|██████    | 122/200 [03:42<01:43,  1.33s/it]Running Inference:  62%|██████▏   | 123/200 [03:45<02:15,  1.76s/it]Running Inference:  62%|██████▏   | 124/200 [03:48<02:41,  2.13s/it]Running Inference:  62%|██████▎   | 125/200 [03:49<02:07,  1.71s/it]Running Inference:  63%|██████▎   | 126/200 [03:52<02:27,  2.00s/it]Running Inference:  64%|██████▎   | 127/200 [03:52<01:50,  1.51s/it]Running Inference:  64%|██████▍   | 128/200 [03:53<01:34,  1.32s/it]Running Inference:  64%|██████▍   | 129/200 [03:54<01:37,  1.37s/it]Running Inference:  65%|██████▌   | 130/200 [03:55<01:26,  1.24s/it]Running Inference:  66%|██████▌   | 131/200 [03:58<01:59,  1.74s/it]Running Inference:  66%|██████▌   | 132/200 [04:02<02:49,  2.49s/it]Running Inference:  66%|██████▋   | 133/200 [04:04<02:27,  2.20s/it]Running Inference:  67%|██████▋   | 134/200 [04:07<02:41,  2.44s/it]Running Inference:  68%|██████▊   | 135/200 [04:09<02:37,  2.43s/it]Running Inference:  68%|██████▊   | 136/200 [04:10<02:03,  1.92s/it]Running Inference:  68%|██████▊   | 137/200 [04:13<02:19,  2.21s/it]Running Inference:  69%|██████▉   | 138/200 [04:14<01:55,  1.86s/it]Running Inference:  70%|██████▉   | 139/200 [04:16<01:54,  1.88s/it]Running Inference:  70%|███████   | 140/200 [04:17<01:47,  1.79s/it]Running Inference:  70%|███████   | 141/200 [04:21<02:19,  2.37s/it]Running Inference:  71%|███████   | 142/200 [04:24<02:26,  2.53s/it]Running Inference:  72%|███████▏  | 143/200 [04:25<01:52,  1.98s/it]Running Inference:  72%|███████▏  | 144/200 [04:28<02:06,  2.26s/it]Running Inference:  72%|███████▎  | 145/200 [04:30<02:09,  2.36s/it]Running Inference:  73%|███████▎  | 146/200 [04:33<02:18,  2.56s/it]Running Inference:  74%|███████▎  | 147/200 [04:36<02:19,  2.63s/it]Running Inference:  74%|███████▍  | 148/200 [04:39<02:18,  2.67s/it]Running Inference:  74%|███████▍  | 149/200 [04:40<01:51,  2.18s/it]Running Inference:  75%|███████▌  | 150/200 [04:43<02:02,  2.46s/it]Running Inference:  76%|███████▌  | 151/200 [04:43<01:28,  1.80s/it]Running Inference:  76%|███████▌  | 152/200 [04:46<01:38,  2.05s/it]Running Inference:  76%|███████▋  | 153/200 [04:49<01:45,  2.25s/it]Running Inference:  77%|███████▋  | 154/200 [04:51<01:50,  2.41s/it]Running Inference:  78%|███████▊  | 155/200 [04:54<01:53,  2.53s/it]Running Inference:  78%|███████▊  | 156/200 [04:56<01:36,  2.20s/it]Running Inference:  78%|███████▊  | 157/200 [04:56<01:12,  1.68s/it]Running Inference:  79%|███████▉  | 158/200 [04:59<01:24,  2.02s/it]Running Inference:  80%|███████▉  | 159/200 [05:01<01:27,  2.13s/it]Running Inference:  80%|████████  | 160/200 [05:07<02:04,  3.11s/it]Running Inference:  80%|████████  | 161/200 [05:08<01:41,  2.60s/it]Running Inference:  81%|████████  | 162/200 [05:09<01:21,  2.14s/it]Running Inference:  82%|████████▏ | 163/200 [05:10<01:07,  1.82s/it]Running Inference:  82%|████████▏ | 164/200 [05:13<01:15,  2.10s/it]Running Inference:  82%|████████▎ | 165/200 [05:14<00:59,  1.70s/it]Running Inference:  83%|████████▎ | 166/200 [05:18<01:21,  2.39s/it]Running Inference:  84%|████████▎ | 167/200 [05:20<01:14,  2.26s/it]Running Inference:  84%|████████▍ | 168/200 [05:20<00:55,  1.74s/it]Running Inference:  84%|████████▍ | 169/200 [05:22<00:51,  1.68s/it]Running Inference:  85%|████████▌ | 170/200 [05:25<01:01,  2.06s/it]Running Inference:  86%|████████▌ | 171/200 [05:26<00:49,  1.71s/it]Running Inference:  86%|████████▌ | 172/200 [05:29<00:57,  2.07s/it]Running Inference:  86%|████████▋ | 173/200 [05:30<00:47,  1.76s/it]Running Inference:  87%|████████▋ | 174/200 [05:31<00:39,  1.52s/it]Running Inference:  88%|████████▊ | 175/200 [05:33<00:41,  1.67s/it]Running Inference:  88%|████████▊ | 176/200 [05:35<00:49,  2.05s/it]Running Inference:  88%|████████▊ | 177/200 [05:37<00:41,  1.82s/it]Running Inference:  89%|████████▉ | 178/200 [05:39<00:41,  1.88s/it]Running Inference:  90%|████████▉ | 179/200 [05:43<00:51,  2.46s/it]Running Inference:  90%|█████████ | 180/200 [05:44<00:42,  2.12s/it]Running Inference:  90%|█████████ | 181/200 [05:45<00:32,  1.69s/it]Running Inference:  91%|█████████ | 182/200 [05:46<00:30,  1.71s/it]Running Inference:  92%|█████████▏| 183/200 [05:48<00:29,  1.73s/it]Running Inference:  92%|█████████▏| 184/200 [05:49<00:23,  1.44s/it]Running Inference:  92%|█████████▎| 185/200 [05:51<00:25,  1.72s/it]Running Inference:  93%|█████████▎| 186/200 [05:52<00:19,  1.41s/it]Running Inference:  94%|█████████▎| 187/200 [05:53<00:17,  1.32s/it]Running Inference:  94%|█████████▍| 188/200 [05:56<00:21,  1.76s/it]Running Inference:  94%|█████████▍| 189/200 [05:59<00:23,  2.09s/it]Running Inference:  95%|█████████▌| 190/200 [05:59<00:15,  1.58s/it]Running Inference:  96%|█████████▌| 191/200 [06:03<00:19,  2.21s/it]Running Inference:  96%|█████████▌| 192/200 [06:06<00:19,  2.45s/it]Running Inference:  96%|█████████▋| 193/200 [06:10<00:19,  2.83s/it]Running Inference:  97%|█████████▋| 194/200 [06:10<00:13,  2.19s/it]Running Inference:  98%|█████████▊| 195/200 [06:13<00:12,  2.46s/it]Running Inference:  98%|█████████▊| 196/200 [06:15<00:09,  2.30s/it]Running Inference:  98%|█████████▊| 197/200 [06:16<00:05,  1.95s/it]Running Inference:  99%|█████████▉| 198/200 [06:17<00:03,  1.54s/it]Running Inference: 100%|█████████▉| 199/200 [06:18<00:01,  1.31s/it]Running Inference: 100%|██████████| 200/200 [06:18<00:00,  1.09s/it]Running Inference: 100%|██████████| 200/200 [06:18<00:00,  1.89s/it]
2025-12-13 18:07:49,630 - INFO - Inference completed.
2025-12-13 18:07:49,638 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.20/predictions.csv
2025-12-13 18:07:49,638 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:07:49,643 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.20/metrics.json
2025-12-13 18:07:49,643 - INFO - Metrics:
21.79
2025-12-13 18:07:49,644 - INFO - Evaluation run completed successfully.
✓ Completed: keydiff (task=2wikimqa, ratio=0.2) on GPU 5

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:07:55,979 - INFO - Set deterministic seeds to 42
2025-12-13 18:07:55,980 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "keydiff",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:07:55,980 - INFO - Starting evaluation run...
2025-12-13 18:07:55,980 - INFO - Output directory set to: longbenchresult
2025-12-13 18:07:55,980 - INFO - Set KeyDiffPress compression_ratio to 0.3
2025-12-13 18:07:55,980 - INFO - KV Press 'keydiff' setup.
2025-12-13 18:07:55,980 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:07:55,980 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.32it/s]
Device set to use cuda:0
2025-12-13 18:08:07,789 - INFO - Model pipeline loaded.
2025-12-13 18:08:07,789 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:08:11,559 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:08:11,559 - INFO - Dataset processed with 200 entries.
2025-12-13 18:08:11,577 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<08:53,  2.68s/it]Running Inference:   1%|          | 2/200 [00:05<09:25,  2.86s/it]Running Inference:   2%|▏         | 3/200 [00:06<06:50,  2.09s/it]Running Inference:   2%|▏         | 4/200 [00:09<08:01,  2.46s/it]Running Inference:   2%|▎         | 5/200 [00:13<09:02,  2.78s/it]Running Inference:   3%|▎         | 6/200 [00:16<09:20,  2.89s/it]Running Inference:   4%|▎         | 7/200 [00:17<07:24,  2.31s/it]Running Inference:   4%|▍         | 8/200 [00:20<08:20,  2.61s/it]Running Inference:   4%|▍         | 9/200 [00:21<06:54,  2.17s/it]Running Inference:   5%|▌         | 10/200 [00:22<05:20,  1.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<05:04,  1.61s/it]Running Inference:   6%|▌         | 12/200 [00:25<04:57,  1.58s/it]Running Inference:   6%|▋         | 13/200 [00:26<04:51,  1.56s/it]Running Inference:   7%|▋         | 14/200 [00:27<03:56,  1.27s/it]Running Inference:   8%|▊         | 15/200 [00:28<03:36,  1.17s/it]Running Inference:   8%|▊         | 16/200 [00:29<03:21,  1.10s/it]Running Inference:   8%|▊         | 17/200 [00:33<05:44,  1.88s/it]Running Inference:   9%|▉         | 18/200 [00:34<04:53,  1.61s/it]Running Inference:  10%|▉         | 19/200 [00:36<05:59,  1.99s/it]Running Inference:  10%|█         | 20/200 [00:37<04:26,  1.48s/it]Running Inference:  10%|█         | 21/200 [00:41<06:54,  2.32s/it]Running Inference:  11%|█         | 22/200 [00:44<07:23,  2.49s/it]Running Inference:  12%|█▏        | 23/200 [00:45<06:23,  2.17s/it]Running Inference:  12%|█▏        | 24/200 [00:46<05:10,  1.76s/it]Running Inference:  12%|█▎        | 25/200 [00:47<04:33,  1.56s/it]Running Inference:  13%|█▎        | 26/200 [00:50<05:45,  1.98s/it]Running Inference:  14%|█▎        | 27/200 [00:53<06:23,  2.22s/it]Running Inference:  14%|█▍        | 28/200 [00:54<05:05,  1.78s/it]Running Inference:  14%|█▍        | 29/200 [00:54<04:10,  1.46s/it]Running Inference:  15%|█▌        | 30/200 [00:55<03:07,  1.11s/it]Running Inference:  16%|█▌        | 31/200 [00:58<04:50,  1.72s/it]Running Inference:  16%|█▌        | 32/200 [00:59<04:11,  1.50s/it]Running Inference:  16%|█▋        | 33/200 [01:01<04:29,  1.62s/it]Running Inference:  17%|█▋        | 34/200 [01:04<05:31,  2.00s/it]Running Inference:  18%|█▊        | 35/200 [01:04<04:30,  1.64s/it]Running Inference:  18%|█▊        | 36/200 [01:05<03:54,  1.43s/it]Running Inference:  18%|█▊        | 37/200 [01:11<06:57,  2.56s/it]Running Inference:  19%|█▉        | 38/200 [01:12<05:58,  2.21s/it]Running Inference:  20%|█▉        | 39/200 [01:13<05:05,  1.90s/it]Running Inference:  20%|██        | 40/200 [01:14<04:20,  1.63s/it]Running Inference:  20%|██        | 41/200 [01:17<05:24,  2.04s/it]Running Inference:  21%|██        | 42/200 [01:19<04:59,  1.89s/it]Running Inference:  22%|██▏       | 43/200 [01:20<04:14,  1.62s/it]Running Inference:  22%|██▏       | 44/200 [01:20<03:28,  1.34s/it]Running Inference:  22%|██▎       | 45/200 [01:21<03:09,  1.22s/it]Running Inference:  23%|██▎       | 46/200 [01:24<04:31,  1.76s/it]Running Inference:  24%|██▎       | 47/200 [01:26<04:42,  1.85s/it]Running Inference:  24%|██▍       | 48/200 [01:29<05:33,  2.20s/it]Running Inference:  24%|██▍       | 49/200 [01:31<04:59,  1.98s/it]Running Inference:  25%|██▌       | 50/200 [01:34<05:38,  2.26s/it]Running Inference:  26%|██▌       | 51/200 [01:36<05:30,  2.22s/it]Running Inference:  26%|██▌       | 52/200 [01:38<05:40,  2.30s/it]Running Inference:  26%|██▋       | 53/200 [01:39<04:29,  1.83s/it]Running Inference:  27%|██▋       | 54/200 [01:42<05:15,  2.16s/it]Running Inference:  28%|██▊       | 55/200 [01:43<04:16,  1.77s/it]Running Inference:  28%|██▊       | 56/200 [01:44<03:52,  1.61s/it]Running Inference:  28%|██▊       | 57/200 [01:45<03:16,  1.38s/it]Running Inference:  29%|██▉       | 58/200 [01:46<03:05,  1.31s/it]Running Inference:  30%|██▉       | 59/200 [01:49<04:07,  1.75s/it]Running Inference:  30%|███       | 60/200 [01:50<03:37,  1.56s/it]Running Inference:  30%|███       | 61/200 [01:51<02:57,  1.27s/it]Running Inference:  31%|███       | 62/200 [01:52<03:14,  1.41s/it]Running Inference:  32%|███▏      | 63/200 [01:53<02:25,  1.06s/it]Running Inference:  32%|███▏      | 64/200 [01:57<04:25,  1.95s/it]Running Inference:  32%|███▎      | 65/200 [01:57<03:23,  1.51s/it]Running Inference:  33%|███▎      | 66/200 [01:58<03:09,  1.42s/it]Running Inference:  34%|███▎      | 67/200 [02:02<04:20,  1.96s/it]Running Inference:  34%|███▍      | 68/200 [02:05<05:16,  2.40s/it]Running Inference:  34%|███▍      | 69/200 [02:06<04:34,  2.10s/it]Running Inference:  35%|███▌      | 70/200 [02:07<03:50,  1.77s/it]Running Inference:  36%|███▌      | 71/200 [02:08<03:03,  1.42s/it]Running Inference:  36%|███▌      | 72/200 [02:09<02:53,  1.35s/it]Running Inference:  36%|███▋      | 73/200 [02:12<03:32,  1.68s/it]Running Inference:  37%|███▋      | 74/200 [02:14<03:47,  1.81s/it]Running Inference:  38%|███▊      | 75/200 [02:15<03:26,  1.65s/it]Running Inference:  38%|███▊      | 76/200 [02:16<03:04,  1.49s/it]Running Inference:  38%|███▊      | 77/200 [02:17<02:48,  1.37s/it]Running Inference:  39%|███▉      | 78/200 [02:21<03:59,  1.97s/it]Running Inference:  40%|███▉      | 79/200 [02:24<04:38,  2.30s/it]Running Inference:  40%|████      | 80/200 [02:27<05:20,  2.67s/it]Running Inference:  40%|████      | 81/200 [02:28<04:25,  2.23s/it]Running Inference:  41%|████      | 82/200 [02:31<04:54,  2.49s/it]Running Inference:  42%|████▏     | 83/200 [02:35<05:34,  2.86s/it]Running Inference:  42%|████▏     | 84/200 [02:36<04:29,  2.32s/it]Running Inference:  42%|████▎     | 85/200 [02:37<03:44,  1.96s/it]Running Inference:  43%|████▎     | 86/200 [02:41<04:48,  2.53s/it]Running Inference:  44%|████▎     | 87/200 [02:42<03:45,  2.00s/it]Running Inference:  44%|████▍     | 88/200 [02:45<04:17,  2.30s/it]Running Inference:  44%|████▍     | 89/200 [02:46<03:44,  2.02s/it]Running Inference:  45%|████▌     | 90/200 [02:50<04:18,  2.35s/it]Running Inference:  46%|████▌     | 91/200 [02:52<04:31,  2.49s/it]Running Inference:  46%|████▌     | 92/200 [02:55<04:46,  2.66s/it]Running Inference:  46%|████▋     | 93/200 [02:58<04:33,  2.56s/it]Running Inference:  47%|████▋     | 94/200 [02:58<03:31,  2.00s/it]Running Inference:  48%|████▊     | 95/200 [02:59<02:49,  1.62s/it]Running Inference:  48%|████▊     | 96/200 [03:00<02:27,  1.42s/it]Running Inference:  48%|████▊     | 97/200 [03:03<03:25,  2.00s/it]Running Inference:  49%|████▉     | 98/200 [03:04<02:53,  1.71s/it]Running Inference:  50%|████▉     | 99/200 [03:06<02:35,  1.54s/it]Running Inference:  50%|█████     | 100/200 [03:07<02:34,  1.54s/it]Running Inference:  50%|█████     | 101/200 [03:10<03:10,  1.92s/it]Running Inference:  51%|█████     | 102/200 [03:13<03:35,  2.20s/it]Running Inference:  52%|█████▏    | 103/200 [03:16<03:51,  2.38s/it]Running Inference:  52%|█████▏    | 104/200 [03:19<04:05,  2.56s/it]Running Inference:  52%|█████▎    | 105/200 [03:20<03:17,  2.08s/it]Running Inference:  53%|█████▎    | 106/200 [03:20<02:43,  1.74s/it]Running Inference:  54%|█████▎    | 107/200 [03:24<03:33,  2.30s/it]Running Inference:  54%|█████▍    | 108/200 [03:27<03:49,  2.49s/it]Running Inference:  55%|█████▍    | 109/200 [03:28<03:03,  2.01s/it]Running Inference:  55%|█████▌    | 110/200 [03:29<02:49,  1.88s/it]Running Inference:  56%|█████▌    | 111/200 [03:30<02:15,  1.52s/it]Running Inference:  56%|█████▌    | 112/200 [03:33<02:52,  1.96s/it]Running Inference:  56%|█████▋    | 113/200 [03:34<02:26,  1.69s/it]Running Inference:  57%|█████▋    | 114/200 [03:37<02:56,  2.05s/it]Running Inference:  57%|█████▊    | 115/200 [03:40<03:12,  2.27s/it]Running Inference:  58%|█████▊    | 116/200 [03:44<03:54,  2.79s/it]Running Inference:  58%|█████▊    | 117/200 [03:48<04:24,  3.18s/it]Running Inference:  59%|█████▉    | 118/200 [03:51<04:09,  3.04s/it]Running Inference:  60%|█████▉    | 119/200 [03:52<03:28,  2.57s/it]Running Inference:  60%|██████    | 120/200 [03:53<02:44,  2.05s/it]Running Inference:  60%|██████    | 121/200 [03:54<02:13,  1.69s/it]Running Inference:  61%|██████    | 122/200 [03:56<02:33,  1.97s/it]Running Inference:  62%|██████▏   | 123/200 [03:57<02:05,  1.63s/it]Running Inference:  62%|██████▏   | 124/200 [04:00<02:36,  2.06s/it]Running Inference:  62%|██████▎   | 125/200 [04:01<02:04,  1.66s/it]Running Inference:  63%|██████▎   | 126/200 [04:02<01:43,  1.40s/it]Running Inference:  64%|██████▎   | 127/200 [04:02<01:19,  1.10s/it]Running Inference:  64%|██████▍   | 128/200 [04:03<01:18,  1.08s/it]Running Inference:  64%|██████▍   | 129/200 [04:05<01:25,  1.21s/it]Running Inference:  65%|██████▌   | 130/200 [04:08<01:59,  1.71s/it]Running Inference:  66%|██████▌   | 131/200 [04:11<02:24,  2.09s/it]Running Inference:  66%|██████▌   | 132/200 [04:15<03:08,  2.77s/it]Running Inference:  66%|██████▋   | 133/200 [04:17<02:45,  2.47s/it]Running Inference:  67%|██████▋   | 134/200 [04:18<02:18,  2.10s/it]Running Inference:  68%|██████▊   | 135/200 [04:20<02:22,  2.19s/it]Running Inference:  68%|██████▊   | 136/200 [04:21<01:51,  1.75s/it]Running Inference:  68%|██████▊   | 137/200 [04:24<02:12,  2.10s/it]Running Inference:  69%|██████▉   | 138/200 [04:25<01:51,  1.79s/it]Running Inference:  70%|██████▉   | 139/200 [04:27<01:52,  1.84s/it]Running Inference:  70%|███████   | 140/200 [04:29<01:46,  1.77s/it]Running Inference:  70%|███████   | 141/200 [04:33<02:21,  2.39s/it]Running Inference:  71%|███████   | 142/200 [04:34<01:55,  1.99s/it]Running Inference:  72%|███████▏  | 143/200 [04:36<02:07,  2.23s/it]Running Inference:  72%|███████▏  | 144/200 [04:39<02:17,  2.45s/it]Running Inference:  72%|███████▎  | 145/200 [04:41<02:04,  2.26s/it]Running Inference:  73%|███████▎  | 146/200 [04:44<02:15,  2.51s/it]Running Inference:  74%|███████▎  | 147/200 [04:47<02:18,  2.61s/it]Running Inference:  74%|███████▍  | 148/200 [04:50<02:19,  2.68s/it]Running Inference:  74%|███████▍  | 149/200 [04:51<01:51,  2.19s/it]Running Inference:  75%|███████▌  | 150/200 [04:54<02:04,  2.48s/it]Running Inference:  76%|███████▌  | 151/200 [04:54<01:29,  1.82s/it]Running Inference:  76%|███████▌  | 152/200 [04:57<01:40,  2.08s/it]Running Inference:  76%|███████▋  | 153/200 [05:00<01:47,  2.28s/it]Running Inference:  77%|███████▋  | 154/200 [05:03<01:52,  2.45s/it]Running Inference:  78%|███████▊  | 155/200 [05:04<01:27,  1.95s/it]Running Inference:  78%|███████▊  | 156/200 [05:06<01:26,  1.97s/it]Running Inference:  78%|███████▊  | 157/200 [05:06<01:05,  1.51s/it]Running Inference:  79%|███████▉  | 158/200 [05:09<01:20,  1.91s/it]Running Inference:  80%|███████▉  | 159/200 [05:11<01:24,  2.07s/it]Running Inference:  80%|████████  | 160/200 [05:15<01:42,  2.56s/it]Running Inference:  80%|████████  | 161/200 [05:18<01:42,  2.63s/it]Running Inference:  81%|████████  | 162/200 [05:19<01:19,  2.09s/it]Running Inference:  82%|████████▏ | 163/200 [05:20<01:06,  1.80s/it]Running Inference:  82%|████████▏ | 164/200 [05:22<01:15,  2.09s/it]Running Inference:  82%|████████▎ | 165/200 [05:23<00:59,  1.70s/it]Running Inference:  83%|████████▎ | 166/200 [05:27<01:21,  2.40s/it]Running Inference:  84%|████████▎ | 167/200 [05:29<01:14,  2.27s/it]Running Inference:  84%|████████▍ | 168/200 [05:30<00:56,  1.75s/it]Running Inference:  84%|████████▍ | 169/200 [05:32<00:58,  1.89s/it]Running Inference:  85%|████████▌ | 170/200 [05:35<01:06,  2.22s/it]Running Inference:  86%|████████▌ | 171/200 [05:36<00:53,  1.83s/it]Running Inference:  86%|████████▌ | 172/200 [05:37<00:44,  1.58s/it]Running Inference:  86%|████████▋ | 173/200 [05:38<00:38,  1.41s/it]Running Inference:  87%|████████▋ | 174/200 [05:39<00:36,  1.41s/it]Running Inference:  88%|████████▊ | 175/200 [05:43<00:54,  2.20s/it]Running Inference:  88%|████████▊ | 176/200 [05:46<00:58,  2.43s/it]Running Inference:  88%|████████▊ | 177/200 [05:49<00:57,  2.52s/it]Running Inference:  89%|████████▉ | 178/200 [05:51<00:53,  2.41s/it]Running Inference:  90%|████████▉ | 179/200 [05:55<00:59,  2.83s/it]Running Inference:  90%|█████████ | 180/200 [05:56<00:47,  2.38s/it]Running Inference:  90%|█████████ | 181/200 [05:57<00:35,  1.87s/it]Running Inference:  91%|█████████ | 182/200 [05:59<00:33,  1.84s/it]Running Inference:  92%|█████████▏| 183/200 [06:02<00:39,  2.30s/it]Running Inference:  92%|█████████▏| 184/200 [06:03<00:29,  1.85s/it]Running Inference:  92%|█████████▎| 185/200 [06:05<00:29,  1.96s/it]Running Inference:  93%|█████████▎| 186/200 [06:08<00:30,  2.19s/it]Running Inference:  94%|█████████▎| 187/200 [06:09<00:24,  1.86s/it]Running Inference:  94%|█████████▍| 188/200 [06:12<00:25,  2.15s/it]Running Inference:  94%|█████████▍| 189/200 [06:15<00:26,  2.38s/it]Running Inference:  95%|█████████▌| 190/200 [06:15<00:18,  1.83s/it]Running Inference:  96%|█████████▌| 191/200 [06:19<00:20,  2.30s/it]Running Inference:  96%|█████████▌| 192/200 [06:22<00:20,  2.53s/it]Running Inference:  96%|█████████▋| 193/200 [06:26<00:20,  2.90s/it]Running Inference:  97%|█████████▋| 194/200 [06:28<00:16,  2.81s/it]Running Inference:  98%|█████████▊| 195/200 [06:31<00:14,  2.93s/it]Running Inference:  98%|█████████▊| 196/200 [06:33<00:09,  2.39s/it]Running Inference:  98%|█████████▊| 197/200 [06:34<00:06,  2.02s/it]Running Inference:  99%|█████████▉| 198/200 [06:34<00:03,  1.58s/it]Running Inference: 100%|█████████▉| 199/200 [06:37<00:01,  1.94s/it]Running Inference: 100%|██████████| 200/200 [06:38<00:00,  1.53s/it]Running Inference: 100%|██████████| 200/200 [06:38<00:00,  1.99s/it]
2025-12-13 18:14:49,686 - INFO - Inference completed.
2025-12-13 18:14:49,694 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.30/predictions.csv
2025-12-13 18:14:49,694 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:14:49,699 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.30/metrics.json
2025-12-13 18:14:49,699 - INFO - Metrics:
21.22
2025-12-13 18:14:49,700 - INFO - Evaluation run completed successfully.
✓ Completed: keydiff (task=2wikimqa, ratio=0.3) on GPU 5

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:14:56,107 - INFO - Set deterministic seeds to 42
2025-12-13 18:14:56,108 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "keydiff",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:14:56,108 - INFO - Starting evaluation run...
2025-12-13 18:14:56,108 - INFO - Output directory set to: longbenchresult
2025-12-13 18:14:56,108 - INFO - Set KeyDiffPress compression_ratio to 0.5
2025-12-13 18:14:56,108 - INFO - KV Press 'keydiff' setup.
2025-12-13 18:14:56,108 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:14:56,108 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.98it/s]
Device set to use cuda:0
2025-12-13 18:15:08,358 - INFO - Model pipeline loaded.
2025-12-13 18:15:08,358 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:15:17,067 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:15:17,067 - INFO - Dataset processed with 200 entries.
2025-12-13 18:15:17,085 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:05,  4.25s/it]Running Inference:   1%|          | 2/200 [00:07<11:25,  3.46s/it]Running Inference:   2%|▏         | 3/200 [00:08<07:54,  2.41s/it]Running Inference:   2%|▏         | 4/200 [00:11<08:35,  2.63s/it]Running Inference:   2%|▎         | 5/200 [00:15<10:19,  3.18s/it]Running Inference:   3%|▎         | 6/200 [00:18<10:04,  3.12s/it]Running Inference:   4%|▎         | 7/200 [00:19<07:53,  2.45s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:34,  2.68s/it]Running Inference:   4%|▍         | 9/200 [00:25<09:07,  2.87s/it]Running Inference:   5%|▌         | 10/200 [00:26<06:50,  2.16s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:30<08:06,  2.58s/it]Running Inference:   6%|▌         | 12/200 [00:33<08:56,  2.85s/it]Running Inference:   6%|▋         | 13/200 [00:35<07:34,  2.43s/it]Running Inference:   7%|▋         | 14/200 [00:35<05:50,  1.89s/it]Running Inference:   8%|▊         | 15/200 [00:36<04:57,  1.61s/it]Running Inference:   8%|▊         | 16/200 [00:37<04:17,  1.40s/it]Running Inference:   8%|▊         | 17/200 [00:41<06:18,  2.07s/it]Running Inference:   9%|▉         | 18/200 [00:42<05:16,  1.74s/it]Running Inference:  10%|▉         | 19/200 [00:44<06:10,  2.05s/it]Running Inference:  10%|█         | 20/200 [00:45<04:33,  1.52s/it]Running Inference:  10%|█         | 21/200 [00:48<06:11,  2.08s/it]Running Inference:  11%|█         | 22/200 [00:51<06:48,  2.30s/it]Running Inference:  12%|█▏        | 23/200 [00:52<05:56,  2.01s/it]Running Inference:  12%|█▏        | 24/200 [00:55<06:29,  2.21s/it]Running Inference:  12%|█▎        | 25/200 [00:56<05:27,  1.87s/it]Running Inference:  13%|█▎        | 26/200 [00:59<06:18,  2.17s/it]Running Inference:  14%|█▎        | 27/200 [01:00<05:20,  1.85s/it]Running Inference:  14%|█▍        | 28/200 [01:01<04:19,  1.51s/it]Running Inference:  14%|█▍        | 29/200 [01:03<05:19,  1.87s/it]Running Inference:  15%|█▌        | 30/200 [01:04<03:55,  1.39s/it]Running Inference:  16%|█▌        | 31/200 [01:06<04:23,  1.56s/it]Running Inference:  16%|█▌        | 32/200 [01:07<04:10,  1.49s/it]Running Inference:  16%|█▋        | 33/200 [01:09<04:26,  1.60s/it]Running Inference:  17%|█▋        | 34/200 [01:12<05:24,  1.96s/it]Running Inference:  18%|█▊        | 35/200 [01:12<04:25,  1.61s/it]Running Inference:  18%|█▊        | 36/200 [01:15<05:26,  1.99s/it]Running Inference:  18%|█▊        | 37/200 [01:18<06:16,  2.31s/it]Running Inference:  19%|█▉        | 38/200 [01:20<05:29,  2.03s/it]Running Inference:  20%|█▉        | 39/200 [01:23<06:07,  2.28s/it]Running Inference:  20%|██        | 40/200 [01:23<05:01,  1.88s/it]Running Inference:  20%|██        | 41/200 [01:24<04:11,  1.58s/it]Running Inference:  21%|██        | 42/200 [01:27<05:12,  1.98s/it]Running Inference:  22%|██▏       | 43/200 [01:30<05:52,  2.25s/it]Running Inference:  22%|██▏       | 44/200 [01:31<04:35,  1.77s/it]Running Inference:  22%|██▎       | 45/200 [01:34<05:23,  2.09s/it]Running Inference:  23%|██▎       | 46/200 [01:37<06:01,  2.35s/it]Running Inference:  24%|██▎       | 47/200 [01:40<06:32,  2.56s/it]Running Inference:  24%|██▍       | 48/200 [01:43<06:43,  2.66s/it]Running Inference:  24%|██▍       | 49/200 [01:44<05:46,  2.29s/it]Running Inference:  25%|██▌       | 50/200 [01:46<05:45,  2.31s/it]Running Inference:  26%|██▌       | 51/200 [01:51<07:10,  2.89s/it]Running Inference:  26%|██▌       | 52/200 [01:51<05:21,  2.17s/it]Running Inference:  26%|██▋       | 53/200 [01:52<04:14,  1.73s/it]Running Inference:  27%|██▋       | 54/200 [01:55<05:01,  2.07s/it]Running Inference:  28%|██▊       | 55/200 [01:56<04:09,  1.72s/it]Running Inference:  28%|██▊       | 56/200 [01:57<03:47,  1.58s/it]Running Inference:  28%|██▊       | 57/200 [01:58<03:16,  1.38s/it]Running Inference:  29%|██▉       | 58/200 [01:59<03:10,  1.34s/it]Running Inference:  30%|██▉       | 59/200 [02:00<02:50,  1.21s/it]Running Inference:  30%|███       | 60/200 [02:01<02:39,  1.14s/it]Running Inference:  30%|███       | 61/200 [02:04<03:44,  1.62s/it]Running Inference:  31%|███       | 62/200 [02:07<04:43,  2.06s/it]Running Inference:  32%|███▏      | 63/200 [02:07<03:27,  1.51s/it]Running Inference:  32%|███▏      | 64/200 [02:11<05:05,  2.24s/it]Running Inference:  32%|███▎      | 65/200 [02:11<03:51,  1.71s/it]Running Inference:  33%|███▎      | 66/200 [02:12<03:28,  1.56s/it]Running Inference:  34%|███▎      | 67/200 [02:14<03:16,  1.47s/it]Running Inference:  34%|███▍      | 68/200 [02:15<03:23,  1.54s/it]Running Inference:  34%|███▍      | 69/200 [02:17<03:15,  1.49s/it]Running Inference:  35%|███▌      | 70/200 [02:18<03:01,  1.39s/it]Running Inference:  36%|███▌      | 71/200 [02:19<02:28,  1.15s/it]Running Inference:  36%|███▌      | 72/200 [02:20<02:28,  1.16s/it]Running Inference:  36%|███▋      | 73/200 [02:21<02:47,  1.32s/it]Running Inference:  37%|███▋      | 74/200 [02:24<03:15,  1.55s/it]Running Inference:  38%|███▊      | 75/200 [02:25<02:57,  1.42s/it]Running Inference:  38%|███▊      | 76/200 [02:26<02:43,  1.32s/it]Running Inference:  38%|███▊      | 77/200 [02:27<02:33,  1.25s/it]Running Inference:  39%|███▉      | 78/200 [02:29<03:16,  1.61s/it]Running Inference:  40%|███▉      | 79/200 [02:30<02:54,  1.44s/it]Running Inference:  40%|████      | 80/200 [02:34<04:05,  2.04s/it]Running Inference:  40%|████      | 81/200 [02:37<04:29,  2.26s/it]Running Inference:  41%|████      | 82/200 [02:40<04:53,  2.49s/it]Running Inference:  42%|████▏     | 83/200 [02:41<04:27,  2.28s/it]Running Inference:  42%|████▏     | 84/200 [02:42<03:39,  1.89s/it]Running Inference:  42%|████▎     | 85/200 [02:45<04:15,  2.22s/it]Running Inference:  43%|████▎     | 86/200 [02:49<05:06,  2.69s/it]Running Inference:  44%|████▎     | 87/200 [02:50<03:57,  2.10s/it]Running Inference:  44%|████▍     | 88/200 [02:53<04:22,  2.35s/it]Running Inference:  44%|████▍     | 89/200 [02:56<04:36,  2.49s/it]Running Inference:  45%|████▌     | 90/200 [02:57<03:53,  2.12s/it]Running Inference:  46%|████▌     | 91/200 [03:00<04:11,  2.30s/it]Running Inference:  46%|████▌     | 92/200 [03:03<04:30,  2.50s/it]Running Inference:  46%|████▋     | 93/200 [03:04<04:03,  2.28s/it]Running Inference:  47%|████▋     | 94/200 [03:05<03:09,  1.79s/it]Running Inference:  48%|████▊     | 95/200 [03:06<02:33,  1.47s/it]Running Inference:  48%|████▊     | 96/200 [03:07<02:17,  1.32s/it]Running Inference:  48%|████▊     | 97/200 [03:08<02:14,  1.30s/it]Running Inference:  49%|████▉     | 98/200 [03:09<02:02,  1.20s/it]Running Inference:  50%|████▉     | 99/200 [03:11<02:18,  1.37s/it]Running Inference:  50%|█████     | 100/200 [03:13<02:59,  1.80s/it]Running Inference:  50%|█████     | 101/200 [03:16<03:25,  2.07s/it]Running Inference:  51%|█████     | 102/200 [03:19<03:43,  2.28s/it]Running Inference:  52%|█████▏    | 103/200 [03:21<03:22,  2.09s/it]Running Inference:  52%|█████▏    | 104/200 [03:23<03:43,  2.33s/it]Running Inference:  52%|█████▎    | 105/200 [03:24<03:03,  1.93s/it]Running Inference:  53%|█████▎    | 106/200 [03:25<02:33,  1.64s/it]Running Inference:  54%|█████▎    | 107/200 [03:29<03:24,  2.20s/it]Running Inference:  54%|█████▍    | 108/200 [03:32<03:40,  2.39s/it]Running Inference:  55%|█████▍    | 109/200 [03:35<03:50,  2.54s/it]Running Inference:  55%|█████▌    | 110/200 [03:36<03:23,  2.26s/it]Running Inference:  56%|█████▌    | 111/200 [03:37<02:38,  1.78s/it]Running Inference:  56%|█████▌    | 112/200 [03:38<02:13,  1.52s/it]Running Inference:  56%|█████▋    | 113/200 [03:39<02:12,  1.53s/it]Running Inference:  57%|█████▋    | 114/200 [03:42<02:44,  1.91s/it]Running Inference:  57%|█████▊    | 115/200 [03:45<03:03,  2.16s/it]Running Inference:  58%|█████▊    | 116/200 [03:49<03:45,  2.69s/it]Running Inference:  58%|█████▊    | 117/200 [03:53<04:15,  3.08s/it]Running Inference:  59%|█████▉    | 118/200 [03:55<04:01,  2.94s/it]Running Inference:  60%|█████▉    | 119/200 [03:59<04:03,  3.00s/it]Running Inference:  60%|██████    | 120/200 [03:59<03:08,  2.35s/it]Running Inference:  60%|██████    | 121/200 [04:00<02:30,  1.90s/it]Running Inference:  61%|██████    | 122/200 [04:03<02:42,  2.09s/it]Running Inference:  62%|██████▏   | 123/200 [04:04<02:14,  1.74s/it]Running Inference:  62%|██████▏   | 124/200 [04:05<01:53,  1.49s/it]Running Inference:  62%|██████▎   | 125/200 [04:07<02:22,  1.89s/it]Running Inference:  63%|██████▎   | 126/200 [04:10<02:37,  2.13s/it]Running Inference:  64%|██████▎   | 127/200 [04:11<01:57,  1.61s/it]Running Inference:  64%|██████▍   | 128/200 [04:12<01:43,  1.43s/it]Running Inference:  64%|██████▍   | 129/200 [04:13<01:42,  1.45s/it]Running Inference:  65%|██████▌   | 130/200 [04:14<01:32,  1.32s/it]Running Inference:  66%|██████▌   | 131/200 [04:17<02:03,  1.80s/it]Running Inference:  66%|██████▌   | 132/200 [04:21<02:52,  2.53s/it]Running Inference:  66%|██████▋   | 133/200 [04:23<02:28,  2.22s/it]Running Inference:  67%|██████▋   | 134/200 [04:24<02:06,  1.92s/it]Running Inference:  68%|██████▊   | 135/200 [04:28<02:49,  2.61s/it]Running Inference:  68%|██████▊   | 136/200 [04:31<02:48,  2.64s/it]Running Inference:  68%|██████▊   | 137/200 [04:34<02:50,  2.71s/it]Running Inference:  69%|██████▉   | 138/200 [04:35<02:16,  2.21s/it]Running Inference:  70%|██████▉   | 139/200 [04:39<02:46,  2.73s/it]Running Inference:  70%|███████   | 140/200 [04:40<02:23,  2.38s/it]Running Inference:  70%|███████   | 141/200 [04:44<02:44,  2.78s/it]Running Inference:  71%|███████   | 142/200 [04:45<02:11,  2.26s/it]Running Inference:  72%|███████▏  | 143/200 [04:48<02:17,  2.40s/it]Running Inference:  72%|███████▏  | 144/200 [04:51<02:22,  2.55s/it]Running Inference:  72%|███████▎  | 145/200 [04:53<02:08,  2.33s/it]Running Inference:  73%|███████▎  | 146/200 [04:56<02:16,  2.54s/it]Running Inference:  74%|███████▎  | 147/200 [04:58<02:07,  2.41s/it]Running Inference:  74%|███████▍  | 148/200 [04:59<01:41,  1.95s/it]Running Inference:  74%|███████▍  | 149/200 [05:00<01:25,  1.68s/it]Running Inference:  75%|███████▌  | 150/200 [05:01<01:26,  1.74s/it]Running Inference:  76%|███████▌  | 151/200 [05:02<01:03,  1.30s/it]Running Inference:  76%|███████▌  | 152/200 [05:04<01:21,  1.70s/it]Running Inference:  76%|███████▋  | 153/200 [05:07<01:33,  2.00s/it]Running Inference:  77%|███████▋  | 154/200 [05:10<01:42,  2.24s/it]Running Inference:  78%|███████▊  | 155/200 [05:11<01:28,  1.97s/it]Running Inference:  78%|███████▊  | 156/200 [05:15<01:44,  2.38s/it]Running Inference:  78%|███████▊  | 157/200 [05:15<01:17,  1.80s/it]Running Inference:  79%|███████▉  | 158/200 [05:16<01:04,  1.53s/it]Running Inference:  80%|███████▉  | 159/200 [05:20<01:29,  2.19s/it]Running Inference:  80%|████████  | 160/200 [05:22<01:25,  2.15s/it]Running Inference:  80%|████████  | 161/200 [05:24<01:30,  2.33s/it]Running Inference:  81%|████████  | 162/200 [05:25<01:14,  1.95s/it]Running Inference:  82%|████████▏ | 163/200 [05:27<01:04,  1.74s/it]Running Inference:  82%|████████▏ | 164/200 [05:28<00:54,  1.51s/it]Running Inference:  82%|████████▎ | 165/200 [05:28<00:45,  1.29s/it]Running Inference:  83%|████████▎ | 166/200 [05:32<01:11,  2.10s/it]Running Inference:  84%|████████▎ | 167/200 [05:34<01:07,  2.05s/it]Running Inference:  84%|████████▍ | 168/200 [05:35<00:51,  1.60s/it]Running Inference:  84%|████████▍ | 169/200 [05:37<00:54,  1.76s/it]Running Inference:  85%|████████▌ | 170/200 [05:40<01:03,  2.12s/it]Running Inference:  86%|████████▌ | 171/200 [05:41<00:50,  1.76s/it]Running Inference:  86%|████████▌ | 172/200 [05:44<00:58,  2.10s/it]Running Inference:  86%|████████▋ | 173/200 [05:45<00:47,  1.77s/it]Running Inference:  87%|████████▋ | 174/200 [05:46<00:39,  1.51s/it]Running Inference:  88%|████████▊ | 175/200 [05:48<00:41,  1.65s/it]Running Inference:  88%|████████▊ | 176/200 [05:51<00:48,  2.03s/it]Running Inference:  88%|████████▊ | 177/200 [05:51<00:38,  1.67s/it]Running Inference:  89%|████████▉ | 178/200 [05:54<00:39,  1.81s/it]Running Inference:  90%|████████▉ | 179/200 [05:57<00:50,  2.39s/it]Running Inference:  90%|█████████ | 180/200 [05:59<00:41,  2.07s/it]Running Inference:  90%|█████████ | 181/200 [06:00<00:36,  1.92s/it]Running Inference:  91%|█████████ | 182/200 [06:02<00:33,  1.87s/it]Running Inference:  92%|█████████▏| 183/200 [06:05<00:39,  2.30s/it]Running Inference:  92%|█████████▏| 184/200 [06:06<00:29,  1.84s/it]Running Inference:  92%|█████████▎| 185/200 [06:08<00:29,  1.98s/it]Running Inference:  93%|█████████▎| 186/200 [06:09<00:22,  1.59s/it]Running Inference:  94%|█████████▎| 187/200 [06:10<00:19,  1.51s/it]Running Inference:  94%|█████████▍| 188/200 [06:13<00:22,  1.88s/it]Running Inference:  94%|█████████▍| 189/200 [06:16<00:23,  2.17s/it]Running Inference:  95%|█████████▌| 190/200 [06:16<00:16,  1.62s/it]Running Inference:  96%|█████████▌| 191/200 [06:20<00:20,  2.24s/it]Running Inference:  96%|█████████▌| 192/200 [06:23<00:19,  2.46s/it]Running Inference:  96%|█████████▋| 193/200 [06:27<00:19,  2.84s/it]Running Inference:  97%|█████████▋| 194/200 [06:27<00:12,  2.14s/it]Running Inference:  98%|█████████▊| 195/200 [06:30<00:12,  2.43s/it]Running Inference:  98%|█████████▊| 196/200 [06:33<00:10,  2.60s/it]Running Inference:  98%|█████████▊| 197/200 [06:34<00:06,  2.16s/it]Running Inference:  99%|█████████▉| 198/200 [06:35<00:03,  1.68s/it]Running Inference: 100%|█████████▉| 199/200 [06:36<00:01,  1.42s/it]Running Inference: 100%|██████████| 200/200 [06:36<00:00,  1.17s/it]Running Inference: 100%|██████████| 200/200 [06:36<00:00,  1.98s/it]
2025-12-13 18:21:54,012 - INFO - Inference completed.
2025-12-13 18:21:54,020 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.50/predictions.csv
2025-12-13 18:21:54,020 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:21:54,025 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__keydiff__0.50/metrics.json
2025-12-13 18:21:54,025 - INFO - Metrics:
17.29
2025-12-13 18:21:54,027 - INFO - Evaluation run completed successfully.
✓ Completed: keydiff (task=2wikimqa, ratio=0.5) on GPU 5


========================================
LongBench Task: hotpotqa
========================================
----------------------------------------
Task: hotpotqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:22:00,360 - INFO - Set deterministic seeds to 42
2025-12-13 18:22:00,360 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "keydiff",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:22:00,360 - INFO - Starting evaluation run...
2025-12-13 18:22:00,360 - INFO - Output directory set to: longbenchresult
2025-12-13 18:22:00,360 - INFO - Set KeyDiffPress compression_ratio to 0.1
2025-12-13 18:22:00,360 - INFO - KV Press 'keydiff' setup.
2025-12-13 18:22:00,360 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:22:00,360 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.19it/s]
Device set to use cuda:0
2025-12-13 18:22:12,202 - INFO - Model pipeline loaded.
2025-12-13 18:22:12,202 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:22:16,730 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:22:16,730 - INFO - Dataset processed with 200 entries.
2025-12-13 18:22:16,764 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<17:30,  5.28s/it]Running Inference:   1%|          | 2/200 [00:05<08:24,  2.55s/it]Running Inference:   2%|▏         | 3/200 [00:08<07:51,  2.39s/it]Running Inference:   2%|▏         | 4/200 [00:12<10:16,  3.14s/it]Running Inference:   2%|▎         | 5/200 [00:13<07:38,  2.35s/it]Running Inference:   3%|▎         | 6/200 [00:16<08:35,  2.66s/it]Running Inference:   4%|▎         | 7/200 [00:18<07:51,  2.44s/it]Running Inference:   4%|▍         | 8/200 [00:19<06:34,  2.05s/it]Running Inference:   4%|▍         | 9/200 [00:21<06:14,  1.96s/it]Running Inference:   5%|▌         | 10/200 [00:22<05:03,  1.60s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<04:55,  1.56s/it]Running Inference:   6%|▌         | 12/200 [00:25<04:35,  1.46s/it]Running Inference:   6%|▋         | 13/200 [00:27<05:01,  1.61s/it]Running Inference:   7%|▋         | 14/200 [00:29<05:46,  1.86s/it]Running Inference:   8%|▊         | 15/200 [00:32<06:21,  2.06s/it]Running Inference:   8%|▊         | 16/200 [00:34<06:30,  2.12s/it]Running Inference:   8%|▊         | 17/200 [00:38<08:23,  2.75s/it]Running Inference:   9%|▉         | 18/200 [00:40<07:47,  2.57s/it]Running Inference:  10%|▉         | 19/200 [00:41<06:03,  2.01s/it]Running Inference:  10%|█         | 20/200 [00:44<07:07,  2.37s/it]Running Inference:  10%|█         | 21/200 [00:45<05:36,  1.88s/it]Running Inference:  11%|█         | 22/200 [00:45<04:25,  1.49s/it]Running Inference:  12%|█▏        | 23/200 [00:48<05:35,  1.90s/it]Running Inference:  12%|█▏        | 24/200 [00:52<07:32,  2.57s/it]Running Inference:  12%|█▎        | 25/200 [00:54<06:28,  2.22s/it]Running Inference:  13%|█▎        | 26/200 [00:54<05:04,  1.75s/it]Running Inference:  14%|█▎        | 27/200 [00:56<05:07,  1.78s/it]Running Inference:  14%|█▍        | 28/200 [00:59<05:45,  2.01s/it]Running Inference:  14%|█▍        | 29/200 [01:01<06:13,  2.18s/it]Running Inference:  15%|█▌        | 30/200 [01:06<08:02,  2.84s/it]Running Inference:  16%|█▌        | 31/200 [01:07<06:41,  2.37s/it]Running Inference:  16%|█▌        | 32/200 [01:11<08:15,  2.95s/it]Running Inference:  16%|█▋        | 33/200 [01:14<07:44,  2.78s/it]Running Inference:  17%|█▋        | 34/200 [01:16<07:10,  2.59s/it]Running Inference:  18%|█▊        | 35/200 [01:18<06:50,  2.49s/it]Running Inference:  18%|█▊        | 36/200 [01:21<06:43,  2.46s/it]Running Inference:  18%|█▊        | 37/200 [01:23<06:35,  2.42s/it]Running Inference:  19%|█▉        | 38/200 [01:25<06:22,  2.36s/it]Running Inference:  20%|█▉        | 39/200 [01:29<07:53,  2.94s/it]Running Inference:  20%|██        | 40/200 [01:32<07:27,  2.80s/it]Running Inference:  20%|██        | 41/200 [01:33<06:27,  2.44s/it]Running Inference:  21%|██        | 42/200 [01:36<06:10,  2.34s/it]Running Inference:  22%|██▏       | 43/200 [01:37<05:17,  2.02s/it]Running Inference:  22%|██▏       | 44/200 [01:41<06:54,  2.66s/it]Running Inference:  22%|██▎       | 45/200 [01:43<06:25,  2.49s/it]Running Inference:  23%|██▎       | 46/200 [01:45<06:09,  2.40s/it]Running Inference:  24%|██▎       | 47/200 [01:49<07:32,  2.95s/it]Running Inference:  24%|██▍       | 48/200 [01:51<06:29,  2.56s/it]Running Inference:  24%|██▍       | 49/200 [01:55<07:34,  3.01s/it]Running Inference:  25%|██▌       | 50/200 [01:57<06:32,  2.61s/it]Running Inference:  26%|██▌       | 51/200 [01:59<05:55,  2.39s/it]Running Inference:  26%|██▌       | 52/200 [02:01<05:49,  2.36s/it]Running Inference:  26%|██▋       | 53/200 [02:02<05:00,  2.05s/it]Running Inference:  27%|██▋       | 54/200 [02:04<05:00,  2.06s/it]Running Inference:  28%|██▊       | 55/200 [02:07<05:15,  2.17s/it]Running Inference:  28%|██▊       | 56/200 [02:09<05:17,  2.20s/it]Running Inference:  28%|██▊       | 57/200 [02:13<06:15,  2.63s/it]Running Inference:  29%|██▉       | 58/200 [02:16<06:55,  2.93s/it]Running Inference:  30%|██▉       | 59/200 [02:18<05:42,  2.43s/it]Running Inference:  30%|███       | 60/200 [02:20<05:27,  2.34s/it]Running Inference:  30%|███       | 61/200 [02:22<05:08,  2.22s/it]Running Inference:  31%|███       | 62/200 [02:26<06:30,  2.83s/it]Running Inference:  32%|███▏      | 63/200 [02:30<06:59,  3.06s/it]Running Inference:  32%|███▏      | 64/200 [02:34<07:44,  3.42s/it]Running Inference:  32%|███▎      | 65/200 [02:36<06:49,  3.04s/it]Running Inference:  33%|███▎      | 66/200 [02:38<06:01,  2.70s/it]Running Inference:  34%|███▎      | 67/200 [02:39<05:05,  2.30s/it]Running Inference:  34%|███▍      | 68/200 [02:41<04:45,  2.16s/it]Running Inference:  34%|███▍      | 69/200 [02:44<05:27,  2.50s/it]Running Inference:  35%|███▌      | 70/200 [02:48<06:09,  2.84s/it]Running Inference:  36%|███▌      | 71/200 [02:51<06:05,  2.84s/it]Running Inference:  36%|███▌      | 72/200 [02:53<05:38,  2.65s/it]Running Inference:  36%|███▋      | 73/200 [02:57<06:42,  3.17s/it]Running Inference:  37%|███▋      | 74/200 [02:59<05:46,  2.75s/it]Running Inference:  38%|███▊      | 75/200 [03:01<05:02,  2.42s/it]Running Inference:  38%|███▊      | 76/200 [03:04<05:28,  2.65s/it]Running Inference:  38%|███▊      | 77/200 [03:05<04:19,  2.11s/it]Running Inference:  39%|███▉      | 78/200 [03:07<04:10,  2.06s/it]Running Inference:  40%|███▉      | 79/200 [03:09<03:58,  1.97s/it]Running Inference:  40%|████      | 80/200 [03:09<03:15,  1.63s/it]Running Inference:  40%|████      | 81/200 [03:12<03:36,  1.82s/it]Running Inference:  41%|████      | 82/200 [03:13<03:33,  1.81s/it]Running Inference:  42%|████▏     | 83/200 [03:15<03:38,  1.87s/it]Running Inference:  42%|████▏     | 84/200 [03:19<04:39,  2.41s/it]Running Inference:  42%|████▎     | 85/200 [03:22<04:54,  2.56s/it]Running Inference:  43%|████▎     | 86/200 [03:25<05:11,  2.74s/it]Running Inference:  44%|████▎     | 87/200 [03:29<05:51,  3.11s/it]Running Inference:  44%|████▍     | 88/200 [03:31<04:52,  2.61s/it]Running Inference:  44%|████▍     | 89/200 [03:33<04:39,  2.52s/it]Running Inference:  45%|████▌     | 90/200 [03:35<04:15,  2.32s/it]Running Inference:  46%|████▌     | 91/200 [03:36<03:26,  1.89s/it]Running Inference:  46%|████▌     | 92/200 [03:38<03:45,  2.09s/it]Running Inference:  46%|████▋     | 93/200 [03:40<03:28,  1.95s/it]Running Inference:  47%|████▋     | 94/200 [03:43<04:20,  2.46s/it]Running Inference:  48%|████▊     | 95/200 [03:45<03:52,  2.21s/it]Running Inference:  48%|████▊     | 96/200 [03:47<03:48,  2.20s/it]Running Inference:  48%|████▊     | 97/200 [03:49<03:34,  2.09s/it]Running Inference:  49%|████▉     | 98/200 [03:52<03:44,  2.20s/it]Running Inference:  50%|████▉     | 99/200 [03:54<03:38,  2.16s/it]Running Inference:  50%|█████     | 100/200 [03:56<03:39,  2.19s/it]Running Inference:  50%|█████     | 101/200 [03:58<03:46,  2.29s/it]Running Inference:  51%|█████     | 102/200 [04:01<03:37,  2.22s/it]Running Inference:  52%|█████▏    | 103/200 [04:01<02:54,  1.80s/it]Running Inference:  52%|█████▏    | 104/200 [04:03<02:45,  1.72s/it]Running Inference:  52%|█████▎    | 105/200 [04:04<02:35,  1.64s/it]Running Inference:  53%|█████▎    | 106/200 [04:06<02:45,  1.76s/it]Running Inference:  54%|█████▎    | 107/200 [04:09<03:15,  2.11s/it]Running Inference:  54%|█████▍    | 108/200 [04:10<02:36,  1.70s/it]Running Inference:  55%|█████▍    | 109/200 [04:11<02:28,  1.63s/it]Running Inference:  55%|█████▌    | 110/200 [04:13<02:22,  1.58s/it]Running Inference:  56%|█████▌    | 111/200 [04:15<02:39,  1.80s/it]Running Inference:  56%|█████▌    | 112/200 [04:19<03:41,  2.52s/it]Running Inference:  56%|█████▋    | 113/200 [04:22<03:36,  2.48s/it]Running Inference:  57%|█████▋    | 114/200 [04:24<03:26,  2.40s/it]Running Inference:  57%|█████▊    | 115/200 [04:26<03:20,  2.36s/it]Running Inference:  58%|█████▊    | 116/200 [04:28<03:08,  2.24s/it]Running Inference:  58%|█████▊    | 117/200 [04:31<03:10,  2.29s/it]Running Inference:  59%|█████▉    | 118/200 [04:33<03:07,  2.29s/it]Running Inference:  60%|█████▉    | 119/200 [04:34<02:37,  1.94s/it]Running Inference:  60%|██████    | 120/200 [04:38<03:23,  2.54s/it]Running Inference:  60%|██████    | 121/200 [04:40<03:14,  2.46s/it]Running Inference:  61%|██████    | 122/200 [04:43<03:05,  2.38s/it]Running Inference:  62%|██████▏   | 123/200 [04:47<03:45,  2.93s/it]Running Inference:  62%|██████▏   | 124/200 [04:49<03:20,  2.63s/it]Running Inference:  62%|██████▎   | 125/200 [04:51<03:12,  2.56s/it]Running Inference:  63%|██████▎   | 126/200 [04:53<03:02,  2.46s/it]Running Inference:  64%|██████▎   | 127/200 [04:54<02:28,  2.04s/it]Running Inference:  64%|██████▍   | 128/200 [04:59<03:17,  2.74s/it]Running Inference:  64%|██████▍   | 129/200 [05:01<03:01,  2.56s/it]Running Inference:  65%|██████▌   | 130/200 [05:03<02:43,  2.33s/it]Running Inference:  66%|██████▌   | 131/200 [05:07<03:13,  2.81s/it]Running Inference:  66%|██████▌   | 132/200 [05:08<02:42,  2.38s/it]Running Inference:  66%|██████▋   | 133/200 [05:11<02:58,  2.67s/it]Running Inference:  67%|██████▋   | 134/200 [05:14<02:48,  2.56s/it]Running Inference:  68%|██████▊   | 135/200 [05:16<02:35,  2.40s/it]Running Inference:  68%|██████▊   | 136/200 [05:18<02:26,  2.28s/it]Running Inference:  68%|██████▊   | 137/200 [05:19<02:06,  2.01s/it]Running Inference:  69%|██████▉   | 138/200 [05:22<02:14,  2.17s/it]Running Inference:  70%|██████▉   | 139/200 [05:26<02:50,  2.79s/it]Running Inference:  70%|███████   | 140/200 [05:30<03:07,  3.12s/it]Running Inference:  70%|███████   | 141/200 [05:32<02:48,  2.85s/it]Running Inference:  71%|███████   | 142/200 [05:36<03:07,  3.24s/it]Running Inference:  72%|███████▏  | 143/200 [05:37<02:22,  2.51s/it]Running Inference:  72%|███████▏  | 144/200 [05:38<01:52,  2.01s/it]Running Inference:  72%|███████▎  | 145/200 [05:38<01:22,  1.51s/it]Running Inference:  73%|███████▎  | 146/200 [05:42<02:05,  2.33s/it]Running Inference:  74%|███████▎  | 147/200 [05:44<02:00,  2.27s/it]Running Inference:  74%|███████▍  | 148/200 [05:46<01:52,  2.16s/it]Running Inference:  74%|███████▍  | 149/200 [05:47<01:25,  1.69s/it]Running Inference:  75%|███████▌  | 150/200 [05:50<01:52,  2.26s/it]Running Inference:  76%|███████▌  | 151/200 [05:54<02:06,  2.58s/it]Running Inference:  76%|███████▌  | 152/200 [05:56<01:53,  2.37s/it]Running Inference:  76%|███████▋  | 153/200 [05:58<01:46,  2.26s/it]Running Inference:  77%|███████▋  | 154/200 [06:00<01:44,  2.28s/it]Running Inference:  78%|███████▊  | 155/200 [06:02<01:42,  2.27s/it]Running Inference:  78%|███████▊  | 156/200 [06:03<01:25,  1.94s/it]Running Inference:  78%|███████▊  | 157/200 [06:05<01:17,  1.79s/it]Running Inference:  79%|███████▉  | 158/200 [06:06<01:01,  1.47s/it]Running Inference:  80%|███████▉  | 159/200 [06:07<01:00,  1.47s/it]Running Inference:  80%|████████  | 160/200 [06:11<01:30,  2.27s/it]Running Inference:  80%|████████  | 161/200 [06:15<01:45,  2.72s/it]Running Inference:  81%|████████  | 162/200 [06:16<01:20,  2.13s/it]Running Inference:  82%|████████▏ | 163/200 [06:17<01:13,  1.99s/it]Running Inference:  82%|████████▏ | 164/200 [06:20<01:13,  2.04s/it]Running Inference:  82%|████████▎ | 165/200 [06:24<01:33,  2.68s/it]Running Inference:  83%|████████▎ | 166/200 [06:26<01:25,  2.50s/it]Running Inference:  84%|████████▎ | 167/200 [06:28<01:17,  2.36s/it]Running Inference:  84%|████████▍ | 168/200 [06:32<01:31,  2.86s/it]Running Inference:  84%|████████▍ | 169/200 [06:34<01:24,  2.74s/it]Running Inference:  85%|████████▌ | 170/200 [06:38<01:28,  2.94s/it]Running Inference:  86%|████████▌ | 171/200 [06:41<01:24,  2.92s/it]Running Inference:  86%|████████▌ | 172/200 [06:43<01:18,  2.81s/it]Running Inference:  86%|████████▋ | 173/200 [06:45<01:10,  2.59s/it]Running Inference:  87%|████████▋ | 174/200 [06:49<01:16,  2.94s/it]Running Inference:  88%|████████▊ | 175/200 [06:51<01:08,  2.75s/it]Running Inference:  88%|████████▊ | 176/200 [06:53<01:01,  2.58s/it]Running Inference:  88%|████████▊ | 177/200 [06:56<00:57,  2.50s/it]Running Inference:  89%|████████▉ | 178/200 [07:00<01:06,  3.03s/it]Running Inference:  90%|████████▉ | 179/200 [07:02<00:58,  2.81s/it]Running Inference:  90%|█████████ | 180/200 [07:06<01:00,  3.04s/it]Running Inference:  90%|█████████ | 181/200 [07:08<00:53,  2.80s/it]Running Inference:  91%|█████████ | 182/200 [07:10<00:45,  2.51s/it]Running Inference:  92%|█████████▏| 183/200 [07:12<00:37,  2.22s/it]Running Inference:  92%|█████████▏| 184/200 [07:16<00:44,  2.76s/it]Running Inference:  92%|█████████▎| 185/200 [07:17<00:35,  2.40s/it]Running Inference:  93%|█████████▎| 186/200 [07:19<00:32,  2.31s/it]Running Inference:  94%|█████████▎| 187/200 [07:21<00:28,  2.17s/it]Running Inference:  94%|█████████▍| 188/200 [07:22<00:20,  1.73s/it]Running Inference:  94%|█████████▍| 189/200 [07:25<00:23,  2.11s/it]Running Inference:  95%|█████████▌| 190/200 [07:26<00:19,  1.92s/it]Running Inference:  96%|█████████▌| 191/200 [07:28<00:17,  1.99s/it]Running Inference:  96%|█████████▌| 192/200 [07:30<00:14,  1.79s/it]Running Inference:  96%|█████████▋| 193/200 [07:32<00:13,  1.97s/it]Running Inference:  97%|█████████▋| 194/200 [07:36<00:14,  2.44s/it]Running Inference:  98%|█████████▊| 195/200 [07:37<00:10,  2.04s/it]Running Inference:  98%|█████████▊| 196/200 [07:39<00:08,  2.16s/it]Running Inference:  98%|█████████▊| 197/200 [07:41<00:05,  1.93s/it]Running Inference:  99%|█████████▉| 198/200 [07:45<00:05,  2.53s/it]Running Inference: 100%|█████████▉| 199/200 [07:47<00:02,  2.53s/it]Running Inference: 100%|██████████| 200/200 [07:48<00:00,  1.97s/it]Running Inference: 100%|██████████| 200/200 [07:48<00:00,  2.34s/it]
2025-12-13 18:30:04,987 - INFO - Inference completed.
2025-12-13 18:30:04,996 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__keydiff__0.10/predictions.csv
2025-12-13 18:30:04,996 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:30:05,000 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__keydiff__0.10/metrics.json
2025-12-13 18:30:05,000 - INFO - Metrics:
36.59
2025-12-13 18:30:05,001 - INFO - Evaluation run completed successfully.
✓ Completed: keydiff (task=hotpotqa, ratio=0.1) on GPU 5

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:30:11,391 - INFO - Set deterministic seeds to 42
2025-12-13 18:30:11,391 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "keydiff",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:30:11,391 - INFO - Starting evaluation run...
2025-12-13 18:30:11,391 - INFO - Output directory set to: longbenchresult
2025-12-13 18:30:11,391 - INFO - Set KeyDiffPress compression_ratio to 0.2
2025-12-13 18:30:11,392 - INFO - KV Press 'keydiff' setup.
2025-12-13 18:30:11,392 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:30:11,392 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.50it/s]
Device set to use cuda:0
2025-12-13 18:31:01,074 - INFO - Model pipeline loaded.
2025-12-13 18:31:01,075 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
Using the latest cached version of the dataset since Xnhyacinth/LongBench couldn't be found on the Hugging Face Hub
Traceback (most recent call last):
  File "/data/kvpress-main1/evaluation/evaluate.py", line 685, in <module>
    Fire(CliEntryPoint)
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 568, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/evaluation/evaluate.py", line 681, in __call__
    runner.run_evaluation()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 631, in run_evaluation
    self._load_and_prepare_dataset()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 411, in _load_and_prepare_dataset
    df = load_dataset(DATASET_REGISTRY[dataset_name], data_dir=data_dir, split="test").to_pandas()
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2606, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2314, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 140, in __init__
    config_name, version, hash = _find_hash_in_cache(
                                 ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 65, in _find_hash_in_cache
    raise ValueError(
ValueError: Couldn't find cache for Xnhyacinth/LongBench for config 'default-data_dir=hotpotqa'
Available configs in the cache: ['2wikimqa', '2wikimqa_e', 'default-531e246ba9fed12a', 'default-dce78539c6759811', 'dureader', 'gov_report', 'gov_report_e', 'hotpotqa', 'hotpotqa_e', 'lcc', 'lcc_e', 'lsht', 'multi_news', 'multi_news_e', 'multifieldqa_en', 'multifieldqa_en_e', 'multifieldqa_zh', 'musique', 'narrativeqa', 'passage_count', 'passage_count_e', 'passage_retrieval_en', 'passage_retrieval_en_e', 'passage_retrieval_zh', 'qasper', 'qasper_e', 'qmsum', 'repobench-p', 'repobench-p_e', 'samsum', 'samsum_e', 'trec', 'trec_e', 'triviaqa', 'triviaqa_e', 'vcsum']
