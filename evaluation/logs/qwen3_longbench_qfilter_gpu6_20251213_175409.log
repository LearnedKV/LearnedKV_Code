==========================================
Method: qfilter
GPU: 6
Start Time: Sat Dec 13 05:54:17 PM CST 2025
==========================================

========================================
LongBench Task: 2wikimqa
========================================
----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 17:54:22,627 - INFO - Set deterministic seeds to 42
2025-12-13 17:54:22,627 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "qfilter",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 17:54:22,627 - INFO - Starting evaluation run...
2025-12-13 17:54:22,628 - INFO - Output directory set to: longbenchresult
2025-12-13 17:54:22,628 - INFO - Set QFilterPress compression_ratio to 0.1
2025-12-13 17:54:22,628 - INFO - KV Press 'qfilter' setup.
2025-12-13 17:54:22,628 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 17:54:22,628 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.38it/s]
Device set to use cuda:0
2025-12-13 17:54:36,482 - INFO - Model pipeline loaded.
2025-12-13 17:54:36,482 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 17:54:41,278 - INFO - Dataset loaded with 200 entries.
2025-12-13 17:54:41,278 - INFO - Dataset processed with 200 entries.
2025-12-13 17:54:41,298 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 0/200 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/data/kvpress-main1/kvpress/presses/qfilter_press.py", line 64, in load_q_filters
    return QFilters.from_pretrained(f"nthngdy/{model_name}_qfilt").q_filters
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/huggingface_hub/hub_mixin.py", line 566, in from_pretrained
    instance = cls._from_pretrained(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/huggingface_hub/hub_mixin.py", line 789, in _from_pretrained
    model = cls(**model_kwargs)
            ^^^^^^^^^^^^^^^^^^^
TypeError: QFilters.__init__() missing 3 required positional arguments: 'num_layers', 'num_kv_heads', and 'kv_head_dim'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/kvpress-main1/evaluation/evaluate.py", line 685, in <module>
    Fire(CliEntryPoint)
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 568, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/evaluation/evaluate.py", line 681, in __call__
    runner.run_evaluation()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 633, in run_evaluation
    self._run_inference()
  File "/data/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/evaluation/evaluate.py", line 549, in _run_inference
    output = self.pipeline(  # type: ignore[misc]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1467, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1474, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1374, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/kvpress/pipeline.py", line 208, in _forward
    with press(self.model) if perform_prefill_compression else contextlib.nullcontext():
  File "/data/anaconda3/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/data/kvpress-main1/kvpress/presses/qfilter_press.py", line 87, in __call__
    self.__post_init_from_model__(model)
  File "/data/kvpress-main1/kvpress/presses/qfilter_press.py", line 56, in __post_init_from_model__
    self.q_filters = self.load_q_filters(model_name)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/kvpress/presses/qfilter_press.py", line 66, in load_q_filters
    raise ValueError(
ValueError: Could not load Q-filters for Qwen3-8B. Available models: ['Llama-3.1-8B-Instruct', 'Llama-3.2-1B-Instruct', 'Llama-3.2-3B-Instruct', 'Llama-3.2-3B', 'Llama-3.1-8B', 'Llama-3.1-70B-Instruct', 'Llama-3.1-70B', 'Meta-Llama-3.1-405B', 'Mistral-Small-24B-Instruct-2501', 'phi-4', 'Llama-3.2-1B', 'Qwen2.5-7B', 'Qwen2.5-7B-Instruct', 'DeepSeek-R1-Distill-Llama-8B', 'DeepSeek-R1-Distill-Qwen-1.5B']
