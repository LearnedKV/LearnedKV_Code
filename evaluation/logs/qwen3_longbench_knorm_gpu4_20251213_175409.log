==========================================
Method: knorm
GPU: 4
Start Time: Sat Dec 13 05:54:13 PM CST 2025
==========================================

========================================
LongBench Task: 2wikimqa
========================================
----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 17:54:18,679 - INFO - Set deterministic seeds to 42
2025-12-13 17:54:18,679 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 17:54:18,679 - INFO - Starting evaluation run...
2025-12-13 17:54:18,679 - INFO - Output directory set to: longbenchresult
2025-12-13 17:54:18,679 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 17:54:18,679 - INFO - KV Press 'knorm' setup.
2025-12-13 17:54:18,680 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 17:54:18,680 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.94it/s]
Device set to use cuda:0
2025-12-13 17:54:32,704 - INFO - Model pipeline loaded.
2025-12-13 17:54:32,705 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 17:54:38,870 - INFO - Dataset loaded with 200 entries.
2025-12-13 17:54:38,870 - INFO - Dataset processed with 200 entries.
2025-12-13 17:54:38,889 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:08,  4.26s/it]Running Inference:   1%|          | 2/200 [00:07<11:30,  3.49s/it]Running Inference:   2%|▏         | 3/200 [00:08<07:57,  2.43s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:29,  2.29s/it]Running Inference:   2%|▎         | 5/200 [00:14<09:40,  2.97s/it]Running Inference:   3%|▎         | 6/200 [00:16<08:02,  2.49s/it]Running Inference:   4%|▎         | 7/200 [00:17<06:56,  2.16s/it]Running Inference:   4%|▍         | 8/200 [00:20<07:57,  2.49s/it]Running Inference:   4%|▍         | 9/200 [00:22<06:36,  2.08s/it]Running Inference:   5%|▌         | 10/200 [00:22<05:06,  1.61s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:26<07:00,  2.22s/it]Running Inference:   6%|▌         | 12/200 [00:27<06:26,  2.06s/it]Running Inference:   6%|▋         | 13/200 [00:31<07:49,  2.51s/it]Running Inference:   7%|▋         | 14/200 [00:31<05:57,  1.92s/it]Running Inference:   8%|▊         | 15/200 [00:32<05:02,  1.63s/it]Running Inference:   8%|▊         | 16/200 [00:33<04:21,  1.42s/it]Running Inference:   8%|▊         | 17/200 [00:36<05:10,  1.69s/it]Running Inference:   9%|▉         | 18/200 [00:37<04:29,  1.48s/it]Running Inference:  10%|▉         | 19/200 [00:37<03:44,  1.24s/it]Running Inference:  10%|█         | 20/200 [00:38<02:52,  1.05it/s]Running Inference:  10%|█         | 21/200 [00:40<03:51,  1.29s/it]Running Inference:  11%|█         | 22/200 [00:41<03:27,  1.17s/it]Running Inference:  12%|█▏        | 23/200 [00:42<03:39,  1.24s/it]Running Inference:  12%|█▏        | 24/200 [00:43<03:18,  1.13s/it]Running Inference:  12%|█▎        | 25/200 [00:44<03:15,  1.12s/it]Running Inference:  13%|█▎        | 26/200 [00:47<04:48,  1.66s/it]Running Inference:  14%|█▎        | 27/200 [00:50<05:41,  1.97s/it]Running Inference:  14%|█▍        | 28/200 [00:50<04:34,  1.59s/it]Running Inference:  14%|█▍        | 29/200 [00:53<05:43,  2.01s/it]Running Inference:  15%|█▌        | 30/200 [00:54<04:12,  1.49s/it]Running Inference:  16%|█▌        | 31/200 [00:55<03:50,  1.36s/it]Running Inference:  16%|█▌        | 32/200 [00:58<05:12,  1.86s/it]Running Inference:  16%|█▋        | 33/200 [00:59<05:00,  1.80s/it]Running Inference:  17%|█▋        | 34/200 [01:00<04:22,  1.58s/it]Running Inference:  18%|█▊        | 35/200 [01:01<03:44,  1.36s/it]Running Inference:  18%|█▊        | 36/200 [01:02<03:20,  1.22s/it]Running Inference:  18%|█▊        | 37/200 [01:05<04:50,  1.79s/it]Running Inference:  19%|█▉        | 38/200 [01:07<04:28,  1.66s/it]Running Inference:  20%|█▉        | 39/200 [01:10<05:27,  2.03s/it]Running Inference:  20%|██        | 40/200 [01:10<04:29,  1.68s/it]Running Inference:  20%|██        | 41/200 [01:13<05:25,  2.05s/it]Running Inference:  21%|██        | 42/200 [01:14<04:30,  1.71s/it]Running Inference:  22%|██▏       | 43/200 [01:15<03:49,  1.46s/it]Running Inference:  22%|██▏       | 44/200 [01:16<03:10,  1.22s/it]Running Inference:  22%|██▎       | 45/200 [01:17<02:50,  1.10s/it]Running Inference:  23%|██▎       | 46/200 [01:20<04:17,  1.67s/it]Running Inference:  24%|██▎       | 47/200 [01:21<03:43,  1.46s/it]Running Inference:  24%|██▍       | 48/200 [01:23<04:47,  1.89s/it]Running Inference:  24%|██▍       | 49/200 [01:27<05:40,  2.25s/it]Running Inference:  25%|██▌       | 50/200 [01:28<05:18,  2.12s/it]Running Inference:  26%|██▌       | 51/200 [01:31<05:17,  2.13s/it]Running Inference:  26%|██▌       | 52/200 [01:33<05:29,  2.23s/it]Running Inference:  26%|██▋       | 53/200 [01:38<07:37,  3.11s/it]Running Inference:  27%|██▋       | 54/200 [01:41<07:24,  3.04s/it]Running Inference:  28%|██▊       | 55/200 [01:42<05:45,  2.38s/it]Running Inference:  28%|██▊       | 56/200 [01:43<04:54,  2.05s/it]Running Inference:  28%|██▊       | 57/200 [01:46<05:24,  2.27s/it]Running Inference:  29%|██▉       | 58/200 [01:47<04:36,  1.94s/it]Running Inference:  30%|██▉       | 59/200 [01:50<05:08,  2.19s/it]Running Inference:  30%|███       | 60/200 [01:51<04:18,  1.84s/it]Running Inference:  30%|███       | 61/200 [01:52<03:26,  1.49s/it]Running Inference:  31%|███       | 62/200 [01:53<03:35,  1.56s/it]Running Inference:  32%|███▏      | 63/200 [01:54<03:11,  1.40s/it]Running Inference:  32%|███▏      | 64/200 [01:56<03:36,  1.59s/it]Running Inference:  32%|███▎      | 65/200 [01:57<02:49,  1.25s/it]Running Inference:  33%|███▎      | 66/200 [01:58<02:45,  1.24s/it]Running Inference:  34%|███▎      | 67/200 [01:59<02:46,  1.25s/it]Running Inference:  34%|███▍      | 68/200 [02:01<03:16,  1.49s/it]Running Inference:  34%|███▍      | 69/200 [02:03<03:10,  1.46s/it]Running Inference:  35%|███▌      | 70/200 [02:04<02:52,  1.32s/it]Running Inference:  36%|███▌      | 71/200 [02:04<02:24,  1.12s/it]Running Inference:  36%|███▌      | 72/200 [02:08<03:43,  1.75s/it]Running Inference:  36%|███▋      | 73/200 [02:11<04:27,  2.11s/it]Running Inference:  37%|███▋      | 74/200 [02:13<04:26,  2.12s/it]Running Inference:  38%|███▊      | 75/200 [02:14<03:46,  1.82s/it]Running Inference:  38%|███▊      | 76/200 [02:15<03:22,  1.63s/it]Running Inference:  38%|███▊      | 77/200 [02:16<03:00,  1.47s/it]Running Inference:  39%|███▉      | 78/200 [02:17<02:49,  1.39s/it]Running Inference:  40%|███▉      | 79/200 [02:18<02:35,  1.29s/it]Running Inference:  40%|████      | 80/200 [02:22<03:54,  1.95s/it]Running Inference:  40%|████      | 81/200 [02:24<03:44,  1.89s/it]Running Inference:  41%|████      | 82/200 [02:29<05:52,  2.99s/it]Running Inference:  42%|████▏     | 83/200 [02:31<05:01,  2.58s/it]Running Inference:  42%|████▏     | 84/200 [02:32<04:01,  2.08s/it]Running Inference:  42%|████▎     | 85/200 [02:33<03:26,  1.80s/it]Running Inference:  43%|████▎     | 86/200 [02:35<03:33,  1.87s/it]Running Inference:  44%|████▎     | 87/200 [02:38<04:02,  2.15s/it]Running Inference:  44%|████▍     | 88/200 [02:39<03:35,  1.93s/it]Running Inference:  44%|████▍     | 89/200 [02:42<04:04,  2.21s/it]Running Inference:  45%|████▌     | 90/200 [02:45<04:30,  2.46s/it]Running Inference:  46%|████▌     | 91/200 [02:48<04:37,  2.55s/it]Running Inference:  46%|████▌     | 92/200 [02:51<04:49,  2.68s/it]Running Inference:  46%|████▋     | 93/200 [02:53<04:19,  2.42s/it]Running Inference:  47%|████▋     | 94/200 [02:53<03:22,  1.91s/it]Running Inference:  48%|████▊     | 95/200 [02:54<02:43,  1.56s/it]Running Inference:  48%|████▊     | 96/200 [02:55<02:23,  1.38s/it]Running Inference:  48%|████▊     | 97/200 [02:56<02:21,  1.37s/it]Running Inference:  49%|████▉     | 98/200 [02:57<02:10,  1.28s/it]Running Inference:  50%|████▉     | 99/200 [02:58<01:51,  1.11s/it]Running Inference:  50%|█████     | 100/200 [03:01<02:42,  1.62s/it]Running Inference:  50%|█████     | 101/200 [03:04<03:14,  1.96s/it]Running Inference:  51%|█████     | 102/200 [03:04<02:39,  1.62s/it]Running Inference:  52%|█████▏    | 103/200 [03:05<02:10,  1.35s/it]Running Inference:  52%|█████▏    | 104/200 [03:07<02:16,  1.43s/it]Running Inference:  52%|█████▎    | 105/200 [03:08<01:57,  1.23s/it]Running Inference:  53%|█████▎    | 106/200 [03:10<02:42,  1.73s/it]Running Inference:  54%|█████▎    | 107/200 [03:14<03:31,  2.27s/it]Running Inference:  54%|█████▍    | 108/200 [03:17<03:46,  2.46s/it]Running Inference:  55%|█████▍    | 109/200 [03:18<03:00,  1.99s/it]Running Inference:  55%|█████▌    | 110/200 [03:19<02:47,  1.86s/it]Running Inference:  56%|█████▌    | 111/200 [03:20<02:14,  1.51s/it]Running Inference:  56%|█████▌    | 112/200 [03:21<01:57,  1.33s/it]Running Inference:  56%|█████▋    | 113/200 [03:22<01:48,  1.25s/it]Running Inference:  57%|█████▋    | 114/200 [03:25<02:28,  1.73s/it]Running Inference:  57%|█████▊    | 115/200 [03:27<02:33,  1.80s/it]Running Inference:  58%|█████▊    | 116/200 [03:30<02:57,  2.11s/it]Running Inference:  58%|█████▊    | 117/200 [03:34<03:43,  2.70s/it]Running Inference:  59%|█████▉    | 118/200 [03:34<02:50,  2.08s/it]Running Inference:  60%|█████▉    | 119/200 [03:38<03:15,  2.41s/it]Running Inference:  60%|██████    | 120/200 [03:38<02:35,  1.94s/it]Running Inference:  60%|██████    | 121/200 [03:39<02:07,  1.61s/it]Running Inference:  61%|██████    | 122/200 [03:40<01:45,  1.36s/it]Running Inference:  62%|██████▏   | 123/200 [03:41<01:32,  1.20s/it]Running Inference:  62%|██████▏   | 124/200 [03:42<01:24,  1.11s/it]Running Inference:  62%|██████▎   | 125/200 [03:45<02:03,  1.64s/it]Running Inference:  63%|██████▎   | 126/200 [03:47<02:25,  1.97s/it]Running Inference:  64%|██████▎   | 127/200 [03:50<02:35,  2.13s/it]Running Inference:  64%|██████▍   | 128/200 [03:51<02:04,  1.72s/it]Running Inference:  64%|██████▍   | 129/200 [03:52<01:57,  1.66s/it]Running Inference:  65%|██████▌   | 130/200 [03:53<01:38,  1.41s/it]Running Inference:  66%|██████▌   | 131/200 [03:56<02:08,  1.87s/it]Running Inference:  66%|██████▌   | 132/200 [03:58<02:18,  2.03s/it]Running Inference:  66%|██████▋   | 133/200 [04:00<02:09,  1.93s/it]Running Inference:  67%|██████▋   | 134/200 [04:02<02:03,  1.87s/it]Running Inference:  68%|██████▊   | 135/200 [04:06<02:49,  2.60s/it]Running Inference:  68%|██████▊   | 136/200 [04:09<02:48,  2.64s/it]Running Inference:  68%|██████▊   | 137/200 [04:12<02:51,  2.72s/it]Running Inference:  69%|██████▉   | 138/200 [04:13<02:16,  2.20s/it]Running Inference:  70%|██████▉   | 139/200 [04:15<02:09,  2.12s/it]Running Inference:  70%|███████   | 140/200 [04:16<01:57,  1.96s/it]Running Inference:  70%|███████   | 141/200 [04:20<02:27,  2.50s/it]Running Inference:  71%|███████   | 142/200 [04:23<02:33,  2.64s/it]Running Inference:  72%|███████▏  | 143/200 [04:24<01:57,  2.06s/it]Running Inference:  72%|███████▏  | 144/200 [04:27<02:09,  2.32s/it]Running Inference:  72%|███████▎  | 145/200 [04:28<01:59,  2.17s/it]Running Inference:  73%|███████▎  | 146/200 [04:29<01:38,  1.82s/it]Running Inference:  74%|███████▎  | 147/200 [04:32<01:52,  2.13s/it]Running Inference:  74%|███████▍  | 148/200 [04:33<01:32,  1.78s/it]Running Inference:  74%|███████▍  | 149/200 [04:34<01:19,  1.56s/it]Running Inference:  75%|███████▌  | 150/200 [04:36<01:15,  1.51s/it]Running Inference:  76%|███████▌  | 151/200 [04:37<01:10,  1.45s/it]Running Inference:  76%|███████▌  | 152/200 [04:40<01:27,  1.82s/it]Running Inference:  76%|███████▋  | 153/200 [04:42<01:38,  2.09s/it]Running Inference:  77%|███████▋  | 154/200 [04:45<01:46,  2.31s/it]Running Inference:  78%|███████▊  | 155/200 [04:48<01:51,  2.47s/it]Running Inference:  78%|███████▊  | 156/200 [04:50<01:42,  2.32s/it]Running Inference:  78%|███████▊  | 157/200 [04:50<01:15,  1.76s/it]Running Inference:  79%|███████▉  | 158/200 [04:53<01:27,  2.08s/it]Running Inference:  80%|███████▉  | 159/200 [04:56<01:30,  2.21s/it]Running Inference:  80%|████████  | 160/200 [04:59<01:46,  2.65s/it]Running Inference:  80%|████████  | 161/200 [05:01<01:29,  2.30s/it]Running Inference:  81%|████████  | 162/200 [05:02<01:10,  1.85s/it]Running Inference:  82%|████████▏ | 163/200 [05:03<00:56,  1.53s/it]Running Inference:  82%|████████▏ | 164/200 [05:04<00:50,  1.42s/it]Running Inference:  82%|████████▎ | 165/200 [05:04<00:42,  1.23s/it]Running Inference:  83%|████████▎ | 166/200 [05:07<00:57,  1.68s/it]Running Inference:  84%|████████▎ | 167/200 [05:09<00:58,  1.76s/it]Running Inference:  84%|████████▍ | 168/200 [05:10<00:44,  1.38s/it]Running Inference:  84%|████████▍ | 169/200 [05:10<00:33,  1.07s/it]Running Inference:  85%|████████▌ | 170/200 [05:13<00:49,  1.64s/it]Running Inference:  86%|████████▌ | 171/200 [05:14<00:41,  1.43s/it]Running Inference:  86%|████████▌ | 172/200 [05:15<00:36,  1.31s/it]Running Inference:  86%|████████▋ | 173/200 [05:16<00:33,  1.23s/it]Running Inference:  87%|████████▋ | 174/200 [05:19<00:44,  1.71s/it]Running Inference:  88%|████████▊ | 175/200 [05:21<00:44,  1.78s/it]Running Inference:  88%|████████▊ | 176/200 [05:22<00:36,  1.52s/it]Running Inference:  88%|████████▊ | 177/200 [05:24<00:42,  1.87s/it]Running Inference:  89%|████████▉ | 178/200 [05:26<00:42,  1.92s/it]Running Inference:  90%|████████▉ | 179/200 [05:30<00:52,  2.49s/it]Running Inference:  90%|█████████ | 180/200 [05:32<00:43,  2.16s/it]Running Inference:  90%|█████████ | 181/200 [05:32<00:31,  1.68s/it]Running Inference:  91%|█████████ | 182/200 [05:34<00:30,  1.71s/it]Running Inference:  92%|█████████▏| 183/200 [05:35<00:28,  1.66s/it]Running Inference:  92%|█████████▏| 184/200 [05:36<00:22,  1.39s/it]Running Inference:  92%|█████████▎| 185/200 [05:39<00:25,  1.68s/it]Running Inference:  93%|█████████▎| 186/200 [05:39<00:19,  1.38s/it]Running Inference:  94%|█████████▎| 187/200 [05:40<00:16,  1.30s/it]Running Inference:  94%|█████████▍| 188/200 [05:43<00:21,  1.75s/it]Running Inference:  94%|█████████▍| 189/200 [05:44<00:16,  1.49s/it]Running Inference:  95%|█████████▌| 190/200 [05:44<00:11,  1.14s/it]Running Inference:  96%|█████████▌| 191/200 [05:48<00:16,  1.79s/it]Running Inference:  96%|█████████▌| 192/200 [05:51<00:17,  2.16s/it]Running Inference:  96%|█████████▋| 193/200 [05:54<00:18,  2.64s/it]Running Inference:  97%|█████████▋| 194/200 [05:57<00:15,  2.63s/it]Running Inference:  98%|█████████▊| 195/200 [06:00<00:13,  2.79s/it]Running Inference:  98%|█████████▊| 196/200 [06:01<00:09,  2.30s/it]Running Inference:  98%|█████████▊| 197/200 [06:04<00:07,  2.54s/it]Running Inference:  99%|█████████▉| 198/200 [06:05<00:03,  1.94s/it]Running Inference: 100%|█████████▉| 199/200 [06:06<00:01,  1.60s/it]Running Inference: 100%|██████████| 200/200 [06:06<00:00,  1.30s/it]Running Inference: 100%|██████████| 200/200 [06:06<00:00,  1.83s/it]
2025-12-13 18:00:45,789 - INFO - Inference completed.
2025-12-13 18:00:45,799 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 18:00:45,799 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:00:45,803 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 18:00:45,803 - INFO - Metrics:
22.37
2025-12-13 18:00:45,805 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=2wikimqa, ratio=0.1) on GPU 4

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:00:52,174 - INFO - Set deterministic seeds to 42
2025-12-13 18:00:52,174 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:00:52,174 - INFO - Starting evaluation run...
2025-12-13 18:00:52,174 - INFO - Output directory set to: longbenchresult
2025-12-13 18:00:52,175 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 18:00:52,175 - INFO - KV Press 'knorm' setup.
2025-12-13 18:00:52,175 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:00:52,175 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.16it/s]
Device set to use cuda:0
2025-12-13 18:01:05,082 - INFO - Model pipeline loaded.
2025-12-13 18:01:05,083 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:01:09,521 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:01:09,521 - INFO - Dataset processed with 200 entries.
2025-12-13 18:01:09,540 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<13:58,  4.22s/it]Running Inference:   1%|          | 2/200 [00:05<07:28,  2.27s/it]Running Inference:   2%|▏         | 3/200 [00:06<05:46,  1.76s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:10,  1.89s/it]Running Inference:   2%|▎         | 5/200 [00:12<08:48,  2.71s/it]Running Inference:   3%|▎         | 6/200 [00:14<07:29,  2.32s/it]Running Inference:   4%|▎         | 7/200 [00:17<08:14,  2.56s/it]Running Inference:   4%|▍         | 8/200 [00:20<08:47,  2.75s/it]Running Inference:   4%|▍         | 9/200 [00:21<07:12,  2.27s/it]Running Inference:   5%|▌         | 10/200 [00:22<05:33,  1.75s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:25<07:12,  2.29s/it]Running Inference:   6%|▌         | 12/200 [00:27<06:35,  2.10s/it]Running Inference:   6%|▋         | 13/200 [00:30<07:51,  2.52s/it]Running Inference:   7%|▋         | 14/200 [00:31<05:57,  1.92s/it]Running Inference:   8%|▊         | 15/200 [00:32<05:02,  1.64s/it]Running Inference:   8%|▊         | 16/200 [00:33<04:24,  1.44s/it]Running Inference:   8%|▊         | 17/200 [00:34<04:26,  1.46s/it]Running Inference:   9%|▉         | 18/200 [00:35<03:58,  1.31s/it]Running Inference:  10%|▉         | 19/200 [00:36<03:23,  1.12s/it]Running Inference:  10%|█         | 20/200 [00:36<02:41,  1.11it/s]Running Inference:  10%|█         | 21/200 [00:38<03:43,  1.25s/it]Running Inference:  11%|█         | 22/200 [00:39<03:19,  1.12s/it]Running Inference:  12%|█▏        | 23/200 [00:41<03:34,  1.21s/it]Running Inference:  12%|█▏        | 24/200 [00:42<03:18,  1.13s/it]Running Inference:  12%|█▎        | 25/200 [00:43<03:15,  1.12s/it]Running Inference:  13%|█▎        | 26/200 [00:45<04:45,  1.64s/it]Running Inference:  14%|█▎        | 27/200 [00:46<04:09,  1.44s/it]Running Inference:  14%|█▍        | 28/200 [00:49<05:09,  1.80s/it]Running Inference:  14%|█▍        | 29/200 [00:50<04:15,  1.49s/it]Running Inference:  15%|█▌        | 30/200 [00:51<03:58,  1.40s/it]Running Inference:  16%|█▌        | 31/200 [00:52<03:40,  1.31s/it]Running Inference:  16%|█▌        | 32/200 [00:55<05:02,  1.80s/it]Running Inference:  16%|█▋        | 33/200 [00:58<05:50,  2.10s/it]Running Inference:  17%|█▋        | 34/200 [00:59<04:56,  1.79s/it]Running Inference:  18%|█▊        | 35/200 [01:02<05:48,  2.11s/it]Running Inference:  18%|█▊        | 36/200 [01:03<04:46,  1.74s/it]Running Inference:  18%|█▊        | 37/200 [01:06<05:47,  2.13s/it]Running Inference:  19%|█▉        | 38/200 [01:07<05:08,  1.90s/it]Running Inference:  20%|█▉        | 39/200 [01:08<04:12,  1.57s/it]Running Inference:  20%|██        | 40/200 [01:09<03:51,  1.45s/it]Running Inference:  20%|██        | 41/200 [01:12<04:56,  1.87s/it]Running Inference:  21%|██        | 42/200 [01:13<04:37,  1.76s/it]Running Inference:  22%|██▏       | 43/200 [01:14<03:54,  1.49s/it]Running Inference:  22%|██▏       | 44/200 [01:15<03:14,  1.24s/it]Running Inference:  22%|██▎       | 45/200 [01:16<02:50,  1.10s/it]Running Inference:  23%|██▎       | 46/200 [01:19<04:14,  1.65s/it]Running Inference:  24%|██▎       | 47/200 [01:20<03:40,  1.44s/it]Running Inference:  24%|██▍       | 48/200 [01:21<03:29,  1.38s/it]Running Inference:  24%|██▍       | 49/200 [01:24<04:42,  1.87s/it]Running Inference:  25%|██▌       | 50/200 [01:26<04:32,  1.82s/it]Running Inference:  26%|██▌       | 51/200 [01:28<05:02,  2.03s/it]Running Inference:  26%|██▌       | 52/200 [01:30<05:15,  2.13s/it]Running Inference:  26%|██▋       | 53/200 [01:32<04:42,  1.92s/it]Running Inference:  27%|██▋       | 54/200 [01:35<05:19,  2.19s/it]Running Inference:  28%|██▊       | 55/200 [01:36<04:19,  1.79s/it]Running Inference:  28%|██▊       | 56/200 [01:37<03:54,  1.63s/it]Running Inference:  28%|██▊       | 57/200 [01:38<03:23,  1.42s/it]Running Inference:  29%|██▉       | 58/200 [01:41<04:31,  1.91s/it]Running Inference:  30%|██▉       | 59/200 [01:44<05:02,  2.15s/it]Running Inference:  30%|███       | 60/200 [01:46<05:34,  2.39s/it]Running Inference:  30%|███       | 61/200 [01:47<04:19,  1.87s/it]Running Inference:  31%|███       | 62/200 [01:49<04:06,  1.79s/it]Running Inference:  32%|███▏      | 63/200 [01:50<03:29,  1.53s/it]Running Inference:  32%|███▏      | 64/200 [01:52<03:46,  1.67s/it]Running Inference:  32%|███▎      | 65/200 [01:52<02:56,  1.30s/it]Running Inference:  33%|███▎      | 66/200 [01:55<04:13,  1.89s/it]Running Inference:  34%|███▎      | 67/200 [01:57<03:46,  1.70s/it]Running Inference:  34%|███▍      | 68/200 [02:00<04:48,  2.18s/it]Running Inference:  34%|███▍      | 69/200 [02:01<04:08,  1.90s/it]Running Inference:  35%|███▌      | 70/200 [02:02<03:32,  1.63s/it]Running Inference:  36%|███▌      | 71/200 [02:03<02:52,  1.33s/it]Running Inference:  36%|███▌      | 72/200 [02:04<02:45,  1.29s/it]Running Inference:  36%|███▋      | 73/200 [02:05<02:34,  1.22s/it]Running Inference:  37%|███▋      | 74/200 [02:07<03:07,  1.49s/it]Running Inference:  38%|███▊      | 75/200 [02:08<02:57,  1.42s/it]Running Inference:  38%|███▊      | 76/200 [02:10<02:53,  1.40s/it]Running Inference:  38%|███▊      | 77/200 [02:11<02:40,  1.30s/it]Running Inference:  39%|███▉      | 78/200 [02:12<02:35,  1.27s/it]Running Inference:  40%|███▉      | 79/200 [02:13<02:25,  1.20s/it]Running Inference:  40%|████      | 80/200 [02:17<03:45,  1.88s/it]Running Inference:  40%|████      | 81/200 [02:19<04:15,  2.14s/it]Running Inference:  41%|████      | 82/200 [02:22<04:43,  2.40s/it]Running Inference:  42%|████▏     | 83/200 [02:24<04:09,  2.13s/it]Running Inference:  42%|████▏     | 84/200 [02:25<03:25,  1.77s/it]Running Inference:  42%|████▎     | 85/200 [02:28<04:05,  2.13s/it]Running Inference:  43%|████▎     | 86/200 [02:30<03:59,  2.10s/it]Running Inference:  44%|████▎     | 87/200 [02:31<03:19,  1.77s/it]Running Inference:  44%|████▍     | 88/200 [02:32<03:02,  1.63s/it]Running Inference:  44%|████▍     | 89/200 [02:35<03:39,  1.98s/it]Running Inference:  45%|████▌     | 90/200 [02:38<04:11,  2.29s/it]Running Inference:  46%|████▌     | 91/200 [02:41<04:22,  2.41s/it]Running Inference:  46%|████▌     | 92/200 [02:43<04:37,  2.57s/it]Running Inference:  46%|████▋     | 93/200 [02:45<04:15,  2.39s/it]Running Inference:  47%|████▋     | 94/200 [02:46<03:12,  1.81s/it]Running Inference:  48%|████▊     | 95/200 [02:47<02:35,  1.48s/it]Running Inference:  48%|████▊     | 96/200 [02:48<02:18,  1.33s/it]Running Inference:  48%|████▊     | 97/200 [02:51<03:16,  1.91s/it]Running Inference:  49%|████▉     | 98/200 [02:52<02:48,  1.65s/it]Running Inference:  50%|████▉     | 99/200 [02:55<03:19,  1.98s/it]Running Inference:  50%|█████     | 100/200 [02:56<03:06,  1.87s/it]Running Inference:  50%|█████     | 101/200 [02:59<03:29,  2.12s/it]Running Inference:  51%|█████     | 102/200 [03:00<02:49,  1.73s/it]Running Inference:  52%|█████▏    | 103/200 [03:02<03:15,  2.02s/it]Running Inference:  52%|█████▏    | 104/200 [03:03<02:39,  1.66s/it]Running Inference:  52%|█████▎    | 105/200 [03:06<03:12,  2.03s/it]Running Inference:  53%|█████▎    | 106/200 [03:07<02:40,  1.70s/it]Running Inference:  54%|█████▎    | 107/200 [03:11<03:28,  2.24s/it]Running Inference:  54%|█████▍    | 108/200 [03:13<03:42,  2.42s/it]Running Inference:  55%|█████▍    | 109/200 [03:14<02:58,  1.96s/it]Running Inference:  55%|█████▌    | 110/200 [03:16<02:45,  1.84s/it]Running Inference:  56%|█████▌    | 111/200 [03:17<02:12,  1.49s/it]Running Inference:  56%|█████▌    | 112/200 [03:19<02:48,  1.91s/it]Running Inference:  56%|█████▋    | 113/200 [03:21<02:23,  1.65s/it]Running Inference:  57%|█████▋    | 114/200 [03:23<02:51,  1.99s/it]Running Inference:  57%|█████▊    | 115/200 [03:27<03:39,  2.58s/it]Running Inference:  58%|█████▊    | 116/200 [03:29<03:19,  2.38s/it]Running Inference:  58%|█████▊    | 117/200 [03:33<03:57,  2.86s/it]Running Inference:  59%|█████▉    | 118/200 [03:34<03:00,  2.20s/it]Running Inference:  60%|█████▉    | 119/200 [03:37<03:20,  2.48s/it]Running Inference:  60%|██████    | 120/200 [03:38<02:38,  1.99s/it]Running Inference:  60%|██████    | 121/200 [03:41<02:58,  2.26s/it]Running Inference:  61%|██████    | 122/200 [03:42<02:27,  1.89s/it]Running Inference:  62%|██████▏   | 123/200 [03:43<02:01,  1.58s/it]Running Inference:  62%|██████▏   | 124/200 [03:43<01:44,  1.38s/it]Running Inference:  62%|██████▎   | 125/200 [03:46<02:15,  1.81s/it]Running Inference:  63%|██████▎   | 126/200 [03:47<01:54,  1.55s/it]Running Inference:  64%|██████▎   | 127/200 [03:50<02:12,  1.81s/it]Running Inference:  64%|██████▍   | 128/200 [03:52<02:29,  2.08s/it]Running Inference:  64%|██████▍   | 129/200 [03:54<02:15,  1.90s/it]Running Inference:  65%|██████▌   | 130/200 [03:55<01:50,  1.58s/it]Running Inference:  66%|██████▌   | 131/200 [03:56<01:39,  1.44s/it]Running Inference:  66%|██████▌   | 132/200 [03:58<01:56,  1.71s/it]Running Inference:  66%|██████▋   | 133/200 [04:02<02:30,  2.25s/it]Running Inference:  67%|██████▋   | 134/200 [04:03<02:17,  2.08s/it]Running Inference:  68%|██████▊   | 135/200 [04:08<02:57,  2.73s/it]Running Inference:  68%|██████▊   | 136/200 [04:10<02:53,  2.72s/it]Running Inference:  68%|██████▊   | 137/200 [04:13<02:53,  2.76s/it]Running Inference:  69%|██████▉   | 138/200 [04:14<02:17,  2.22s/it]Running Inference:  70%|██████▉   | 139/200 [04:16<02:11,  2.15s/it]Running Inference:  70%|███████   | 140/200 [04:18<01:58,  1.98s/it]Running Inference:  70%|███████   | 141/200 [04:20<01:56,  1.97s/it]Running Inference:  71%|███████   | 142/200 [04:22<02:10,  2.25s/it]Running Inference:  72%|███████▏  | 143/200 [04:23<01:41,  1.78s/it]Running Inference:  72%|███████▏  | 144/200 [04:26<01:58,  2.11s/it]Running Inference:  72%|███████▎  | 145/200 [04:28<01:49,  1.99s/it]Running Inference:  73%|███████▎  | 146/200 [04:31<02:03,  2.29s/it]Running Inference:  74%|███████▎  | 147/200 [04:34<02:09,  2.44s/it]Running Inference:  74%|███████▍  | 148/200 [04:34<01:42,  1.97s/it]Running Inference:  74%|███████▍  | 149/200 [04:35<01:23,  1.64s/it]Running Inference:  75%|███████▌  | 150/200 [04:37<01:18,  1.56s/it]Running Inference:  76%|███████▌  | 151/200 [04:37<00:58,  1.20s/it]Running Inference:  76%|███████▌  | 152/200 [04:38<00:48,  1.01s/it]Running Inference:  76%|███████▋  | 153/200 [04:40<01:10,  1.51s/it]Running Inference:  77%|███████▋  | 154/200 [04:43<01:26,  1.89s/it]Running Inference:  78%|███████▊  | 155/200 [04:44<01:16,  1.70s/it]Running Inference:  78%|███████▊  | 156/200 [04:46<01:18,  1.78s/it]Running Inference:  78%|███████▊  | 157/200 [04:47<00:59,  1.39s/it]Running Inference:  79%|███████▉  | 158/200 [04:49<01:15,  1.80s/it]Running Inference:  80%|███████▉  | 159/200 [04:52<01:19,  1.94s/it]Running Inference:  80%|████████  | 160/200 [04:55<01:38,  2.45s/it]Running Inference:  80%|████████  | 161/200 [04:57<01:23,  2.15s/it]Running Inference:  81%|████████  | 162/200 [04:58<01:06,  1.75s/it]Running Inference:  82%|████████▏ | 163/200 [04:58<00:53,  1.45s/it]Running Inference:  82%|████████▏ | 164/200 [05:01<01:05,  1.82s/it]Running Inference:  82%|████████▎ | 165/200 [05:04<01:14,  2.12s/it]Running Inference:  83%|████████▎ | 166/200 [05:06<01:13,  2.17s/it]Running Inference:  84%|████████▎ | 167/200 [05:08<01:09,  2.10s/it]Running Inference:  84%|████████▍ | 168/200 [05:09<00:52,  1.64s/it]Running Inference:  84%|████████▍ | 169/200 [05:10<00:47,  1.53s/it]Running Inference:  85%|████████▌ | 170/200 [05:12<00:50,  1.67s/it]Running Inference:  86%|████████▌ | 171/200 [05:13<00:41,  1.44s/it]Running Inference:  86%|████████▌ | 172/200 [05:14<00:36,  1.31s/it]Running Inference:  86%|████████▋ | 173/200 [05:15<00:32,  1.22s/it]Running Inference:  87%|████████▋ | 174/200 [05:16<00:29,  1.14s/it]Running Inference:  88%|████████▊ | 175/200 [05:18<00:35,  1.43s/it]Running Inference:  88%|████████▊ | 176/200 [05:19<00:30,  1.28s/it]Running Inference:  88%|████████▊ | 177/200 [05:20<00:25,  1.10s/it]Running Inference:  89%|████████▉ | 178/200 [05:22<00:31,  1.42s/it]Running Inference:  90%|████████▉ | 179/200 [05:28<01:00,  2.87s/it]Running Inference:  90%|█████████ | 180/200 [05:29<00:48,  2.41s/it]Running Inference:  90%|█████████ | 181/200 [05:30<00:35,  1.85s/it]Running Inference:  91%|█████████ | 182/200 [05:34<00:43,  2.42s/it]Running Inference:  92%|█████████▏| 183/200 [05:35<00:36,  2.15s/it]Running Inference:  92%|█████████▏| 184/200 [05:38<00:36,  2.31s/it]Running Inference:  92%|█████████▎| 185/200 [05:40<00:34,  2.32s/it]Running Inference:  93%|█████████▎| 186/200 [05:41<00:25,  1.83s/it]Running Inference:  94%|█████████▎| 187/200 [05:42<00:20,  1.61s/it]Running Inference:  94%|█████████▍| 188/200 [05:43<00:16,  1.34s/it]Running Inference:  94%|█████████▍| 189/200 [05:43<00:12,  1.16s/it]Running Inference:  95%|█████████▌| 190/200 [05:44<00:09,  1.10it/s]Running Inference:  96%|█████████▌| 191/200 [05:46<00:11,  1.25s/it]Running Inference:  96%|█████████▌| 192/200 [05:49<00:14,  1.77s/it]Running Inference:  96%|█████████▋| 193/200 [05:52<00:16,  2.35s/it]Running Inference:  97%|█████████▋| 194/200 [05:55<00:14,  2.40s/it]Running Inference:  98%|█████████▊| 195/200 [05:58<00:13,  2.61s/it]Running Inference:  98%|█████████▊| 196/200 [05:59<00:08,  2.16s/it]Running Inference:  98%|█████████▊| 197/200 [06:00<00:05,  1.86s/it]Running Inference:  99%|█████████▉| 198/200 [06:01<00:02,  1.45s/it]Running Inference: 100%|█████████▉| 199/200 [06:02<00:01,  1.27s/it]Running Inference: 100%|██████████| 200/200 [06:02<00:00,  1.07s/it]Running Inference: 100%|██████████| 200/200 [06:02<00:00,  1.81s/it]
2025-12-13 18:07:12,314 - INFO - Inference completed.
2025-12-13 18:07:12,322 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-13 18:07:12,322 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:07:12,326 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-13 18:07:12,326 - INFO - Metrics:
18.16
2025-12-13 18:07:12,328 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=2wikimqa, ratio=0.2) on GPU 4

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:07:18,672 - INFO - Set deterministic seeds to 42
2025-12-13 18:07:18,672 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:07:18,672 - INFO - Starting evaluation run...
2025-12-13 18:07:18,672 - INFO - Output directory set to: longbenchresult
2025-12-13 18:07:18,673 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-13 18:07:18,673 - INFO - KV Press 'knorm' setup.
2025-12-13 18:07:18,673 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:07:18,673 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.75it/s]
Device set to use cuda:0
2025-12-13 18:07:31,327 - INFO - Model pipeline loaded.
2025-12-13 18:07:31,327 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:07:37,251 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:07:37,251 - INFO - Dataset processed with 200 entries.
2025-12-13 18:07:37,271 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:17,  4.31s/it]Running Inference:   1%|          | 2/200 [00:07<11:39,  3.53s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:02,  2.45s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:32,  2.31s/it]Running Inference:   2%|▎         | 5/200 [00:13<08:33,  2.63s/it]Running Inference:   3%|▎         | 6/200 [00:15<07:20,  2.27s/it]Running Inference:   4%|▎         | 7/200 [00:18<08:16,  2.57s/it]Running Inference:   4%|▍         | 8/200 [00:21<08:56,  2.80s/it]Running Inference:   4%|▍         | 9/200 [00:25<09:29,  2.98s/it]Running Inference:   5%|▌         | 10/200 [00:25<07:05,  2.24s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:29<08:25,  2.67s/it]Running Inference:   6%|▌         | 12/200 [00:30<07:11,  2.29s/it]Running Inference:   6%|▋         | 13/200 [00:32<06:54,  2.22s/it]Running Inference:   7%|▋         | 14/200 [00:33<05:19,  1.72s/it]Running Inference:   8%|▊         | 15/200 [00:34<04:35,  1.49s/it]Running Inference:   8%|▊         | 16/200 [00:35<04:06,  1.34s/it]Running Inference:   8%|▊         | 17/200 [00:39<06:16,  2.06s/it]Running Inference:   9%|▉         | 18/200 [00:40<05:14,  1.73s/it]Running Inference:  10%|▉         | 19/200 [00:42<06:14,  2.07s/it]Running Inference:  10%|█         | 20/200 [00:43<04:41,  1.56s/it]Running Inference:  10%|█         | 21/200 [00:45<05:09,  1.73s/it]Running Inference:  11%|█         | 22/200 [00:46<04:19,  1.46s/it]Running Inference:  12%|█▏        | 23/200 [00:47<04:13,  1.43s/it]Running Inference:  12%|█▏        | 24/200 [00:48<03:35,  1.22s/it]Running Inference:  12%|█▎        | 25/200 [00:51<05:20,  1.83s/it]Running Inference:  13%|█▎        | 26/200 [00:52<04:30,  1.55s/it]Running Inference:  14%|█▎        | 27/200 [00:53<03:59,  1.39s/it]Running Inference:  14%|█▍        | 28/200 [00:56<05:08,  1.79s/it]Running Inference:  14%|█▍        | 29/200 [00:57<04:12,  1.47s/it]Running Inference:  15%|█▌        | 30/200 [00:59<04:59,  1.76s/it]Running Inference:  16%|█▌        | 31/200 [01:00<04:22,  1.56s/it]Running Inference:  16%|█▌        | 32/200 [01:01<03:45,  1.34s/it]Running Inference:  16%|█▋        | 33/200 [01:04<05:02,  1.81s/it]Running Inference:  17%|█▋        | 34/200 [01:07<05:54,  2.14s/it]Running Inference:  18%|█▊        | 35/200 [01:10<06:34,  2.39s/it]Running Inference:  18%|█▊        | 36/200 [01:11<05:20,  1.96s/it]Running Inference:  18%|█▊        | 37/200 [01:14<06:17,  2.32s/it]Running Inference:  19%|█▉        | 38/200 [01:15<05:29,  2.03s/it]Running Inference:  20%|█▉        | 39/200 [01:16<04:47,  1.78s/it]Running Inference:  20%|██        | 40/200 [01:19<05:43,  2.15s/it]Running Inference:  20%|██        | 41/200 [01:22<06:20,  2.39s/it]Running Inference:  21%|██        | 42/200 [01:24<05:37,  2.14s/it]Running Inference:  22%|██▏       | 43/200 [01:25<04:37,  1.76s/it]Running Inference:  22%|██▏       | 44/200 [01:25<03:44,  1.44s/it]Running Inference:  22%|██▎       | 45/200 [01:26<03:11,  1.24s/it]Running Inference:  23%|██▎       | 46/200 [01:29<04:34,  1.78s/it]Running Inference:  24%|██▎       | 47/200 [01:31<04:21,  1.71s/it]Running Inference:  24%|██▍       | 48/200 [01:34<05:17,  2.09s/it]Running Inference:  24%|██▍       | 49/200 [01:37<06:03,  2.41s/it]Running Inference:  25%|██▌       | 50/200 [01:39<05:31,  2.21s/it]Running Inference:  26%|██▌       | 51/200 [01:43<06:46,  2.73s/it]Running Inference:  26%|██▌       | 52/200 [01:43<05:03,  2.05s/it]Running Inference:  26%|██▋       | 53/200 [01:46<05:32,  2.26s/it]Running Inference:  27%|██▋       | 54/200 [01:49<06:02,  2.48s/it]Running Inference:  28%|██▊       | 55/200 [01:50<04:51,  2.01s/it]Running Inference:  28%|██▊       | 56/200 [01:51<04:16,  1.78s/it]Running Inference:  28%|██▊       | 57/200 [01:52<03:39,  1.54s/it]Running Inference:  29%|██▉       | 58/200 [01:53<03:34,  1.51s/it]Running Inference:  30%|██▉       | 59/200 [01:54<03:00,  1.28s/it]Running Inference:  30%|███       | 60/200 [01:57<04:15,  1.82s/it]Running Inference:  30%|███       | 61/200 [01:58<03:24,  1.47s/it]Running Inference:  31%|███       | 62/200 [02:01<04:34,  1.99s/it]Running Inference:  32%|███▏      | 63/200 [02:02<03:56,  1.72s/it]Running Inference:  32%|███▏      | 64/200 [02:04<04:11,  1.85s/it]Running Inference:  32%|███▎      | 65/200 [02:05<03:39,  1.62s/it]Running Inference:  33%|███▎      | 66/200 [02:09<04:47,  2.15s/it]Running Inference:  34%|███▎      | 67/200 [02:10<04:11,  1.89s/it]Running Inference:  34%|███▍      | 68/200 [02:13<05:10,  2.35s/it]Running Inference:  34%|███▍      | 69/200 [02:16<05:25,  2.49s/it]Running Inference:  35%|███▌      | 70/200 [02:17<04:26,  2.05s/it]Running Inference:  36%|███▌      | 71/200 [02:18<03:29,  1.63s/it]Running Inference:  36%|███▌      | 72/200 [02:19<03:09,  1.48s/it]Running Inference:  36%|███▋      | 73/200 [02:22<04:06,  1.94s/it]Running Inference:  37%|███▋      | 74/200 [02:24<04:11,  2.00s/it]Running Inference:  38%|███▊      | 75/200 [02:25<03:36,  1.73s/it]Running Inference:  38%|███▊      | 76/200 [02:26<03:13,  1.56s/it]Running Inference:  38%|███▊      | 77/200 [02:28<02:54,  1.42s/it]Running Inference:  39%|███▉      | 78/200 [02:29<02:45,  1.36s/it]Running Inference:  40%|███▉      | 79/200 [02:30<02:33,  1.27s/it]Running Inference:  40%|████      | 80/200 [02:33<03:54,  1.95s/it]Running Inference:  40%|████      | 81/200 [02:36<04:25,  2.23s/it]Running Inference:  41%|████      | 82/200 [02:39<04:54,  2.50s/it]Running Inference:  42%|████▏     | 83/200 [02:41<04:27,  2.29s/it]Running Inference:  42%|████▏     | 84/200 [02:42<03:39,  1.89s/it]Running Inference:  42%|████▎     | 85/200 [02:45<04:18,  2.25s/it]Running Inference:  43%|████▎     | 86/200 [02:47<04:09,  2.19s/it]Running Inference:  44%|████▎     | 87/200 [02:50<04:28,  2.38s/it]Running Inference:  44%|████▍     | 88/200 [02:52<04:00,  2.15s/it]Running Inference:  44%|████▍     | 89/200 [02:55<04:23,  2.38s/it]Running Inference:  45%|████▌     | 90/200 [02:58<04:45,  2.60s/it]Running Inference:  46%|████▌     | 91/200 [03:01<04:50,  2.66s/it]Running Inference:  46%|████▌     | 92/200 [03:04<05:00,  2.78s/it]Running Inference:  46%|████▋     | 93/200 [03:06<04:31,  2.54s/it]Running Inference:  47%|████▋     | 94/200 [03:06<03:31,  2.00s/it]Running Inference:  48%|████▊     | 95/200 [03:07<02:49,  1.62s/it]Running Inference:  48%|████▊     | 96/200 [03:08<02:27,  1.42s/it]Running Inference:  48%|████▊     | 97/200 [03:11<03:26,  2.00s/it]Running Inference:  49%|████▉     | 98/200 [03:12<02:55,  1.72s/it]Running Inference:  50%|████▉     | 99/200 [03:15<03:28,  2.07s/it]Running Inference:  50%|█████     | 100/200 [03:18<03:51,  2.31s/it]Running Inference:  50%|█████     | 101/200 [03:21<04:04,  2.47s/it]Running Inference:  51%|█████     | 102/200 [03:22<03:12,  1.96s/it]Running Inference:  52%|█████▏    | 103/200 [03:25<03:35,  2.22s/it]Running Inference:  52%|█████▏    | 104/200 [03:28<03:54,  2.45s/it]Running Inference:  52%|█████▎    | 105/200 [03:29<03:15,  2.05s/it]Running Inference:  53%|█████▎    | 106/200 [03:30<02:42,  1.73s/it]Running Inference:  54%|█████▎    | 107/200 [03:33<03:32,  2.29s/it]Running Inference:  54%|█████▍    | 108/200 [03:36<03:48,  2.49s/it]Running Inference:  55%|█████▍    | 109/200 [03:37<03:03,  2.01s/it]Running Inference:  55%|█████▌    | 110/200 [03:39<02:49,  1.88s/it]Running Inference:  56%|█████▌    | 111/200 [03:39<02:15,  1.52s/it]Running Inference:  56%|█████▌    | 112/200 [03:42<02:53,  1.97s/it]Running Inference:  56%|█████▋    | 113/200 [03:46<03:23,  2.34s/it]Running Inference:  57%|█████▋    | 114/200 [03:49<03:35,  2.51s/it]Running Inference:  57%|█████▊    | 115/200 [03:53<04:13,  2.98s/it]Running Inference:  58%|█████▊    | 116/200 [03:55<03:43,  2.66s/it]Running Inference:  58%|█████▊    | 117/200 [03:59<04:16,  3.09s/it]Running Inference:  59%|█████▉    | 118/200 [03:59<03:16,  2.40s/it]Running Inference:  60%|█████▉    | 119/200 [04:03<03:35,  2.65s/it]Running Inference:  60%|██████    | 120/200 [04:06<03:40,  2.76s/it]Running Inference:  60%|██████    | 121/200 [04:09<03:44,  2.84s/it]Running Inference:  61%|██████    | 122/200 [04:11<03:36,  2.78s/it]Running Inference:  62%|██████▏   | 123/200 [04:12<02:49,  2.20s/it]Running Inference:  62%|██████▏   | 124/200 [04:15<03:07,  2.47s/it]Running Inference:  62%|██████▎   | 125/200 [04:18<03:15,  2.61s/it]Running Inference:  63%|██████▎   | 126/200 [04:19<02:31,  2.05s/it]Running Inference:  64%|██████▎   | 127/200 [04:22<02:40,  2.20s/it]Running Inference:  64%|██████▍   | 128/200 [04:23<02:25,  2.02s/it]Running Inference:  64%|██████▍   | 129/200 [04:25<02:12,  1.86s/it]Running Inference:  65%|██████▌   | 130/200 [04:25<01:48,  1.55s/it]Running Inference:  66%|██████▌   | 131/200 [04:28<02:16,  1.98s/it]Running Inference:  66%|██████▌   | 132/200 [04:33<03:03,  2.70s/it]Running Inference:  66%|██████▋   | 133/200 [04:36<03:18,  2.97s/it]Running Inference:  67%|██████▋   | 134/200 [04:38<02:50,  2.59s/it]Running Inference:  68%|██████▊   | 135/200 [04:42<03:22,  3.12s/it]Running Inference:  68%|██████▊   | 136/200 [04:45<03:13,  3.02s/it]Running Inference:  68%|██████▊   | 137/200 [04:48<03:09,  3.01s/it]Running Inference:  69%|██████▉   | 138/200 [04:49<02:30,  2.43s/it]Running Inference:  70%|██████▉   | 139/200 [04:51<02:20,  2.30s/it]Running Inference:  70%|███████   | 140/200 [04:53<02:05,  2.09s/it]Running Inference:  70%|███████   | 141/200 [04:57<02:33,  2.61s/it]Running Inference:  71%|███████   | 142/200 [05:00<02:38,  2.73s/it]Running Inference:  72%|███████▏  | 143/200 [05:03<02:37,  2.76s/it]Running Inference:  72%|███████▏  | 144/200 [05:06<02:39,  2.86s/it]Running Inference:  72%|███████▎  | 145/200 [05:09<02:53,  3.16s/it]Running Inference:  73%|███████▎  | 146/200 [05:13<02:49,  3.15s/it]Running Inference:  74%|███████▎  | 147/200 [05:16<02:42,  3.07s/it]Running Inference:  74%|███████▍  | 148/200 [05:18<02:36,  3.01s/it]Running Inference:  74%|███████▍  | 149/200 [05:19<02:01,  2.38s/it]Running Inference:  75%|███████▌  | 150/200 [05:22<02:11,  2.62s/it]Running Inference:  76%|███████▌  | 151/200 [05:23<01:35,  1.95s/it]Running Inference:  76%|███████▌  | 152/200 [05:26<01:44,  2.18s/it]Running Inference:  76%|███████▋  | 153/200 [05:28<01:51,  2.37s/it]Running Inference:  77%|███████▋  | 154/200 [05:29<01:31,  1.98s/it]Running Inference:  78%|███████▊  | 155/200 [05:32<01:41,  2.25s/it]Running Inference:  78%|███████▊  | 156/200 [05:36<01:54,  2.61s/it]Running Inference:  78%|███████▊  | 157/200 [05:36<01:25,  1.98s/it]Running Inference:  79%|███████▉  | 158/200 [05:39<01:34,  2.25s/it]Running Inference:  80%|███████▉  | 159/200 [05:41<01:31,  2.24s/it]Running Inference:  80%|████████  | 160/200 [05:45<01:47,  2.69s/it]Running Inference:  80%|████████  | 161/200 [05:47<01:31,  2.36s/it]Running Inference:  81%|████████  | 162/200 [05:48<01:15,  1.99s/it]Running Inference:  82%|████████▏ | 163/200 [05:49<01:00,  1.63s/it]Running Inference:  82%|████████▏ | 164/200 [05:51<01:11,  1.98s/it]Running Inference:  82%|████████▎ | 165/200 [05:54<01:19,  2.27s/it]Running Inference:  83%|████████▎ | 166/200 [05:57<01:17,  2.28s/it]Running Inference:  84%|████████▎ | 167/200 [05:59<01:19,  2.41s/it]Running Inference:  84%|████████▍ | 168/200 [06:00<00:59,  1.87s/it]Running Inference:  84%|████████▍ | 169/200 [06:02<01:03,  2.06s/it]Running Inference:  85%|████████▌ | 170/200 [06:04<00:59,  1.97s/it]Running Inference:  86%|████████▌ | 171/200 [06:05<00:48,  1.67s/it]Running Inference:  86%|████████▌ | 172/200 [06:06<00:41,  1.48s/it]Running Inference:  86%|████████▋ | 173/200 [06:08<00:39,  1.46s/it]Running Inference:  87%|████████▋ | 174/200 [06:09<00:33,  1.30s/it]Running Inference:  88%|████████▊ | 175/200 [06:11<00:37,  1.49s/it]Running Inference:  88%|████████▊ | 176/200 [06:12<00:32,  1.34s/it]Running Inference:  88%|████████▊ | 177/200 [06:14<00:40,  1.77s/it]Running Inference:  89%|████████▉ | 178/200 [06:16<00:41,  1.89s/it]Running Inference:  90%|████████▉ | 179/200 [06:20<00:52,  2.48s/it]Running Inference:  90%|█████████ | 180/200 [06:22<00:42,  2.14s/it]Running Inference:  90%|█████████ | 181/200 [06:24<00:43,  2.28s/it]Running Inference:  91%|█████████ | 182/200 [06:28<00:49,  2.74s/it]Running Inference:  92%|█████████▏| 183/200 [06:30<00:40,  2.38s/it]Running Inference:  92%|█████████▏| 184/200 [06:32<00:40,  2.51s/it]Running Inference:  92%|█████████▎| 185/200 [06:35<00:37,  2.47s/it]Running Inference:  93%|█████████▎| 186/200 [06:36<00:27,  1.94s/it]Running Inference:  94%|█████████▎| 187/200 [06:37<00:21,  1.69s/it]Running Inference:  94%|█████████▍| 188/200 [06:37<00:17,  1.43s/it]Running Inference:  94%|█████████▍| 189/200 [06:40<00:20,  1.88s/it]Running Inference:  95%|█████████▌| 190/200 [06:41<00:14,  1.42s/it]Running Inference:  96%|█████████▌| 191/200 [06:45<00:19,  2.13s/it]Running Inference:  96%|█████████▌| 192/200 [06:48<00:19,  2.42s/it]Running Inference:  96%|█████████▋| 193/200 [06:51<00:19,  2.84s/it]Running Inference:  97%|█████████▋| 194/200 [06:54<00:16,  2.78s/it]Running Inference:  98%|█████████▊| 195/200 [06:57<00:14,  2.91s/it]Running Inference:  98%|█████████▊| 196/200 [06:58<00:09,  2.38s/it]Running Inference:  98%|█████████▊| 197/200 [07:00<00:06,  2.02s/it]Running Inference:  99%|█████████▉| 198/200 [07:00<00:03,  1.57s/it]Running Inference: 100%|█████████▉| 199/200 [07:01<00:01,  1.36s/it]Running Inference: 100%|██████████| 200/200 [07:02<00:00,  1.13s/it]Running Inference: 100%|██████████| 200/200 [07:02<00:00,  2.11s/it]
2025-12-13 18:14:39,348 - INFO - Inference completed.
2025-12-13 18:14:39,356 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-13 18:14:39,356 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:14:39,361 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-13 18:14:39,361 - INFO - Metrics:
16.36
2025-12-13 18:14:39,363 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=2wikimqa, ratio=0.3) on GPU 4

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:14:45,755 - INFO - Set deterministic seeds to 42
2025-12-13 18:14:45,755 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:14:45,755 - INFO - Starting evaluation run...
2025-12-13 18:14:45,755 - INFO - Output directory set to: longbenchresult
2025-12-13 18:14:45,755 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-13 18:14:45,755 - INFO - KV Press 'knorm' setup.
2025-12-13 18:14:45,755 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:14:45,755 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.94it/s]
Device set to use cuda:0
2025-12-13 18:14:58,964 - INFO - Model pipeline loaded.
2025-12-13 18:14:58,964 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:15:02,694 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:15:02,694 - INFO - Dataset processed with 200 entries.
2025-12-13 18:15:02,715 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:17,  4.31s/it]Running Inference:   1%|          | 2/200 [00:07<11:32,  3.50s/it]Running Inference:   2%|▏         | 3/200 [00:08<07:59,  2.43s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:30,  2.30s/it]Running Inference:   2%|▎         | 5/200 [00:12<07:12,  2.22s/it]Running Inference:   3%|▎         | 6/200 [00:14<06:21,  1.97s/it]Running Inference:   4%|▎         | 7/200 [00:17<07:30,  2.33s/it]Running Inference:   4%|▍         | 8/200 [00:20<08:19,  2.60s/it]Running Inference:   4%|▍         | 9/200 [00:23<08:58,  2.82s/it]Running Inference:   5%|▌         | 10/200 [00:24<06:39,  2.10s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:26<06:48,  2.16s/it]Running Inference:   6%|▌         | 12/200 [00:28<06:22,  2.04s/it]Running Inference:   6%|▋         | 13/200 [00:31<07:43,  2.48s/it]Running Inference:   7%|▋         | 14/200 [00:34<07:42,  2.49s/it]Running Inference:   8%|▊         | 15/200 [00:35<06:15,  2.03s/it]Running Inference:   8%|▊         | 16/200 [00:36<05:14,  1.71s/it]Running Inference:   8%|▊         | 17/200 [00:39<06:58,  2.29s/it]Running Inference:   9%|▉         | 18/200 [00:40<05:44,  1.89s/it]Running Inference:  10%|▉         | 19/200 [00:43<06:30,  2.16s/it]Running Inference:  10%|█         | 20/200 [00:43<04:51,  1.62s/it]Running Inference:  10%|█         | 21/200 [00:46<05:48,  1.95s/it]Running Inference:  11%|█         | 22/200 [00:49<06:32,  2.21s/it]Running Inference:  12%|█▏        | 23/200 [00:50<05:47,  1.96s/it]Running Inference:  12%|█▏        | 24/200 [00:53<06:30,  2.22s/it]Running Inference:  12%|█▎        | 25/200 [00:56<07:18,  2.50s/it]Running Inference:  13%|█▎        | 26/200 [00:59<07:35,  2.62s/it]Running Inference:  14%|█▎        | 27/200 [01:02<07:35,  2.64s/it]Running Inference:  14%|█▍        | 28/200 [01:04<07:34,  2.64s/it]Running Inference:  14%|█▍        | 29/200 [01:05<05:51,  2.05s/it]Running Inference:  15%|█▌        | 30/200 [01:07<06:03,  2.14s/it]Running Inference:  16%|█▌        | 31/200 [01:09<05:11,  1.84s/it]Running Inference:  16%|█▌        | 32/200 [01:12<06:06,  2.18s/it]Running Inference:  16%|█▋        | 33/200 [01:14<06:36,  2.37s/it]Running Inference:  17%|█▋        | 34/200 [01:17<06:55,  2.50s/it]Running Inference:  18%|█▊        | 35/200 [01:20<07:12,  2.62s/it]Running Inference:  18%|█▊        | 36/200 [01:21<05:44,  2.10s/it]Running Inference:  18%|█▊        | 37/200 [01:24<06:29,  2.39s/it]Running Inference:  19%|█▉        | 38/200 [01:27<07:12,  2.67s/it]Running Inference:  20%|█▉        | 39/200 [01:30<07:19,  2.73s/it]Running Inference:  20%|██        | 40/200 [01:33<07:25,  2.78s/it]Running Inference:  20%|██        | 41/200 [01:36<07:26,  2.81s/it]Running Inference:  21%|██        | 42/200 [01:38<06:32,  2.48s/it]Running Inference:  22%|██▏       | 43/200 [01:39<05:14,  2.00s/it]Running Inference:  22%|██▏       | 44/200 [01:39<04:09,  1.60s/it]Running Inference:  22%|██▎       | 45/200 [01:42<05:05,  1.97s/it]Running Inference:  23%|██▎       | 46/200 [01:45<05:49,  2.27s/it]Running Inference:  24%|██▎       | 47/200 [01:47<05:14,  2.05s/it]Running Inference:  24%|██▍       | 48/200 [01:48<04:40,  1.84s/it]Running Inference:  24%|██▍       | 49/200 [01:51<05:33,  2.21s/it]Running Inference:  25%|██▌       | 50/200 [01:53<05:10,  2.07s/it]Running Inference:  26%|██▌       | 51/200 [01:57<06:46,  2.73s/it]Running Inference:  26%|██▌       | 52/200 [02:00<06:29,  2.63s/it]Running Inference:  26%|██▋       | 53/200 [02:02<06:28,  2.65s/it]Running Inference:  27%|██▋       | 54/200 [02:05<06:35,  2.71s/it]Running Inference:  28%|██▊       | 55/200 [02:06<05:11,  2.15s/it]Running Inference:  28%|██▊       | 56/200 [02:07<04:30,  1.88s/it]Running Inference:  28%|██▊       | 57/200 [02:08<03:40,  1.54s/it]Running Inference:  29%|██▉       | 58/200 [02:11<04:46,  2.02s/it]Running Inference:  30%|██▉       | 59/200 [02:14<05:14,  2.23s/it]Running Inference:  30%|███       | 60/200 [02:15<04:25,  1.90s/it]Running Inference:  30%|███       | 61/200 [02:18<04:58,  2.14s/it]Running Inference:  31%|███       | 62/200 [02:21<05:34,  2.42s/it]Running Inference:  32%|███▏      | 63/200 [02:22<04:35,  2.01s/it]Running Inference:  32%|███▏      | 64/200 [02:24<04:34,  2.02s/it]Running Inference:  32%|███▎      | 65/200 [02:26<04:54,  2.18s/it]Running Inference:  33%|███▎      | 66/200 [02:30<05:35,  2.51s/it]Running Inference:  34%|███▎      | 67/200 [02:31<04:44,  2.14s/it]Running Inference:  34%|███▍      | 68/200 [02:33<04:33,  2.07s/it]Running Inference:  34%|███▍      | 69/200 [02:35<04:56,  2.26s/it]Running Inference:  35%|███▌      | 70/200 [02:38<05:20,  2.47s/it]Running Inference:  36%|███▌      | 71/200 [02:39<04:05,  1.91s/it]Running Inference:  36%|███▌      | 72/200 [02:40<03:34,  1.67s/it]Running Inference:  36%|███▋      | 73/200 [02:43<04:19,  2.04s/it]Running Inference:  37%|███▋      | 74/200 [02:45<04:21,  2.07s/it]Running Inference:  38%|███▊      | 75/200 [02:46<03:44,  1.79s/it]Running Inference:  38%|███▊      | 76/200 [02:48<03:25,  1.66s/it]Running Inference:  38%|███▊      | 77/200 [02:49<03:02,  1.48s/it]Running Inference:  39%|███▉      | 78/200 [02:50<02:50,  1.40s/it]Running Inference:  40%|███▉      | 79/200 [02:51<02:38,  1.31s/it]Running Inference:  40%|████      | 80/200 [02:55<03:54,  1.96s/it]Running Inference:  40%|████      | 81/200 [02:57<04:21,  2.20s/it]Running Inference:  41%|████      | 82/200 [03:00<04:47,  2.44s/it]Running Inference:  42%|████▏     | 83/200 [03:02<04:14,  2.17s/it]Running Inference:  42%|████▏     | 84/200 [03:03<03:33,  1.84s/it]Running Inference:  42%|████▎     | 85/200 [03:06<04:10,  2.18s/it]Running Inference:  43%|████▎     | 86/200 [03:08<04:02,  2.13s/it]Running Inference:  44%|████▎     | 87/200 [03:11<04:20,  2.31s/it]Running Inference:  44%|████▍     | 88/200 [03:12<03:31,  1.89s/it]Running Inference:  44%|████▍     | 89/200 [03:14<04:00,  2.16s/it]Running Inference:  45%|████▌     | 90/200 [03:17<04:25,  2.42s/it]Running Inference:  46%|████▌     | 91/200 [03:20<04:33,  2.51s/it]Running Inference:  46%|████▌     | 92/200 [03:23<04:48,  2.67s/it]Running Inference:  46%|████▋     | 93/200 [03:25<04:23,  2.47s/it]Running Inference:  47%|████▋     | 94/200 [03:26<03:23,  1.92s/it]Running Inference:  48%|████▊     | 95/200 [03:26<02:43,  1.56s/it]Running Inference:  48%|████▊     | 96/200 [03:27<02:23,  1.38s/it]Running Inference:  48%|████▊     | 97/200 [03:29<02:16,  1.33s/it]Running Inference:  49%|████▉     | 98/200 [03:30<02:04,  1.22s/it]Running Inference:  50%|████▉     | 99/200 [03:32<02:51,  1.70s/it]Running Inference:  50%|█████     | 100/200 [03:35<03:24,  2.05s/it]Running Inference:  50%|█████     | 101/200 [03:38<03:44,  2.27s/it]Running Inference:  51%|█████     | 102/200 [03:41<03:57,  2.43s/it]Running Inference:  52%|█████▏    | 103/200 [03:44<04:05,  2.53s/it]Running Inference:  52%|█████▏    | 104/200 [03:46<04:13,  2.64s/it]Running Inference:  52%|█████▎    | 105/200 [03:49<04:18,  2.72s/it]Running Inference:  53%|█████▎    | 106/200 [03:52<04:20,  2.77s/it]Running Inference:  54%|█████▎    | 107/200 [03:56<04:38,  2.99s/it]Running Inference:  54%|█████▍    | 108/200 [03:57<03:39,  2.38s/it]Running Inference:  55%|█████▍    | 109/200 [03:58<02:56,  1.93s/it]Running Inference:  55%|█████▌    | 110/200 [04:01<03:39,  2.44s/it]Running Inference:  56%|█████▌    | 111/200 [04:02<02:49,  1.91s/it]Running Inference:  56%|█████▌    | 112/200 [04:05<03:14,  2.21s/it]Running Inference:  56%|█████▋    | 113/200 [04:08<03:36,  2.49s/it]Running Inference:  57%|█████▋    | 114/200 [04:11<03:42,  2.59s/it]Running Inference:  57%|█████▊    | 115/200 [04:15<04:15,  3.00s/it]Running Inference:  58%|█████▊    | 116/200 [04:17<03:45,  2.69s/it]Running Inference:  58%|█████▊    | 117/200 [04:19<03:27,  2.50s/it]Running Inference:  59%|█████▉    | 118/200 [04:21<03:28,  2.54s/it]Running Inference:  60%|█████▉    | 119/200 [04:25<03:41,  2.73s/it]Running Inference:  60%|██████    | 120/200 [04:28<03:43,  2.79s/it]Running Inference:  60%|██████    | 121/200 [04:30<03:43,  2.83s/it]Running Inference:  61%|██████    | 122/200 [04:31<02:58,  2.29s/it]Running Inference:  62%|██████▏   | 123/200 [04:32<02:22,  1.86s/it]Running Inference:  62%|██████▏   | 124/200 [04:33<01:59,  1.57s/it]Running Inference:  62%|██████▎   | 125/200 [04:36<02:26,  1.96s/it]Running Inference:  63%|██████▎   | 126/200 [04:39<02:41,  2.18s/it]Running Inference:  64%|██████▎   | 127/200 [04:40<02:11,  1.81s/it]Running Inference:  64%|██████▍   | 128/200 [04:42<02:30,  2.09s/it]Running Inference:  64%|██████▍   | 129/200 [04:44<02:15,  1.91s/it]Running Inference:  65%|██████▌   | 130/200 [04:45<01:50,  1.58s/it]Running Inference:  66%|██████▌   | 131/200 [04:48<02:16,  1.98s/it]Running Inference:  66%|██████▌   | 132/200 [04:52<03:01,  2.67s/it]Running Inference:  66%|██████▋   | 133/200 [04:55<03:15,  2.92s/it]Running Inference:  67%|██████▋   | 134/200 [04:57<02:44,  2.50s/it]Running Inference:  68%|██████▊   | 135/200 [05:00<02:47,  2.57s/it]Running Inference:  68%|██████▊   | 136/200 [05:02<02:47,  2.62s/it]Running Inference:  68%|██████▊   | 137/200 [05:05<02:50,  2.70s/it]Running Inference:  69%|██████▉   | 138/200 [05:06<02:16,  2.21s/it]Running Inference:  70%|██████▉   | 139/200 [05:08<02:11,  2.16s/it]Running Inference:  70%|███████   | 140/200 [05:10<01:59,  1.99s/it]Running Inference:  70%|███████   | 141/200 [05:14<02:28,  2.51s/it]Running Inference:  71%|███████   | 142/200 [05:17<02:33,  2.64s/it]Running Inference:  72%|███████▏  | 143/200 [05:19<02:32,  2.67s/it]Running Inference:  72%|███████▏  | 144/200 [05:22<02:33,  2.75s/it]Running Inference:  72%|███████▎  | 145/200 [05:26<02:48,  3.05s/it]Running Inference:  73%|███████▎  | 146/200 [05:28<02:20,  2.60s/it]Running Inference:  74%|███████▎  | 147/200 [05:31<02:21,  2.67s/it]Running Inference:  74%|███████▍  | 148/200 [05:31<01:50,  2.13s/it]Running Inference:  74%|███████▍  | 149/200 [05:32<01:29,  1.75s/it]Running Inference:  75%|███████▌  | 150/200 [05:35<01:48,  2.16s/it]Running Inference:  76%|███████▌  | 151/200 [05:36<01:19,  1.62s/it]Running Inference:  76%|███████▌  | 152/200 [05:38<01:32,  1.93s/it]Running Inference:  76%|███████▋  | 153/200 [05:41<01:41,  2.17s/it]Running Inference:  77%|███████▋  | 154/200 [05:42<01:23,  1.82s/it]Running Inference:  78%|███████▊  | 155/200 [05:45<01:35,  2.12s/it]Running Inference:  78%|███████▊  | 156/200 [05:48<01:49,  2.49s/it]Running Inference:  78%|███████▊  | 157/200 [05:49<01:21,  1.89s/it]Running Inference:  79%|███████▉  | 158/200 [05:50<01:07,  1.60s/it]Running Inference:  80%|███████▉  | 159/200 [05:52<01:15,  1.83s/it]Running Inference:  80%|████████  | 160/200 [05:56<01:35,  2.38s/it]Running Inference:  80%|████████  | 161/200 [05:59<01:37,  2.50s/it]Running Inference:  81%|████████  | 162/200 [06:00<01:19,  2.09s/it]Running Inference:  82%|████████▏ | 163/200 [06:02<01:24,  2.29s/it]Running Inference:  82%|████████▏ | 164/200 [06:05<01:27,  2.42s/it]Running Inference:  82%|████████▎ | 165/200 [06:08<01:29,  2.55s/it]Running Inference:  83%|████████▎ | 166/200 [06:10<01:22,  2.41s/it]Running Inference:  84%|████████▎ | 167/200 [06:13<01:21,  2.48s/it]Running Inference:  84%|████████▍ | 168/200 [06:13<01:01,  1.91s/it]Running Inference:  84%|████████▍ | 169/200 [06:16<01:04,  2.07s/it]Running Inference:  85%|████████▌ | 170/200 [06:17<00:59,  1.97s/it]Running Inference:  86%|████████▌ | 171/200 [06:18<00:48,  1.66s/it]Running Inference:  86%|████████▌ | 172/200 [06:19<00:41,  1.47s/it]Running Inference:  86%|████████▋ | 173/200 [06:22<00:51,  1.92s/it]Running Inference:  87%|████████▋ | 174/200 [06:23<00:41,  1.61s/it]Running Inference:  88%|████████▊ | 175/200 [06:25<00:42,  1.71s/it]Running Inference:  88%|████████▊ | 176/200 [06:26<00:35,  1.49s/it]Running Inference:  88%|████████▊ | 177/200 [06:29<00:42,  1.84s/it]Running Inference:  89%|████████▉ | 178/200 [06:31<00:41,  1.90s/it]Running Inference:  90%|████████▉ | 179/200 [06:35<00:51,  2.46s/it]Running Inference:  90%|█████████ | 180/200 [06:36<00:42,  2.12s/it]Running Inference:  90%|█████████ | 181/200 [06:39<00:42,  2.24s/it]Running Inference:  91%|█████████ | 182/200 [06:42<00:48,  2.69s/it]Running Inference:  92%|█████████▏| 183/200 [06:46<00:48,  2.87s/it]Running Inference:  92%|█████████▏| 184/200 [06:48<00:45,  2.83s/it]Running Inference:  92%|█████████▎| 185/200 [06:50<00:39,  2.63s/it]Running Inference:  93%|█████████▎| 186/200 [06:51<00:28,  2.05s/it]Running Inference:  94%|█████████▎| 187/200 [06:52<00:22,  1.76s/it]Running Inference:  94%|█████████▍| 188/200 [06:55<00:24,  2.07s/it]Running Inference:  94%|█████████▍| 189/200 [06:56<00:18,  1.68s/it]Running Inference:  95%|█████████▌| 190/200 [06:56<00:12,  1.29s/it]Running Inference:  96%|█████████▌| 191/200 [07:00<00:18,  2.02s/it]Running Inference:  96%|█████████▌| 192/200 [07:03<00:18,  2.31s/it]Running Inference:  96%|█████████▋| 193/200 [07:07<00:19,  2.75s/it]Running Inference:  97%|█████████▋| 194/200 [07:07<00:12,  2.08s/it]Running Inference:  98%|█████████▊| 195/200 [07:08<00:09,  1.83s/it]Running Inference:  98%|█████████▊| 196/200 [07:11<00:08,  2.19s/it]Running Inference:  98%|█████████▊| 197/200 [07:13<00:05,  1.88s/it]Running Inference:  99%|█████████▉| 198/200 [07:13<00:02,  1.47s/it]Running Inference: 100%|█████████▉| 199/200 [07:14<00:01,  1.29s/it]Running Inference: 100%|██████████| 200/200 [07:15<00:00,  1.08s/it]Running Inference: 100%|██████████| 200/200 [07:15<00:00,  2.18s/it]
2025-12-13 18:22:17,805 - INFO - Inference completed.
2025-12-13 18:22:17,813 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-13 18:22:17,813 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:22:17,818 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-13 18:22:17,818 - INFO - Metrics:
12.48
2025-12-13 18:22:17,819 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=2wikimqa, ratio=0.5) on GPU 4


========================================
LongBench Task: hotpotqa
========================================
----------------------------------------
Task: hotpotqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:22:24,237 - INFO - Set deterministic seeds to 42
2025-12-13 18:22:24,237 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:22:24,238 - INFO - Starting evaluation run...
2025-12-13 18:22:24,238 - INFO - Output directory set to: longbenchresult
2025-12-13 18:22:24,238 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 18:22:24,238 - INFO - KV Press 'knorm' setup.
2025-12-13 18:22:24,238 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:22:24,238 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.34it/s]
Device set to use cuda:0
2025-12-13 18:22:39,122 - INFO - Model pipeline loaded.
2025-12-13 18:22:39,123 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:22:45,653 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:22:45,653 - INFO - Dataset processed with 200 entries.
2025-12-13 18:22:45,688 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<16:49,  5.07s/it]Running Inference:   1%|          | 2/200 [00:05<08:07,  2.46s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:34,  2.61s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:08,  2.49s/it]Running Inference:   2%|▎         | 5/200 [00:11<06:19,  1.94s/it]Running Inference:   3%|▎         | 6/200 [00:14<07:40,  2.37s/it]Running Inference:   4%|▎         | 7/200 [00:16<07:14,  2.25s/it]Running Inference:   4%|▍         | 8/200 [00:18<06:09,  1.92s/it]Running Inference:   4%|▍         | 9/200 [00:19<05:56,  1.87s/it]Running Inference:   5%|▌         | 10/200 [00:20<04:53,  1.54s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:22<04:48,  1.53s/it]Running Inference:   6%|▌         | 12/200 [00:23<04:30,  1.44s/it]Running Inference:   6%|▋         | 13/200 [00:25<04:57,  1.59s/it]Running Inference:   7%|▋         | 14/200 [00:27<05:29,  1.77s/it]Running Inference:   8%|▊         | 15/200 [00:30<06:37,  2.15s/it]Running Inference:   8%|▊         | 16/200 [00:32<06:36,  2.16s/it]Running Inference:   8%|▊         | 17/200 [00:34<06:35,  2.16s/it]Running Inference:   9%|▉         | 18/200 [00:37<06:33,  2.16s/it]Running Inference:  10%|▉         | 19/200 [00:37<05:14,  1.74s/it]Running Inference:  10%|█         | 20/200 [00:39<05:12,  1.74s/it]Running Inference:  10%|█         | 21/200 [00:40<04:17,  1.44s/it]Running Inference:  11%|█         | 22/200 [00:40<03:29,  1.18s/it]Running Inference:  12%|█▏        | 23/200 [00:42<04:12,  1.43s/it]Running Inference:  12%|█▏        | 24/200 [00:47<06:32,  2.23s/it]Running Inference:  12%|█▎        | 25/200 [00:50<07:28,  2.56s/it]Running Inference:  13%|█▎        | 26/200 [00:51<05:48,  2.00s/it]Running Inference:  14%|█▎        | 27/200 [00:52<05:37,  1.95s/it]Running Inference:  14%|█▍        | 28/200 [00:55<06:16,  2.19s/it]Running Inference:  14%|█▍        | 29/200 [00:58<06:52,  2.41s/it]Running Inference:  15%|█▌        | 30/200 [01:02<08:27,  2.99s/it]Running Inference:  16%|█▌        | 31/200 [01:04<06:58,  2.48s/it]Running Inference:  16%|█▌        | 32/200 [01:08<08:25,  3.01s/it]Running Inference:  16%|█▋        | 33/200 [01:10<07:51,  2.82s/it]Running Inference:  17%|█▋        | 34/200 [01:13<07:16,  2.63s/it]Running Inference:  18%|█▊        | 35/200 [01:15<06:54,  2.51s/it]Running Inference:  18%|█▊        | 36/200 [01:17<06:35,  2.41s/it]Running Inference:  18%|█▊        | 37/200 [01:19<06:30,  2.39s/it]Running Inference:  19%|█▉        | 38/200 [01:22<06:19,  2.34s/it]Running Inference:  20%|█▉        | 39/200 [01:24<06:18,  2.35s/it]Running Inference:  20%|██        | 40/200 [01:26<06:19,  2.37s/it]Running Inference:  20%|██        | 41/200 [01:28<05:40,  2.14s/it]Running Inference:  21%|██        | 42/200 [01:30<05:38,  2.14s/it]Running Inference:  22%|██▏       | 43/200 [01:31<04:58,  1.90s/it]Running Inference:  22%|██▏       | 44/200 [01:34<05:27,  2.10s/it]Running Inference:  22%|██▎       | 45/200 [01:36<05:27,  2.11s/it]Running Inference:  23%|██▎       | 46/200 [01:40<06:44,  2.63s/it]Running Inference:  24%|██▎       | 47/200 [01:42<06:24,  2.51s/it]Running Inference:  24%|██▍       | 48/200 [01:44<05:43,  2.26s/it]Running Inference:  24%|██▍       | 49/200 [01:46<05:30,  2.19s/it]Running Inference:  25%|██▌       | 50/200 [01:48<05:05,  2.04s/it]Running Inference:  26%|██▌       | 51/200 [01:51<05:54,  2.38s/it]Running Inference:  26%|██▌       | 52/200 [01:53<05:57,  2.41s/it]Running Inference:  26%|██▋       | 53/200 [01:55<05:31,  2.25s/it]Running Inference:  27%|██▋       | 54/200 [01:57<05:22,  2.21s/it]Running Inference:  28%|██▊       | 55/200 [01:59<05:22,  2.23s/it]Running Inference:  28%|██▊       | 56/200 [02:03<06:16,  2.61s/it]Running Inference:  28%|██▊       | 57/200 [02:07<06:54,  2.90s/it]Running Inference:  29%|██▉       | 58/200 [02:08<05:57,  2.52s/it]Running Inference:  30%|██▉       | 59/200 [02:10<05:05,  2.17s/it]Running Inference:  30%|███       | 60/200 [02:12<05:00,  2.15s/it]Running Inference:  30%|███       | 61/200 [02:16<06:16,  2.71s/it]Running Inference:  31%|███       | 62/200 [02:18<06:10,  2.69s/it]Running Inference:  32%|███▏      | 63/200 [02:22<06:43,  2.94s/it]Running Inference:  32%|███▏      | 64/200 [02:25<06:59,  3.09s/it]Running Inference:  32%|███▎      | 65/200 [02:28<06:49,  3.03s/it]Running Inference:  33%|███▎      | 66/200 [02:30<06:13,  2.79s/it]Running Inference:  34%|███▎      | 67/200 [02:32<05:15,  2.37s/it]Running Inference:  34%|███▍      | 68/200 [02:34<04:52,  2.21s/it]Running Inference:  34%|███▍      | 69/200 [02:37<05:28,  2.51s/it]Running Inference:  35%|███▌      | 70/200 [02:38<04:48,  2.22s/it]Running Inference:  36%|███▌      | 71/200 [02:42<05:59,  2.78s/it]Running Inference:  36%|███▌      | 72/200 [02:45<05:37,  2.64s/it]Running Inference:  36%|███▋      | 73/200 [02:47<05:18,  2.51s/it]Running Inference:  37%|███▋      | 74/200 [02:51<05:58,  2.85s/it]Running Inference:  38%|███▊      | 75/200 [02:52<05:11,  2.49s/it]Running Inference:  38%|███▊      | 76/200 [02:55<05:34,  2.70s/it]Running Inference:  38%|███▊      | 77/200 [02:56<04:23,  2.14s/it]Running Inference:  39%|███▉      | 78/200 [02:59<04:33,  2.24s/it]Running Inference:  40%|███▉      | 79/200 [03:01<04:18,  2.14s/it]Running Inference:  40%|████      | 80/200 [03:01<03:29,  1.74s/it]Running Inference:  40%|████      | 81/200 [03:06<04:57,  2.50s/it]Running Inference:  41%|████      | 82/200 [03:08<04:29,  2.28s/it]Running Inference:  42%|████▏     | 83/200 [03:09<03:55,  2.01s/it]Running Inference:  42%|████▏     | 84/200 [03:13<04:49,  2.49s/it]Running Inference:  42%|████▎     | 85/200 [03:17<05:49,  3.04s/it]Running Inference:  43%|████▎     | 86/200 [03:20<05:33,  2.93s/it]Running Inference:  44%|████▎     | 87/200 [03:22<05:00,  2.66s/it]Running Inference:  44%|████▍     | 88/200 [03:23<04:17,  2.30s/it]Running Inference:  44%|████▍     | 89/200 [03:25<04:15,  2.30s/it]Running Inference:  45%|████▌     | 90/200 [03:27<03:58,  2.17s/it]Running Inference:  46%|████▌     | 91/200 [03:28<03:14,  1.78s/it]Running Inference:  46%|████▌     | 92/200 [03:32<04:35,  2.55s/it]Running Inference:  46%|████▋     | 93/200 [03:34<04:03,  2.28s/it]Running Inference:  47%|████▋     | 94/200 [03:37<04:11,  2.37s/it]Running Inference:  48%|████▊     | 95/200 [03:38<03:46,  2.16s/it]Running Inference:  48%|████▊     | 96/200 [03:40<03:44,  2.16s/it]Running Inference:  48%|████▊     | 97/200 [03:42<03:26,  2.01s/it]Running Inference:  49%|████▉     | 98/200 [03:46<04:29,  2.65s/it]Running Inference:  50%|████▉     | 99/200 [03:50<05:04,  3.01s/it]Running Inference:  50%|█████     | 100/200 [03:52<04:37,  2.78s/it]Running Inference:  50%|█████     | 101/200 [03:57<05:18,  3.22s/it]Running Inference:  51%|█████     | 102/200 [03:59<04:44,  2.90s/it]Running Inference:  52%|█████▏    | 103/200 [04:00<03:40,  2.27s/it]Running Inference:  52%|█████▏    | 104/200 [04:01<03:15,  2.04s/it]Running Inference:  52%|█████▎    | 105/200 [04:03<02:57,  1.87s/it]Running Inference:  53%|█████▎    | 106/200 [04:04<02:58,  1.90s/it]Running Inference:  54%|█████▎    | 107/200 [04:07<03:22,  2.18s/it]Running Inference:  54%|█████▍    | 108/200 [04:08<02:40,  1.75s/it]Running Inference:  55%|█████▍    | 109/200 [04:10<02:31,  1.67s/it]Running Inference:  55%|█████▌    | 110/200 [04:12<02:51,  1.90s/it]Running Inference:  56%|█████▌    | 111/200 [04:14<03:02,  2.05s/it]Running Inference:  56%|█████▌    | 112/200 [04:18<03:54,  2.67s/it]Running Inference:  56%|█████▋    | 113/200 [04:21<03:45,  2.59s/it]Running Inference:  57%|█████▋    | 114/200 [04:23<03:32,  2.48s/it]Running Inference:  57%|█████▊    | 115/200 [04:25<03:25,  2.42s/it]Running Inference:  58%|█████▊    | 116/200 [04:29<04:00,  2.86s/it]Running Inference:  58%|█████▊    | 117/200 [04:32<03:45,  2.71s/it]Running Inference:  59%|█████▉    | 118/200 [04:34<03:30,  2.57s/it]Running Inference:  60%|█████▉    | 119/200 [04:35<02:47,  2.07s/it]Running Inference:  60%|██████    | 120/200 [04:37<02:42,  2.03s/it]Running Inference:  60%|██████    | 121/200 [04:41<03:31,  2.68s/it]Running Inference:  61%|██████    | 122/200 [04:43<03:16,  2.52s/it]Running Inference:  62%|██████▏   | 123/200 [04:47<03:52,  3.02s/it]Running Inference:  62%|██████▏   | 124/200 [04:49<03:23,  2.68s/it]Running Inference:  62%|██████▎   | 125/200 [04:51<03:07,  2.50s/it]Running Inference:  63%|██████▎   | 126/200 [04:53<02:58,  2.42s/it]Running Inference:  64%|██████▎   | 127/200 [04:54<02:18,  1.90s/it]Running Inference:  64%|██████▍   | 128/200 [04:58<03:09,  2.63s/it]Running Inference:  64%|██████▍   | 129/200 [05:01<02:56,  2.48s/it]Running Inference:  65%|██████▌   | 130/200 [05:02<02:39,  2.28s/it]Running Inference:  66%|██████▌   | 131/200 [05:06<03:09,  2.75s/it]Running Inference:  66%|██████▌   | 132/200 [05:08<02:38,  2.33s/it]Running Inference:  66%|██████▋   | 133/200 [05:11<02:55,  2.61s/it]Running Inference:  67%|██████▋   | 134/200 [05:13<02:46,  2.52s/it]Running Inference:  68%|██████▊   | 135/200 [05:17<03:14,  2.99s/it]Running Inference:  68%|██████▊   | 136/200 [05:19<02:53,  2.71s/it]Running Inference:  68%|██████▊   | 137/200 [05:22<02:57,  2.81s/it]Running Inference:  69%|██████▉   | 138/200 [05:25<02:44,  2.66s/it]Running Inference:  70%|██████▉   | 139/200 [05:29<03:09,  3.11s/it]Running Inference:  70%|███████   | 140/200 [05:31<02:51,  2.86s/it]Running Inference:  70%|███████   | 141/200 [05:33<02:37,  2.67s/it]Running Inference:  71%|███████   | 142/200 [05:37<02:59,  3.10s/it]Running Inference:  72%|███████▏  | 143/200 [05:38<02:12,  2.33s/it]Running Inference:  72%|███████▏  | 144/200 [05:40<02:03,  2.20s/it]Running Inference:  72%|███████▎  | 145/200 [05:40<01:30,  1.65s/it]Running Inference:  73%|███████▎  | 146/200 [05:44<01:55,  2.14s/it]Running Inference:  74%|███████▎  | 147/200 [05:46<01:53,  2.14s/it]Running Inference:  74%|███████▍  | 148/200 [05:48<01:47,  2.07s/it]Running Inference:  74%|███████▍  | 149/200 [05:50<01:48,  2.12s/it]Running Inference:  75%|███████▌  | 150/200 [05:51<01:36,  1.93s/it]Running Inference:  76%|███████▌  | 151/200 [05:53<01:29,  1.82s/it]Running Inference:  76%|███████▌  | 152/200 [05:55<01:30,  1.88s/it]Running Inference:  76%|███████▋  | 153/200 [05:59<01:59,  2.54s/it]Running Inference:  77%|███████▋  | 154/200 [06:01<01:53,  2.48s/it]Running Inference:  78%|███████▊  | 155/200 [06:05<02:13,  2.96s/it]Running Inference:  78%|███████▊  | 156/200 [06:07<01:46,  2.42s/it]Running Inference:  78%|███████▊  | 157/200 [06:08<01:31,  2.13s/it]Running Inference:  79%|███████▉  | 158/200 [06:11<01:37,  2.32s/it]Running Inference:  80%|███████▉  | 159/200 [06:12<01:24,  2.06s/it]Running Inference:  80%|████████  | 160/200 [06:16<01:46,  2.67s/it]Running Inference:  80%|████████  | 161/200 [06:19<01:38,  2.53s/it]Running Inference:  81%|████████  | 162/200 [06:19<01:13,  1.94s/it]Running Inference:  82%|████████▏ | 163/200 [06:21<01:09,  1.87s/it]Running Inference:  82%|████████▏ | 164/200 [06:23<01:10,  1.95s/it]Running Inference:  82%|████████▎ | 165/200 [06:27<01:31,  2.60s/it]Running Inference:  83%|████████▎ | 166/200 [06:29<01:22,  2.44s/it]Running Inference:  84%|████████▎ | 167/200 [06:31<01:16,  2.31s/it]Running Inference:  84%|████████▍ | 168/200 [06:35<01:29,  2.81s/it]Running Inference:  84%|████████▍ | 169/200 [06:37<01:23,  2.68s/it]Running Inference:  85%|████████▌ | 170/200 [06:40<01:15,  2.50s/it]Running Inference:  86%|████████▌ | 171/200 [06:42<01:14,  2.57s/it]Running Inference:  86%|████████▌ | 172/200 [06:46<01:25,  3.04s/it]Running Inference:  86%|████████▋ | 173/200 [06:49<01:14,  2.76s/it]Running Inference:  87%|████████▋ | 174/200 [06:52<01:18,  3.03s/it]Running Inference:  88%|████████▊ | 175/200 [06:54<01:10,  2.80s/it]Running Inference:  88%|████████▊ | 176/200 [06:57<01:02,  2.59s/it]Running Inference:  88%|████████▊ | 177/200 [06:59<00:57,  2.51s/it]Running Inference:  89%|████████▉ | 178/200 [07:01<00:54,  2.50s/it]Running Inference:  90%|████████▉ | 179/200 [07:04<00:51,  2.44s/it]Running Inference:  90%|█████████ | 180/200 [07:07<00:55,  2.76s/it]Running Inference:  90%|█████████ | 181/200 [07:09<00:49,  2.59s/it]Running Inference:  91%|█████████ | 182/200 [07:11<00:42,  2.37s/it]Running Inference:  92%|█████████▏| 183/200 [07:13<00:36,  2.13s/it]Running Inference:  92%|█████████▏| 184/200 [07:17<00:42,  2.68s/it]Running Inference:  92%|█████████▎| 185/200 [07:18<00:35,  2.34s/it]Running Inference:  93%|█████████▎| 186/200 [07:20<00:31,  2.28s/it]Running Inference:  94%|█████████▎| 187/200 [07:22<00:27,  2.14s/it]Running Inference:  94%|█████████▍| 188/200 [07:23<00:20,  1.72s/it]Running Inference:  94%|█████████▍| 189/200 [07:25<00:20,  1.87s/it]Running Inference:  95%|█████████▌| 190/200 [07:26<00:16,  1.63s/it]Running Inference:  96%|█████████▌| 191/200 [07:28<00:16,  1.78s/it]Running Inference:  96%|█████████▌| 192/200 [07:32<00:17,  2.24s/it]Running Inference:  96%|█████████▋| 193/200 [07:36<00:20,  2.88s/it]Running Inference:  97%|█████████▋| 194/200 [07:39<00:17,  2.86s/it]Running Inference:  98%|█████████▊| 195/200 [07:42<00:14,  2.90s/it]Running Inference:  98%|█████████▊| 196/200 [07:44<00:11,  2.78s/it]Running Inference:  98%|█████████▊| 197/200 [07:46<00:07,  2.37s/it]Running Inference:  99%|█████████▉| 198/200 [07:50<00:05,  2.81s/it]Running Inference: 100%|█████████▉| 199/200 [07:54<00:03,  3.25s/it]Running Inference: 100%|██████████| 200/200 [07:56<00:00,  3.04s/it]Running Inference: 100%|██████████| 200/200 [07:56<00:00,  2.38s/it]
2025-12-13 18:30:42,667 - INFO - Inference completed.
2025-12-13 18:30:42,676 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 18:30:42,676 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:30:42,680 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 18:30:42,680 - INFO - Metrics:
32.49
2025-12-13 18:30:42,681 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=hotpotqa, ratio=0.1) on GPU 4

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:30:49,023 - INFO - Set deterministic seeds to 42
2025-12-13 18:30:49,023 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:30:49,023 - INFO - Starting evaluation run...
2025-12-13 18:30:49,023 - INFO - Output directory set to: longbenchresult
2025-12-13 18:30:49,023 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 18:30:49,023 - INFO - KV Press 'knorm' setup.
2025-12-13 18:30:49,024 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:30:49,024 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.60it/s]
Device set to use cuda:0
2025-12-13 18:31:36,011 - INFO - Model pipeline loaded.
2025-12-13 18:31:36,012 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:31:40,841 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:31:40,841 - INFO - Dataset processed with 200 entries.
2025-12-13 18:31:40,877 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<16:56,  5.11s/it]Running Inference:   1%|          | 2/200 [00:05<08:12,  2.49s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:54,  2.71s/it]Running Inference:   2%|▏         | 4/200 [00:13<10:53,  3.33s/it]Running Inference:   2%|▎         | 5/200 [00:15<09:40,  2.98s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:55,  3.07s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:45,  2.72s/it]Running Inference:   4%|▍         | 8/200 [00:23<09:13,  2.88s/it]Running Inference:   4%|▍         | 9/200 [00:25<08:03,  2.53s/it]Running Inference:   5%|▌         | 10/200 [00:27<07:15,  2.29s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:30<08:23,  2.66s/it]Running Inference:   6%|▌         | 12/200 [00:32<06:58,  2.23s/it]Running Inference:   6%|▋         | 13/200 [00:34<06:40,  2.14s/it]Running Inference:   7%|▋         | 14/200 [00:36<06:41,  2.16s/it]Running Inference:   8%|▊         | 15/200 [00:39<07:23,  2.40s/it]Running Inference:   8%|▊         | 16/200 [00:41<07:12,  2.35s/it]Running Inference:   8%|▊         | 17/200 [00:43<07:00,  2.30s/it]Running Inference:   9%|▉         | 18/200 [00:45<06:50,  2.26s/it]Running Inference:  10%|▉         | 19/200 [00:46<05:49,  1.93s/it]Running Inference:  10%|█         | 20/200 [00:48<05:38,  1.88s/it]Running Inference:  10%|█         | 21/200 [00:49<04:37,  1.55s/it]Running Inference:  11%|█         | 22/200 [00:50<03:42,  1.25s/it]Running Inference:  12%|█▏        | 23/200 [00:51<03:55,  1.33s/it]Running Inference:  12%|█▏        | 24/200 [00:55<06:22,  2.17s/it]Running Inference:  12%|█▎        | 25/200 [00:57<05:39,  1.94s/it]Running Inference:  13%|█▎        | 26/200 [00:58<04:52,  1.68s/it]Running Inference:  14%|█▎        | 27/200 [01:00<05:00,  1.73s/it]Running Inference:  14%|█▍        | 28/200 [01:01<05:02,  1.76s/it]Running Inference:  14%|█▍        | 29/200 [01:06<07:09,  2.51s/it]Running Inference:  15%|█▌        | 30/200 [01:08<07:00,  2.47s/it]Running Inference:  16%|█▌        | 31/200 [01:09<05:58,  2.12s/it]Running Inference:  16%|█▌        | 32/200 [01:14<07:45,  2.77s/it]Running Inference:  16%|█▋        | 33/200 [01:16<07:24,  2.66s/it]Running Inference:  17%|█▋        | 34/200 [01:18<06:58,  2.52s/it]Running Inference:  18%|█▊        | 35/200 [01:20<06:42,  2.44s/it]Running Inference:  18%|█▊        | 36/200 [01:23<06:27,  2.36s/it]Running Inference:  18%|█▊        | 37/200 [01:25<06:24,  2.36s/it]Running Inference:  19%|█▉        | 38/200 [01:27<06:15,  2.32s/it]Running Inference:  20%|█▉        | 39/200 [01:29<06:11,  2.31s/it]Running Inference:  20%|██        | 40/200 [01:32<06:15,  2.34s/it]Running Inference:  20%|██        | 41/200 [01:34<05:49,  2.20s/it]Running Inference:  21%|██        | 42/200 [01:36<05:44,  2.18s/it]Running Inference:  22%|██▏       | 43/200 [01:37<05:05,  1.95s/it]Running Inference:  22%|██▏       | 44/200 [01:40<05:32,  2.13s/it]Running Inference:  22%|██▎       | 45/200 [01:42<05:43,  2.22s/it]Running Inference:  23%|██▎       | 46/200 [01:46<06:57,  2.71s/it]Running Inference:  24%|██▎       | 47/200 [01:48<06:33,  2.57s/it]Running Inference:  24%|██▍       | 48/200 [01:52<07:08,  2.82s/it]Running Inference:  24%|██▍       | 49/200 [01:56<08:02,  3.20s/it]Running Inference:  25%|██▌       | 50/200 [02:00<08:22,  3.35s/it]Running Inference:  26%|██▌       | 51/200 [02:03<08:32,  3.44s/it]Running Inference:  26%|██▌       | 52/200 [02:06<07:47,  3.16s/it]Running Inference:  26%|██▋       | 53/200 [02:07<06:30,  2.65s/it]Running Inference:  27%|██▋       | 54/200 [02:09<06:02,  2.49s/it]Running Inference:  28%|██▊       | 55/200 [02:14<07:16,  3.01s/it]Running Inference:  28%|██▊       | 56/200 [02:17<07:37,  3.18s/it]Running Inference:  28%|██▊       | 57/200 [02:21<07:54,  3.32s/it]Running Inference:  29%|██▉       | 58/200 [02:22<06:38,  2.81s/it]Running Inference:  30%|██▉       | 59/200 [02:24<05:35,  2.38s/it]Running Inference:  30%|███       | 60/200 [02:26<05:22,  2.31s/it]Running Inference:  30%|███       | 61/200 [02:28<05:16,  2.28s/it]Running Inference:  31%|███       | 62/200 [02:32<06:37,  2.88s/it]Running Inference:  32%|███▏      | 63/200 [02:36<07:04,  3.10s/it]Running Inference:  32%|███▏      | 64/200 [02:40<07:48,  3.45s/it]Running Inference:  32%|███▎      | 65/200 [02:44<08:07,  3.61s/it]Running Inference:  33%|███▎      | 66/200 [02:46<07:02,  3.15s/it]Running Inference:  34%|███▎      | 67/200 [02:50<07:12,  3.25s/it]Running Inference:  34%|███▍      | 68/200 [02:52<06:25,  2.92s/it]Running Inference:  34%|███▍      | 69/200 [02:53<05:16,  2.42s/it]Running Inference:  35%|███▌      | 70/200 [02:55<04:40,  2.15s/it]Running Inference:  36%|███▌      | 71/200 [02:59<05:55,  2.76s/it]Running Inference:  36%|███▌      | 72/200 [03:01<05:35,  2.62s/it]Running Inference:  36%|███▋      | 73/200 [03:06<06:37,  3.13s/it]Running Inference:  37%|███▋      | 74/200 [03:09<06:56,  3.30s/it]Running Inference:  38%|███▊      | 75/200 [03:13<07:12,  3.46s/it]Running Inference:  38%|███▊      | 76/200 [03:16<07:02,  3.40s/it]Running Inference:  38%|███▊      | 77/200 [03:17<05:27,  2.66s/it]Running Inference:  39%|███▉      | 78/200 [03:21<06:12,  3.05s/it]Running Inference:  40%|███▉      | 79/200 [03:23<05:27,  2.71s/it]Running Inference:  40%|████      | 80/200 [03:24<04:17,  2.15s/it]Running Inference:  40%|████      | 81/200 [03:28<05:32,  2.79s/it]Running Inference:  41%|████      | 82/200 [03:30<04:54,  2.49s/it]Running Inference:  42%|████▏     | 83/200 [03:31<04:11,  2.15s/it]Running Inference:  42%|████▏     | 84/200 [03:35<05:02,  2.61s/it]Running Inference:  42%|████▎     | 85/200 [03:39<06:00,  3.13s/it]Running Inference:  43%|████▎     | 86/200 [03:42<05:34,  2.93s/it]Running Inference:  44%|████▎     | 87/200 [03:46<06:07,  3.26s/it]Running Inference:  44%|████▍     | 88/200 [03:48<05:15,  2.82s/it]Running Inference:  44%|████▍     | 89/200 [03:50<04:55,  2.66s/it]Running Inference:  45%|████▌     | 90/200 [03:52<04:27,  2.43s/it]Running Inference:  46%|████▌     | 91/200 [03:53<03:34,  1.96s/it]Running Inference:  46%|████▌     | 92/200 [03:57<04:51,  2.70s/it]Running Inference:  46%|████▋     | 93/200 [03:59<04:14,  2.38s/it]Running Inference:  47%|████▋     | 94/200 [04:02<04:52,  2.76s/it]Running Inference:  48%|████▊     | 95/200 [04:04<04:14,  2.43s/it]Running Inference:  48%|████▊     | 96/200 [04:06<04:04,  2.35s/it]Running Inference:  48%|████▊     | 97/200 [04:08<03:40,  2.14s/it]Running Inference:  49%|████▉     | 98/200 [04:12<04:41,  2.76s/it]Running Inference:  50%|████▉     | 99/200 [04:14<04:09,  2.47s/it]Running Inference:  50%|█████     | 100/200 [04:16<04:01,  2.41s/it]Running Inference:  50%|█████     | 101/200 [04:19<04:02,  2.45s/it]Running Inference:  51%|█████     | 102/200 [04:21<03:51,  2.36s/it]Running Inference:  52%|█████▏    | 103/200 [04:22<03:04,  1.90s/it]Running Inference:  52%|█████▏    | 104/200 [04:24<03:04,  1.92s/it]Running Inference:  52%|█████▎    | 105/200 [04:25<02:50,  1.79s/it]Running Inference:  53%|█████▎    | 106/200 [04:27<02:53,  1.84s/it]Running Inference:  54%|█████▎    | 107/200 [04:30<03:21,  2.17s/it]Running Inference:  54%|█████▍    | 108/200 [04:31<02:40,  1.74s/it]Running Inference:  55%|█████▍    | 109/200 [04:32<02:31,  1.67s/it]Running Inference:  55%|█████▌    | 110/200 [04:36<03:13,  2.15s/it]Running Inference:  56%|█████▌    | 111/200 [04:38<03:17,  2.22s/it]Running Inference:  56%|█████▌    | 112/200 [04:42<04:06,  2.81s/it]Running Inference:  56%|█████▋    | 113/200 [04:47<04:45,  3.28s/it]Running Inference:  57%|█████▋    | 114/200 [04:49<04:15,  2.97s/it]Running Inference:  57%|█████▊    | 115/200 [04:51<03:55,  2.77s/it]Running Inference:  58%|█████▊    | 116/200 [04:55<04:22,  3.12s/it]Running Inference:  58%|█████▊    | 117/200 [04:57<04:00,  2.90s/it]Running Inference:  59%|█████▉    | 118/200 [05:00<03:42,  2.71s/it]Running Inference:  60%|█████▉    | 119/200 [05:01<02:56,  2.17s/it]Running Inference:  60%|██████    | 120/200 [05:03<02:48,  2.10s/it]Running Inference:  60%|██████    | 121/200 [05:05<02:50,  2.16s/it]Running Inference:  61%|██████    | 122/200 [05:07<02:48,  2.15s/it]Running Inference:  62%|██████▏   | 123/200 [05:11<03:34,  2.78s/it]Running Inference:  62%|██████▏   | 124/200 [05:13<03:12,  2.53s/it]Running Inference:  62%|██████▎   | 125/200 [05:15<03:00,  2.41s/it]Running Inference:  63%|██████▎   | 126/200 [05:18<02:54,  2.36s/it]Running Inference:  64%|██████▎   | 127/200 [05:18<02:15,  1.86s/it]Running Inference:  64%|██████▍   | 128/200 [05:23<03:08,  2.61s/it]Running Inference:  64%|██████▍   | 129/200 [05:25<02:55,  2.47s/it]Running Inference:  65%|██████▌   | 130/200 [05:27<02:39,  2.28s/it]Running Inference:  66%|██████▌   | 131/200 [05:30<03:11,  2.77s/it]Running Inference:  66%|██████▌   | 132/200 [05:32<02:39,  2.35s/it]Running Inference:  66%|██████▋   | 133/200 [05:35<02:57,  2.65s/it]Running Inference:  67%|██████▋   | 134/200 [05:38<02:48,  2.55s/it]Running Inference:  68%|██████▊   | 135/200 [05:42<03:16,  3.03s/it]Running Inference:  68%|██████▊   | 136/200 [05:44<02:55,  2.74s/it]Running Inference:  68%|██████▊   | 137/200 [05:45<02:25,  2.31s/it]Running Inference:  69%|██████▉   | 138/200 [05:49<02:59,  2.89s/it]Running Inference:  70%|██████▉   | 139/200 [05:54<03:21,  3.30s/it]Running Inference:  70%|███████   | 140/200 [05:57<03:29,  3.49s/it]Running Inference:  70%|███████   | 141/200 [06:00<03:04,  3.13s/it]Running Inference:  71%|███████   | 142/200 [06:04<03:19,  3.44s/it]Running Inference:  72%|███████▏  | 143/200 [06:06<02:59,  3.15s/it]Running Inference:  72%|███████▏  | 144/200 [06:08<02:35,  2.78s/it]Running Inference:  72%|███████▎  | 145/200 [06:10<02:14,  2.45s/it]Running Inference:  73%|███████▎  | 146/200 [06:14<02:41,  2.99s/it]Running Inference:  74%|███████▎  | 147/200 [06:16<02:25,  2.74s/it]Running Inference:  74%|███████▍  | 148/200 [06:18<02:09,  2.48s/it]Running Inference:  74%|███████▍  | 149/200 [06:19<01:39,  1.95s/it]Running Inference:  75%|███████▌  | 150/200 [06:20<01:30,  1.81s/it]Running Inference:  76%|███████▌  | 151/200 [06:22<01:24,  1.73s/it]Running Inference:  76%|███████▌  | 152/200 [06:26<01:55,  2.40s/it]Running Inference:  76%|███████▋  | 153/200 [06:30<02:17,  2.93s/it]Running Inference:  77%|███████▋  | 154/200 [06:32<02:06,  2.75s/it]Running Inference:  78%|███████▊  | 155/200 [06:36<02:11,  2.92s/it]Running Inference:  78%|███████▊  | 156/200 [06:37<01:45,  2.39s/it]Running Inference:  78%|███████▊  | 157/200 [06:38<01:30,  2.11s/it]Running Inference:  79%|███████▉  | 158/200 [06:39<01:11,  1.69s/it]Running Inference:  80%|███████▉  | 159/200 [06:43<01:32,  2.26s/it]Running Inference:  80%|████████  | 160/200 [06:47<01:53,  2.83s/it]Running Inference:  80%|████████  | 161/200 [06:49<01:42,  2.64s/it]Running Inference:  81%|████████  | 162/200 [06:50<01:16,  2.02s/it]Running Inference:  82%|████████▏ | 163/200 [06:51<01:11,  1.92s/it]Running Inference:  82%|████████▏ | 164/200 [06:53<01:11,  1.99s/it]Running Inference:  82%|████████▎ | 165/200 [06:58<01:32,  2.65s/it]Running Inference:  83%|████████▎ | 166/200 [07:00<01:24,  2.48s/it]Running Inference:  84%|████████▎ | 167/200 [07:02<01:17,  2.34s/it]Running Inference:  84%|████████▍ | 168/200 [07:06<01:31,  2.85s/it]Running Inference:  84%|████████▍ | 169/200 [07:10<01:41,  3.28s/it]Running Inference:  85%|████████▌ | 170/200 [07:12<01:27,  2.92s/it]Running Inference:  86%|████████▌ | 171/200 [07:15<01:23,  2.88s/it]Running Inference:  86%|████████▌ | 172/200 [07:19<01:31,  3.28s/it]Running Inference:  86%|████████▋ | 173/200 [07:21<01:18,  2.93s/it]Running Inference:  87%|████████▋ | 174/200 [07:25<01:22,  3.18s/it]Running Inference:  88%|████████▊ | 175/200 [07:27<01:12,  2.90s/it]Running Inference:  88%|████████▊ | 176/200 [07:30<01:06,  2.75s/it]Running Inference:  88%|████████▊ | 177/200 [07:32<01:00,  2.63s/it]Running Inference:  89%|████████▉ | 178/200 [07:34<00:55,  2.53s/it]Running Inference:  90%|████████▉ | 179/200 [07:37<00:51,  2.46s/it]Running Inference:  90%|█████████ | 180/200 [07:40<00:55,  2.80s/it]Running Inference:  90%|█████████ | 181/200 [07:42<00:49,  2.62s/it]Running Inference:  91%|█████████ | 182/200 [07:44<00:43,  2.39s/it]Running Inference:  92%|█████████▏| 183/200 [07:46<00:37,  2.22s/it]Running Inference:  92%|█████████▏| 184/200 [07:50<00:44,  2.77s/it]Running Inference:  92%|█████████▎| 185/200 [07:52<00:36,  2.41s/it]Running Inference:  93%|█████████▎| 186/200 [07:54<00:32,  2.32s/it]Running Inference:  94%|█████████▎| 187/200 [07:56<00:28,  2.18s/it]Running Inference:  94%|█████████▍| 188/200 [07:56<00:20,  1.74s/it]Running Inference:  94%|█████████▍| 189/200 [07:59<00:20,  1.88s/it]Running Inference:  95%|█████████▌| 190/200 [08:00<00:16,  1.62s/it]Running Inference:  96%|█████████▌| 191/200 [08:04<00:21,  2.36s/it]Running Inference:  96%|█████████▌| 192/200 [08:07<00:21,  2.67s/it]Running Inference:  96%|█████████▋| 193/200 [08:12<00:22,  3.20s/it]Running Inference:  97%|█████████▋| 194/200 [08:14<00:18,  3.07s/it]Running Inference:  98%|█████████▊| 195/200 [08:17<00:15,  3.07s/it]Running Inference:  98%|█████████▊| 196/200 [08:20<00:11,  2.89s/it]Running Inference:  98%|█████████▊| 197/200 [08:21<00:07,  2.45s/it]Running Inference:  99%|█████████▉| 198/200 [08:24<00:05,  2.54s/it]Running Inference: 100%|█████████▉| 199/200 [08:28<00:03,  3.07s/it]Running Inference: 100%|██████████| 200/200 [08:29<00:00,  2.45s/it]Running Inference: 100%|██████████| 200/200 [08:29<00:00,  2.55s/it]
2025-12-13 18:40:10,712 - INFO - Inference completed.
2025-12-13 18:40:10,720 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-13 18:40:10,720 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:40:10,725 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-13 18:40:10,725 - INFO - Metrics:
27.39
2025-12-13 18:40:10,726 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=hotpotqa, ratio=0.2) on GPU 4

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:40:17,155 - INFO - Set deterministic seeds to 42
2025-12-13 18:40:17,155 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:40:17,155 - INFO - Starting evaluation run...
2025-12-13 18:40:17,156 - INFO - Output directory set to: longbenchresult
2025-12-13 18:40:17,156 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-13 18:40:17,156 - INFO - KV Press 'knorm' setup.
2025-12-13 18:40:17,156 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:40:17,156 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.66it/s]
Device set to use cuda:0
2025-12-13 18:40:30,055 - INFO - Model pipeline loaded.
2025-12-13 18:40:30,055 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:40:38,206 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:40:38,207 - INFO - Dataset processed with 200 entries.
2025-12-13 18:40:38,242 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<16:47,  5.06s/it]Running Inference:   1%|          | 2/200 [00:06<08:47,  2.66s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:11,  2.50s/it]Running Inference:   2%|▏         | 4/200 [00:12<10:25,  3.19s/it]Running Inference:   2%|▎         | 5/200 [00:15<10:02,  3.09s/it]Running Inference:   3%|▎         | 6/200 [00:18<10:08,  3.14s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:53,  2.76s/it]Running Inference:   4%|▍         | 8/200 [00:23<09:17,  2.90s/it]Running Inference:   4%|▍         | 9/200 [00:25<08:05,  2.54s/it]Running Inference:   5%|▌         | 10/200 [00:28<08:14,  2.60s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:31<09:02,  2.87s/it]Running Inference:   6%|▌         | 12/200 [00:33<07:25,  2.37s/it]Running Inference:   6%|▋         | 13/200 [00:35<06:58,  2.24s/it]Running Inference:   7%|▋         | 14/200 [00:39<08:49,  2.85s/it]Running Inference:   8%|▊         | 15/200 [00:42<09:12,  2.99s/it]Running Inference:   8%|▊         | 16/200 [00:45<08:38,  2.82s/it]Running Inference:   8%|▊         | 17/200 [00:49<09:50,  3.23s/it]Running Inference:   9%|▉         | 18/200 [00:51<08:48,  2.91s/it]Running Inference:  10%|▉         | 19/200 [00:54<08:40,  2.88s/it]Running Inference:  10%|█         | 20/200 [00:57<08:53,  2.96s/it]Running Inference:  10%|█         | 21/200 [00:58<06:52,  2.31s/it]Running Inference:  11%|█         | 22/200 [00:58<05:16,  1.78s/it]Running Inference:  12%|█▏        | 23/200 [01:02<06:45,  2.29s/it]Running Inference:  12%|█▏        | 24/200 [01:06<08:18,  2.83s/it]Running Inference:  12%|█▎        | 25/200 [01:07<07:15,  2.49s/it]Running Inference:  13%|█▎        | 26/200 [01:09<05:58,  2.06s/it]Running Inference:  14%|█▎        | 27/200 [01:10<05:45,  2.00s/it]Running Inference:  14%|█▍        | 28/200 [01:12<05:31,  1.93s/it]Running Inference:  14%|█▍        | 29/200 [01:16<07:26,  2.61s/it]Running Inference:  15%|█▌        | 30/200 [01:21<08:51,  3.12s/it]Running Inference:  16%|█▌        | 31/200 [01:22<07:15,  2.57s/it]Running Inference:  16%|█▌        | 32/200 [01:26<08:36,  3.08s/it]Running Inference:  16%|█▋        | 33/200 [01:30<09:31,  3.42s/it]Running Inference:  17%|█▋        | 34/200 [01:33<08:27,  3.06s/it]Running Inference:  18%|█▊        | 35/200 [01:35<07:44,  2.81s/it]Running Inference:  18%|█▊        | 36/200 [01:37<07:09,  2.62s/it]Running Inference:  18%|█▊        | 37/200 [01:39<06:53,  2.54s/it]Running Inference:  19%|█▉        | 38/200 [01:42<06:35,  2.44s/it]Running Inference:  20%|█▉        | 39/200 [01:44<06:27,  2.40s/it]Running Inference:  20%|██        | 40/200 [01:46<06:25,  2.41s/it]Running Inference:  20%|██        | 41/200 [01:50<07:25,  2.80s/it]Running Inference:  21%|██        | 42/200 [01:52<06:52,  2.61s/it]Running Inference:  22%|██▏       | 43/200 [01:55<07:18,  2.79s/it]Running Inference:  22%|██▏       | 44/200 [01:58<07:04,  2.72s/it]Running Inference:  22%|██▎       | 45/200 [02:02<08:02,  3.11s/it]Running Inference:  23%|██▎       | 46/200 [02:06<08:38,  3.37s/it]Running Inference:  24%|██▎       | 47/200 [02:08<07:41,  3.02s/it]Running Inference:  24%|██▍       | 48/200 [02:12<07:57,  3.14s/it]Running Inference:  24%|██▍       | 49/200 [02:16<08:37,  3.43s/it]Running Inference:  25%|██▌       | 50/200 [02:19<08:47,  3.52s/it]Running Inference:  26%|██▌       | 51/200 [02:23<08:51,  3.57s/it]Running Inference:  26%|██▌       | 52/200 [02:25<07:41,  3.12s/it]Running Inference:  26%|██▋       | 53/200 [02:28<07:33,  3.09s/it]Running Inference:  27%|██▋       | 54/200 [02:32<08:20,  3.43s/it]Running Inference:  28%|██▊       | 55/200 [02:37<08:48,  3.65s/it]Running Inference:  28%|██▊       | 56/200 [02:40<08:40,  3.62s/it]Running Inference:  28%|██▊       | 57/200 [02:44<08:36,  3.61s/it]Running Inference:  29%|██▉       | 58/200 [02:45<07:06,  3.00s/it]Running Inference:  30%|██▉       | 59/200 [02:47<05:50,  2.48s/it]Running Inference:  30%|███       | 60/200 [02:51<07:00,  3.00s/it]Running Inference:  30%|███       | 61/200 [02:53<06:23,  2.76s/it]Running Inference:  31%|███       | 62/200 [02:57<07:22,  3.21s/it]Running Inference:  32%|███▏      | 63/200 [03:01<07:34,  3.32s/it]Running Inference:  32%|███▏      | 64/200 [03:05<08:09,  3.60s/it]Running Inference:  32%|███▎      | 65/200 [03:08<07:38,  3.39s/it]Running Inference:  33%|███▎      | 66/200 [03:11<07:02,  3.15s/it]Running Inference:  34%|███▎      | 67/200 [03:14<07:11,  3.24s/it]Running Inference:  34%|███▍      | 68/200 [03:16<06:34,  2.99s/it]Running Inference:  34%|███▍      | 69/200 [03:18<05:26,  2.49s/it]Running Inference:  35%|███▌      | 70/200 [03:21<06:07,  2.83s/it]Running Inference:  36%|███▌      | 71/200 [03:26<06:55,  3.22s/it]Running Inference:  36%|███▌      | 72/200 [03:28<06:11,  2.90s/it]Running Inference:  36%|███▋      | 73/200 [03:30<05:41,  2.69s/it]Running Inference:  37%|███▋      | 74/200 [03:34<06:16,  2.99s/it]Running Inference:  38%|███▊      | 75/200 [03:37<06:43,  3.23s/it]Running Inference:  38%|███▊      | 76/200 [03:41<07:05,  3.44s/it]Running Inference:  38%|███▊      | 77/200 [03:43<06:04,  2.97s/it]Running Inference:  39%|███▉      | 78/200 [03:47<06:36,  3.25s/it]Running Inference:  40%|███▉      | 79/200 [03:49<05:34,  2.77s/it]Running Inference:  40%|████      | 80/200 [03:50<04:23,  2.20s/it]Running Inference:  40%|████      | 81/200 [03:52<04:24,  2.22s/it]Running Inference:  41%|████      | 82/200 [03:54<04:06,  2.09s/it]Running Inference:  42%|████▏     | 83/200 [03:55<03:38,  1.87s/it]Running Inference:  42%|████▏     | 84/200 [03:59<04:38,  2.40s/it]Running Inference:  42%|████▎     | 85/200 [04:03<05:42,  2.98s/it]Running Inference:  43%|████▎     | 86/200 [04:06<05:25,  2.86s/it]Running Inference:  44%|████▎     | 87/200 [04:09<06:00,  3.19s/it]Running Inference:  44%|████▍     | 88/200 [04:13<06:08,  3.29s/it]Running Inference:  44%|████▍     | 89/200 [04:15<05:32,  2.99s/it]Running Inference:  45%|████▌     | 90/200 [04:17<04:51,  2.65s/it]Running Inference:  46%|████▌     | 91/200 [04:18<03:51,  2.12s/it]Running Inference:  46%|████▌     | 92/200 [04:22<05:01,  2.79s/it]Running Inference:  46%|████▋     | 93/200 [04:24<04:21,  2.45s/it]Running Inference:  47%|████▋     | 94/200 [04:26<03:58,  2.25s/it]Running Inference:  48%|████▊     | 95/200 [04:27<03:36,  2.07s/it]Running Inference:  48%|████▊     | 96/200 [04:30<03:37,  2.09s/it]Running Inference:  48%|████▊     | 97/200 [04:31<03:22,  1.96s/it]Running Inference:  49%|████▉     | 98/200 [04:35<04:27,  2.62s/it]Running Inference:  50%|████▉     | 99/200 [04:39<05:02,  2.99s/it]Running Inference:  50%|█████     | 100/200 [04:43<05:10,  3.11s/it]Running Inference:  50%|█████     | 101/200 [04:45<04:49,  2.93s/it]Running Inference:  51%|█████     | 102/200 [04:48<04:46,  2.93s/it]Running Inference:  52%|█████▏    | 103/200 [04:49<03:42,  2.29s/it]Running Inference:  52%|█████▏    | 104/200 [04:51<03:30,  2.19s/it]Running Inference:  52%|█████▎    | 105/200 [04:52<03:06,  1.97s/it]Running Inference:  53%|█████▎    | 106/200 [04:54<03:04,  1.96s/it]Running Inference:  54%|█████▎    | 107/200 [04:57<03:28,  2.24s/it]Running Inference:  54%|█████▍    | 108/200 [04:58<02:44,  1.79s/it]Running Inference:  55%|█████▍    | 109/200 [04:59<02:34,  1.70s/it]Running Inference:  55%|█████▌    | 110/200 [05:03<03:14,  2.16s/it]Running Inference:  56%|█████▌    | 111/200 [05:07<04:08,  2.79s/it]Running Inference:  56%|█████▌    | 112/200 [05:11<04:41,  3.19s/it]Running Inference:  56%|█████▋    | 113/200 [05:15<05:08,  3.54s/it]Running Inference:  57%|█████▋    | 114/200 [05:18<04:31,  3.15s/it]Running Inference:  57%|█████▊    | 115/200 [05:20<04:05,  2.89s/it]Running Inference:  58%|█████▊    | 116/200 [05:24<04:28,  3.20s/it]Running Inference:  58%|█████▊    | 117/200 [05:26<04:02,  2.93s/it]Running Inference:  59%|█████▉    | 118/200 [05:30<04:31,  3.31s/it]Running Inference:  60%|█████▉    | 119/200 [05:31<03:31,  2.61s/it]Running Inference:  60%|██████    | 120/200 [05:33<03:12,  2.40s/it]Running Inference:  60%|██████    | 121/200 [05:35<03:07,  2.37s/it]Running Inference:  61%|██████    | 122/200 [05:38<03:00,  2.31s/it]Running Inference:  62%|██████▏   | 123/200 [05:42<03:41,  2.88s/it]Running Inference:  62%|██████▏   | 124/200 [05:44<03:17,  2.60s/it]Running Inference:  62%|██████▎   | 125/200 [05:48<03:49,  3.06s/it]Running Inference:  63%|██████▎   | 126/200 [05:50<03:28,  2.81s/it]Running Inference:  64%|██████▎   | 127/200 [05:51<02:38,  2.18s/it]Running Inference:  64%|██████▍   | 128/200 [05:55<03:23,  2.82s/it]Running Inference:  64%|██████▍   | 129/200 [05:57<03:05,  2.62s/it]Running Inference:  65%|██████▌   | 130/200 [05:59<02:46,  2.38s/it]Running Inference:  66%|██████▌   | 131/200 [06:01<02:37,  2.28s/it]Running Inference:  66%|██████▌   | 132/200 [06:03<02:16,  2.00s/it]Running Inference:  66%|██████▋   | 133/200 [06:06<02:40,  2.40s/it]Running Inference:  67%|██████▋   | 134/200 [06:08<02:36,  2.37s/it]Running Inference:  68%|██████▊   | 135/200 [06:12<03:07,  2.89s/it]Running Inference:  68%|██████▊   | 136/200 [06:15<02:57,  2.77s/it]Running Inference:  68%|██████▊   | 137/200 [06:18<03:00,  2.86s/it]Running Inference:  69%|██████▉   | 138/200 [06:20<02:52,  2.78s/it]Running Inference:  70%|██████▉   | 139/200 [06:25<03:15,  3.21s/it]Running Inference:  70%|███████   | 140/200 [06:29<03:24,  3.41s/it]Running Inference:  70%|███████   | 141/200 [06:33<03:36,  3.67s/it]Running Inference:  71%|███████   | 142/200 [06:37<03:40,  3.80s/it]Running Inference:  72%|███████▏  | 143/200 [06:39<03:13,  3.39s/it]Running Inference:  72%|███████▏  | 144/200 [06:41<02:36,  2.79s/it]Running Inference:  72%|███████▎  | 145/200 [06:43<02:27,  2.68s/it]Running Inference:  73%|███████▎  | 146/200 [06:47<02:49,  3.14s/it]Running Inference:  74%|███████▎  | 147/200 [06:50<02:30,  2.84s/it]Running Inference:  74%|███████▍  | 148/200 [06:54<02:45,  3.19s/it]Running Inference:  74%|███████▍  | 149/200 [06:54<02:03,  2.42s/it]Running Inference:  75%|███████▌  | 150/200 [06:56<01:46,  2.14s/it]Running Inference:  76%|███████▌  | 151/200 [06:58<01:45,  2.15s/it]Running Inference:  76%|███████▌  | 152/200 [07:02<02:08,  2.68s/it]Running Inference:  76%|███████▋  | 153/200 [07:06<02:26,  3.11s/it]Running Inference:  77%|███████▋  | 154/200 [07:10<02:39,  3.47s/it]Running Inference:  78%|███████▊  | 155/200 [07:14<02:45,  3.67s/it]Running Inference:  78%|███████▊  | 156/200 [07:17<02:34,  3.51s/it]Running Inference:  78%|███████▊  | 157/200 [07:19<02:07,  2.98s/it]Running Inference:  79%|███████▉  | 158/200 [07:22<02:02,  2.92s/it]Running Inference:  80%|███████▉  | 159/200 [07:25<02:07,  3.11s/it]Running Inference:  80%|████████  | 160/200 [07:30<02:16,  3.41s/it]Running Inference:  80%|████████  | 161/200 [07:32<01:58,  3.05s/it]Running Inference:  81%|████████  | 162/200 [07:32<01:27,  2.30s/it]Running Inference:  82%|████████▏ | 163/200 [07:34<01:18,  2.13s/it]Running Inference:  82%|████████▏ | 164/200 [07:36<01:16,  2.13s/it]Running Inference:  82%|████████▎ | 165/200 [07:40<01:35,  2.73s/it]Running Inference:  83%|████████▎ | 166/200 [07:44<01:44,  3.08s/it]Running Inference:  84%|████████▎ | 167/200 [07:48<01:52,  3.40s/it]Running Inference:  84%|████████▍ | 168/200 [07:52<01:54,  3.58s/it]Running Inference:  84%|████████▍ | 169/200 [07:55<01:42,  3.30s/it]Running Inference:  85%|████████▌ | 170/200 [07:57<01:28,  2.94s/it]Running Inference:  86%|████████▌ | 171/200 [08:00<01:21,  2.82s/it]Running Inference:  86%|████████▌ | 172/200 [08:04<01:30,  3.23s/it]Running Inference:  86%|████████▋ | 173/200 [08:06<01:17,  2.89s/it]Running Inference:  87%|████████▋ | 174/200 [08:08<01:08,  2.62s/it]Running Inference:  88%|████████▊ | 175/200 [08:12<01:17,  3.11s/it]Running Inference:  88%|████████▊ | 176/200 [08:16<01:21,  3.42s/it]Running Inference:  88%|████████▊ | 177/200 [08:21<01:24,  3.69s/it]Running Inference:  89%|████████▉ | 178/200 [08:23<01:11,  3.27s/it]Running Inference:  90%|████████▉ | 179/200 [08:25<01:02,  2.98s/it]Running Inference:  90%|█████████ | 180/200 [08:29<01:03,  3.15s/it]Running Inference:  90%|█████████ | 181/200 [08:31<00:54,  2.86s/it]Running Inference:  91%|█████████ | 182/200 [08:33<00:46,  2.56s/it]Running Inference:  92%|█████████▏| 183/200 [08:35<00:39,  2.34s/it]Running Inference:  92%|█████████▏| 184/200 [08:39<00:45,  2.84s/it]Running Inference:  92%|█████████▎| 185/200 [08:40<00:36,  2.45s/it]Running Inference:  93%|█████████▎| 186/200 [08:42<00:32,  2.35s/it]Running Inference:  94%|█████████▎| 187/200 [08:46<00:36,  2.83s/it]Running Inference:  94%|█████████▍| 188/200 [08:47<00:26,  2.20s/it]Running Inference:  94%|█████████▍| 189/200 [08:49<00:24,  2.20s/it]Running Inference:  95%|█████████▌| 190/200 [08:50<00:18,  1.87s/it]Running Inference:  96%|█████████▌| 191/200 [08:52<00:17,  1.95s/it]Running Inference:  96%|█████████▌| 192/200 [08:55<00:15,  1.98s/it]Running Inference:  96%|█████████▋| 193/200 [08:59<00:18,  2.70s/it]Running Inference:  97%|█████████▋| 194/200 [09:02<00:16,  2.68s/it]Running Inference:  98%|█████████▊| 195/200 [09:05<00:13,  2.78s/it]Running Inference:  98%|█████████▊| 196/200 [09:07<00:10,  2.63s/it]Running Inference:  98%|█████████▊| 197/200 [09:08<00:06,  2.27s/it]Running Inference:  99%|█████████▉| 198/200 [09:12<00:05,  2.75s/it]Running Inference: 100%|█████████▉| 199/200 [09:15<00:02,  2.70s/it]Running Inference: 100%|██████████| 200/200 [09:17<00:00,  2.67s/it]Running Inference: 100%|██████████| 200/200 [09:17<00:00,  2.79s/it]
2025-12-13 18:49:56,065 - INFO - Inference completed.
2025-12-13 18:49:56,074 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-13 18:49:56,074 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:49:56,078 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-13 18:49:56,078 - INFO - Metrics:
18.12
2025-12-13 18:49:56,080 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=hotpotqa, ratio=0.3) on GPU 4

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:50:02,462 - INFO - Set deterministic seeds to 42
2025-12-13 18:50:02,463 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:50:02,463 - INFO - Starting evaluation run...
2025-12-13 18:50:02,463 - INFO - Output directory set to: longbenchresult
2025-12-13 18:50:02,463 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-13 18:50:02,463 - INFO - KV Press 'knorm' setup.
2025-12-13 18:50:02,463 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:50:02,463 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.43it/s]
Device set to use cuda:0
2025-12-13 18:50:15,910 - INFO - Model pipeline loaded.
2025-12-13 18:50:15,910 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:50:21,231 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:50:21,231 - INFO - Dataset processed with 200 entries.
2025-12-13 18:50:21,266 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<12:17,  3.71s/it]Running Inference:   1%|          | 2/200 [00:05<08:37,  2.61s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:02,  2.45s/it]Running Inference:   2%|▏         | 4/200 [00:12<10:19,  3.16s/it]Running Inference:   2%|▎         | 5/200 [00:14<10:00,  3.08s/it]Running Inference:   3%|▎         | 6/200 [00:18<10:07,  3.13s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:53,  2.76s/it]Running Inference:   4%|▍         | 8/200 [00:23<09:19,  2.91s/it]Running Inference:   4%|▍         | 9/200 [00:25<08:06,  2.55s/it]Running Inference:   5%|▌         | 10/200 [00:26<06:40,  2.11s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:29<07:57,  2.53s/it]Running Inference:   6%|▌         | 12/200 [00:31<06:40,  2.13s/it]Running Inference:   6%|▋         | 13/200 [00:35<08:27,  2.72s/it]Running Inference:   7%|▋         | 14/200 [00:37<08:05,  2.61s/it]Running Inference:   8%|▊         | 15/200 [00:41<09:32,  3.10s/it]Running Inference:   8%|▊         | 16/200 [00:44<08:55,  2.91s/it]Running Inference:   8%|▊         | 17/200 [00:48<10:03,  3.30s/it]Running Inference:   9%|▉         | 18/200 [00:52<10:57,  3.61s/it]Running Inference:  10%|▉         | 19/200 [00:55<10:11,  3.38s/it]Running Inference:  10%|█         | 20/200 [00:58<09:59,  3.33s/it]Running Inference:  10%|█         | 21/200 [01:01<09:26,  3.16s/it]Running Inference:  11%|█         | 22/200 [01:04<08:52,  2.99s/it]Running Inference:  12%|█▏        | 23/200 [01:05<07:42,  2.61s/it]Running Inference:  12%|█▏        | 24/200 [01:09<08:58,  3.06s/it]Running Inference:  12%|█▎        | 25/200 [01:13<09:14,  3.17s/it]Running Inference:  13%|█▎        | 26/200 [01:14<07:21,  2.54s/it]Running Inference:  14%|█▎        | 27/200 [01:18<08:15,  2.86s/it]Running Inference:  14%|█▍        | 28/200 [01:20<07:49,  2.73s/it]Running Inference:  14%|█▍        | 29/200 [01:24<09:04,  3.19s/it]Running Inference:  15%|█▌        | 30/200 [01:29<09:59,  3.53s/it]Running Inference:  16%|█▌        | 31/200 [01:30<08:03,  2.86s/it]Running Inference:  16%|█▌        | 32/200 [01:34<09:11,  3.28s/it]Running Inference:  16%|█▋        | 33/200 [01:38<09:57,  3.58s/it]Running Inference:  17%|█▋        | 34/200 [01:41<09:00,  3.26s/it]Running Inference:  18%|█▊        | 35/200 [01:45<09:48,  3.56s/it]Running Inference:  18%|█▊        | 36/200 [01:48<09:08,  3.35s/it]Running Inference:  18%|█▊        | 37/200 [01:50<08:16,  3.05s/it]Running Inference:  19%|█▉        | 38/200 [01:53<07:32,  2.80s/it]Running Inference:  20%|█▉        | 39/200 [01:55<07:10,  2.68s/it]Running Inference:  20%|██        | 40/200 [01:57<06:56,  2.60s/it]Running Inference:  20%|██        | 41/200 [01:59<06:09,  2.33s/it]Running Inference:  21%|██        | 42/200 [02:01<05:59,  2.28s/it]Running Inference:  22%|██▏       | 43/200 [02:04<06:41,  2.56s/it]Running Inference:  22%|██▏       | 44/200 [02:07<06:38,  2.56s/it]Running Inference:  22%|██▎       | 45/200 [02:11<07:45,  3.00s/it]Running Inference:  23%|██▎       | 46/200 [02:13<07:09,  2.79s/it]Running Inference:  24%|██▎       | 47/200 [02:16<06:39,  2.61s/it]Running Inference:  24%|██▍       | 48/200 [02:19<07:11,  2.84s/it]Running Inference:  24%|██▍       | 49/200 [02:23<08:03,  3.20s/it]Running Inference:  25%|██▌       | 50/200 [02:27<08:21,  3.34s/it]Running Inference:  26%|██▌       | 51/200 [02:30<08:30,  3.43s/it]Running Inference:  26%|██▌       | 52/200 [02:32<07:25,  3.01s/it]Running Inference:  26%|██▋       | 53/200 [02:35<07:23,  3.01s/it]Running Inference:  27%|██▋       | 54/200 [02:39<07:34,  3.12s/it]Running Inference:  28%|██▊       | 55/200 [02:43<08:17,  3.43s/it]Running Inference:  28%|██▊       | 56/200 [02:46<08:18,  3.46s/it]Running Inference:  28%|██▊       | 57/200 [02:50<08:20,  3.50s/it]Running Inference:  29%|██▉       | 58/200 [02:52<06:54,  2.92s/it]Running Inference:  30%|██▉       | 59/200 [02:55<07:04,  3.01s/it]Running Inference:  30%|███       | 60/200 [02:57<06:34,  2.82s/it]Running Inference:  30%|███       | 61/200 [02:59<05:59,  2.59s/it]Running Inference:  31%|███       | 62/200 [03:03<07:07,  3.10s/it]Running Inference:  32%|███▏      | 63/200 [03:07<07:23,  3.24s/it]Running Inference:  32%|███▏      | 64/200 [03:11<08:00,  3.53s/it]Running Inference:  32%|███▎      | 65/200 [03:14<07:14,  3.22s/it]Running Inference:  33%|███▎      | 66/200 [03:17<06:57,  3.12s/it]Running Inference:  34%|███▎      | 67/200 [03:20<07:07,  3.22s/it]Running Inference:  34%|███▍      | 68/200 [03:24<07:32,  3.43s/it]Running Inference:  34%|███▍      | 69/200 [03:27<07:20,  3.37s/it]Running Inference:  35%|███▌      | 70/200 [03:31<07:26,  3.44s/it]Running Inference:  36%|███▌      | 71/200 [03:35<07:50,  3.65s/it]Running Inference:  36%|███▌      | 72/200 [03:37<06:56,  3.25s/it]Running Inference:  36%|███▋      | 73/200 [03:41<07:29,  3.54s/it]Running Inference:  37%|███▋      | 74/200 [03:45<07:30,  3.58s/it]Running Inference:  38%|███▊      | 75/200 [03:49<07:34,  3.64s/it]Running Inference:  38%|███▊      | 76/200 [03:53<07:40,  3.72s/it]Running Inference:  38%|███▊      | 77/200 [03:55<06:30,  3.17s/it]Running Inference:  39%|███▉      | 78/200 [03:59<06:54,  3.40s/it]Running Inference:  40%|███▉      | 79/200 [04:02<07:00,  3.48s/it]Running Inference:  40%|████      | 80/200 [04:05<06:34,  3.29s/it]Running Inference:  40%|████      | 81/200 [04:09<07:05,  3.58s/it]Running Inference:  41%|████      | 82/200 [04:11<05:58,  3.04s/it]Running Inference:  42%|████▏     | 83/200 [04:14<06:02,  3.10s/it]Running Inference:  42%|████▏     | 84/200 [04:18<06:18,  3.26s/it]Running Inference:  42%|████▎     | 85/200 [04:22<06:51,  3.58s/it]Running Inference:  43%|████▎     | 86/200 [04:27<07:09,  3.77s/it]Running Inference:  44%|████▎     | 87/200 [04:31<07:15,  3.85s/it]Running Inference:  44%|████▍     | 88/200 [04:32<06:01,  3.23s/it]Running Inference:  44%|████▍     | 89/200 [04:35<05:25,  2.93s/it]Running Inference:  45%|████▌     | 90/200 [04:38<05:51,  3.19s/it]Running Inference:  46%|████▌     | 91/200 [04:41<05:38,  3.11s/it]Running Inference:  46%|████▌     | 92/200 [04:45<05:56,  3.30s/it]Running Inference:  46%|████▋     | 93/200 [04:49<06:05,  3.42s/it]Running Inference:  47%|████▋     | 94/200 [04:51<05:11,  2.94s/it]Running Inference:  48%|████▊     | 95/200 [04:52<04:27,  2.55s/it]Running Inference:  48%|████▊     | 96/200 [04:54<04:12,  2.43s/it]Running Inference:  48%|████▊     | 97/200 [04:56<03:45,  2.19s/it]Running Inference:  49%|████▉     | 98/200 [05:00<04:43,  2.78s/it]Running Inference:  50%|████▉     | 99/200 [05:04<05:12,  3.09s/it]Running Inference:  50%|█████     | 100/200 [05:08<05:41,  3.41s/it]Running Inference:  50%|█████     | 101/200 [05:12<06:02,  3.66s/it]Running Inference:  51%|█████     | 102/200 [05:17<06:12,  3.80s/it]Running Inference:  52%|█████▏    | 103/200 [05:17<04:41,  2.90s/it]Running Inference:  52%|█████▏    | 104/200 [05:21<04:58,  3.11s/it]Running Inference:  52%|█████▎    | 105/200 [05:22<04:08,  2.62s/it]Running Inference:  53%|█████▎    | 106/200 [05:25<04:03,  2.59s/it]Running Inference:  54%|█████▎    | 107/200 [05:28<04:10,  2.69s/it]Running Inference:  54%|█████▍    | 108/200 [05:29<03:24,  2.23s/it]Running Inference:  55%|█████▍    | 109/200 [05:31<03:02,  2.00s/it]Running Inference:  55%|█████▌    | 110/200 [05:34<03:33,  2.37s/it]Running Inference:  56%|█████▌    | 111/200 [05:38<04:20,  2.93s/it]Running Inference:  56%|█████▌    | 112/200 [05:42<04:49,  3.29s/it]Running Inference:  56%|█████▋    | 113/200 [05:45<04:29,  3.09s/it]Running Inference:  57%|█████▋    | 114/200 [05:49<04:56,  3.44s/it]Running Inference:  57%|█████▊    | 115/200 [05:51<04:22,  3.09s/it]Running Inference:  58%|█████▊    | 116/200 [05:55<04:40,  3.34s/it]Running Inference:  58%|█████▊    | 117/200 [05:58<04:11,  3.04s/it]Running Inference:  59%|█████▉    | 118/200 [06:02<04:37,  3.39s/it]Running Inference:  60%|█████▉    | 119/200 [06:03<03:34,  2.64s/it]Running Inference:  60%|██████    | 120/200 [06:05<03:14,  2.43s/it]Running Inference:  60%|██████    | 121/200 [06:07<03:06,  2.36s/it]Running Inference:  61%|██████    | 122/200 [06:11<03:46,  2.91s/it]Running Inference:  62%|██████▏   | 123/200 [06:15<04:13,  3.29s/it]Running Inference:  62%|██████▏   | 124/200 [06:19<04:26,  3.51s/it]Running Inference:  62%|██████▎   | 125/200 [06:23<04:37,  3.70s/it]Running Inference:  63%|██████▎   | 126/200 [06:28<04:45,  3.86s/it]Running Inference:  64%|██████▎   | 127/200 [06:28<03:32,  2.91s/it]Running Inference:  64%|██████▍   | 128/200 [06:31<03:35,  2.99s/it]Running Inference:  64%|██████▍   | 129/200 [06:34<03:23,  2.87s/it]Running Inference:  65%|██████▌   | 130/200 [06:36<02:58,  2.55s/it]Running Inference:  66%|██████▌   | 131/200 [06:40<03:23,  2.95s/it]Running Inference:  66%|██████▌   | 132/200 [06:43<03:28,  3.06s/it]Running Inference:  66%|██████▋   | 133/200 [06:46<03:29,  3.13s/it]Running Inference:  67%|██████▋   | 134/200 [06:49<03:10,  2.88s/it]Running Inference:  68%|██████▊   | 135/200 [06:51<02:50,  2.62s/it]Running Inference:  68%|██████▊   | 136/200 [06:53<02:45,  2.58s/it]Running Inference:  68%|██████▊   | 137/200 [06:56<02:52,  2.73s/it]Running Inference:  69%|██████▉   | 138/200 [06:59<02:46,  2.69s/it]Running Inference:  70%|██████▉   | 139/200 [07:03<03:11,  3.14s/it]Running Inference:  70%|███████   | 140/200 [07:07<03:21,  3.36s/it]Running Inference:  70%|███████   | 141/200 [07:11<03:33,  3.62s/it]Running Inference:  71%|███████   | 142/200 [07:15<03:38,  3.76s/it]Running Inference:  72%|███████▏  | 143/200 [07:18<03:11,  3.37s/it]Running Inference:  72%|███████▏  | 144/200 [07:19<02:28,  2.66s/it]Running Inference:  72%|███████▎  | 145/200 [07:21<02:21,  2.58s/it]Running Inference:  73%|███████▎  | 146/200 [07:25<02:45,  3.07s/it]Running Inference:  74%|███████▎  | 147/200 [07:29<03:00,  3.40s/it]Running Inference:  74%|███████▍  | 148/200 [07:33<03:06,  3.58s/it]Running Inference:  74%|███████▍  | 149/200 [07:36<02:49,  3.32s/it]Running Inference:  75%|███████▌  | 150/200 [07:38<02:18,  2.77s/it]Running Inference:  76%|███████▌  | 151/200 [07:39<01:57,  2.39s/it]Running Inference:  76%|███████▌  | 152/200 [07:43<02:16,  2.85s/it]Running Inference:  76%|███████▋  | 153/200 [07:47<02:31,  3.22s/it]Running Inference:  77%|███████▋  | 154/200 [07:51<02:43,  3.55s/it]Running Inference:  78%|███████▊  | 155/200 [07:56<02:47,  3.72s/it]Running Inference:  78%|███████▊  | 156/200 [07:59<02:35,  3.53s/it]Running Inference:  78%|███████▊  | 157/200 [08:02<02:31,  3.52s/it]Running Inference:  79%|███████▉  | 158/200 [08:05<02:18,  3.30s/it]Running Inference:  80%|███████▉  | 159/200 [08:08<02:18,  3.37s/it]Running Inference:  80%|████████  | 160/200 [08:13<02:23,  3.59s/it]Running Inference:  80%|████████  | 161/200 [08:15<02:11,  3.37s/it]Running Inference:  81%|████████  | 162/200 [08:18<02:00,  3.17s/it]Running Inference:  82%|████████▏ | 163/200 [08:22<02:04,  3.36s/it]Running Inference:  82%|████████▏ | 164/200 [08:26<02:04,  3.47s/it]Running Inference:  82%|████████▎ | 165/200 [08:30<02:08,  3.67s/it]Running Inference:  83%|████████▎ | 166/200 [08:32<01:46,  3.14s/it]Running Inference:  84%|████████▎ | 167/200 [08:36<01:53,  3.44s/it]Running Inference:  84%|████████▍ | 168/200 [08:40<01:55,  3.61s/it]Running Inference:  84%|████████▍ | 169/200 [08:43<01:46,  3.44s/it]Running Inference:  85%|████████▌ | 170/200 [08:47<01:48,  3.61s/it]Running Inference:  86%|████████▌ | 171/200 [08:50<01:36,  3.34s/it]Running Inference:  86%|████████▌ | 172/200 [08:52<01:27,  3.13s/it]Running Inference:  86%|████████▋ | 173/200 [08:55<01:20,  2.98s/it]Running Inference:  87%|████████▋ | 174/200 [08:59<01:23,  3.20s/it]Running Inference:  88%|████████▊ | 175/200 [09:03<01:27,  3.51s/it]Running Inference:  88%|████████▊ | 176/200 [09:07<01:28,  3.70s/it]Running Inference:  88%|████████▊ | 177/200 [09:11<01:29,  3.88s/it]Running Inference:  89%|████████▉ | 178/200 [09:15<01:27,  3.98s/it]Running Inference:  90%|████████▉ | 179/200 [09:18<01:12,  3.47s/it]Running Inference:  90%|█████████ | 180/200 [09:21<01:09,  3.49s/it]Running Inference:  90%|█████████ | 181/200 [09:23<00:58,  3.10s/it]Running Inference:  91%|█████████ | 182/200 [09:27<00:59,  3.31s/it]Running Inference:  92%|█████████▏| 183/200 [09:31<00:57,  3.41s/it]Running Inference:  92%|█████████▏| 184/200 [09:35<00:57,  3.58s/it]Running Inference:  92%|█████████▎| 185/200 [09:36<00:44,  2.97s/it]Running Inference:  93%|█████████▎| 186/200 [09:40<00:42,  3.01s/it]Running Inference:  94%|█████████▎| 187/200 [09:43<00:42,  3.30s/it]Running Inference:  94%|█████████▍| 188/200 [09:46<00:37,  3.15s/it]Running Inference:  94%|█████████▍| 189/200 [09:48<00:31,  2.86s/it]Running Inference:  95%|█████████▌| 190/200 [09:50<00:23,  2.36s/it]Running Inference:  96%|█████████▌| 191/200 [09:54<00:25,  2.85s/it]Running Inference:  96%|█████████▌| 192/200 [09:57<00:23,  2.99s/it]Running Inference:  96%|█████████▋| 193/200 [10:01<00:23,  3.40s/it]Running Inference:  97%|█████████▋| 194/200 [10:03<00:18,  3.01s/it]Running Inference:  98%|█████████▊| 195/200 [10:06<00:15,  3.01s/it]Running Inference:  98%|█████████▊| 196/200 [10:09<00:11,  2.79s/it]Running Inference:  98%|█████████▊| 197/200 [10:10<00:07,  2.38s/it]Running Inference:  99%|█████████▉| 198/200 [10:14<00:05,  2.82s/it]Running Inference: 100%|█████████▉| 199/200 [10:17<00:02,  2.78s/it]Running Inference: 100%|██████████| 200/200 [10:19<00:00,  2.72s/it]Running Inference: 100%|██████████| 200/200 [10:19<00:00,  3.10s/it]
2025-12-13 19:00:41,036 - INFO - Inference completed.
2025-12-13 19:00:41,045 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-13 19:00:41,045 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:00:41,050 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-13 19:00:41,050 - INFO - Metrics:
9.34
2025-12-13 19:00:41,051 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=hotpotqa, ratio=0.5) on GPU 4


========================================
LongBench Task: multifieldqa_en
========================================
----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:00:47,586 - INFO - Set deterministic seeds to 42
2025-12-13 19:00:47,586 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:00:47,586 - INFO - Starting evaluation run...
2025-12-13 19:00:47,587 - INFO - Output directory set to: longbenchresult
2025-12-13 19:00:47,587 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 19:00:47,587 - INFO - KV Press 'knorm' setup.
2025-12-13 19:00:47,587 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:00:47,587 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.96it/s]
Device set to use cuda:0
2025-12-13 19:00:59,926 - INFO - Model pipeline loaded.
2025-12-13 19:00:59,926 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:01:06,341 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:01:06,341 - INFO - Dataset processed with 150 entries.
2025-12-13 19:01:06,353 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<04:03,  2.20s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:20,  1.83s/it]Running Inference:   3%|▎         | 3/112 [00:05<03:22,  1.86s/it]Running Inference:   4%|▎         | 4/112 [00:08<03:55,  2.18s/it]Running Inference:   4%|▍         | 5/112 [00:12<05:07,  2.88s/it]Running Inference:   5%|▌         | 6/112 [00:14<04:17,  2.43s/it]Running Inference:   6%|▋         | 7/112 [00:19<05:43,  3.27s/it]Running Inference:   7%|▋         | 8/112 [00:20<04:41,  2.70s/it]Running Inference:   8%|▊         | 9/112 [00:21<03:41,  2.15s/it]Running Inference:   9%|▉         | 10/112 [00:25<04:31,  2.66s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:26<03:40,  2.18s/it]Running Inference:  11%|█         | 12/112 [00:27<03:16,  1.96s/it]Running Inference:  12%|█▏        | 13/112 [00:29<02:56,  1.78s/it]Running Inference:  12%|█▎        | 14/112 [00:30<02:27,  1.50s/it]Running Inference:  13%|█▎        | 15/112 [00:30<01:57,  1.21s/it]Running Inference:  14%|█▍        | 16/112 [00:31<01:44,  1.08s/it]Running Inference:  15%|█▌        | 17/112 [00:31<01:24,  1.12it/s]Running Inference:  16%|█▌        | 18/112 [00:32<01:28,  1.07it/s]Running Inference:  17%|█▋        | 19/112 [00:37<03:11,  2.06s/it]Running Inference:  18%|█▊        | 20/112 [00:38<02:52,  1.88s/it]Running Inference:  19%|█▉        | 21/112 [00:40<02:53,  1.90s/it]Running Inference:  20%|█▉        | 22/112 [00:42<02:46,  1.85s/it]Running Inference:  21%|██        | 23/112 [00:44<02:37,  1.76s/it]Running Inference:  21%|██▏       | 24/112 [00:45<02:17,  1.57s/it]Running Inference:  22%|██▏       | 25/112 [00:47<02:20,  1.61s/it]Running Inference:  23%|██▎       | 26/112 [00:52<03:45,  2.62s/it]Running Inference:  24%|██▍       | 27/112 [00:54<03:41,  2.61s/it]Running Inference:  25%|██▌       | 28/112 [00:55<03:07,  2.24s/it]Running Inference:  26%|██▌       | 29/112 [00:57<02:45,  1.99s/it]Running Inference:  27%|██▋       | 30/112 [01:00<03:10,  2.33s/it]Running Inference:  28%|██▊       | 31/112 [01:05<04:03,  3.00s/it]Running Inference:  29%|██▊       | 32/112 [01:13<06:19,  4.74s/it]Running Inference:  29%|██▉       | 33/112 [01:15<05:05,  3.87s/it]Running Inference:  30%|███       | 34/112 [01:17<04:06,  3.17s/it]Running Inference:  31%|███▏      | 35/112 [01:21<04:36,  3.58s/it]Running Inference:  32%|███▏      | 36/112 [01:22<03:36,  2.85s/it]Running Inference:  33%|███▎      | 37/112 [01:23<02:39,  2.13s/it]Running Inference:  34%|███▍      | 38/112 [01:24<02:07,  1.72s/it]Running Inference:  35%|███▍      | 39/112 [01:29<03:35,  2.96s/it]Running Inference:  36%|███▌      | 40/112 [01:34<04:11,  3.49s/it]Running Inference:  37%|███▋      | 41/112 [01:37<03:50,  3.25s/it]Running Inference:  38%|███▊      | 42/112 [01:40<03:53,  3.33s/it]Running Inference:  38%|███▊      | 43/112 [01:43<03:28,  3.01s/it]Running Inference:  39%|███▉      | 44/112 [01:46<03:29,  3.09s/it]Running Inference:  40%|████      | 45/112 [01:50<03:51,  3.46s/it]Running Inference:  41%|████      | 46/112 [01:51<03:03,  2.79s/it]Running Inference:  42%|████▏     | 47/112 [01:53<02:26,  2.26s/it]Running Inference:  43%|████▎     | 48/112 [01:54<02:16,  2.13s/it]Running Inference:  44%|████▍     | 49/112 [01:56<02:03,  1.96s/it]Running Inference:  45%|████▍     | 50/112 [01:57<01:51,  1.80s/it]Running Inference:  46%|████▌     | 51/112 [01:59<01:45,  1.72s/it]Running Inference:  46%|████▋     | 52/112 [02:02<02:02,  2.05s/it]Running Inference:  47%|████▋     | 53/112 [02:03<01:51,  1.89s/it]Running Inference:  48%|████▊     | 54/112 [02:05<01:48,  1.86s/it]Running Inference:  49%|████▉     | 55/112 [02:07<01:49,  1.92s/it]Running Inference:  50%|█████     | 56/112 [02:08<01:27,  1.56s/it]Running Inference:  51%|█████     | 57/112 [02:09<01:24,  1.54s/it]Running Inference:  52%|█████▏    | 58/112 [02:12<01:38,  1.83s/it]Running Inference:  53%|█████▎    | 59/112 [02:14<01:41,  1.92s/it]Running Inference:  54%|█████▎    | 60/112 [02:16<01:38,  1.89s/it]Running Inference:  54%|█████▍    | 61/112 [02:17<01:32,  1.82s/it]Running Inference:  55%|█████▌    | 62/112 [02:19<01:24,  1.69s/it]Running Inference:  56%|█████▋    | 63/112 [02:21<01:28,  1.80s/it]Running Inference:  57%|█████▋    | 64/112 [02:24<01:53,  2.36s/it]Running Inference:  58%|█████▊    | 65/112 [02:26<01:43,  2.21s/it]Running Inference:  59%|█████▉    | 66/112 [02:30<01:55,  2.52s/it]Running Inference:  60%|█████▉    | 67/112 [02:31<01:41,  2.25s/it]Running Inference:  61%|██████    | 68/112 [02:35<02:04,  2.83s/it]Running Inference:  62%|██████▏   | 69/112 [02:36<01:37,  2.28s/it]Running Inference:  62%|██████▎   | 70/112 [02:38<01:23,  1.99s/it]Running Inference:  63%|██████▎   | 71/112 [02:42<01:48,  2.64s/it]Running Inference:  64%|██████▍   | 72/112 [02:50<02:47,  4.19s/it]Running Inference:  65%|██████▌   | 73/112 [02:52<02:24,  3.70s/it]Running Inference:  66%|██████▌   | 74/112 [02:55<02:12,  3.49s/it]Running Inference:  67%|██████▋   | 75/112 [03:03<02:51,  4.64s/it]Running Inference:  68%|██████▊   | 76/112 [03:03<02:01,  3.39s/it]Running Inference:  69%|██████▉   | 77/112 [03:04<01:34,  2.71s/it]Running Inference:  70%|██████▉   | 78/112 [03:12<02:29,  4.39s/it]Running Inference:  71%|███████   | 79/112 [03:14<01:54,  3.47s/it]Running Inference:  71%|███████▏  | 80/112 [03:15<01:27,  2.72s/it]Running Inference:  72%|███████▏  | 81/112 [03:17<01:19,  2.57s/it]Running Inference:  73%|███████▎  | 82/112 [03:20<01:22,  2.75s/it]Running Inference:  74%|███████▍  | 83/112 [03:27<01:59,  4.11s/it]Running Inference:  75%|███████▌  | 84/112 [03:32<02:01,  4.32s/it]Running Inference:  76%|███████▌  | 85/112 [03:33<01:24,  3.13s/it]Running Inference:  77%|███████▋  | 86/112 [03:34<01:06,  2.55s/it]Running Inference:  78%|███████▊  | 87/112 [03:35<00:54,  2.19s/it]Running Inference:  79%|███████▊  | 88/112 [03:37<00:49,  2.06s/it]Running Inference:  79%|███████▉  | 89/112 [03:44<01:24,  3.66s/it]Running Inference:  80%|████████  | 90/112 [03:47<01:13,  3.35s/it]Running Inference:  81%|████████▏ | 91/112 [03:50<01:09,  3.29s/it]Running Inference:  82%|████████▏ | 92/112 [03:52<00:59,  2.97s/it]Running Inference:  83%|████████▎ | 93/112 [03:55<00:56,  2.96s/it]Running Inference:  84%|████████▍ | 94/112 [03:56<00:44,  2.45s/it]Running Inference:  85%|████████▍ | 95/112 [03:58<00:37,  2.19s/it]Running Inference:  86%|████████▌ | 96/112 [03:59<00:29,  1.82s/it]Running Inference:  87%|████████▋ | 97/112 [04:04<00:43,  2.92s/it]Running Inference:  88%|████████▊ | 98/112 [04:13<01:03,  4.53s/it]Running Inference:  88%|████████▊ | 99/112 [04:16<00:52,  4.02s/it]Running Inference:  89%|████████▉ | 100/112 [04:23<00:58,  4.90s/it]Running Inference:  90%|█████████ | 101/112 [04:24<00:44,  4.01s/it]Running Inference:  91%|█████████ | 102/112 [04:26<00:34,  3.40s/it]Running Inference:  92%|█████████▏| 103/112 [04:28<00:25,  2.78s/it]Running Inference:  93%|█████████▎| 104/112 [04:29<00:17,  2.25s/it]Running Inference:  94%|█████████▍| 105/112 [04:32<00:17,  2.45s/it]Running Inference:  95%|█████████▍| 106/112 [04:34<00:14,  2.47s/it]Running Inference:  96%|█████████▌| 107/112 [04:37<00:12,  2.53s/it]Running Inference:  96%|█████████▋| 108/112 [04:40<00:10,  2.61s/it]Running Inference:  97%|█████████▋| 109/112 [04:42<00:07,  2.51s/it]Running Inference:  98%|█████████▊| 110/112 [04:43<00:04,  2.16s/it]Running Inference:  99%|█████████▉| 111/112 [04:47<00:02,  2.64s/it]Running Inference: 100%|██████████| 112/112 [04:49<00:00,  2.28s/it]Running Inference: 100%|██████████| 112/112 [04:49<00:00,  2.58s/it]
2025-12-13 19:05:55,389 - INFO - Inference completed.
2025-12-13 19:05:55,396 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 19:05:55,397 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:05:55,402 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 19:05:55,402 - INFO - Metrics:
32.7
2025-12-13 19:05:55,403 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_en, ratio=0.1) on GPU 4

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:06:01,893 - INFO - Set deterministic seeds to 42
2025-12-13 19:06:01,893 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:06:01,893 - INFO - Starting evaluation run...
2025-12-13 19:06:01,893 - INFO - Output directory set to: longbenchresult
2025-12-13 19:06:01,893 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 19:06:01,893 - INFO - KV Press 'knorm' setup.
2025-12-13 19:06:01,893 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:06:01,893 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.62it/s]
Device set to use cuda:0
2025-12-13 19:06:25,978 - INFO - Model pipeline loaded.
2025-12-13 19:06:25,979 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:06:30,746 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:06:30,746 - INFO - Dataset processed with 150 entries.
2025-12-13 19:06:30,759 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<04:39,  2.52s/it]Running Inference:   2%|▏         | 2/112 [00:04<03:32,  1.93s/it]Running Inference:   3%|▎         | 3/112 [00:06<03:45,  2.07s/it]Running Inference:   4%|▎         | 4/112 [00:08<04:09,  2.31s/it]Running Inference:   4%|▍         | 5/112 [00:12<05:12,  2.92s/it]Running Inference:   5%|▌         | 6/112 [00:14<04:19,  2.45s/it]Running Inference:   6%|▋         | 7/112 [00:19<05:44,  3.28s/it]Running Inference:   7%|▋         | 8/112 [00:23<06:20,  3.66s/it]Running Inference:   8%|▊         | 9/112 [00:25<05:03,  2.95s/it]Running Inference:   9%|▉         | 10/112 [00:28<05:05,  2.99s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:29<04:15,  2.53s/it]Running Inference:  11%|█         | 12/112 [00:31<03:37,  2.17s/it]Running Inference:  12%|█▏        | 13/112 [00:32<03:04,  1.87s/it]Running Inference:  12%|█▎        | 14/112 [00:36<04:01,  2.46s/it]Running Inference:  13%|█▎        | 15/112 [00:36<03:02,  1.88s/it]Running Inference:  14%|█▍        | 16/112 [00:40<03:56,  2.47s/it]Running Inference:  15%|█▌        | 17/112 [00:41<02:58,  1.88s/it]Running Inference:  16%|█▌        | 18/112 [00:42<02:33,  1.63s/it]Running Inference:  17%|█▋        | 19/112 [00:46<03:54,  2.52s/it]Running Inference:  18%|█▊        | 20/112 [00:48<03:21,  2.19s/it]Running Inference:  19%|█▉        | 21/112 [00:49<03:06,  2.05s/it]Running Inference:  20%|█▉        | 22/112 [00:51<02:58,  1.99s/it]Running Inference:  21%|██        | 23/112 [00:53<02:44,  1.84s/it]Running Inference:  21%|██▏       | 24/112 [00:54<02:13,  1.51s/it]Running Inference:  22%|██▏       | 25/112 [00:55<02:18,  1.60s/it]Running Inference:  23%|██▎       | 26/112 [00:58<02:37,  1.83s/it]Running Inference:  24%|██▍       | 27/112 [01:03<04:01,  2.84s/it]Running Inference:  25%|██▌       | 28/112 [01:04<03:21,  2.40s/it]Running Inference:  26%|██▌       | 29/112 [01:06<02:54,  2.10s/it]Running Inference:  27%|██▋       | 30/112 [01:08<02:53,  2.12s/it]Running Inference:  28%|██▊       | 31/112 [01:12<03:48,  2.82s/it]Running Inference:  29%|██▊       | 32/112 [01:17<04:28,  3.36s/it]Running Inference:  29%|██▉       | 33/112 [01:19<03:49,  2.90s/it]Running Inference:  30%|███       | 34/112 [01:20<03:13,  2.48s/it]Running Inference:  31%|███▏      | 35/112 [01:25<03:56,  3.07s/it]Running Inference:  32%|███▏      | 36/112 [01:26<03:08,  2.49s/it]Running Inference:  33%|███▎      | 37/112 [01:26<02:22,  1.90s/it]Running Inference:  34%|███▍      | 38/112 [01:27<01:56,  1.57s/it]Running Inference:  35%|███▍      | 39/112 [01:41<06:34,  5.41s/it]Running Inference:  36%|███▌      | 40/112 [01:43<05:00,  4.18s/it]Running Inference:  37%|███▋      | 41/112 [01:48<05:10,  4.38s/it]Running Inference:  38%|███▊      | 42/112 [01:55<06:13,  5.33s/it]Running Inference:  38%|███▊      | 43/112 [01:58<05:05,  4.42s/it]Running Inference:  39%|███▉      | 44/112 [02:01<04:35,  4.05s/it]Running Inference:  40%|████      | 45/112 [02:05<04:35,  4.11s/it]Running Inference:  41%|████      | 46/112 [02:06<03:33,  3.24s/it]Running Inference:  42%|████▏     | 47/112 [02:08<02:57,  2.73s/it]Running Inference:  43%|████▎     | 48/112 [02:12<03:33,  3.33s/it]Running Inference:  44%|████▍     | 49/112 [02:14<02:48,  2.67s/it]Running Inference:  45%|████▍     | 50/112 [02:15<02:22,  2.29s/it]Running Inference:  46%|████▌     | 51/112 [02:16<02:05,  2.06s/it]Running Inference:  46%|████▋     | 52/112 [02:19<02:14,  2.25s/it]Running Inference:  47%|████▋     | 53/112 [02:23<02:47,  2.83s/it]Running Inference:  48%|████▊     | 54/112 [02:26<02:34,  2.66s/it]Running Inference:  49%|████▉     | 55/112 [02:28<02:26,  2.57s/it]Running Inference:  50%|█████     | 56/112 [02:29<01:54,  2.05s/it]Running Inference:  51%|█████     | 57/112 [02:30<01:37,  1.78s/it]Running Inference:  52%|█████▏    | 58/112 [02:33<01:53,  2.10s/it]Running Inference:  53%|█████▎    | 59/112 [02:37<02:26,  2.76s/it]Running Inference:  54%|█████▎    | 60/112 [02:39<02:02,  2.36s/it]Running Inference:  54%|█████▍    | 61/112 [02:43<02:26,  2.88s/it]Running Inference:  55%|█████▌    | 62/112 [02:45<02:11,  2.63s/it]Running Inference:  56%|█████▋    | 63/112 [02:47<01:59,  2.44s/it]Running Inference:  57%|█████▋    | 64/112 [02:51<02:19,  2.91s/it]Running Inference:  58%|█████▊    | 65/112 [02:53<02:02,  2.60s/it]Running Inference:  59%|█████▉    | 66/112 [02:59<02:49,  3.68s/it]Running Inference:  60%|█████▉    | 67/112 [03:00<02:16,  3.02s/it]Running Inference:  61%|██████    | 68/112 [03:03<02:05,  2.85s/it]Running Inference:  62%|██████▏   | 69/112 [03:04<01:38,  2.29s/it]Running Inference:  62%|██████▎   | 70/112 [03:05<01:26,  2.05s/it]Running Inference:  63%|██████▎   | 71/112 [03:09<01:48,  2.65s/it]Running Inference:  64%|██████▍   | 72/112 [03:10<01:24,  2.12s/it]Running Inference:  65%|██████▌   | 73/112 [03:13<01:27,  2.23s/it]Running Inference:  66%|██████▌   | 74/112 [03:16<01:34,  2.48s/it]Running Inference:  67%|██████▋   | 75/112 [03:23<02:20,  3.81s/it]Running Inference:  68%|██████▊   | 76/112 [03:23<01:41,  2.81s/it]Running Inference:  69%|██████▉   | 77/112 [03:25<01:25,  2.45s/it]Running Inference:  70%|██████▉   | 78/112 [03:33<02:21,  4.16s/it]Running Inference:  71%|███████   | 79/112 [03:34<01:48,  3.30s/it]Running Inference:  71%|███████▏  | 80/112 [03:35<01:23,  2.60s/it]Running Inference:  72%|███████▏  | 81/112 [03:39<01:37,  3.15s/it]Running Inference:  73%|███████▎  | 82/112 [03:43<01:34,  3.15s/it]Running Inference:  74%|███████▍  | 83/112 [03:50<02:04,  4.28s/it]Running Inference:  75%|███████▌  | 84/112 [03:55<02:06,  4.50s/it]Running Inference:  76%|███████▌  | 85/112 [03:55<01:29,  3.33s/it]Running Inference:  77%|███████▋  | 86/112 [03:56<01:06,  2.55s/it]Running Inference:  78%|███████▊  | 87/112 [04:00<01:17,  3.11s/it]Running Inference:  79%|███████▊  | 88/112 [04:02<01:04,  2.70s/it]Running Inference:  79%|███████▉  | 89/112 [04:08<01:27,  3.81s/it]Running Inference:  80%|████████  | 90/112 [04:11<01:15,  3.45s/it]Running Inference:  81%|████████▏ | 91/112 [04:14<01:10,  3.35s/it]Running Inference:  82%|████████▏ | 92/112 [04:16<01:00,  3.00s/it]Running Inference:  83%|████████▎ | 93/112 [04:20<01:02,  3.29s/it]Running Inference:  84%|████████▍ | 94/112 [04:25<01:05,  3.63s/it]Running Inference:  85%|████████▍ | 95/112 [04:26<00:51,  3.02s/it]Running Inference:  86%|████████▌ | 96/112 [04:28<00:41,  2.57s/it]Running Inference:  87%|████████▋ | 97/112 [04:33<00:51,  3.42s/it]Running Inference:  88%|████████▊ | 98/112 [04:41<01:07,  4.82s/it]Running Inference:  88%|████████▊ | 99/112 [04:43<00:51,  3.95s/it]Running Inference:  89%|████████▉ | 100/112 [04:50<00:58,  4.85s/it]Running Inference:  90%|█████████ | 101/112 [04:52<00:43,  3.97s/it]Running Inference:  91%|█████████ | 102/112 [04:54<00:33,  3.39s/it]Running Inference:  92%|█████████▏| 103/112 [04:55<00:24,  2.76s/it]Running Inference:  93%|█████████▎| 104/112 [04:56<00:17,  2.23s/it]Running Inference:  94%|█████████▍| 105/112 [05:00<00:17,  2.51s/it]Running Inference:  95%|█████████▍| 106/112 [05:02<00:15,  2.53s/it]Running Inference:  96%|█████████▌| 107/112 [05:05<00:13,  2.65s/it]Running Inference:  96%|█████████▋| 108/112 [05:08<00:10,  2.67s/it]Running Inference:  97%|█████████▋| 109/112 [05:12<00:09,  3.24s/it]Running Inference:  98%|█████████▊| 110/112 [05:14<00:05,  2.66s/it]Running Inference:  99%|█████████▉| 111/112 [05:17<00:02,  2.93s/it]Running Inference: 100%|██████████| 112/112 [05:19<00:00,  2.47s/it]Running Inference: 100%|██████████| 112/112 [05:19<00:00,  2.85s/it]
2025-12-13 19:11:49,982 - INFO - Inference completed.
2025-12-13 19:11:49,990 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-13 19:11:49,990 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:11:49,995 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-13 19:11:49,995 - INFO - Metrics:
28.47
2025-12-13 19:11:49,996 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_en, ratio=0.2) on GPU 4

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:11:56,455 - INFO - Set deterministic seeds to 42
2025-12-13 19:11:56,455 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:11:56,455 - INFO - Starting evaluation run...
2025-12-13 19:11:56,455 - INFO - Output directory set to: longbenchresult
2025-12-13 19:11:56,455 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-13 19:11:56,455 - INFO - KV Press 'knorm' setup.
2025-12-13 19:11:56,455 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:11:56,455 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.07it/s]
Device set to use cuda:0
2025-12-13 19:12:10,566 - INFO - Model pipeline loaded.
2025-12-13 19:12:10,566 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:12:17,145 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:12:17,145 - INFO - Dataset processed with 150 entries.
2025-12-13 19:12:17,158 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<04:27,  2.41s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:27,  1.88s/it]Running Inference:   3%|▎         | 3/112 [00:05<03:17,  1.81s/it]Running Inference:   4%|▎         | 4/112 [00:08<03:46,  2.10s/it]Running Inference:   4%|▍         | 5/112 [00:12<05:02,  2.82s/it]Running Inference:   5%|▌         | 6/112 [00:13<04:15,  2.41s/it]Running Inference:   6%|▋         | 7/112 [00:19<06:03,  3.46s/it]Running Inference:   7%|▋         | 8/112 [00:24<06:37,  3.82s/it]Running Inference:   8%|▊         | 9/112 [00:25<05:15,  3.07s/it]Running Inference:   9%|▉         | 10/112 [00:30<06:16,  3.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:32<05:07,  3.04s/it]Running Inference:  11%|█         | 12/112 [00:33<04:14,  2.54s/it]Running Inference:  12%|█▏        | 13/112 [00:34<03:35,  2.18s/it]Running Inference:  12%|█▎        | 14/112 [00:35<02:52,  1.76s/it]Running Inference:  13%|█▎        | 15/112 [00:36<02:24,  1.49s/it]Running Inference:  14%|█▍        | 16/112 [00:37<02:02,  1.28s/it]Running Inference:  15%|█▌        | 17/112 [00:37<01:42,  1.08s/it]Running Inference:  16%|█▌        | 18/112 [00:39<01:40,  1.07s/it]Running Inference:  17%|█▋        | 19/112 [00:43<03:20,  2.15s/it]Running Inference:  18%|█▊        | 20/112 [00:45<02:58,  1.94s/it]Running Inference:  19%|█▉        | 21/112 [00:46<02:50,  1.88s/it]Running Inference:  20%|█▉        | 22/112 [00:48<02:48,  1.87s/it]Running Inference:  21%|██        | 23/112 [00:50<02:38,  1.78s/it]Running Inference:  21%|██▏       | 24/112 [00:51<02:08,  1.46s/it]Running Inference:  22%|██▏       | 25/112 [00:55<03:36,  2.49s/it]Running Inference:  23%|██▎       | 26/112 [01:00<04:37,  3.23s/it]Running Inference:  24%|██▍       | 27/112 [01:06<05:26,  3.85s/it]Running Inference:  25%|██▌       | 28/112 [01:07<04:29,  3.21s/it]Running Inference:  26%|██▌       | 29/112 [01:09<03:39,  2.64s/it]Running Inference:  27%|██▋       | 30/112 [01:13<04:26,  3.25s/it]Running Inference:  28%|██▊       | 31/112 [01:18<04:54,  3.64s/it]Running Inference:  29%|██▊       | 32/112 [01:29<07:52,  5.90s/it]Running Inference:  29%|██▉       | 33/112 [01:31<06:19,  4.80s/it]Running Inference:  30%|███       | 34/112 [01:33<04:57,  3.81s/it]Running Inference:  31%|███▏      | 35/112 [01:37<05:10,  4.03s/it]Running Inference:  32%|███▏      | 36/112 [01:41<05:04,  4.00s/it]Running Inference:  33%|███▎      | 37/112 [01:42<03:42,  2.96s/it]Running Inference:  34%|███▍      | 38/112 [01:43<02:49,  2.28s/it]Running Inference:  35%|███▍      | 39/112 [01:54<06:07,  5.04s/it]Running Inference:  36%|███▌      | 40/112 [01:56<04:51,  4.04s/it]Running Inference:  37%|███▋      | 41/112 [02:04<06:15,  5.29s/it]Running Inference:  38%|███▊      | 42/112 [02:10<06:22,  5.46s/it]Running Inference:  38%|███▊      | 43/112 [02:14<06:00,  5.22s/it]Running Inference:  39%|███▉      | 44/112 [02:17<05:05,  4.50s/it]Running Inference:  40%|████      | 45/112 [02:22<04:57,  4.44s/it]Running Inference:  41%|████      | 46/112 [02:23<03:49,  3.47s/it]Running Inference:  42%|████▏     | 47/112 [02:24<03:09,  2.92s/it]Running Inference:  43%|████▎     | 48/112 [02:27<02:56,  2.76s/it]Running Inference:  44%|████▍     | 49/112 [02:31<03:23,  3.22s/it]Running Inference:  45%|████▍     | 50/112 [02:32<02:44,  2.65s/it]Running Inference:  46%|████▌     | 51/112 [02:34<02:21,  2.32s/it]Running Inference:  46%|████▋     | 52/112 [02:38<02:51,  2.86s/it]Running Inference:  47%|████▋     | 53/112 [02:43<03:16,  3.33s/it]Running Inference:  48%|████▊     | 54/112 [02:44<02:46,  2.87s/it]Running Inference:  49%|████▉     | 55/112 [02:47<02:35,  2.73s/it]Running Inference:  50%|█████     | 56/112 [02:48<02:00,  2.16s/it]Running Inference:  51%|█████     | 57/112 [02:49<01:42,  1.86s/it]Running Inference:  52%|█████▏    | 58/112 [02:55<02:47,  3.10s/it]Running Inference:  53%|█████▎    | 59/112 [02:58<02:46,  3.14s/it]Running Inference:  54%|█████▎    | 60/112 [03:02<02:57,  3.41s/it]Running Inference:  54%|█████▍    | 61/112 [03:06<03:05,  3.64s/it]Running Inference:  55%|█████▌    | 62/112 [03:08<02:33,  3.08s/it]Running Inference:  56%|█████▋    | 63/112 [03:10<02:12,  2.71s/it]Running Inference:  57%|█████▋    | 64/112 [03:13<02:17,  2.86s/it]Running Inference:  58%|█████▊    | 65/112 [03:15<01:58,  2.52s/it]Running Inference:  59%|█████▉    | 66/112 [03:19<02:20,  3.05s/it]Running Inference:  60%|█████▉    | 67/112 [03:21<02:02,  2.72s/it]Running Inference:  61%|██████    | 68/112 [03:25<02:18,  3.15s/it]Running Inference:  62%|██████▏   | 69/112 [03:26<01:48,  2.53s/it]Running Inference:  62%|██████▎   | 70/112 [03:30<02:06,  3.00s/it]Running Inference:  63%|██████▎   | 71/112 [03:34<02:17,  3.34s/it]Running Inference:  64%|██████▍   | 72/112 [03:35<01:43,  2.59s/it]Running Inference:  65%|██████▌   | 73/112 [03:37<01:33,  2.40s/it]Running Inference:  66%|██████▌   | 74/112 [03:40<01:34,  2.49s/it]Running Inference:  67%|██████▋   | 75/112 [03:46<02:12,  3.58s/it]Running Inference:  68%|██████▊   | 76/112 [03:47<01:35,  2.65s/it]Running Inference:  69%|██████▉   | 77/112 [03:48<01:21,  2.34s/it]Running Inference:  70%|██████▉   | 78/112 [03:55<02:05,  3.68s/it]Running Inference:  71%|███████   | 79/112 [03:56<01:37,  2.97s/it]Running Inference:  71%|███████▏  | 80/112 [03:57<01:15,  2.37s/it]Running Inference:  72%|███████▏  | 81/112 [04:02<01:33,  3.02s/it]Running Inference:  73%|███████▎  | 82/112 [04:06<01:44,  3.48s/it]Running Inference:  74%|███████▍  | 83/112 [04:14<02:16,  4.71s/it]Running Inference:  75%|███████▌  | 84/112 [04:19<02:15,  4.85s/it]Running Inference:  76%|███████▌  | 85/112 [04:20<01:36,  3.58s/it]Running Inference:  77%|███████▋  | 86/112 [04:21<01:14,  2.86s/it]Running Inference:  78%|███████▊  | 87/112 [04:22<00:58,  2.35s/it]Running Inference:  79%|███████▊  | 88/112 [04:27<01:14,  3.11s/it]Running Inference:  79%|███████▉  | 89/112 [04:34<01:41,  4.39s/it]Running Inference:  80%|████████  | 90/112 [04:39<01:36,  4.38s/it]Running Inference:  81%|████████▏ | 91/112 [04:42<01:24,  4.01s/it]Running Inference:  82%|████████▏ | 92/112 [04:44<01:09,  3.47s/it]Running Inference:  83%|████████▎ | 93/112 [04:48<01:09,  3.63s/it]Running Inference:  84%|████████▍ | 94/112 [04:53<01:10,  3.90s/it]Running Inference:  85%|████████▍ | 95/112 [04:54<00:54,  3.21s/it]Running Inference:  86%|████████▌ | 96/112 [04:56<00:43,  2.71s/it]Running Inference:  87%|████████▋ | 97/112 [05:01<00:53,  3.54s/it]Running Inference:  88%|████████▊ | 98/112 [05:09<01:09,  4.96s/it]Running Inference:  88%|████████▊ | 99/112 [05:12<00:54,  4.21s/it]Running Inference:  89%|████████▉ | 100/112 [05:19<01:01,  5.12s/it]Running Inference:  90%|█████████ | 101/112 [05:21<00:46,  4.20s/it]Running Inference:  91%|█████████ | 102/112 [05:23<00:35,  3.59s/it]Running Inference:  92%|█████████▏| 103/112 [05:25<00:27,  3.01s/it]Running Inference:  93%|█████████▎| 104/112 [05:26<00:19,  2.41s/it]Running Inference:  94%|█████████▍| 105/112 [05:29<00:17,  2.49s/it]Running Inference:  95%|█████████▍| 106/112 [05:31<00:14,  2.46s/it]Running Inference:  96%|█████████▌| 107/112 [05:34<00:12,  2.49s/it]Running Inference:  96%|█████████▋| 108/112 [05:36<00:10,  2.55s/it]Running Inference:  97%|█████████▋| 109/112 [05:41<00:09,  3.18s/it]Running Inference:  98%|█████████▊| 110/112 [05:43<00:05,  2.87s/it]Running Inference:  99%|█████████▉| 111/112 [05:47<00:03,  3.06s/it]Running Inference: 100%|██████████| 112/112 [05:48<00:00,  2.60s/it]Running Inference: 100%|██████████| 112/112 [05:48<00:00,  3.11s/it]
2025-12-13 19:18:05,836 - INFO - Inference completed.
2025-12-13 19:18:05,844 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-13 19:18:05,844 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:18:05,849 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-13 19:18:05,849 - INFO - Metrics:
24.38
2025-12-13 19:18:05,851 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_en, ratio=0.3) on GPU 4

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:18:12,328 - INFO - Set deterministic seeds to 42
2025-12-13 19:18:12,328 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:18:12,328 - INFO - Starting evaluation run...
2025-12-13 19:18:12,328 - INFO - Output directory set to: longbenchresult
2025-12-13 19:18:12,328 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-13 19:18:12,328 - INFO - KV Press 'knorm' setup.
2025-12-13 19:18:12,328 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:18:12,328 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.51it/s]
Device set to use cuda:0
2025-12-13 19:18:26,954 - INFO - Model pipeline loaded.
2025-12-13 19:18:26,955 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:18:32,126 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:18:32,126 - INFO - Dataset processed with 150 entries.
2025-12-13 19:18:32,138 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<04:25,  2.39s/it]Running Inference:   2%|▏         | 2/112 [00:06<06:09,  3.36s/it]Running Inference:   3%|▎         | 3/112 [00:08<04:50,  2.67s/it]Running Inference:   4%|▎         | 4/112 [00:13<06:23,  3.55s/it]Running Inference:   4%|▍         | 5/112 [00:17<06:40,  3.74s/it]Running Inference:   5%|▌         | 6/112 [00:18<05:17,  2.99s/it]Running Inference:   6%|▋         | 7/112 [00:24<06:42,  3.84s/it]Running Inference:   7%|▋         | 8/112 [00:25<05:20,  3.08s/it]Running Inference:   8%|▊         | 9/112 [00:27<04:24,  2.57s/it]Running Inference:   9%|▉         | 10/112 [00:30<04:28,  2.63s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:31<03:38,  2.16s/it]Running Inference:  11%|█         | 12/112 [00:32<03:10,  1.91s/it]Running Inference:  12%|█▏        | 13/112 [00:34<03:11,  1.93s/it]Running Inference:  12%|█▎        | 14/112 [00:38<04:07,  2.52s/it]Running Inference:  13%|█▎        | 15/112 [00:39<03:22,  2.09s/it]Running Inference:  14%|█▍        | 16/112 [00:43<04:12,  2.63s/it]Running Inference:  15%|█▌        | 17/112 [00:43<03:07,  1.97s/it]Running Inference:  16%|█▌        | 18/112 [00:45<02:51,  1.82s/it]Running Inference:  17%|█▋        | 19/112 [00:49<04:08,  2.67s/it]Running Inference:  18%|█▊        | 20/112 [00:51<03:31,  2.30s/it]Running Inference:  19%|█▉        | 21/112 [00:56<04:42,  3.10s/it]Running Inference:  20%|█▉        | 22/112 [00:57<04:00,  2.67s/it]Running Inference:  21%|██        | 23/112 [01:02<04:52,  3.29s/it]Running Inference:  21%|██▏       | 24/112 [01:03<03:42,  2.53s/it]Running Inference:  22%|██▏       | 25/112 [01:08<04:39,  3.21s/it]Running Inference:  23%|██▎       | 26/112 [01:10<04:10,  2.91s/it]Running Inference:  24%|██▍       | 27/112 [01:12<03:57,  2.79s/it]Running Inference:  25%|██▌       | 28/112 [01:14<03:26,  2.46s/it]Running Inference:  26%|██▌       | 29/112 [01:15<02:56,  2.12s/it]Running Inference:  27%|██▋       | 30/112 [01:17<02:40,  1.96s/it]Running Inference:  28%|██▊       | 31/112 [01:22<03:41,  2.73s/it]Running Inference:  29%|██▊       | 32/112 [01:30<06:01,  4.52s/it]Running Inference:  29%|██▉       | 33/112 [01:32<04:51,  3.69s/it]Running Inference:  30%|███       | 34/112 [01:34<03:56,  3.04s/it]Running Inference:  31%|███▏      | 35/112 [01:38<04:27,  3.48s/it]Running Inference:  32%|███▏      | 36/112 [01:39<03:24,  2.69s/it]Running Inference:  33%|███▎      | 37/112 [01:39<02:33,  2.04s/it]Running Inference:  34%|███▍      | 38/112 [01:40<02:04,  1.68s/it]Running Inference:  35%|███▍      | 39/112 [01:49<04:28,  3.68s/it]Running Inference:  36%|███▌      | 40/112 [01:53<04:44,  3.95s/it]Running Inference:  37%|███▋      | 41/112 [01:58<05:03,  4.28s/it]Running Inference:  38%|███▊      | 42/112 [02:01<04:26,  3.81s/it]Running Inference:  38%|███▊      | 43/112 [02:06<04:39,  4.05s/it]Running Inference:  39%|███▉      | 44/112 [02:08<04:01,  3.56s/it]Running Inference:  40%|████      | 45/112 [02:12<04:13,  3.78s/it]Running Inference:  41%|████      | 46/112 [02:17<04:25,  4.02s/it]Running Inference:  42%|████▏     | 47/112 [02:18<03:33,  3.28s/it]Running Inference:  43%|████▎     | 48/112 [02:21<03:16,  3.07s/it]Running Inference:  44%|████▍     | 49/112 [02:22<02:36,  2.49s/it]Running Inference:  45%|████▍     | 50/112 [02:23<02:10,  2.10s/it]Running Inference:  46%|████▌     | 51/112 [02:24<01:50,  1.82s/it]Running Inference:  46%|████▋     | 52/112 [02:26<01:39,  1.65s/it]Running Inference:  47%|████▋     | 53/112 [02:27<01:35,  1.63s/it]Running Inference:  48%|████▊     | 54/112 [02:29<01:37,  1.69s/it]Running Inference:  49%|████▉     | 55/112 [02:31<01:42,  1.80s/it]Running Inference:  50%|█████     | 56/112 [02:32<01:24,  1.50s/it]Running Inference:  51%|█████     | 57/112 [02:33<01:16,  1.40s/it]Running Inference:  52%|█████▏    | 58/112 [02:39<02:28,  2.75s/it]Running Inference:  53%|█████▎    | 59/112 [02:47<03:42,  4.20s/it]Running Inference:  54%|█████▎    | 60/112 [02:48<02:52,  3.32s/it]Running Inference:  54%|█████▍    | 61/112 [02:49<02:14,  2.63s/it]Running Inference:  55%|█████▌    | 62/112 [02:53<02:39,  3.18s/it]Running Inference:  56%|█████▋    | 63/112 [02:56<02:21,  2.88s/it]Running Inference:  57%|█████▋    | 64/112 [02:58<02:14,  2.80s/it]Running Inference:  58%|█████▊    | 65/112 [03:06<03:23,  4.34s/it]Running Inference:  59%|█████▉    | 66/112 [03:12<03:44,  4.88s/it]Running Inference:  60%|█████▉    | 67/112 [03:14<02:54,  3.88s/it]Running Inference:  61%|██████    | 68/112 [03:16<02:23,  3.25s/it]Running Inference:  62%|██████▏   | 69/112 [03:17<01:51,  2.60s/it]Running Inference:  62%|██████▎   | 70/112 [03:21<02:07,  3.04s/it]Running Inference:  63%|██████▎   | 71/112 [03:22<01:41,  2.47s/it]Running Inference:  64%|██████▍   | 72/112 [03:27<02:05,  3.13s/it]Running Inference:  65%|██████▌   | 73/112 [03:29<01:50,  2.83s/it]Running Inference:  66%|██████▌   | 74/112 [03:33<02:07,  3.35s/it]Running Inference:  67%|██████▋   | 75/112 [03:41<02:57,  4.80s/it]Running Inference:  68%|██████▊   | 76/112 [03:42<02:06,  3.50s/it]Running Inference:  69%|██████▉   | 77/112 [03:44<01:43,  2.96s/it]Running Inference:  70%|██████▉   | 78/112 [03:49<02:07,  3.74s/it]Running Inference:  71%|███████   | 79/112 [03:50<01:39,  3.01s/it]Running Inference:  71%|███████▏  | 80/112 [03:52<01:21,  2.54s/it]Running Inference:  72%|███████▏  | 81/112 [03:56<01:36,  3.12s/it]Running Inference:  73%|███████▎  | 82/112 [03:58<01:23,  2.80s/it]Running Inference:  74%|███████▍  | 83/112 [04:06<02:00,  4.15s/it]Running Inference:  75%|███████▌  | 84/112 [04:11<02:04,  4.43s/it]Running Inference:  76%|███████▌  | 85/112 [04:15<01:54,  4.25s/it]Running Inference:  77%|███████▋  | 86/112 [04:16<01:28,  3.40s/it]Running Inference:  78%|███████▊  | 87/112 [04:20<01:32,  3.71s/it]Running Inference:  79%|███████▊  | 88/112 [04:25<01:37,  4.05s/it]Running Inference:  79%|███████▉  | 89/112 [04:33<01:55,  5.04s/it]Running Inference:  80%|████████  | 90/112 [04:35<01:35,  4.33s/it]Running Inference:  81%|████████▏ | 91/112 [04:40<01:32,  4.41s/it]Running Inference:  82%|████████▏ | 92/112 [04:42<01:14,  3.74s/it]Running Inference:  83%|████████▎ | 93/112 [04:46<01:09,  3.64s/it]Running Inference:  84%|████████▍ | 94/112 [04:50<01:10,  3.89s/it]Running Inference:  85%|████████▍ | 95/112 [04:55<01:09,  4.08s/it]Running Inference:  86%|████████▌ | 96/112 [04:59<01:06,  4.15s/it]Running Inference:  87%|████████▋ | 97/112 [05:02<00:58,  3.91s/it]Running Inference:  88%|████████▊ | 98/112 [05:08<01:03,  4.55s/it]Running Inference:  88%|████████▊ | 99/112 [05:10<00:48,  3.74s/it]Running Inference:  89%|████████▉ | 100/112 [05:18<00:58,  4.89s/it]Running Inference:  90%|█████████ | 101/112 [05:20<00:44,  4.03s/it]Running Inference:  91%|█████████ | 102/112 [05:24<00:42,  4.20s/it]Running Inference:  92%|█████████▏| 103/112 [05:26<00:30,  3.37s/it]Running Inference:  93%|█████████▎| 104/112 [05:27<00:21,  2.66s/it]Running Inference:  94%|█████████▍| 105/112 [05:28<00:15,  2.27s/it]Running Inference:  95%|█████████▍| 106/112 [05:30<00:12,  2.12s/it]Running Inference:  96%|█████████▌| 107/112 [05:35<00:14,  2.88s/it]Running Inference:  96%|█████████▋| 108/112 [05:36<00:10,  2.57s/it]Running Inference:  97%|█████████▋| 109/112 [05:41<00:09,  3.18s/it]Running Inference:  98%|█████████▊| 110/112 [05:42<00:05,  2.65s/it]Running Inference:  99%|█████████▉| 111/112 [05:49<00:03,  3.71s/it]Running Inference: 100%|██████████| 112/112 [05:53<00:00,  3.84s/it]Running Inference: 100%|██████████| 112/112 [05:53<00:00,  3.15s/it]
2025-12-13 19:24:25,374 - INFO - Inference completed.
2025-12-13 19:24:25,381 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-13 19:24:25,381 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:24:25,387 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-13 19:24:25,387 - INFO - Metrics:
18.74
2025-12-13 19:24:25,388 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_en, ratio=0.5) on GPU 4


========================================
LongBench Task: musique
========================================
----------------------------------------
Task: musique | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:24:31,795 - INFO - Set deterministic seeds to 42
2025-12-13 19:24:31,795 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:24:31,796 - INFO - Starting evaluation run...
2025-12-13 19:24:31,796 - INFO - Output directory set to: longbenchresult
2025-12-13 19:24:31,796 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 19:24:31,796 - INFO - KV Press 'knorm' setup.
2025-12-13 19:24:31,796 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:24:31,796 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.67it/s]
Device set to use cuda:0
2025-12-13 19:24:45,655 - INFO - Model pipeline loaded.
2025-12-13 19:24:45,656 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:24:49,908 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:24:49,908 - INFO - Dataset processed with 200 entries.
2025-12-13 19:24:49,951 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<16:10,  4.88s/it]Running Inference:   1%|          | 2/200 [00:07<11:17,  3.42s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:34,  2.91s/it]Running Inference:   2%|▏         | 4/200 [00:13<11:25,  3.50s/it]Running Inference:   2%|▎         | 5/200 [00:16<09:53,  3.05s/it]Running Inference:   3%|▎         | 6/200 [00:20<11:09,  3.45s/it]Running Inference:   4%|▎         | 7/200 [00:24<12:03,  3.75s/it]Running Inference:   4%|▍         | 8/200 [00:27<10:24,  3.25s/it]Running Inference:   4%|▍         | 9/200 [00:29<09:15,  2.91s/it]Running Inference:   5%|▌         | 10/200 [00:33<10:33,  3.34s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:35<09:09,  2.91s/it]Running Inference:   6%|▌         | 12/200 [00:39<10:25,  3.33s/it]Running Inference:   6%|▋         | 13/200 [00:42<09:31,  3.05s/it]Running Inference:   7%|▋         | 14/200 [00:46<10:41,  3.45s/it]Running Inference:   8%|▊         | 15/200 [00:50<11:30,  3.73s/it]Running Inference:   8%|▊         | 16/200 [00:53<10:05,  3.29s/it]Running Inference:   8%|▊         | 17/200 [00:55<09:36,  3.15s/it]Running Inference:   9%|▉         | 18/200 [00:58<08:50,  2.91s/it]Running Inference:  10%|▉         | 19/200 [01:02<10:02,  3.33s/it]Running Inference:  10%|█         | 20/200 [01:06<10:49,  3.61s/it]Running Inference:  10%|█         | 21/200 [01:09<10:08,  3.40s/it]Running Inference:  11%|█         | 22/200 [01:13<10:44,  3.62s/it]Running Inference:  12%|█▏        | 23/200 [01:16<09:23,  3.18s/it]Running Inference:  12%|█▏        | 24/200 [01:17<07:53,  2.69s/it]Running Inference:  12%|█▎        | 25/200 [01:20<07:37,  2.62s/it]Running Inference:  13%|█▎        | 26/200 [01:24<09:04,  3.13s/it]Running Inference:  14%|█▎        | 27/200 [01:26<08:12,  2.84s/it]Running Inference:  14%|█▍        | 28/200 [01:29<08:14,  2.87s/it]Running Inference:  14%|█▍        | 29/200 [01:33<09:24,  3.30s/it]Running Inference:  15%|█▌        | 30/200 [01:36<08:28,  2.99s/it]Running Inference:  16%|█▌        | 31/200 [01:38<07:47,  2.76s/it]Running Inference:  16%|█▌        | 32/200 [01:40<07:07,  2.54s/it]Running Inference:  16%|█▋        | 33/200 [01:44<08:31,  3.06s/it]Running Inference:  17%|█▋        | 34/200 [01:47<07:56,  2.87s/it]Running Inference:  18%|█▊        | 35/200 [01:51<08:55,  3.25s/it]Running Inference:  18%|█▊        | 36/200 [01:55<09:46,  3.58s/it]Running Inference:  18%|█▊        | 37/200 [01:59<10:19,  3.80s/it]Running Inference:  19%|█▉        | 38/200 [02:03<10:29,  3.88s/it]Running Inference:  20%|█▉        | 39/200 [02:08<10:46,  4.01s/it]Running Inference:  20%|██        | 40/200 [02:12<10:55,  4.10s/it]Running Inference:  20%|██        | 41/200 [02:16<10:56,  4.13s/it]Running Inference:  21%|██        | 42/200 [02:19<09:58,  3.79s/it]Running Inference:  22%|██▏       | 43/200 [02:22<08:48,  3.37s/it]Running Inference:  22%|██▏       | 44/200 [02:26<09:30,  3.65s/it]Running Inference:  22%|██▎       | 45/200 [02:30<09:47,  3.79s/it]Running Inference:  23%|██▎       | 46/200 [02:34<10:05,  3.93s/it]Running Inference:  24%|██▎       | 47/200 [02:39<10:24,  4.08s/it]Running Inference:  24%|██▍       | 48/200 [02:43<10:27,  4.13s/it]Running Inference:  24%|██▍       | 49/200 [02:47<10:30,  4.18s/it]Running Inference:  25%|██▌       | 50/200 [02:52<10:32,  4.22s/it]Running Inference:  26%|██▌       | 51/200 [02:56<10:31,  4.24s/it]Running Inference:  26%|██▌       | 52/200 [02:58<09:10,  3.72s/it]Running Inference:  26%|██▋       | 53/200 [03:02<08:49,  3.60s/it]Running Inference:  27%|██▋       | 54/200 [03:04<07:47,  3.20s/it]Running Inference:  28%|██▊       | 55/200 [03:07<07:17,  3.02s/it]Running Inference:  28%|██▊       | 56/200 [03:11<08:10,  3.40s/it]Running Inference:  28%|██▊       | 57/200 [03:15<08:49,  3.71s/it]Running Inference:  29%|██▉       | 58/200 [03:18<07:45,  3.28s/it]Running Inference:  30%|██▉       | 59/200 [03:20<06:57,  2.96s/it]Running Inference:  30%|███       | 60/200 [03:24<07:49,  3.35s/it]Running Inference:  30%|███       | 61/200 [03:28<08:30,  3.67s/it]Running Inference:  31%|███       | 62/200 [03:31<07:23,  3.22s/it]Running Inference:  32%|███▏      | 63/200 [03:35<08:05,  3.54s/it]Running Inference:  32%|███▏      | 64/200 [03:38<07:29,  3.31s/it]Running Inference:  32%|███▎      | 65/200 [03:40<06:27,  2.87s/it]Running Inference:  33%|███▎      | 66/200 [03:44<07:24,  3.32s/it]Running Inference:  34%|███▎      | 67/200 [03:46<06:40,  3.01s/it]Running Inference:  34%|███▍      | 68/200 [03:50<07:25,  3.38s/it]Running Inference:  34%|███▍      | 69/200 [03:53<06:37,  3.04s/it]Running Inference:  35%|███▌      | 70/200 [03:57<07:22,  3.41s/it]Running Inference:  36%|███▌      | 71/200 [04:01<07:54,  3.68s/it]Running Inference:  36%|███▌      | 72/200 [04:04<07:03,  3.31s/it]Running Inference:  36%|███▋      | 73/200 [04:06<06:40,  3.16s/it]Running Inference:  37%|███▋      | 74/200 [04:11<07:22,  3.51s/it]Running Inference:  38%|███▊      | 75/200 [04:15<07:53,  3.79s/it]Running Inference:  38%|███▊      | 76/200 [04:17<06:50,  3.31s/it]Running Inference:  38%|███▊      | 77/200 [04:19<05:45,  2.81s/it]Running Inference:  39%|███▉      | 78/200 [04:21<05:20,  2.63s/it]Running Inference:  40%|███▉      | 79/200 [04:24<05:26,  2.69s/it]Running Inference:  40%|████      | 80/200 [04:28<06:21,  3.18s/it]Running Inference:  40%|████      | 81/200 [04:33<06:57,  3.51s/it]Running Inference:  41%|████      | 82/200 [04:36<06:39,  3.39s/it]Running Inference:  42%|████▏     | 83/200 [04:38<05:53,  3.02s/it]Running Inference:  42%|████▏     | 84/200 [04:42<06:30,  3.36s/it]Running Inference:  42%|████▎     | 85/200 [04:45<06:11,  3.23s/it]Running Inference:  43%|████▎     | 86/200 [04:49<06:42,  3.53s/it]Running Inference:  44%|████▎     | 87/200 [04:52<05:55,  3.14s/it]Running Inference:  44%|████▍     | 88/200 [04:56<06:29,  3.48s/it]Running Inference:  44%|████▍     | 89/200 [05:00<06:52,  3.72s/it]Running Inference:  45%|████▌     | 90/200 [05:03<06:37,  3.61s/it]Running Inference:  46%|████▌     | 91/200 [05:08<06:51,  3.78s/it]Running Inference:  46%|████▌     | 92/200 [05:11<06:20,  3.52s/it]Running Inference:  46%|████▋     | 93/200 [05:13<05:32,  3.11s/it]Running Inference:  47%|████▋     | 94/200 [05:17<06:05,  3.45s/it]Running Inference:  48%|████▊     | 95/200 [05:19<05:22,  3.07s/it]Running Inference:  48%|████▊     | 96/200 [05:21<04:27,  2.57s/it]Running Inference:  48%|████▊     | 97/200 [05:23<04:19,  2.52s/it]Running Inference:  49%|████▉     | 98/200 [05:25<04:08,  2.44s/it]Running Inference:  50%|████▉     | 99/200 [05:29<04:50,  2.88s/it]Running Inference:  50%|█████     | 100/200 [05:31<04:22,  2.62s/it]Running Inference:  50%|█████     | 101/200 [05:34<04:28,  2.72s/it]Running Inference:  51%|█████     | 102/200 [05:38<05:13,  3.20s/it]Running Inference:  52%|█████▏    | 103/200 [05:43<05:39,  3.50s/it]Running Inference:  52%|█████▏    | 104/200 [05:47<06:02,  3.78s/it]Running Inference:  52%|█████▎    | 105/200 [05:50<05:35,  3.53s/it]Running Inference:  53%|█████▎    | 106/200 [05:54<05:52,  3.75s/it]Running Inference:  54%|█████▎    | 107/200 [05:58<05:59,  3.86s/it]Running Inference:  54%|█████▍    | 108/200 [06:01<05:09,  3.36s/it]Running Inference:  55%|█████▍    | 109/200 [06:05<05:34,  3.67s/it]Running Inference:  55%|█████▌    | 110/200 [06:08<05:03,  3.37s/it]Running Inference:  56%|█████▌    | 111/200 [06:11<04:47,  3.23s/it]Running Inference:  56%|█████▌    | 112/200 [06:15<05:08,  3.51s/it]Running Inference:  56%|█████▋    | 113/200 [06:19<05:25,  3.74s/it]Running Inference:  57%|█████▋    | 114/200 [06:23<05:34,  3.89s/it]Running Inference:  57%|█████▊    | 115/200 [06:25<04:49,  3.40s/it]Running Inference:  58%|█████▊    | 116/200 [06:28<04:15,  3.04s/it]Running Inference:  58%|█████▊    | 117/200 [06:32<04:42,  3.40s/it]Running Inference:  59%|█████▉    | 118/200 [06:35<04:28,  3.28s/it]Running Inference:  60%|█████▉    | 119/200 [06:37<03:56,  2.92s/it]Running Inference:  60%|██████    | 120/200 [06:39<03:38,  2.73s/it]Running Inference:  60%|██████    | 121/200 [06:42<03:24,  2.59s/it]Running Inference:  61%|██████    | 122/200 [06:46<04:00,  3.09s/it]Running Inference:  62%|██████▏   | 123/200 [06:50<04:27,  3.48s/it]Running Inference:  62%|██████▏   | 124/200 [06:54<04:42,  3.71s/it]Running Inference:  62%|██████▎   | 125/200 [06:58<04:34,  3.66s/it]Running Inference:  63%|██████▎   | 126/200 [07:02<04:45,  3.86s/it]Running Inference:  64%|██████▎   | 127/200 [07:05<04:08,  3.40s/it]Running Inference:  64%|██████▍   | 128/200 [07:09<04:23,  3.65s/it]Running Inference:  64%|██████▍   | 129/200 [07:11<03:48,  3.21s/it]Running Inference:  65%|██████▌   | 130/200 [07:15<04:07,  3.53s/it]Running Inference:  66%|██████▌   | 131/200 [07:18<03:38,  3.16s/it]Running Inference:  66%|██████▌   | 132/200 [07:22<03:57,  3.49s/it]Running Inference:  66%|██████▋   | 133/200 [07:26<04:06,  3.68s/it]Running Inference:  67%|██████▋   | 134/200 [07:27<03:13,  2.94s/it]Running Inference:  68%|██████▊   | 135/200 [07:32<03:39,  3.38s/it]Running Inference:  68%|██████▊   | 136/200 [07:34<03:16,  3.06s/it]Running Inference:  68%|██████▊   | 137/200 [07:38<03:36,  3.44s/it]Running Inference:  69%|██████▉   | 138/200 [07:42<03:47,  3.66s/it]Running Inference:  70%|██████▉   | 139/200 [07:47<03:55,  3.86s/it]Running Inference:  70%|███████   | 140/200 [07:50<03:33,  3.56s/it]Running Inference:  70%|███████   | 141/200 [07:54<03:43,  3.79s/it]Running Inference:  71%|███████   | 142/200 [07:56<03:10,  3.28s/it]Running Inference:  72%|███████▏  | 143/200 [08:00<03:24,  3.58s/it]Running Inference:  72%|███████▏  | 144/200 [08:01<02:37,  2.81s/it]Running Inference:  72%|███████▎  | 145/200 [08:06<02:57,  3.23s/it]Running Inference:  73%|███████▎  | 146/200 [08:08<02:38,  2.94s/it]Running Inference:  74%|███████▎  | 147/200 [08:12<02:57,  3.36s/it]Running Inference:  74%|███████▍  | 148/200 [08:16<03:09,  3.64s/it]Running Inference:  74%|███████▍  | 149/200 [08:20<03:03,  3.60s/it]Running Inference:  75%|███████▌  | 150/200 [08:22<02:39,  3.18s/it]Running Inference:  76%|███████▌  | 151/200 [08:27<02:53,  3.54s/it]Running Inference:  76%|███████▌  | 152/200 [08:31<03:02,  3.81s/it]Running Inference:  76%|███████▋  | 153/200 [08:35<03:08,  4.01s/it]Running Inference:  77%|███████▋  | 154/200 [08:38<02:40,  3.48s/it]Running Inference:  78%|███████▊  | 155/200 [08:40<02:20,  3.12s/it]Running Inference:  78%|███████▊  | 156/200 [08:44<02:27,  3.34s/it]Running Inference:  78%|███████▊  | 157/200 [08:48<02:38,  3.68s/it]Running Inference:  79%|███████▉  | 158/200 [08:51<02:17,  3.26s/it]Running Inference:  80%|███████▉  | 159/200 [08:54<02:12,  3.23s/it]Running Inference:  80%|████████  | 160/200 [08:56<01:57,  2.95s/it]Running Inference:  80%|████████  | 161/200 [08:58<01:46,  2.74s/it]Running Inference:  81%|████████  | 162/200 [09:01<01:39,  2.61s/it]Running Inference:  82%|████████▏ | 163/200 [09:05<01:56,  3.14s/it]Running Inference:  82%|████████▏ | 164/200 [09:08<01:50,  3.08s/it]Running Inference:  82%|████████▎ | 165/200 [09:12<02:00,  3.45s/it]Running Inference:  83%|████████▎ | 166/200 [09:14<01:44,  3.08s/it]Running Inference:  84%|████████▎ | 167/200 [09:17<01:35,  2.90s/it]Running Inference:  84%|████████▍ | 168/200 [09:19<01:29,  2.79s/it]Running Inference:  84%|████████▍ | 169/200 [09:24<01:41,  3.27s/it]Running Inference:  85%|████████▌ | 170/200 [09:28<01:47,  3.59s/it]Running Inference:  86%|████████▌ | 171/200 [09:33<01:50,  3.81s/it]Running Inference:  86%|████████▌ | 172/200 [09:37<01:51,  3.98s/it]Running Inference:  86%|████████▋ | 173/200 [09:41<01:44,  3.88s/it]Running Inference:  87%|████████▋ | 174/200 [09:43<01:30,  3.49s/it]Running Inference:  88%|████████▊ | 175/200 [09:47<01:33,  3.75s/it]Running Inference:  88%|████████▊ | 176/200 [09:52<01:34,  3.96s/it]Running Inference:  88%|████████▊ | 177/200 [09:56<01:33,  4.06s/it]Running Inference:  89%|████████▉ | 178/200 [09:59<01:22,  3.77s/it]Running Inference:  90%|████████▉ | 179/200 [10:01<01:09,  3.30s/it]Running Inference:  90%|█████████ | 180/200 [10:03<00:56,  2.81s/it]Running Inference:  90%|█████████ | 181/200 [10:05<00:50,  2.65s/it]Running Inference:  91%|█████████ | 182/200 [10:08<00:46,  2.57s/it]Running Inference:  92%|█████████▏| 183/200 [10:12<00:52,  3.07s/it]Running Inference:  92%|█████████▏| 184/200 [10:16<00:55,  3.46s/it]Running Inference:  92%|█████████▎| 185/200 [10:19<00:46,  3.09s/it]Running Inference:  93%|█████████▎| 186/200 [10:21<00:39,  2.84s/it]Running Inference:  94%|█████████▎| 187/200 [10:24<00:36,  2.83s/it]Running Inference:  94%|█████████▍| 188/200 [10:25<00:29,  2.48s/it]Running Inference:  94%|█████████▍| 189/200 [10:28<00:29,  2.65s/it]Running Inference:  95%|█████████▌| 190/200 [10:30<00:24,  2.44s/it]Running Inference:  96%|█████████▌| 191/200 [10:35<00:27,  3.01s/it]Running Inference:  96%|█████████▌| 192/200 [10:39<00:27,  3.41s/it]Running Inference:  96%|█████████▋| 193/200 [10:41<00:21,  3.08s/it]Running Inference:  97%|█████████▋| 194/200 [10:46<00:20,  3.46s/it]Running Inference:  98%|█████████▊| 195/200 [10:50<00:18,  3.75s/it]Running Inference:  98%|█████████▊| 196/200 [10:53<00:13,  3.37s/it]Running Inference:  98%|█████████▊| 197/200 [10:54<00:08,  2.74s/it]Running Inference:  99%|█████████▉| 198/200 [10:58<00:06,  3.22s/it]Running Inference: 100%|█████████▉| 199/200 [11:02<00:03,  3.33s/it]Running Inference: 100%|██████████| 200/200 [11:06<00:00,  3.62s/it]Running Inference: 100%|██████████| 200/200 [11:06<00:00,  3.33s/it]
2025-12-13 19:35:56,619 - INFO - Inference completed.
2025-12-13 19:35:56,633 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 19:35:56,633 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:35:56,640 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 19:35:56,640 - INFO - Metrics:
11.57
2025-12-13 19:35:56,642 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=musique, ratio=0.1) on GPU 4

----------------------------------------
Task: musique | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:36:03,342 - INFO - Set deterministic seeds to 42
2025-12-13 19:36:03,342 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:36:03,342 - INFO - Starting evaluation run...
2025-12-13 19:36:03,342 - INFO - Output directory set to: longbenchresult
2025-12-13 19:36:03,342 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 19:36:03,342 - INFO - KV Press 'knorm' setup.
2025-12-13 19:36:03,342 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:36:03,342 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.00it/s]
Device set to use cuda:0
2025-12-13 19:36:15,468 - INFO - Model pipeline loaded.
2025-12-13 19:36:15,468 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:36:22,574 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:36:22,574 - INFO - Dataset processed with 200 entries.
2025-12-13 19:36:22,619 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:44,  4.45s/it]Running Inference:   1%|          | 2/200 [00:07<11:12,  3.40s/it]Running Inference:   2%|▏         | 3/200 [00:11<12:26,  3.79s/it]Running Inference:   2%|▏         | 4/200 [00:15<13:07,  4.02s/it]Running Inference:   2%|▎         | 5/200 [00:18<11:46,  3.62s/it]Running Inference:   3%|▎         | 6/200 [00:20<10:17,  3.19s/it]Running Inference:   4%|▎         | 7/200 [00:25<11:26,  3.56s/it]Running Inference:   4%|▍         | 8/200 [00:29<12:05,  3.78s/it]Running Inference:   4%|▍         | 9/200 [00:31<10:24,  3.27s/it]Running Inference:   5%|▌         | 10/200 [00:35<11:19,  3.58s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:37<09:40,  3.07s/it]Running Inference:   6%|▌         | 12/200 [00:42<10:45,  3.43s/it]Running Inference:   6%|▋         | 13/200 [00:44<09:44,  3.13s/it]Running Inference:   7%|▋         | 14/200 [00:48<10:48,  3.49s/it]Running Inference:   8%|▊         | 15/200 [00:53<11:32,  3.75s/it]Running Inference:   8%|▊         | 16/200 [00:55<09:55,  3.24s/it]Running Inference:   8%|▊         | 17/200 [00:58<09:28,  3.10s/it]Running Inference:   9%|▉         | 18/200 [01:00<08:37,  2.84s/it]Running Inference:  10%|▉         | 19/200 [01:04<09:51,  3.27s/it]Running Inference:  10%|█         | 20/200 [01:08<10:40,  3.56s/it]Running Inference:  10%|█         | 21/200 [01:13<11:12,  3.76s/it]Running Inference:  11%|█         | 22/200 [01:17<11:27,  3.86s/it]Running Inference:  12%|█▏        | 23/200 [01:19<09:54,  3.36s/it]Running Inference:  12%|█▏        | 24/200 [01:20<08:14,  2.81s/it]Running Inference:  12%|█▎        | 25/200 [01:25<09:25,  3.23s/it]Running Inference:  13%|█▎        | 26/200 [01:29<10:17,  3.55s/it]Running Inference:  14%|█▎        | 27/200 [01:31<09:02,  3.14s/it]Running Inference:  14%|█▍        | 28/200 [01:34<08:48,  3.07s/it]Running Inference:  14%|█▍        | 29/200 [01:38<09:46,  3.43s/it]Running Inference:  15%|█▌        | 30/200 [01:42<10:24,  3.68s/it]Running Inference:  16%|█▌        | 31/200 [01:45<09:08,  3.24s/it]Running Inference:  16%|█▌        | 32/200 [01:48<09:28,  3.39s/it]Running Inference:  16%|█▋        | 33/200 [01:53<10:07,  3.64s/it]Running Inference:  17%|█▋        | 34/200 [01:55<09:05,  3.28s/it]Running Inference:  18%|█▊        | 35/200 [01:59<09:41,  3.53s/it]Running Inference:  18%|█▊        | 36/200 [02:04<10:17,  3.76s/it]Running Inference:  18%|█▊        | 37/200 [02:08<10:38,  3.92s/it]Running Inference:  19%|█▉        | 38/200 [02:10<09:02,  3.35s/it]Running Inference:  20%|█▉        | 39/200 [02:12<08:01,  2.99s/it]Running Inference:  20%|██        | 40/200 [02:16<08:59,  3.37s/it]Running Inference:  20%|██        | 41/200 [02:20<09:34,  3.61s/it]Running Inference:  21%|██        | 42/200 [02:23<08:56,  3.40s/it]Running Inference:  22%|██▏       | 43/200 [02:26<08:05,  3.09s/it]Running Inference:  22%|██▏       | 44/200 [02:28<07:29,  2.88s/it]Running Inference:  22%|██▎       | 45/200 [02:31<07:37,  2.95s/it]Running Inference:  23%|██▎       | 46/200 [02:35<08:33,  3.34s/it]Running Inference:  24%|██▎       | 47/200 [02:40<09:18,  3.65s/it]Running Inference:  24%|██▍       | 48/200 [02:44<09:40,  3.82s/it]Running Inference:  24%|██▍       | 49/200 [02:48<09:56,  3.95s/it]Running Inference:  25%|██▌       | 50/200 [02:53<10:07,  4.05s/it]Running Inference:  26%|██▌       | 51/200 [02:55<08:46,  3.54s/it]Running Inference:  26%|██▌       | 52/200 [02:59<09:17,  3.77s/it]Running Inference:  26%|██▋       | 53/200 [03:03<09:32,  3.90s/it]Running Inference:  27%|██▋       | 54/200 [03:06<08:28,  3.48s/it]Running Inference:  28%|██▊       | 55/200 [03:09<07:45,  3.21s/it]Running Inference:  28%|██▊       | 56/200 [03:11<07:02,  2.93s/it]Running Inference:  28%|██▊       | 57/200 [03:15<08:01,  3.36s/it]Running Inference:  29%|██▉       | 58/200 [03:17<07:11,  3.04s/it]Running Inference:  30%|██▉       | 59/200 [03:20<06:34,  2.80s/it]Running Inference:  30%|███       | 60/200 [03:24<07:32,  3.23s/it]Running Inference:  30%|███       | 61/200 [03:28<08:16,  3.57s/it]Running Inference:  31%|███       | 62/200 [03:30<07:13,  3.14s/it]Running Inference:  32%|███▏      | 63/200 [03:35<07:56,  3.48s/it]Running Inference:  32%|███▏      | 64/200 [03:37<07:07,  3.15s/it]Running Inference:  32%|███▎      | 65/200 [03:41<07:32,  3.35s/it]Running Inference:  33%|███▎      | 66/200 [03:44<07:30,  3.36s/it]Running Inference:  34%|███▎      | 67/200 [03:47<06:44,  3.04s/it]Running Inference:  34%|███▍      | 68/200 [03:51<07:27,  3.39s/it]Running Inference:  34%|███▍      | 69/200 [03:53<06:40,  3.06s/it]Running Inference:  35%|███▌      | 70/200 [03:57<07:23,  3.41s/it]Running Inference:  36%|███▌      | 71/200 [04:02<07:53,  3.67s/it]Running Inference:  36%|███▌      | 72/200 [04:04<07:02,  3.30s/it]Running Inference:  36%|███▋      | 73/200 [04:08<07:34,  3.58s/it]Running Inference:  37%|███▋      | 74/200 [04:13<07:58,  3.80s/it]Running Inference:  38%|███▊      | 75/200 [04:17<08:16,  3.97s/it]Running Inference:  38%|███▊      | 76/200 [04:19<07:08,  3.46s/it]Running Inference:  38%|███▊      | 77/200 [04:21<06:00,  2.93s/it]Running Inference:  39%|███▉      | 78/200 [04:23<05:33,  2.74s/it]Running Inference:  40%|███▉      | 79/200 [04:26<05:32,  2.75s/it]Running Inference:  40%|████      | 80/200 [04:30<06:24,  3.21s/it]Running Inference:  40%|████      | 81/200 [04:34<06:58,  3.52s/it]Running Inference:  41%|████      | 82/200 [04:39<07:24,  3.77s/it]Running Inference:  42%|████▏     | 83/200 [04:41<06:27,  3.31s/it]Running Inference:  42%|████▏     | 84/200 [04:43<05:39,  2.93s/it]Running Inference:  42%|████▎     | 85/200 [04:45<05:13,  2.72s/it]Running Inference:  43%|████▎     | 86/200 [04:50<06:01,  3.17s/it]Running Inference:  44%|████▎     | 87/200 [04:52<05:26,  2.89s/it]Running Inference:  44%|████▍     | 88/200 [04:54<05:13,  2.80s/it]Running Inference:  44%|████▍     | 89/200 [04:57<04:52,  2.64s/it]Running Inference:  45%|████▌     | 90/200 [05:01<05:43,  3.13s/it]Running Inference:  46%|████▌     | 91/200 [05:05<06:13,  3.43s/it]Running Inference:  46%|████▌     | 92/200 [05:09<06:13,  3.46s/it]Running Inference:  46%|████▋     | 93/200 [05:11<05:27,  3.06s/it]Running Inference:  47%|████▋     | 94/200 [05:13<04:57,  2.81s/it]Running Inference:  48%|████▊     | 95/200 [05:15<04:35,  2.62s/it]Running Inference:  48%|████▊     | 96/200 [05:19<04:57,  2.86s/it]Running Inference:  48%|████▊     | 97/200 [05:21<04:39,  2.71s/it]Running Inference:  49%|████▉     | 98/200 [05:25<05:23,  3.17s/it]Running Inference:  50%|████▉     | 99/200 [05:28<05:13,  3.10s/it]Running Inference:  50%|█████     | 100/200 [05:30<04:38,  2.78s/it]Running Inference:  50%|█████     | 101/200 [05:34<05:14,  3.18s/it]Running Inference:  51%|█████     | 102/200 [05:38<05:43,  3.50s/it]Running Inference:  52%|█████▏    | 103/200 [05:43<05:59,  3.71s/it]Running Inference:  52%|█████▏    | 104/200 [05:47<06:15,  3.92s/it]Running Inference:  52%|█████▎    | 105/200 [05:50<05:42,  3.61s/it]Running Inference:  53%|█████▎    | 106/200 [05:54<05:57,  3.80s/it]Running Inference:  54%|█████▎    | 107/200 [05:58<06:01,  3.89s/it]Running Inference:  54%|█████▍    | 108/200 [06:00<05:11,  3.38s/it]Running Inference:  55%|█████▍    | 109/200 [06:05<05:34,  3.67s/it]Running Inference:  55%|█████▌    | 110/200 [06:09<05:46,  3.85s/it]Running Inference:  56%|█████▌    | 111/200 [06:12<05:13,  3.53s/it]Running Inference:  56%|█████▌    | 112/200 [06:16<05:26,  3.71s/it]Running Inference:  56%|█████▋    | 113/200 [06:20<05:36,  3.87s/it]Running Inference:  57%|█████▋    | 114/200 [06:24<05:41,  3.97s/it]Running Inference:  57%|█████▊    | 115/200 [06:27<04:49,  3.40s/it]Running Inference:  58%|█████▊    | 116/200 [06:29<04:16,  3.06s/it]Running Inference:  58%|█████▊    | 117/200 [06:33<04:42,  3.40s/it]Running Inference:  59%|█████▉    | 118/200 [06:37<04:58,  3.65s/it]Running Inference:  60%|█████▉    | 119/200 [06:39<04:17,  3.18s/it]Running Inference:  60%|██████    | 120/200 [06:44<04:40,  3.50s/it]Running Inference:  60%|██████    | 121/200 [06:46<04:07,  3.13s/it]Running Inference:  61%|██████    | 122/200 [06:50<04:29,  3.46s/it]Running Inference:  62%|██████▏   | 123/200 [06:53<04:03,  3.16s/it]Running Inference:  62%|██████▏   | 124/200 [06:57<04:24,  3.48s/it]Running Inference:  62%|██████▎   | 125/200 [06:58<03:38,  2.91s/it]Running Inference:  63%|██████▎   | 126/200 [07:03<04:05,  3.32s/it]Running Inference:  64%|██████▎   | 127/200 [07:06<04:10,  3.44s/it]Running Inference:  64%|██████▍   | 128/200 [07:11<04:24,  3.67s/it]Running Inference:  64%|██████▍   | 129/200 [07:13<03:49,  3.23s/it]Running Inference:  65%|██████▌   | 130/200 [07:16<03:39,  3.14s/it]Running Inference:  66%|██████▌   | 131/200 [07:18<03:19,  2.89s/it]Running Inference:  66%|██████▌   | 132/200 [07:20<03:06,  2.75s/it]Running Inference:  66%|██████▋   | 133/200 [07:24<03:30,  3.15s/it]Running Inference:  67%|██████▋   | 134/200 [07:26<02:51,  2.59s/it]Running Inference:  68%|██████▊   | 135/200 [07:30<03:23,  3.12s/it]Running Inference:  68%|██████▊   | 136/200 [07:32<03:03,  2.87s/it]Running Inference:  68%|██████▊   | 137/200 [07:37<03:27,  3.29s/it]Running Inference:  69%|██████▉   | 138/200 [07:41<03:40,  3.55s/it]Running Inference:  70%|██████▉   | 139/200 [07:45<03:49,  3.77s/it]Running Inference:  70%|███████   | 140/200 [07:48<03:32,  3.54s/it]Running Inference:  70%|███████   | 141/200 [07:52<03:42,  3.76s/it]Running Inference:  71%|███████   | 142/200 [07:57<03:46,  3.90s/it]Running Inference:  72%|███████▏  | 143/200 [08:01<03:48,  4.00s/it]Running Inference:  72%|███████▏  | 144/200 [08:02<02:54,  3.12s/it]Running Inference:  72%|███████▎  | 145/200 [08:06<03:09,  3.44s/it]Running Inference:  73%|███████▎  | 146/200 [08:08<02:46,  3.09s/it]Running Inference:  74%|███████▎  | 147/200 [08:13<03:02,  3.44s/it]Running Inference:  74%|███████▍  | 148/200 [08:17<03:12,  3.70s/it]Running Inference:  74%|███████▍  | 149/200 [08:20<03:04,  3.61s/it]Running Inference:  75%|███████▌  | 150/200 [08:23<02:39,  3.19s/it]Running Inference:  76%|███████▌  | 151/200 [08:27<02:52,  3.51s/it]Running Inference:  76%|███████▌  | 152/200 [08:31<02:59,  3.73s/it]Running Inference:  76%|███████▋  | 153/200 [08:35<03:03,  3.90s/it]Running Inference:  77%|███████▋  | 154/200 [08:38<02:36,  3.40s/it]Running Inference:  78%|███████▊  | 155/200 [08:41<02:29,  3.32s/it]Running Inference:  78%|███████▊  | 156/200 [08:44<02:31,  3.44s/it]Running Inference:  78%|███████▊  | 157/200 [08:47<02:13,  3.11s/it]Running Inference:  79%|███████▉  | 158/200 [08:49<02:02,  2.91s/it]Running Inference:  80%|███████▉  | 159/200 [08:52<02:00,  2.93s/it]Running Inference:  80%|████████  | 160/200 [08:56<02:12,  3.31s/it]Running Inference:  80%|████████  | 161/200 [08:59<01:56,  2.99s/it]Running Inference:  81%|████████  | 162/200 [09:01<01:45,  2.77s/it]Running Inference:  82%|████████▏ | 163/200 [09:05<01:58,  3.20s/it]Running Inference:  82%|████████▏ | 164/200 [09:08<01:49,  3.03s/it]Running Inference:  82%|████████▎ | 165/200 [09:12<01:58,  3.38s/it]Running Inference:  83%|████████▎ | 166/200 [09:14<01:43,  3.03s/it]Running Inference:  84%|████████▎ | 167/200 [09:17<01:34,  2.87s/it]Running Inference:  84%|████████▍ | 168/200 [09:19<01:30,  2.82s/it]Running Inference:  84%|████████▍ | 169/200 [09:24<01:41,  3.26s/it]Running Inference:  85%|████████▌ | 170/200 [09:28<01:46,  3.55s/it]Running Inference:  86%|████████▌ | 171/200 [09:32<01:48,  3.75s/it]Running Inference:  86%|████████▌ | 172/200 [09:36<01:49,  3.90s/it]Running Inference:  86%|████████▋ | 173/200 [09:40<01:42,  3.80s/it]Running Inference:  87%|████████▋ | 174/200 [09:43<01:30,  3.48s/it]Running Inference:  88%|████████▊ | 175/200 [09:47<01:32,  3.71s/it]Running Inference:  88%|████████▊ | 176/200 [09:49<01:18,  3.28s/it]Running Inference:  88%|████████▊ | 177/200 [09:53<01:21,  3.55s/it]Running Inference:  89%|████████▉ | 178/200 [09:58<01:23,  3.77s/it]Running Inference:  90%|████████▉ | 179/200 [10:02<01:22,  3.94s/it]Running Inference:  90%|█████████ | 180/200 [10:03<01:04,  3.20s/it]Running Inference:  90%|█████████ | 181/200 [10:06<00:55,  2.92s/it]Running Inference:  91%|█████████ | 182/200 [10:08<00:49,  2.72s/it]Running Inference:  92%|█████████▏| 183/200 [10:12<00:53,  3.16s/it]Running Inference:  92%|█████████▏| 184/200 [10:16<00:55,  3.49s/it]Running Inference:  92%|█████████▎| 185/200 [10:19<00:46,  3.10s/it]Running Inference:  93%|█████████▎| 186/200 [10:21<00:40,  2.86s/it]Running Inference:  94%|█████████▎| 187/200 [10:23<00:35,  2.75s/it]Running Inference:  94%|█████████▍| 188/200 [10:25<00:29,  2.45s/it]Running Inference:  94%|█████████▍| 189/200 [10:29<00:33,  3.01s/it]Running Inference:  95%|█████████▌| 190/200 [10:31<00:27,  2.70s/it]Running Inference:  96%|█████████▌| 191/200 [10:36<00:28,  3.17s/it]Running Inference:  96%|█████████▌| 192/200 [10:38<00:23,  2.88s/it]Running Inference:  96%|█████████▋| 193/200 [10:42<00:23,  3.30s/it]Running Inference:  97%|█████████▋| 194/200 [10:46<00:21,  3.58s/it]Running Inference:  98%|█████████▊| 195/200 [10:51<00:19,  3.80s/it]Running Inference:  98%|█████████▊| 196/200 [10:53<00:13,  3.40s/it]Running Inference:  98%|█████████▊| 197/200 [10:54<00:07,  2.66s/it]Running Inference:  99%|█████████▉| 198/200 [10:58<00:06,  3.14s/it]Running Inference: 100%|█████████▉| 199/200 [11:02<00:03,  3.27s/it]Running Inference: 100%|██████████| 200/200 [11:05<00:00,  3.16s/it]Running Inference: 100%|██████████| 200/200 [11:05<00:00,  3.33s/it]
2025-12-13 19:47:28,010 - INFO - Inference completed.
2025-12-13 19:47:28,019 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-13 19:47:28,019 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:47:28,026 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-13 19:47:28,026 - INFO - Metrics:
11.05
2025-12-13 19:47:28,027 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=musique, ratio=0.2) on GPU 4

----------------------------------------
Task: musique | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:47:34,436 - INFO - Set deterministic seeds to 42
2025-12-13 19:47:34,436 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:47:34,436 - INFO - Starting evaluation run...
2025-12-13 19:47:34,436 - INFO - Output directory set to: longbenchresult
2025-12-13 19:47:34,436 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-13 19:47:34,436 - INFO - KV Press 'knorm' setup.
2025-12-13 19:47:34,436 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:47:34,436 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.05it/s]
Device set to use cuda:0
2025-12-13 19:48:17,990 - INFO - Model pipeline loaded.
2025-12-13 19:48:17,990 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:48:23,601 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:48:23,601 - INFO - Dataset processed with 200 entries.
2025-12-13 19:48:23,643 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<15:22,  4.64s/it]Running Inference:   1%|          | 2/200 [00:09<14:46,  4.48s/it]Running Inference:   2%|▏         | 3/200 [00:13<14:23,  4.39s/it]Running Inference:   2%|▏         | 4/200 [00:17<14:18,  4.38s/it]Running Inference:   2%|▎         | 5/200 [00:20<12:22,  3.81s/it]Running Inference:   3%|▎         | 6/200 [00:24<12:45,  3.95s/it]Running Inference:   4%|▎         | 7/200 [00:29<13:07,  4.08s/it]Running Inference:   4%|▍         | 8/200 [00:33<13:12,  4.13s/it]Running Inference:   4%|▍         | 9/200 [00:35<11:09,  3.51s/it]Running Inference:   5%|▌         | 10/200 [00:37<09:51,  3.11s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:39<08:39,  2.75s/it]Running Inference:   6%|▌         | 12/200 [00:43<10:03,  3.21s/it]Running Inference:   6%|▋         | 13/200 [00:46<09:16,  2.97s/it]Running Inference:   7%|▋         | 14/200 [00:50<10:29,  3.39s/it]Running Inference:   8%|▊         | 15/200 [00:53<09:38,  3.13s/it]Running Inference:   8%|▊         | 16/200 [00:55<08:35,  2.80s/it]Running Inference:   8%|▊         | 17/200 [00:57<08:33,  2.80s/it]Running Inference:   9%|▉         | 18/200 [01:00<08:04,  2.66s/it]Running Inference:  10%|▉         | 19/200 [01:03<08:05,  2.68s/it]Running Inference:  10%|█         | 20/200 [01:07<09:27,  3.15s/it]Running Inference:  10%|█         | 21/200 [01:11<10:23,  3.48s/it]Running Inference:  11%|█         | 22/200 [01:15<10:54,  3.68s/it]Running Inference:  12%|█▏        | 23/200 [01:19<11:23,  3.86s/it]Running Inference:  12%|█▏        | 24/200 [01:21<09:16,  3.16s/it]Running Inference:  12%|█▎        | 25/200 [01:25<10:10,  3.49s/it]Running Inference:  13%|█▎        | 26/200 [01:30<10:49,  3.73s/it]Running Inference:  14%|█▎        | 27/200 [01:34<11:14,  3.90s/it]Running Inference:  14%|█▍        | 28/200 [01:37<10:20,  3.61s/it]Running Inference:  14%|█▍        | 29/200 [01:41<10:51,  3.81s/it]Running Inference:  15%|█▌        | 30/200 [01:43<09:35,  3.38s/it]Running Inference:  16%|█▌        | 31/200 [01:46<08:33,  3.04s/it]Running Inference:  16%|█▌        | 32/200 [01:47<07:22,  2.63s/it]Running Inference:  16%|█▋        | 33/200 [01:50<07:28,  2.68s/it]Running Inference:  17%|█▋        | 34/200 [01:53<07:14,  2.62s/it]Running Inference:  18%|█▊        | 35/200 [01:56<08:01,  2.92s/it]Running Inference:  18%|█▊        | 36/200 [02:01<09:08,  3.34s/it]Running Inference:  18%|█▊        | 37/200 [02:05<09:52,  3.64s/it]Running Inference:  19%|█▉        | 38/200 [02:09<10:10,  3.77s/it]Running Inference:  20%|█▉        | 39/200 [02:11<08:49,  3.29s/it]Running Inference:  20%|██        | 40/200 [02:15<09:34,  3.59s/it]Running Inference:  20%|██        | 41/200 [02:18<09:02,  3.41s/it]Running Inference:  21%|██        | 42/200 [02:21<08:40,  3.30s/it]Running Inference:  22%|██▏       | 43/200 [02:26<09:27,  3.62s/it]Running Inference:  22%|██▏       | 44/200 [02:28<08:34,  3.30s/it]Running Inference:  22%|██▎       | 45/200 [02:31<08:23,  3.25s/it]Running Inference:  23%|██▎       | 46/200 [02:34<07:33,  2.95s/it]Running Inference:  24%|██▎       | 47/200 [02:38<08:38,  3.39s/it]Running Inference:  24%|██▍       | 48/200 [02:42<09:14,  3.65s/it]Running Inference:  24%|██▍       | 49/200 [02:47<09:40,  3.84s/it]Running Inference:  25%|██▌       | 50/200 [02:51<09:57,  3.98s/it]Running Inference:  26%|██▌       | 51/200 [02:55<10:06,  4.07s/it]Running Inference:  26%|██▌       | 52/200 [03:00<10:14,  4.15s/it]Running Inference:  26%|██▋       | 53/200 [03:04<10:13,  4.17s/it]Running Inference:  27%|██▋       | 54/200 [03:06<08:49,  3.62s/it]Running Inference:  28%|██▊       | 55/200 [03:09<07:59,  3.31s/it]Running Inference:  28%|██▊       | 56/200 [03:11<07:08,  2.98s/it]Running Inference:  28%|██▊       | 57/200 [03:15<08:06,  3.40s/it]Running Inference:  29%|██▉       | 58/200 [03:18<07:21,  3.11s/it]Running Inference:  30%|██▉       | 59/200 [03:20<06:40,  2.84s/it]Running Inference:  30%|███       | 60/200 [03:24<07:37,  3.27s/it]Running Inference:  30%|███       | 61/200 [03:29<08:21,  3.61s/it]Running Inference:  31%|███       | 62/200 [03:31<07:17,  3.17s/it]Running Inference:  32%|███▏      | 63/200 [03:34<06:55,  3.03s/it]Running Inference:  32%|███▏      | 64/200 [03:36<06:25,  2.84s/it]Running Inference:  32%|███▎      | 65/200 [03:40<07:04,  3.15s/it]Running Inference:  33%|███▎      | 66/200 [03:44<07:49,  3.51s/it]Running Inference:  34%|███▎      | 67/200 [03:46<06:57,  3.14s/it]Running Inference:  34%|███▍      | 68/200 [03:49<06:18,  2.86s/it]Running Inference:  34%|███▍      | 69/200 [03:51<05:50,  2.68s/it]Running Inference:  35%|███▌      | 70/200 [03:55<06:50,  3.15s/it]Running Inference:  36%|███▌      | 71/200 [03:59<07:31,  3.50s/it]Running Inference:  36%|███▌      | 72/200 [04:02<06:48,  3.19s/it]Running Inference:  36%|███▋      | 73/200 [04:06<07:25,  3.51s/it]Running Inference:  37%|███▋      | 74/200 [04:10<07:53,  3.76s/it]Running Inference:  38%|███▊      | 75/200 [04:15<08:14,  3.96s/it]Running Inference:  38%|███▊      | 76/200 [04:17<07:07,  3.45s/it]Running Inference:  38%|███▊      | 77/200 [04:19<06:01,  2.94s/it]Running Inference:  39%|███▉      | 78/200 [04:21<05:34,  2.74s/it]Running Inference:  40%|███▉      | 79/200 [04:24<05:46,  2.86s/it]Running Inference:  40%|████      | 80/200 [04:27<05:20,  2.67s/it]Running Inference:  40%|████      | 81/200 [04:31<06:15,  3.16s/it]Running Inference:  41%|████      | 82/200 [04:35<06:56,  3.53s/it]Running Inference:  42%|████▏     | 83/200 [04:37<06:07,  3.14s/it]Running Inference:  42%|████▏     | 84/200 [04:40<05:25,  2.80s/it]Running Inference:  42%|████▎     | 85/200 [04:43<05:31,  2.88s/it]Running Inference:  43%|████▎     | 86/200 [04:47<06:14,  3.29s/it]Running Inference:  44%|████▎     | 87/200 [04:51<06:47,  3.61s/it]Running Inference:  44%|████▍     | 88/200 [04:54<06:09,  3.30s/it]Running Inference:  44%|████▍     | 89/200 [04:58<06:38,  3.59s/it]Running Inference:  45%|████▌     | 90/200 [05:02<06:58,  3.80s/it]Running Inference:  46%|████▌     | 91/200 [05:06<07:06,  3.91s/it]Running Inference:  46%|████▌     | 92/200 [05:10<06:51,  3.81s/it]Running Inference:  46%|████▋     | 93/200 [05:12<05:53,  3.31s/it]Running Inference:  47%|████▋     | 94/200 [05:14<05:12,  2.95s/it]Running Inference:  48%|████▊     | 95/200 [05:16<04:45,  2.72s/it]Running Inference:  48%|████▊     | 96/200 [05:20<05:05,  2.94s/it]Running Inference:  48%|████▊     | 97/200 [05:22<04:45,  2.77s/it]Running Inference:  49%|████▉     | 98/200 [05:27<05:28,  3.22s/it]Running Inference:  50%|████▉     | 99/200 [05:31<05:49,  3.46s/it]Running Inference:  50%|█████     | 100/200 [05:33<05:03,  3.03s/it]Running Inference:  50%|█████     | 101/200 [05:37<05:32,  3.36s/it]Running Inference:  51%|█████     | 102/200 [05:41<05:56,  3.64s/it]Running Inference:  52%|█████▏    | 103/200 [05:45<06:09,  3.81s/it]Running Inference:  52%|█████▏    | 104/200 [05:48<05:42,  3.57s/it]Running Inference:  52%|█████▎    | 105/200 [05:53<06:01,  3.81s/it]Running Inference:  53%|█████▎    | 106/200 [05:57<06:10,  3.95s/it]Running Inference:  54%|█████▎    | 107/200 [06:01<06:11,  4.00s/it]Running Inference:  54%|█████▍    | 108/200 [06:03<05:18,  3.46s/it]Running Inference:  55%|█████▍    | 109/200 [06:08<05:40,  3.74s/it]Running Inference:  55%|█████▌    | 110/200 [06:11<05:26,  3.62s/it]Running Inference:  56%|█████▌    | 111/200 [06:14<05:03,  3.41s/it]Running Inference:  56%|█████▌    | 112/200 [06:18<05:20,  3.64s/it]Running Inference:  56%|█████▋    | 113/200 [06:22<05:32,  3.82s/it]Running Inference:  57%|█████▋    | 114/200 [06:27<05:39,  3.95s/it]Running Inference:  57%|█████▊    | 115/200 [06:31<05:42,  4.03s/it]Running Inference:  58%|█████▊    | 116/200 [06:33<04:54,  3.51s/it]Running Inference:  58%|█████▊    | 117/200 [06:37<05:09,  3.73s/it]Running Inference:  59%|█████▉    | 118/200 [06:42<05:18,  3.88s/it]Running Inference:  60%|█████▉    | 119/200 [06:44<04:33,  3.37s/it]Running Inference:  60%|██████    | 120/200 [06:46<04:03,  3.04s/it]Running Inference:  60%|██████    | 121/200 [06:48<03:41,  2.81s/it]Running Inference:  61%|██████    | 122/200 [06:53<04:14,  3.26s/it]Running Inference:  62%|██████▏   | 123/200 [06:55<03:48,  2.96s/it]Running Inference:  62%|██████▏   | 124/200 [06:59<04:14,  3.35s/it]Running Inference:  62%|██████▎   | 125/200 [07:03<04:15,  3.41s/it]Running Inference:  63%|██████▎   | 126/200 [07:07<04:31,  3.67s/it]Running Inference:  64%|██████▎   | 127/200 [07:11<04:26,  3.65s/it]Running Inference:  64%|██████▍   | 128/200 [07:15<04:35,  3.83s/it]Running Inference:  64%|██████▍   | 129/200 [07:18<04:13,  3.57s/it]Running Inference:  65%|██████▌   | 130/200 [07:21<03:56,  3.38s/it]Running Inference:  66%|██████▌   | 131/200 [07:23<03:30,  3.05s/it]Running Inference:  66%|██████▌   | 132/200 [07:27<03:52,  3.42s/it]Running Inference:  66%|██████▋   | 133/200 [07:31<04:02,  3.62s/it]Running Inference:  67%|██████▋   | 134/200 [07:33<03:11,  2.90s/it]Running Inference:  68%|██████▊   | 135/200 [07:37<03:37,  3.34s/it]Running Inference:  68%|██████▊   | 136/200 [07:39<03:13,  3.03s/it]Running Inference:  68%|██████▊   | 137/200 [07:44<03:34,  3.40s/it]Running Inference:  69%|██████▉   | 138/200 [07:48<03:45,  3.64s/it]Running Inference:  70%|██████▉   | 139/200 [07:50<03:22,  3.32s/it]Running Inference:  70%|███████   | 140/200 [07:55<03:35,  3.59s/it]Running Inference:  70%|███████   | 141/200 [07:59<03:44,  3.81s/it]Running Inference:  71%|███████   | 142/200 [08:03<03:48,  3.93s/it]Running Inference:  72%|███████▏  | 143/200 [08:06<03:19,  3.51s/it]Running Inference:  72%|███████▏  | 144/200 [08:08<03:06,  3.34s/it]Running Inference:  72%|███████▎  | 145/200 [08:13<03:17,  3.59s/it]Running Inference:  73%|███████▎  | 146/200 [08:15<02:52,  3.19s/it]Running Inference:  74%|███████▎  | 147/200 [08:19<03:07,  3.53s/it]Running Inference:  74%|███████▍  | 148/200 [08:24<03:15,  3.76s/it]Running Inference:  74%|███████▍  | 149/200 [08:27<03:06,  3.66s/it]Running Inference:  75%|███████▌  | 150/200 [08:29<02:41,  3.23s/it]Running Inference:  76%|███████▌  | 151/200 [08:32<02:37,  3.21s/it]Running Inference:  76%|███████▌  | 152/200 [08:37<02:49,  3.53s/it]Running Inference:  76%|███████▋  | 153/200 [08:41<02:57,  3.77s/it]Running Inference:  77%|███████▋  | 154/200 [08:43<02:31,  3.30s/it]Running Inference:  78%|███████▊  | 155/200 [08:45<02:14,  2.98s/it]Running Inference:  78%|███████▊  | 156/200 [08:49<02:21,  3.22s/it]Running Inference:  78%|███████▊  | 157/200 [08:53<02:20,  3.26s/it]Running Inference:  79%|███████▉  | 158/200 [08:57<02:30,  3.58s/it]Running Inference:  80%|███████▉  | 159/200 [09:00<02:18,  3.39s/it]Running Inference:  80%|████████  | 160/200 [09:04<02:25,  3.64s/it]Running Inference:  80%|████████  | 161/200 [09:06<02:07,  3.28s/it]Running Inference:  81%|████████  | 162/200 [09:09<01:52,  2.97s/it]Running Inference:  82%|████████▏ | 163/200 [09:13<02:03,  3.35s/it]Running Inference:  82%|████████▏ | 164/200 [09:17<02:08,  3.58s/it]Running Inference:  82%|████████▎ | 165/200 [09:21<02:12,  3.78s/it]Running Inference:  83%|████████▎ | 166/200 [09:24<01:52,  3.31s/it]Running Inference:  84%|████████▎ | 167/200 [09:26<01:40,  3.05s/it]Running Inference:  84%|████████▍ | 168/200 [09:29<01:33,  2.93s/it]Running Inference:  84%|████████▍ | 169/200 [09:33<01:43,  3.35s/it]Running Inference:  85%|████████▌ | 170/200 [09:37<01:48,  3.63s/it]Running Inference:  86%|████████▌ | 171/200 [09:41<01:50,  3.81s/it]Running Inference:  86%|████████▌ | 172/200 [09:46<01:50,  3.94s/it]Running Inference:  86%|████████▋ | 173/200 [09:48<01:32,  3.44s/it]Running Inference:  87%|████████▋ | 174/200 [09:51<01:23,  3.23s/it]Running Inference:  88%|████████▊ | 175/200 [09:55<01:28,  3.55s/it]Running Inference:  88%|████████▊ | 176/200 [09:59<01:30,  3.79s/it]Running Inference:  88%|████████▊ | 177/200 [10:04<01:30,  3.92s/it]Running Inference:  89%|████████▉ | 178/200 [10:08<01:28,  4.03s/it]Running Inference:  90%|████████▉ | 179/200 [10:12<01:26,  4.12s/it]Running Inference:  90%|█████████ | 180/200 [10:14<01:07,  3.36s/it]Running Inference:  90%|█████████ | 181/200 [10:16<00:57,  3.02s/it]Running Inference:  91%|█████████ | 182/200 [10:18<00:50,  2.80s/it]Running Inference:  92%|█████████▏| 183/200 [10:22<00:54,  3.21s/it]Running Inference:  92%|█████████▏| 184/200 [10:27<00:56,  3.54s/it]Running Inference:  92%|█████████▎| 185/200 [10:29<00:47,  3.14s/it]Running Inference:  93%|█████████▎| 186/200 [10:32<00:41,  2.98s/it]Running Inference:  94%|█████████▎| 187/200 [10:36<00:44,  3.39s/it]Running Inference:  94%|█████████▍| 188/200 [10:40<00:41,  3.49s/it]Running Inference:  94%|█████████▍| 189/200 [10:42<00:35,  3.26s/it]Running Inference:  95%|█████████▌| 190/200 [10:44<00:28,  2.88s/it]Running Inference:  96%|█████████▌| 191/200 [10:47<00:24,  2.69s/it]Running Inference:  96%|█████████▌| 192/200 [10:51<00:25,  3.16s/it]Running Inference:  96%|█████████▋| 193/200 [10:55<00:24,  3.50s/it]Running Inference:  97%|█████████▋| 194/200 [10:59<00:22,  3.73s/it]Running Inference:  98%|█████████▊| 195/200 [11:04<00:19,  3.91s/it]Running Inference:  98%|█████████▊| 196/200 [11:06<00:13,  3.46s/it]Running Inference:  98%|█████████▊| 197/200 [11:07<00:08,  2.77s/it]Running Inference:  99%|█████████▉| 198/200 [11:12<00:06,  3.22s/it]Running Inference: 100%|█████████▉| 199/200 [11:15<00:03,  3.25s/it]Running Inference: 100%|██████████| 200/200 [11:18<00:00,  3.05s/it]Running Inference: 100%|██████████| 200/200 [11:18<00:00,  3.39s/it]
2025-12-13 19:59:41,705 - INFO - Inference completed.
2025-12-13 19:59:41,714 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-13 19:59:41,714 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:59:41,721 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-13 19:59:41,721 - INFO - Metrics:
8.88
2025-12-13 19:59:41,723 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=musique, ratio=0.3) on GPU 4

----------------------------------------
Task: musique | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:59:48,153 - INFO - Set deterministic seeds to 42
2025-12-13 19:59:48,154 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:59:48,154 - INFO - Starting evaluation run...
2025-12-13 19:59:48,154 - INFO - Output directory set to: longbenchresult
2025-12-13 19:59:48,154 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-13 19:59:48,154 - INFO - KV Press 'knorm' setup.
2025-12-13 19:59:48,154 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:59:48,154 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.92it/s]
Device set to use cuda:0
2025-12-13 20:00:01,749 - INFO - Model pipeline loaded.
2025-12-13 20:00:01,749 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 20:00:06,418 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:00:06,418 - INFO - Dataset processed with 200 entries.
2025-12-13 20:00:06,462 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<16:50,  5.08s/it]Running Inference:   1%|          | 2/200 [00:09<15:25,  4.67s/it]Running Inference:   2%|▏         | 3/200 [00:13<14:46,  4.50s/it]Running Inference:   2%|▏         | 4/200 [00:18<14:35,  4.47s/it]Running Inference:   2%|▎         | 5/200 [00:21<12:37,  3.88s/it]Running Inference:   3%|▎         | 6/200 [00:25<12:57,  4.01s/it]Running Inference:   4%|▎         | 7/200 [00:29<13:21,  4.15s/it]Running Inference:   4%|▍         | 8/200 [00:33<13:23,  4.18s/it]Running Inference:   4%|▍         | 9/200 [00:36<11:58,  3.76s/it]Running Inference:   5%|▌         | 10/200 [00:41<12:28,  3.94s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:43<10:43,  3.41s/it]Running Inference:   6%|▌         | 12/200 [00:47<11:30,  3.67s/it]Running Inference:   6%|▋         | 13/200 [00:50<10:16,  3.30s/it]Running Inference:   7%|▋         | 14/200 [00:54<11:11,  3.61s/it]Running Inference:   8%|▊         | 15/200 [00:58<11:19,  3.68s/it]Running Inference:   8%|▊         | 16/200 [01:00<09:46,  3.19s/it]Running Inference:   8%|▊         | 17/200 [01:03<09:21,  3.07s/it]Running Inference:   9%|▉         | 18/200 [01:07<10:19,  3.40s/it]Running Inference:  10%|▉         | 19/200 [01:11<11:03,  3.67s/it]Running Inference:  10%|█         | 20/200 [01:15<11:32,  3.85s/it]Running Inference:  10%|█         | 21/200 [01:20<11:49,  3.97s/it]Running Inference:  11%|█         | 22/200 [01:24<11:54,  4.02s/it]Running Inference:  12%|█▏        | 23/200 [01:26<10:18,  3.49s/it]Running Inference:  12%|█▏        | 24/200 [01:27<08:31,  2.90s/it]Running Inference:  12%|█▎        | 25/200 [01:32<09:38,  3.31s/it]Running Inference:  13%|█▎        | 26/200 [01:36<10:28,  3.61s/it]Running Inference:  14%|█▎        | 27/200 [01:40<10:58,  3.81s/it]Running Inference:  14%|█▍        | 28/200 [01:43<10:16,  3.59s/it]Running Inference:  14%|█▍        | 29/200 [01:48<10:48,  3.79s/it]Running Inference:  15%|█▌        | 30/200 [01:50<09:26,  3.33s/it]Running Inference:  16%|█▌        | 31/200 [01:52<08:26,  3.00s/it]Running Inference:  16%|█▌        | 32/200 [01:54<07:17,  2.60s/it]Running Inference:  16%|█▋        | 33/200 [01:56<07:13,  2.60s/it]Running Inference:  17%|█▋        | 34/200 [02:01<08:27,  3.06s/it]Running Inference:  18%|█▊        | 35/200 [02:03<08:13,  2.99s/it]Running Inference:  18%|█▊        | 36/200 [02:08<09:17,  3.40s/it]Running Inference:  18%|█▊        | 37/200 [02:12<09:58,  3.67s/it]Running Inference:  19%|█▉        | 38/200 [02:15<09:18,  3.45s/it]Running Inference:  20%|█▉        | 39/200 [02:17<08:12,  3.06s/it]Running Inference:  20%|██        | 40/200 [02:21<09:08,  3.43s/it]Running Inference:  20%|██        | 41/200 [02:26<09:41,  3.66s/it]Running Inference:  21%|██        | 42/200 [02:29<09:07,  3.47s/it]Running Inference:  22%|██▏       | 43/200 [02:33<09:46,  3.74s/it]Running Inference:  22%|██▏       | 44/200 [02:37<10:09,  3.91s/it]Running Inference:  22%|██▎       | 45/200 [02:41<10:15,  3.97s/it]Running Inference:  23%|██▎       | 46/200 [02:46<10:25,  4.06s/it]Running Inference:  24%|██▎       | 47/200 [02:50<10:38,  4.17s/it]Running Inference:  24%|██▍       | 48/200 [02:54<10:37,  4.20s/it]Running Inference:  24%|██▍       | 49/200 [02:59<10:38,  4.23s/it]Running Inference:  25%|██▌       | 50/200 [03:03<10:37,  4.25s/it]Running Inference:  26%|██▌       | 51/200 [03:07<10:34,  4.26s/it]Running Inference:  26%|██▌       | 52/200 [03:12<10:33,  4.28s/it]Running Inference:  26%|██▋       | 53/200 [03:16<10:27,  4.27s/it]Running Inference:  27%|██▋       | 54/200 [03:20<10:23,  4.27s/it]Running Inference:  28%|██▊       | 55/200 [03:24<10:21,  4.29s/it]Running Inference:  28%|██▊       | 56/200 [03:29<10:17,  4.29s/it]Running Inference:  28%|██▊       | 57/200 [03:33<10:17,  4.32s/it]Running Inference:  29%|██▉       | 58/200 [03:37<09:52,  4.17s/it]Running Inference:  30%|██▉       | 59/200 [03:39<08:25,  3.58s/it]Running Inference:  30%|███       | 60/200 [03:43<08:50,  3.79s/it]Running Inference:  30%|███       | 61/200 [03:48<09:11,  3.96s/it]Running Inference:  31%|███       | 62/200 [03:50<07:51,  3.42s/it]Running Inference:  32%|███▏      | 63/200 [03:54<08:24,  3.68s/it]Running Inference:  32%|███▏      | 64/200 [03:57<07:26,  3.28s/it]Running Inference:  32%|███▎      | 65/200 [04:00<07:46,  3.46s/it]Running Inference:  33%|███▎      | 66/200 [04:05<08:18,  3.72s/it]Running Inference:  34%|███▎      | 67/200 [04:07<07:17,  3.29s/it]Running Inference:  34%|███▍      | 68/200 [04:11<07:50,  3.57s/it]Running Inference:  34%|███▍      | 69/200 [04:16<08:15,  3.78s/it]Running Inference:  35%|███▌      | 70/200 [04:20<08:30,  3.93s/it]Running Inference:  36%|███▌      | 71/200 [04:23<07:51,  3.65s/it]Running Inference:  36%|███▌      | 72/200 [04:25<07:03,  3.30s/it]Running Inference:  36%|███▋      | 73/200 [04:30<07:35,  3.58s/it]Running Inference:  37%|███▋      | 74/200 [04:34<07:59,  3.81s/it]Running Inference:  38%|███▊      | 75/200 [04:38<08:18,  3.99s/it]Running Inference:  38%|███▊      | 76/200 [04:40<07:08,  3.45s/it]Running Inference:  38%|███▊      | 77/200 [04:44<07:16,  3.55s/it]Running Inference:  39%|███▉      | 78/200 [04:47<06:41,  3.29s/it]Running Inference:  40%|███▉      | 79/200 [04:51<07:14,  3.59s/it]Running Inference:  40%|████      | 80/200 [04:56<07:36,  3.81s/it]Running Inference:  40%|████      | 81/200 [05:00<07:49,  3.95s/it]Running Inference:  41%|████      | 82/200 [05:04<08:00,  4.07s/it]Running Inference:  42%|████▏     | 83/200 [05:06<06:52,  3.52s/it]Running Inference:  42%|████▏     | 84/200 [05:11<07:10,  3.72s/it]Running Inference:  42%|████▎     | 85/200 [05:13<06:25,  3.35s/it]Running Inference:  43%|████▎     | 86/200 [05:17<06:51,  3.61s/it]Running Inference:  44%|████▎     | 87/200 [05:20<06:33,  3.48s/it]Running Inference:  44%|████▍     | 88/200 [05:25<06:56,  3.72s/it]Running Inference:  44%|████▍     | 89/200 [05:29<07:12,  3.89s/it]Running Inference:  45%|████▌     | 90/200 [05:33<07:21,  4.01s/it]Running Inference:  46%|████▌     | 91/200 [05:38<07:22,  4.06s/it]Running Inference:  46%|████▌     | 92/200 [05:41<07:03,  3.93s/it]Running Inference:  46%|████▋     | 93/200 [05:45<07:08,  4.01s/it]Running Inference:  47%|████▋     | 94/200 [05:50<07:11,  4.07s/it]Running Inference:  48%|████▊     | 95/200 [05:54<07:13,  4.13s/it]Running Inference:  48%|████▊     | 96/200 [05:57<06:47,  3.92s/it]Running Inference:  48%|████▊     | 97/200 [06:00<05:55,  3.45s/it]Running Inference:  49%|████▉     | 98/200 [06:04<06:16,  3.69s/it]Running Inference:  50%|████▉     | 99/200 [06:07<05:42,  3.39s/it]Running Inference:  50%|█████     | 100/200 [06:11<05:58,  3.59s/it]Running Inference:  50%|█████     | 101/200 [06:15<06:11,  3.75s/it]Running Inference:  51%|█████     | 102/200 [06:19<06:23,  3.91s/it]Running Inference:  52%|█████▏    | 103/200 [06:23<06:29,  4.01s/it]Running Inference:  52%|█████▏    | 104/200 [06:28<06:38,  4.15s/it]Running Inference:  52%|█████▎    | 105/200 [06:32<06:40,  4.22s/it]Running Inference:  53%|█████▎    | 106/200 [06:36<06:38,  4.24s/it]Running Inference:  54%|█████▎    | 107/200 [06:41<06:31,  4.21s/it]Running Inference:  54%|█████▍    | 108/200 [06:43<05:33,  3.62s/it]Running Inference:  55%|█████▍    | 109/200 [06:47<05:50,  3.86s/it]Running Inference:  55%|█████▌    | 110/200 [06:51<05:59,  3.99s/it]Running Inference:  56%|█████▌    | 111/200 [06:56<05:56,  4.01s/it]Running Inference:  56%|█████▌    | 112/200 [07:00<05:56,  4.06s/it]Running Inference:  56%|█████▋    | 113/200 [07:04<05:58,  4.12s/it]Running Inference:  57%|█████▋    | 114/200 [07:08<05:58,  4.17s/it]Running Inference:  57%|█████▊    | 115/200 [07:13<05:56,  4.19s/it]Running Inference:  58%|█████▊    | 116/200 [07:15<05:02,  3.61s/it]Running Inference:  58%|█████▊    | 117/200 [07:18<04:38,  3.35s/it]Running Inference:  59%|█████▉    | 118/200 [07:22<04:56,  3.62s/it]Running Inference:  60%|█████▉    | 119/200 [07:24<04:18,  3.19s/it]Running Inference:  60%|██████    | 120/200 [07:28<04:42,  3.53s/it]Running Inference:  60%|██████    | 121/200 [07:31<04:08,  3.15s/it]Running Inference:  61%|██████    | 122/200 [07:35<04:31,  3.48s/it]Running Inference:  62%|██████▏   | 123/200 [07:39<04:48,  3.75s/it]Running Inference:  62%|██████▏   | 124/200 [07:43<04:56,  3.91s/it]Running Inference:  62%|██████▎   | 125/200 [07:47<04:45,  3.81s/it]Running Inference:  63%|██████▎   | 126/200 [07:51<04:53,  3.96s/it]Running Inference:  64%|██████▎   | 127/200 [07:55<04:53,  4.02s/it]Running Inference:  64%|██████▍   | 128/200 [08:00<04:54,  4.09s/it]Running Inference:  64%|██████▍   | 129/200 [08:03<04:27,  3.76s/it]Running Inference:  65%|██████▌   | 130/200 [08:07<04:34,  3.92s/it]Running Inference:  66%|██████▌   | 131/200 [08:11<04:37,  4.03s/it]Running Inference:  66%|██████▌   | 132/200 [08:16<04:39,  4.10s/it]Running Inference:  66%|██████▋   | 133/200 [08:19<04:15,  3.81s/it]Running Inference:  67%|██████▋   | 134/200 [08:20<03:25,  3.11s/it]Running Inference:  68%|██████▊   | 135/200 [08:25<03:47,  3.50s/it]Running Inference:  68%|██████▊   | 136/200 [08:29<04:00,  3.76s/it]Running Inference:  68%|██████▊   | 137/200 [08:31<03:31,  3.35s/it]Running Inference:  69%|██████▉   | 138/200 [08:35<03:24,  3.30s/it]Running Inference:  70%|██████▉   | 139/200 [08:39<03:39,  3.60s/it]Running Inference:  70%|███████   | 140/200 [08:42<03:20,  3.34s/it]Running Inference:  70%|███████   | 141/200 [08:46<03:35,  3.65s/it]Running Inference:  71%|███████   | 142/200 [08:50<03:42,  3.83s/it]Running Inference:  72%|███████▏  | 143/200 [08:55<03:46,  3.97s/it]Running Inference:  72%|███████▏  | 144/200 [08:58<03:26,  3.69s/it]Running Inference:  72%|███████▎  | 145/200 [09:02<03:31,  3.84s/it]Running Inference:  73%|███████▎  | 146/200 [09:04<03:02,  3.37s/it]Running Inference:  74%|███████▎  | 147/200 [09:08<03:04,  3.48s/it]Running Inference:  74%|███████▍  | 148/200 [09:12<03:14,  3.74s/it]Running Inference:  74%|███████▍  | 149/200 [09:16<03:06,  3.65s/it]Running Inference:  75%|███████▌  | 150/200 [09:20<03:12,  3.84s/it]Running Inference:  76%|███████▌  | 151/200 [09:24<03:15,  3.99s/it]Running Inference:  76%|███████▌  | 152/200 [09:28<03:15,  4.06s/it]Running Inference:  76%|███████▋  | 153/200 [09:33<03:14,  4.14s/it]Running Inference:  77%|███████▋  | 154/200 [09:37<03:11,  4.17s/it]Running Inference:  78%|███████▊  | 155/200 [09:39<02:41,  3.60s/it]Running Inference:  78%|███████▊  | 156/200 [09:43<02:40,  3.64s/it]Running Inference:  78%|███████▊  | 157/200 [09:47<02:45,  3.84s/it]Running Inference:  79%|███████▉  | 158/200 [09:52<02:47,  3.98s/it]Running Inference:  80%|███████▉  | 159/200 [09:54<02:30,  3.66s/it]Running Inference:  80%|████████  | 160/200 [09:59<02:32,  3.82s/it]Running Inference:  80%|████████  | 161/200 [10:01<02:10,  3.36s/it]Running Inference:  81%|████████  | 162/200 [10:05<02:18,  3.63s/it]Running Inference:  82%|████████▏ | 163/200 [10:09<02:20,  3.81s/it]Running Inference:  82%|████████▏ | 164/200 [10:14<02:20,  3.89s/it]Running Inference:  82%|████████▎ | 165/200 [10:18<02:19,  3.98s/it]Running Inference:  83%|████████▎ | 166/200 [10:22<02:17,  4.06s/it]Running Inference:  84%|████████▎ | 167/200 [10:26<02:17,  4.16s/it]Running Inference:  84%|████████▍ | 168/200 [10:31<02:14,  4.19s/it]Running Inference:  84%|████████▍ | 169/200 [10:35<02:10,  4.21s/it]Running Inference:  85%|████████▌ | 170/200 [10:39<02:06,  4.22s/it]Running Inference:  86%|████████▌ | 171/200 [10:43<02:02,  4.21s/it]Running Inference:  86%|████████▌ | 172/200 [10:48<01:58,  4.22s/it]Running Inference:  86%|████████▋ | 173/200 [10:51<01:48,  4.01s/it]Running Inference:  87%|████████▋ | 174/200 [10:55<01:46,  4.08s/it]Running Inference:  88%|████████▊ | 175/200 [11:00<01:43,  4.15s/it]Running Inference:  88%|████████▊ | 176/200 [11:04<01:40,  4.20s/it]Running Inference:  88%|████████▊ | 177/200 [11:08<01:36,  4.20s/it]Running Inference:  89%|████████▉ | 178/200 [11:11<01:25,  3.86s/it]Running Inference:  90%|████████▉ | 179/200 [11:16<01:24,  4.00s/it]Running Inference:  90%|█████████ | 180/200 [11:17<01:05,  3.29s/it]Running Inference:  90%|█████████ | 181/200 [11:19<00:56,  2.96s/it]Running Inference:  91%|█████████ | 182/200 [11:21<00:48,  2.71s/it]Running Inference:  92%|█████████▏| 183/200 [11:26<00:53,  3.15s/it]Running Inference:  92%|█████████▏| 184/200 [11:30<00:55,  3.48s/it]Running Inference:  92%|█████████▎| 185/200 [11:32<00:46,  3.11s/it]Running Inference:  93%|█████████▎| 186/200 [11:36<00:48,  3.45s/it]Running Inference:  94%|█████████▎| 187/200 [11:41<00:48,  3.71s/it]Running Inference:  94%|█████████▍| 188/200 [11:43<00:40,  3.35s/it]Running Inference:  94%|█████████▍| 189/200 [11:46<00:34,  3.17s/it]Running Inference:  95%|█████████▌| 190/200 [11:50<00:34,  3.41s/it]Running Inference:  96%|█████████▌| 191/200 [11:54<00:32,  3.66s/it]Running Inference:  96%|█████████▌| 192/200 [11:58<00:30,  3.84s/it]Running Inference:  96%|█████████▋| 193/200 [12:03<00:27,  3.96s/it]Running Inference:  97%|█████████▋| 194/200 [12:07<00:24,  4.05s/it]Running Inference:  98%|█████████▊| 195/200 [12:11<00:20,  4.13s/it]Running Inference:  98%|█████████▊| 196/200 [12:15<00:16,  4.07s/it]Running Inference:  98%|█████████▊| 197/200 [12:16<00:09,  3.16s/it]Running Inference:  99%|█████████▉| 198/200 [12:20<00:06,  3.48s/it]Running Inference: 100%|█████████▉| 199/200 [12:24<00:03,  3.55s/it]Running Inference: 100%|██████████| 200/200 [12:28<00:00,  3.74s/it]Running Inference: 100%|██████████| 200/200 [12:28<00:00,  3.74s/it]
2025-12-13 20:12:35,358 - INFO - Inference completed.
2025-12-13 20:12:35,367 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-13 20:12:35,367 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:12:35,375 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-13 20:12:35,375 - INFO - Metrics:
3.6
2025-12-13 20:12:35,377 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=musique, ratio=0.5) on GPU 4


========================================
LongBench Task: narrativeqa
========================================
----------------------------------------
Task: narrativeqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:12:41,684 - INFO - Set deterministic seeds to 42
2025-12-13 20:12:41,684 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:12:41,684 - INFO - Starting evaluation run...
2025-12-13 20:12:41,684 - INFO - Output directory set to: longbenchresult
2025-12-13 20:12:41,685 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 20:12:41,685 - INFO - KV Press 'knorm' setup.
2025-12-13 20:12:41,685 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:12:41,685 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.53it/s]
Device set to use cuda:0
2025-12-13 20:12:54,008 - INFO - Model pipeline loaded.
2025-12-13 20:12:54,008 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:12:59,843 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:12:59,844 - INFO - Dataset processed with 200 entries.
2025-12-13 20:12:59,868 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:13<04:18, 13.63s/it]Running Inference:  10%|█         | 2/20 [00:23<03:26, 11.45s/it]Running Inference:  15%|█▌        | 3/20 [01:00<06:29, 22.94s/it]Running Inference:  20%|██        | 4/20 [01:25<06:19, 23.75s/it]Running Inference:  25%|██▌       | 5/20 [02:04<07:21, 29.44s/it]Running Inference:  30%|███       | 6/20 [02:20<05:46, 24.74s/it]Running Inference:  35%|███▌      | 7/20 [02:53<05:56, 27.43s/it]Running Inference:  40%|████      | 8/20 [03:18<05:20, 26.69s/it]Running Inference:  45%|████▌     | 9/20 [03:24<03:43, 20.34s/it]Running Inference:  50%|█████     | 10/20 [03:54<03:51, 23.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [04:05<02:54, 19.39s/it]Running Inference:  60%|██████    | 12/20 [04:16<02:14, 16.83s/it]Running Inference:  65%|██████▌   | 13/20 [04:22<01:36, 13.74s/it]Running Inference:  70%|███████   | 14/20 [04:28<01:08, 11.46s/it]Running Inference:  75%|███████▌  | 15/20 [04:43<01:01, 12.36s/it]Running Inference:  80%|████████  | 16/20 [04:48<00:40, 10.12s/it]Running Inference:  85%|████████▌ | 17/20 [04:54<00:27,  9.02s/it]Running Inference:  90%|█████████ | 18/20 [05:00<00:15,  7.98s/it]Running Inference:  95%|█████████▌| 19/20 [05:04<00:06,  6.91s/it]Running Inference: 100%|██████████| 20/20 [05:14<00:00,  7.93s/it]Running Inference: 100%|██████████| 20/20 [05:14<00:00, 15.75s/it]
2025-12-13 20:18:14,871 - INFO - Inference completed.
2025-12-13 20:18:14,879 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 20:18:14,879 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:18:14,884 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 20:18:14,884 - INFO - Metrics:
14.47
2025-12-13 20:18:14,885 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=narrativeqa, ratio=0.1) on GPU 4

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:18:21,292 - INFO - Set deterministic seeds to 42
2025-12-13 20:18:21,293 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:18:21,293 - INFO - Starting evaluation run...
2025-12-13 20:18:21,293 - INFO - Output directory set to: longbenchresult
2025-12-13 20:18:21,293 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 20:18:21,293 - INFO - KV Press 'knorm' setup.
2025-12-13 20:18:21,293 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:18:21,293 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.76it/s]
Device set to use cuda:0
2025-12-13 20:18:35,595 - INFO - Model pipeline loaded.
2025-12-13 20:18:35,596 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:18:42,039 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:18:42,040 - INFO - Dataset processed with 200 entries.
2025-12-13 20:18:42,063 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:13<04:17, 13.54s/it]Running Inference:  10%|█         | 2/20 [00:23<03:21, 11.17s/it]Running Inference:  15%|█▌        | 3/20 [00:57<06:08, 21.65s/it]Running Inference:  20%|██        | 4/20 [01:21<06:05, 22.87s/it]Running Inference:  25%|██▌       | 5/20 [02:01<07:14, 28.94s/it]Running Inference:  30%|███       | 6/20 [02:16<05:37, 24.08s/it]Running Inference:  35%|███▌      | 7/20 [02:48<05:46, 26.68s/it]Running Inference:  40%|████      | 8/20 [03:26<06:03, 30.29s/it]Running Inference:  45%|████▌     | 9/20 [03:38<04:32, 24.75s/it]Running Inference:  50%|█████     | 10/20 [03:56<03:46, 22.68s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [04:07<02:50, 18.91s/it]Running Inference:  60%|██████    | 12/20 [04:38<03:00, 22.55s/it]Running Inference:  65%|██████▌   | 13/20 [04:55<02:27, 21.08s/it]Running Inference:  70%|███████   | 14/20 [05:01<01:37, 16.33s/it]Running Inference:  75%|███████▌  | 15/20 [05:14<01:17, 15.51s/it]Running Inference:  80%|████████  | 16/20 [05:26<00:57, 14.28s/it]Running Inference:  85%|████████▌ | 17/20 [05:32<00:35, 11.84s/it]Running Inference:  90%|█████████ | 18/20 [05:38<00:20, 10.20s/it]Running Inference:  95%|█████████▌| 19/20 [05:43<00:08,  8.50s/it]Running Inference: 100%|██████████| 20/20 [06:10<00:00, 14.05s/it]Running Inference: 100%|██████████| 20/20 [06:10<00:00, 18.52s/it]
2025-12-13 20:24:52,418 - INFO - Inference completed.
2025-12-13 20:24:52,426 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-13 20:24:52,426 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:24:52,431 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-13 20:24:52,432 - INFO - Metrics:
13.26
2025-12-13 20:24:52,433 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=narrativeqa, ratio=0.2) on GPU 4

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:24:59,036 - INFO - Set deterministic seeds to 42
2025-12-13 20:24:59,036 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:24:59,036 - INFO - Starting evaluation run...
2025-12-13 20:24:59,036 - INFO - Output directory set to: longbenchresult
2025-12-13 20:24:59,037 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-13 20:24:59,037 - INFO - KV Press 'knorm' setup.
2025-12-13 20:24:59,037 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:24:59,037 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.92it/s]
Device set to use cuda:0
2025-12-13 20:25:11,544 - INFO - Model pipeline loaded.
2025-12-13 20:25:11,544 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:25:17,062 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:25:17,062 - INFO - Dataset processed with 200 entries.
2025-12-13 20:25:17,088 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:13<04:09, 13.13s/it]Running Inference:  10%|█         | 2/20 [00:22<03:19, 11.08s/it]Running Inference:  15%|█▌        | 3/20 [00:45<04:40, 16.52s/it]Running Inference:  20%|██        | 4/20 [01:17<06:02, 22.67s/it]Running Inference:  25%|██▌       | 5/20 [01:51<06:36, 26.46s/it]Running Inference:  30%|███       | 6/20 [02:05<05:13, 22.40s/it]Running Inference:  35%|███▌      | 7/20 [02:41<05:46, 26.69s/it]Running Inference:  40%|████      | 8/20 [03:16<05:54, 29.51s/it]Running Inference:  45%|████▌     | 9/20 [03:23<04:06, 22.44s/it]Running Inference:  50%|█████     | 10/20 [03:47<03:49, 22.94s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [03:52<02:36, 17.44s/it]Running Inference:  60%|██████    | 12/20 [04:22<02:49, 21.17s/it]Running Inference:  65%|██████▌   | 13/20 [04:35<02:10, 18.65s/it]Running Inference:  70%|███████   | 14/20 [04:40<01:28, 14.71s/it]Running Inference:  75%|███████▌  | 15/20 [05:00<01:20, 16.14s/it]Running Inference:  80%|████████  | 16/20 [05:25<01:15, 18.77s/it]Running Inference:  85%|████████▌ | 17/20 [05:38<00:51, 17.17s/it]Running Inference:  90%|█████████ | 18/20 [05:43<00:27, 13.60s/it]Running Inference:  95%|█████████▌| 19/20 [05:53<00:12, 12.50s/it]Running Inference: 100%|██████████| 20/20 [06:14<00:00, 15.03s/it]Running Inference: 100%|██████████| 20/20 [06:14<00:00, 18.73s/it]
2025-12-13 20:31:31,765 - INFO - Inference completed.
2025-12-13 20:31:31,773 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-13 20:31:31,773 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:31:31,778 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-13 20:31:31,778 - INFO - Metrics:
11.55
2025-12-13 20:31:31,780 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=narrativeqa, ratio=0.3) on GPU 4

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:31:38,231 - INFO - Set deterministic seeds to 42
2025-12-13 20:31:38,231 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:31:38,232 - INFO - Starting evaluation run...
2025-12-13 20:31:38,232 - INFO - Output directory set to: longbenchresult
2025-12-13 20:31:38,232 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-13 20:31:38,232 - INFO - KV Press 'knorm' setup.
2025-12-13 20:31:38,232 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:31:38,232 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.02it/s]
Device set to use cuda:0
2025-12-13 20:31:49,565 - INFO - Model pipeline loaded.
2025-12-13 20:31:49,566 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:31:56,717 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:31:56,717 - INFO - Dataset processed with 200 entries.
2025-12-13 20:31:56,741 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:12<04:06, 12.95s/it]Running Inference:  10%|█         | 2/20 [00:34<05:27, 18.17s/it]Running Inference:  15%|█▌        | 3/20 [01:06<06:50, 24.15s/it]Running Inference:  20%|██        | 4/20 [01:43<07:52, 29.51s/it]Running Inference:  25%|██▌       | 5/20 [02:34<09:17, 37.15s/it]Running Inference:  30%|███       | 6/20 [02:48<06:51, 29.41s/it]Running Inference:  35%|███▌      | 7/20 [03:36<07:38, 35.29s/it]Running Inference:  40%|████      | 8/20 [04:10<07:01, 35.12s/it]Running Inference:  45%|████▌     | 9/20 [04:17<04:49, 26.30s/it]Running Inference:  50%|█████     | 10/20 [05:13<05:53, 35.35s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [05:23<04:08, 27.65s/it]Running Inference:  60%|██████    | 12/20 [05:53<03:46, 28.27s/it]Running Inference:  65%|██████▌   | 13/20 [06:18<03:10, 27.17s/it]Running Inference:  70%|███████   | 14/20 [06:36<02:27, 24.61s/it]Running Inference:  75%|███████▌  | 15/20 [06:50<01:47, 21.43s/it]Running Inference:  80%|████████  | 16/20 [07:01<01:12, 18.15s/it]Running Inference:  85%|████████▌ | 17/20 [07:21<00:56, 18.75s/it]Running Inference:  90%|█████████ | 18/20 [07:27<00:29, 14.92s/it]Running Inference:  95%|█████████▌| 19/20 [07:32<00:11, 11.85s/it]Running Inference: 100%|██████████| 20/20 [07:40<00:00, 10.86s/it]Running Inference: 100%|██████████| 20/20 [07:40<00:00, 23.04s/it]
2025-12-13 20:39:37,497 - INFO - Inference completed.
2025-12-13 20:39:37,505 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-13 20:39:37,506 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:39:37,512 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-13 20:39:37,512 - INFO - Metrics:
7.88
2025-12-13 20:39:37,513 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=narrativeqa, ratio=0.5) on GPU 4


========================================
LongBench Task: qasper
========================================
----------------------------------------
Task: qasper | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:39:43,990 - INFO - Set deterministic seeds to 42
2025-12-13 20:39:43,991 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:39:43,991 - INFO - Starting evaluation run...
2025-12-13 20:39:43,991 - INFO - Output directory set to: longbenchresult
2025-12-13 20:39:43,991 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 20:39:43,991 - INFO - KV Press 'knorm' setup.
2025-12-13 20:39:43,991 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:39:43,991 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.22it/s]
Device set to use cuda:0
2025-12-13 20:39:56,805 - INFO - Model pipeline loaded.
2025-12-13 20:39:56,805 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:40:03,559 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:40:03,559 - INFO - Dataset processed with 200 entries.
2025-12-13 20:40:03,571 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:09<23:31,  9.60s/it]Running Inference:   1%|▏         | 2/148 [00:09<10:09,  4.18s/it]Running Inference:   2%|▏         | 3/148 [00:11<06:42,  2.78s/it]Running Inference:   3%|▎         | 4/148 [00:12<05:25,  2.26s/it]Running Inference:   3%|▎         | 5/148 [00:14<05:16,  2.21s/it]Running Inference:   4%|▍         | 6/148 [00:16<04:32,  1.92s/it]Running Inference:   5%|▍         | 7/148 [00:16<03:42,  1.58s/it]Running Inference:   5%|▌         | 8/148 [00:17<03:18,  1.42s/it]Running Inference:   6%|▌         | 9/148 [00:19<03:32,  1.53s/it]Running Inference:   7%|▋         | 10/148 [00:21<03:42,  1.61s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:23<03:39,  1.60s/it]Running Inference:   8%|▊         | 12/148 [00:24<03:31,  1.55s/it]Running Inference:   9%|▉         | 13/148 [00:31<07:17,  3.24s/it]Running Inference:   9%|▉         | 14/148 [00:32<05:51,  2.62s/it]Running Inference:  10%|█         | 15/148 [00:33<04:30,  2.04s/it]Running Inference:  11%|█         | 16/148 [00:36<05:00,  2.27s/it]Running Inference:  11%|█▏        | 17/148 [00:38<04:36,  2.11s/it]Running Inference:  12%|█▏        | 18/148 [00:40<04:46,  2.21s/it]Running Inference:  13%|█▎        | 19/148 [00:41<03:36,  1.68s/it]Running Inference:  14%|█▎        | 20/148 [00:41<03:02,  1.42s/it]Running Inference:  14%|█▍        | 21/148 [00:42<02:50,  1.34s/it]Running Inference:  15%|█▍        | 22/148 [00:44<03:08,  1.50s/it]Running Inference:  16%|█▌        | 23/148 [00:45<02:29,  1.19s/it]Running Inference:  16%|█▌        | 24/148 [00:46<02:30,  1.21s/it]Running Inference:  17%|█▋        | 25/148 [00:48<03:07,  1.52s/it]Running Inference:  18%|█▊        | 26/148 [00:53<04:47,  2.36s/it]Running Inference:  18%|█▊        | 27/148 [00:54<04:01,  1.99s/it]Running Inference:  19%|█▉        | 28/148 [00:55<03:21,  1.68s/it]Running Inference:  20%|█▉        | 29/148 [00:57<03:31,  1.78s/it]Running Inference:  20%|██        | 30/148 [00:57<02:45,  1.40s/it]Running Inference:  21%|██        | 31/148 [00:59<02:38,  1.35s/it]Running Inference:  22%|██▏       | 32/148 [01:00<02:53,  1.49s/it]Running Inference:  22%|██▏       | 33/148 [01:01<02:34,  1.34s/it]Running Inference:  23%|██▎       | 34/148 [01:03<02:34,  1.35s/it]Running Inference:  24%|██▎       | 35/148 [01:04<02:30,  1.33s/it]Running Inference:  24%|██▍       | 36/148 [01:06<02:52,  1.54s/it]Running Inference:  25%|██▌       | 37/148 [01:07<02:36,  1.41s/it]Running Inference:  26%|██▌       | 38/148 [01:08<02:20,  1.28s/it]Running Inference:  26%|██▋       | 39/148 [01:08<01:50,  1.02s/it]Running Inference:  27%|██▋       | 40/148 [01:09<01:35,  1.13it/s]Running Inference:  28%|██▊       | 41/148 [01:10<01:45,  1.02it/s]Running Inference:  28%|██▊       | 42/148 [01:11<01:31,  1.15it/s]Running Inference:  29%|██▉       | 43/148 [01:12<01:54,  1.09s/it]Running Inference:  30%|██▉       | 44/148 [01:14<02:13,  1.28s/it]Running Inference:  30%|███       | 45/148 [01:16<02:30,  1.47s/it]Running Inference:  31%|███       | 46/148 [01:23<05:11,  3.05s/it]Running Inference:  32%|███▏      | 47/148 [01:25<04:50,  2.88s/it]Running Inference:  32%|███▏      | 48/148 [01:27<04:04,  2.45s/it]Running Inference:  33%|███▎      | 49/148 [01:37<08:02,  4.88s/it]Running Inference:  34%|███▍      | 50/148 [01:39<06:14,  3.82s/it]Running Inference:  34%|███▍      | 51/148 [01:40<05:11,  3.21s/it]Running Inference:  35%|███▌      | 52/148 [01:42<04:31,  2.83s/it]Running Inference:  36%|███▌      | 53/148 [01:49<06:25,  4.05s/it]Running Inference:  36%|███▋      | 54/148 [01:50<04:52,  3.11s/it]Running Inference:  37%|███▋      | 55/148 [01:51<03:42,  2.39s/it]Running Inference:  38%|███▊      | 56/148 [01:52<03:06,  2.02s/it]Running Inference:  39%|███▊      | 57/148 [01:59<05:16,  3.48s/it]Running Inference:  39%|███▉      | 58/148 [02:00<04:14,  2.83s/it]Running Inference:  40%|███▉      | 59/148 [02:01<03:17,  2.22s/it]Running Inference:  41%|████      | 60/148 [02:02<02:35,  1.76s/it]Running Inference:  41%|████      | 61/148 [02:03<02:19,  1.61s/it]Running Inference:  42%|████▏     | 62/148 [02:04<01:57,  1.37s/it]Running Inference:  43%|████▎     | 63/148 [02:05<01:59,  1.41s/it]Running Inference:  43%|████▎     | 64/148 [02:08<02:32,  1.82s/it]Running Inference:  44%|████▍     | 65/148 [02:10<02:37,  1.89s/it]Running Inference:  45%|████▍     | 66/148 [02:12<02:21,  1.73s/it]Running Inference:  45%|████▌     | 67/148 [02:13<02:12,  1.63s/it]Running Inference:  46%|████▌     | 68/148 [02:14<01:48,  1.35s/it]Running Inference:  47%|████▋     | 69/148 [02:15<01:44,  1.32s/it]Running Inference:  47%|████▋     | 70/148 [02:16<01:35,  1.22s/it]Running Inference:  48%|████▊     | 71/148 [02:23<03:51,  3.01s/it]Running Inference:  49%|████▊     | 72/148 [02:25<03:22,  2.67s/it]Running Inference:  49%|████▉     | 73/148 [02:26<02:49,  2.25s/it]Running Inference:  50%|█████     | 74/148 [02:28<02:33,  2.08s/it]Running Inference:  51%|█████     | 75/148 [02:35<04:16,  3.52s/it]Running Inference:  51%|█████▏    | 76/148 [02:37<03:49,  3.18s/it]Running Inference:  52%|█████▏    | 77/148 [02:38<03:04,  2.59s/it]Running Inference:  53%|█████▎    | 78/148 [02:39<02:26,  2.09s/it]Running Inference:  53%|█████▎    | 79/148 [02:40<01:57,  1.70s/it]Running Inference:  54%|█████▍    | 80/148 [02:41<01:49,  1.61s/it]Running Inference:  55%|█████▍    | 81/148 [02:42<01:34,  1.40s/it]Running Inference:  55%|█████▌    | 82/148 [02:49<03:21,  3.06s/it]Running Inference:  56%|█████▌    | 83/148 [02:50<02:37,  2.43s/it]Running Inference:  57%|█████▋    | 84/148 [02:51<01:57,  1.84s/it]Running Inference:  57%|█████▋    | 85/148 [02:52<01:38,  1.56s/it]Running Inference:  58%|█████▊    | 86/148 [02:53<01:27,  1.40s/it]Running Inference:  59%|█████▉    | 87/148 [02:54<01:31,  1.49s/it]Running Inference:  59%|█████▉    | 88/148 [02:55<01:19,  1.33s/it]Running Inference:  60%|██████    | 89/148 [02:57<01:27,  1.49s/it]Running Inference:  61%|██████    | 90/148 [03:04<03:01,  3.13s/it]Running Inference:  61%|██████▏   | 91/148 [03:05<02:22,  2.50s/it]Running Inference:  62%|██████▏   | 92/148 [03:06<01:50,  1.97s/it]Running Inference:  63%|██████▎   | 93/148 [03:08<01:50,  2.01s/it]Running Inference:  64%|██████▎   | 94/148 [03:10<01:43,  1.91s/it]Running Inference:  64%|██████▍   | 95/148 [03:11<01:29,  1.68s/it]Running Inference:  65%|██████▍   | 96/148 [03:18<02:50,  3.28s/it]Running Inference:  66%|██████▌   | 97/148 [03:19<02:07,  2.50s/it]Running Inference:  66%|██████▌   | 98/148 [03:21<02:04,  2.48s/it]Running Inference:  67%|██████▋   | 99/148 [03:22<01:43,  2.12s/it]Running Inference:  68%|██████▊   | 100/148 [03:24<01:34,  1.96s/it]Running Inference:  68%|██████▊   | 101/148 [03:25<01:21,  1.74s/it]Running Inference:  69%|██████▉   | 102/148 [03:29<01:56,  2.53s/it]Running Inference:  70%|██████▉   | 103/148 [03:31<01:34,  2.10s/it]Running Inference:  70%|███████   | 104/148 [03:33<01:30,  2.06s/it]Running Inference:  71%|███████   | 105/148 [03:35<01:29,  2.07s/it]Running Inference:  72%|███████▏  | 106/148 [03:38<01:37,  2.33s/it]Running Inference:  72%|███████▏  | 107/148 [03:38<01:12,  1.76s/it]Running Inference:  73%|███████▎  | 108/148 [03:39<00:57,  1.43s/it]Running Inference:  74%|███████▎  | 109/148 [03:39<00:48,  1.25s/it]Running Inference:  74%|███████▍  | 110/148 [03:40<00:38,  1.02s/it]Running Inference:  75%|███████▌  | 111/148 [03:41<00:35,  1.03it/s]Running Inference:  76%|███████▌  | 112/148 [03:41<00:30,  1.20it/s]Running Inference:  76%|███████▋  | 113/148 [03:43<00:39,  1.13s/it]Running Inference:  77%|███████▋  | 114/148 [03:53<02:03,  3.64s/it]Running Inference:  78%|███████▊  | 115/148 [03:53<01:27,  2.64s/it]Running Inference:  78%|███████▊  | 116/148 [03:54<01:10,  2.22s/it]Running Inference:  79%|███████▉  | 117/148 [03:55<00:58,  1.89s/it]Running Inference:  80%|███████▉  | 118/148 [03:56<00:44,  1.48s/it]Running Inference:  80%|████████  | 119/148 [03:58<00:46,  1.60s/it]Running Inference:  81%|████████  | 120/148 [03:59<00:46,  1.65s/it]Running Inference:  82%|████████▏ | 121/148 [04:01<00:43,  1.60s/it]Running Inference:  82%|████████▏ | 122/148 [04:02<00:34,  1.33s/it]Running Inference:  83%|████████▎ | 123/148 [04:04<00:37,  1.48s/it]Running Inference:  84%|████████▍ | 124/148 [04:05<00:37,  1.55s/it]Running Inference:  84%|████████▍ | 125/148 [04:15<01:30,  3.95s/it]Running Inference:  85%|████████▌ | 126/148 [04:18<01:20,  3.67s/it]Running Inference:  86%|████████▌ | 127/148 [04:19<01:00,  2.87s/it]Running Inference:  86%|████████▋ | 128/148 [04:20<00:44,  2.25s/it]Running Inference:  87%|████████▋ | 129/148 [04:20<00:32,  1.71s/it]Running Inference:  88%|████████▊ | 130/148 [04:21<00:27,  1.51s/it]Running Inference:  89%|████████▊ | 131/148 [04:24<00:32,  1.92s/it]Running Inference:  89%|████████▉ | 132/148 [04:27<00:34,  2.16s/it]Running Inference:  90%|████████▉ | 133/148 [04:34<00:57,  3.84s/it]Running Inference:  91%|█████████ | 134/148 [04:38<00:51,  3.71s/it]Running Inference:  91%|█████████ | 135/148 [04:39<00:37,  2.85s/it]Running Inference:  92%|█████████▏| 136/148 [04:40<00:28,  2.35s/it]Running Inference:  93%|█████████▎| 137/148 [04:42<00:24,  2.25s/it]Running Inference:  93%|█████████▎| 138/148 [04:43<00:20,  2.02s/it]Running Inference:  94%|█████████▍| 139/148 [04:44<00:14,  1.65s/it]Running Inference:  95%|█████████▍| 140/148 [04:45<00:11,  1.41s/it]Running Inference:  95%|█████████▌| 141/148 [04:47<00:11,  1.60s/it]Running Inference:  96%|█████████▌| 142/148 [04:48<00:08,  1.40s/it]Running Inference:  97%|█████████▋| 143/148 [04:55<00:15,  3.13s/it]Running Inference:  97%|█████████▋| 144/148 [04:57<00:10,  2.61s/it]Running Inference:  98%|█████████▊| 145/148 [04:57<00:05,  1.98s/it]Running Inference:  99%|█████████▊| 146/148 [04:59<00:03,  1.94s/it]Running Inference:  99%|█████████▉| 147/148 [05:00<00:01,  1.70s/it]Running Inference: 100%|██████████| 148/148 [05:01<00:00,  1.35s/it]Running Inference: 100%|██████████| 148/148 [05:01<00:00,  2.03s/it]
2025-12-13 20:45:04,667 - INFO - Inference completed.
2025-12-13 20:45:04,677 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 20:45:04,677 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:45:04,687 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 20:45:04,687 - INFO - Metrics:
26.64
2025-12-13 20:45:04,688 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qasper, ratio=0.1) on GPU 4

----------------------------------------
Task: qasper | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:45:11,096 - INFO - Set deterministic seeds to 42
2025-12-13 20:45:11,096 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:45:11,096 - INFO - Starting evaluation run...
2025-12-13 20:45:11,096 - INFO - Output directory set to: longbenchresult
2025-12-13 20:45:11,096 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 20:45:11,096 - INFO - KV Press 'knorm' setup.
2025-12-13 20:45:11,096 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:45:11,096 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.16it/s]
Device set to use cuda:0
2025-12-13 20:45:29,593 - INFO - Model pipeline loaded.
2025-12-13 20:45:29,593 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:45:35,182 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:45:35,182 - INFO - Dataset processed with 200 entries.
2025-12-13 20:45:35,196 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:09<23:55,  9.76s/it]Running Inference:   1%|▏         | 2/148 [00:10<10:19,  4.24s/it]Running Inference:   2%|▏         | 3/148 [00:11<06:48,  2.82s/it]Running Inference:   3%|▎         | 4/148 [00:12<05:29,  2.29s/it]Running Inference:   3%|▎         | 5/148 [00:13<04:25,  1.86s/it]Running Inference:   4%|▍         | 6/148 [00:27<13:33,  5.73s/it]Running Inference:   5%|▍         | 7/148 [00:27<09:41,  4.13s/it]Running Inference:   5%|▌         | 8/148 [00:28<07:08,  3.06s/it]Running Inference:   6%|▌         | 9/148 [00:30<06:09,  2.66s/it]Running Inference:   7%|▋         | 10/148 [00:32<05:27,  2.37s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:33<04:52,  2.13s/it]Running Inference:   8%|▊         | 12/148 [00:35<04:21,  1.93s/it]Running Inference:   9%|▉         | 13/148 [00:42<07:54,  3.52s/it]Running Inference:   9%|▉         | 14/148 [00:44<06:34,  2.95s/it]Running Inference:  10%|█         | 15/148 [00:44<05:00,  2.26s/it]Running Inference:  11%|█         | 16/148 [00:47<05:20,  2.43s/it]Running Inference:  11%|█▏        | 17/148 [00:49<04:42,  2.15s/it]Running Inference:  12%|█▏        | 18/148 [00:51<04:47,  2.21s/it]Running Inference:  13%|█▎        | 19/148 [00:52<04:00,  1.87s/it]Running Inference:  14%|█▎        | 20/148 [00:53<03:20,  1.57s/it]Running Inference:  14%|█▍        | 21/148 [00:54<03:19,  1.57s/it]Running Inference:  15%|█▍        | 22/148 [00:57<04:01,  1.91s/it]Running Inference:  16%|█▌        | 23/148 [00:58<03:05,  1.49s/it]Running Inference:  16%|█▌        | 24/148 [00:59<02:49,  1.37s/it]Running Inference:  17%|█▋        | 25/148 [01:01<03:24,  1.66s/it]Running Inference:  18%|█▊        | 26/148 [01:03<03:46,  1.86s/it]Running Inference:  18%|█▊        | 27/148 [01:06<04:29,  2.23s/it]Running Inference:  19%|█▉        | 28/148 [01:07<03:44,  1.87s/it]Running Inference:  20%|█▉        | 29/148 [01:09<03:39,  1.85s/it]Running Inference:  20%|██        | 30/148 [01:11<03:21,  1.71s/it]Running Inference:  21%|██        | 31/148 [01:12<02:52,  1.47s/it]Running Inference:  22%|██▏       | 32/148 [01:13<03:00,  1.56s/it]Running Inference:  22%|██▏       | 33/148 [01:14<02:39,  1.39s/it]Running Inference:  23%|██▎       | 34/148 [01:21<05:53,  3.10s/it]Running Inference:  24%|██▎       | 35/148 [01:23<04:49,  2.56s/it]Running Inference:  24%|██▍       | 36/148 [01:31<08:05,  4.33s/it]Running Inference:  25%|██▌       | 37/148 [01:32<06:15,  3.38s/it]Running Inference:  26%|██▌       | 38/148 [01:33<04:53,  2.67s/it]Running Inference:  26%|██▋       | 39/148 [01:34<03:36,  1.99s/it]Running Inference:  27%|██▋       | 40/148 [01:34<02:48,  1.56s/it]Running Inference:  28%|██▊       | 41/148 [01:39<04:10,  2.35s/it]Running Inference:  28%|██▊       | 42/148 [01:39<03:14,  1.83s/it]Running Inference:  29%|██▉       | 43/148 [01:41<03:05,  1.76s/it]Running Inference:  30%|██▉       | 44/148 [01:42<02:51,  1.65s/it]Running Inference:  30%|███       | 45/148 [01:43<02:37,  1.53s/it]Running Inference:  31%|███       | 46/148 [01:45<02:33,  1.51s/it]Running Inference:  32%|███▏      | 47/148 [01:46<02:25,  1.44s/it]Running Inference:  32%|███▏      | 48/148 [01:47<02:06,  1.26s/it]Running Inference:  33%|███▎      | 49/148 [01:49<02:36,  1.59s/it]Running Inference:  34%|███▍      | 50/148 [01:51<02:32,  1.55s/it]Running Inference:  34%|███▍      | 51/148 [01:53<02:37,  1.62s/it]Running Inference:  35%|███▌      | 52/148 [01:56<03:19,  2.08s/it]Running Inference:  36%|███▌      | 53/148 [02:03<05:32,  3.50s/it]Running Inference:  36%|███▋      | 54/148 [02:03<04:15,  2.72s/it]Running Inference:  37%|███▋      | 55/148 [02:04<03:16,  2.12s/it]Running Inference:  38%|███▊      | 56/148 [02:05<02:43,  1.78s/it]Running Inference:  39%|███▊      | 57/148 [02:06<02:17,  1.51s/it]Running Inference:  39%|███▉      | 58/148 [02:08<02:18,  1.54s/it]Running Inference:  40%|███▉      | 59/148 [02:08<01:59,  1.34s/it]Running Inference:  41%|████      | 60/148 [02:09<01:40,  1.15s/it]Running Inference:  41%|████      | 61/148 [02:10<01:27,  1.00s/it]Running Inference:  42%|████▏     | 62/148 [02:11<01:21,  1.06it/s]Running Inference:  43%|████▎     | 63/148 [02:12<01:28,  1.04s/it]Running Inference:  43%|████▎     | 64/148 [02:14<02:02,  1.46s/it]Running Inference:  44%|████▍     | 65/148 [02:16<02:16,  1.64s/it]Running Inference:  45%|████▍     | 66/148 [02:18<02:01,  1.48s/it]Running Inference:  45%|████▌     | 67/148 [02:19<01:48,  1.34s/it]Running Inference:  46%|████▌     | 68/148 [02:19<01:31,  1.15s/it]Running Inference:  47%|████▋     | 69/148 [02:20<01:30,  1.15s/it]Running Inference:  47%|████▋     | 70/148 [02:22<01:51,  1.43s/it]Running Inference:  48%|████▊     | 71/148 [02:30<04:02,  3.15s/it]Running Inference:  49%|████▊     | 72/148 [02:32<03:30,  2.77s/it]Running Inference:  49%|████▉     | 73/148 [02:33<02:54,  2.32s/it]Running Inference:  50%|█████     | 74/148 [02:34<02:37,  2.13s/it]Running Inference:  51%|█████     | 75/148 [02:35<02:07,  1.75s/it]Running Inference:  51%|█████▏    | 76/148 [02:42<04:00,  3.34s/it]Running Inference:  52%|█████▏    | 77/148 [02:44<03:14,  2.74s/it]Running Inference:  53%|█████▎    | 78/148 [02:51<04:41,  4.02s/it]Running Inference:  53%|█████▎    | 79/148 [02:52<03:32,  3.08s/it]Running Inference:  54%|█████▍    | 80/148 [02:53<02:49,  2.49s/it]Running Inference:  55%|█████▍    | 81/148 [02:54<02:15,  2.02s/it]Running Inference:  55%|█████▌    | 82/148 [02:55<01:57,  1.78s/it]Running Inference:  56%|█████▌    | 83/148 [02:56<01:39,  1.53s/it]Running Inference:  57%|█████▋    | 84/148 [02:56<01:17,  1.22s/it]Running Inference:  57%|█████▋    | 85/148 [02:58<01:18,  1.24s/it]Running Inference:  58%|█████▊    | 86/148 [02:59<01:19,  1.28s/it]Running Inference:  59%|█████▉    | 87/148 [03:01<01:26,  1.42s/it]Running Inference:  59%|█████▉    | 88/148 [03:02<01:16,  1.28s/it]Running Inference:  60%|██████    | 89/148 [03:03<01:22,  1.39s/it]Running Inference:  61%|██████    | 90/148 [03:10<02:57,  3.07s/it]Running Inference:  61%|██████▏   | 91/148 [03:12<02:24,  2.54s/it]Running Inference:  62%|██████▏   | 92/148 [03:12<01:51,  2.00s/it]Running Inference:  63%|██████▎   | 93/148 [03:14<01:40,  1.82s/it]Running Inference:  64%|██████▎   | 94/148 [03:16<01:37,  1.81s/it]Running Inference:  64%|██████▍   | 95/148 [03:17<01:22,  1.56s/it]Running Inference:  65%|██████▍   | 96/148 [03:24<02:47,  3.22s/it]Running Inference:  66%|██████▌   | 97/148 [03:24<02:05,  2.46s/it]Running Inference:  66%|██████▌   | 98/148 [03:26<01:54,  2.28s/it]Running Inference:  67%|██████▋   | 99/148 [03:33<02:57,  3.62s/it]Running Inference:  68%|██████▊   | 100/148 [03:34<02:15,  2.82s/it]Running Inference:  68%|██████▊   | 101/148 [03:35<01:53,  2.42s/it]Running Inference:  69%|██████▉   | 102/148 [03:40<02:18,  3.02s/it]Running Inference:  70%|██████▉   | 103/148 [03:41<01:49,  2.43s/it]Running Inference:  70%|███████   | 104/148 [03:42<01:31,  2.08s/it]Running Inference:  71%|███████   | 105/148 [03:44<01:32,  2.15s/it]Running Inference:  72%|███████▏  | 106/148 [03:47<01:40,  2.38s/it]Running Inference:  72%|███████▏  | 107/148 [03:48<01:14,  1.81s/it]Running Inference:  73%|███████▎  | 108/148 [03:49<01:00,  1.52s/it]Running Inference:  74%|███████▎  | 109/148 [03:50<00:55,  1.42s/it]Running Inference:  74%|███████▍  | 110/148 [03:50<00:43,  1.14s/it]Running Inference:  75%|███████▌  | 111/148 [03:51<00:39,  1.06s/it]Running Inference:  76%|███████▌  | 112/148 [03:52<00:32,  1.12it/s]Running Inference:  76%|███████▋  | 113/148 [03:53<00:40,  1.14s/it]Running Inference:  77%|███████▋  | 114/148 [04:03<02:03,  3.62s/it]Running Inference:  78%|███████▊  | 115/148 [04:03<01:27,  2.66s/it]Running Inference:  78%|███████▊  | 116/148 [04:05<01:11,  2.24s/it]Running Inference:  79%|███████▉  | 117/148 [04:06<01:00,  1.94s/it]Running Inference:  80%|███████▉  | 118/148 [04:06<00:45,  1.51s/it]Running Inference:  80%|████████  | 119/148 [04:08<00:46,  1.61s/it]Running Inference:  81%|████████  | 120/148 [04:10<00:44,  1.58s/it]Running Inference:  82%|████████▏ | 121/148 [04:11<00:41,  1.54s/it]Running Inference:  82%|████████▏ | 122/148 [04:12<00:33,  1.30s/it]Running Inference:  83%|████████▎ | 123/148 [04:19<01:16,  3.06s/it]Running Inference:  84%|████████▍ | 124/148 [04:20<01:00,  2.54s/it]Running Inference:  84%|████████▍ | 125/148 [04:28<01:36,  4.18s/it]Running Inference:  85%|████████▌ | 126/148 [04:30<01:16,  3.49s/it]Running Inference:  86%|████████▌ | 127/148 [04:31<00:57,  2.72s/it]Running Inference:  86%|████████▋ | 128/148 [04:32<00:42,  2.14s/it]Running Inference:  87%|████████▋ | 129/148 [04:33<00:32,  1.72s/it]Running Inference:  88%|████████▊ | 130/148 [04:33<00:26,  1.45s/it]Running Inference:  89%|████████▊ | 131/148 [04:35<00:25,  1.48s/it]Running Inference:  89%|████████▉ | 132/148 [04:36<00:22,  1.44s/it]Running Inference:  90%|████████▉ | 133/148 [04:44<00:50,  3.33s/it]Running Inference:  91%|█████████ | 134/148 [04:48<00:48,  3.47s/it]Running Inference:  91%|█████████ | 135/148 [04:49<00:34,  2.69s/it]Running Inference:  92%|█████████▏| 136/148 [04:50<00:27,  2.27s/it]Running Inference:  93%|█████████▎| 137/148 [04:52<00:24,  2.21s/it]Running Inference:  93%|█████████▎| 138/148 [04:54<00:21,  2.13s/it]Running Inference:  94%|█████████▍| 139/148 [04:55<00:15,  1.72s/it]Running Inference:  95%|█████████▍| 140/148 [04:56<00:12,  1.53s/it]Running Inference:  95%|█████████▌| 141/148 [05:04<00:24,  3.44s/it]Running Inference:  96%|█████████▌| 142/148 [05:04<00:15,  2.57s/it]Running Inference:  97%|█████████▋| 143/148 [05:12<00:19,  3.95s/it]Running Inference:  97%|█████████▋| 144/148 [05:13<00:12,  3.25s/it]Running Inference:  98%|█████████▊| 145/148 [05:14<00:07,  2.43s/it]Running Inference:  99%|█████████▊| 146/148 [05:21<00:07,  3.85s/it]Running Inference:  99%|█████████▉| 147/148 [05:22<00:02,  2.96s/it]Running Inference: 100%|██████████| 148/148 [05:22<00:00,  2.27s/it]Running Inference: 100%|██████████| 148/148 [05:22<00:00,  2.18s/it]
2025-12-13 20:50:58,033 - INFO - Inference completed.
2025-12-13 20:50:58,043 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-13 20:50:58,043 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:50:58,053 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-13 20:50:58,053 - INFO - Metrics:
20.95
2025-12-13 20:50:58,054 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qasper, ratio=0.2) on GPU 4

----------------------------------------
Task: qasper | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:51:04,401 - INFO - Set deterministic seeds to 42
2025-12-13 20:51:04,401 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:51:04,401 - INFO - Starting evaluation run...
2025-12-13 20:51:04,401 - INFO - Output directory set to: longbenchresult
2025-12-13 20:51:04,401 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-13 20:51:04,401 - INFO - KV Press 'knorm' setup.
2025-12-13 20:51:04,401 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:51:04,401 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.35it/s]
Device set to use cuda:0
2025-12-13 20:51:19,041 - INFO - Model pipeline loaded.
2025-12-13 20:51:19,041 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:51:23,852 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:51:23,852 - INFO - Dataset processed with 200 entries.
2025-12-13 20:51:23,865 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:08<21:46,  8.89s/it]Running Inference:   1%|▏         | 2/148 [00:09<09:26,  3.88s/it]Running Inference:   2%|▏         | 3/148 [00:10<06:17,  2.61s/it]Running Inference:   3%|▎         | 4/148 [00:12<05:21,  2.23s/it]Running Inference:   3%|▎         | 5/148 [00:13<04:18,  1.81s/it]Running Inference:   4%|▍         | 6/148 [00:14<04:04,  1.72s/it]Running Inference:   5%|▍         | 7/148 [00:15<03:20,  1.42s/it]Running Inference:   5%|▌         | 8/148 [00:16<02:51,  1.23s/it]Running Inference:   6%|▌         | 9/148 [00:17<03:13,  1.39s/it]Running Inference:   7%|▋         | 10/148 [00:19<03:16,  1.43s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:20<03:11,  1.40s/it]Running Inference:   8%|▊         | 12/148 [00:22<03:11,  1.41s/it]Running Inference:   9%|▉         | 13/148 [00:29<06:58,  3.10s/it]Running Inference:   9%|▉         | 14/148 [00:30<05:55,  2.65s/it]Running Inference:  10%|█         | 15/148 [00:31<04:46,  2.16s/it]Running Inference:  11%|█         | 16/148 [00:34<05:10,  2.35s/it]Running Inference:  11%|█▏        | 17/148 [00:36<04:33,  2.09s/it]Running Inference:  12%|█▏        | 18/148 [00:44<08:23,  3.88s/it]Running Inference:  13%|█▎        | 19/148 [00:44<06:07,  2.85s/it]Running Inference:  14%|█▎        | 20/148 [00:45<04:46,  2.24s/it]Running Inference:  14%|█▍        | 21/148 [00:46<04:02,  1.91s/it]Running Inference:  15%|█▍        | 22/148 [00:48<04:04,  1.94s/it]Running Inference:  16%|█▌        | 23/148 [00:49<03:10,  1.53s/it]Running Inference:  16%|█▌        | 24/148 [00:50<02:49,  1.36s/it]Running Inference:  17%|█▋        | 25/148 [00:52<03:25,  1.67s/it]Running Inference:  18%|█▊        | 26/148 [01:00<07:25,  3.65s/it]Running Inference:  18%|█▊        | 27/148 [01:01<05:47,  2.87s/it]Running Inference:  19%|█▉        | 28/148 [01:02<04:34,  2.29s/it]Running Inference:  20%|█▉        | 29/148 [01:15<10:50,  5.47s/it]Running Inference:  20%|██        | 30/148 [01:22<11:29,  5.84s/it]Running Inference:  21%|██        | 31/148 [01:23<08:29,  4.36s/it]Running Inference:  22%|██▏       | 32/148 [01:24<06:52,  3.55s/it]Running Inference:  22%|██▏       | 33/148 [01:25<05:20,  2.78s/it]Running Inference:  23%|██▎       | 34/148 [01:32<07:40,  4.04s/it]Running Inference:  24%|██▎       | 35/148 [01:39<09:09,  4.87s/it]Running Inference:  24%|██▍       | 36/148 [01:47<10:49,  5.80s/it]Running Inference:  25%|██▌       | 37/148 [01:48<08:08,  4.40s/it]Running Inference:  26%|██▌       | 38/148 [01:49<06:10,  3.37s/it]Running Inference:  26%|██▋       | 39/148 [01:50<04:39,  2.57s/it]Running Inference:  27%|██▋       | 40/148 [01:51<03:38,  2.02s/it]Running Inference:  28%|██▊       | 41/148 [01:53<03:42,  2.08s/it]Running Inference:  28%|██▊       | 42/148 [01:54<02:54,  1.65s/it]Running Inference:  29%|██▉       | 43/148 [01:55<02:57,  1.69s/it]Running Inference:  30%|██▉       | 44/148 [01:57<02:55,  1.68s/it]Running Inference:  30%|███       | 45/148 [01:58<02:39,  1.55s/it]Running Inference:  31%|███       | 46/148 [02:05<05:12,  3.06s/it]Running Inference:  32%|███▏      | 47/148 [02:06<04:03,  2.41s/it]Running Inference:  32%|███▏      | 48/148 [02:13<06:12,  3.72s/it]Running Inference:  33%|███▎      | 49/148 [02:16<05:56,  3.60s/it]Running Inference:  34%|███▍      | 50/148 [02:18<04:56,  3.02s/it]Running Inference:  34%|███▍      | 51/148 [02:19<04:15,  2.64s/it]Running Inference:  35%|███▌      | 52/148 [02:22<04:11,  2.62s/it]Running Inference:  36%|███▌      | 53/148 [02:23<03:22,  2.13s/it]Running Inference:  36%|███▋      | 54/148 [02:24<02:41,  1.72s/it]Running Inference:  37%|███▋      | 55/148 [02:24<02:11,  1.41s/it]Running Inference:  38%|███▊      | 56/148 [02:26<02:05,  1.36s/it]Running Inference:  39%|███▊      | 57/148 [02:26<01:52,  1.24s/it]Running Inference:  39%|███▉      | 58/148 [02:28<01:54,  1.28s/it]Running Inference:  40%|███▉      | 59/148 [02:29<01:42,  1.15s/it]Running Inference:  41%|████      | 60/148 [02:30<01:37,  1.11s/it]Running Inference:  41%|████      | 61/148 [02:31<01:34,  1.09s/it]Running Inference:  42%|████▏     | 62/148 [02:32<01:26,  1.00s/it]Running Inference:  43%|████▎     | 63/148 [02:33<01:38,  1.15s/it]Running Inference:  43%|████▎     | 64/148 [02:35<01:58,  1.41s/it]Running Inference:  44%|████▍     | 65/148 [02:37<02:13,  1.60s/it]Running Inference:  45%|████▍     | 66/148 [02:38<01:54,  1.39s/it]Running Inference:  45%|████▌     | 67/148 [02:39<01:42,  1.27s/it]Running Inference:  46%|████▌     | 68/148 [02:40<01:29,  1.12s/it]Running Inference:  47%|████▋     | 69/148 [02:41<01:31,  1.15s/it]Running Inference:  47%|████▋     | 70/148 [02:43<01:38,  1.26s/it]Running Inference:  48%|████▊     | 71/148 [02:50<03:54,  3.05s/it]Running Inference:  49%|████▊     | 72/148 [02:52<03:21,  2.66s/it]Running Inference:  49%|████▉     | 73/148 [02:53<02:48,  2.25s/it]Running Inference:  50%|█████     | 74/148 [02:54<02:32,  2.07s/it]Running Inference:  51%|█████     | 75/148 [02:55<02:04,  1.70s/it]Running Inference:  51%|█████▏    | 76/148 [02:57<02:10,  1.81s/it]Running Inference:  52%|█████▏    | 77/148 [02:58<01:51,  1.57s/it]Running Inference:  53%|█████▎    | 78/148 [02:59<01:36,  1.38s/it]Running Inference:  53%|█████▎    | 79/148 [03:00<01:21,  1.18s/it]Running Inference:  54%|█████▍    | 80/148 [03:07<03:15,  2.88s/it]Running Inference:  55%|█████▍    | 81/148 [03:08<02:33,  2.29s/it]Running Inference:  55%|█████▌    | 82/148 [03:09<02:06,  1.92s/it]Running Inference:  56%|█████▌    | 83/148 [03:10<01:45,  1.63s/it]Running Inference:  57%|█████▋    | 84/148 [03:10<01:21,  1.28s/it]Running Inference:  57%|█████▋    | 85/148 [03:11<01:13,  1.16s/it]Running Inference:  58%|█████▊    | 86/148 [03:12<01:13,  1.18s/it]Running Inference:  59%|█████▉    | 87/148 [03:14<01:26,  1.42s/it]Running Inference:  59%|█████▉    | 88/148 [03:15<01:16,  1.27s/it]Running Inference:  60%|██████    | 89/148 [03:17<01:21,  1.39s/it]Running Inference:  61%|██████    | 90/148 [03:24<02:54,  3.01s/it]Running Inference:  61%|██████▏   | 91/148 [03:30<03:53,  4.10s/it]Running Inference:  62%|██████▏   | 92/148 [03:31<02:52,  3.09s/it]Running Inference:  63%|██████▎   | 93/148 [03:32<02:18,  2.52s/it]Running Inference:  64%|██████▎   | 94/148 [03:34<01:55,  2.14s/it]Running Inference:  64%|██████▍   | 95/148 [03:35<01:34,  1.79s/it]Running Inference:  65%|██████▍   | 96/148 [03:41<02:52,  3.31s/it]Running Inference:  66%|██████▌   | 97/148 [03:42<02:08,  2.52s/it]Running Inference:  66%|██████▌   | 98/148 [03:44<01:51,  2.23s/it]Running Inference:  67%|██████▋   | 99/148 [03:50<02:53,  3.54s/it]Running Inference:  68%|██████▊   | 100/148 [03:51<02:10,  2.72s/it]Running Inference:  68%|██████▊   | 101/148 [03:52<01:46,  2.27s/it]Running Inference:  69%|██████▉   | 102/148 [03:55<01:45,  2.29s/it]Running Inference:  70%|██████▉   | 103/148 [04:01<02:45,  3.67s/it]Running Inference:  70%|███████   | 104/148 [04:03<02:07,  2.90s/it]Running Inference:  71%|███████   | 105/148 [04:10<03:03,  4.28s/it]Running Inference:  72%|███████▏  | 106/148 [04:13<02:42,  3.87s/it]Running Inference:  72%|███████▏  | 107/148 [04:13<01:56,  2.85s/it]Running Inference:  73%|███████▎  | 108/148 [04:14<01:29,  2.24s/it]Running Inference:  74%|███████▎  | 109/148 [04:15<01:12,  1.85s/it]Running Inference:  74%|███████▍  | 110/148 [04:16<00:58,  1.54s/it]Running Inference:  75%|███████▌  | 111/148 [04:17<00:49,  1.33s/it]Running Inference:  76%|███████▌  | 112/148 [04:17<00:39,  1.09s/it]Running Inference:  76%|███████▋  | 113/148 [04:24<01:41,  2.90s/it]Running Inference:  77%|███████▋  | 114/148 [04:34<02:42,  4.78s/it]Running Inference:  78%|███████▊  | 115/148 [04:34<01:53,  3.45s/it]Running Inference:  78%|███████▊  | 116/148 [04:35<01:28,  2.78s/it]Running Inference:  79%|███████▉  | 117/148 [04:36<01:10,  2.28s/it]Running Inference:  80%|███████▉  | 118/148 [04:37<00:53,  1.78s/it]Running Inference:  80%|████████  | 119/148 [04:39<00:52,  1.80s/it]Running Inference:  81%|████████  | 120/148 [04:46<01:37,  3.47s/it]Running Inference:  82%|████████▏ | 121/148 [04:48<01:17,  2.87s/it]Running Inference:  82%|████████▏ | 122/148 [04:49<00:59,  2.29s/it]Running Inference:  83%|████████▎ | 123/148 [04:56<01:32,  3.69s/it]Running Inference:  84%|████████▍ | 124/148 [04:57<01:11,  2.97s/it]Running Inference:  84%|████████▍ | 125/148 [05:05<01:41,  4.40s/it]Running Inference:  85%|████████▌ | 126/148 [05:12<01:55,  5.23s/it]Running Inference:  86%|████████▌ | 127/148 [05:13<01:22,  3.93s/it]Running Inference:  86%|████████▋ | 128/148 [05:13<00:59,  2.99s/it]Running Inference:  87%|████████▋ | 129/148 [05:14<00:42,  2.25s/it]Running Inference:  88%|████████▊ | 130/148 [05:15<00:32,  1.82s/it]Running Inference:  89%|████████▊ | 131/148 [05:22<00:58,  3.41s/it]Running Inference:  89%|████████▉ | 132/148 [05:23<00:44,  2.78s/it]Running Inference:  90%|████████▉ | 133/148 [05:30<01:01,  4.13s/it]Running Inference:  91%|█████████ | 134/148 [05:44<01:36,  6.90s/it]Running Inference:  91%|█████████ | 135/148 [05:45<01:07,  5.18s/it]Running Inference:  92%|█████████▏| 136/148 [05:46<00:48,  4.01s/it]Running Inference:  93%|█████████▎| 137/148 [05:48<00:38,  3.46s/it]Running Inference:  93%|█████████▎| 138/148 [05:50<00:30,  3.02s/it]Running Inference:  94%|█████████▍| 139/148 [05:51<00:21,  2.34s/it]Running Inference:  95%|█████████▍| 140/148 [05:52<00:15,  1.88s/it]Running Inference:  95%|█████████▌| 141/148 [06:00<00:25,  3.60s/it]Running Inference:  96%|█████████▌| 142/148 [06:00<00:16,  2.73s/it]Running Inference:  97%|█████████▋| 143/148 [06:02<00:11,  2.32s/it]Running Inference:  97%|█████████▋| 144/148 [06:03<00:08,  2.07s/it]Running Inference:  98%|█████████▊| 145/148 [06:04<00:04,  1.60s/it]Running Inference:  99%|█████████▊| 146/148 [06:05<00:03,  1.56s/it]Running Inference:  99%|█████████▉| 147/148 [06:06<00:01,  1.36s/it]Running Inference: 100%|██████████| 148/148 [06:07<00:00,  1.11s/it]Running Inference: 100%|██████████| 148/148 [06:07<00:00,  2.48s/it]
2025-12-13 20:57:30,963 - INFO - Inference completed.
2025-12-13 20:57:30,972 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-13 20:57:30,972 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:57:30,984 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-13 20:57:30,984 - INFO - Metrics:
18.45
2025-12-13 20:57:30,985 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qasper, ratio=0.3) on GPU 4

----------------------------------------
Task: qasper | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:57:37,306 - INFO - Set deterministic seeds to 42
2025-12-13 20:57:37,306 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:57:37,306 - INFO - Starting evaluation run...
2025-12-13 20:57:37,306 - INFO - Output directory set to: longbenchresult
2025-12-13 20:57:37,307 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-13 20:57:37,307 - INFO - KV Press 'knorm' setup.
2025-12-13 20:57:37,307 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:57:37,307 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.80it/s]
Device set to use cuda:0
2025-12-13 20:57:58,069 - INFO - Model pipeline loaded.
2025-12-13 20:57:58,069 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:58:03,162 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:58:03,163 - INFO - Dataset processed with 200 entries.
2025-12-13 20:58:03,182 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:09<23:33,  9.62s/it]Running Inference:   1%|▏         | 2/148 [00:10<10:36,  4.36s/it]Running Inference:   2%|▏         | 3/148 [00:11<06:41,  2.77s/it]Running Inference:   3%|▎         | 4/148 [00:12<04:57,  2.07s/it]Running Inference:   3%|▎         | 5/148 [00:19<09:35,  4.03s/it]Running Inference:   4%|▍         | 6/148 [00:21<07:45,  3.28s/it]Running Inference:   5%|▍         | 7/148 [00:22<05:49,  2.48s/it]Running Inference:   5%|▌         | 8/148 [00:23<04:33,  1.95s/it]Running Inference:   6%|▌         | 9/148 [00:25<04:27,  1.92s/it]Running Inference:   7%|▋         | 10/148 [00:31<07:59,  3.48s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:33<06:38,  2.91s/it]Running Inference:   8%|▊         | 12/148 [00:35<05:34,  2.46s/it]Running Inference:   9%|▉         | 13/148 [00:36<04:34,  2.03s/it]Running Inference:   9%|▉         | 14/148 [00:38<04:35,  2.06s/it]Running Inference:  10%|█         | 15/148 [00:39<03:46,  1.70s/it]Running Inference:  11%|█         | 16/148 [00:41<04:14,  1.93s/it]Running Inference:  11%|█▏        | 17/148 [00:43<03:58,  1.82s/it]Running Inference:  12%|█▏        | 18/148 [00:45<04:18,  1.99s/it]Running Inference:  13%|█▎        | 19/148 [00:45<03:16,  1.53s/it]Running Inference:  14%|█▎        | 20/148 [00:46<02:38,  1.24s/it]Running Inference:  14%|█▍        | 21/148 [00:47<02:44,  1.30s/it]Running Inference:  15%|█▍        | 22/148 [00:50<03:27,  1.64s/it]Running Inference:  16%|█▌        | 23/148 [00:51<02:49,  1.36s/it]Running Inference:  16%|█▌        | 24/148 [00:51<02:25,  1.17s/it]Running Inference:  17%|█▋        | 25/148 [00:59<06:26,  3.14s/it]Running Inference:  18%|█▊        | 26/148 [01:02<06:32,  3.21s/it]Running Inference:  18%|█▊        | 27/148 [01:03<05:05,  2.52s/it]Running Inference:  19%|█▉        | 28/148 [01:04<04:05,  2.04s/it]Running Inference:  20%|█▉        | 29/148 [01:11<07:00,  3.54s/it]Running Inference:  20%|██        | 30/148 [01:12<05:32,  2.82s/it]Running Inference:  21%|██        | 31/148 [01:13<04:22,  2.24s/it]Running Inference:  22%|██▏       | 32/148 [01:14<03:40,  1.91s/it]Running Inference:  22%|██▏       | 33/148 [01:16<03:19,  1.73s/it]Running Inference:  23%|██▎       | 34/148 [01:17<02:57,  1.56s/it]Running Inference:  24%|██▎       | 35/148 [01:18<02:50,  1.51s/it]Running Inference:  24%|██▍       | 36/148 [01:32<09:38,  5.16s/it]Running Inference:  25%|██▌       | 37/148 [01:33<07:17,  3.95s/it]Running Inference:  26%|██▌       | 38/148 [01:34<05:36,  3.06s/it]Running Inference:  26%|██▋       | 39/148 [01:35<04:20,  2.39s/it]Running Inference:  27%|██▋       | 40/148 [01:36<03:24,  1.90s/it]Running Inference:  28%|██▊       | 41/148 [01:37<02:56,  1.65s/it]Running Inference:  28%|██▊       | 42/148 [01:38<02:31,  1.42s/it]Running Inference:  29%|██▉       | 43/148 [01:39<02:41,  1.54s/it]Running Inference:  30%|██▉       | 44/148 [01:41<02:41,  1.55s/it]Running Inference:  30%|███       | 45/148 [01:48<05:26,  3.17s/it]Running Inference:  31%|███       | 46/148 [01:49<04:15,  2.51s/it]Running Inference:  32%|███▏      | 47/148 [01:50<03:26,  2.05s/it]Running Inference:  32%|███▏      | 48/148 [01:51<02:54,  1.75s/it]Running Inference:  33%|███▎      | 49/148 [01:53<03:04,  1.87s/it]Running Inference:  34%|███▍      | 50/148 [01:54<02:46,  1.70s/it]Running Inference:  34%|███▍      | 51/148 [01:56<02:49,  1.75s/it]Running Inference:  35%|███▌      | 52/148 [01:57<02:29,  1.55s/it]Running Inference:  36%|███▌      | 53/148 [01:58<02:08,  1.36s/it]Running Inference:  36%|███▋      | 54/148 [01:59<01:56,  1.24s/it]Running Inference:  37%|███▋      | 55/148 [02:06<04:36,  2.97s/it]Running Inference:  38%|███▊      | 56/148 [02:07<03:42,  2.42s/it]Running Inference:  39%|███▊      | 57/148 [02:09<03:08,  2.07s/it]Running Inference:  39%|███▉      | 58/148 [02:10<02:39,  1.77s/it]Running Inference:  40%|███▉      | 59/148 [02:11<02:13,  1.50s/it]Running Inference:  41%|████      | 60/148 [02:12<01:58,  1.35s/it]Running Inference:  41%|████      | 61/148 [02:13<01:49,  1.26s/it]Running Inference:  42%|████▏     | 62/148 [02:13<01:37,  1.13s/it]Running Inference:  43%|████▎     | 63/148 [02:15<01:34,  1.11s/it]Running Inference:  43%|████▎     | 64/148 [02:23<04:27,  3.18s/it]Running Inference:  44%|████▍     | 65/148 [02:25<04:02,  2.92s/it]Running Inference:  45%|████▍     | 66/148 [02:26<03:15,  2.38s/it]Running Inference:  45%|████▌     | 67/148 [02:27<02:38,  1.96s/it]Running Inference:  46%|████▌     | 68/148 [02:28<02:25,  1.82s/it]Running Inference:  47%|████▋     | 69/148 [02:30<02:09,  1.64s/it]Running Inference:  47%|████▋     | 70/148 [02:31<02:01,  1.55s/it]Running Inference:  48%|████▊     | 71/148 [02:38<04:13,  3.29s/it]Running Inference:  49%|████▊     | 72/148 [02:45<05:36,  4.43s/it]Running Inference:  49%|████▉     | 73/148 [02:47<04:21,  3.49s/it]Running Inference:  50%|█████     | 74/148 [02:48<03:35,  2.91s/it]Running Inference:  51%|█████     | 75/148 [02:49<02:43,  2.24s/it]Running Inference:  51%|█████▏    | 76/148 [02:51<02:35,  2.16s/it]Running Inference:  52%|█████▏    | 77/148 [02:58<04:22,  3.70s/it]Running Inference:  53%|█████▎    | 78/148 [02:59<03:20,  2.86s/it]Running Inference:  53%|█████▎    | 79/148 [03:00<02:31,  2.20s/it]Running Inference:  54%|█████▍    | 80/148 [03:01<02:09,  1.91s/it]Running Inference:  55%|█████▍    | 81/148 [03:02<01:54,  1.70s/it]Running Inference:  55%|█████▌    | 82/148 [03:03<01:42,  1.55s/it]Running Inference:  56%|█████▌    | 83/148 [03:04<01:28,  1.36s/it]Running Inference:  57%|█████▋    | 84/148 [03:05<01:20,  1.26s/it]Running Inference:  57%|█████▋    | 85/148 [03:06<01:12,  1.15s/it]Running Inference:  58%|█████▊    | 86/148 [03:13<02:56,  2.85s/it]Running Inference:  59%|█████▉    | 87/148 [03:20<04:13,  4.15s/it]Running Inference:  59%|█████▉    | 88/148 [03:21<03:11,  3.19s/it]Running Inference:  60%|██████    | 89/148 [03:22<02:29,  2.53s/it]Running Inference:  61%|██████    | 90/148 [03:29<03:43,  3.85s/it]Running Inference:  61%|██████▏   | 91/148 [03:36<04:28,  4.71s/it]Running Inference:  62%|██████▏   | 92/148 [03:37<03:20,  3.57s/it]Running Inference:  63%|██████▎   | 93/148 [03:44<04:16,  4.66s/it]Running Inference:  64%|██████▎   | 94/148 [03:45<03:16,  3.64s/it]Running Inference:  64%|██████▍   | 95/148 [03:46<02:30,  2.84s/it]Running Inference:  65%|██████▍   | 96/148 [03:47<01:57,  2.26s/it]Running Inference:  66%|██████▌   | 97/148 [03:48<01:39,  1.96s/it]Running Inference:  66%|██████▌   | 98/148 [03:50<01:36,  1.93s/it]Running Inference:  67%|██████▋   | 99/148 [03:57<02:45,  3.39s/it]Running Inference:  68%|██████▊   | 100/148 [03:58<02:05,  2.61s/it]Running Inference:  68%|██████▊   | 101/148 [03:59<01:46,  2.27s/it]Running Inference:  69%|██████▉   | 102/148 [04:02<01:54,  2.48s/it]Running Inference:  70%|██████▉   | 103/148 [04:03<01:32,  2.05s/it]Running Inference:  70%|███████   | 104/148 [04:05<01:19,  1.81s/it]Running Inference:  71%|███████   | 105/148 [04:07<01:19,  1.85s/it]Running Inference:  72%|███████▏  | 106/148 [04:10<01:32,  2.21s/it]Running Inference:  72%|███████▏  | 107/148 [04:10<01:09,  1.68s/it]Running Inference:  73%|███████▎  | 108/148 [04:17<02:08,  3.21s/it]Running Inference:  74%|███████▎  | 109/148 [04:24<02:49,  4.35s/it]Running Inference:  74%|███████▍  | 110/148 [04:25<02:06,  3.32s/it]Running Inference:  75%|███████▌  | 111/148 [04:26<01:37,  2.63s/it]Running Inference:  76%|███████▌  | 112/148 [04:26<01:11,  2.00s/it]Running Inference:  76%|███████▋  | 113/148 [04:28<01:02,  1.79s/it]Running Inference:  77%|███████▋  | 114/148 [04:32<01:28,  2.59s/it]Running Inference:  78%|███████▊  | 115/148 [04:32<01:03,  1.92s/it]Running Inference:  78%|███████▊  | 116/148 [04:34<00:54,  1.71s/it]Running Inference:  79%|███████▉  | 117/148 [04:35<00:46,  1.51s/it]Running Inference:  80%|███████▉  | 118/148 [04:35<00:37,  1.24s/it]Running Inference:  80%|████████  | 119/148 [04:36<00:35,  1.21s/it]Running Inference:  81%|████████  | 120/148 [04:44<01:25,  3.05s/it]Running Inference:  82%|████████▏ | 121/148 [04:45<01:09,  2.59s/it]Running Inference:  82%|████████▏ | 122/148 [04:46<00:54,  2.08s/it]Running Inference:  83%|████████▎ | 123/148 [04:47<00:44,  1.79s/it]Running Inference:  84%|████████▍ | 124/148 [04:49<00:40,  1.69s/it]Running Inference:  84%|████████▍ | 125/148 [04:57<01:23,  3.65s/it]Running Inference:  85%|████████▌ | 126/148 [04:58<01:05,  2.95s/it]Running Inference:  86%|████████▌ | 127/148 [04:59<00:49,  2.37s/it]Running Inference:  86%|████████▋ | 128/148 [05:00<00:39,  1.98s/it]Running Inference:  87%|████████▋ | 129/148 [05:01<00:29,  1.55s/it]Running Inference:  88%|████████▊ | 130/148 [05:02<00:22,  1.27s/it]Running Inference:  89%|████████▊ | 131/148 [05:09<00:51,  3.01s/it]Running Inference:  89%|████████▉ | 132/148 [05:10<00:40,  2.50s/it]Running Inference:  90%|████████▉ | 133/148 [05:17<00:59,  3.98s/it]Running Inference:  91%|█████████ | 134/148 [05:26<01:15,  5.41s/it]Running Inference:  91%|█████████ | 135/148 [05:27<00:54,  4.20s/it]Running Inference:  92%|█████████▏| 136/148 [05:29<00:39,  3.26s/it]Running Inference:  93%|█████████▎| 137/148 [05:30<00:29,  2.72s/it]Running Inference:  93%|█████████▎| 138/148 [05:33<00:27,  2.70s/it]Running Inference:  94%|█████████▍| 139/148 [05:34<00:19,  2.20s/it]Running Inference:  95%|█████████▍| 140/148 [05:34<00:14,  1.78s/it]Running Inference:  95%|█████████▌| 141/148 [05:37<00:13,  1.94s/it]Running Inference:  96%|█████████▌| 142/148 [05:37<00:09,  1.56s/it]Running Inference:  97%|█████████▋| 143/148 [05:39<00:07,  1.44s/it]Running Inference:  97%|█████████▋| 144/148 [05:39<00:04,  1.24s/it]Running Inference:  98%|█████████▊| 145/148 [05:46<00:08,  2.90s/it]Running Inference:  99%|█████████▊| 146/148 [05:48<00:04,  2.48s/it]Running Inference:  99%|█████████▉| 147/148 [05:49<00:02,  2.06s/it]Running Inference: 100%|██████████| 148/148 [05:50<00:00,  1.70s/it]Running Inference: 100%|██████████| 148/148 [05:50<00:00,  2.37s/it]
2025-12-13 21:03:53,308 - INFO - Inference completed.
2025-12-13 21:03:53,318 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-13 21:03:53,318 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:03:53,328 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-13 21:03:53,328 - INFO - Metrics:
9.37
2025-12-13 21:03:53,330 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qasper, ratio=0.5) on GPU 4


========================================
LongBench Task: triviaqa
========================================
----------------------------------------
Task: triviaqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:03:59,917 - INFO - Set deterministic seeds to 42
2025-12-13 21:03:59,918 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:03:59,918 - INFO - Starting evaluation run...
2025-12-13 21:03:59,918 - INFO - Output directory set to: longbenchresult
2025-12-13 21:03:59,918 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 21:03:59,918 - INFO - KV Press 'knorm' setup.
2025-12-13 21:03:59,918 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:03:59,918 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.29it/s]
Device set to use cuda:0
2025-12-13 21:04:17,302 - INFO - Model pipeline loaded.
2025-12-13 21:04:17,302 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 21:04:24,051 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:04:24,051 - INFO - Dataset processed with 200 entries.
2025-12-13 21:04:24,081 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<10:04,  3.04s/it]Running Inference:   1%|          | 2/200 [00:06<11:23,  3.45s/it]Running Inference:   2%|▏         | 3/200 [00:07<07:40,  2.34s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:49,  2.40s/it]Running Inference:   2%|▎         | 5/200 [00:10<05:34,  1.72s/it]Running Inference:   3%|▎         | 6/200 [00:11<04:33,  1.41s/it]Running Inference:   4%|▎         | 7/200 [00:11<03:23,  1.06s/it]Running Inference:   4%|▍         | 8/200 [00:12<02:47,  1.15it/s]Running Inference:   4%|▍         | 9/200 [00:13<03:13,  1.01s/it]Running Inference:   5%|▌         | 10/200 [00:15<03:42,  1.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:18<05:26,  1.73s/it]Running Inference:   6%|▌         | 12/200 [00:23<08:46,  2.80s/it]Running Inference:   6%|▋         | 13/200 [00:24<07:11,  2.31s/it]Running Inference:   7%|▋         | 14/200 [00:27<07:59,  2.58s/it]Running Inference:   8%|▊         | 15/200 [00:30<07:46,  2.52s/it]Running Inference:   8%|▊         | 16/200 [00:33<08:10,  2.67s/it]Running Inference:   8%|▊         | 17/200 [00:33<06:05,  2.00s/it]Running Inference:   9%|▉         | 18/200 [00:34<04:53,  1.61s/it]Running Inference:  10%|▉         | 19/200 [00:39<08:02,  2.67s/it]Running Inference:  10%|█         | 20/200 [00:41<06:57,  2.32s/it]Running Inference:  10%|█         | 21/200 [00:41<05:11,  1.74s/it]Running Inference:  11%|█         | 22/200 [00:44<06:34,  2.22s/it]Running Inference:  12%|█▏        | 23/200 [00:46<05:41,  1.93s/it]Running Inference:  12%|█▏        | 24/200 [00:48<05:45,  1.97s/it]Running Inference:  12%|█▎        | 25/200 [00:49<05:28,  1.88s/it]Running Inference:  13%|█▎        | 26/200 [00:51<04:57,  1.71s/it]Running Inference:  14%|█▎        | 27/200 [00:51<03:56,  1.36s/it]Running Inference:  14%|█▍        | 28/200 [00:52<03:20,  1.16s/it]Running Inference:  14%|█▍        | 29/200 [00:56<05:48,  2.04s/it]Running Inference:  15%|█▌        | 30/200 [00:57<04:32,  1.60s/it]Running Inference:  16%|█▌        | 31/200 [00:58<04:24,  1.57s/it]Running Inference:  16%|█▌        | 32/200 [01:00<04:42,  1.68s/it]Running Inference:  16%|█▋        | 33/200 [01:01<04:34,  1.64s/it]Running Inference:  17%|█▋        | 34/200 [01:03<04:33,  1.65s/it]Running Inference:  18%|█▊        | 35/200 [01:04<04:12,  1.53s/it]Running Inference:  18%|█▊        | 36/200 [01:09<06:28,  2.37s/it]Running Inference:  18%|█▊        | 37/200 [01:10<05:28,  2.02s/it]Running Inference:  19%|█▉        | 38/200 [01:15<07:47,  2.89s/it]Running Inference:  20%|█▉        | 39/200 [01:15<05:51,  2.18s/it]Running Inference:  20%|██        | 40/200 [01:19<06:41,  2.51s/it]Running Inference:  20%|██        | 41/200 [01:20<05:49,  2.20s/it]Running Inference:  21%|██        | 42/200 [01:23<06:13,  2.36s/it]Running Inference:  22%|██▏       | 43/200 [01:23<04:39,  1.78s/it]Running Inference:  22%|██▏       | 44/200 [01:25<04:37,  1.78s/it]Running Inference:  22%|██▎       | 45/200 [01:26<03:33,  1.38s/it]Running Inference:  23%|██▎       | 46/200 [01:27<03:37,  1.41s/it]Running Inference:  24%|██▎       | 47/200 [01:28<03:31,  1.39s/it]Running Inference:  24%|██▍       | 48/200 [01:34<06:33,  2.59s/it]Running Inference:  24%|██▍       | 49/200 [01:36<06:10,  2.45s/it]Running Inference:  25%|██▌       | 50/200 [01:39<06:23,  2.56s/it]Running Inference:  26%|██▌       | 51/200 [01:39<04:59,  2.01s/it]Running Inference:  26%|██▌       | 52/200 [01:42<05:07,  2.08s/it]Running Inference:  26%|██▋       | 53/200 [01:42<03:53,  1.59s/it]Running Inference:  27%|██▋       | 54/200 [01:45<04:38,  1.91s/it]Running Inference:  28%|██▊       | 55/200 [01:46<04:03,  1.68s/it]Running Inference:  28%|██▊       | 56/200 [01:47<03:58,  1.66s/it]Running Inference:  28%|██▊       | 57/200 [01:51<05:01,  2.11s/it]Running Inference:  29%|██▉       | 58/200 [01:52<04:42,  1.99s/it]Running Inference:  30%|██▉       | 59/200 [01:55<04:54,  2.09s/it]Running Inference:  30%|███       | 60/200 [01:56<04:31,  1.94s/it]Running Inference:  30%|███       | 61/200 [02:00<05:39,  2.44s/it]Running Inference:  31%|███       | 62/200 [02:02<05:05,  2.21s/it]Running Inference:  32%|███▏      | 63/200 [02:04<05:18,  2.32s/it]Running Inference:  32%|███▏      | 64/200 [02:05<04:18,  1.90s/it]Running Inference:  32%|███▎      | 65/200 [02:07<04:35,  2.04s/it]Running Inference:  33%|███▎      | 66/200 [02:10<04:43,  2.12s/it]Running Inference:  34%|███▎      | 67/200 [02:12<04:33,  2.06s/it]Running Inference:  34%|███▍      | 68/200 [02:13<03:49,  1.74s/it]Running Inference:  34%|███▍      | 69/200 [02:15<03:59,  1.83s/it]Running Inference:  35%|███▌      | 70/200 [02:19<05:36,  2.59s/it]Running Inference:  36%|███▌      | 71/200 [02:22<05:46,  2.68s/it]Running Inference:  36%|███▌      | 72/200 [02:26<06:26,  3.02s/it]Running Inference:  36%|███▋      | 73/200 [02:27<05:26,  2.57s/it]Running Inference:  37%|███▋      | 74/200 [02:30<05:30,  2.62s/it]Running Inference:  38%|███▊      | 75/200 [02:30<04:03,  1.95s/it]Running Inference:  38%|███▊      | 76/200 [02:34<05:17,  2.56s/it]Running Inference:  38%|███▊      | 77/200 [02:37<05:16,  2.57s/it]Running Inference:  39%|███▉      | 78/200 [02:37<03:51,  1.90s/it]Running Inference:  40%|███▉      | 79/200 [02:39<03:55,  1.95s/it]Running Inference:  40%|████      | 80/200 [02:44<05:20,  2.67s/it]Running Inference:  40%|████      | 81/200 [02:47<05:24,  2.73s/it]Running Inference:  41%|████      | 82/200 [02:47<04:02,  2.05s/it]Running Inference:  42%|████▏     | 83/200 [02:51<04:54,  2.51s/it]Running Inference:  42%|████▏     | 84/200 [02:54<05:10,  2.67s/it]Running Inference:  42%|████▎     | 85/200 [02:55<04:27,  2.33s/it]Running Inference:  43%|████▎     | 86/200 [03:01<06:17,  3.31s/it]Running Inference:  44%|████▎     | 87/200 [03:03<05:46,  3.07s/it]Running Inference:  44%|████▍     | 88/200 [03:05<04:59,  2.68s/it]Running Inference:  44%|████▍     | 89/200 [03:06<04:06,  2.22s/it]Running Inference:  45%|████▌     | 90/200 [03:08<03:49,  2.08s/it]Running Inference:  46%|████▌     | 91/200 [03:11<04:13,  2.33s/it]Running Inference:  46%|████▌     | 92/200 [03:12<03:19,  1.85s/it]Running Inference:  46%|████▋     | 93/200 [03:15<03:53,  2.18s/it]Running Inference:  47%|████▋     | 94/200 [03:16<03:32,  2.00s/it]Running Inference:  48%|████▊     | 95/200 [03:21<04:53,  2.80s/it]Running Inference:  48%|████▊     | 96/200 [03:25<05:46,  3.33s/it]Running Inference:  48%|████▊     | 97/200 [03:27<04:43,  2.76s/it]Running Inference:  49%|████▉     | 98/200 [03:29<04:30,  2.66s/it]Running Inference:  50%|████▉     | 99/200 [03:33<04:52,  2.90s/it]Running Inference:  50%|█████     | 100/200 [03:36<05:02,  3.02s/it]Running Inference:  50%|█████     | 101/200 [03:38<04:36,  2.80s/it]Running Inference:  51%|█████     | 102/200 [03:41<04:20,  2.66s/it]Running Inference:  52%|█████▏    | 103/200 [03:43<04:01,  2.48s/it]Running Inference:  52%|█████▏    | 104/200 [03:44<03:15,  2.04s/it]Running Inference:  52%|█████▎    | 105/200 [03:46<03:23,  2.14s/it]Running Inference:  53%|█████▎    | 106/200 [03:49<03:51,  2.46s/it]Running Inference:  54%|█████▎    | 107/200 [03:52<03:44,  2.42s/it]Running Inference:  54%|█████▍    | 108/200 [03:55<04:03,  2.65s/it]Running Inference:  55%|█████▍    | 109/200 [03:56<03:13,  2.13s/it]Running Inference:  55%|█████▌    | 110/200 [03:56<02:29,  1.66s/it]Running Inference:  56%|█████▌    | 111/200 [03:58<02:17,  1.54s/it]Running Inference:  56%|█████▌    | 112/200 [04:01<03:06,  2.12s/it]Running Inference:  56%|█████▋    | 113/200 [04:02<02:37,  1.81s/it]Running Inference:  57%|█████▋    | 114/200 [04:07<03:54,  2.72s/it]Running Inference:  57%|█████▊    | 115/200 [04:09<03:27,  2.45s/it]Running Inference:  58%|█████▊    | 116/200 [04:09<02:34,  1.83s/it]Running Inference:  58%|█████▊    | 117/200 [04:13<03:17,  2.38s/it]Running Inference:  59%|█████▉    | 118/200 [04:14<02:37,  1.92s/it]Running Inference:  60%|█████▉    | 119/200 [04:15<02:24,  1.78s/it]Running Inference:  60%|██████    | 120/200 [04:17<02:28,  1.86s/it]Running Inference:  60%|██████    | 121/200 [04:17<01:50,  1.40s/it]Running Inference:  61%|██████    | 122/200 [04:19<01:49,  1.41s/it]Running Inference:  62%|██████▏   | 123/200 [04:19<01:25,  1.11s/it]Running Inference:  62%|██████▏   | 124/200 [04:20<01:15,  1.00it/s]Running Inference:  62%|██████▎   | 125/200 [04:22<01:43,  1.38s/it]Running Inference:  63%|██████▎   | 126/200 [04:23<01:28,  1.19s/it]Running Inference:  64%|██████▎   | 127/200 [04:25<01:48,  1.49s/it]Running Inference:  64%|██████▍   | 128/200 [04:26<01:30,  1.26s/it]Running Inference:  64%|██████▍   | 129/200 [04:27<01:25,  1.20s/it]Running Inference:  65%|██████▌   | 130/200 [04:28<01:14,  1.06s/it]Running Inference:  66%|██████▌   | 131/200 [04:31<02:02,  1.77s/it]Running Inference:  66%|██████▌   | 132/200 [04:35<02:40,  2.37s/it]Running Inference:  66%|██████▋   | 133/200 [04:36<02:19,  2.08s/it]Running Inference:  67%|██████▋   | 134/200 [04:38<02:14,  2.03s/it]Running Inference:  68%|██████▊   | 135/200 [04:40<02:02,  1.89s/it]Running Inference:  68%|██████▊   | 136/200 [04:43<02:23,  2.24s/it]Running Inference:  68%|██████▊   | 137/200 [04:44<02:06,  2.00s/it]Running Inference:  69%|██████▉   | 138/200 [04:47<02:18,  2.23s/it]Running Inference:  70%|██████▉   | 139/200 [04:50<02:23,  2.36s/it]Running Inference:  70%|███████   | 140/200 [04:52<02:16,  2.28s/it]Running Inference:  70%|███████   | 141/200 [04:55<02:20,  2.38s/it]Running Inference:  71%|███████   | 142/200 [04:57<02:25,  2.51s/it]Running Inference:  72%|███████▏  | 143/200 [05:00<02:27,  2.60s/it]Running Inference:  72%|███████▏  | 144/200 [05:02<02:06,  2.26s/it]Running Inference:  72%|███████▎  | 145/200 [05:03<01:49,  1.99s/it]Running Inference:  73%|███████▎  | 146/200 [05:06<01:56,  2.16s/it]Running Inference:  74%|███████▎  | 147/200 [05:06<01:28,  1.68s/it]Running Inference:  74%|███████▍  | 148/200 [05:10<01:59,  2.30s/it]Running Inference:  74%|███████▍  | 149/200 [05:13<02:15,  2.65s/it]Running Inference:  75%|███████▌  | 150/200 [05:14<01:46,  2.13s/it]Running Inference:  76%|███████▌  | 151/200 [05:16<01:38,  2.01s/it]Running Inference:  76%|███████▌  | 152/200 [05:17<01:26,  1.81s/it]Running Inference:  76%|███████▋  | 153/200 [05:20<01:35,  2.03s/it]Running Inference:  77%|███████▋  | 154/200 [05:24<02:05,  2.73s/it]Running Inference:  78%|███████▊  | 155/200 [05:26<01:52,  2.49s/it]Running Inference:  78%|███████▊  | 156/200 [05:29<01:50,  2.50s/it]Running Inference:  78%|███████▊  | 157/200 [05:31<01:47,  2.50s/it]Running Inference:  79%|███████▉  | 158/200 [05:33<01:36,  2.30s/it]Running Inference:  80%|███████▉  | 159/200 [05:34<01:23,  2.03s/it]Running Inference:  80%|████████  | 160/200 [05:35<01:09,  1.74s/it]Running Inference:  80%|████████  | 161/200 [05:37<01:08,  1.76s/it]Running Inference:  81%|████████  | 162/200 [05:39<01:06,  1.75s/it]Running Inference:  82%|████████▏ | 163/200 [05:41<01:09,  1.87s/it]Running Inference:  82%|████████▏ | 164/200 [05:44<01:14,  2.08s/it]Running Inference:  82%|████████▎ | 165/200 [05:46<01:10,  2.02s/it]Running Inference:  83%|████████▎ | 166/200 [05:47<01:07,  1.98s/it]Running Inference:  84%|████████▎ | 167/200 [05:49<00:58,  1.77s/it]Running Inference:  84%|████████▍ | 168/200 [05:53<01:23,  2.61s/it]Running Inference:  84%|████████▍ | 169/200 [05:54<01:06,  2.16s/it]Running Inference:  85%|████████▌ | 170/200 [05:55<00:52,  1.76s/it]Running Inference:  86%|████████▌ | 171/200 [05:56<00:43,  1.49s/it]Running Inference:  86%|████████▌ | 172/200 [05:58<00:44,  1.59s/it]Running Inference:  86%|████████▋ | 173/200 [06:00<00:43,  1.62s/it]Running Inference:  87%|████████▋ | 174/200 [06:02<00:44,  1.71s/it]Running Inference:  88%|████████▊ | 175/200 [06:03<00:38,  1.53s/it]Running Inference:  88%|████████▊ | 176/200 [06:04<00:33,  1.41s/it]Running Inference:  88%|████████▊ | 177/200 [06:06<00:36,  1.59s/it]Running Inference:  89%|████████▉ | 178/200 [06:08<00:42,  1.92s/it]Running Inference:  90%|████████▉ | 179/200 [06:09<00:31,  1.51s/it]Running Inference:  90%|█████████ | 180/200 [06:10<00:28,  1.44s/it]Running Inference:  90%|█████████ | 181/200 [06:11<00:23,  1.24s/it]Running Inference:  91%|█████████ | 182/200 [06:12<00:20,  1.12s/it]Running Inference:  92%|█████████▏| 183/200 [06:17<00:37,  2.22s/it]Running Inference:  92%|█████████▏| 184/200 [06:20<00:39,  2.46s/it]Running Inference:  92%|█████████▎| 185/200 [06:22<00:34,  2.30s/it]Running Inference:  93%|█████████▎| 186/200 [06:23<00:29,  2.08s/it]Running Inference:  94%|█████████▎| 187/200 [06:26<00:28,  2.17s/it]Running Inference:  94%|█████████▍| 188/200 [06:28<00:26,  2.23s/it]Running Inference:  94%|█████████▍| 189/200 [06:28<00:18,  1.72s/it]Running Inference:  95%|█████████▌| 190/200 [06:30<00:17,  1.74s/it]Running Inference:  96%|█████████▌| 191/200 [06:32<00:15,  1.71s/it]Running Inference:  96%|█████████▌| 192/200 [06:34<00:14,  1.86s/it]Running Inference:  96%|█████████▋| 193/200 [06:36<00:12,  1.78s/it]Running Inference:  97%|█████████▋| 194/200 [06:38<00:12,  2.03s/it]Running Inference:  98%|█████████▊| 195/200 [06:41<00:11,  2.21s/it]Running Inference:  98%|█████████▊| 196/200 [06:42<00:06,  1.74s/it]Running Inference:  98%|█████████▊| 197/200 [06:43<00:05,  1.70s/it]Running Inference:  99%|█████████▉| 198/200 [06:45<00:03,  1.79s/it]Running Inference: 100%|█████████▉| 199/200 [06:46<00:01,  1.43s/it]Running Inference: 100%|██████████| 200/200 [06:47<00:00,  1.32s/it]Running Inference: 100%|██████████| 200/200 [06:47<00:00,  2.04s/it]
2025-12-13 21:11:11,465 - INFO - Inference completed.
2025-12-13 21:11:11,488 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 21:11:11,489 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:11:11,527 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 21:11:11,527 - INFO - Metrics:
50.63
2025-12-13 21:11:11,528 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=triviaqa, ratio=0.1) on GPU 4

----------------------------------------
Task: triviaqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:11:17,907 - INFO - Set deterministic seeds to 42
2025-12-13 21:11:17,907 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:11:17,907 - INFO - Starting evaluation run...
2025-12-13 21:11:17,908 - INFO - Output directory set to: longbenchresult
2025-12-13 21:11:17,908 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 21:11:17,908 - INFO - KV Press 'knorm' setup.
2025-12-13 21:11:17,908 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:11:17,908 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.69it/s]
Device set to use cuda:0
2025-12-13 21:12:15,732 - INFO - Model pipeline loaded.
2025-12-13 21:12:15,732 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 21:12:26,863 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:12:26,863 - INFO - Dataset processed with 200 entries.
2025-12-13 21:12:26,893 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:53,  2.98s/it]Running Inference:   1%|          | 2/200 [00:04<07:39,  2.32s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:43,  1.74s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:38,  2.03s/it]Running Inference:   2%|▎         | 5/200 [00:10<07:13,  2.22s/it]Running Inference:   3%|▎         | 6/200 [00:11<05:37,  1.74s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:22,  1.98s/it]Running Inference:   4%|▍         | 8/200 [00:14<04:48,  1.50s/it]Running Inference:   4%|▍         | 9/200 [00:16<04:35,  1.44s/it]Running Inference:   5%|▌         | 10/200 [00:19<06:44,  2.13s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:25,  1.72s/it]Running Inference:   6%|▌         | 12/200 [00:23<06:37,  2.12s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:42,  1.83s/it]Running Inference:   7%|▋         | 14/200 [00:27<06:46,  2.18s/it]Running Inference:   8%|▊         | 15/200 [00:30<06:56,  2.25s/it]Running Inference:   8%|▊         | 16/200 [00:33<07:34,  2.47s/it]Running Inference:   8%|▊         | 17/200 [00:35<07:40,  2.52s/it]Running Inference:   9%|▉         | 18/200 [00:36<05:59,  1.97s/it]Running Inference:  10%|▉         | 19/200 [00:39<06:51,  2.27s/it]Running Inference:  10%|█         | 20/200 [00:40<06:07,  2.04s/it]Running Inference:  10%|█         | 21/200 [00:41<04:37,  1.55s/it]Running Inference:  11%|█         | 22/200 [00:44<06:10,  2.08s/it]Running Inference:  12%|█▏        | 23/200 [00:45<05:24,  1.83s/it]Running Inference:  12%|█▏        | 24/200 [00:47<05:31,  1.88s/it]Running Inference:  12%|█▎        | 25/200 [00:49<05:18,  1.82s/it]Running Inference:  13%|█▎        | 26/200 [00:50<04:50,  1.67s/it]Running Inference:  14%|█▎        | 27/200 [00:51<03:50,  1.33s/it]Running Inference:  14%|█▍        | 28/200 [00:52<03:16,  1.14s/it]Running Inference:  14%|█▍        | 29/200 [00:56<05:45,  2.02s/it]Running Inference:  15%|█▌        | 30/200 [00:56<04:30,  1.59s/it]Running Inference:  16%|█▌        | 31/200 [00:58<04:22,  1.56s/it]Running Inference:  16%|█▌        | 32/200 [01:00<04:45,  1.70s/it]Running Inference:  16%|█▋        | 33/200 [01:01<04:32,  1.63s/it]Running Inference:  17%|█▋        | 34/200 [01:03<04:31,  1.64s/it]Running Inference:  18%|█▊        | 35/200 [01:04<04:10,  1.52s/it]Running Inference:  18%|█▊        | 36/200 [01:06<04:48,  1.76s/it]Running Inference:  18%|█▊        | 37/200 [01:08<04:18,  1.59s/it]Running Inference:  19%|█▉        | 38/200 [01:10<05:09,  1.91s/it]Running Inference:  20%|█▉        | 39/200 [01:11<04:01,  1.50s/it]Running Inference:  20%|██        | 40/200 [01:14<05:22,  2.02s/it]Running Inference:  20%|██        | 41/200 [01:16<05:00,  1.89s/it]Running Inference:  21%|██        | 42/200 [01:18<05:38,  2.14s/it]Running Inference:  22%|██▏       | 43/200 [01:19<04:17,  1.64s/it]Running Inference:  22%|██▏       | 44/200 [01:21<04:21,  1.68s/it]Running Inference:  22%|██▎       | 45/200 [01:21<03:22,  1.31s/it]Running Inference:  23%|██▎       | 46/200 [01:23<03:29,  1.36s/it]Running Inference:  24%|██▎       | 47/200 [01:24<03:14,  1.27s/it]Running Inference:  24%|██▍       | 48/200 [01:27<04:38,  1.83s/it]Running Inference:  24%|██▍       | 49/200 [01:29<04:50,  1.92s/it]Running Inference:  25%|██▌       | 50/200 [01:32<05:28,  2.19s/it]Running Inference:  26%|██▌       | 51/200 [01:32<04:21,  1.75s/it]Running Inference:  26%|██▌       | 52/200 [01:35<04:40,  1.89s/it]Running Inference:  26%|██▋       | 53/200 [01:35<03:36,  1.47s/it]Running Inference:  27%|██▋       | 54/200 [01:36<02:54,  1.20s/it]Running Inference:  28%|██▊       | 55/200 [01:37<02:51,  1.18s/it]Running Inference:  28%|██▊       | 56/200 [01:38<03:08,  1.31s/it]Running Inference:  28%|██▊       | 57/200 [01:41<04:21,  1.83s/it]Running Inference:  29%|██▉       | 58/200 [01:43<04:14,  1.79s/it]Running Inference:  30%|██▉       | 59/200 [01:45<04:35,  1.95s/it]Running Inference:  30%|███       | 60/200 [01:47<04:17,  1.84s/it]Running Inference:  30%|███       | 61/200 [01:50<05:09,  2.22s/it]Running Inference:  31%|███       | 62/200 [01:52<04:44,  2.06s/it]Running Inference:  32%|███▏      | 63/200 [01:54<05:03,  2.21s/it]Running Inference:  32%|███▏      | 64/200 [01:55<04:07,  1.82s/it]Running Inference:  32%|███▎      | 65/200 [01:58<04:28,  1.99s/it]Running Inference:  33%|███▎      | 66/200 [02:00<04:33,  2.04s/it]Running Inference:  34%|███▎      | 67/200 [02:02<04:28,  2.02s/it]Running Inference:  34%|███▍      | 68/200 [02:03<03:44,  1.70s/it]Running Inference:  34%|███▍      | 69/200 [02:06<04:58,  2.27s/it]Running Inference:  35%|███▌      | 70/200 [02:11<06:17,  2.90s/it]Running Inference:  36%|███▌      | 71/200 [02:14<06:13,  2.90s/it]Running Inference:  36%|███▌      | 72/200 [02:17<06:46,  3.17s/it]Running Inference:  36%|███▋      | 73/200 [02:19<05:39,  2.67s/it]Running Inference:  37%|███▋      | 74/200 [02:22<05:39,  2.69s/it]Running Inference:  38%|███▊      | 75/200 [02:22<04:13,  2.03s/it]Running Inference:  38%|███▊      | 76/200 [02:24<04:06,  1.98s/it]Running Inference:  38%|███▊      | 77/200 [02:27<04:26,  2.17s/it]Running Inference:  39%|███▉      | 78/200 [02:27<03:17,  1.62s/it]Running Inference:  40%|███▉      | 79/200 [02:29<03:31,  1.75s/it]Running Inference:  40%|████      | 80/200 [02:31<03:44,  1.87s/it]Running Inference:  40%|████      | 81/200 [02:34<04:18,  2.17s/it]Running Inference:  41%|████      | 82/200 [02:35<03:15,  1.66s/it]Running Inference:  42%|████▏     | 83/200 [02:38<04:22,  2.24s/it]Running Inference:  42%|████▏     | 84/200 [02:41<04:47,  2.48s/it]Running Inference:  42%|████▎     | 85/200 [02:43<04:11,  2.19s/it]Running Inference:  43%|████▎     | 86/200 [02:48<06:05,  3.21s/it]Running Inference:  44%|████▎     | 87/200 [02:51<05:38,  3.00s/it]Running Inference:  44%|████▍     | 88/200 [02:53<05:22,  2.88s/it]Running Inference:  44%|████▍     | 89/200 [02:55<04:21,  2.36s/it]Running Inference:  45%|████▌     | 90/200 [02:56<04:00,  2.18s/it]Running Inference:  46%|████▌     | 91/200 [02:59<04:24,  2.42s/it]Running Inference:  46%|████▌     | 92/200 [03:00<03:26,  1.92s/it]Running Inference:  46%|████▋     | 93/200 [03:03<03:58,  2.23s/it]Running Inference:  47%|████▋     | 94/200 [03:05<03:35,  2.03s/it]Running Inference:  48%|████▊     | 95/200 [03:09<04:54,  2.81s/it]Running Inference:  48%|████▊     | 96/200 [03:12<04:37,  2.67s/it]Running Inference:  48%|████▊     | 97/200 [03:13<03:55,  2.29s/it]Running Inference:  49%|████▉     | 98/200 [03:15<03:57,  2.33s/it]Running Inference:  50%|████▉     | 99/200 [03:19<04:29,  2.67s/it]Running Inference:  50%|█████     | 100/200 [03:20<03:41,  2.22s/it]Running Inference:  50%|█████     | 101/200 [03:22<03:40,  2.23s/it]Running Inference:  51%|█████     | 102/200 [03:25<03:41,  2.27s/it]Running Inference:  52%|█████▏    | 103/200 [03:27<03:41,  2.28s/it]Running Inference:  52%|█████▏    | 104/200 [03:28<03:01,  1.90s/it]Running Inference:  52%|█████▎    | 105/200 [03:30<03:13,  2.04s/it]Running Inference:  53%|█████▎    | 106/200 [03:33<03:44,  2.39s/it]Running Inference:  54%|█████▎    | 107/200 [03:36<03:40,  2.37s/it]Running Inference:  54%|█████▍    | 108/200 [03:39<04:00,  2.61s/it]Running Inference:  55%|█████▍    | 109/200 [03:40<03:11,  2.11s/it]Running Inference:  55%|█████▌    | 110/200 [03:40<02:27,  1.64s/it]Running Inference:  56%|█████▌    | 111/200 [03:42<02:16,  1.53s/it]Running Inference:  56%|█████▌    | 112/200 [03:45<03:05,  2.11s/it]Running Inference:  56%|█████▋    | 113/200 [03:47<02:42,  1.87s/it]Running Inference:  57%|█████▋    | 114/200 [03:49<03:02,  2.12s/it]Running Inference:  57%|█████▊    | 115/200 [03:51<02:51,  2.02s/it]Running Inference:  58%|█████▊    | 116/200 [03:51<02:09,  1.54s/it]Running Inference:  58%|█████▊    | 117/200 [03:55<03:00,  2.17s/it]Running Inference:  59%|█████▉    | 118/200 [03:56<02:25,  1.77s/it]Running Inference:  60%|█████▉    | 119/200 [03:57<02:17,  1.70s/it]Running Inference:  60%|██████    | 120/200 [03:59<02:24,  1.80s/it]Running Inference:  60%|██████    | 121/200 [04:00<01:47,  1.36s/it]Running Inference:  61%|██████    | 122/200 [04:01<01:47,  1.38s/it]Running Inference:  62%|██████▏   | 123/200 [04:02<01:24,  1.09s/it]Running Inference:  62%|██████▏   | 124/200 [04:02<01:14,  1.02it/s]Running Inference:  62%|██████▎   | 125/200 [04:05<01:47,  1.44s/it]Running Inference:  63%|██████▎   | 126/200 [04:08<02:16,  1.84s/it]Running Inference:  64%|██████▎   | 127/200 [04:10<02:21,  1.94s/it]Running Inference:  64%|██████▍   | 128/200 [04:11<01:54,  1.59s/it]Running Inference:  64%|██████▍   | 129/200 [04:12<01:41,  1.43s/it]Running Inference:  65%|██████▌   | 130/200 [04:12<01:25,  1.22s/it]Running Inference:  66%|██████▌   | 131/200 [04:14<01:26,  1.26s/it]Running Inference:  66%|██████▌   | 132/200 [04:15<01:33,  1.37s/it]Running Inference:  66%|██████▋   | 133/200 [04:17<01:39,  1.49s/it]Running Inference:  67%|██████▋   | 134/200 [04:21<02:28,  2.25s/it]Running Inference:  68%|██████▊   | 135/200 [04:23<02:12,  2.04s/it]Running Inference:  68%|██████▊   | 136/200 [04:26<02:30,  2.34s/it]Running Inference:  68%|██████▊   | 137/200 [04:27<02:10,  2.07s/it]Running Inference:  69%|██████▉   | 138/200 [04:30<02:27,  2.37s/it]Running Inference:  70%|██████▉   | 139/200 [04:33<02:29,  2.46s/it]Running Inference:  70%|███████   | 140/200 [04:35<02:20,  2.34s/it]Running Inference:  70%|███████   | 141/200 [04:38<02:21,  2.40s/it]Running Inference:  71%|███████   | 142/200 [04:40<02:26,  2.52s/it]Running Inference:  72%|███████▏  | 143/200 [04:43<02:28,  2.60s/it]Running Inference:  72%|███████▏  | 144/200 [04:45<02:06,  2.27s/it]Running Inference:  72%|███████▎  | 145/200 [04:46<01:49,  1.99s/it]Running Inference:  73%|███████▎  | 146/200 [04:49<01:56,  2.15s/it]Running Inference:  74%|███████▎  | 147/200 [04:49<01:28,  1.68s/it]Running Inference:  74%|███████▍  | 148/200 [04:53<01:59,  2.29s/it]Running Inference:  74%|███████▍  | 149/200 [04:56<02:14,  2.64s/it]Running Inference:  75%|███████▌  | 150/200 [04:57<01:46,  2.13s/it]Running Inference:  76%|███████▌  | 151/200 [04:59<01:38,  2.01s/it]Running Inference:  76%|███████▌  | 152/200 [05:00<01:20,  1.69s/it]Running Inference:  76%|███████▋  | 153/200 [05:02<01:32,  1.96s/it]Running Inference:  77%|███████▋  | 154/200 [05:05<01:35,  2.07s/it]Running Inference:  78%|███████▊  | 155/200 [05:07<01:29,  1.98s/it]Running Inference:  78%|███████▊  | 156/200 [05:09<01:34,  2.14s/it]Running Inference:  78%|███████▊  | 157/200 [05:12<01:36,  2.25s/it]Running Inference:  79%|███████▉  | 158/200 [05:13<01:29,  2.12s/it]Running Inference:  80%|███████▉  | 159/200 [05:15<01:17,  1.90s/it]Running Inference:  80%|████████  | 160/200 [05:16<01:06,  1.65s/it]Running Inference:  80%|████████  | 161/200 [05:18<01:06,  1.69s/it]Running Inference:  81%|████████  | 162/200 [05:19<01:05,  1.72s/it]Running Inference:  82%|████████▏ | 163/200 [05:22<01:08,  1.85s/it]Running Inference:  82%|████████▏ | 164/200 [05:23<00:58,  1.62s/it]Running Inference:  82%|████████▎ | 165/200 [05:24<00:58,  1.68s/it]Running Inference:  83%|████████▎ | 166/200 [05:26<00:59,  1.74s/it]Running Inference:  84%|████████▎ | 167/200 [05:28<00:53,  1.61s/it]Running Inference:  84%|████████▍ | 168/200 [05:32<01:19,  2.48s/it]Running Inference:  84%|████████▍ | 169/200 [05:33<01:04,  2.07s/it]Running Inference:  85%|████████▌ | 170/200 [05:36<01:07,  2.25s/it]Running Inference:  86%|████████▌ | 171/200 [05:37<00:53,  1.85s/it]Running Inference:  86%|████████▌ | 172/200 [05:39<00:51,  1.83s/it]Running Inference:  86%|████████▋ | 173/200 [05:40<00:48,  1.79s/it]Running Inference:  87%|████████▋ | 174/200 [05:42<00:47,  1.83s/it]Running Inference:  88%|████████▊ | 175/200 [05:43<00:40,  1.62s/it]Running Inference:  88%|████████▊ | 176/200 [05:45<00:35,  1.48s/it]Running Inference:  88%|████████▊ | 177/200 [05:46<00:37,  1.61s/it]Running Inference:  89%|████████▉ | 178/200 [05:49<00:42,  1.94s/it]Running Inference:  90%|████████▉ | 179/200 [05:50<00:31,  1.51s/it]Running Inference:  90%|█████████ | 180/200 [05:51<00:28,  1.45s/it]Running Inference:  90%|█████████ | 181/200 [05:54<00:35,  1.86s/it]Running Inference:  91%|█████████ | 182/200 [05:55<00:28,  1.56s/it]Running Inference:  92%|█████████▏| 183/200 [05:59<00:42,  2.52s/it]Running Inference:  92%|█████████▏| 184/200 [06:02<00:42,  2.66s/it]Running Inference:  92%|█████████▎| 185/200 [06:03<00:32,  2.14s/it]Running Inference:  93%|█████████▎| 186/200 [06:05<00:27,  1.96s/it]Running Inference:  94%|█████████▎| 187/200 [06:07<00:27,  2.08s/it]Running Inference:  94%|█████████▍| 188/200 [06:10<00:25,  2.16s/it]Running Inference:  94%|█████████▍| 189/200 [06:10<00:18,  1.66s/it]Running Inference:  95%|█████████▌| 190/200 [06:12<00:17,  1.70s/it]Running Inference:  96%|█████████▌| 191/200 [06:14<00:15,  1.68s/it]Running Inference:  96%|█████████▌| 192/200 [06:16<00:14,  1.84s/it]Running Inference:  96%|█████████▋| 193/200 [06:17<00:12,  1.76s/it]Running Inference:  97%|█████████▋| 194/200 [06:20<00:12,  2.01s/it]Running Inference:  98%|█████████▊| 195/200 [06:23<00:10,  2.20s/it]Running Inference:  98%|█████████▊| 196/200 [06:23<00:06,  1.73s/it]Running Inference:  98%|█████████▊| 197/200 [06:25<00:05,  1.69s/it]Running Inference:  99%|█████████▉| 198/200 [06:26<00:03,  1.69s/it]Running Inference: 100%|█████████▉| 199/200 [06:27<00:01,  1.42s/it]Running Inference: 100%|██████████| 200/200 [06:28<00:00,  1.31s/it]Running Inference: 100%|██████████| 200/200 [06:28<00:00,  1.94s/it]
2025-12-13 21:18:55,747 - INFO - Inference completed.
2025-12-13 21:18:55,771 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-13 21:18:55,771 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:18:55,808 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-13 21:18:55,808 - INFO - Metrics:
48.22
2025-12-13 21:18:55,809 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=triviaqa, ratio=0.2) on GPU 4

----------------------------------------
Task: triviaqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:19:02,250 - INFO - Set deterministic seeds to 42
2025-12-13 21:19:02,251 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:19:02,251 - INFO - Starting evaluation run...
2025-12-13 21:19:02,251 - INFO - Output directory set to: longbenchresult
2025-12-13 21:19:02,251 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-13 21:19:02,251 - INFO - KV Press 'knorm' setup.
2025-12-13 21:19:02,251 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:19:02,251 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.57it/s]
Device set to use cuda:0
2025-12-13 21:19:15,379 - INFO - Model pipeline loaded.
2025-12-13 21:19:15,379 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 21:19:22,123 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:19:22,123 - INFO - Dataset processed with 200 entries.
2025-12-13 21:19:22,151 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:43,  2.93s/it]Running Inference:   1%|          | 2/200 [00:06<11:06,  3.37s/it]Running Inference:   2%|▏         | 3/200 [00:09<10:28,  3.19s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:29,  2.91s/it]Running Inference:   2%|▎         | 5/200 [00:12<06:34,  2.02s/it]Running Inference:   3%|▎         | 6/200 [00:13<05:11,  1.61s/it]Running Inference:   4%|▎         | 7/200 [00:15<06:01,  1.87s/it]Running Inference:   4%|▍         | 8/200 [00:16<04:34,  1.43s/it]Running Inference:   4%|▍         | 9/200 [00:17<04:25,  1.39s/it]Running Inference:   5%|▌         | 10/200 [00:19<04:31,  1.43s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:19<03:53,  1.24s/it]Running Inference:   6%|▌         | 12/200 [00:23<06:08,  1.96s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:21,  1.72s/it]Running Inference:   7%|▋         | 14/200 [00:27<06:30,  2.10s/it]Running Inference:   8%|▊         | 15/200 [00:30<06:48,  2.21s/it]Running Inference:   8%|▊         | 16/200 [00:33<07:28,  2.44s/it]Running Inference:   8%|▊         | 17/200 [00:33<05:38,  1.85s/it]Running Inference:   9%|▉         | 18/200 [00:34<04:33,  1.50s/it]Running Inference:  10%|▉         | 19/200 [00:39<07:40,  2.54s/it]Running Inference:  10%|█         | 20/200 [00:40<06:41,  2.23s/it]Running Inference:  10%|█         | 21/200 [00:41<05:00,  1.68s/it]Running Inference:  11%|█         | 22/200 [00:44<06:23,  2.16s/it]Running Inference:  12%|█▏        | 23/200 [00:45<05:33,  1.88s/it]Running Inference:  12%|█▏        | 24/200 [00:47<05:41,  1.94s/it]Running Inference:  12%|█▎        | 25/200 [00:49<05:24,  1.86s/it]Running Inference:  13%|█▎        | 26/200 [00:50<04:54,  1.69s/it]Running Inference:  14%|█▎        | 27/200 [00:51<03:53,  1.35s/it]Running Inference:  14%|█▍        | 28/200 [00:51<03:17,  1.15s/it]Running Inference:  14%|█▍        | 29/200 [00:55<05:42,  2.00s/it]Running Inference:  15%|█▌        | 30/200 [00:56<04:28,  1.58s/it]Running Inference:  16%|█▌        | 31/200 [00:57<04:21,  1.54s/it]Running Inference:  16%|█▌        | 32/200 [00:59<04:43,  1.69s/it]Running Inference:  16%|█▋        | 33/200 [01:01<04:29,  1.62s/it]Running Inference:  17%|█▋        | 34/200 [01:03<04:29,  1.62s/it]Running Inference:  18%|█▊        | 35/200 [01:04<04:08,  1.51s/it]Running Inference:  18%|█▊        | 36/200 [01:08<06:22,  2.33s/it]Running Inference:  18%|█▊        | 37/200 [01:09<05:24,  1.99s/it]Running Inference:  19%|█▉        | 38/200 [01:12<05:55,  2.19s/it]Running Inference:  20%|█▉        | 39/200 [01:12<04:32,  1.69s/it]Running Inference:  20%|██        | 40/200 [01:16<05:43,  2.15s/it]Running Inference:  20%|██        | 41/200 [01:17<05:08,  1.94s/it]Running Inference:  21%|██        | 42/200 [01:18<04:04,  1.55s/it]Running Inference:  22%|██▏       | 43/200 [01:18<03:12,  1.23s/it]Running Inference:  22%|██▏       | 44/200 [01:20<03:36,  1.39s/it]Running Inference:  22%|██▎       | 45/200 [01:22<04:28,  1.74s/it]Running Inference:  23%|██▎       | 46/200 [01:24<04:04,  1.59s/it]Running Inference:  24%|██▎       | 47/200 [01:25<03:44,  1.47s/it]Running Inference:  24%|██▍       | 48/200 [01:28<04:58,  1.97s/it]Running Inference:  24%|██▍       | 49/200 [01:30<05:03,  2.01s/it]Running Inference:  25%|██▌       | 50/200 [01:33<05:36,  2.24s/it]Running Inference:  26%|██▌       | 51/200 [01:34<04:26,  1.79s/it]Running Inference:  26%|██▌       | 52/200 [01:36<04:44,  1.92s/it]Running Inference:  26%|██▋       | 53/200 [01:36<03:38,  1.49s/it]Running Inference:  27%|██▋       | 54/200 [01:37<02:56,  1.21s/it]Running Inference:  28%|██▊       | 55/200 [01:38<02:52,  1.19s/it]Running Inference:  28%|██▊       | 56/200 [01:40<03:08,  1.31s/it]Running Inference:  28%|██▊       | 57/200 [01:44<05:32,  2.33s/it]Running Inference:  29%|██▉       | 58/200 [01:46<05:05,  2.15s/it]Running Inference:  30%|██▉       | 59/200 [01:48<05:11,  2.21s/it]Running Inference:  30%|███       | 60/200 [01:50<04:42,  2.02s/it]Running Inference:  30%|███       | 61/200 [01:53<05:25,  2.34s/it]Running Inference:  31%|███       | 62/200 [01:55<04:54,  2.14s/it]Running Inference:  32%|███▏      | 63/200 [01:56<04:28,  1.96s/it]Running Inference:  32%|███▏      | 64/200 [01:57<03:43,  1.64s/it]Running Inference:  32%|███▎      | 65/200 [02:00<04:08,  1.84s/it]Running Inference:  33%|███▎      | 66/200 [02:02<04:23,  1.97s/it]Running Inference:  34%|███▎      | 67/200 [02:04<04:19,  1.95s/it]Running Inference:  34%|███▍      | 68/200 [02:05<03:37,  1.65s/it]Running Inference:  34%|███▍      | 69/200 [02:08<04:50,  2.21s/it]Running Inference:  35%|███▌      | 70/200 [02:12<06:07,  2.83s/it]Running Inference:  36%|███▌      | 71/200 [02:15<06:06,  2.84s/it]Running Inference:  36%|███▌      | 72/200 [02:19<06:39,  3.12s/it]Running Inference:  36%|███▋      | 73/200 [02:21<05:33,  2.63s/it]Running Inference:  37%|███▋      | 74/200 [02:23<05:34,  2.66s/it]Running Inference:  38%|███▊      | 75/200 [02:24<04:06,  1.97s/it]Running Inference:  38%|███▊      | 76/200 [02:26<04:00,  1.94s/it]Running Inference:  38%|███▊      | 77/200 [02:28<04:24,  2.15s/it]Running Inference:  39%|███▉      | 78/200 [02:28<03:15,  1.60s/it]Running Inference:  40%|███▉      | 79/200 [02:31<03:29,  1.73s/it]Running Inference:  40%|████      | 80/200 [02:33<03:42,  1.85s/it]Running Inference:  40%|████      | 81/200 [02:35<04:16,  2.15s/it]Running Inference:  41%|████      | 82/200 [02:36<03:14,  1.64s/it]Running Inference:  42%|████▏     | 83/200 [02:39<04:18,  2.21s/it]Running Inference:  42%|████▏     | 84/200 [02:43<04:44,  2.45s/it]Running Inference:  42%|████▎     | 85/200 [02:44<04:09,  2.17s/it]Running Inference:  43%|████▎     | 86/200 [02:49<06:00,  3.16s/it]Running Inference:  44%|████▎     | 87/200 [02:52<05:34,  2.96s/it]Running Inference:  44%|████▍     | 88/200 [02:55<05:18,  2.84s/it]Running Inference:  44%|████▍     | 89/200 [02:56<04:18,  2.33s/it]Running Inference:  45%|████▌     | 90/200 [02:57<03:57,  2.16s/it]Running Inference:  46%|████▌     | 91/200 [03:00<04:21,  2.40s/it]Running Inference:  46%|████▌     | 92/200 [03:01<03:24,  1.89s/it]Running Inference:  46%|████▋     | 93/200 [03:04<03:56,  2.21s/it]Running Inference:  47%|████▋     | 94/200 [03:06<03:33,  2.01s/it]Running Inference:  48%|████▊     | 95/200 [03:10<04:51,  2.77s/it]Running Inference:  48%|████▊     | 96/200 [03:12<04:34,  2.63s/it]Running Inference:  48%|████▊     | 97/200 [03:14<03:53,  2.27s/it]Running Inference:  49%|████▉     | 98/200 [03:16<03:55,  2.31s/it]Running Inference:  50%|████▉     | 99/200 [03:20<04:25,  2.63s/it]Running Inference:  50%|█████     | 100/200 [03:21<03:38,  2.19s/it]Running Inference:  50%|█████     | 101/200 [03:23<03:38,  2.21s/it]Running Inference:  51%|█████     | 102/200 [03:25<03:39,  2.24s/it]Running Inference:  52%|█████▏    | 103/200 [03:27<03:32,  2.19s/it]Running Inference:  52%|█████▏    | 104/200 [03:28<02:55,  1.83s/it]Running Inference:  52%|█████▎    | 105/200 [03:31<03:09,  1.99s/it]Running Inference:  53%|█████▎    | 106/200 [03:34<03:39,  2.33s/it]Running Inference:  54%|█████▎    | 107/200 [03:38<04:31,  2.92s/it]Running Inference:  54%|█████▍    | 108/200 [03:41<04:35,  3.00s/it]Running Inference:  55%|█████▍    | 109/200 [03:42<03:36,  2.37s/it]Running Inference:  55%|█████▌    | 110/200 [03:43<02:44,  1.83s/it]Running Inference:  56%|█████▌    | 111/200 [03:44<02:27,  1.66s/it]Running Inference:  56%|█████▌    | 112/200 [03:48<03:11,  2.17s/it]Running Inference:  56%|█████▋    | 113/200 [03:49<02:40,  1.85s/it]Running Inference:  57%|█████▋    | 114/200 [03:51<03:00,  2.10s/it]Running Inference:  57%|█████▊    | 115/200 [03:53<02:50,  2.00s/it]Running Inference:  58%|█████▊    | 116/200 [03:53<02:08,  1.52s/it]Running Inference:  58%|█████▊    | 117/200 [03:57<02:57,  2.14s/it]Running Inference:  59%|█████▉    | 118/200 [03:58<02:23,  1.75s/it]Running Inference:  60%|█████▉    | 119/200 [03:59<02:14,  1.66s/it]Running Inference:  60%|██████    | 120/200 [04:03<02:57,  2.21s/it]Running Inference:  60%|██████    | 121/200 [04:03<02:10,  1.65s/it]Running Inference:  61%|██████    | 122/200 [04:05<02:03,  1.58s/it]Running Inference:  62%|██████▏   | 123/200 [04:05<01:34,  1.23s/it]Running Inference:  62%|██████▏   | 124/200 [04:06<01:21,  1.08s/it]Running Inference:  62%|██████▎   | 125/200 [04:08<01:47,  1.43s/it]Running Inference:  63%|██████▎   | 126/200 [04:09<01:26,  1.18s/it]Running Inference:  64%|██████▎   | 127/200 [04:11<01:47,  1.47s/it]Running Inference:  64%|██████▍   | 128/200 [04:12<01:30,  1.26s/it]Running Inference:  64%|██████▍   | 129/200 [04:13<01:24,  1.19s/it]Running Inference:  65%|██████▌   | 130/200 [04:13<01:13,  1.05s/it]Running Inference:  66%|██████▌   | 131/200 [04:15<01:18,  1.14s/it]Running Inference:  66%|██████▌   | 132/200 [04:16<01:27,  1.29s/it]Running Inference:  66%|██████▋   | 133/200 [04:18<01:27,  1.31s/it]Running Inference:  67%|██████▋   | 134/200 [04:22<02:18,  2.10s/it]Running Inference:  68%|██████▊   | 135/200 [04:25<02:38,  2.44s/it]Running Inference:  68%|██████▊   | 136/200 [04:28<02:47,  2.62s/it]Running Inference:  68%|██████▊   | 137/200 [04:29<02:22,  2.26s/it]Running Inference:  69%|██████▉   | 138/200 [04:32<02:29,  2.40s/it]Running Inference:  70%|██████▉   | 139/200 [04:35<02:30,  2.47s/it]Running Inference:  70%|███████   | 140/200 [04:37<02:21,  2.35s/it]Running Inference:  70%|███████   | 141/200 [04:39<02:21,  2.40s/it]Running Inference:  71%|███████   | 142/200 [04:42<02:26,  2.52s/it]Running Inference:  72%|███████▏  | 143/200 [04:43<02:01,  2.12s/it]Running Inference:  72%|███████▏  | 144/200 [04:45<01:48,  1.93s/it]Running Inference:  72%|███████▎  | 145/200 [04:46<01:36,  1.75s/it]Running Inference:  73%|███████▎  | 146/200 [04:49<01:47,  1.98s/it]Running Inference:  74%|███████▎  | 147/200 [04:49<01:22,  1.55s/it]Running Inference:  74%|███████▍  | 148/200 [04:53<01:53,  2.19s/it]Running Inference:  74%|███████▍  | 149/200 [04:58<02:41,  3.16s/it]Running Inference:  75%|███████▌  | 150/200 [04:59<02:04,  2.49s/it]Running Inference:  76%|███████▌  | 151/200 [05:01<01:50,  2.26s/it]Running Inference:  76%|███████▌  | 152/200 [05:03<01:43,  2.15s/it]Running Inference:  76%|███████▋  | 153/200 [05:05<01:46,  2.28s/it]Running Inference:  77%|███████▋  | 154/200 [05:08<01:45,  2.29s/it]Running Inference:  78%|███████▊  | 155/200 [05:09<01:36,  2.14s/it]Running Inference:  78%|███████▊  | 156/200 [05:12<01:39,  2.25s/it]Running Inference:  78%|███████▊  | 157/200 [05:14<01:39,  2.32s/it]Running Inference:  79%|███████▉  | 158/200 [05:16<01:31,  2.17s/it]Running Inference:  80%|███████▉  | 159/200 [05:18<01:19,  1.93s/it]Running Inference:  80%|████████  | 160/200 [05:19<01:06,  1.67s/it]Running Inference:  80%|████████  | 161/200 [05:21<01:07,  1.72s/it]Running Inference:  81%|████████  | 162/200 [05:24<01:29,  2.34s/it]Running Inference:  82%|████████▏ | 163/200 [05:26<01:24,  2.28s/it]Running Inference:  82%|████████▏ | 164/200 [05:27<01:04,  1.79s/it]Running Inference:  82%|████████▎ | 165/200 [05:29<01:02,  1.80s/it]Running Inference:  83%|████████▎ | 166/200 [05:31<01:01,  1.82s/it]Running Inference:  84%|████████▎ | 167/200 [05:32<00:54,  1.66s/it]Running Inference:  84%|████████▍ | 168/200 [05:37<01:19,  2.49s/it]Running Inference:  84%|████████▍ | 169/200 [05:38<01:04,  2.09s/it]Running Inference:  85%|████████▌ | 170/200 [05:40<01:07,  2.24s/it]Running Inference:  86%|████████▌ | 171/200 [05:41<00:53,  1.84s/it]Running Inference:  86%|████████▌ | 172/200 [05:43<00:51,  1.82s/it]Running Inference:  86%|████████▋ | 173/200 [05:45<00:48,  1.78s/it]Running Inference:  87%|████████▋ | 174/200 [05:47<00:47,  1.82s/it]Running Inference:  88%|████████▊ | 175/200 [05:48<00:40,  1.61s/it]Running Inference:  88%|████████▊ | 176/200 [05:49<00:35,  1.47s/it]Running Inference:  88%|████████▊ | 177/200 [05:51<00:36,  1.60s/it]Running Inference:  89%|████████▉ | 178/200 [05:53<00:42,  1.93s/it]Running Inference:  90%|████████▉ | 179/200 [05:54<00:31,  1.51s/it]Running Inference:  90%|█████████ | 180/200 [05:55<00:28,  1.44s/it]Running Inference:  90%|█████████ | 181/200 [05:56<00:23,  1.25s/it]Running Inference:  91%|█████████ | 182/200 [05:57<00:20,  1.13s/it]Running Inference:  92%|█████████▏| 183/200 [06:02<00:37,  2.20s/it]Running Inference:  92%|█████████▏| 184/200 [06:05<00:38,  2.43s/it]Running Inference:  92%|█████████▎| 185/200 [06:05<00:29,  1.97s/it]Running Inference:  93%|█████████▎| 186/200 [06:07<00:25,  1.85s/it]Running Inference:  94%|█████████▎| 187/200 [06:09<00:25,  1.99s/it]Running Inference:  94%|█████████▍| 188/200 [06:12<00:25,  2.09s/it]Running Inference:  94%|█████████▍| 189/200 [06:12<00:17,  1.61s/it]Running Inference:  95%|█████████▌| 190/200 [06:14<00:16,  1.66s/it]Running Inference:  96%|█████████▌| 191/200 [06:16<00:14,  1.65s/it]Running Inference:  96%|█████████▌| 192/200 [06:18<00:14,  1.82s/it]Running Inference:  96%|█████████▋| 193/200 [06:19<00:12,  1.74s/it]Running Inference:  97%|█████████▋| 194/200 [06:22<00:11,  2.00s/it]Running Inference:  98%|█████████▊| 195/200 [06:25<00:10,  2.18s/it]Running Inference:  98%|█████████▊| 196/200 [06:25<00:06,  1.72s/it]Running Inference:  98%|█████████▊| 197/200 [06:27<00:05,  1.68s/it]Running Inference:  99%|█████████▉| 198/200 [06:28<00:03,  1.69s/it]Running Inference: 100%|█████████▉| 199/200 [06:29<00:01,  1.35s/it]Running Inference: 100%|██████████| 200/200 [06:30<00:00,  1.26s/it]Running Inference: 100%|██████████| 200/200 [06:30<00:00,  1.95s/it]
2025-12-13 21:25:52,752 - INFO - Inference completed.
2025-12-13 21:25:52,776 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-13 21:25:52,776 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:25:52,814 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-13 21:25:52,814 - INFO - Metrics:
48.3
2025-12-13 21:25:52,815 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=triviaqa, ratio=0.3) on GPU 4

----------------------------------------
Task: triviaqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:25:59,205 - INFO - Set deterministic seeds to 42
2025-12-13 21:25:59,205 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:25:59,205 - INFO - Starting evaluation run...
2025-12-13 21:25:59,205 - INFO - Output directory set to: longbenchresult
2025-12-13 21:25:59,205 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-13 21:25:59,205 - INFO - KV Press 'knorm' setup.
2025-12-13 21:25:59,205 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:25:59,205 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.12it/s]
Device set to use cuda:0
2025-12-13 21:26:12,274 - INFO - Model pipeline loaded.
2025-12-13 21:26:12,274 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 21:26:21,745 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:26:21,745 - INFO - Dataset processed with 200 entries.
2025-12-13 21:26:21,774 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:41,  2.92s/it]Running Inference:   1%|          | 2/200 [00:06<11:05,  3.36s/it]Running Inference:   2%|▏         | 3/200 [00:07<07:32,  2.30s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:51,  2.41s/it]Running Inference:   2%|▎         | 5/200 [00:12<07:53,  2.43s/it]Running Inference:   3%|▎         | 6/200 [00:13<06:03,  1.87s/it]Running Inference:   4%|▎         | 7/200 [00:15<06:36,  2.06s/it]Running Inference:   4%|▍         | 8/200 [00:16<04:57,  1.55s/it]Running Inference:   4%|▍         | 9/200 [00:17<04:36,  1.45s/it]Running Inference:   5%|▌         | 10/200 [00:19<04:53,  1.55s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<04:07,  1.31s/it]Running Inference:   6%|▌         | 12/200 [00:25<07:41,  2.46s/it]Running Inference:   6%|▋         | 13/200 [00:28<08:26,  2.71s/it]Running Inference:   7%|▋         | 14/200 [00:31<08:38,  2.79s/it]Running Inference:   8%|▊         | 15/200 [00:34<08:22,  2.72s/it]Running Inference:   8%|▊         | 16/200 [00:36<08:33,  2.79s/it]Running Inference:   8%|▊         | 17/200 [00:37<06:38,  2.18s/it]Running Inference:   9%|▉         | 18/200 [00:38<05:14,  1.73s/it]Running Inference:  10%|▉         | 19/200 [00:43<08:07,  2.69s/it]Running Inference:  10%|█         | 20/200 [00:44<06:59,  2.33s/it]Running Inference:  10%|█         | 21/200 [00:45<05:13,  1.75s/it]Running Inference:  11%|█         | 22/200 [00:48<06:31,  2.20s/it]Running Inference:  12%|█▏        | 23/200 [00:49<05:38,  1.91s/it]Running Inference:  12%|█▏        | 24/200 [00:51<05:44,  1.96s/it]Running Inference:  12%|█▎        | 25/200 [00:53<05:26,  1.87s/it]Running Inference:  13%|█▎        | 26/200 [00:54<04:54,  1.69s/it]Running Inference:  14%|█▎        | 27/200 [00:55<03:48,  1.32s/it]Running Inference:  14%|█▍        | 28/200 [00:55<03:14,  1.13s/it]Running Inference:  14%|█▍        | 29/200 [00:59<05:40,  1.99s/it]Running Inference:  15%|█▌        | 30/200 [01:02<06:04,  2.14s/it]Running Inference:  16%|█▌        | 31/200 [01:03<05:27,  1.94s/it]Running Inference:  16%|█▌        | 32/200 [01:05<05:22,  1.92s/it]Running Inference:  16%|█▋        | 33/200 [01:08<06:17,  2.26s/it]Running Inference:  17%|█▋        | 34/200 [01:10<05:52,  2.12s/it]Running Inference:  18%|█▊        | 35/200 [01:11<05:06,  1.86s/it]Running Inference:  18%|█▊        | 36/200 [01:14<06:05,  2.23s/it]Running Inference:  18%|█▊        | 37/200 [01:16<05:12,  1.91s/it]Running Inference:  19%|█▉        | 38/200 [01:18<05:45,  2.14s/it]Running Inference:  20%|█▉        | 39/200 [01:19<04:28,  1.67s/it]Running Inference:  20%|██        | 40/200 [01:22<05:40,  2.13s/it]Running Inference:  20%|██        | 41/200 [01:23<05:04,  1.92s/it]Running Inference:  21%|██        | 42/200 [01:24<04:02,  1.53s/it]Running Inference:  22%|██▏       | 43/200 [01:24<03:08,  1.20s/it]Running Inference:  22%|██▏       | 44/200 [01:26<03:32,  1.36s/it]Running Inference:  22%|██▎       | 45/200 [01:27<03:03,  1.18s/it]Running Inference:  23%|██▎       | 46/200 [01:30<04:24,  1.72s/it]Running Inference:  24%|██▎       | 47/200 [01:31<03:51,  1.51s/it]Running Inference:  24%|██▍       | 48/200 [01:34<05:02,  1.99s/it]Running Inference:  24%|██▍       | 49/200 [01:36<05:03,  2.01s/it]Running Inference:  25%|██▌       | 50/200 [01:39<05:35,  2.24s/it]Running Inference:  26%|██▌       | 51/200 [01:40<04:26,  1.79s/it]Running Inference:  26%|██▌       | 52/200 [01:42<04:43,  1.91s/it]Running Inference:  26%|██▋       | 53/200 [01:42<03:36,  1.47s/it]Running Inference:  27%|██▋       | 54/200 [01:43<02:57,  1.22s/it]Running Inference:  28%|██▊       | 55/200 [01:44<02:52,  1.19s/it]Running Inference:  28%|██▊       | 56/200 [01:46<03:08,  1.31s/it]Running Inference:  28%|██▊       | 57/200 [01:50<05:32,  2.32s/it]Running Inference:  29%|██▉       | 58/200 [01:52<05:01,  2.12s/it]Running Inference:  30%|██▉       | 59/200 [01:55<05:16,  2.24s/it]Running Inference:  30%|███       | 60/200 [01:58<06:11,  2.65s/it]Running Inference:  30%|███       | 61/200 [02:03<07:37,  3.29s/it]Running Inference:  31%|███       | 62/200 [02:05<06:26,  2.80s/it]Running Inference:  32%|███▏      | 63/200 [02:07<06:10,  2.71s/it]Running Inference:  32%|███▏      | 64/200 [02:08<04:54,  2.16s/it]Running Inference:  32%|███▎      | 65/200 [02:10<04:58,  2.21s/it]Running Inference:  33%|███▎      | 66/200 [02:12<04:52,  2.18s/it]Running Inference:  34%|███▎      | 67/200 [02:14<04:40,  2.11s/it]Running Inference:  34%|███▍      | 68/200 [02:15<03:50,  1.75s/it]Running Inference:  34%|███▍      | 69/200 [02:19<04:59,  2.28s/it]Running Inference:  35%|███▌      | 70/200 [02:23<06:15,  2.89s/it]Running Inference:  36%|███▌      | 71/200 [02:26<06:10,  2.88s/it]Running Inference:  36%|███▌      | 72/200 [02:30<06:44,  3.16s/it]Running Inference:  36%|███▋      | 73/200 [02:31<05:37,  2.66s/it]Running Inference:  37%|███▋      | 74/200 [02:36<06:55,  3.30s/it]Running Inference:  38%|███▊      | 75/200 [02:38<06:19,  3.03s/it]Running Inference:  38%|███▊      | 76/200 [02:40<05:32,  2.68s/it]Running Inference:  38%|███▊      | 77/200 [02:43<05:42,  2.79s/it]Running Inference:  39%|███▉      | 78/200 [02:44<04:10,  2.05s/it]Running Inference:  40%|███▉      | 79/200 [02:46<04:06,  2.04s/it]Running Inference:  40%|████      | 80/200 [02:48<04:07,  2.06s/it]Running Inference:  40%|████      | 81/200 [02:51<04:32,  2.29s/it]Running Inference:  41%|████      | 82/200 [02:51<03:25,  1.74s/it]Running Inference:  42%|████▏     | 83/200 [02:55<04:26,  2.28s/it]Running Inference:  42%|████▏     | 84/200 [02:58<04:48,  2.49s/it]Running Inference:  42%|████▎     | 85/200 [02:59<04:12,  2.19s/it]Running Inference:  43%|████▎     | 86/200 [03:04<06:00,  3.16s/it]Running Inference:  44%|████▎     | 87/200 [03:07<05:33,  2.96s/it]Running Inference:  44%|████▍     | 88/200 [03:09<04:53,  2.62s/it]Running Inference:  44%|████▍     | 89/200 [03:10<04:00,  2.17s/it]Running Inference:  45%|████▌     | 90/200 [03:12<03:44,  2.04s/it]Running Inference:  46%|████▌     | 91/200 [03:15<04:16,  2.36s/it]Running Inference:  46%|████▌     | 92/200 [03:17<04:25,  2.46s/it]Running Inference:  46%|████▋     | 93/200 [03:20<04:37,  2.60s/it]Running Inference:  47%|████▋     | 94/200 [03:22<04:03,  2.29s/it]Running Inference:  48%|████▊     | 95/200 [03:26<05:10,  2.96s/it]Running Inference:  48%|████▊     | 96/200 [03:29<04:47,  2.76s/it]Running Inference:  48%|████▊     | 97/200 [03:30<04:01,  2.35s/it]Running Inference:  49%|████▉     | 98/200 [03:33<04:00,  2.36s/it]Running Inference:  50%|████▉     | 99/200 [03:36<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [03:39<04:42,  2.82s/it]Running Inference:  50%|█████     | 101/200 [03:41<04:21,  2.64s/it]Running Inference:  51%|█████     | 102/200 [03:44<04:09,  2.54s/it]Running Inference:  52%|█████▏    | 103/200 [03:46<03:57,  2.44s/it]Running Inference:  52%|█████▏    | 104/200 [03:47<03:12,  2.00s/it]Running Inference:  52%|█████▎    | 105/200 [03:49<03:20,  2.11s/it]Running Inference:  53%|█████▎    | 106/200 [03:52<03:45,  2.40s/it]Running Inference:  54%|█████▎    | 107/200 [03:54<03:37,  2.34s/it]Running Inference:  54%|█████▍    | 108/200 [03:58<03:55,  2.56s/it]Running Inference:  55%|█████▍    | 109/200 [03:58<03:07,  2.06s/it]Running Inference:  55%|█████▌    | 110/200 [03:59<02:24,  1.61s/it]Running Inference:  56%|█████▌    | 111/200 [04:00<02:13,  1.50s/it]Running Inference:  56%|█████▌    | 112/200 [04:02<02:15,  1.53s/it]Running Inference:  56%|█████▋    | 113/200 [04:03<02:01,  1.39s/it]Running Inference:  57%|█████▋    | 114/200 [04:06<02:32,  1.78s/it]Running Inference:  57%|█████▊    | 115/200 [04:07<02:30,  1.77s/it]Running Inference:  58%|█████▊    | 116/200 [04:08<01:54,  1.36s/it]Running Inference:  58%|█████▊    | 117/200 [04:11<02:47,  2.01s/it]Running Inference:  59%|█████▉    | 118/200 [04:12<02:15,  1.66s/it]Running Inference:  60%|█████▉    | 119/200 [04:14<02:09,  1.60s/it]Running Inference:  60%|██████    | 120/200 [04:17<02:52,  2.16s/it]Running Inference:  60%|██████    | 121/200 [04:17<02:07,  1.61s/it]Running Inference:  61%|██████    | 122/200 [04:19<02:00,  1.55s/it]Running Inference:  62%|██████▏   | 123/200 [04:19<01:33,  1.21s/it]Running Inference:  62%|██████▏   | 124/200 [04:20<01:20,  1.06s/it]Running Inference:  62%|██████▎   | 125/200 [04:22<01:43,  1.37s/it]Running Inference:  63%|██████▎   | 126/200 [04:23<01:24,  1.14s/it]Running Inference:  64%|██████▎   | 127/200 [04:25<01:50,  1.52s/it]Running Inference:  64%|██████▍   | 128/200 [04:26<01:32,  1.29s/it]Running Inference:  64%|██████▍   | 129/200 [04:27<01:25,  1.21s/it]Running Inference:  65%|██████▌   | 130/200 [04:27<01:14,  1.06s/it]Running Inference:  66%|██████▌   | 131/200 [04:29<01:18,  1.14s/it]Running Inference:  66%|██████▌   | 132/200 [04:30<01:27,  1.29s/it]Running Inference:  66%|██████▋   | 133/200 [04:31<01:17,  1.16s/it]Running Inference:  67%|██████▋   | 134/200 [04:35<02:12,  2.01s/it]Running Inference:  68%|██████▊   | 135/200 [04:38<02:33,  2.37s/it]Running Inference:  68%|██████▊   | 136/200 [04:44<03:23,  3.18s/it]Running Inference:  68%|██████▊   | 137/200 [04:45<02:46,  2.65s/it]Running Inference:  69%|██████▉   | 138/200 [04:50<03:19,  3.21s/it]Running Inference:  70%|██████▉   | 139/200 [04:52<03:04,  3.02s/it]Running Inference:  70%|███████   | 140/200 [04:54<02:46,  2.78s/it]Running Inference:  70%|███████   | 141/200 [04:57<02:39,  2.70s/it]Running Inference:  71%|███████   | 142/200 [05:00<02:37,  2.72s/it]Running Inference:  72%|███████▏  | 143/200 [05:02<02:34,  2.71s/it]Running Inference:  72%|███████▏  | 144/200 [05:04<02:10,  2.34s/it]Running Inference:  72%|███████▎  | 145/200 [05:07<02:25,  2.65s/it]Running Inference:  73%|███████▎  | 146/200 [05:10<02:20,  2.60s/it]Running Inference:  74%|███████▎  | 147/200 [05:10<01:45,  1.98s/it]Running Inference:  74%|███████▍  | 148/200 [05:14<02:09,  2.48s/it]Running Inference:  74%|███████▍  | 149/200 [05:17<02:18,  2.72s/it]Running Inference:  75%|███████▌  | 150/200 [05:18<01:48,  2.18s/it]Running Inference:  76%|███████▌  | 151/200 [05:20<01:39,  2.04s/it]Running Inference:  76%|███████▌  | 152/200 [05:21<01:20,  1.67s/it]Running Inference:  76%|███████▋  | 153/200 [05:23<01:31,  1.94s/it]Running Inference:  77%|███████▋  | 154/200 [05:25<01:34,  2.05s/it]Running Inference:  78%|███████▊  | 155/200 [05:27<01:28,  1.96s/it]Running Inference:  78%|███████▊  | 156/200 [05:30<01:33,  2.13s/it]Running Inference:  78%|███████▊  | 157/200 [05:33<01:43,  2.40s/it]Running Inference:  79%|███████▉  | 158/200 [05:35<01:33,  2.22s/it]Running Inference:  80%|███████▉  | 159/200 [05:36<01:20,  1.97s/it]Running Inference:  80%|████████  | 160/200 [05:37<01:07,  1.69s/it]Running Inference:  80%|████████  | 161/200 [05:39<01:06,  1.71s/it]Running Inference:  81%|████████  | 162/200 [05:40<01:05,  1.74s/it]Running Inference:  82%|████████▏ | 163/200 [05:43<01:08,  1.85s/it]Running Inference:  82%|████████▏ | 164/200 [05:45<01:15,  2.11s/it]Running Inference:  82%|████████▎ | 165/200 [05:47<01:10,  2.01s/it]Running Inference:  83%|████████▎ | 166/200 [05:49<01:06,  1.97s/it]Running Inference:  84%|████████▎ | 167/200 [05:50<00:58,  1.76s/it]Running Inference:  84%|████████▍ | 168/200 [05:55<01:21,  2.55s/it]Running Inference:  84%|████████▍ | 169/200 [05:56<01:05,  2.10s/it]Running Inference:  85%|████████▌ | 170/200 [05:56<00:50,  1.70s/it]Running Inference:  86%|████████▌ | 171/200 [05:57<00:42,  1.45s/it]Running Inference:  86%|████████▌ | 172/200 [05:59<00:43,  1.55s/it]Running Inference:  86%|████████▋ | 173/200 [06:01<00:43,  1.62s/it]Running Inference:  87%|████████▋ | 174/200 [06:03<00:44,  1.71s/it]Running Inference:  88%|████████▊ | 175/200 [06:04<00:37,  1.49s/it]Running Inference:  88%|████████▊ | 176/200 [06:05<00:33,  1.39s/it]Running Inference:  88%|████████▊ | 177/200 [06:07<00:35,  1.54s/it]Running Inference:  89%|████████▉ | 178/200 [06:10<00:41,  1.89s/it]Running Inference:  90%|████████▉ | 179/200 [06:10<00:30,  1.48s/it]Running Inference:  90%|█████████ | 180/200 [06:11<00:28,  1.41s/it]Running Inference:  90%|█████████ | 181/200 [06:14<00:34,  1.81s/it]Running Inference:  91%|█████████ | 182/200 [06:15<00:27,  1.52s/it]Running Inference:  92%|█████████▏| 183/200 [06:20<00:41,  2.46s/it]Running Inference:  92%|█████████▏| 184/200 [06:22<00:41,  2.60s/it]Running Inference:  92%|█████████▎| 185/200 [06:23<00:31,  2.09s/it]Running Inference:  93%|█████████▎| 186/200 [06:25<00:27,  1.94s/it]Running Inference:  94%|█████████▎| 187/200 [06:28<00:31,  2.42s/it]Running Inference:  94%|█████████▍| 188/200 [06:31<00:28,  2.38s/it]Running Inference:  94%|█████████▍| 189/200 [06:31<00:19,  1.81s/it]Running Inference:  95%|█████████▌| 190/200 [06:33<00:17,  1.80s/it]Running Inference:  96%|█████████▌| 191/200 [06:35<00:15,  1.74s/it]Running Inference:  96%|█████████▌| 192/200 [06:37<00:14,  1.87s/it]Running Inference:  96%|█████████▋| 193/200 [06:38<00:12,  1.78s/it]Running Inference:  97%|█████████▋| 194/200 [06:41<00:12,  2.01s/it]Running Inference:  98%|█████████▊| 195/200 [06:44<00:10,  2.19s/it]Running Inference:  98%|█████████▊| 196/200 [06:44<00:06,  1.72s/it]Running Inference:  98%|█████████▊| 197/200 [06:46<00:05,  1.68s/it]Running Inference:  99%|█████████▉| 198/200 [06:47<00:03,  1.67s/it]Running Inference: 100%|█████████▉| 199/200 [06:48<00:01,  1.34s/it]Running Inference: 100%|██████████| 200/200 [06:49<00:00,  1.25s/it]Running Inference: 100%|██████████| 200/200 [06:49<00:00,  2.05s/it]
2025-12-13 21:33:11,256 - INFO - Inference completed.
2025-12-13 21:33:11,279 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-13 21:33:11,279 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:33:11,318 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-13 21:33:11,318 - INFO - Metrics:
43.92
2025-12-13 21:33:11,320 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=triviaqa, ratio=0.5) on GPU 4


========================================
LongBench Task: gov_report
========================================
----------------------------------------
Task: gov_report | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:33:17,717 - INFO - Set deterministic seeds to 42
2025-12-13 21:33:17,717 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:33:17,717 - INFO - Starting evaluation run...
2025-12-13 21:33:17,717 - INFO - Output directory set to: longbenchresult
2025-12-13 21:33:17,717 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-13 21:33:17,717 - INFO - KV Press 'knorm' setup.
2025-12-13 21:33:17,718 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:33:17,718 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.68it/s]
Device set to use cuda:0
2025-12-13 21:33:53,139 - INFO - Model pipeline loaded.
2025-12-13 21:33:53,139 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-13 21:34:03,719 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:34:03,720 - INFO - Dataset processed with 200 entries.
2025-12-13 21:34:03,753 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:27, 24.26s/it]Running Inference:   1%|          | 2/200 [00:47<1:18:33, 23.81s/it]Running Inference:   2%|▏         | 3/200 [00:56<55:53, 17.02s/it]  Running Inference:   2%|▏         | 4/200 [01:19<1:03:20, 19.39s/it]Running Inference:   2%|▎         | 5/200 [01:29<51:43, 15.92s/it]  Running Inference:   3%|▎         | 6/200 [01:54<1:01:38, 19.07s/it]Running Inference:   4%|▎         | 7/200 [02:18<1:06:21, 20.63s/it]Running Inference:   4%|▍         | 8/200 [02:41<1:08:56, 21.54s/it]Running Inference:   4%|▍         | 9/200 [03:05<1:10:38, 22.19s/it]Running Inference:   5%|▌         | 10/200 [03:29<1:11:42, 22.64s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:52<1:12:01, 22.87s/it]Running Inference:   6%|▌         | 12/200 [04:16<1:12:08, 23.02s/it]Running Inference:   6%|▋         | 13/200 [04:40<1:12:52, 23.38s/it]Running Inference:   7%|▋         | 14/200 [05:03<1:12:46, 23.48s/it]Running Inference:   8%|▊         | 15/200 [05:27<1:12:32, 23.53s/it]Running Inference:   8%|▊         | 16/200 [05:51<1:12:33, 23.66s/it]Running Inference:   8%|▊         | 17/200 [06:04<1:02:20, 20.44s/it]Running Inference:   9%|▉         | 18/200 [06:28<1:05:18, 21.53s/it]Running Inference:  10%|▉         | 19/200 [06:55<1:09:23, 23.01s/it]Running Inference:  10%|█         | 20/200 [07:18<1:09:26, 23.15s/it]Running Inference:  10%|█         | 21/200 [07:42<1:09:39, 23.35s/it]Running Inference:  11%|█         | 22/200 [08:06<1:09:58, 23.59s/it]Running Inference:  12%|█▏        | 23/200 [08:30<1:10:05, 23.76s/it]Running Inference:  12%|█▏        | 24/200 [08:56<1:11:44, 24.46s/it]Running Inference:  12%|█▎        | 25/200 [09:19<1:10:08, 24.05s/it]Running Inference:  13%|█▎        | 26/200 [09:43<1:09:19, 23.91s/it]Running Inference:  14%|█▎        | 27/200 [10:06<1:08:31, 23.77s/it]Running Inference:  14%|█▍        | 28/200 [10:29<1:07:33, 23.57s/it]Running Inference:  14%|█▍        | 29/200 [10:56<1:09:57, 24.55s/it]Running Inference:  15%|█▌        | 30/200 [11:20<1:09:05, 24.39s/it]Running Inference:  16%|█▌        | 31/200 [11:44<1:08:03, 24.16s/it]Running Inference:  16%|█▌        | 32/200 [12:08<1:07:58, 24.28s/it]Running Inference:  16%|█▋        | 33/200 [12:31<1:06:23, 23.86s/it]Running Inference:  17%|█▋        | 34/200 [12:56<1:06:19, 23.98s/it]Running Inference:  18%|█▊        | 35/200 [13:19<1:05:39, 23.87s/it]Running Inference:  18%|█▊        | 36/200 [13:44<1:05:48, 24.07s/it]Running Inference:  18%|█▊        | 37/200 [14:07<1:04:52, 23.88s/it]Running Inference:  19%|█▉        | 38/200 [14:31<1:04:36, 23.93s/it]Running Inference:  20%|█▉        | 39/200 [14:55<1:03:53, 23.81s/it]Running Inference:  20%|██        | 40/200 [15:19<1:03:31, 23.82s/it]Running Inference:  20%|██        | 41/200 [15:43<1:03:14, 23.87s/it]Running Inference:  21%|██        | 42/200 [16:06<1:02:53, 23.88s/it]Running Inference:  22%|██▏       | 43/200 [16:31<1:02:38, 23.94s/it]Running Inference:  22%|██▏       | 44/200 [16:54<1:01:36, 23.69s/it]Running Inference:  22%|██▎       | 45/200 [17:17<1:00:49, 23.54s/it]Running Inference:  23%|██▎       | 46/200 [17:40<1:00:16, 23.48s/it]Running Inference:  24%|██▎       | 47/200 [18:05<1:01:09, 23.99s/it]Running Inference:  24%|██▍       | 48/200 [18:29<1:00:25, 23.85s/it]Running Inference:  24%|██▍       | 49/200 [18:52<59:44, 23.74s/it]  Running Inference:  25%|██▌       | 50/200 [19:16<59:06, 23.65s/it]Running Inference:  26%|██▌       | 51/200 [19:35<55:33, 22.37s/it]Running Inference:  26%|██▌       | 52/200 [19:59<56:12, 22.79s/it]Running Inference:  26%|██▋       | 53/200 [20:27<59:31, 24.30s/it]Running Inference:  27%|██▋       | 54/200 [20:50<58:07, 23.89s/it]Running Inference:  28%|██▊       | 55/200 [21:13<57:19, 23.72s/it]Running Inference:  28%|██▊       | 56/200 [21:37<56:56, 23.73s/it]Running Inference:  28%|██▊       | 57/200 [22:00<56:18, 23.63s/it]Running Inference:  29%|██▉       | 58/200 [22:24<55:53, 23.62s/it]Running Inference:  30%|██▉       | 59/200 [22:47<55:31, 23.63s/it]Running Inference:  30%|███       | 60/200 [23:13<56:25, 24.19s/it]Running Inference:  30%|███       | 61/200 [23:37<55:37, 24.01s/it]Running Inference:  31%|███       | 62/200 [24:00<54:46, 23.82s/it]Running Inference:  32%|███▏      | 63/200 [24:23<53:46, 23.55s/it]Running Inference:  32%|███▏      | 64/200 [24:46<53:09, 23.45s/it]Running Inference:  32%|███▎      | 65/200 [25:09<52:45, 23.45s/it]Running Inference:  33%|███▎      | 66/200 [25:33<52:32, 23.52s/it]Running Inference:  34%|███▎      | 67/200 [25:57<52:30, 23.69s/it]Running Inference:  34%|███▍      | 68/200 [26:21<51:59, 23.63s/it]Running Inference:  34%|███▍      | 69/200 [26:44<51:28, 23.58s/it]Running Inference:  35%|███▌      | 70/200 [27:09<51:35, 23.82s/it]Running Inference:  36%|███▌      | 71/200 [27:32<51:17, 23.85s/it]Running Inference:  36%|███▌      | 72/200 [27:56<50:52, 23.85s/it]Running Inference:  36%|███▋      | 73/200 [28:21<51:11, 24.19s/it]Running Inference:  37%|███▋      | 74/200 [28:45<50:17, 23.95s/it]Running Inference:  38%|███▊      | 75/200 [29:08<49:37, 23.82s/it]Running Inference:  38%|███▊      | 76/200 [29:32<49:29, 23.95s/it]Running Inference:  38%|███▊      | 77/200 [29:56<49:06, 23.96s/it]Running Inference:  39%|███▉      | 78/200 [30:20<48:15, 23.74s/it]Running Inference:  40%|███▉      | 79/200 [30:43<47:41, 23.65s/it]Running Inference:  40%|████      | 80/200 [31:07<47:41, 23.84s/it]Running Inference:  40%|████      | 81/200 [31:31<47:14, 23.82s/it]Running Inference:  41%|████      | 82/200 [31:55<46:56, 23.87s/it]Running Inference:  42%|████▏     | 83/200 [32:18<46:00, 23.60s/it]Running Inference:  42%|████▏     | 84/200 [32:42<45:34, 23.58s/it]Running Inference:  42%|████▎     | 85/200 [33:06<45:27, 23.72s/it]Running Inference:  43%|████▎     | 86/200 [33:29<44:51, 23.61s/it]Running Inference:  44%|████▎     | 87/200 [33:53<44:24, 23.58s/it]Running Inference:  44%|████▍     | 88/200 [34:19<45:33, 24.41s/it]Running Inference:  44%|████▍     | 89/200 [34:42<44:34, 24.10s/it]Running Inference:  45%|████▌     | 90/200 [35:06<43:51, 23.93s/it]Running Inference:  46%|████▌     | 91/200 [35:30<43:24, 23.90s/it]Running Inference:  46%|████▌     | 92/200 [35:53<42:35, 23.66s/it]Running Inference:  46%|████▋     | 93/200 [36:16<42:13, 23.68s/it]Running Inference:  47%|████▋     | 94/200 [36:39<41:24, 23.44s/it]Running Inference:  48%|████▊     | 95/200 [37:04<41:37, 23.78s/it]Running Inference:  48%|████▊     | 96/200 [37:28<41:16, 23.81s/it]Running Inference:  48%|████▊     | 97/200 [37:51<40:27, 23.57s/it]Running Inference:  49%|████▉     | 98/200 [38:14<39:54, 23.48s/it]Running Inference:  50%|████▉     | 99/200 [38:37<39:24, 23.41s/it]Running Inference:  50%|█████     | 100/200 [39:02<39:23, 23.64s/it]Running Inference:  50%|█████     | 101/200 [39:25<38:42, 23.46s/it]Running Inference:  51%|█████     | 102/200 [39:49<38:39, 23.67s/it]Running Inference:  52%|█████▏    | 103/200 [40:12<37:58, 23.49s/it]Running Inference:  52%|█████▏    | 104/200 [40:36<37:44, 23.59s/it]Running Inference:  52%|█████▎    | 105/200 [40:59<37:11, 23.49s/it]Running Inference:  53%|█████▎    | 106/200 [41:22<36:36, 23.36s/it]Running Inference:  54%|█████▎    | 107/200 [41:46<36:42, 23.68s/it]Running Inference:  54%|█████▍    | 108/200 [42:09<35:59, 23.47s/it]Running Inference:  55%|█████▍    | 109/200 [42:32<35:24, 23.35s/it]Running Inference:  55%|█████▌    | 110/200 [42:56<35:09, 23.43s/it]Running Inference:  56%|█████▌    | 111/200 [43:04<27:56, 18.84s/it]Running Inference:  56%|█████▌    | 112/200 [43:27<29:31, 20.13s/it]Running Inference:  56%|█████▋    | 113/200 [43:51<30:44, 21.20s/it]Running Inference:  57%|█████▋    | 114/200 [44:15<31:34, 22.03s/it]Running Inference:  57%|█████▊    | 115/200 [44:43<33:47, 23.85s/it]Running Inference:  58%|█████▊    | 116/200 [45:06<33:04, 23.62s/it]Running Inference:  58%|█████▊    | 117/200 [45:29<32:24, 23.42s/it]Running Inference:  59%|█████▉    | 118/200 [45:52<31:50, 23.30s/it]Running Inference:  60%|█████▉    | 119/200 [46:18<32:20, 23.95s/it]Running Inference:  60%|██████    | 120/200 [46:41<31:38, 23.73s/it]Running Inference:  60%|██████    | 121/200 [47:05<31:20, 23.80s/it]Running Inference:  61%|██████    | 122/200 [47:28<30:38, 23.58s/it]Running Inference:  62%|██████▏   | 123/200 [47:51<30:08, 23.48s/it]Running Inference:  62%|██████▏   | 124/200 [48:15<29:46, 23.51s/it]Running Inference:  62%|██████▎   | 125/200 [48:38<29:28, 23.58s/it]Running Inference:  63%|██████▎   | 126/200 [49:01<28:52, 23.41s/it]Running Inference:  64%|██████▎   | 127/200 [49:15<24:50, 20.42s/it]Running Inference:  64%|██████▍   | 128/200 [49:38<25:22, 21.15s/it]Running Inference:  64%|██████▍   | 129/200 [50:01<25:41, 21.71s/it]Running Inference:  65%|██████▌   | 130/200 [50:24<25:51, 22.16s/it]Running Inference:  66%|██████▌   | 131/200 [50:47<25:47, 22.43s/it]Running Inference:  66%|██████▌   | 132/200 [51:11<26:07, 23.05s/it]Running Inference:  66%|██████▋   | 133/200 [51:35<25:44, 23.06s/it]Running Inference:  67%|██████▋   | 134/200 [51:58<25:38, 23.31s/it]Running Inference:  68%|██████▊   | 135/200 [52:22<25:11, 23.26s/it]Running Inference:  68%|██████▊   | 136/200 [52:47<25:24, 23.83s/it]Running Inference:  68%|██████▊   | 137/200 [53:12<25:18, 24.11s/it]Running Inference:  69%|██████▉   | 138/200 [53:38<25:31, 24.69s/it]Running Inference:  70%|██████▉   | 139/200 [54:02<24:52, 24.47s/it]Running Inference:  70%|███████   | 140/200 [54:25<24:08, 24.15s/it]Running Inference:  70%|███████   | 141/200 [54:48<23:28, 23.88s/it]Running Inference:  71%|███████   | 142/200 [55:12<23:03, 23.85s/it]Running Inference:  72%|███████▏  | 143/200 [55:36<22:42, 23.91s/it]Running Inference:  72%|███████▏  | 144/200 [55:59<22:07, 23.71s/it]Running Inference:  72%|███████▎  | 145/200 [56:24<22:03, 24.07s/it]Running Inference:  73%|███████▎  | 146/200 [56:47<21:27, 23.83s/it]Running Inference:  74%|███████▎  | 147/200 [57:15<22:06, 25.04s/it]Running Inference:  74%|███████▍  | 148/200 [57:39<21:22, 24.66s/it]Running Inference:  74%|███████▍  | 149/200 [58:03<20:50, 24.52s/it]Running Inference:  75%|███████▌  | 150/200 [58:27<20:16, 24.34s/it]Running Inference:  76%|███████▌  | 151/200 [58:51<19:44, 24.18s/it]Running Inference:  76%|███████▌  | 152/200 [59:14<19:02, 23.80s/it]Running Inference:  76%|███████▋  | 153/200 [59:38<18:37, 23.77s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:01<18:06, 23.63s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:25<17:45, 23.68s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:49<17:27, 23.80s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:12<16:53, 23.56s/it]Running Inference:  79%|███████▉  | 158/200 [1:01:37<16:44, 23.91s/it]Running Inference:  80%|███████▉  | 159/200 [1:01:45<13:15, 19.41s/it]Running Inference:  80%|████████  | 160/200 [1:02:10<13:55, 20.89s/it]Running Inference:  80%|████████  | 161/200 [1:02:34<14:16, 21.95s/it]Running Inference:  81%|████████  | 162/200 [1:02:57<14:06, 22.28s/it]Running Inference:  82%|████████▏ | 163/200 [1:03:20<13:54, 22.55s/it]Running Inference:  82%|████████▏ | 164/200 [1:03:46<14:00, 23.35s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:09<13:32, 23.22s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:31<13:05, 23.12s/it]Running Inference:  84%|████████▎ | 167/200 [1:04:56<12:54, 23.46s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:19<12:29, 23.42s/it]Running Inference:  84%|████████▍ | 169/200 [1:05:42<12:02, 23.32s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:06<11:41, 23.38s/it]Running Inference:  86%|████████▌ | 171/200 [1:06:31<11:36, 24.02s/it]Running Inference:  86%|████████▌ | 172/200 [1:06:54<11:05, 23.77s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:18<10:42, 23.80s/it]Running Inference:  87%|████████▋ | 174/200 [1:07:42<10:17, 23.73s/it]Running Inference:  88%|████████▊ | 175/200 [1:08:05<09:48, 23.55s/it]Running Inference:  88%|████████▊ | 176/200 [1:08:42<11:01, 27.55s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:16<11:20, 29.59s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:40<10:15, 27.99s/it]Running Inference:  90%|████████▉ | 179/200 [1:10:03<09:16, 26.51s/it]Running Inference:  90%|█████████ | 180/200 [1:10:28<08:38, 25.90s/it]Running Inference:  90%|█████████ | 181/200 [1:10:52<08:00, 25.27s/it]Running Inference:  91%|█████████ | 182/200 [1:10:56<05:41, 18.98s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:29<06:33, 23.12s/it]Running Inference:  92%|█████████▏| 184/200 [1:11:52<06:09, 23.10s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:16<05:51, 23.41s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:39<05:27, 23.36s/it]Running Inference:  94%|█████████▎| 187/200 [1:13:03<05:03, 23.34s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:27<04:42, 23.58s/it]Running Inference:  94%|█████████▍| 189/200 [1:13:50<04:19, 23.55s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:13<03:54, 23.47s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:37<03:31, 23.55s/it]Running Inference:  96%|█████████▌| 192/200 [1:15:05<03:19, 24.88s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:29<02:50, 24.42s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:53<02:26, 24.35s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:16<01:59, 23.89s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:39<01:35, 23.76s/it]Running Inference:  98%|█████████▊| 197/200 [1:17:03<01:11, 23.92s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:30<00:49, 24.64s/it]Running Inference: 100%|█████████▉| 199/200 [1:17:52<00:24, 24.10s/it]Running Inference: 100%|██████████| 200/200 [1:18:16<00:00, 23.88s/it]Running Inference: 100%|██████████| 200/200 [1:18:16<00:00, 23.48s/it]
2025-12-13 22:52:20,074 - INFO - Inference completed.
2025-12-13 22:52:20,109 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-13 22:52:20,109 - INFO - Calculating metrics for dataset: longbench
2025-12-13 22:52:40,545 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-13 22:52:40,545 - INFO - Metrics:
10.47
2025-12-13 22:52:40,547 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=gov_report, ratio=0.1) on GPU 4

----------------------------------------
Task: gov_report | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 22:52:47,745 - INFO - Set deterministic seeds to 42
2025-12-13 22:52:47,745 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 22:52:47,745 - INFO - Starting evaluation run...
2025-12-13 22:52:47,745 - INFO - Output directory set to: longbenchresult
2025-12-13 22:52:47,745 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-13 22:52:47,745 - INFO - KV Press 'knorm' setup.
2025-12-13 22:52:47,745 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 22:52:47,745 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.60it/s]
Device set to use cuda:0
2025-12-13 22:52:59,379 - INFO - Model pipeline loaded.
2025-12-13 22:52:59,380 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-13 22:53:06,879 - INFO - Dataset loaded with 200 entries.
2025-12-13 22:53:06,879 - INFO - Dataset processed with 200 entries.
2025-12-13 22:53:06,910 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:22, 23.93s/it]Running Inference:   1%|          | 2/200 [00:47<1:17:33, 23.50s/it]Running Inference:   2%|▏         | 3/200 [00:54<53:07, 16.18s/it]  Running Inference:   2%|▏         | 4/200 [01:17<1:01:08, 18.72s/it]Running Inference:   2%|▎         | 5/200 [01:40<1:06:03, 20.32s/it]Running Inference:   3%|▎         | 6/200 [02:05<1:10:26, 21.79s/it]Running Inference:   4%|▎         | 7/200 [02:28<1:11:54, 22.36s/it]Running Inference:   4%|▍         | 8/200 [02:51<1:12:23, 22.62s/it]Running Inference:   4%|▍         | 9/200 [03:15<1:12:42, 22.84s/it]Running Inference:   5%|▌         | 10/200 [03:38<1:12:45, 22.97s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:01<1:12:25, 22.99s/it]Running Inference:   6%|▌         | 12/200 [04:24<1:12:04, 23.00s/it]Running Inference:   6%|▋         | 13/200 [04:48<1:12:33, 23.28s/it]Running Inference:   7%|▋         | 14/200 [05:11<1:12:17, 23.32s/it]Running Inference:   8%|▊         | 15/200 [05:35<1:11:55, 23.33s/it]Running Inference:   8%|▊         | 16/200 [05:58<1:11:48, 23.41s/it]Running Inference:   8%|▊         | 17/200 [06:22<1:12:09, 23.66s/it]Running Inference:   9%|▉         | 18/200 [06:46<1:11:47, 23.67s/it]Running Inference:  10%|▉         | 19/200 [07:12<1:13:24, 24.34s/it]Running Inference:  10%|█         | 20/200 [07:35<1:11:57, 23.98s/it]Running Inference:  10%|█         | 21/200 [07:59<1:11:07, 23.84s/it]Running Inference:  11%|█         | 22/200 [08:22<1:10:41, 23.83s/it]Running Inference:  12%|█▏        | 23/200 [08:46<1:10:20, 23.84s/it]Running Inference:  12%|█▏        | 24/200 [09:12<1:11:23, 24.34s/it]Running Inference:  12%|█▎        | 25/200 [09:35<1:09:39, 23.88s/it]Running Inference:  13%|█▎        | 26/200 [09:58<1:08:39, 23.68s/it]Running Inference:  14%|█▎        | 27/200 [10:21<1:07:47, 23.51s/it]Running Inference:  14%|█▍        | 28/200 [10:44<1:06:44, 23.28s/it]Running Inference:  14%|█▍        | 29/200 [11:10<1:08:46, 24.13s/it]Running Inference:  15%|█▌        | 30/200 [11:33<1:07:56, 23.98s/it]Running Inference:  16%|█▌        | 31/200 [11:57<1:07:00, 23.79s/it]Running Inference:  16%|█▌        | 32/200 [12:21<1:06:55, 23.90s/it]Running Inference:  16%|█▋        | 33/200 [12:44<1:05:26, 23.51s/it]Running Inference:  17%|█▋        | 34/200 [13:07<1:05:21, 23.62s/it]Running Inference:  18%|█▊        | 35/200 [13:31<1:04:43, 23.53s/it]Running Inference:  18%|█▊        | 36/200 [13:55<1:04:54, 23.75s/it]Running Inference:  18%|█▊        | 37/200 [14:18<1:03:58, 23.55s/it]Running Inference:  19%|█▉        | 38/200 [14:42<1:03:45, 23.61s/it]Running Inference:  20%|█▉        | 39/200 [15:05<1:03:04, 23.51s/it]Running Inference:  20%|██        | 40/200 [15:29<1:02:41, 23.51s/it]Running Inference:  20%|██        | 41/200 [15:52<1:02:26, 23.56s/it]Running Inference:  21%|██        | 42/200 [16:16<1:02:02, 23.56s/it]Running Inference:  22%|██▏       | 43/200 [16:40<1:01:51, 23.64s/it]Running Inference:  22%|██▏       | 44/200 [17:03<1:00:49, 23.40s/it]Running Inference:  22%|██▎       | 45/200 [17:25<1:00:02, 23.24s/it]Running Inference:  23%|██▎       | 46/200 [17:48<59:27, 23.17s/it]  Running Inference:  24%|██▎       | 47/200 [18:13<1:00:10, 23.60s/it]Running Inference:  24%|██▍       | 48/200 [18:36<59:29, 23.49s/it]  Running Inference:  24%|██▍       | 49/200 [18:59<58:53, 23.40s/it]Running Inference:  25%|██▌       | 50/200 [19:23<58:16, 23.31s/it]Running Inference:  26%|██▌       | 51/200 [19:45<57:28, 23.14s/it]Running Inference:  26%|██▌       | 52/200 [20:09<57:18, 23.23s/it]Running Inference:  26%|██▋       | 53/200 [20:36<59:45, 24.39s/it]Running Inference:  27%|██▋       | 54/200 [20:58<58:06, 23.88s/it]Running Inference:  28%|██▊       | 55/200 [21:22<57:04, 23.62s/it]Running Inference:  28%|██▊       | 56/200 [21:45<56:32, 23.56s/it]Running Inference:  28%|██▊       | 57/200 [22:08<55:50, 23.43s/it]Running Inference:  29%|██▉       | 58/200 [22:31<55:20, 23.39s/it]Running Inference:  30%|██▉       | 59/200 [22:55<54:55, 23.37s/it]Running Inference:  30%|███       | 60/200 [23:20<55:37, 23.84s/it]Running Inference:  30%|███       | 61/200 [23:43<54:52, 23.69s/it]Running Inference:  31%|███       | 62/200 [24:06<54:02, 23.50s/it]Running Inference:  32%|███▏      | 63/200 [24:29<53:04, 23.24s/it]Running Inference:  32%|███▏      | 64/200 [24:52<52:29, 23.16s/it]Running Inference:  32%|███▎      | 65/200 [25:15<52:04, 23.15s/it]Running Inference:  33%|███▎      | 66/200 [25:38<51:53, 23.23s/it]Running Inference:  34%|███▎      | 67/200 [26:02<51:49, 23.38s/it]Running Inference:  34%|███▍      | 68/200 [26:25<51:18, 23.32s/it]Running Inference:  34%|███▍      | 69/200 [26:48<50:50, 23.29s/it]Running Inference:  35%|███▌      | 70/200 [27:12<50:57, 23.52s/it]Running Inference:  36%|███▌      | 71/200 [27:22<41:29, 19.30s/it]Running Inference:  36%|███▌      | 72/200 [27:45<43:52, 20.57s/it]Running Inference:  36%|███▋      | 73/200 [28:10<46:02, 21.75s/it]Running Inference:  37%|███▋      | 74/200 [28:33<46:32, 22.16s/it]Running Inference:  38%|███▊      | 75/200 [28:56<46:51, 22.49s/it]Running Inference:  38%|███▊      | 76/200 [29:20<47:23, 22.93s/it]Running Inference:  38%|███▊      | 77/200 [29:44<47:27, 23.15s/it]Running Inference:  39%|███▉      | 78/200 [30:07<46:53, 23.06s/it]Running Inference:  40%|███▉      | 79/200 [30:30<46:32, 23.08s/it]Running Inference:  40%|████      | 80/200 [30:54<46:40, 23.34s/it]Running Inference:  40%|████      | 81/200 [31:17<46:19, 23.35s/it]Running Inference:  41%|████      | 82/200 [31:41<46:06, 23.44s/it]Running Inference:  42%|████▏     | 83/200 [32:03<45:13, 23.20s/it]Running Inference:  42%|████▏     | 84/200 [32:27<45:02, 23.29s/it]Running Inference:  42%|████▎     | 85/200 [32:51<44:54, 23.43s/it]Running Inference:  43%|████▎     | 86/200 [33:14<44:16, 23.30s/it]Running Inference:  44%|████▎     | 87/200 [33:37<43:49, 23.27s/it]Running Inference:  44%|████▍     | 88/200 [34:03<44:48, 24.01s/it]Running Inference:  44%|████▍     | 89/200 [34:26<43:51, 23.71s/it]Running Inference:  45%|████▌     | 90/200 [34:49<43:12, 23.56s/it]Running Inference:  46%|████▌     | 91/200 [35:12<42:48, 23.57s/it]Running Inference:  46%|████▌     | 92/200 [35:35<42:00, 23.34s/it]Running Inference:  46%|████▋     | 93/200 [35:59<41:38, 23.35s/it]Running Inference:  47%|████▋     | 94/200 [36:21<40:51, 23.12s/it]Running Inference:  48%|████▊     | 95/200 [36:45<41:04, 23.47s/it]Running Inference:  48%|████▊     | 96/200 [37:09<40:44, 23.51s/it]Running Inference:  48%|████▊     | 97/200 [37:32<39:58, 23.29s/it]Running Inference:  49%|████▉     | 98/200 [37:55<39:25, 23.19s/it]Running Inference:  50%|████▉     | 99/200 [38:18<38:57, 23.14s/it]Running Inference:  50%|█████     | 100/200 [38:42<38:54, 23.35s/it]Running Inference:  50%|█████     | 101/200 [39:04<38:15, 23.19s/it]Running Inference:  51%|█████     | 102/200 [39:28<38:12, 23.39s/it]Running Inference:  52%|█████▏    | 103/200 [39:51<37:30, 23.20s/it]Running Inference:  52%|█████▏    | 104/200 [40:15<37:17, 23.31s/it]Running Inference:  52%|█████▎    | 105/200 [40:38<36:45, 23.22s/it]Running Inference:  53%|█████▎    | 106/200 [41:00<36:09, 23.08s/it]Running Inference:  54%|█████▎    | 107/200 [41:25<36:16, 23.41s/it]Running Inference:  54%|█████▍    | 108/200 [41:47<35:33, 23.19s/it]Running Inference:  55%|█████▍    | 109/200 [42:10<35:00, 23.08s/it]Running Inference:  55%|█████▌    | 110/200 [42:34<34:48, 23.20s/it]Running Inference:  56%|█████▌    | 111/200 [42:58<34:50, 23.49s/it]Running Inference:  56%|█████▌    | 112/200 [43:21<34:09, 23.29s/it]Running Inference:  56%|█████▋    | 113/200 [43:44<33:50, 23.34s/it]Running Inference:  57%|█████▋    | 114/200 [43:51<26:26, 18.44s/it]Running Inference:  57%|█████▊    | 115/200 [44:19<29:57, 21.14s/it]Running Inference:  58%|█████▊    | 116/200 [44:41<30:18, 21.65s/it]Running Inference:  58%|█████▊    | 117/200 [45:04<30:23, 21.97s/it]Running Inference:  59%|█████▉    | 118/200 [45:27<30:21, 22.21s/it]Running Inference:  60%|█████▉    | 119/200 [45:52<31:05, 23.03s/it]Running Inference:  60%|██████    | 120/200 [46:14<30:29, 22.86s/it]Running Inference:  60%|██████    | 121/200 [46:38<30:26, 23.12s/it]Running Inference:  61%|██████    | 122/200 [47:01<29:56, 23.04s/it]Running Inference:  62%|██████▏   | 123/200 [47:24<29:33, 23.03s/it]Running Inference:  62%|██████▏   | 124/200 [47:47<29:16, 23.11s/it]Running Inference:  62%|██████▎   | 125/200 [48:11<29:02, 23.23s/it]Running Inference:  63%|██████▎   | 126/200 [48:33<28:29, 23.10s/it]Running Inference:  64%|██████▎   | 127/200 [48:57<28:08, 23.14s/it]Running Inference:  64%|██████▍   | 128/200 [49:19<27:34, 22.98s/it]Running Inference:  64%|██████▍   | 129/200 [49:42<27:07, 22.92s/it]Running Inference:  65%|██████▌   | 130/200 [50:05<26:45, 22.94s/it]Running Inference:  66%|██████▌   | 131/200 [50:28<26:20, 22.91s/it]Running Inference:  66%|██████▌   | 132/200 [50:52<26:23, 23.29s/it]Running Inference:  66%|██████▋   | 133/200 [51:15<25:51, 23.15s/it]Running Inference:  67%|██████▋   | 134/200 [51:38<25:37, 23.30s/it]Running Inference:  68%|██████▊   | 135/200 [52:01<25:06, 23.17s/it]Running Inference:  68%|██████▊   | 136/200 [52:26<25:12, 23.63s/it]Running Inference:  68%|██████▊   | 137/200 [52:50<25:02, 23.86s/it]Running Inference:  69%|██████▉   | 138/200 [53:16<25:08, 24.34s/it]Running Inference:  70%|██████▉   | 139/200 [53:40<24:32, 24.14s/it]Running Inference:  70%|███████   | 140/200 [54:03<23:49, 23.83s/it]Running Inference:  70%|███████   | 141/200 [54:26<23:12, 23.59s/it]Running Inference:  71%|███████   | 142/200 [54:49<22:46, 23.56s/it]Running Inference:  72%|███████▏  | 143/200 [55:13<22:26, 23.62s/it]Running Inference:  72%|███████▏  | 144/200 [55:36<21:52, 23.43s/it]Running Inference:  72%|███████▎  | 145/200 [56:00<21:44, 23.73s/it]Running Inference:  73%|███████▎  | 146/200 [56:23<21:09, 23.51s/it]Running Inference:  74%|███████▎  | 147/200 [56:51<21:44, 24.61s/it]Running Inference:  74%|███████▍  | 148/200 [57:14<21:02, 24.29s/it]Running Inference:  74%|███████▍  | 149/200 [57:38<20:33, 24.19s/it]Running Inference:  75%|███████▌  | 150/200 [58:02<20:00, 24.02s/it]Running Inference:  76%|███████▌  | 151/200 [58:25<19:29, 23.87s/it]Running Inference:  76%|███████▌  | 152/200 [58:48<18:48, 23.51s/it]Running Inference:  76%|███████▋  | 153/200 [59:11<18:23, 23.47s/it]Running Inference:  77%|███████▋  | 154/200 [59:34<17:53, 23.33s/it]Running Inference:  78%|███████▊  | 155/200 [59:58<17:32, 23.39s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:22<17:15, 23.52s/it]Running Inference:  78%|███████▊  | 157/200 [1:00:44<16:41, 23.28s/it]Running Inference:  79%|███████▉  | 158/200 [1:01:09<16:31, 23.60s/it]Running Inference:  80%|███████▉  | 159/200 [1:01:33<16:22, 23.95s/it]Running Inference:  80%|████████  | 160/200 [1:01:58<16:00, 24.00s/it]Running Inference:  80%|████████  | 161/200 [1:02:22<15:38, 24.06s/it]Running Inference:  81%|████████  | 162/200 [1:02:45<14:59, 23.67s/it]Running Inference:  82%|████████▏ | 163/200 [1:03:08<14:27, 23.45s/it]Running Inference:  82%|████████▏ | 164/200 [1:03:32<14:18, 23.85s/it]Running Inference:  82%|████████▎ | 165/200 [1:03:55<13:41, 23.49s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:18<13:10, 23.24s/it]Running Inference:  84%|████████▎ | 167/200 [1:04:42<12:54, 23.48s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:05<12:27, 23.36s/it]Running Inference:  84%|████████▍ | 169/200 [1:05:28<11:59, 23.21s/it]Running Inference:  85%|████████▌ | 170/200 [1:05:51<11:36, 23.22s/it]Running Inference:  86%|████████▌ | 171/200 [1:06:16<11:28, 23.75s/it]Running Inference:  86%|████████▌ | 172/200 [1:06:39<10:58, 23.52s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:03<10:37, 23.61s/it]Running Inference:  87%|████████▋ | 174/200 [1:07:26<10:12, 23.56s/it]Running Inference:  88%|████████▊ | 175/200 [1:07:49<09:45, 23.40s/it]Running Inference:  88%|████████▊ | 176/200 [1:08:25<10:51, 27.17s/it]Running Inference:  88%|████████▊ | 177/200 [1:08:59<11:09, 29.10s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:23<10:07, 27.60s/it]Running Inference:  90%|████████▉ | 179/200 [1:09:46<09:10, 26.20s/it]Running Inference:  90%|█████████ | 180/200 [1:10:10<08:32, 25.61s/it]Running Inference:  90%|█████████ | 181/200 [1:10:33<07:54, 25.00s/it]Running Inference:  91%|█████████ | 182/200 [1:10:58<07:27, 24.89s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:30<07:39, 27.02s/it]Running Inference:  92%|█████████▏| 184/200 [1:11:53<06:52, 25.79s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:17<06:18, 25.22s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:40<05:44, 24.58s/it]Running Inference:  94%|█████████▎| 187/200 [1:13:03<05:13, 24.15s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:21<04:26, 22.24s/it]Running Inference:  94%|█████████▍| 189/200 [1:13:44<04:07, 22.54s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:07<03:47, 22.72s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:31<03:27, 23.00s/it]Running Inference:  96%|█████████▌| 192/200 [1:14:58<03:14, 24.33s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:22<02:48, 24.02s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:46<02:24, 24.04s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:08<01:58, 23.63s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:32<01:34, 23.54s/it]Running Inference:  98%|█████████▊| 197/200 [1:16:56<01:11, 23.74s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:22<00:48, 24.39s/it]Running Inference: 100%|█████████▉| 199/200 [1:17:45<00:23, 23.91s/it]Running Inference: 100%|██████████| 200/200 [1:18:00<00:00, 21.27s/it]Running Inference: 100%|██████████| 200/200 [1:18:00<00:00, 23.40s/it]
2025-12-14 00:11:07,207 - INFO - Inference completed.
2025-12-14 00:11:07,241 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 00:11:07,241 - INFO - Calculating metrics for dataset: longbench
2025-12-14 00:11:28,145 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 00:11:28,146 - INFO - Metrics:
9.24
2025-12-14 00:11:28,147 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=gov_report, ratio=0.2) on GPU 4

----------------------------------------
Task: gov_report | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 00:11:35,433 - INFO - Set deterministic seeds to 42
2025-12-14 00:11:35,433 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 00:11:35,433 - INFO - Starting evaluation run...
2025-12-14 00:11:35,433 - INFO - Output directory set to: longbenchresult
2025-12-14 00:11:35,434 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 00:11:35,434 - INFO - KV Press 'knorm' setup.
2025-12-14 00:11:35,434 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 00:11:35,434 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.99it/s]
Device set to use cuda:0
2025-12-14 00:11:50,071 - INFO - Model pipeline loaded.
2025-12-14 00:11:50,071 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-14 00:11:55,228 - INFO - Dataset loaded with 200 entries.
2025-12-14 00:11:55,228 - INFO - Dataset processed with 200 entries.
2025-12-14 00:11:55,259 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:33, 24.29s/it]Running Inference:   1%|          | 2/200 [00:47<1:18:38, 23.83s/it]Running Inference:   2%|▏         | 3/200 [01:12<1:19:54, 24.34s/it]Running Inference:   2%|▏         | 4/200 [01:35<1:17:38, 23.77s/it]Running Inference:   2%|▎         | 5/200 [01:59<1:16:51, 23.65s/it]Running Inference:   3%|▎         | 6/200 [02:23<1:17:44, 24.04s/it]Running Inference:   4%|▎         | 7/200 [02:47<1:17:02, 23.95s/it]Running Inference:   4%|▍         | 8/200 [03:11<1:16:10, 23.81s/it]Running Inference:   4%|▍         | 9/200 [03:34<1:15:31, 23.73s/it]Running Inference:   5%|▌         | 10/200 [03:58<1:14:58, 23.68s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:21<1:14:12, 23.56s/it]Running Inference:   6%|▌         | 12/200 [04:44<1:13:33, 23.48s/it]Running Inference:   6%|▋         | 13/200 [05:09<1:13:51, 23.70s/it]Running Inference:   7%|▋         | 14/200 [05:32<1:13:28, 23.70s/it]Running Inference:   8%|▊         | 15/200 [05:56<1:12:57, 23.66s/it]Running Inference:   8%|▊         | 16/200 [06:20<1:12:46, 23.73s/it]Running Inference:   8%|▊         | 17/200 [06:44<1:13:04, 23.96s/it]Running Inference:   9%|▉         | 18/200 [07:08<1:12:40, 23.96s/it]Running Inference:  10%|▉         | 19/200 [07:34<1:13:55, 24.51s/it]Running Inference:  10%|█         | 20/200 [07:57<1:12:32, 24.18s/it]Running Inference:  10%|█         | 21/200 [08:21<1:11:47, 24.06s/it]Running Inference:  11%|█         | 22/200 [08:45<1:11:22, 24.06s/it]Running Inference:  12%|█▏        | 23/200 [09:09<1:11:02, 24.08s/it]Running Inference:  12%|█▏        | 24/200 [09:35<1:11:50, 24.49s/it]Running Inference:  12%|█▎        | 25/200 [09:58<1:10:11, 24.07s/it]Running Inference:  13%|█▎        | 26/200 [10:21<1:09:15, 23.88s/it]Running Inference:  14%|█▎        | 27/200 [10:45<1:08:26, 23.74s/it]Running Inference:  14%|█▍        | 28/200 [11:08<1:07:23, 23.51s/it]Running Inference:  14%|█▍        | 29/200 [11:12<50:51, 17.84s/it]  Running Inference:  15%|█▌        | 30/200 [11:36<55:42, 19.66s/it]Running Inference:  16%|█▌        | 31/200 [12:00<58:41, 20.84s/it]Running Inference:  16%|█▌        | 32/200 [12:24<1:01:22, 21.92s/it]Running Inference:  16%|█▋        | 33/200 [12:47<1:01:47, 22.20s/it]Running Inference:  17%|█▋        | 34/200 [13:11<1:03:02, 22.79s/it]Running Inference:  18%|█▊        | 35/200 [13:35<1:03:19, 23.03s/it]Running Inference:  18%|█▊        | 36/200 [13:59<1:04:07, 23.46s/it]Running Inference:  18%|█▊        | 37/200 [14:23<1:03:36, 23.42s/it]Running Inference:  19%|█▉        | 38/200 [14:47<1:03:42, 23.60s/it]Running Inference:  20%|█▉        | 39/200 [15:10<1:03:14, 23.57s/it]Running Inference:  20%|██        | 40/200 [15:20<51:48, 19.43s/it]  Running Inference:  20%|██        | 41/200 [15:44<55:02, 20.77s/it]Running Inference:  21%|██        | 42/200 [16:08<57:07, 21.69s/it]Running Inference:  22%|██▏       | 43/200 [16:32<58:34, 22.39s/it]Running Inference:  22%|██▏       | 44/200 [16:46<52:01, 20.01s/it]Running Inference:  22%|██▎       | 45/200 [17:09<54:06, 20.95s/it]Running Inference:  23%|██▎       | 46/200 [17:33<55:32, 21.64s/it]Running Inference:  24%|██▎       | 47/200 [17:57<57:34, 22.58s/it]Running Inference:  24%|██▍       | 48/200 [18:21<57:52, 22.84s/it]Running Inference:  24%|██▍       | 49/200 [18:25<43:45, 17.39s/it]Running Inference:  25%|██▌       | 50/200 [18:49<47:56, 19.17s/it]Running Inference:  26%|██▌       | 51/200 [19:12<50:26, 20.31s/it]Running Inference:  26%|██▌       | 52/200 [19:35<52:35, 21.32s/it]Running Inference:  26%|██▋       | 53/200 [20:02<56:21, 23.00s/it]Running Inference:  27%|██▋       | 54/200 [20:25<55:55, 22.98s/it]Running Inference:  28%|██▊       | 55/200 [20:48<55:43, 23.06s/it]Running Inference:  28%|██▊       | 56/200 [21:12<55:46, 23.24s/it]Running Inference:  28%|██▊       | 57/200 [21:35<55:27, 23.27s/it]Running Inference:  29%|██▉       | 58/200 [21:59<55:17, 23.37s/it]Running Inference:  30%|██▉       | 59/200 [22:23<55:09, 23.47s/it]Running Inference:  30%|███       | 60/200 [22:48<55:49, 23.93s/it]Running Inference:  30%|███       | 61/200 [23:12<55:18, 23.87s/it]Running Inference:  31%|███       | 62/200 [23:35<54:32, 23.71s/it]Running Inference:  32%|███▏      | 63/200 [23:58<53:35, 23.47s/it]Running Inference:  32%|███▏      | 64/200 [24:21<53:01, 23.39s/it]Running Inference:  32%|███▎      | 65/200 [24:44<52:39, 23.40s/it]Running Inference:  33%|███▎      | 66/200 [25:08<52:28, 23.50s/it]Running Inference:  34%|███▎      | 67/200 [25:32<52:24, 23.64s/it]Running Inference:  34%|███▍      | 68/200 [25:56<51:52, 23.58s/it]Running Inference:  34%|███▍      | 69/200 [26:19<51:25, 23.55s/it]Running Inference:  35%|███▌      | 70/200 [26:43<51:32, 23.79s/it]Running Inference:  36%|███▌      | 71/200 [27:07<51:19, 23.87s/it]Running Inference:  36%|███▌      | 72/200 [27:31<50:53, 23.86s/it]Running Inference:  36%|███▋      | 73/200 [27:56<51:05, 24.14s/it]Running Inference:  37%|███▋      | 74/200 [28:19<50:14, 23.92s/it]Running Inference:  38%|███▊      | 75/200 [28:43<49:35, 23.81s/it]Running Inference:  38%|███▊      | 76/200 [29:07<49:30, 23.96s/it]Running Inference:  38%|███▊      | 77/200 [29:31<49:06, 23.96s/it]Running Inference:  39%|███▉      | 78/200 [29:54<48:11, 23.70s/it]Running Inference:  40%|███▉      | 79/200 [30:18<47:28, 23.54s/it]Running Inference:  40%|████      | 80/200 [30:42<47:25, 23.71s/it]Running Inference:  40%|████      | 81/200 [31:05<46:52, 23.63s/it]Running Inference:  41%|████      | 82/200 [31:29<46:29, 23.64s/it]Running Inference:  42%|████▏     | 83/200 [31:51<45:30, 23.34s/it]Running Inference:  42%|████▏     | 84/200 [32:15<45:15, 23.41s/it]Running Inference:  42%|████▎     | 85/200 [32:39<45:04, 23.52s/it]Running Inference:  43%|████▎     | 86/200 [33:02<44:24, 23.37s/it]Running Inference:  44%|████▎     | 87/200 [33:25<43:57, 23.34s/it]Running Inference:  44%|████▍     | 88/200 [33:50<44:43, 23.96s/it]Running Inference:  44%|████▍     | 89/200 [34:14<43:49, 23.69s/it]Running Inference:  45%|████▌     | 90/200 [34:37<43:11, 23.56s/it]Running Inference:  46%|████▌     | 91/200 [35:00<42:49, 23.58s/it]Running Inference:  46%|████▌     | 92/200 [35:23<42:02, 23.36s/it]Running Inference:  46%|████▋     | 93/200 [35:47<41:41, 23.38s/it]Running Inference:  47%|████▋     | 94/200 [36:09<40:53, 23.15s/it]Running Inference:  48%|████▊     | 95/200 [36:34<41:06, 23.49s/it]Running Inference:  48%|████▊     | 96/200 [36:57<40:47, 23.53s/it]Running Inference:  48%|████▊     | 97/200 [37:20<40:00, 23.31s/it]Running Inference:  49%|████▉     | 98/200 [37:43<39:27, 23.21s/it]Running Inference:  50%|████▉     | 99/200 [38:06<38:59, 23.16s/it]Running Inference:  50%|█████     | 100/200 [38:30<38:57, 23.37s/it]Running Inference:  50%|█████     | 101/200 [38:53<38:18, 23.22s/it]Running Inference:  51%|█████     | 102/200 [39:17<38:17, 23.44s/it]Running Inference:  52%|█████▏    | 103/200 [39:39<37:34, 23.24s/it]Running Inference:  52%|█████▏    | 104/200 [40:03<37:22, 23.36s/it]Running Inference:  52%|█████▎    | 105/200 [40:26<36:52, 23.29s/it]Running Inference:  53%|█████▎    | 106/200 [40:49<36:15, 23.14s/it]Running Inference:  54%|█████▎    | 107/200 [41:13<36:24, 23.49s/it]Running Inference:  54%|█████▍    | 108/200 [41:36<35:41, 23.28s/it]Running Inference:  55%|█████▍    | 109/200 [41:59<35:08, 23.17s/it]Running Inference:  55%|█████▌    | 110/200 [42:22<34:51, 23.23s/it]Running Inference:  56%|█████▌    | 111/200 [42:47<34:53, 23.52s/it]Running Inference:  56%|█████▌    | 112/200 [43:10<34:14, 23.34s/it]Running Inference:  56%|█████▋    | 113/200 [43:33<33:57, 23.42s/it]Running Inference:  57%|█████▋    | 114/200 [43:57<33:42, 23.52s/it]Running Inference:  57%|█████▊    | 115/200 [44:24<34:54, 24.64s/it]Running Inference:  58%|█████▊    | 116/200 [44:47<33:47, 24.13s/it]Running Inference:  58%|█████▊    | 117/200 [45:10<32:53, 23.77s/it]Running Inference:  59%|█████▉    | 118/200 [45:33<32:08, 23.52s/it]Running Inference:  60%|█████▉    | 119/200 [45:58<32:18, 23.94s/it]Running Inference:  60%|██████    | 120/200 [46:21<31:38, 23.74s/it]Running Inference:  60%|██████    | 121/200 [46:45<31:16, 23.76s/it]Running Inference:  61%|██████    | 122/200 [47:08<30:32, 23.49s/it]Running Inference:  62%|██████▏   | 123/200 [47:31<29:59, 23.37s/it]Running Inference:  62%|██████▏   | 124/200 [47:54<29:37, 23.39s/it]Running Inference:  62%|██████▎   | 125/200 [48:18<29:20, 23.47s/it]Running Inference:  63%|██████▎   | 126/200 [48:41<28:44, 23.30s/it]Running Inference:  64%|██████▎   | 127/200 [49:04<28:21, 23.31s/it]Running Inference:  64%|██████▍   | 128/200 [49:27<27:45, 23.13s/it]Running Inference:  64%|██████▍   | 129/200 [49:50<27:17, 23.06s/it]Running Inference:  65%|██████▌   | 130/200 [50:13<26:56, 23.10s/it]Running Inference:  66%|██████▌   | 131/200 [50:36<26:33, 23.09s/it]Running Inference:  66%|██████▌   | 132/200 [51:00<26:34, 23.45s/it]Running Inference:  66%|██████▋   | 133/200 [51:23<26:00, 23.30s/it]Running Inference:  67%|██████▋   | 134/200 [51:47<25:46, 23.43s/it]Running Inference:  68%|██████▊   | 135/200 [52:10<25:14, 23.30s/it]Running Inference:  68%|██████▊   | 136/200 [52:35<25:17, 23.71s/it]Running Inference:  68%|██████▊   | 137/200 [52:59<25:08, 23.94s/it]Running Inference:  69%|██████▉   | 138/200 [53:24<25:09, 24.34s/it]Running Inference:  70%|██████▉   | 139/200 [53:48<24:34, 24.17s/it]Running Inference:  70%|███████   | 140/200 [54:11<23:53, 23.89s/it]Running Inference:  70%|███████   | 141/200 [54:35<23:16, 23.66s/it]Running Inference:  71%|███████   | 142/200 [54:58<22:51, 23.65s/it]Running Inference:  72%|███████▏  | 143/200 [55:22<22:31, 23.71s/it]Running Inference:  72%|███████▏  | 144/200 [55:45<21:57, 23.53s/it]Running Inference:  72%|███████▎  | 145/200 [56:10<21:50, 23.82s/it]Running Inference:  73%|███████▎  | 146/200 [56:33<21:14, 23.60s/it]Running Inference:  74%|███████▎  | 147/200 [57:00<21:43, 24.59s/it]Running Inference:  74%|███████▍  | 148/200 [57:23<21:03, 24.30s/it]Running Inference:  74%|███████▍  | 149/200 [57:47<20:35, 24.23s/it]Running Inference:  75%|███████▌  | 150/200 [58:11<20:03, 24.08s/it]Running Inference:  76%|███████▌  | 151/200 [58:35<19:33, 23.95s/it]Running Inference:  76%|███████▌  | 152/200 [58:57<18:52, 23.59s/it]Running Inference:  76%|███████▋  | 153/200 [59:21<18:27, 23.56s/it]Running Inference:  77%|███████▋  | 154/200 [59:44<17:57, 23.43s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:08<17:37, 23.49s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:32<17:19, 23.63s/it]Running Inference:  78%|███████▊  | 157/200 [1:00:55<16:45, 23.39s/it]Running Inference:  79%|███████▉  | 158/200 [1:01:19<16:35, 23.70s/it]Running Inference:  80%|███████▉  | 159/200 [1:01:44<16:23, 23.98s/it]Running Inference:  80%|████████  | 160/200 [1:02:08<16:01, 24.04s/it]Running Inference:  80%|████████  | 161/200 [1:02:32<15:39, 24.10s/it]Running Inference:  81%|████████  | 162/200 [1:02:55<15:01, 23.71s/it]Running Inference:  82%|████████▏ | 163/200 [1:03:18<14:29, 23.50s/it]Running Inference:  82%|████████▏ | 164/200 [1:03:43<14:20, 23.90s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:05<13:43, 23.53s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:28<13:10, 23.26s/it]Running Inference:  84%|████████▎ | 167/200 [1:04:52<12:55, 23.50s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:15<12:28, 23.39s/it]Running Inference:  84%|████████▍ | 169/200 [1:05:38<11:59, 23.22s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:01<11:36, 23.22s/it]Running Inference:  86%|████████▌ | 171/200 [1:06:26<11:26, 23.69s/it]Running Inference:  86%|████████▌ | 172/200 [1:06:49<10:57, 23.47s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:13<10:34, 23.51s/it]Running Inference:  87%|████████▋ | 174/200 [1:07:36<10:10, 23.46s/it]Running Inference:  88%|████████▊ | 175/200 [1:07:59<09:41, 23.28s/it]Running Inference:  88%|████████▊ | 176/200 [1:08:34<10:44, 26.84s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:07<10:58, 28.63s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:31<09:59, 27.26s/it]Running Inference:  90%|████████▉ | 179/200 [1:09:54<09:04, 25.92s/it]Running Inference:  90%|█████████ | 180/200 [1:10:18<08:27, 25.38s/it]Running Inference:  90%|█████████ | 181/200 [1:10:41<07:51, 24.80s/it]Running Inference:  91%|█████████ | 182/200 [1:11:06<07:24, 24.69s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:37<07:33, 26.67s/it]Running Inference:  92%|█████████▏| 184/200 [1:12:00<06:48, 25.51s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:23<06:14, 24.99s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:46<05:41, 24.38s/it]Running Inference:  94%|█████████▎| 187/200 [1:13:09<05:11, 23.96s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:33<04:47, 23.95s/it]Running Inference:  94%|█████████▍| 189/200 [1:13:56<04:20, 23.69s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:19<03:54, 23.49s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:43<03:31, 23.51s/it]Running Inference:  96%|█████████▌| 192/200 [1:15:10<03:16, 24.53s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:33<02:48, 24.13s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:57<02:24, 24.09s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:20<01:58, 23.67s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:43<01:34, 23.55s/it]Running Inference:  98%|█████████▊| 197/200 [1:16:50<00:55, 18.61s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:16<00:41, 20.71s/it]Running Inference: 100%|█████████▉| 199/200 [1:17:38<00:21, 21.31s/it]Running Inference: 100%|██████████| 200/200 [1:18:02<00:00, 21.87s/it]Running Inference: 100%|██████████| 200/200 [1:18:02<00:00, 23.41s/it]
2025-12-14 01:29:57,360 - INFO - Inference completed.
2025-12-14 01:29:57,394 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 01:29:57,394 - INFO - Calculating metrics for dataset: longbench
2025-12-14 01:30:17,562 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 01:30:17,562 - INFO - Metrics:
8.15
2025-12-14 01:30:17,564 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=gov_report, ratio=0.3) on GPU 4

----------------------------------------
Task: gov_report | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 01:30:25,233 - INFO - Set deterministic seeds to 42
2025-12-14 01:30:25,233 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 01:30:25,233 - INFO - Starting evaluation run...
2025-12-14 01:30:25,233 - INFO - Output directory set to: longbenchresult
2025-12-14 01:30:25,233 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 01:30:25,233 - INFO - KV Press 'knorm' setup.
2025-12-14 01:30:25,233 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 01:30:25,233 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.74it/s]
Device set to use cuda:0
2025-12-14 01:31:13,329 - INFO - Model pipeline loaded.
2025-12-14 01:31:13,329 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-14 01:31:18,025 - INFO - Dataset loaded with 200 entries.
2025-12-14 01:31:18,026 - INFO - Dataset processed with 200 entries.
2025-12-14 01:31:18,060 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:40, 24.32s/it]Running Inference:   1%|          | 2/200 [00:47<1:19:00, 23.94s/it]Running Inference:   2%|▏         | 3/200 [01:13<1:20:21, 24.48s/it]Running Inference:   2%|▏         | 4/200 [01:36<1:18:04, 23.90s/it]Running Inference:   2%|▎         | 5/200 [01:59<1:17:20, 23.80s/it]Running Inference:   3%|▎         | 6/200 [02:24<1:18:19, 24.22s/it]Running Inference:   4%|▎         | 7/200 [02:48<1:17:40, 24.15s/it]Running Inference:   4%|▍         | 8/200 [03:12<1:16:47, 24.00s/it]Running Inference:   4%|▍         | 9/200 [03:36<1:16:09, 23.92s/it]Running Inference:   5%|▌         | 10/200 [03:59<1:15:33, 23.86s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:23<1:14:46, 23.74s/it]Running Inference:   6%|▌         | 12/200 [04:46<1:14:08, 23.66s/it]Running Inference:   6%|▋         | 13/200 [05:11<1:14:28, 23.90s/it]Running Inference:   7%|▋         | 14/200 [05:35<1:14:01, 23.88s/it]Running Inference:   8%|▊         | 15/200 [05:58<1:13:30, 23.84s/it]Running Inference:   8%|▊         | 16/200 [06:22<1:13:19, 23.91s/it]Running Inference:   8%|▊         | 17/200 [06:47<1:13:39, 24.15s/it]Running Inference:   9%|▉         | 18/200 [07:11<1:13:15, 24.15s/it]Running Inference:  10%|▉         | 19/200 [07:37<1:14:15, 24.61s/it]Running Inference:  10%|█         | 20/200 [08:01<1:12:56, 24.31s/it]Running Inference:  10%|█         | 21/200 [08:25<1:12:24, 24.27s/it]Running Inference:  11%|█         | 22/200 [08:49<1:11:59, 24.27s/it]Running Inference:  12%|█▏        | 23/200 [09:13<1:11:38, 24.29s/it]Running Inference:  12%|█▏        | 24/200 [09:39<1:12:20, 24.66s/it]Running Inference:  12%|█▎        | 25/200 [10:02<1:10:44, 24.26s/it]Running Inference:  13%|█▎        | 26/200 [10:26<1:09:48, 24.07s/it]Running Inference:  14%|█▎        | 27/200 [10:50<1:09:00, 23.93s/it]Running Inference:  14%|█▍        | 28/200 [11:13<1:07:58, 23.71s/it]Running Inference:  14%|█▍        | 29/200 [11:17<51:09, 17.95s/it]  Running Inference:  15%|█▌        | 30/200 [11:41<55:46, 19.69s/it]Running Inference:  16%|█▌        | 31/200 [12:04<58:35, 20.80s/it]Running Inference:  16%|█▌        | 32/200 [12:29<1:01:06, 21.83s/it]Running Inference:  16%|█▋        | 33/200 [12:51<1:01:26, 22.08s/it]Running Inference:  17%|█▋        | 34/200 [13:15<1:02:44, 22.68s/it]Running Inference:  18%|█▊        | 35/200 [13:39<1:02:55, 22.88s/it]Running Inference:  18%|█▊        | 36/200 [14:03<1:03:42, 23.31s/it]Running Inference:  18%|█▊        | 37/200 [14:26<1:03:09, 23.25s/it]Running Inference:  19%|█▉        | 38/200 [14:50<1:03:19, 23.46s/it]Running Inference:  20%|█▉        | 39/200 [15:13<1:02:49, 23.41s/it]Running Inference:  20%|██        | 40/200 [15:37<1:02:35, 23.47s/it]Running Inference:  20%|██        | 41/200 [16:01<1:02:25, 23.55s/it]Running Inference:  21%|██        | 42/200 [16:24<1:02:04, 23.57s/it]Running Inference:  22%|██▏       | 43/200 [16:48<1:01:53, 23.66s/it]Running Inference:  22%|██▏       | 44/200 [17:11<1:01:02, 23.48s/it]Running Inference:  22%|██▎       | 45/200 [17:34<1:00:13, 23.31s/it]Running Inference:  23%|██▎       | 46/200 [17:57<59:39, 23.24s/it]  Running Inference:  24%|██▎       | 47/200 [18:22<1:00:16, 23.64s/it]Running Inference:  24%|██▍       | 48/200 [18:45<59:34, 23.52s/it]  Running Inference:  24%|██▍       | 49/200 [19:08<59:04, 23.48s/it]Running Inference:  25%|██▌       | 50/200 [19:32<58:27, 23.38s/it]Running Inference:  26%|██▌       | 51/200 [19:54<57:35, 23.19s/it]Running Inference:  26%|██▌       | 52/200 [20:18<57:25, 23.28s/it]Running Inference:  26%|██▋       | 53/200 [20:44<59:02, 24.10s/it]Running Inference:  27%|██▋       | 54/200 [21:07<57:36, 23.67s/it]Running Inference:  28%|██▊       | 55/200 [21:30<56:50, 23.52s/it]Running Inference:  28%|██▊       | 56/200 [21:53<56:31, 23.56s/it]Running Inference:  28%|██▊       | 57/200 [22:17<55:54, 23.46s/it]Running Inference:  29%|██▉       | 58/200 [22:40<55:28, 23.44s/it]Running Inference:  30%|██▉       | 59/200 [23:03<55:05, 23.44s/it]Running Inference:  30%|███       | 60/200 [23:28<55:44, 23.89s/it]Running Inference:  30%|███       | 61/200 [23:52<55:02, 23.76s/it]Running Inference:  31%|███       | 62/200 [24:15<54:14, 23.58s/it]Running Inference:  32%|███▏      | 63/200 [24:38<53:17, 23.34s/it]Running Inference:  32%|███▏      | 64/200 [25:01<52:43, 23.26s/it]Running Inference:  32%|███▎      | 65/200 [25:24<52:16, 23.23s/it]Running Inference:  33%|███▎      | 66/200 [25:48<52:09, 23.35s/it]Running Inference:  34%|███▎      | 67/200 [26:11<52:03, 23.48s/it]Running Inference:  34%|███▍      | 68/200 [26:35<51:30, 23.41s/it]Running Inference:  34%|███▍      | 69/200 [26:58<51:01, 23.37s/it]Running Inference:  35%|███▌      | 70/200 [27:22<51:09, 23.61s/it]Running Inference:  36%|███▌      | 71/200 [27:46<50:54, 23.68s/it]Running Inference:  36%|███▌      | 72/200 [28:10<50:29, 23.66s/it]Running Inference:  36%|███▋      | 73/200 [28:34<50:40, 23.94s/it]Running Inference:  37%|███▋      | 74/200 [28:57<49:50, 23.73s/it]Running Inference:  38%|███▊      | 75/200 [29:21<49:11, 23.61s/it]Running Inference:  38%|███▊      | 76/200 [29:45<49:05, 23.75s/it]Running Inference:  38%|███▊      | 77/200 [30:09<48:43, 23.77s/it]Running Inference:  39%|███▉      | 78/200 [30:32<47:50, 23.53s/it]Running Inference:  40%|███▉      | 79/200 [30:55<47:16, 23.45s/it]Running Inference:  40%|████      | 80/200 [31:19<47:16, 23.64s/it]Running Inference:  40%|████      | 81/200 [31:42<46:49, 23.61s/it]Running Inference:  41%|████      | 82/200 [32:06<46:31, 23.65s/it]Running Inference:  42%|████▏     | 83/200 [32:29<45:34, 23.37s/it]Running Inference:  42%|████▏     | 84/200 [32:53<45:21, 23.46s/it]Running Inference:  42%|████▎     | 85/200 [33:16<45:10, 23.57s/it]Running Inference:  43%|████▎     | 86/200 [33:40<44:32, 23.44s/it]Running Inference:  44%|████▎     | 87/200 [34:03<44:05, 23.41s/it]Running Inference:  44%|████▍     | 88/200 [34:28<44:47, 23.99s/it]Running Inference:  44%|████▍     | 89/200 [34:51<43:55, 23.75s/it]Running Inference:  45%|████▌     | 90/200 [35:15<43:19, 23.64s/it]Running Inference:  46%|████▌     | 91/200 [35:38<42:57, 23.65s/it]Running Inference:  46%|████▌     | 92/200 [36:01<42:11, 23.44s/it]Running Inference:  46%|████▋     | 93/200 [36:25<41:50, 23.46s/it]Running Inference:  47%|████▋     | 94/200 [36:48<41:03, 23.24s/it]Running Inference:  48%|████▊     | 95/200 [37:12<41:15, 23.57s/it]Running Inference:  48%|████▊     | 96/200 [37:36<40:55, 23.61s/it]Running Inference:  48%|████▊     | 97/200 [37:59<40:10, 23.40s/it]Running Inference:  49%|████▉     | 98/200 [38:22<39:37, 23.31s/it]Running Inference:  50%|████▉     | 99/200 [38:45<39:09, 23.26s/it]Running Inference:  50%|█████     | 100/200 [39:09<39:07, 23.48s/it]Running Inference:  50%|█████     | 101/200 [39:32<38:28, 23.32s/it]Running Inference:  51%|█████     | 102/200 [39:56<38:26, 23.54s/it]Running Inference:  52%|█████▏    | 103/200 [40:19<37:44, 23.35s/it]Running Inference:  52%|█████▏    | 104/200 [40:42<37:31, 23.46s/it]Running Inference:  52%|█████▎    | 105/200 [41:06<37:00, 23.37s/it]Running Inference:  53%|█████▎    | 106/200 [41:29<36:23, 23.23s/it]Running Inference:  54%|█████▎    | 107/200 [41:53<36:30, 23.55s/it]Running Inference:  54%|█████▍    | 108/200 [42:16<35:44, 23.31s/it]Running Inference:  55%|█████▍    | 109/200 [42:39<35:14, 23.24s/it]Running Inference:  55%|█████▌    | 110/200 [43:02<34:59, 23.33s/it]Running Inference:  56%|█████▌    | 111/200 [43:26<35:00, 23.61s/it]Running Inference:  56%|█████▌    | 112/200 [43:49<34:20, 23.41s/it]Running Inference:  56%|█████▋    | 113/200 [44:13<34:02, 23.48s/it]Running Inference:  57%|█████▋    | 114/200 [44:37<33:47, 23.57s/it]Running Inference:  57%|█████▊    | 115/200 [45:03<34:34, 24.41s/it]Running Inference:  58%|█████▊    | 116/200 [45:26<33:35, 23.99s/it]Running Inference:  58%|█████▊    | 117/200 [45:49<32:42, 23.64s/it]Running Inference:  59%|█████▉    | 118/200 [46:12<32:02, 23.45s/it]Running Inference:  60%|█████▉    | 119/200 [46:37<32:14, 23.89s/it]Running Inference:  60%|██████    | 120/200 [47:00<31:36, 23.71s/it]Running Inference:  60%|██████    | 121/200 [47:24<31:17, 23.76s/it]Running Inference:  61%|██████    | 122/200 [47:47<30:33, 23.51s/it]Running Inference:  62%|██████▏   | 123/200 [48:10<30:01, 23.40s/it]Running Inference:  62%|██████▏   | 124/200 [48:15<22:35, 17.83s/it]Running Inference:  62%|██████▎   | 125/200 [48:39<24:28, 19.58s/it]Running Inference:  63%|██████▎   | 126/200 [49:02<25:22, 20.58s/it]Running Inference:  64%|██████▎   | 127/200 [49:25<26:03, 21.41s/it]Running Inference:  64%|██████▍   | 128/200 [49:48<26:10, 21.81s/it]Running Inference:  64%|██████▍   | 129/200 [50:11<26:10, 22.13s/it]Running Inference:  65%|██████▌   | 130/200 [50:34<26:09, 22.42s/it]Running Inference:  66%|██████▌   | 131/200 [50:57<25:58, 22.59s/it]Running Inference:  66%|██████▌   | 132/200 [51:21<26:11, 23.10s/it]Running Inference:  66%|██████▋   | 133/200 [51:44<25:46, 23.08s/it]Running Inference:  67%|██████▋   | 134/200 [52:08<25:37, 23.29s/it]Running Inference:  68%|██████▊   | 135/200 [52:31<25:08, 23.21s/it]Running Inference:  68%|██████▊   | 136/200 [52:55<25:14, 23.67s/it]Running Inference:  68%|██████▊   | 137/200 [53:20<25:05, 23.90s/it]Running Inference:  69%|██████▉   | 138/200 [53:45<25:05, 24.28s/it]Running Inference:  70%|██████▉   | 139/200 [54:09<24:32, 24.13s/it]Running Inference:  70%|███████   | 140/200 [54:32<23:51, 23.86s/it]Running Inference:  70%|███████   | 141/200 [54:55<23:15, 23.65s/it]Running Inference:  71%|███████   | 142/200 [55:19<22:51, 23.64s/it]Running Inference:  72%|███████▏  | 143/200 [55:43<22:31, 23.70s/it]Running Inference:  72%|███████▏  | 144/200 [56:06<21:58, 23.54s/it]Running Inference:  72%|███████▎  | 145/200 [56:30<21:51, 23.85s/it]Running Inference:  73%|███████▎  | 146/200 [56:54<21:16, 23.64s/it]Running Inference:  74%|███████▎  | 147/200 [57:20<21:32, 24.39s/it]Running Inference:  74%|███████▍  | 148/200 [57:43<20:56, 24.17s/it]Running Inference:  74%|███████▍  | 149/200 [58:07<20:31, 24.14s/it]Running Inference:  75%|███████▌  | 150/200 [58:31<20:00, 24.02s/it]Running Inference:  76%|███████▌  | 151/200 [58:55<19:32, 23.92s/it]Running Inference:  76%|███████▌  | 152/200 [59:18<18:56, 23.68s/it]Running Inference:  76%|███████▋  | 153/200 [59:42<18:33, 23.69s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:05<18:04, 23.58s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:29<17:43, 23.64s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:53<17:26, 23.78s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:16<16:52, 23.54s/it]Running Inference:  79%|███████▉  | 158/200 [1:01:41<16:42, 23.88s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:06<16:31, 24.19s/it]Running Inference:  80%|████████  | 160/200 [1:02:30<16:10, 24.26s/it]Running Inference:  80%|████████  | 161/200 [1:02:54<15:48, 24.33s/it]Running Inference:  81%|████████  | 162/200 [1:03:17<15:10, 23.95s/it]Running Inference:  82%|████████▏ | 163/200 [1:03:41<14:37, 23.73s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:06<14:26, 24.07s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:28<13:49, 23.71s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:51<13:17, 23.46s/it]Running Inference:  84%|████████▎ | 167/200 [1:05:16<13:02, 23.72s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:39<12:35, 23.62s/it]Running Inference:  84%|████████▍ | 169/200 [1:06:02<12:07, 23.47s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:26<11:44, 23.48s/it]Running Inference:  86%|████████▌ | 171/200 [1:06:30<08:30, 17.61s/it]Running Inference:  86%|████████▌ | 172/200 [1:06:53<09:00, 19.30s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:17<09:18, 20.67s/it]Running Inference:  87%|████████▋ | 174/200 [1:07:40<09:20, 21.56s/it]Running Inference:  88%|████████▊ | 175/200 [1:08:03<09:10, 22.03s/it]Running Inference:  88%|████████▊ | 176/200 [1:08:37<10:13, 25.55s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:09<10:29, 27.38s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:33<09:41, 26.43s/it]Running Inference:  90%|████████▉ | 179/200 [1:09:56<08:53, 25.42s/it]Running Inference:  90%|█████████ | 180/200 [1:10:21<08:22, 25.14s/it]Running Inference:  90%|█████████ | 181/200 [1:10:22<05:43, 18.07s/it]Running Inference:  91%|█████████ | 182/200 [1:10:47<06:01, 20.06s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:17<06:32, 23.12s/it]Running Inference:  92%|█████████▏| 184/200 [1:11:40<06:09, 23.09s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:04<05:50, 23.36s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:27<05:26, 23.31s/it]Running Inference:  94%|█████████▎| 187/200 [1:12:51<05:02, 23.29s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:15<04:42, 23.53s/it]Running Inference:  94%|█████████▍| 189/200 [1:13:38<04:18, 23.48s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:01<03:54, 23.42s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:25<03:31, 23.53s/it]Running Inference:  96%|█████████▌| 192/200 [1:14:51<03:15, 24.38s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:15<02:48, 24.10s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:39<02:24, 24.15s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:02<01:58, 23.76s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:26<01:34, 23.69s/it]Running Inference:  98%|█████████▊| 197/200 [1:16:50<01:11, 23.89s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:15<00:48, 24.38s/it]Running Inference: 100%|█████████▉| 199/200 [1:17:38<00:23, 23.94s/it]Running Inference: 100%|██████████| 200/200 [1:18:02<00:00, 23.78s/it]Running Inference: 100%|██████████| 200/200 [1:18:02<00:00, 23.41s/it]
2025-12-14 02:49:20,328 - INFO - Inference completed.
2025-12-14 02:49:20,361 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 02:49:20,361 - INFO - Calculating metrics for dataset: longbench
2025-12-14 02:49:40,257 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 02:49:40,257 - INFO - Metrics:
6.89
2025-12-14 02:49:40,258 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=gov_report, ratio=0.5) on GPU 4


========================================
LongBench Task: multi_news
========================================
----------------------------------------
Task: multi_news | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 02:49:47,300 - INFO - Set deterministic seeds to 42
2025-12-14 02:49:47,300 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 02:49:47,300 - INFO - Starting evaluation run...
2025-12-14 02:49:47,300 - INFO - Output directory set to: longbenchresult
2025-12-14 02:49:47,301 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 02:49:47,301 - INFO - KV Press 'knorm' setup.
2025-12-14 02:49:47,301 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 02:49:47,301 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.40it/s]
Device set to use cuda:0
2025-12-14 02:49:59,129 - INFO - Model pipeline loaded.
2025-12-14 02:49:59,129 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 02:50:02,305 - INFO - Dataset loaded with 200 entries.
2025-12-14 02:50:02,305 - INFO - Dataset processed with 200 entries.
2025-12-14 02:50:02,314 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:05, 23.85s/it]Running Inference:   1%|          | 2/200 [00:46<1:16:50, 23.28s/it]Running Inference:   2%|▏         | 3/200 [01:09<1:16:01, 23.15s/it]Running Inference:   2%|▏         | 4/200 [01:32<1:15:23, 23.08s/it]Running Inference:   2%|▎         | 5/200 [01:55<1:14:52, 23.04s/it]Running Inference:   3%|▎         | 6/200 [02:18<1:14:24, 23.01s/it]Running Inference:   4%|▎         | 7/200 [02:41<1:14:14, 23.08s/it]Running Inference:   4%|▍         | 8/200 [03:04<1:13:50, 23.08s/it]Running Inference:   4%|▍         | 9/200 [03:27<1:13:22, 23.05s/it]Running Inference:   5%|▌         | 10/200 [03:50<1:12:47, 22.99s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:13<1:12:14, 22.93s/it]Running Inference:   6%|▌         | 12/200 [04:36<1:11:53, 22.95s/it]Running Inference:   6%|▋         | 13/200 [04:59<1:11:31, 22.95s/it]Running Inference:   7%|▋         | 14/200 [05:22<1:11:04, 22.93s/it]Running Inference:   8%|▊         | 15/200 [05:45<1:10:55, 23.00s/it]Running Inference:   8%|▊         | 16/200 [06:08<1:10:33, 23.01s/it]Running Inference:   8%|▊         | 17/200 [06:31<1:10:04, 22.98s/it]Running Inference:   9%|▉         | 18/200 [06:55<1:10:15, 23.16s/it]Running Inference:  10%|▉         | 19/200 [07:17<1:09:37, 23.08s/it]Running Inference:  10%|█         | 20/200 [07:40<1:09:00, 23.00s/it]Running Inference:  10%|█         | 21/200 [08:03<1:08:24, 22.93s/it]Running Inference:  11%|█         | 22/200 [08:26<1:08:01, 22.93s/it]Running Inference:  12%|█▏        | 23/200 [08:49<1:07:33, 22.90s/it]Running Inference:  12%|█▏        | 24/200 [09:12<1:07:23, 22.97s/it]Running Inference:  12%|█▎        | 25/200 [09:35<1:06:52, 22.93s/it]Running Inference:  13%|█▎        | 26/200 [09:58<1:06:31, 22.94s/it]Running Inference:  14%|█▎        | 27/200 [10:22<1:07:16, 23.33s/it]Running Inference:  14%|█▍        | 28/200 [10:45<1:06:51, 23.32s/it]Running Inference:  14%|█▍        | 29/200 [11:08<1:06:04, 23.18s/it]Running Inference:  15%|█▌        | 30/200 [11:31<1:05:23, 23.08s/it]Running Inference:  16%|█▌        | 31/200 [11:54<1:05:00, 23.08s/it]Running Inference:  16%|█▌        | 32/200 [12:17<1:04:25, 23.01s/it]Running Inference:  16%|█▋        | 33/200 [12:40<1:03:54, 22.96s/it]Running Inference:  17%|█▋        | 34/200 [13:03<1:03:35, 22.99s/it]Running Inference:  18%|█▊        | 35/200 [13:26<1:03:07, 22.95s/it]Running Inference:  18%|█▊        | 36/200 [13:49<1:02:45, 22.96s/it]Running Inference:  18%|█▊        | 37/200 [14:12<1:02:38, 23.06s/it]Running Inference:  19%|█▉        | 38/200 [14:35<1:02:36, 23.19s/it]Running Inference:  20%|█▉        | 39/200 [14:58<1:01:58, 23.10s/it]Running Inference:  20%|██        | 40/200 [15:21<1:01:24, 23.03s/it]Running Inference:  20%|██        | 41/200 [15:44<1:00:59, 23.01s/it]Running Inference:  21%|██        | 42/200 [16:07<1:00:26, 22.95s/it]Running Inference:  22%|██▏       | 43/200 [16:30<59:58, 22.92s/it]  Running Inference:  22%|██▏       | 44/200 [16:53<59:33, 22.91s/it]Running Inference:  22%|██▎       | 45/200 [17:16<59:13, 22.92s/it]Running Inference:  23%|██▎       | 46/200 [17:38<58:46, 22.90s/it]Running Inference:  24%|██▎       | 47/200 [18:02<58:40, 23.01s/it]Running Inference:  24%|██▍       | 48/200 [18:25<58:23, 23.05s/it]Running Inference:  24%|██▍       | 49/200 [18:48<57:51, 22.99s/it]Running Inference:  25%|██▌       | 50/200 [19:11<57:30, 23.01s/it]Running Inference:  26%|██▌       | 51/200 [19:34<57:06, 22.99s/it]Running Inference:  26%|██▌       | 52/200 [19:57<56:43, 22.99s/it]Running Inference:  26%|██▋       | 53/200 [20:20<56:14, 22.96s/it]Running Inference:  27%|██▋       | 54/200 [20:43<55:48, 22.94s/it]Running Inference:  28%|██▊       | 55/200 [21:05<55:19, 22.90s/it]Running Inference:  28%|██▊       | 56/200 [21:28<55:00, 22.92s/it]Running Inference:  28%|██▊       | 57/200 [21:51<54:43, 22.96s/it]Running Inference:  29%|██▉       | 58/200 [22:14<54:16, 22.93s/it]Running Inference:  30%|██▉       | 59/200 [22:37<53:52, 22.93s/it]Running Inference:  30%|███       | 60/200 [23:00<53:26, 22.90s/it]Running Inference:  30%|███       | 61/200 [23:23<53:06, 22.92s/it]Running Inference:  31%|███       | 62/200 [23:46<52:37, 22.88s/it]Running Inference:  32%|███▏      | 63/200 [24:09<52:17, 22.90s/it]Running Inference:  32%|███▏      | 64/200 [24:32<51:52, 22.88s/it]Running Inference:  32%|███▎      | 65/200 [24:55<51:48, 23.02s/it]Running Inference:  33%|███▎      | 66/200 [25:18<51:21, 22.99s/it]Running Inference:  34%|███▎      | 67/200 [25:41<51:15, 23.13s/it]Running Inference:  34%|███▍      | 68/200 [26:04<50:50, 23.11s/it]Running Inference:  34%|███▍      | 69/200 [26:27<50:19, 23.05s/it]Running Inference:  35%|███▌      | 70/200 [26:50<49:48, 22.99s/it]Running Inference:  36%|███▌      | 71/200 [27:13<49:21, 22.96s/it]Running Inference:  36%|███▌      | 72/200 [27:36<49:05, 23.01s/it]Running Inference:  36%|███▋      | 73/200 [27:59<48:35, 22.95s/it]Running Inference:  37%|███▋      | 74/200 [28:22<48:11, 22.95s/it]Running Inference:  38%|███▊      | 75/200 [28:45<47:45, 22.92s/it]Running Inference:  38%|███▊      | 76/200 [29:07<47:17, 22.88s/it]Running Inference:  38%|███▊      | 77/200 [29:30<46:54, 22.88s/it]Running Inference:  39%|███▉      | 78/200 [29:53<46:32, 22.89s/it]Running Inference:  40%|███▉      | 79/200 [30:16<46:07, 22.87s/it]Running Inference:  40%|████      | 80/200 [30:39<45:49, 22.91s/it]Running Inference:  40%|████      | 81/200 [31:02<45:21, 22.87s/it]Running Inference:  41%|████      | 82/200 [31:25<44:57, 22.86s/it]Running Inference:  42%|████▏     | 83/200 [31:48<44:37, 22.88s/it]Running Inference:  42%|████▏     | 84/200 [32:11<44:21, 22.94s/it]Running Inference:  42%|████▎     | 85/200 [32:34<43:55, 22.92s/it]Running Inference:  43%|████▎     | 86/200 [32:57<43:41, 22.99s/it]Running Inference:  44%|████▎     | 87/200 [33:20<43:11, 22.93s/it]Running Inference:  44%|████▍     | 88/200 [33:43<42:53, 22.98s/it]Running Inference:  44%|████▍     | 89/200 [34:06<42:36, 23.03s/it]Running Inference:  45%|████▌     | 90/200 [34:29<42:16, 23.06s/it]Running Inference:  46%|████▌     | 91/200 [34:52<41:50, 23.03s/it]Running Inference:  46%|████▌     | 92/200 [35:15<41:24, 23.00s/it]Running Inference:  46%|████▋     | 93/200 [35:38<40:54, 22.94s/it]Running Inference:  47%|████▋     | 94/200 [36:01<40:34, 22.97s/it]Running Inference:  48%|████▊     | 95/200 [36:24<40:13, 22.99s/it]Running Inference:  48%|████▊     | 96/200 [36:47<39:49, 22.97s/it]Running Inference:  48%|████▊     | 97/200 [37:10<39:27, 22.98s/it]Running Inference:  49%|████▉     | 98/200 [37:33<39:05, 22.99s/it]Running Inference:  50%|████▉     | 99/200 [37:57<39:22, 23.39s/it]Running Inference:  50%|█████     | 100/200 [38:20<38:51, 23.31s/it]Running Inference:  50%|█████     | 101/200 [38:43<38:20, 23.24s/it]Running Inference:  51%|█████     | 102/200 [38:44<26:51, 16.45s/it]Running Inference:  52%|█████▏    | 103/200 [39:07<29:57, 18.53s/it]Running Inference:  52%|█████▏    | 104/200 [39:30<31:57, 19.97s/it]Running Inference:  52%|█████▎    | 105/200 [39:54<33:04, 20.89s/it]Running Inference:  53%|█████▎    | 106/200 [40:16<33:41, 21.51s/it]Running Inference:  54%|█████▎    | 107/200 [40:39<34:00, 21.94s/it]Running Inference:  54%|█████▍    | 108/200 [41:02<34:07, 22.26s/it]Running Inference:  55%|█████▍    | 109/200 [41:25<34:00, 22.43s/it]Running Inference:  55%|█████▌    | 110/200 [41:48<33:48, 22.54s/it]Running Inference:  56%|█████▌    | 111/200 [42:11<33:36, 22.66s/it]Running Inference:  56%|█████▌    | 112/200 [42:34<33:18, 22.71s/it]Running Inference:  56%|█████▋    | 113/200 [42:57<33:05, 22.82s/it]Running Inference:  57%|█████▋    | 114/200 [43:20<32:42, 22.82s/it]Running Inference:  57%|█████▊    | 115/200 [43:43<32:20, 22.82s/it]Running Inference:  58%|█████▊    | 116/200 [44:05<31:57, 22.83s/it]Running Inference:  58%|█████▊    | 117/200 [44:28<31:34, 22.83s/it]Running Inference:  59%|█████▉    | 118/200 [44:51<31:13, 22.85s/it]Running Inference:  60%|█████▉    | 119/200 [45:14<30:59, 22.96s/it]Running Inference:  60%|██████    | 120/200 [45:37<30:33, 22.92s/it]Running Inference:  60%|██████    | 121/200 [46:00<30:10, 22.92s/it]Running Inference:  61%|██████    | 122/200 [46:23<29:45, 22.89s/it]Running Inference:  62%|██████▏   | 123/200 [46:46<29:25, 22.93s/it]Running Inference:  62%|██████▏   | 124/200 [47:09<28:59, 22.89s/it]Running Inference:  62%|██████▎   | 125/200 [47:32<28:36, 22.88s/it]Running Inference:  63%|██████▎   | 126/200 [47:54<28:11, 22.86s/it]Running Inference:  64%|██████▎   | 127/200 [48:17<27:49, 22.88s/it]Running Inference:  64%|██████▍   | 128/200 [48:41<27:46, 23.15s/it]Running Inference:  64%|██████▍   | 129/200 [49:04<27:15, 23.04s/it]Running Inference:  65%|██████▌   | 130/200 [49:27<26:47, 22.96s/it]Running Inference:  66%|██████▌   | 131/200 [49:51<26:53, 23.38s/it]Running Inference:  66%|██████▌   | 132/200 [50:14<26:21, 23.26s/it]Running Inference:  66%|██████▋   | 133/200 [50:37<25:50, 23.13s/it]Running Inference:  67%|██████▋   | 134/200 [51:00<25:23, 23.09s/it]Running Inference:  68%|██████▊   | 135/200 [51:23<25:09, 23.22s/it]Running Inference:  68%|██████▊   | 136/200 [51:46<24:44, 23.20s/it]Running Inference:  68%|██████▊   | 137/200 [52:10<24:19, 23.16s/it]Running Inference:  69%|██████▉   | 138/200 [52:32<23:49, 23.06s/it]Running Inference:  70%|██████▉   | 139/200 [52:55<23:22, 22.99s/it]Running Inference:  70%|███████   | 140/200 [53:18<22:58, 22.98s/it]Running Inference:  70%|███████   | 141/200 [53:41<22:35, 22.98s/it]Running Inference:  71%|███████   | 142/200 [54:04<22:11, 22.96s/it]Running Inference:  72%|███████▏  | 143/200 [54:27<21:47, 22.94s/it]Running Inference:  72%|███████▏  | 144/200 [54:50<21:22, 22.90s/it]Running Inference:  72%|███████▎  | 145/200 [55:13<21:00, 22.91s/it]Running Inference:  73%|███████▎  | 146/200 [55:36<20:36, 22.91s/it]Running Inference:  74%|███████▎  | 147/200 [55:58<20:13, 22.89s/it]Running Inference:  74%|███████▍  | 148/200 [56:21<19:48, 22.86s/it]Running Inference:  74%|███████▍  | 149/200 [56:44<19:25, 22.85s/it]Running Inference:  75%|███████▌  | 150/200 [57:07<19:09, 22.99s/it]Running Inference:  76%|███████▌  | 151/200 [57:30<18:44, 22.95s/it]Running Inference:  76%|███████▌  | 152/200 [57:53<18:22, 22.97s/it]Running Inference:  76%|███████▋  | 153/200 [58:16<18:03, 23.06s/it]Running Inference:  77%|███████▋  | 154/200 [58:40<17:41, 23.07s/it]Running Inference:  78%|███████▊  | 155/200 [59:03<17:19, 23.11s/it]Running Inference:  78%|███████▊  | 156/200 [59:26<16:52, 23.00s/it]Running Inference:  78%|███████▊  | 157/200 [59:49<16:31, 23.07s/it]Running Inference:  79%|███████▉  | 158/200 [1:00:12<16:10, 23.10s/it]Running Inference:  80%|███████▉  | 159/200 [1:00:35<15:44, 23.03s/it]Running Inference:  80%|████████  | 160/200 [1:00:58<15:23, 23.09s/it]Running Inference:  80%|████████  | 161/200 [1:01:21<14:58, 23.04s/it]Running Inference:  81%|████████  | 162/200 [1:01:44<14:33, 22.99s/it]Running Inference:  82%|████████▏ | 163/200 [1:02:07<14:09, 22.96s/it]Running Inference:  82%|████████▏ | 164/200 [1:02:30<13:48, 23.02s/it]Running Inference:  82%|████████▎ | 165/200 [1:02:53<13:23, 22.97s/it]Running Inference:  83%|████████▎ | 166/200 [1:03:16<13:00, 22.96s/it]Running Inference:  84%|████████▎ | 167/200 [1:03:39<12:37, 22.95s/it]Running Inference:  84%|████████▍ | 168/200 [1:04:01<12:13, 22.93s/it]Running Inference:  84%|████████▍ | 169/200 [1:04:25<11:51, 22.97s/it]Running Inference:  85%|████████▌ | 170/200 [1:04:48<11:30, 23.02s/it]Running Inference:  86%|████████▌ | 171/200 [1:05:11<11:06, 22.98s/it]Running Inference:  86%|████████▌ | 172/200 [1:05:34<10:43, 22.99s/it]Running Inference:  86%|████████▋ | 173/200 [1:05:56<10:19, 22.94s/it]Running Inference:  87%|████████▋ | 174/200 [1:06:19<09:56, 22.95s/it]Running Inference:  88%|████████▊ | 175/200 [1:06:42<09:33, 22.94s/it]Running Inference:  88%|████████▊ | 176/200 [1:07:05<09:10, 22.92s/it]Running Inference:  88%|████████▊ | 177/200 [1:07:28<08:48, 22.96s/it]Running Inference:  89%|████████▉ | 178/200 [1:07:52<08:27, 23.08s/it]Running Inference:  90%|████████▉ | 179/200 [1:08:14<08:03, 23.01s/it]Running Inference:  90%|█████████ | 180/200 [1:08:37<07:39, 22.97s/it]Running Inference:  90%|█████████ | 181/200 [1:09:00<07:15, 22.93s/it]Running Inference:  91%|█████████ | 182/200 [1:09:23<06:52, 22.90s/it]Running Inference:  92%|█████████▏| 183/200 [1:09:46<06:28, 22.88s/it]Running Inference:  92%|█████████▏| 184/200 [1:10:10<06:11, 23.24s/it]Running Inference:  92%|█████████▎| 185/200 [1:10:33<05:47, 23.17s/it]Running Inference:  93%|█████████▎| 186/200 [1:10:56<05:23, 23.08s/it]Running Inference:  94%|█████████▎| 187/200 [1:11:19<04:59, 23.01s/it]Running Inference:  94%|█████████▍| 188/200 [1:11:42<04:35, 22.99s/it]Running Inference:  94%|█████████▍| 189/200 [1:12:04<04:12, 22.92s/it]Running Inference:  95%|█████████▌| 190/200 [1:12:27<03:49, 22.90s/it]Running Inference:  96%|█████████▌| 191/200 [1:12:50<03:26, 22.99s/it]Running Inference:  96%|█████████▌| 192/200 [1:13:13<03:04, 23.03s/it]Running Inference:  96%|█████████▋| 193/200 [1:13:37<02:41, 23.05s/it]Running Inference:  97%|█████████▋| 194/200 [1:13:59<02:17, 22.99s/it]Running Inference:  98%|█████████▊| 195/200 [1:14:22<01:55, 23.00s/it]Running Inference:  98%|█████████▊| 196/200 [1:14:46<01:32, 23.08s/it]Running Inference:  98%|█████████▊| 197/200 [1:15:09<01:09, 23.01s/it]Running Inference:  99%|█████████▉| 198/200 [1:15:31<00:45, 22.96s/it]Running Inference: 100%|█████████▉| 199/200 [1:15:54<00:22, 22.93s/it]Running Inference: 100%|██████████| 200/200 [1:16:17<00:00, 22.99s/it]Running Inference: 100%|██████████| 200/200 [1:16:17<00:00, 22.89s/it]
2025-12-14 04:06:20,218 - INFO - Inference completed.
2025-12-14 04:06:20,239 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 04:06:20,240 - INFO - Calculating metrics for dataset: longbench
2025-12-14 04:06:27,726 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 04:06:27,727 - INFO - Metrics:
9.17
2025-12-14 04:06:27,728 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multi_news, ratio=0.1) on GPU 4

----------------------------------------
Task: multi_news | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 04:06:34,532 - INFO - Set deterministic seeds to 42
2025-12-14 04:06:34,532 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 04:06:34,532 - INFO - Starting evaluation run...
2025-12-14 04:06:34,532 - INFO - Output directory set to: longbenchresult
2025-12-14 04:06:34,532 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 04:06:34,532 - INFO - KV Press 'knorm' setup.
2025-12-14 04:06:34,532 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 04:06:34,532 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.26it/s]
Device set to use cuda:0
2025-12-14 04:06:45,121 - INFO - Model pipeline loaded.
2025-12-14 04:06:45,121 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 04:06:48,859 - INFO - Dataset loaded with 200 entries.
2025-12-14 04:06:48,860 - INFO - Dataset processed with 200 entries.
2025-12-14 04:06:48,869 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:19:56, 24.10s/it]Running Inference:   1%|          | 2/200 [00:47<1:17:38, 23.53s/it]Running Inference:   2%|▏         | 3/200 [01:10<1:16:49, 23.40s/it]Running Inference:   2%|▏         | 4/200 [01:33<1:16:12, 23.33s/it]Running Inference:   2%|▎         | 5/200 [01:56<1:15:38, 23.27s/it]Running Inference:   3%|▎         | 6/200 [02:20<1:15:09, 23.25s/it]Running Inference:   4%|▎         | 7/200 [02:43<1:15:00, 23.32s/it]Running Inference:   4%|▍         | 8/200 [03:06<1:14:38, 23.33s/it]Running Inference:   4%|▍         | 9/200 [03:30<1:14:13, 23.31s/it]Running Inference:   5%|▌         | 10/200 [03:53<1:13:38, 23.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:16<1:13:07, 23.21s/it]Running Inference:   6%|▌         | 12/200 [04:39<1:12:48, 23.24s/it]Running Inference:   6%|▋         | 13/200 [05:02<1:12:25, 23.24s/it]Running Inference:   7%|▋         | 14/200 [05:26<1:11:56, 23.21s/it]Running Inference:   8%|▊         | 15/200 [05:49<1:11:47, 23.28s/it]Running Inference:   8%|▊         | 16/200 [06:12<1:11:24, 23.29s/it]Running Inference:   8%|▊         | 17/200 [06:36<1:10:56, 23.26s/it]Running Inference:   9%|▉         | 18/200 [06:59<1:11:07, 23.45s/it]Running Inference:  10%|▉         | 19/200 [07:23<1:10:30, 23.37s/it]Running Inference:  10%|█         | 20/200 [07:46<1:09:54, 23.30s/it]Running Inference:  10%|█         | 21/200 [08:09<1:09:17, 23.23s/it]Running Inference:  11%|█         | 22/200 [08:32<1:08:55, 23.23s/it]Running Inference:  12%|█▏        | 23/200 [08:55<1:08:28, 23.21s/it]Running Inference:  12%|█▏        | 24/200 [09:19<1:08:16, 23.27s/it]Running Inference:  12%|█▎        | 25/200 [09:42<1:07:44, 23.22s/it]Running Inference:  13%|█▎        | 26/200 [10:05<1:07:22, 23.23s/it]Running Inference:  14%|█▎        | 27/200 [10:29<1:08:03, 23.61s/it]Running Inference:  14%|█▍        | 28/200 [10:53<1:07:37, 23.59s/it]Running Inference:  14%|█▍        | 29/200 [11:16<1:06:50, 23.45s/it]Running Inference:  15%|█▌        | 30/200 [11:39<1:06:10, 23.36s/it]Running Inference:  16%|█▌        | 31/200 [12:03<1:05:48, 23.36s/it]Running Inference:  16%|█▌        | 32/200 [12:26<1:05:15, 23.31s/it]Running Inference:  16%|█▋        | 33/200 [12:49<1:04:44, 23.26s/it]Running Inference:  17%|█▋        | 34/200 [13:12<1:04:24, 23.28s/it]Running Inference:  18%|█▊        | 35/200 [13:36<1:03:57, 23.26s/it]Running Inference:  18%|█▊        | 36/200 [13:59<1:03:33, 23.25s/it]Running Inference:  18%|█▊        | 37/200 [14:22<1:03:25, 23.35s/it]Running Inference:  19%|█▉        | 38/200 [14:46<1:03:20, 23.46s/it]Running Inference:  20%|█▉        | 39/200 [15:09<1:02:41, 23.36s/it]Running Inference:  20%|██        | 40/200 [15:32<1:02:08, 23.30s/it]Running Inference:  20%|██        | 41/200 [15:56<1:01:43, 23.29s/it]Running Inference:  21%|██        | 42/200 [16:19<1:01:09, 23.23s/it]Running Inference:  22%|██▏       | 43/200 [16:42<1:00:42, 23.20s/it]Running Inference:  22%|██▏       | 44/200 [17:05<1:00:18, 23.20s/it]Running Inference:  22%|██▎       | 45/200 [17:28<59:57, 23.21s/it]  Running Inference:  23%|██▎       | 46/200 [17:51<59:32, 23.20s/it]Running Inference:  24%|██▎       | 47/200 [18:15<59:25, 23.30s/it]Running Inference:  24%|██▍       | 48/200 [18:38<59:07, 23.34s/it]Running Inference:  24%|██▍       | 49/200 [19:02<58:35, 23.28s/it]Running Inference:  25%|██▌       | 50/200 [19:25<58:14, 23.30s/it]Running Inference:  26%|██▌       | 51/200 [19:48<57:48, 23.28s/it]Running Inference:  26%|██▌       | 52/200 [20:11<57:26, 23.29s/it]Running Inference:  26%|██▋       | 53/200 [20:35<56:57, 23.25s/it]Running Inference:  27%|██▋       | 54/200 [20:58<56:30, 23.22s/it]Running Inference:  28%|██▊       | 55/200 [21:21<56:00, 23.18s/it]Running Inference:  28%|██▊       | 56/200 [21:44<55:41, 23.21s/it]Running Inference:  28%|██▊       | 57/200 [22:07<55:24, 23.25s/it]Running Inference:  29%|██▉       | 58/200 [22:31<54:55, 23.21s/it]Running Inference:  30%|██▉       | 59/200 [22:54<54:29, 23.19s/it]Running Inference:  30%|███       | 60/200 [23:17<54:02, 23.16s/it]Running Inference:  30%|███       | 61/200 [23:40<53:43, 23.19s/it]Running Inference:  31%|███       | 62/200 [24:03<53:15, 23.15s/it]Running Inference:  32%|███▏      | 63/200 [24:26<52:54, 23.17s/it]Running Inference:  32%|███▏      | 64/200 [24:49<52:30, 23.17s/it]Running Inference:  32%|███▎      | 65/200 [25:13<52:26, 23.31s/it]Running Inference:  33%|███▎      | 66/200 [25:36<51:58, 23.27s/it]Running Inference:  34%|███▎      | 67/200 [26:00<51:53, 23.41s/it]Running Inference:  34%|███▍      | 68/200 [26:23<51:28, 23.40s/it]Running Inference:  34%|███▍      | 69/200 [26:39<46:09, 21.14s/it]Running Inference:  35%|███▌      | 70/200 [27:02<47:07, 21.75s/it]Running Inference:  36%|███▌      | 71/200 [27:26<47:42, 22.19s/it]Running Inference:  36%|███▌      | 72/200 [27:49<48:08, 22.57s/it]Running Inference:  36%|███▋      | 73/200 [28:12<48:06, 22.73s/it]Running Inference:  37%|███▋      | 74/200 [28:35<48:03, 22.89s/it]Running Inference:  38%|███▊      | 75/200 [28:59<47:51, 22.97s/it]Running Inference:  38%|███▊      | 76/200 [29:22<47:32, 23.01s/it]Running Inference:  38%|███▊      | 77/200 [29:45<47:16, 23.06s/it]Running Inference:  39%|███▉      | 78/200 [30:08<46:57, 23.09s/it]Running Inference:  40%|███▉      | 79/200 [30:31<46:34, 23.09s/it]Running Inference:  40%|████      | 80/200 [30:54<46:18, 23.15s/it]Running Inference:  40%|████      | 81/200 [31:18<45:52, 23.13s/it]Running Inference:  41%|████      | 82/200 [31:41<45:30, 23.14s/it]Running Inference:  42%|████▏     | 83/200 [32:04<45:11, 23.17s/it]Running Inference:  42%|████▏     | 84/200 [32:27<44:56, 23.24s/it]Running Inference:  42%|████▎     | 85/200 [32:51<44:34, 23.26s/it]Running Inference:  43%|████▎     | 86/200 [33:14<44:20, 23.33s/it]Running Inference:  44%|████▎     | 87/200 [33:37<43:49, 23.27s/it]Running Inference:  44%|████▍     | 88/200 [34:01<43:30, 23.31s/it]Running Inference:  44%|████▍     | 89/200 [34:24<43:13, 23.37s/it]Running Inference:  45%|████▌     | 90/200 [34:48<42:51, 23.38s/it]Running Inference:  46%|████▌     | 91/200 [35:11<42:24, 23.35s/it]Running Inference:  46%|████▌     | 92/200 [35:34<41:57, 23.31s/it]Running Inference:  46%|████▋     | 93/200 [35:57<41:27, 23.25s/it]Running Inference:  47%|████▋     | 94/200 [36:21<41:06, 23.27s/it]Running Inference:  48%|████▊     | 95/200 [36:44<40:49, 23.32s/it]Running Inference:  48%|████▊     | 96/200 [37:07<40:19, 23.27s/it]Running Inference:  48%|████▊     | 97/200 [37:30<39:50, 23.21s/it]Running Inference:  49%|████▉     | 98/200 [37:53<39:23, 23.17s/it]Running Inference:  50%|████▉     | 99/200 [38:18<39:38, 23.55s/it]Running Inference:  50%|█████     | 100/200 [38:41<39:01, 23.41s/it]Running Inference:  50%|█████     | 101/200 [39:04<38:27, 23.31s/it]Running Inference:  51%|█████     | 102/200 [39:27<37:53, 23.20s/it]Running Inference:  52%|█████▏    | 103/200 [39:50<37:37, 23.28s/it]Running Inference:  52%|█████▏    | 104/200 [40:14<37:18, 23.32s/it]Running Inference:  52%|█████▎    | 105/200 [40:37<36:49, 23.25s/it]Running Inference:  53%|█████▎    | 106/200 [41:00<36:17, 23.16s/it]Running Inference:  54%|█████▎    | 107/200 [41:23<35:49, 23.12s/it]Running Inference:  54%|█████▍    | 108/200 [41:46<35:25, 23.10s/it]Running Inference:  55%|█████▍    | 109/200 [42:09<34:57, 23.05s/it]Running Inference:  55%|█████▌    | 110/200 [42:32<34:29, 22.99s/it]Running Inference:  56%|█████▌    | 111/200 [42:55<34:06, 22.99s/it]Running Inference:  56%|█████▌    | 112/200 [43:18<33:41, 22.97s/it]Running Inference:  56%|█████▋    | 113/200 [43:41<33:23, 23.03s/it]Running Inference:  57%|█████▋    | 114/200 [44:04<32:55, 22.97s/it]Running Inference:  57%|█████▊    | 115/200 [44:26<32:31, 22.96s/it]Running Inference:  58%|█████▊    | 116/200 [44:49<32:07, 22.94s/it]Running Inference:  58%|█████▊    | 117/200 [45:12<31:42, 22.92s/it]Running Inference:  59%|█████▉    | 118/200 [45:35<31:19, 22.93s/it]Running Inference:  60%|█████▉    | 119/200 [45:58<31:05, 23.03s/it]Running Inference:  60%|██████    | 120/200 [46:21<30:39, 23.00s/it]Running Inference:  60%|██████    | 121/200 [46:44<30:16, 22.99s/it]Running Inference:  61%|██████    | 122/200 [47:07<29:50, 22.95s/it]Running Inference:  62%|██████▏   | 123/200 [47:30<29:30, 22.99s/it]Running Inference:  62%|██████▏   | 124/200 [47:53<29:03, 22.95s/it]Running Inference:  62%|██████▎   | 125/200 [48:16<28:40, 22.94s/it]Running Inference:  63%|██████▎   | 126/200 [48:39<28:15, 22.91s/it]Running Inference:  64%|██████▎   | 127/200 [49:02<27:53, 22.93s/it]Running Inference:  64%|██████▍   | 128/200 [49:26<27:50, 23.21s/it]Running Inference:  64%|██████▍   | 129/200 [49:49<27:19, 23.10s/it]Running Inference:  65%|██████▌   | 130/200 [50:11<26:51, 23.01s/it]Running Inference:  66%|██████▌   | 131/200 [50:36<26:57, 23.45s/it]Running Inference:  66%|██████▌   | 132/200 [50:59<26:24, 23.30s/it]Running Inference:  66%|██████▋   | 133/200 [51:22<25:53, 23.19s/it]Running Inference:  67%|██████▋   | 134/200 [51:45<25:26, 23.14s/it]Running Inference:  68%|██████▊   | 135/200 [52:08<25:12, 23.27s/it]Running Inference:  68%|██████▊   | 136/200 [52:32<24:47, 23.25s/it]Running Inference:  68%|██████▊   | 137/200 [52:55<24:22, 23.22s/it]Running Inference:  69%|██████▉   | 138/200 [53:18<23:55, 23.16s/it]Running Inference:  70%|██████▉   | 139/200 [53:41<23:28, 23.09s/it]Running Inference:  70%|███████   | 140/200 [54:04<23:03, 23.06s/it]Running Inference:  70%|███████   | 141/200 [54:27<22:40, 23.06s/it]Running Inference:  71%|███████   | 142/200 [54:50<22:15, 23.03s/it]Running Inference:  72%|███████▏  | 143/200 [55:13<21:50, 22.99s/it]Running Inference:  72%|███████▏  | 144/200 [55:35<21:25, 22.95s/it]Running Inference:  72%|███████▎  | 145/200 [55:58<21:02, 22.95s/it]Running Inference:  73%|███████▎  | 146/200 [56:21<20:38, 22.94s/it]Running Inference:  74%|███████▎  | 147/200 [56:44<20:15, 22.93s/it]Running Inference:  74%|███████▍  | 148/200 [57:07<19:50, 22.90s/it]Running Inference:  74%|███████▍  | 149/200 [57:30<19:27, 22.89s/it]Running Inference:  75%|███████▌  | 150/200 [57:53<19:10, 23.02s/it]Running Inference:  76%|███████▌  | 151/200 [58:16<18:46, 22.99s/it]Running Inference:  76%|███████▌  | 152/200 [58:39<18:24, 23.02s/it]Running Inference:  76%|███████▋  | 153/200 [59:02<18:04, 23.07s/it]Running Inference:  77%|███████▋  | 154/200 [59:25<17:40, 23.06s/it]Running Inference:  78%|███████▊  | 155/200 [59:49<17:19, 23.11s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:12<16:54, 23.05s/it]Running Inference:  78%|███████▊  | 157/200 [1:00:35<16:33, 23.10s/it]Running Inference:  79%|███████▉  | 158/200 [1:00:58<16:10, 23.12s/it]Running Inference:  80%|███████▉  | 159/200 [1:01:21<15:45, 23.06s/it]Running Inference:  80%|████████  | 160/200 [1:01:44<15:24, 23.12s/it]Running Inference:  80%|████████  | 161/200 [1:02:07<14:59, 23.07s/it]Running Inference:  81%|████████  | 162/200 [1:02:30<14:34, 23.03s/it]Running Inference:  82%|████████▏ | 163/200 [1:02:53<14:11, 23.00s/it]Running Inference:  82%|████████▏ | 164/200 [1:03:16<13:49, 23.05s/it]Running Inference:  82%|████████▎ | 165/200 [1:03:39<13:25, 23.00s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:02<13:01, 22.99s/it]Running Inference:  84%|████████▎ | 167/200 [1:04:25<12:38, 22.98s/it]Running Inference:  84%|████████▍ | 168/200 [1:04:48<12:14, 22.95s/it]Running Inference:  84%|████████▍ | 169/200 [1:05:11<11:52, 22.99s/it]Running Inference:  85%|████████▌ | 170/200 [1:05:34<11:31, 23.04s/it]Running Inference:  86%|████████▌ | 171/200 [1:05:57<11:08, 23.04s/it]Running Inference:  86%|████████▌ | 172/200 [1:06:20<10:45, 23.05s/it]Running Inference:  86%|████████▋ | 173/200 [1:06:43<10:20, 22.99s/it]Running Inference:  87%|████████▋ | 174/200 [1:07:06<09:57, 22.99s/it]Running Inference:  88%|████████▊ | 175/200 [1:07:29<09:34, 22.98s/it]Running Inference:  88%|████████▊ | 176/200 [1:07:52<09:11, 22.97s/it]Running Inference:  88%|████████▊ | 177/200 [1:08:15<08:48, 23.00s/it]Running Inference:  89%|████████▉ | 178/200 [1:08:38<08:28, 23.12s/it]Running Inference:  90%|████████▉ | 179/200 [1:09:01<08:04, 23.06s/it]Running Inference:  90%|█████████ | 180/200 [1:09:24<07:40, 23.02s/it]Running Inference:  90%|█████████ | 181/200 [1:09:47<07:16, 22.98s/it]Running Inference:  91%|█████████ | 182/200 [1:10:10<06:53, 22.95s/it]Running Inference:  92%|█████████▏| 183/200 [1:10:33<06:29, 22.93s/it]Running Inference:  92%|█████████▏| 184/200 [1:10:57<06:12, 23.28s/it]Running Inference:  92%|█████████▎| 185/200 [1:11:20<05:48, 23.21s/it]Running Inference:  93%|█████████▎| 186/200 [1:11:43<05:23, 23.11s/it]Running Inference:  94%|█████████▎| 187/200 [1:12:06<04:59, 23.05s/it]Running Inference:  94%|█████████▍| 188/200 [1:12:29<04:36, 23.03s/it]Running Inference:  94%|█████████▍| 189/200 [1:12:52<04:13, 23.01s/it]Running Inference:  95%|█████████▌| 190/200 [1:13:15<03:49, 22.98s/it]Running Inference:  96%|█████████▌| 191/200 [1:13:38<03:27, 23.06s/it]Running Inference:  96%|█████████▌| 192/200 [1:14:01<03:04, 23.09s/it]Running Inference:  96%|█████████▋| 193/200 [1:14:24<02:41, 23.11s/it]Running Inference:  97%|█████████▋| 194/200 [1:14:47<02:18, 23.05s/it]Running Inference:  98%|█████████▊| 195/200 [1:15:10<01:55, 23.05s/it]Running Inference:  98%|█████████▊| 196/200 [1:15:33<01:32, 23.12s/it]Running Inference:  98%|█████████▊| 197/200 [1:15:56<01:09, 23.05s/it]Running Inference:  99%|█████████▉| 198/200 [1:16:19<00:46, 23.00s/it]Running Inference: 100%|█████████▉| 199/200 [1:16:42<00:22, 22.97s/it]Running Inference: 100%|██████████| 200/200 [1:17:05<00:00, 23.03s/it]Running Inference: 100%|██████████| 200/200 [1:17:05<00:00, 23.13s/it]
2025-12-14 05:23:54,647 - INFO - Inference completed.
2025-12-14 05:23:54,668 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 05:23:54,668 - INFO - Calculating metrics for dataset: longbench
2025-12-14 05:24:02,117 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 05:24:02,117 - INFO - Metrics:
6.91
2025-12-14 05:24:02,119 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multi_news, ratio=0.2) on GPU 4

----------------------------------------
Task: multi_news | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 05:24:08,982 - INFO - Set deterministic seeds to 42
2025-12-14 05:24:08,982 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 05:24:08,982 - INFO - Starting evaluation run...
2025-12-14 05:24:08,982 - INFO - Output directory set to: longbenchresult
2025-12-14 05:24:08,982 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 05:24:08,982 - INFO - KV Press 'knorm' setup.
2025-12-14 05:24:08,982 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 05:24:08,982 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.72it/s]
Device set to use cuda:0
2025-12-14 05:24:19,620 - INFO - Model pipeline loaded.
2025-12-14 05:24:19,620 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 05:24:22,726 - INFO - Dataset loaded with 200 entries.
2025-12-14 05:24:22,726 - INFO - Dataset processed with 200 entries.
2025-12-14 05:24:22,736 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:17, 23.91s/it]Running Inference:   1%|          | 2/200 [00:46<1:17:10, 23.39s/it]Running Inference:   2%|▏         | 3/200 [01:10<1:16:21, 23.25s/it]Running Inference:   2%|▏         | 4/200 [01:33<1:15:41, 23.17s/it]Running Inference:   2%|▎         | 5/200 [01:56<1:15:11, 23.14s/it]Running Inference:   3%|▎         | 6/200 [02:19<1:14:42, 23.11s/it]Running Inference:   4%|▎         | 7/200 [02:42<1:14:33, 23.18s/it]Running Inference:   4%|▍         | 8/200 [03:05<1:14:11, 23.19s/it]Running Inference:   4%|▍         | 9/200 [03:28<1:13:41, 23.15s/it]Running Inference:   5%|▌         | 10/200 [03:51<1:13:10, 23.11s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:14<1:12:38, 23.06s/it]Running Inference:   6%|▌         | 12/200 [04:37<1:12:19, 23.08s/it]Running Inference:   6%|▋         | 13/200 [05:00<1:11:57, 23.09s/it]Running Inference:   7%|▋         | 14/200 [05:24<1:11:29, 23.06s/it]Running Inference:   8%|▊         | 15/200 [05:47<1:11:22, 23.15s/it]Running Inference:   8%|▊         | 16/200 [06:10<1:11:00, 23.16s/it]Running Inference:   8%|▊         | 17/200 [06:33<1:10:33, 23.13s/it]Running Inference:   9%|▉         | 18/200 [06:57<1:10:44, 23.32s/it]Running Inference:  10%|▉         | 19/200 [07:20<1:10:07, 23.25s/it]Running Inference:  10%|█         | 20/200 [07:43<1:09:28, 23.16s/it]Running Inference:  10%|█         | 21/200 [08:06<1:08:52, 23.09s/it]Running Inference:  11%|█         | 22/200 [08:29<1:08:28, 23.08s/it]Running Inference:  12%|█▏        | 23/200 [08:52<1:08:03, 23.07s/it]Running Inference:  12%|█▏        | 24/200 [09:15<1:07:51, 23.14s/it]Running Inference:  12%|█▎        | 25/200 [09:38<1:07:19, 23.08s/it]Running Inference:  13%|█▎        | 26/200 [10:01<1:06:56, 23.08s/it]Running Inference:  14%|█▎        | 27/200 [10:26<1:07:39, 23.46s/it]Running Inference:  14%|█▍        | 28/200 [10:49<1:07:15, 23.46s/it]Running Inference:  14%|█▍        | 29/200 [11:12<1:06:28, 23.33s/it]Running Inference:  15%|█▌        | 30/200 [11:35<1:05:49, 23.23s/it]Running Inference:  16%|█▌        | 31/200 [11:58<1:05:27, 23.24s/it]Running Inference:  16%|█▌        | 32/200 [12:21<1:04:53, 23.18s/it]Running Inference:  16%|█▋        | 33/200 [12:44<1:04:23, 23.13s/it]Running Inference:  17%|█▋        | 34/200 [13:08<1:04:02, 23.15s/it]Running Inference:  18%|█▊        | 35/200 [13:31<1:03:35, 23.12s/it]Running Inference:  18%|█▊        | 36/200 [13:54<1:03:11, 23.12s/it]Running Inference:  18%|█▊        | 37/200 [14:17<1:03:03, 23.21s/it]Running Inference:  19%|█▉        | 38/200 [14:41<1:03:01, 23.34s/it]Running Inference:  20%|█▉        | 39/200 [15:04<1:02:26, 23.27s/it]Running Inference:  20%|██        | 40/200 [15:27<1:01:49, 23.19s/it]Running Inference:  20%|██        | 41/200 [15:50<1:01:24, 23.17s/it]Running Inference:  21%|██        | 42/200 [16:13<1:00:50, 23.11s/it]Running Inference:  22%|██▏       | 43/200 [16:36<1:00:22, 23.07s/it]Running Inference:  22%|██▏       | 44/200 [16:59<59:59, 23.07s/it]  Running Inference:  22%|██▎       | 45/200 [17:22<59:38, 23.09s/it]Running Inference:  23%|██▎       | 46/200 [17:45<59:10, 23.06s/it]Running Inference:  24%|██▎       | 47/200 [18:09<59:05, 23.17s/it]Running Inference:  24%|██▍       | 48/200 [18:32<58:47, 23.21s/it]Running Inference:  24%|██▍       | 49/200 [18:55<58:14, 23.14s/it]Running Inference:  25%|██▌       | 50/200 [19:18<57:55, 23.17s/it]Running Inference:  26%|██▌       | 51/200 [19:41<57:29, 23.15s/it]Running Inference:  26%|██▌       | 52/200 [20:04<57:06, 23.15s/it]Running Inference:  26%|██▋       | 53/200 [20:27<56:36, 23.11s/it]Running Inference:  27%|██▋       | 54/200 [20:50<56:12, 23.10s/it]Running Inference:  28%|██▊       | 55/200 [21:13<55:43, 23.06s/it]Running Inference:  28%|██▊       | 56/200 [21:37<55:24, 23.09s/it]Running Inference:  28%|██▊       | 57/200 [22:00<55:07, 23.13s/it]Running Inference:  29%|██▉       | 58/200 [22:23<54:37, 23.08s/it]Running Inference:  30%|██▉       | 59/200 [22:46<54:14, 23.08s/it]Running Inference:  30%|███       | 60/200 [23:09<53:44, 23.04s/it]Running Inference:  30%|███       | 61/200 [23:32<53:25, 23.06s/it]Running Inference:  31%|███       | 62/200 [23:55<53:04, 23.08s/it]Running Inference:  32%|███▏      | 63/200 [24:18<52:38, 23.06s/it]Running Inference:  32%|███▏      | 64/200 [24:41<52:14, 23.05s/it]Running Inference:  32%|███▎      | 65/200 [25:05<52:09, 23.18s/it]Running Inference:  33%|███▎      | 66/200 [25:28<51:41, 23.15s/it]Running Inference:  34%|███▎      | 67/200 [25:51<51:37, 23.29s/it]Running Inference:  34%|███▍      | 68/200 [26:14<51:11, 23.27s/it]Running Inference:  34%|███▍      | 69/200 [26:38<50:40, 23.21s/it]Running Inference:  35%|███▌      | 70/200 [27:01<50:10, 23.16s/it]Running Inference:  36%|███▌      | 71/200 [27:24<49:44, 23.13s/it]Running Inference:  36%|███▌      | 72/200 [27:47<49:27, 23.18s/it]Running Inference:  36%|███▋      | 73/200 [28:10<48:56, 23.12s/it]Running Inference:  37%|███▋      | 74/200 [28:33<48:31, 23.11s/it]Running Inference:  38%|███▊      | 75/200 [28:56<48:05, 23.09s/it]Running Inference:  38%|███▊      | 76/200 [29:19<47:36, 23.04s/it]Running Inference:  38%|███▊      | 77/200 [29:42<47:15, 23.05s/it]Running Inference:  39%|███▉      | 78/200 [30:05<46:59, 23.11s/it]Running Inference:  40%|███▉      | 79/200 [30:28<46:32, 23.08s/it]Running Inference:  40%|████      | 80/200 [30:52<46:12, 23.11s/it]Running Inference:  40%|████      | 81/200 [31:14<45:44, 23.07s/it]Running Inference:  41%|████      | 82/200 [31:38<45:20, 23.06s/it]Running Inference:  42%|████▏     | 83/200 [32:01<44:58, 23.07s/it]Running Inference:  42%|████▏     | 84/200 [32:24<44:42, 23.12s/it]Running Inference:  42%|████▎     | 85/200 [32:47<44:20, 23.14s/it]Running Inference:  43%|████▎     | 86/200 [33:10<44:08, 23.23s/it]Running Inference:  44%|████▎     | 87/200 [33:33<43:35, 23.15s/it]Running Inference:  44%|████▍     | 88/200 [33:57<43:16, 23.19s/it]Running Inference:  44%|████▍     | 89/200 [34:20<42:59, 23.23s/it]Running Inference:  45%|████▌     | 90/200 [34:43<42:37, 23.25s/it]Running Inference:  46%|████▌     | 91/200 [35:06<42:10, 23.21s/it]Running Inference:  46%|████▌     | 92/200 [35:30<41:42, 23.17s/it]Running Inference:  46%|████▋     | 93/200 [35:53<41:13, 23.12s/it]Running Inference:  47%|████▋     | 94/200 [36:16<40:52, 23.14s/it]Running Inference:  48%|████▊     | 95/200 [36:39<40:32, 23.17s/it]Running Inference:  48%|████▊     | 96/200 [37:02<40:07, 23.15s/it]Running Inference:  48%|████▊     | 97/200 [37:25<39:44, 23.15s/it]Running Inference:  49%|████▉     | 98/200 [37:48<39:22, 23.16s/it]Running Inference:  50%|████▉     | 99/200 [38:13<39:41, 23.58s/it]Running Inference:  50%|█████     | 100/200 [38:36<39:05, 23.45s/it]Running Inference:  50%|█████     | 101/200 [38:59<38:33, 23.37s/it]Running Inference:  51%|█████     | 102/200 [39:22<38:00, 23.27s/it]Running Inference:  52%|█████▏    | 103/200 [39:46<37:46, 23.36s/it]Running Inference:  52%|█████▏    | 104/200 [40:09<37:28, 23.42s/it]Running Inference:  52%|█████▎    | 105/200 [40:33<36:58, 23.35s/it]Running Inference:  53%|█████▎    | 106/200 [40:56<36:27, 23.27s/it]Running Inference:  54%|█████▎    | 107/200 [41:19<35:59, 23.23s/it]Running Inference:  54%|█████▍    | 108/200 [41:42<35:35, 23.21s/it]Running Inference:  55%|█████▍    | 109/200 [42:05<35:07, 23.16s/it]Running Inference:  55%|█████▌    | 110/200 [42:28<34:39, 23.11s/it]Running Inference:  56%|█████▌    | 111/200 [42:51<34:16, 23.11s/it]Running Inference:  56%|█████▌    | 112/200 [43:14<33:51, 23.09s/it]Running Inference:  56%|█████▋    | 113/200 [43:37<33:33, 23.15s/it]Running Inference:  57%|█████▋    | 114/200 [44:00<33:05, 23.08s/it]Running Inference:  57%|█████▊    | 115/200 [44:23<32:40, 23.07s/it]Running Inference:  58%|█████▊    | 116/200 [44:46<32:16, 23.06s/it]Running Inference:  58%|█████▊    | 117/200 [45:09<31:51, 23.03s/it]Running Inference:  59%|█████▉    | 118/200 [45:33<31:29, 23.04s/it]Running Inference:  60%|█████▉    | 119/200 [45:56<31:14, 23.15s/it]Running Inference:  60%|██████    | 120/200 [46:19<30:48, 23.11s/it]Running Inference:  60%|██████    | 121/200 [46:42<30:25, 23.10s/it]Running Inference:  61%|██████    | 122/200 [47:05<29:58, 23.06s/it]Running Inference:  62%|██████▏   | 123/200 [47:28<29:38, 23.09s/it]Running Inference:  62%|██████▏   | 124/200 [47:51<29:12, 23.05s/it]Running Inference:  62%|██████▎   | 125/200 [48:14<28:49, 23.05s/it]Running Inference:  63%|██████▎   | 126/200 [48:37<28:23, 23.02s/it]Running Inference:  64%|██████▎   | 127/200 [49:00<28:02, 23.05s/it]Running Inference:  64%|██████▍   | 128/200 [49:24<27:58, 23.32s/it]Running Inference:  64%|██████▍   | 129/200 [49:47<27:25, 23.18s/it]Running Inference:  65%|██████▌   | 130/200 [50:10<26:54, 23.07s/it]Running Inference:  66%|██████▌   | 131/200 [50:34<27:00, 23.48s/it]Running Inference:  66%|██████▌   | 132/200 [50:57<26:26, 23.33s/it]Running Inference:  66%|██████▋   | 133/200 [51:20<25:54, 23.21s/it]Running Inference:  67%|██████▋   | 134/200 [51:43<25:27, 23.14s/it]Running Inference:  68%|██████▊   | 135/200 [52:07<25:13, 23.28s/it]Running Inference:  68%|██████▊   | 136/200 [52:30<24:47, 23.25s/it]Running Inference:  68%|██████▊   | 137/200 [52:53<24:22, 23.21s/it]Running Inference:  69%|██████▉   | 138/200 [53:16<23:53, 23.12s/it]Running Inference:  70%|██████▉   | 139/200 [53:39<23:26, 23.06s/it]Running Inference:  70%|███████   | 140/200 [54:02<23:02, 23.05s/it]Running Inference:  70%|███████   | 141/200 [54:25<22:40, 23.06s/it]Running Inference:  71%|███████   | 142/200 [54:48<22:16, 23.04s/it]Running Inference:  72%|███████▏  | 143/200 [55:11<21:52, 23.02s/it]Running Inference:  72%|███████▏  | 144/200 [55:34<21:26, 22.97s/it]Running Inference:  72%|███████▎  | 145/200 [55:57<21:04, 22.98s/it]Running Inference:  73%|███████▎  | 146/200 [56:20<20:40, 22.97s/it]Running Inference:  74%|███████▎  | 147/200 [56:43<20:17, 22.96s/it]Running Inference:  74%|███████▍  | 148/200 [57:06<19:51, 22.92s/it]Running Inference:  74%|███████▍  | 149/200 [57:28<19:28, 22.90s/it]Running Inference:  75%|███████▌  | 150/200 [57:52<19:11, 23.03s/it]Running Inference:  76%|███████▌  | 151/200 [58:15<18:47, 23.01s/it]Running Inference:  76%|███████▌  | 152/200 [58:38<18:25, 23.02s/it]Running Inference:  76%|███████▋  | 153/200 [59:01<18:05, 23.09s/it]Running Inference:  77%|███████▋  | 154/200 [59:24<17:41, 23.07s/it]Running Inference:  78%|███████▊  | 155/200 [59:47<17:20, 23.12s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:10<16:54, 23.06s/it]Running Inference:  78%|███████▊  | 157/200 [1:00:33<16:34, 23.12s/it]Running Inference:  79%|███████▉  | 158/200 [1:00:57<16:12, 23.15s/it]Running Inference:  80%|███████▉  | 159/200 [1:01:20<15:46, 23.08s/it]Running Inference:  80%|████████  | 160/200 [1:01:43<15:25, 23.14s/it]Running Inference:  80%|████████  | 161/200 [1:02:06<15:00, 23.09s/it]Running Inference:  81%|████████  | 162/200 [1:02:29<14:35, 23.04s/it]Running Inference:  82%|████████▏ | 163/200 [1:02:52<14:11, 23.02s/it]Running Inference:  82%|████████▏ | 164/200 [1:03:15<13:51, 23.08s/it]Running Inference:  82%|████████▎ | 165/200 [1:03:38<13:26, 23.03s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:01<13:02, 23.01s/it]Running Inference:  84%|████████▎ | 167/200 [1:04:24<12:39, 23.00s/it]Running Inference:  84%|████████▍ | 168/200 [1:04:47<12:15, 22.98s/it]Running Inference:  84%|████████▍ | 169/200 [1:05:10<11:53, 23.02s/it]Running Inference:  85%|████████▌ | 170/200 [1:05:33<11:32, 23.07s/it]Running Inference:  86%|████████▌ | 171/200 [1:05:56<11:09, 23.09s/it]Running Inference:  86%|████████▌ | 172/200 [1:06:19<10:46, 23.10s/it]Running Inference:  86%|████████▋ | 173/200 [1:06:42<10:22, 23.04s/it]Running Inference:  87%|████████▋ | 174/200 [1:07:05<09:58, 23.03s/it]Running Inference:  88%|████████▊ | 175/200 [1:07:28<09:35, 23.00s/it]Running Inference:  88%|████████▊ | 176/200 [1:07:51<09:11, 22.98s/it]Running Inference:  88%|████████▊ | 177/200 [1:08:14<08:49, 23.03s/it]Running Inference:  89%|████████▉ | 178/200 [1:08:38<08:29, 23.16s/it]Running Inference:  90%|████████▉ | 179/200 [1:09:01<08:05, 23.11s/it]Running Inference:  90%|█████████ | 180/200 [1:09:24<07:41, 23.06s/it]Running Inference:  90%|█████████ | 181/200 [1:09:47<07:17, 23.04s/it]Running Inference:  91%|█████████ | 182/200 [1:10:10<06:54, 23.00s/it]Running Inference:  92%|█████████▏| 183/200 [1:10:32<06:30, 22.97s/it]Running Inference:  92%|█████████▏| 184/200 [1:10:57<06:13, 23.34s/it]Running Inference:  92%|█████████▎| 185/200 [1:11:20<05:49, 23.27s/it]Running Inference:  93%|█████████▎| 186/200 [1:11:43<05:24, 23.18s/it]Running Inference:  94%|█████████▎| 187/200 [1:12:06<05:00, 23.11s/it]Running Inference:  94%|█████████▍| 188/200 [1:12:29<04:36, 23.07s/it]Running Inference:  94%|█████████▍| 189/200 [1:12:52<04:13, 23.09s/it]Running Inference:  95%|█████████▌| 190/200 [1:13:15<03:50, 23.06s/it]Running Inference:  96%|█████████▌| 191/200 [1:13:38<03:28, 23.14s/it]Running Inference:  96%|█████████▌| 192/200 [1:14:01<03:05, 23.16s/it]Running Inference:  96%|█████████▋| 193/200 [1:14:24<02:42, 23.17s/it]Running Inference:  97%|█████████▋| 194/200 [1:14:47<02:18, 23.10s/it]Running Inference:  98%|█████████▊| 195/200 [1:15:11<01:55, 23.11s/it]Running Inference:  98%|█████████▊| 196/200 [1:15:34<01:32, 23.20s/it]Running Inference:  98%|█████████▊| 197/200 [1:15:57<01:09, 23.15s/it]Running Inference:  99%|█████████▉| 198/200 [1:16:20<00:46, 23.09s/it]Running Inference: 100%|█████████▉| 199/200 [1:16:43<00:23, 23.04s/it]Running Inference: 100%|██████████| 200/200 [1:17:06<00:00, 23.10s/it]Running Inference: 100%|██████████| 200/200 [1:17:06<00:00, 23.13s/it]
2025-12-14 06:41:29,323 - INFO - Inference completed.
2025-12-14 06:41:29,344 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 06:41:29,345 - INFO - Calculating metrics for dataset: longbench
2025-12-14 06:41:36,503 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 06:41:36,504 - INFO - Metrics:
6.26
2025-12-14 06:41:36,505 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multi_news, ratio=0.3) on GPU 4

----------------------------------------
Task: multi_news | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 06:41:43,408 - INFO - Set deterministic seeds to 42
2025-12-14 06:41:43,408 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 06:41:43,408 - INFO - Starting evaluation run...
2025-12-14 06:41:43,408 - INFO - Output directory set to: longbenchresult
2025-12-14 06:41:43,408 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 06:41:43,408 - INFO - KV Press 'knorm' setup.
2025-12-14 06:41:43,408 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 06:41:43,408 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.11it/s]
Device set to use cuda:0
2025-12-14 06:41:55,378 - INFO - Model pipeline loaded.
2025-12-14 06:41:55,378 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 06:41:59,561 - INFO - Dataset loaded with 200 entries.
2025-12-14 06:41:59,561 - INFO - Dataset processed with 200 entries.
2025-12-14 06:41:59,571 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:18:48, 23.76s/it]Running Inference:   1%|          | 2/200 [00:46<1:16:37, 23.22s/it]Running Inference:   2%|▏         | 3/200 [01:09<1:15:46, 23.08s/it]Running Inference:   2%|▏         | 4/200 [01:32<1:15:08, 23.00s/it]Running Inference:   2%|▎         | 5/200 [01:55<1:14:37, 22.96s/it]Running Inference:   3%|▎         | 6/200 [01:57<51:04, 15.80s/it]  Running Inference:   4%|▎         | 7/200 [02:20<58:34, 18.21s/it]Running Inference:   4%|▍         | 8/200 [02:43<1:03:08, 19.73s/it]Running Inference:   4%|▍         | 9/200 [03:06<1:05:58, 20.73s/it]Running Inference:   5%|▌         | 10/200 [03:29<1:07:41, 21.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:51<1:08:40, 21.80s/it]Running Inference:   6%|▌         | 12/200 [04:14<1:09:24, 22.15s/it]Running Inference:   6%|▋         | 13/200 [04:37<1:09:46, 22.39s/it]Running Inference:   7%|▋         | 14/200 [05:00<1:09:47, 22.51s/it]Running Inference:   8%|▊         | 15/200 [05:23<1:10:00, 22.71s/it]Running Inference:   8%|▊         | 16/200 [05:46<1:09:52, 22.79s/it]Running Inference:   8%|▊         | 17/200 [06:09<1:09:36, 22.82s/it]Running Inference:   9%|▉         | 18/200 [06:33<1:09:52, 23.04s/it]Running Inference:  10%|▉         | 19/200 [06:56<1:09:22, 23.00s/it]Running Inference:  10%|█         | 20/200 [07:18<1:08:47, 22.93s/it]Running Inference:  10%|█         | 21/200 [07:41<1:08:26, 22.94s/it]Running Inference:  11%|█         | 22/200 [08:04<1:08:01, 22.93s/it]Running Inference:  12%|█▏        | 23/200 [08:27<1:07:34, 22.91s/it]Running Inference:  12%|█▏        | 24/200 [08:50<1:07:20, 22.96s/it]Running Inference:  12%|█▎        | 25/200 [09:13<1:06:54, 22.94s/it]Running Inference:  13%|█▎        | 26/200 [09:36<1:06:30, 22.93s/it]Running Inference:  14%|█▎        | 27/200 [10:00<1:07:14, 23.32s/it]Running Inference:  14%|█▍        | 28/200 [10:23<1:06:51, 23.32s/it]Running Inference:  14%|█▍        | 29/200 [10:46<1:06:02, 23.17s/it]Running Inference:  15%|█▌        | 30/200 [11:09<1:05:23, 23.08s/it]Running Inference:  16%|█▌        | 31/200 [11:32<1:04:59, 23.07s/it]Running Inference:  16%|█▌        | 32/200 [11:55<1:04:25, 23.01s/it]Running Inference:  16%|█▋        | 33/200 [12:18<1:03:55, 22.96s/it]Running Inference:  17%|█▋        | 34/200 [12:41<1:03:34, 22.98s/it]Running Inference:  18%|█▊        | 35/200 [13:04<1:03:07, 22.95s/it]Running Inference:  18%|█▊        | 36/200 [13:27<1:02:45, 22.96s/it]Running Inference:  18%|█▊        | 37/200 [13:50<1:02:37, 23.05s/it]Running Inference:  19%|█▉        | 38/200 [14:14<1:02:35, 23.18s/it]Running Inference:  20%|█▉        | 39/200 [14:36<1:01:59, 23.10s/it]Running Inference:  20%|██        | 40/200 [14:59<1:01:22, 23.02s/it]Running Inference:  20%|██        | 41/200 [15:22<1:00:56, 23.00s/it]Running Inference:  21%|██        | 42/200 [15:45<1:00:33, 23.00s/it]Running Inference:  22%|██▏       | 43/200 [16:08<1:00:01, 22.94s/it]Running Inference:  22%|██▏       | 44/200 [16:31<59:37, 22.93s/it]  Running Inference:  22%|██▎       | 45/200 [16:54<59:15, 22.94s/it]Running Inference:  23%|██▎       | 46/200 [17:17<58:47, 22.91s/it]Running Inference:  24%|██▎       | 47/200 [17:40<58:41, 23.01s/it]Running Inference:  24%|██▍       | 48/200 [18:03<58:23, 23.05s/it]Running Inference:  24%|██▍       | 49/200 [18:26<57:51, 22.99s/it]Running Inference:  25%|██▌       | 50/200 [18:49<57:30, 23.00s/it]Running Inference:  26%|██▌       | 51/200 [19:12<57:03, 22.98s/it]Running Inference:  26%|██▌       | 52/200 [19:35<56:40, 22.98s/it]Running Inference:  26%|██▋       | 53/200 [19:58<56:10, 22.93s/it]Running Inference:  27%|██▋       | 54/200 [20:21<55:46, 22.92s/it]Running Inference:  28%|██▊       | 55/200 [20:44<55:26, 22.94s/it]Running Inference:  28%|██▊       | 56/200 [21:07<55:04, 22.95s/it]Running Inference:  28%|██▊       | 57/200 [21:30<54:44, 22.97s/it]Running Inference:  29%|██▉       | 58/200 [21:52<54:14, 22.92s/it]Running Inference:  30%|██▉       | 59/200 [22:15<53:51, 22.92s/it]Running Inference:  30%|███       | 60/200 [22:38<53:25, 22.90s/it]Running Inference:  30%|███       | 61/200 [23:01<53:05, 22.91s/it]Running Inference:  31%|███       | 62/200 [23:24<52:42, 22.92s/it]Running Inference:  32%|███▏      | 63/200 [23:47<52:19, 22.92s/it]Running Inference:  32%|███▏      | 64/200 [24:10<51:52, 22.89s/it]Running Inference:  32%|███▎      | 65/200 [24:33<51:46, 23.01s/it]Running Inference:  33%|███▎      | 66/200 [24:56<51:18, 22.98s/it]Running Inference:  34%|███▎      | 67/200 [25:19<51:12, 23.10s/it]Running Inference:  34%|███▍      | 68/200 [25:42<50:45, 23.07s/it]Running Inference:  34%|███▍      | 69/200 [26:05<50:16, 23.03s/it]Running Inference:  35%|███▌      | 70/200 [26:28<49:48, 22.98s/it]Running Inference:  36%|███▌      | 71/200 [26:51<49:21, 22.96s/it]Running Inference:  36%|███▌      | 72/200 [27:14<49:06, 23.02s/it]Running Inference:  36%|███▋      | 73/200 [27:37<48:34, 22.95s/it]Running Inference:  37%|███▋      | 74/200 [28:00<48:09, 22.94s/it]Running Inference:  38%|███▊      | 75/200 [28:23<47:44, 22.92s/it]Running Inference:  38%|███▊      | 76/200 [28:46<47:23, 22.93s/it]Running Inference:  38%|███▊      | 77/200 [29:09<46:58, 22.92s/it]Running Inference:  39%|███▉      | 78/200 [29:32<46:35, 22.91s/it]Running Inference:  40%|███▉      | 79/200 [29:55<46:14, 22.93s/it]Running Inference:  40%|████      | 80/200 [30:18<45:53, 22.94s/it]Running Inference:  40%|████      | 81/200 [30:40<45:29, 22.93s/it]Running Inference:  41%|████      | 82/200 [31:03<44:54, 22.83s/it]Running Inference:  42%|████▏     | 83/200 [31:26<44:26, 22.79s/it]Running Inference:  42%|████▏     | 84/200 [31:49<44:04, 22.79s/it]Running Inference:  42%|████▎     | 85/200 [32:11<43:36, 22.75s/it]Running Inference:  43%|████▎     | 86/200 [32:34<43:19, 22.80s/it]Running Inference:  44%|████▎     | 87/200 [32:57<42:53, 22.78s/it]Running Inference:  44%|████▍     | 88/200 [33:20<42:34, 22.80s/it]Running Inference:  44%|████▍     | 89/200 [33:43<42:14, 22.83s/it]Running Inference:  45%|████▌     | 90/200 [34:05<41:51, 22.83s/it]Running Inference:  46%|████▌     | 91/200 [34:28<41:23, 22.78s/it]Running Inference:  46%|████▌     | 92/200 [34:51<40:56, 22.75s/it]Running Inference:  46%|████▋     | 93/200 [35:13<40:32, 22.73s/it]Running Inference:  47%|████▋     | 94/200 [35:36<40:08, 22.72s/it]Running Inference:  48%|████▊     | 95/200 [35:59<39:46, 22.72s/it]Running Inference:  48%|████▊     | 96/200 [36:22<39:21, 22.71s/it]Running Inference:  48%|████▊     | 97/200 [36:44<38:58, 22.71s/it]Running Inference:  49%|████▉     | 98/200 [37:07<38:36, 22.71s/it]Running Inference:  50%|████▉     | 99/200 [37:31<38:55, 23.12s/it]Running Inference:  50%|█████     | 100/200 [37:54<38:18, 22.99s/it]Running Inference:  50%|█████     | 101/200 [38:16<37:48, 22.91s/it]Running Inference:  51%|█████     | 102/200 [38:39<37:15, 22.81s/it]Running Inference:  52%|█████▏    | 103/200 [39:02<37:01, 22.90s/it]Running Inference:  52%|█████▏    | 104/200 [39:25<36:43, 22.95s/it]Running Inference:  52%|█████▎    | 105/200 [39:48<36:13, 22.88s/it]Running Inference:  53%|█████▎    | 106/200 [40:11<35:44, 22.81s/it]Running Inference:  54%|█████▎    | 107/200 [40:33<35:17, 22.77s/it]Running Inference:  54%|█████▍    | 108/200 [40:56<34:52, 22.75s/it]Running Inference:  55%|█████▍    | 109/200 [41:19<34:25, 22.70s/it]Running Inference:  55%|█████▌    | 110/200 [41:41<33:57, 22.64s/it]Running Inference:  56%|█████▌    | 111/200 [42:04<33:35, 22.65s/it]Running Inference:  56%|█████▌    | 112/200 [42:26<33:11, 22.63s/it]Running Inference:  56%|█████▋    | 113/200 [42:49<32:54, 22.69s/it]Running Inference:  57%|█████▋    | 114/200 [43:12<32:32, 22.70s/it]Running Inference:  57%|█████▊    | 115/200 [43:34<32:07, 22.68s/it]Running Inference:  58%|█████▊    | 116/200 [43:57<31:42, 22.64s/it]Running Inference:  58%|█████▊    | 117/200 [44:20<31:16, 22.61s/it]Running Inference:  59%|█████▉    | 118/200 [44:42<30:56, 22.64s/it]Running Inference:  60%|█████▉    | 119/200 [45:05<30:38, 22.69s/it]Running Inference:  60%|██████    | 120/200 [45:28<30:09, 22.62s/it]Running Inference:  60%|██████    | 121/200 [45:50<29:44, 22.59s/it]Running Inference:  61%|██████    | 122/200 [46:12<29:16, 22.52s/it]Running Inference:  62%|██████▏   | 123/200 [46:35<28:55, 22.54s/it]Running Inference:  62%|██████▏   | 124/200 [46:57<28:30, 22.50s/it]Running Inference:  62%|██████▎   | 125/200 [47:20<28:06, 22.49s/it]Running Inference:  63%|██████▎   | 126/200 [47:42<27:43, 22.47s/it]Running Inference:  64%|██████▎   | 127/200 [48:05<27:21, 22.48s/it]Running Inference:  64%|██████▍   | 128/200 [48:28<27:17, 22.75s/it]Running Inference:  64%|██████▍   | 129/200 [48:51<26:51, 22.69s/it]Running Inference:  65%|██████▌   | 130/200 [49:13<26:23, 22.61s/it]Running Inference:  66%|██████▌   | 131/200 [49:37<26:27, 23.01s/it]Running Inference:  66%|██████▌   | 132/200 [50:00<25:54, 22.86s/it]Running Inference:  66%|██████▋   | 133/200 [50:22<25:23, 22.73s/it]Running Inference:  67%|██████▋   | 134/200 [50:45<24:55, 22.66s/it]Running Inference:  68%|██████▊   | 135/200 [51:08<24:40, 22.78s/it]Running Inference:  68%|██████▊   | 136/200 [51:30<24:17, 22.77s/it]Running Inference:  68%|██████▊   | 137/200 [51:53<23:50, 22.71s/it]Running Inference:  69%|██████▉   | 138/200 [52:15<23:22, 22.62s/it]Running Inference:  70%|██████▉   | 139/200 [52:38<22:56, 22.56s/it]Running Inference:  70%|███████   | 140/200 [53:00<22:32, 22.55s/it]Running Inference:  70%|███████   | 141/200 [53:23<22:10, 22.55s/it]Running Inference:  71%|███████   | 142/200 [53:45<21:46, 22.53s/it]Running Inference:  72%|███████▏  | 143/200 [54:08<21:23, 22.51s/it]Running Inference:  72%|███████▏  | 144/200 [54:30<21:00, 22.50s/it]Running Inference:  72%|███████▎  | 145/200 [54:53<20:36, 22.49s/it]Running Inference:  73%|███████▎  | 146/200 [55:15<20:13, 22.48s/it]Running Inference:  74%|███████▎  | 147/200 [55:38<19:51, 22.47s/it]Running Inference:  74%|███████▍  | 148/200 [56:00<19:30, 22.51s/it]Running Inference:  74%|███████▍  | 149/200 [56:23<19:05, 22.46s/it]Running Inference:  75%|███████▌  | 150/200 [56:45<18:48, 22.58s/it]Running Inference:  76%|███████▌  | 151/200 [57:08<18:24, 22.55s/it]Running Inference:  76%|███████▌  | 152/200 [57:30<18:01, 22.54s/it]Running Inference:  76%|███████▋  | 153/200 [57:53<17:42, 22.60s/it]Running Inference:  77%|███████▋  | 154/200 [58:16<17:19, 22.59s/it]Running Inference:  78%|███████▊  | 155/200 [58:39<16:59, 22.65s/it]Running Inference:  78%|███████▊  | 156/200 [59:01<16:34, 22.60s/it]Running Inference:  78%|███████▊  | 157/200 [59:24<16:13, 22.65s/it]Running Inference:  79%|███████▉  | 158/200 [59:46<15:51, 22.66s/it]Running Inference:  80%|███████▉  | 159/200 [1:00:09<15:26, 22.60s/it]Running Inference:  80%|████████  | 160/200 [1:00:32<15:06, 22.66s/it]Running Inference:  80%|████████  | 161/200 [1:00:54<14:41, 22.61s/it]Running Inference:  81%|████████  | 162/200 [1:01:17<14:17, 22.58s/it]Running Inference:  82%|████████▏ | 163/200 [1:01:39<13:54, 22.55s/it]Running Inference:  82%|████████▏ | 164/200 [1:02:02<13:33, 22.60s/it]Running Inference:  82%|████████▎ | 165/200 [1:02:24<13:09, 22.55s/it]Running Inference:  83%|████████▎ | 166/200 [1:02:47<12:46, 22.55s/it]Running Inference:  84%|████████▎ | 167/200 [1:03:09<12:23, 22.53s/it]Running Inference:  84%|████████▍ | 168/200 [1:03:32<11:59, 22.49s/it]Running Inference:  84%|████████▍ | 169/200 [1:03:54<11:37, 22.51s/it]Running Inference:  85%|████████▌ | 170/200 [1:04:17<11:16, 22.55s/it]Running Inference:  86%|████████▌ | 171/200 [1:04:40<10:53, 22.55s/it]Running Inference:  86%|████████▌ | 172/200 [1:05:02<10:31, 22.54s/it]Running Inference:  86%|████████▋ | 173/200 [1:05:25<10:08, 22.52s/it]Running Inference:  87%|████████▋ | 174/200 [1:05:47<09:45, 22.51s/it]Running Inference:  88%|████████▊ | 175/200 [1:06:09<09:21, 22.48s/it]Running Inference:  88%|████████▊ | 176/200 [1:06:32<08:59, 22.47s/it]Running Inference:  88%|████████▊ | 177/200 [1:06:54<08:37, 22.49s/it]Running Inference:  89%|████████▉ | 178/200 [1:07:17<08:17, 22.60s/it]Running Inference:  90%|████████▉ | 179/200 [1:07:40<07:53, 22.55s/it]Running Inference:  90%|█████████ | 180/200 [1:08:02<07:30, 22.51s/it]Running Inference:  90%|█████████ | 181/200 [1:08:25<07:07, 22.48s/it]Running Inference:  91%|█████████ | 182/200 [1:08:47<06:43, 22.44s/it]Running Inference:  92%|█████████▏| 183/200 [1:09:09<06:21, 22.41s/it]Running Inference:  92%|█████████▏| 184/200 [1:09:33<06:04, 22.76s/it]Running Inference:  92%|█████████▎| 185/200 [1:09:55<05:40, 22.68s/it]Running Inference:  93%|█████████▎| 186/200 [1:10:18<05:16, 22.60s/it]Running Inference:  94%|█████████▎| 187/200 [1:10:40<04:53, 22.54s/it]Running Inference:  94%|█████████▍| 188/200 [1:11:03<04:30, 22.53s/it]Running Inference:  94%|█████████▍| 189/200 [1:11:25<04:07, 22.50s/it]Running Inference:  95%|█████████▌| 190/200 [1:11:47<03:44, 22.48s/it]Running Inference:  96%|█████████▌| 191/200 [1:12:10<03:22, 22.55s/it]Running Inference:  96%|█████████▌| 192/200 [1:12:33<03:00, 22.58s/it]Running Inference:  96%|█████████▋| 193/200 [1:12:55<02:38, 22.60s/it]Running Inference:  97%|█████████▋| 194/200 [1:13:18<02:15, 22.54s/it]Running Inference:  98%|█████████▊| 195/200 [1:13:40<01:52, 22.54s/it]Running Inference:  98%|█████████▊| 196/200 [1:14:03<01:30, 22.61s/it]Running Inference:  98%|█████████▊| 197/200 [1:14:26<01:07, 22.55s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:48<00:45, 22.50s/it]Running Inference: 100%|█████████▉| 199/200 [1:15:10<00:22, 22.45s/it]Running Inference: 100%|██████████| 200/200 [1:15:33<00:00, 22.51s/it]Running Inference: 100%|██████████| 200/200 [1:15:33<00:00, 22.67s/it]
2025-12-14 07:57:33,035 - INFO - Inference completed.
2025-12-14 07:57:33,056 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 07:57:33,056 - INFO - Calculating metrics for dataset: longbench
2025-12-14 07:57:40,826 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 07:57:40,826 - INFO - Metrics:
5.17
2025-12-14 07:57:40,827 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multi_news, ratio=0.5) on GPU 4


========================================
LongBench Task: qmsum
========================================
----------------------------------------
Task: qmsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 07:57:47,657 - INFO - Set deterministic seeds to 42
2025-12-14 07:57:47,657 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 07:57:47,657 - INFO - Starting evaluation run...
2025-12-14 07:57:47,657 - INFO - Output directory set to: longbenchresult
2025-12-14 07:57:47,658 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 07:57:47,658 - INFO - KV Press 'knorm' setup.
2025-12-14 07:57:47,658 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 07:57:47,658 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.33it/s]
Device set to use cuda:0
2025-12-14 07:57:59,517 - INFO - Model pipeline loaded.
2025-12-14 07:57:59,518 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 07:58:03,147 - INFO - Dataset loaded with 200 entries.
2025-12-14 07:58:03,148 - INFO - Dataset processed with 200 entries.
2025-12-14 07:58:03,162 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:34<53:23, 94.21s/it]Running Inference:   6%|▌         | 2/35 [01:46<25:10, 45.78s/it]Running Inference:   9%|▊         | 3/35 [03:29<38:20, 71.90s/it]Running Inference:  11%|█▏        | 4/35 [04:13<31:35, 61.14s/it]Running Inference:  14%|█▍        | 5/35 [05:27<32:49, 65.64s/it]Running Inference:  17%|█▋        | 6/35 [06:39<32:52, 68.00s/it]Running Inference:  20%|██        | 7/35 [07:32<29:18, 62.80s/it]Running Inference:  23%|██▎       | 8/35 [09:26<35:40, 79.27s/it]Running Inference:  26%|██▌       | 9/35 [12:12<46:06, 106.39s/it]Running Inference:  29%|██▊       | 10/35 [13:48<43:00, 103.20s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [15:22<40:11, 100.48s/it]Running Inference:  34%|███▍      | 12/35 [16:37<35:27, 92.52s/it] Running Inference:  37%|███▋      | 13/35 [17:37<30:20, 82.76s/it]Running Inference:  40%|████      | 14/35 [18:52<28:08, 80.41s/it]Running Inference:  43%|████▎     | 15/35 [19:20<21:32, 64.62s/it]Running Inference:  46%|████▌     | 16/35 [20:15<19:32, 61.69s/it]Running Inference:  49%|████▊     | 17/35 [21:48<21:18, 71.02s/it]Running Inference:  51%|█████▏    | 18/35 [22:48<19:11, 67.74s/it]Running Inference:  54%|█████▍    | 19/35 [23:30<15:59, 59.94s/it]Running Inference:  57%|█████▋    | 20/35 [24:05<13:07, 52.48s/it]Running Inference:  60%|██████    | 21/35 [24:38<10:54, 46.72s/it]Running Inference:  63%|██████▎   | 22/35 [25:13<09:21, 43.19s/it]Running Inference:  66%|██████▌   | 23/35 [26:31<10:44, 53.67s/it]Running Inference:  69%|██████▊   | 24/35 [28:32<13:34, 74.02s/it]Running Inference:  71%|███████▏  | 25/35 [30:10<13:31, 81.14s/it]Running Inference:  74%|███████▍  | 26/35 [30:49<10:14, 68.28s/it]Running Inference:  77%|███████▋  | 27/35 [31:47<08:42, 65.36s/it]Running Inference:  80%|████████  | 28/35 [32:30<06:49, 58.50s/it]Running Inference:  83%|████████▎ | 29/35 [33:51<06:31, 65.29s/it]Running Inference:  86%|████████▌ | 30/35 [33:54<03:52, 46.57s/it]Running Inference:  89%|████████▊ | 31/35 [35:10<03:42, 55.57s/it]Running Inference:  91%|█████████▏| 32/35 [36:14<02:54, 58.10s/it]Running Inference:  94%|█████████▍| 33/35 [39:16<03:10, 95.08s/it]Running Inference:  97%|█████████▋| 34/35 [39:50<01:16, 76.86s/it]Running Inference: 100%|██████████| 35/35 [41:03<00:00, 75.87s/it]Running Inference: 100%|██████████| 35/35 [41:03<00:00, 70.40s/it]
2025-12-14 08:39:07,127 - INFO - Inference completed.
2025-12-14 08:39:07,142 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 08:39:07,142 - INFO - Calculating metrics for dataset: longbench
2025-12-14 08:39:08,649 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 08:39:08,649 - INFO - Metrics:
19.36
2025-12-14 08:39:08,651 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qmsum, ratio=0.1) on GPU 4

----------------------------------------
Task: qmsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 08:39:15,254 - INFO - Set deterministic seeds to 42
2025-12-14 08:39:15,254 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 08:39:15,254 - INFO - Starting evaluation run...
2025-12-14 08:39:15,254 - INFO - Output directory set to: longbenchresult
2025-12-14 08:39:15,254 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 08:39:15,254 - INFO - KV Press 'knorm' setup.
2025-12-14 08:39:15,254 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 08:39:15,255 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.92it/s]
Device set to use cuda:0
2025-12-14 08:39:27,321 - INFO - Model pipeline loaded.
2025-12-14 08:39:27,321 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 08:39:31,164 - INFO - Dataset loaded with 200 entries.
2025-12-14 08:39:31,165 - INFO - Dataset processed with 200 entries.
2025-12-14 08:39:31,179 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:32<52:21, 92.39s/it]Running Inference:   6%|▌         | 2/35 [02:02<30:37, 55.67s/it]Running Inference:   9%|▊         | 3/35 [03:00<30:19, 56.85s/it]Running Inference:  11%|█▏        | 4/35 [04:01<30:08, 58.34s/it]Running Inference:  14%|█▍        | 5/35 [05:33<35:12, 70.40s/it]Running Inference:  17%|█▋        | 6/35 [06:24<30:54, 63.96s/it]Running Inference:  20%|██        | 7/35 [07:16<28:05, 60.19s/it]Running Inference:  23%|██▎       | 8/35 [09:45<39:45, 88.35s/it]Running Inference:  26%|██▌       | 9/35 [12:47<50:55, 117.51s/it]Running Inference:  29%|██▊       | 10/35 [14:42<48:40, 116.83s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [16:47<47:46, 119.42s/it]Running Inference:  34%|███▍      | 12/35 [17:43<38:22, 100.10s/it]Running Inference:  37%|███▋      | 13/35 [19:03<34:28, 94.02s/it] Running Inference:  40%|████      | 14/35 [20:19<30:55, 88.37s/it]Running Inference:  43%|████▎     | 15/35 [20:47<23:23, 70.18s/it]Running Inference:  46%|████▌     | 16/35 [21:43<20:55, 66.06s/it]Running Inference:  49%|████▊     | 17/35 [23:15<22:07, 73.74s/it]Running Inference:  51%|█████▏    | 18/35 [24:53<23:00, 81.22s/it]Running Inference:  54%|█████▍    | 19/35 [26:09<21:14, 79.69s/it]Running Inference:  57%|█████▋    | 20/35 [26:45<16:35, 66.38s/it]Running Inference:  60%|██████    | 21/35 [27:36<14:26, 61.88s/it]Running Inference:  63%|██████▎   | 22/35 [28:27<12:42, 58.66s/it]Running Inference:  66%|██████▌   | 23/35 [29:43<12:45, 63.80s/it]Running Inference:  69%|██████▊   | 24/35 [32:02<15:50, 86.38s/it]Running Inference:  71%|███████▏  | 25/35 [33:20<13:57, 83.76s/it]Running Inference:  74%|███████▍  | 26/35 [34:17<11:21, 75.75s/it]Running Inference:  77%|███████▋  | 27/35 [35:14<09:20, 70.04s/it]Running Inference:  80%|████████  | 28/35 [36:50<09:05, 77.88s/it]Running Inference:  83%|████████▎ | 29/35 [38:31<08:30, 85.01s/it]Running Inference:  86%|████████▌ | 30/35 [38:36<05:04, 60.87s/it]Running Inference:  89%|████████▊ | 31/35 [40:12<04:46, 71.58s/it]Running Inference:  91%|█████████▏| 32/35 [41:35<03:44, 74.71s/it]Running Inference:  94%|█████████▍| 33/35 [44:51<03:42, 111.11s/it]Running Inference:  97%|█████████▋| 34/35 [46:04<01:39, 99.85s/it] Running Inference: 100%|██████████| 35/35 [47:57<00:00, 103.80s/it]Running Inference: 100%|██████████| 35/35 [47:57<00:00, 82.22s/it] 
2025-12-14 09:27:28,829 - INFO - Inference completed.
2025-12-14 09:27:28,844 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 09:27:28,844 - INFO - Calculating metrics for dataset: longbench
2025-12-14 09:27:30,588 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 09:27:30,588 - INFO - Metrics:
19.35
2025-12-14 09:27:30,590 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qmsum, ratio=0.2) on GPU 4

----------------------------------------
Task: qmsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 09:27:37,081 - INFO - Set deterministic seeds to 42
2025-12-14 09:27:37,082 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 09:27:37,082 - INFO - Starting evaluation run...
2025-12-14 09:27:37,082 - INFO - Output directory set to: longbenchresult
2025-12-14 09:27:37,082 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 09:27:37,082 - INFO - KV Press 'knorm' setup.
2025-12-14 09:27:37,082 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 09:27:37,082 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.03it/s]
Device set to use cuda:0
2025-12-14 09:27:47,919 - INFO - Model pipeline loaded.
2025-12-14 09:27:47,919 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 09:27:51,030 - INFO - Dataset loaded with 200 entries.
2025-12-14 09:27:51,030 - INFO - Dataset processed with 200 entries.
2025-12-14 09:27:51,045 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:35<53:52, 95.07s/it]Running Inference:   6%|▌         | 2/35 [02:46<44:30, 80.91s/it]Running Inference:   9%|▊         | 3/35 [04:04<42:27, 79.61s/it]Running Inference:  11%|█▏        | 4/35 [05:08<37:57, 73.45s/it]Running Inference:  14%|█▍        | 5/35 [06:42<40:28, 80.94s/it]Running Inference:  17%|█▋        | 6/35 [07:15<31:15, 64.67s/it]Running Inference:  20%|██        | 7/35 [08:08<28:26, 60.93s/it]Running Inference:  23%|██▎       | 8/35 [11:22<46:23, 103.09s/it]Running Inference:  26%|██▌       | 9/35 [14:27<55:50, 128.85s/it]Running Inference:  29%|██▊       | 10/35 [16:26<52:21, 125.65s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [19:37<58:19, 145.80s/it]Running Inference:  34%|███▍      | 12/35 [21:13<50:07, 130.75s/it]Running Inference:  37%|███▋      | 13/35 [23:15<46:54, 127.93s/it]Running Inference:  40%|████      | 14/35 [24:12<37:16, 106.50s/it]Running Inference:  43%|████▎     | 15/35 [25:02<29:51, 89.57s/it] Running Inference:  46%|████▌     | 16/35 [26:38<28:58, 91.50s/it]Running Inference:  49%|████▊     | 17/35 [28:12<27:39, 92.22s/it]Running Inference:  51%|█████▏    | 18/35 [30:13<28:34, 100.85s/it]Running Inference:  54%|█████▍    | 19/35 [31:30<25:00, 93.79s/it] Running Inference:  57%|█████▋    | 20/35 [32:26<20:37, 82.51s/it]Running Inference:  60%|██████    | 21/35 [33:38<18:29, 79.24s/it]Running Inference:  63%|██████▎   | 22/35 [34:51<16:46, 77.39s/it]Running Inference:  66%|██████▌   | 23/35 [36:52<18:05, 90.43s/it]Running Inference:  69%|██████▊   | 24/35 [38:55<18:20, 100.08s/it]Running Inference:  71%|███████▏  | 25/35 [40:13<15:35, 93.59s/it] Running Inference:  74%|███████▍  | 26/35 [40:50<11:28, 76.52s/it]Running Inference:  77%|███████▋  | 27/35 [43:09<12:43, 95.43s/it]Running Inference:  80%|████████  | 28/35 [45:10<12:00, 102.98s/it]Running Inference:  83%|████████▎ | 29/35 [46:52<10:15, 102.59s/it]Running Inference:  86%|████████▌ | 30/35 [47:15<06:33, 78.80s/it] Running Inference:  89%|████████▊ | 31/35 [48:53<05:38, 84.66s/it]Running Inference:  91%|█████████▏| 32/35 [49:52<03:51, 77.03s/it]Running Inference:  94%|█████████▍| 33/35 [52:46<03:31, 105.97s/it]Running Inference:  97%|█████████▋| 34/35 [53:41<01:30, 90.75s/it] Running Inference: 100%|██████████| 35/35 [54:55<00:00, 85.74s/it]Running Inference: 100%|██████████| 35/35 [54:55<00:00, 94.16s/it]
2025-12-14 10:22:46,752 - INFO - Inference completed.
2025-12-14 10:22:46,769 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 10:22:46,769 - INFO - Calculating metrics for dataset: longbench
2025-12-14 10:22:48,624 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 10:22:48,624 - INFO - Metrics:
18.72
2025-12-14 10:22:48,625 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qmsum, ratio=0.3) on GPU 4

----------------------------------------
Task: qmsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 10:22:55,200 - INFO - Set deterministic seeds to 42
2025-12-14 10:22:55,201 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 10:22:55,201 - INFO - Starting evaluation run...
2025-12-14 10:22:55,201 - INFO - Output directory set to: longbenchresult
2025-12-14 10:22:55,201 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 10:22:55,201 - INFO - KV Press 'knorm' setup.
2025-12-14 10:22:55,201 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 10:22:55,201 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.22it/s]
Device set to use cuda:0
2025-12-14 10:23:08,814 - INFO - Model pipeline loaded.
2025-12-14 10:23:08,815 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 10:23:12,728 - INFO - Dataset loaded with 200 entries.
2025-12-14 10:23:12,728 - INFO - Dataset processed with 200 entries.
2025-12-14 10:23:12,743 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:33<53:02, 93.61s/it]Running Inference:   6%|▌         | 2/35 [02:01<30:21, 55.20s/it]Running Inference:   9%|▊         | 3/35 [03:37<39:20, 73.77s/it]Running Inference:  11%|█▏        | 4/35 [04:57<39:13, 75.93s/it]Running Inference:  14%|█▍        | 5/35 [06:09<37:17, 74.60s/it]Running Inference:  17%|█▋        | 6/35 [07:43<39:13, 81.15s/it]Running Inference:  20%|██        | 7/35 [08:55<36:29, 78.19s/it]Running Inference:  23%|██▎       | 8/35 [11:07<42:52, 95.27s/it]Running Inference:  26%|██▌       | 9/35 [13:50<50:29, 116.53s/it]Running Inference:  29%|██▊       | 10/35 [15:49<48:55, 117.41s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [18:55<55:21, 138.39s/it]Running Inference:  34%|███▍      | 12/35 [19:48<43:05, 112.42s/it]Running Inference:  37%|███▋      | 13/35 [22:07<44:07, 120.33s/it]Running Inference:  40%|████      | 14/35 [23:20<37:09, 106.16s/it]Running Inference:  43%|████▎     | 15/35 [23:49<27:37, 82.89s/it] Running Inference:  46%|████▌     | 16/35 [25:03<25:23, 80.20s/it]Running Inference:  49%|████▊     | 17/35 [25:54<21:22, 71.27s/it]Running Inference:  51%|█████▏    | 18/35 [27:31<22:23, 79.04s/it]Running Inference:  54%|█████▍    | 19/35 [29:05<22:16, 83.52s/it]Running Inference:  57%|█████▋    | 20/35 [29:38<17:06, 68.43s/it]Running Inference:  60%|██████    | 21/35 [30:06<13:10, 56.45s/it]Running Inference:  63%|██████▎   | 22/35 [30:57<11:49, 54.60s/it]Running Inference:  66%|██████▌   | 23/35 [32:34<13:28, 67.37s/it]Running Inference:  69%|██████▊   | 24/35 [34:33<15:13, 83.02s/it]Running Inference:  71%|███████▏  | 25/35 [36:50<16:29, 98.99s/it]Running Inference:  74%|███████▍  | 26/35 [38:45<15:34, 103.89s/it]Running Inference:  77%|███████▋  | 27/35 [40:23<13:36, 102.11s/it]Running Inference:  80%|████████  | 28/35 [42:23<12:32, 107.54s/it]Running Inference:  83%|████████▎ | 29/35 [44:22<11:05, 110.88s/it]Running Inference:  86%|████████▌ | 30/35 [44:45<07:02, 84.47s/it] Running Inference:  89%|████████▊ | 31/35 [46:40<06:15, 93.78s/it]Running Inference:  91%|█████████▏| 32/35 [47:57<04:25, 88.66s/it]Running Inference:  94%|█████████▍| 33/35 [51:24<04:08, 124.10s/it]Running Inference:  97%|█████████▋| 34/35 [52:37<01:48, 108.98s/it]Running Inference: 100%|██████████| 35/35 [53:51<00:00, 98.23s/it] Running Inference: 100%|██████████| 35/35 [53:51<00:00, 92.31s/it]
2025-12-14 11:17:03,761 - INFO - Inference completed.
2025-12-14 11:17:03,777 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 11:17:03,777 - INFO - Calculating metrics for dataset: longbench
2025-12-14 11:17:05,802 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 11:17:05,802 - INFO - Metrics:
17.0
2025-12-14 11:17:05,804 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=qmsum, ratio=0.5) on GPU 4


========================================
LongBench Task: samsum
========================================
----------------------------------------
Task: samsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 11:17:12,312 - INFO - Set deterministic seeds to 42
2025-12-14 11:17:12,312 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 11:17:12,312 - INFO - Starting evaluation run...
2025-12-14 11:17:12,312 - INFO - Output directory set to: longbenchresult
2025-12-14 11:17:12,312 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 11:17:12,312 - INFO - KV Press 'knorm' setup.
2025-12-14 11:17:12,312 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 11:17:12,312 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.69it/s]
Device set to use cuda:0
2025-12-14 11:17:24,552 - INFO - Model pipeline loaded.
2025-12-14 11:17:24,552 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 11:17:29,273 - INFO - Dataset loaded with 200 entries.
2025-12-14 11:17:29,273 - INFO - Dataset processed with 200 entries.
2025-12-14 11:17:29,299 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:11,  2.77s/it]Running Inference:   1%|          | 2/200 [00:06<10:36,  3.21s/it]Running Inference:   2%|▏         | 3/200 [00:12<15:31,  4.73s/it]Running Inference:   2%|▏         | 4/200 [00:19<18:26,  5.64s/it]Running Inference:   2%|▎         | 5/200 [00:20<12:55,  3.98s/it]Running Inference:   3%|▎         | 6/200 [00:28<16:21,  5.06s/it]Running Inference:   4%|▎         | 7/200 [00:29<12:50,  3.99s/it]Running Inference:   4%|▍         | 8/200 [00:32<11:08,  3.48s/it]Running Inference:   4%|▍         | 9/200 [00:39<14:23,  4.52s/it]Running Inference:   5%|▌         | 10/200 [00:47<17:42,  5.59s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:49<15:00,  4.76s/it]Running Inference:   6%|▌         | 12/200 [00:51<12:12,  3.90s/it]Running Inference:   6%|▋         | 13/200 [00:53<10:08,  3.25s/it]Running Inference:   7%|▋         | 14/200 [01:01<14:31,  4.69s/it]Running Inference:   8%|▊         | 15/200 [01:04<12:36,  4.09s/it]Running Inference:   8%|▊         | 16/200 [01:11<15:05,  4.92s/it]Running Inference:   8%|▊         | 17/200 [01:19<17:51,  5.86s/it]Running Inference:   9%|▉         | 18/200 [01:26<18:51,  6.22s/it]Running Inference:  10%|▉         | 19/200 [01:28<15:27,  5.12s/it]Running Inference:  10%|█         | 20/200 [01:30<11:57,  3.99s/it]Running Inference:  10%|█         | 21/200 [01:32<10:24,  3.49s/it]Running Inference:  11%|█         | 22/200 [01:37<11:27,  3.86s/it]Running Inference:  12%|█▏        | 23/200 [01:44<14:33,  4.94s/it]Running Inference:  12%|█▏        | 24/200 [01:46<11:42,  3.99s/it]Running Inference:  12%|█▎        | 25/200 [01:49<10:30,  3.60s/it]Running Inference:  13%|█▎        | 26/200 [01:56<13:36,  4.69s/it]Running Inference:  14%|█▎        | 27/200 [01:59<12:32,  4.35s/it]Running Inference:  14%|█▍        | 28/200 [02:02<11:13,  3.92s/it]Running Inference:  14%|█▍        | 29/200 [02:03<08:36,  3.02s/it]Running Inference:  15%|█▌        | 30/200 [02:05<07:41,  2.72s/it]Running Inference:  16%|█▌        | 31/200 [02:07<07:03,  2.50s/it]Running Inference:  16%|█▌        | 32/200 [02:11<08:04,  2.89s/it]Running Inference:  16%|█▋        | 33/200 [02:18<11:02,  3.97s/it]Running Inference:  17%|█▋        | 34/200 [02:25<14:12,  5.13s/it]Running Inference:  18%|█▊        | 35/200 [02:28<12:02,  4.38s/it]Running Inference:  18%|█▊        | 36/200 [02:31<10:58,  4.01s/it]Running Inference:  18%|█▊        | 37/200 [02:33<09:27,  3.48s/it]Running Inference:  19%|█▉        | 38/200 [02:36<08:48,  3.26s/it]Running Inference:  20%|█▉        | 39/200 [02:40<09:01,  3.36s/it]Running Inference:  20%|██        | 40/200 [02:43<08:28,  3.18s/it]Running Inference:  20%|██        | 41/200 [02:44<07:20,  2.77s/it]Running Inference:  21%|██        | 42/200 [02:51<10:27,  3.97s/it]Running Inference:  22%|██▏       | 43/200 [02:53<08:58,  3.43s/it]Running Inference:  22%|██▏       | 44/200 [03:01<12:11,  4.69s/it]Running Inference:  22%|██▎       | 45/200 [03:03<10:24,  4.03s/it]Running Inference:  23%|██▎       | 46/200 [03:11<13:06,  5.11s/it]Running Inference:  24%|██▎       | 47/200 [03:14<11:08,  4.37s/it]Running Inference:  24%|██▍       | 48/200 [03:16<09:49,  3.88s/it]Running Inference:  24%|██▍       | 49/200 [03:24<12:40,  5.04s/it]Running Inference:  25%|██▌       | 50/200 [03:26<10:07,  4.05s/it]Running Inference:  26%|██▌       | 51/200 [03:28<08:26,  3.40s/it]Running Inference:  26%|██▌       | 52/200 [03:36<11:46,  4.77s/it]Running Inference:  26%|██▋       | 53/200 [03:43<13:44,  5.61s/it]Running Inference:  27%|██▋       | 54/200 [03:45<10:34,  4.34s/it]Running Inference:  28%|██▊       | 55/200 [03:47<09:14,  3.82s/it]Running Inference:  28%|██▊       | 56/200 [03:55<11:40,  4.86s/it]Running Inference:  28%|██▊       | 57/200 [03:58<10:19,  4.33s/it]Running Inference:  29%|██▉       | 58/200 [04:06<12:57,  5.48s/it]Running Inference:  30%|██▉       | 59/200 [04:07<09:48,  4.17s/it]Running Inference:  30%|███       | 60/200 [04:14<11:29,  4.93s/it]Running Inference:  30%|███       | 61/200 [04:15<08:46,  3.79s/it]Running Inference:  31%|███       | 62/200 [04:18<08:17,  3.61s/it]Running Inference:  32%|███▏      | 63/200 [04:25<10:26,  4.58s/it]Running Inference:  32%|███▏      | 64/200 [04:28<09:24,  4.15s/it]Running Inference:  32%|███▎      | 65/200 [04:31<08:49,  3.92s/it]Running Inference:  33%|███▎      | 66/200 [04:34<07:45,  3.47s/it]Running Inference:  34%|███▎      | 67/200 [04:36<06:37,  2.99s/it]Running Inference:  34%|███▍      | 68/200 [04:42<08:58,  4.08s/it]Running Inference:  34%|███▍      | 69/200 [04:44<07:29,  3.43s/it]Running Inference:  35%|███▌      | 70/200 [04:46<06:22,  2.95s/it]Running Inference:  36%|███▌      | 71/200 [04:47<05:15,  2.45s/it]Running Inference:  36%|███▌      | 72/200 [04:50<05:06,  2.40s/it]Running Inference:  36%|███▋      | 73/200 [04:52<04:57,  2.35s/it]Running Inference:  37%|███▋      | 74/200 [04:54<04:31,  2.16s/it]Running Inference:  38%|███▊      | 75/200 [05:00<07:09,  3.44s/it]Running Inference:  38%|███▊      | 76/200 [05:01<05:51,  2.84s/it]Running Inference:  38%|███▊      | 77/200 [05:04<05:55,  2.89s/it]Running Inference:  39%|███▉      | 78/200 [05:07<05:36,  2.76s/it]Running Inference:  40%|███▉      | 79/200 [05:09<05:21,  2.66s/it]Running Inference:  40%|████      | 80/200 [05:10<04:23,  2.20s/it]Running Inference:  40%|████      | 81/200 [05:11<03:26,  1.74s/it]Running Inference:  41%|████      | 82/200 [05:12<02:58,  1.51s/it]Running Inference:  42%|████▏     | 83/200 [05:20<06:52,  3.52s/it]Running Inference:  42%|████▏     | 84/200 [05:28<09:27,  4.89s/it]Running Inference:  42%|████▎     | 85/200 [05:36<10:45,  5.62s/it]Running Inference:  43%|████▎     | 86/200 [05:37<08:13,  4.33s/it]Running Inference:  44%|████▎     | 87/200 [05:39<07:00,  3.72s/it]Running Inference:  44%|████▍     | 88/200 [05:41<06:04,  3.25s/it]Running Inference:  44%|████▍     | 89/200 [05:45<05:56,  3.22s/it]Running Inference:  45%|████▌     | 90/200 [05:47<05:30,  3.01s/it]Running Inference:  46%|████▌     | 91/200 [05:55<07:58,  4.39s/it]Running Inference:  46%|████▌     | 92/200 [06:02<09:30,  5.29s/it]Running Inference:  46%|████▋     | 93/200 [06:06<08:26,  4.73s/it]Running Inference:  47%|████▋     | 94/200 [06:07<06:30,  3.68s/it]Running Inference:  48%|████▊     | 95/200 [06:10<06:05,  3.48s/it]Running Inference:  48%|████▊     | 96/200 [06:17<08:07,  4.69s/it]Running Inference:  48%|████▊     | 97/200 [06:19<06:20,  3.69s/it]Running Inference:  49%|████▉     | 98/200 [06:22<06:12,  3.65s/it]Running Inference:  50%|████▉     | 99/200 [06:29<07:43,  4.59s/it]Running Inference:  50%|█████     | 100/200 [06:31<06:23,  3.83s/it]Running Inference:  50%|█████     | 101/200 [06:38<07:46,  4.71s/it]Running Inference:  51%|█████     | 102/200 [06:40<06:40,  4.08s/it]Running Inference:  52%|█████▏    | 103/200 [06:47<07:57,  4.92s/it]Running Inference:  52%|█████▏    | 104/200 [06:49<06:07,  3.83s/it]Running Inference:  52%|█████▎    | 105/200 [06:51<05:19,  3.36s/it]Running Inference:  53%|█████▎    | 106/200 [06:59<07:22,  4.71s/it]Running Inference:  54%|█████▎    | 107/200 [07:00<05:43,  3.70s/it]Running Inference:  54%|█████▍    | 108/200 [07:03<05:08,  3.35s/it]Running Inference:  55%|█████▍    | 109/200 [07:04<04:15,  2.80s/it]Running Inference:  55%|█████▌    | 110/200 [07:07<04:08,  2.76s/it]Running Inference:  56%|█████▌    | 111/200 [07:09<03:43,  2.52s/it]Running Inference:  56%|█████▌    | 112/200 [07:16<05:34,  3.80s/it]Running Inference:  56%|█████▋    | 113/200 [07:17<04:27,  3.07s/it]Running Inference:  57%|█████▋    | 114/200 [07:23<05:55,  4.14s/it]Running Inference:  57%|█████▊    | 115/200 [07:28<05:54,  4.17s/it]Running Inference:  58%|█████▊    | 116/200 [07:31<05:20,  3.82s/it]Running Inference:  58%|█████▊    | 117/200 [07:33<04:26,  3.21s/it]Running Inference:  59%|█████▉    | 118/200 [07:36<04:36,  3.37s/it]Running Inference:  60%|█████▉    | 119/200 [07:38<04:02,  2.99s/it]Running Inference:  60%|██████    | 120/200 [07:46<05:54,  4.44s/it]Running Inference:  60%|██████    | 121/200 [07:49<05:02,  3.83s/it]Running Inference:  61%|██████    | 122/200 [07:51<04:23,  3.38s/it]Running Inference:  62%|██████▏   | 123/200 [07:58<05:39,  4.40s/it]Running Inference:  62%|██████▏   | 124/200 [08:04<06:23,  5.04s/it]Running Inference:  62%|██████▎   | 125/200 [08:12<07:25,  5.94s/it]Running Inference:  63%|██████▎   | 126/200 [08:15<06:00,  4.87s/it]Running Inference:  64%|██████▎   | 127/200 [08:17<04:53,  4.02s/it]Running Inference:  64%|██████▍   | 128/200 [08:19<04:22,  3.65s/it]Running Inference:  64%|██████▍   | 129/200 [08:22<03:49,  3.23s/it]Running Inference:  65%|██████▌   | 130/200 [08:29<05:17,  4.54s/it]Running Inference:  66%|██████▌   | 131/200 [08:37<06:28,  5.63s/it]Running Inference:  66%|██████▌   | 132/200 [08:45<07:03,  6.22s/it]Running Inference:  66%|██████▋   | 133/200 [08:49<06:02,  5.41s/it]Running Inference:  67%|██████▋   | 134/200 [08:51<05:01,  4.57s/it]Running Inference:  68%|██████▊   | 135/200 [08:54<04:14,  3.92s/it]Running Inference:  68%|██████▊   | 136/200 [08:56<03:44,  3.50s/it]Running Inference:  68%|██████▊   | 137/200 [08:58<03:05,  2.94s/it]Running Inference:  69%|██████▉   | 138/200 [09:05<04:23,  4.25s/it]Running Inference:  70%|██████▉   | 139/200 [09:12<05:02,  4.96s/it]Running Inference:  70%|███████   | 140/200 [09:16<04:49,  4.83s/it]Running Inference:  70%|███████   | 141/200 [09:19<04:00,  4.08s/it]Running Inference:  71%|███████   | 142/200 [09:25<04:46,  4.94s/it]Running Inference:  72%|███████▏  | 143/200 [09:27<03:42,  3.90s/it]Running Inference:  72%|███████▏  | 144/200 [09:34<04:36,  4.94s/it]Running Inference:  72%|███████▎  | 145/200 [09:42<05:10,  5.64s/it]Running Inference:  73%|███████▎  | 146/200 [09:49<05:29,  6.11s/it]Running Inference:  74%|███████▎  | 147/200 [09:57<05:52,  6.65s/it]Running Inference:  74%|███████▍  | 148/200 [09:58<04:27,  5.14s/it]Running Inference:  74%|███████▍  | 149/200 [10:01<03:49,  4.50s/it]Running Inference:  75%|███████▌  | 150/200 [10:09<04:25,  5.30s/it]Running Inference:  76%|███████▌  | 151/200 [10:13<04:09,  5.08s/it]Running Inference:  76%|███████▌  | 152/200 [10:14<03:00,  3.76s/it]Running Inference:  76%|███████▋  | 153/200 [10:17<02:46,  3.54s/it]Running Inference:  77%|███████▋  | 154/200 [10:24<03:31,  4.61s/it]Running Inference:  78%|███████▊  | 155/200 [10:25<02:41,  3.59s/it]Running Inference:  78%|███████▊  | 156/200 [10:27<02:19,  3.17s/it]Running Inference:  78%|███████▊  | 157/200 [10:29<01:52,  2.61s/it]Running Inference:  79%|███████▉  | 158/200 [10:35<02:41,  3.83s/it]Running Inference:  80%|███████▉  | 159/200 [10:38<02:29,  3.64s/it]Running Inference:  80%|████████  | 160/200 [10:45<03:02,  4.57s/it]Running Inference:  80%|████████  | 161/200 [10:48<02:40,  4.12s/it]Running Inference:  81%|████████  | 162/200 [10:51<02:24,  3.80s/it]Running Inference:  82%|████████▏ | 163/200 [10:55<02:14,  3.64s/it]Running Inference:  82%|████████▏ | 164/200 [11:02<02:50,  4.74s/it]Running Inference:  82%|████████▎ | 165/200 [11:09<03:06,  5.33s/it]Running Inference:  83%|████████▎ | 166/200 [11:10<02:19,  4.10s/it]Running Inference:  84%|████████▎ | 167/200 [11:12<01:52,  3.41s/it]Running Inference:  84%|████████▍ | 168/200 [11:15<01:49,  3.43s/it]Running Inference:  84%|████████▍ | 169/200 [11:17<01:29,  2.88s/it]Running Inference:  85%|████████▌ | 170/200 [11:18<01:14,  2.47s/it]Running Inference:  86%|████████▌ | 171/200 [11:21<01:15,  2.59s/it]Running Inference:  86%|████████▌ | 172/200 [11:28<01:47,  3.85s/it]Running Inference:  86%|████████▋ | 173/200 [11:35<02:09,  4.80s/it]Running Inference:  87%|████████▋ | 174/200 [11:38<01:50,  4.25s/it]Running Inference:  88%|████████▊ | 175/200 [11:40<01:31,  3.65s/it]Running Inference:  88%|████████▊ | 176/200 [11:47<01:49,  4.57s/it]Running Inference:  88%|████████▊ | 177/200 [11:48<01:21,  3.54s/it]Running Inference:  89%|████████▉ | 178/200 [11:53<01:28,  4.02s/it]Running Inference:  90%|████████▉ | 179/200 [11:56<01:14,  3.55s/it]Running Inference:  90%|█████████ | 180/200 [11:57<00:59,  2.97s/it]Running Inference:  90%|█████████ | 181/200 [11:59<00:51,  2.68s/it]Running Inference:  91%|█████████ | 182/200 [12:01<00:43,  2.40s/it]Running Inference:  92%|█████████▏| 183/200 [12:05<00:47,  2.79s/it]Running Inference:  92%|█████████▏| 184/200 [12:13<01:10,  4.41s/it]Running Inference:  92%|█████████▎| 185/200 [12:15<00:54,  3.65s/it]Running Inference:  93%|█████████▎| 186/200 [12:16<00:42,  3.05s/it]Running Inference:  94%|█████████▎| 187/200 [12:17<00:32,  2.47s/it]Running Inference:  94%|█████████▍| 188/200 [12:19<00:27,  2.33s/it]Running Inference:  94%|█████████▍| 189/200 [12:22<00:26,  2.45s/it]Running Inference:  95%|█████████▌| 190/200 [12:25<00:26,  2.68s/it]Running Inference:  96%|█████████▌| 191/200 [12:28<00:23,  2.57s/it]Running Inference:  96%|█████████▌| 192/200 [12:34<00:30,  3.78s/it]Running Inference:  96%|█████████▋| 193/200 [12:42<00:35,  5.06s/it]Running Inference:  97%|█████████▋| 194/200 [12:44<00:24,  4.09s/it]Running Inference:  98%|█████████▊| 195/200 [12:46<00:16,  3.39s/it]Running Inference:  98%|█████████▊| 196/200 [12:54<00:19,  4.75s/it]Running Inference:  98%|█████████▊| 197/200 [12:55<00:11,  3.76s/it]Running Inference:  99%|█████████▉| 198/200 [12:59<00:07,  3.58s/it]Running Inference: 100%|█████████▉| 199/200 [13:00<00:03,  3.02s/it]Running Inference: 100%|██████████| 200/200 [13:07<00:00,  4.23s/it]Running Inference: 100%|██████████| 200/200 [13:07<00:00,  3.94s/it]
2025-12-14 11:30:37,085 - INFO - Inference completed.
2025-12-14 11:30:37,096 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 11:30:37,096 - INFO - Calculating metrics for dataset: longbench
2025-12-14 11:30:37,229 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 11:30:37,229 - INFO - Metrics:
28.89
2025-12-14 11:30:37,230 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=samsum, ratio=0.1) on GPU 4

----------------------------------------
Task: samsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 11:30:43,604 - INFO - Set deterministic seeds to 42
2025-12-14 11:30:43,604 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 11:30:43,604 - INFO - Starting evaluation run...
2025-12-14 11:30:43,604 - INFO - Output directory set to: longbenchresult
2025-12-14 11:30:43,604 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 11:30:43,604 - INFO - KV Press 'knorm' setup.
2025-12-14 11:30:43,604 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 11:30:43,604 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.76it/s]
Device set to use cuda:0
2025-12-14 11:31:00,365 - INFO - Model pipeline loaded.
2025-12-14 11:31:00,365 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 11:31:06,536 - INFO - Dataset loaded with 200 entries.
2025-12-14 11:31:06,536 - INFO - Dataset processed with 200 entries.
2025-12-14 11:31:06,563 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:40,  2.91s/it]Running Inference:   1%|          | 2/200 [00:06<10:29,  3.18s/it]Running Inference:   2%|▏         | 3/200 [00:12<15:28,  4.71s/it]Running Inference:   2%|▏         | 4/200 [00:14<10:54,  3.34s/it]Running Inference:   2%|▎         | 5/200 [00:15<08:41,  2.67s/it]Running Inference:   3%|▎         | 6/200 [00:22<13:34,  4.20s/it]Running Inference:   4%|▎         | 7/200 [00:24<10:59,  3.42s/it]Running Inference:   4%|▍         | 8/200 [00:25<08:45,  2.74s/it]Running Inference:   4%|▍         | 9/200 [00:32<12:45,  4.01s/it]Running Inference:   5%|▌         | 10/200 [00:35<11:36,  3.67s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:37<10:19,  3.28s/it]Running Inference:   6%|▌         | 12/200 [00:41<10:06,  3.23s/it]Running Inference:   6%|▋         | 13/200 [00:42<08:24,  2.70s/it]Running Inference:   7%|▋         | 14/200 [00:47<10:15,  3.31s/it]Running Inference:   8%|▊         | 15/200 [00:49<09:33,  3.10s/it]Running Inference:   8%|▊         | 16/200 [00:56<12:58,  4.23s/it]Running Inference:   8%|▊         | 17/200 [01:04<16:23,  5.37s/it]Running Inference:   9%|▉         | 18/200 [01:11<17:49,  5.88s/it]Running Inference:  10%|▉         | 19/200 [01:19<19:32,  6.48s/it]Running Inference:  10%|█         | 20/200 [01:21<15:10,  5.06s/it]Running Inference:  10%|█         | 21/200 [01:23<12:38,  4.24s/it]Running Inference:  11%|█         | 22/200 [01:28<13:17,  4.48s/it]Running Inference:  12%|█▏        | 23/200 [01:31<11:46,  3.99s/it]Running Inference:  12%|█▏        | 24/200 [01:33<09:57,  3.40s/it]Running Inference:  12%|█▎        | 25/200 [01:40<12:42,  4.36s/it]Running Inference:  13%|█▎        | 26/200 [01:47<15:07,  5.22s/it]Running Inference:  14%|█▎        | 27/200 [01:52<14:46,  5.12s/it]Running Inference:  14%|█▍        | 28/200 [01:55<12:39,  4.42s/it]Running Inference:  14%|█▍        | 29/200 [02:01<14:32,  5.11s/it]Running Inference:  15%|█▌        | 30/200 [02:03<11:49,  4.17s/it]Running Inference:  16%|█▌        | 31/200 [02:06<10:07,  3.59s/it]Running Inference:  16%|█▌        | 32/200 [02:13<13:21,  4.77s/it]Running Inference:  16%|█▋        | 33/200 [02:20<14:46,  5.31s/it]Running Inference:  17%|█▋        | 34/200 [02:28<16:51,  6.09s/it]Running Inference:  18%|█▊        | 35/200 [02:30<13:51,  5.04s/it]Running Inference:  18%|█▊        | 36/200 [02:34<12:31,  4.58s/it]Running Inference:  18%|█▊        | 37/200 [02:36<10:51,  4.00s/it]Running Inference:  19%|█▉        | 38/200 [02:39<09:47,  3.62s/it]Running Inference:  20%|█▉        | 39/200 [02:43<09:35,  3.57s/it]Running Inference:  20%|██        | 40/200 [02:45<08:48,  3.30s/it]Running Inference:  20%|██        | 41/200 [02:47<07:34,  2.86s/it]Running Inference:  21%|██        | 42/200 [02:54<10:39,  4.05s/it]Running Inference:  22%|██▏       | 43/200 [02:56<09:11,  3.51s/it]Running Inference:  22%|██▏       | 44/200 [03:04<12:22,  4.76s/it]Running Inference:  22%|██▎       | 45/200 [03:12<14:48,  5.73s/it]Running Inference:  23%|██▎       | 46/200 [03:20<16:16,  6.34s/it]Running Inference:  24%|██▎       | 47/200 [03:22<13:00,  5.10s/it]Running Inference:  24%|██▍       | 48/200 [03:24<11:05,  4.38s/it]Running Inference:  24%|██▍       | 49/200 [03:27<09:26,  3.75s/it]Running Inference:  25%|██▌       | 50/200 [03:28<07:45,  3.11s/it]Running Inference:  26%|██▌       | 51/200 [03:35<10:14,  4.12s/it]Running Inference:  26%|██▌       | 52/200 [03:37<08:54,  3.61s/it]Running Inference:  26%|██▋       | 53/200 [03:39<07:29,  3.06s/it]Running Inference:  27%|██▋       | 54/200 [03:40<06:13,  2.56s/it]Running Inference:  28%|██▊       | 55/200 [03:43<06:12,  2.57s/it]Running Inference:  28%|██▊       | 56/200 [03:46<06:10,  2.57s/it]Running Inference:  28%|██▊       | 57/200 [03:48<06:00,  2.52s/it]Running Inference:  29%|██▉       | 58/200 [03:56<09:57,  4.21s/it]Running Inference:  30%|██▉       | 59/200 [03:57<07:37,  3.24s/it]Running Inference:  30%|███       | 60/200 [03:59<06:37,  2.84s/it]Running Inference:  30%|███       | 61/200 [04:00<05:22,  2.32s/it]Running Inference:  31%|███       | 62/200 [04:07<08:28,  3.69s/it]Running Inference:  32%|███▏      | 63/200 [04:09<07:05,  3.11s/it]Running Inference:  32%|███▏      | 64/200 [04:11<06:32,  2.88s/it]Running Inference:  32%|███▎      | 65/200 [04:14<06:47,  3.02s/it]Running Inference:  33%|███▎      | 66/200 [04:17<06:11,  2.78s/it]Running Inference:  34%|███▎      | 67/200 [04:23<08:44,  3.95s/it]Running Inference:  34%|███▍      | 68/200 [04:30<10:25,  4.74s/it]Running Inference:  34%|███▍      | 69/200 [04:32<08:36,  3.94s/it]Running Inference:  35%|███▌      | 70/200 [04:34<07:08,  3.30s/it]Running Inference:  36%|███▌      | 71/200 [04:35<06:00,  2.80s/it]Running Inference:  36%|███▌      | 72/200 [04:38<05:32,  2.60s/it]Running Inference:  36%|███▋      | 73/200 [04:40<05:15,  2.49s/it]Running Inference:  37%|███▋      | 74/200 [04:41<04:43,  2.25s/it]Running Inference:  38%|███▊      | 75/200 [04:48<07:16,  3.50s/it]Running Inference:  38%|███▊      | 76/200 [04:49<05:56,  2.88s/it]Running Inference:  38%|███▊      | 77/200 [04:52<05:52,  2.86s/it]Running Inference:  39%|███▉      | 78/200 [04:55<05:33,  2.74s/it]Running Inference:  40%|███▉      | 79/200 [04:57<05:19,  2.64s/it]Running Inference:  40%|████      | 80/200 [04:58<04:29,  2.24s/it]Running Inference:  40%|████      | 81/200 [05:05<06:57,  3.51s/it]Running Inference:  41%|████      | 82/200 [05:06<05:24,  2.75s/it]Running Inference:  42%|████▏     | 83/200 [05:10<06:01,  3.09s/it]Running Inference:  42%|████▏     | 84/200 [05:18<08:51,  4.58s/it]Running Inference:  42%|████▎     | 85/200 [05:25<10:19,  5.39s/it]Running Inference:  43%|████▎     | 86/200 [05:27<08:21,  4.40s/it]Running Inference:  44%|████▎     | 87/200 [05:35<10:04,  5.35s/it]Running Inference:  44%|████▍     | 88/200 [05:37<08:09,  4.37s/it]Running Inference:  44%|████▍     | 89/200 [05:39<06:56,  3.75s/it]Running Inference:  45%|████▌     | 90/200 [05:41<05:59,  3.27s/it]Running Inference:  46%|████▌     | 91/200 [05:45<06:11,  3.40s/it]Running Inference:  46%|████▌     | 92/200 [05:49<06:19,  3.51s/it]Running Inference:  46%|████▋     | 93/200 [05:52<06:12,  3.49s/it]Running Inference:  47%|████▋     | 94/200 [05:53<05:04,  2.87s/it]Running Inference:  48%|████▊     | 95/200 [05:56<05:05,  2.91s/it]Running Inference:  48%|████▊     | 96/200 [06:04<07:25,  4.28s/it]Running Inference:  48%|████▊     | 97/200 [06:05<05:45,  3.35s/it]Running Inference:  49%|████▉     | 98/200 [06:08<05:34,  3.28s/it]Running Inference:  50%|████▉     | 99/200 [06:15<07:16,  4.32s/it]Running Inference:  50%|█████     | 100/200 [06:17<06:06,  3.67s/it]Running Inference:  50%|█████     | 101/200 [06:20<05:25,  3.29s/it]Running Inference:  51%|█████     | 102/200 [06:27<07:20,  4.49s/it]Running Inference:  52%|█████▏    | 103/200 [06:34<08:24,  5.20s/it]Running Inference:  52%|█████▏    | 104/200 [06:35<06:27,  4.04s/it]Running Inference:  52%|█████▎    | 105/200 [06:37<05:16,  3.33s/it]Running Inference:  53%|█████▎    | 106/200 [06:45<07:19,  4.68s/it]Running Inference:  54%|█████▎    | 107/200 [06:46<05:46,  3.72s/it]Running Inference:  54%|█████▍    | 108/200 [06:49<05:09,  3.37s/it]Running Inference:  55%|█████▍    | 109/200 [06:50<04:16,  2.81s/it]Running Inference:  55%|█████▌    | 110/200 [06:53<04:05,  2.73s/it]Running Inference:  56%|█████▌    | 111/200 [07:00<06:15,  4.22s/it]Running Inference:  56%|█████▌    | 112/200 [07:07<07:18,  4.99s/it]Running Inference:  56%|█████▋    | 113/200 [07:08<05:37,  3.88s/it]Running Inference:  57%|█████▋    | 114/200 [07:15<06:44,  4.70s/it]Running Inference:  57%|█████▊    | 115/200 [07:23<08:01,  5.66s/it]Running Inference:  58%|█████▊    | 116/200 [07:26<06:56,  4.96s/it]Running Inference:  58%|█████▊    | 117/200 [07:28<05:32,  4.01s/it]Running Inference:  59%|█████▉    | 118/200 [07:35<06:51,  5.02s/it]Running Inference:  60%|█████▉    | 119/200 [07:38<05:35,  4.14s/it]Running Inference:  60%|██████    | 120/200 [07:45<06:58,  5.24s/it]Running Inference:  60%|██████    | 121/200 [07:48<05:46,  4.39s/it]Running Inference:  61%|██████    | 122/200 [07:51<05:05,  3.92s/it]Running Inference:  62%|██████▏   | 123/200 [07:57<06:07,  4.78s/it]Running Inference:  62%|██████▏   | 124/200 [07:58<04:33,  3.60s/it]Running Inference:  62%|██████▎   | 125/200 [08:00<04:00,  3.20s/it]Running Inference:  63%|██████▎   | 126/200 [08:03<03:44,  3.03s/it]Running Inference:  64%|██████▎   | 127/200 [08:05<03:12,  2.64s/it]Running Inference:  64%|██████▍   | 128/200 [08:08<03:16,  2.73s/it]Running Inference:  64%|██████▍   | 129/200 [08:10<03:07,  2.64s/it]Running Inference:  65%|██████▌   | 130/200 [08:18<04:48,  4.12s/it]Running Inference:  66%|██████▌   | 131/200 [08:26<06:07,  5.33s/it]Running Inference:  66%|██████▌   | 132/200 [08:28<04:56,  4.36s/it]Running Inference:  66%|██████▋   | 133/200 [08:32<04:34,  4.10s/it]Running Inference:  67%|██████▋   | 134/200 [08:34<03:53,  3.54s/it]Running Inference:  68%|██████▊   | 135/200 [08:35<02:59,  2.77s/it]Running Inference:  68%|██████▊   | 136/200 [08:37<02:52,  2.69s/it]Running Inference:  68%|██████▊   | 137/200 [08:39<02:34,  2.45s/it]Running Inference:  69%|██████▉   | 138/200 [08:42<02:36,  2.53s/it]Running Inference:  70%|██████▉   | 139/200 [08:43<02:13,  2.19s/it]Running Inference:  70%|███████   | 140/200 [08:51<03:57,  3.96s/it]Running Inference:  70%|███████   | 141/200 [08:53<03:20,  3.39s/it]Running Inference:  71%|███████   | 142/200 [09:00<04:18,  4.46s/it]Running Inference:  72%|███████▏  | 143/200 [09:02<03:25,  3.61s/it]Running Inference:  72%|███████▏  | 144/200 [09:09<04:25,  4.73s/it]Running Inference:  72%|███████▎  | 145/200 [09:11<03:32,  3.87s/it]Running Inference:  73%|███████▎  | 146/200 [09:18<04:22,  4.86s/it]Running Inference:  74%|███████▎  | 147/200 [09:22<04:02,  4.58s/it]Running Inference:  74%|███████▍  | 148/200 [09:24<03:10,  3.67s/it]Running Inference:  74%|███████▍  | 149/200 [09:28<03:13,  3.79s/it]Running Inference:  75%|███████▌  | 150/200 [09:35<03:59,  4.79s/it]Running Inference:  76%|███████▌  | 151/200 [09:38<03:32,  4.33s/it]Running Inference:  76%|███████▌  | 152/200 [09:39<02:34,  3.23s/it]Running Inference:  76%|███████▋  | 153/200 [09:47<03:36,  4.61s/it]Running Inference:  77%|███████▋  | 154/200 [09:54<04:06,  5.35s/it]Running Inference:  78%|███████▊  | 155/200 [09:55<03:05,  4.12s/it]Running Inference:  78%|███████▊  | 156/200 [09:57<02:37,  3.59s/it]Running Inference:  78%|███████▊  | 157/200 [09:58<02:01,  2.82s/it]Running Inference:  79%|███████▉  | 158/200 [10:05<02:46,  3.97s/it]Running Inference:  80%|███████▉  | 159/200 [10:08<02:33,  3.75s/it]Running Inference:  80%|████████  | 160/200 [10:15<03:05,  4.64s/it]Running Inference:  80%|████████  | 161/200 [10:22<03:25,  5.27s/it]Running Inference:  81%|████████  | 162/200 [10:24<02:40,  4.23s/it]Running Inference:  82%|████████▏ | 163/200 [10:27<02:25,  3.93s/it]Running Inference:  82%|████████▏ | 164/200 [10:30<02:08,  3.56s/it]Running Inference:  82%|████████▎ | 165/200 [10:36<02:37,  4.50s/it]Running Inference:  83%|████████▎ | 166/200 [10:38<02:06,  3.73s/it]Running Inference:  84%|████████▎ | 167/200 [10:40<01:44,  3.15s/it]Running Inference:  84%|████████▍ | 168/200 [10:44<01:46,  3.33s/it]Running Inference:  84%|████████▍ | 169/200 [10:45<01:24,  2.71s/it]Running Inference:  85%|████████▌ | 170/200 [10:52<02:02,  4.08s/it]Running Inference:  86%|████████▌ | 171/200 [10:55<01:48,  3.74s/it]Running Inference:  86%|████████▌ | 172/200 [11:02<02:10,  4.65s/it]Running Inference:  86%|████████▋ | 173/200 [11:04<01:42,  3.79s/it]Running Inference:  87%|████████▋ | 174/200 [11:11<02:03,  4.75s/it]Running Inference:  88%|████████▊ | 175/200 [11:13<01:40,  4.03s/it]Running Inference:  88%|████████▊ | 176/200 [11:14<01:16,  3.17s/it]Running Inference:  88%|████████▊ | 177/200 [11:15<00:59,  2.59s/it]Running Inference:  89%|████████▉ | 178/200 [11:19<01:01,  2.81s/it]Running Inference:  90%|████████▉ | 179/200 [11:21<00:55,  2.65s/it]Running Inference:  90%|█████████ | 180/200 [11:28<01:17,  3.87s/it]Running Inference:  90%|█████████ | 181/200 [11:35<01:32,  4.87s/it]Running Inference:  91%|█████████ | 182/200 [11:42<01:37,  5.44s/it]Running Inference:  92%|█████████▏| 183/200 [11:50<01:45,  6.18s/it]Running Inference:  92%|█████████▏| 184/200 [11:58<01:48,  6.78s/it]Running Inference:  92%|█████████▎| 185/200 [12:00<01:20,  5.37s/it]Running Inference:  93%|█████████▎| 186/200 [12:02<00:59,  4.26s/it]Running Inference:  94%|█████████▎| 187/200 [12:03<00:43,  3.37s/it]Running Inference:  94%|█████████▍| 188/200 [12:05<00:35,  2.95s/it]Running Inference:  94%|█████████▍| 189/200 [12:08<00:31,  2.88s/it]Running Inference:  95%|█████████▌| 190/200 [12:10<00:28,  2.88s/it]Running Inference:  96%|█████████▌| 191/200 [12:17<00:37,  4.13s/it]Running Inference:  96%|█████████▌| 192/200 [12:24<00:38,  4.86s/it]Running Inference:  96%|█████████▋| 193/200 [12:32<00:40,  5.81s/it]Running Inference:  97%|█████████▋| 194/200 [12:34<00:27,  4.58s/it]Running Inference:  98%|█████████▊| 195/200 [12:36<00:18,  3.73s/it]Running Inference:  98%|█████████▊| 196/200 [12:43<00:19,  4.98s/it]Running Inference:  98%|█████████▊| 197/200 [12:45<00:11,  3.92s/it]Running Inference:  99%|█████████▉| 198/200 [12:52<00:10,  5.00s/it]Running Inference: 100%|█████████▉| 199/200 [12:54<00:04,  4.10s/it]Running Inference: 100%|██████████| 200/200 [12:56<00:00,  3.45s/it]Running Inference: 100%|██████████| 200/200 [12:56<00:00,  3.88s/it]
2025-12-14 11:44:03,394 - INFO - Inference completed.
2025-12-14 11:44:03,405 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 11:44:03,405 - INFO - Calculating metrics for dataset: longbench
2025-12-14 11:44:03,532 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 11:44:03,532 - INFO - Metrics:
29.19
2025-12-14 11:44:03,534 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=samsum, ratio=0.2) on GPU 4

----------------------------------------
Task: samsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 11:44:10,069 - INFO - Set deterministic seeds to 42
2025-12-14 11:44:10,069 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 11:44:10,069 - INFO - Starting evaluation run...
2025-12-14 11:44:10,069 - INFO - Output directory set to: longbenchresult
2025-12-14 11:44:10,069 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 11:44:10,069 - INFO - KV Press 'knorm' setup.
2025-12-14 11:44:10,069 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 11:44:10,069 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.91it/s]
Device set to use cuda:0
2025-12-14 11:44:24,136 - INFO - Model pipeline loaded.
2025-12-14 11:44:24,136 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 11:45:31,404 - INFO - Dataset loaded with 200 entries.
2025-12-14 11:45:31,404 - INFO - Dataset processed with 200 entries.
2025-12-14 11:45:31,431 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:52,  2.98s/it]Running Inference:   1%|          | 2/200 [00:06<10:30,  3.18s/it]Running Inference:   2%|▏         | 3/200 [00:12<15:34,  4.74s/it]Running Inference:   2%|▏         | 4/200 [00:20<18:32,  5.67s/it]Running Inference:   2%|▎         | 5/200 [00:21<12:59,  4.00s/it]Running Inference:   3%|▎         | 6/200 [00:28<16:27,  5.09s/it]Running Inference:   4%|▎         | 7/200 [00:30<12:58,  4.03s/it]Running Inference:   4%|▍         | 8/200 [00:31<10:30,  3.28s/it]Running Inference:   4%|▍         | 9/200 [00:38<14:00,  4.40s/it]Running Inference:   5%|▌         | 10/200 [00:41<12:31,  3.95s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:49<16:12,  5.14s/it]Running Inference:   6%|▌         | 12/200 [00:56<17:48,  5.68s/it]Running Inference:   6%|▋         | 13/200 [00:57<13:50,  4.44s/it]Running Inference:   7%|▋         | 14/200 [01:05<17:08,  5.53s/it]Running Inference:   8%|▊         | 15/200 [01:08<14:02,  4.56s/it]Running Inference:   8%|▊         | 16/200 [01:15<16:07,  5.26s/it]Running Inference:   8%|▊         | 17/200 [01:17<13:42,  4.50s/it]Running Inference:   9%|▉         | 18/200 [01:19<11:16,  3.72s/it]Running Inference:  10%|▉         | 19/200 [01:27<15:00,  4.98s/it]Running Inference:  10%|█         | 20/200 [01:29<11:37,  3.87s/it]Running Inference:  10%|█         | 21/200 [01:31<10:21,  3.47s/it]Running Inference:  11%|█         | 22/200 [01:36<11:13,  3.79s/it]Running Inference:  12%|█▏        | 23/200 [01:43<14:26,  4.89s/it]Running Inference:  12%|█▏        | 24/200 [01:45<12:12,  4.16s/it]Running Inference:  12%|█▎        | 25/200 [01:49<11:08,  3.82s/it]Running Inference:  13%|█▎        | 26/200 [01:56<14:05,  4.86s/it]Running Inference:  14%|█▎        | 27/200 [02:00<13:28,  4.67s/it]Running Inference:  14%|█▍        | 28/200 [02:03<12:05,  4.22s/it]Running Inference:  14%|█▍        | 29/200 [02:10<14:09,  4.97s/it]Running Inference:  15%|█▌        | 30/200 [02:12<11:33,  4.08s/it]Running Inference:  16%|█▌        | 31/200 [02:14<09:42,  3.45s/it]Running Inference:  16%|█▌        | 32/200 [02:18<09:58,  3.56s/it]Running Inference:  16%|█▋        | 33/200 [02:24<12:25,  4.46s/it]Running Inference:  17%|█▋        | 34/200 [02:27<11:11,  4.04s/it]Running Inference:  18%|█▊        | 35/200 [02:34<13:27,  4.90s/it]Running Inference:  18%|█▊        | 36/200 [02:42<15:46,  5.77s/it]Running Inference:  18%|█▊        | 37/200 [02:45<13:10,  4.85s/it]Running Inference:  19%|█▉        | 38/200 [02:47<11:23,  4.22s/it]Running Inference:  20%|█▉        | 39/200 [02:51<10:49,  4.03s/it]Running Inference:  20%|██        | 40/200 [02:59<13:55,  5.22s/it]Running Inference:  20%|██        | 41/200 [03:06<15:13,  5.75s/it]Running Inference:  21%|██        | 42/200 [03:07<11:12,  4.26s/it]Running Inference:  22%|██▏       | 43/200 [03:09<09:26,  3.61s/it]Running Inference:  22%|██▏       | 44/200 [03:17<12:32,  4.82s/it]Running Inference:  22%|██▎       | 45/200 [03:19<10:29,  4.06s/it]Running Inference:  23%|██▎       | 46/200 [03:27<13:12,  5.14s/it]Running Inference:  24%|██▎       | 47/200 [03:30<11:34,  4.54s/it]Running Inference:  24%|██▍       | 48/200 [03:32<10:05,  3.99s/it]Running Inference:  24%|██▍       | 49/200 [03:34<08:31,  3.39s/it]Running Inference:  25%|██▌       | 50/200 [03:36<07:13,  2.89s/it]Running Inference:  26%|██▌       | 51/200 [03:43<09:55,  3.99s/it]Running Inference:  26%|██▌       | 52/200 [03:51<12:50,  5.20s/it]Running Inference:  26%|██▋       | 53/200 [03:52<10:14,  4.18s/it]Running Inference:  27%|██▋       | 54/200 [03:54<08:08,  3.34s/it]Running Inference:  28%|██▊       | 55/200 [04:02<11:35,  4.79s/it]Running Inference:  28%|██▊       | 56/200 [04:05<09:51,  4.11s/it]Running Inference:  28%|██▊       | 57/200 [04:08<09:16,  3.89s/it]Running Inference:  29%|██▉       | 58/200 [04:16<12:16,  5.19s/it]Running Inference:  30%|██▉       | 59/200 [04:18<09:38,  4.10s/it]Running Inference:  30%|███       | 60/200 [04:19<07:53,  3.38s/it]Running Inference:  30%|███       | 61/200 [04:21<06:15,  2.70s/it]Running Inference:  31%|███       | 62/200 [04:27<09:07,  3.97s/it]Running Inference:  32%|███▏      | 63/200 [04:34<11:04,  4.85s/it]Running Inference:  32%|███▏      | 64/200 [04:37<09:18,  4.10s/it]Running Inference:  32%|███▎      | 65/200 [04:40<08:33,  3.80s/it]Running Inference:  33%|███▎      | 66/200 [04:42<07:34,  3.39s/it]Running Inference:  34%|███▎      | 67/200 [04:49<09:46,  4.41s/it]Running Inference:  34%|███▍      | 68/200 [04:56<11:11,  5.09s/it]Running Inference:  34%|███▍      | 69/200 [04:58<09:19,  4.27s/it]Running Inference:  35%|███▌      | 70/200 [05:06<11:20,  5.24s/it]Running Inference:  36%|███▌      | 71/200 [05:07<08:52,  4.13s/it]Running Inference:  36%|███▌      | 72/200 [05:09<07:24,  3.47s/it]Running Inference:  36%|███▋      | 73/200 [05:11<06:11,  2.93s/it]Running Inference:  37%|███▋      | 74/200 [05:12<05:16,  2.51s/it]Running Inference:  38%|███▊      | 75/200 [05:13<04:19,  2.07s/it]Running Inference:  38%|███▊      | 76/200 [05:15<03:55,  1.90s/it]Running Inference:  38%|███▊      | 77/200 [05:18<04:28,  2.18s/it]Running Inference:  39%|███▉      | 78/200 [05:20<04:34,  2.25s/it]Running Inference:  40%|███▉      | 79/200 [05:23<04:48,  2.38s/it]Running Inference:  40%|████      | 80/200 [05:24<04:01,  2.01s/it]Running Inference:  40%|████      | 81/200 [05:30<06:41,  3.37s/it]Running Inference:  41%|████      | 82/200 [05:37<08:34,  4.36s/it]Running Inference:  42%|████▏     | 83/200 [05:45<10:47,  5.53s/it]Running Inference:  42%|████▏     | 84/200 [05:53<12:12,  6.31s/it]Running Inference:  42%|████▎     | 85/200 [06:01<12:41,  6.63s/it]Running Inference:  43%|████▎     | 86/200 [06:08<12:37,  6.64s/it]Running Inference:  44%|████▎     | 87/200 [06:11<10:41,  5.67s/it]Running Inference:  44%|████▍     | 88/200 [06:18<11:18,  6.06s/it]Running Inference:  44%|████▍     | 89/200 [06:20<09:08,  4.94s/it]Running Inference:  45%|████▌     | 90/200 [06:22<07:31,  4.10s/it]Running Inference:  46%|████▌     | 91/200 [06:25<06:52,  3.78s/it]Running Inference:  46%|████▌     | 92/200 [06:33<08:46,  4.88s/it]Running Inference:  46%|████▋     | 93/200 [06:36<07:49,  4.39s/it]Running Inference:  47%|████▋     | 94/200 [06:37<06:02,  3.42s/it]Running Inference:  48%|████▊     | 95/200 [06:39<04:56,  2.83s/it]Running Inference:  48%|████▊     | 96/200 [06:46<07:21,  4.25s/it]Running Inference:  48%|████▊     | 97/200 [06:47<05:31,  3.21s/it]Running Inference:  49%|████▉     | 98/200 [06:52<06:12,  3.65s/it]Running Inference:  50%|████▉     | 99/200 [06:59<07:45,  4.60s/it]Running Inference:  50%|█████     | 100/200 [07:06<09:07,  5.48s/it]Running Inference:  50%|█████     | 101/200 [07:08<07:30,  4.55s/it]Running Inference:  51%|█████     | 102/200 [07:11<06:23,  3.92s/it]Running Inference:  52%|█████▏    | 103/200 [07:18<07:47,  4.82s/it]Running Inference:  52%|█████▏    | 104/200 [07:19<06:04,  3.80s/it]Running Inference:  52%|█████▎    | 105/200 [07:27<07:45,  4.90s/it]Running Inference:  53%|█████▎    | 106/200 [07:35<09:05,  5.80s/it]Running Inference:  54%|█████▎    | 107/200 [07:36<06:58,  4.50s/it]Running Inference:  54%|█████▍    | 108/200 [07:44<08:16,  5.39s/it]Running Inference:  55%|█████▍    | 109/200 [07:45<06:19,  4.17s/it]Running Inference:  55%|█████▌    | 110/200 [07:53<07:53,  5.26s/it]Running Inference:  56%|█████▌    | 111/200 [07:55<06:18,  4.25s/it]Running Inference:  56%|█████▌    | 112/200 [08:01<07:22,  5.03s/it]Running Inference:  56%|█████▋    | 113/200 [08:03<05:40,  3.91s/it]Running Inference:  57%|█████▋    | 114/200 [08:09<06:47,  4.74s/it]Running Inference:  57%|█████▊    | 115/200 [08:17<08:05,  5.71s/it]Running Inference:  58%|█████▊    | 116/200 [08:21<07:07,  5.09s/it]Running Inference:  58%|█████▊    | 117/200 [08:22<05:32,  4.00s/it]Running Inference:  59%|█████▉    | 118/200 [08:30<06:52,  5.03s/it]Running Inference:  60%|█████▉    | 119/200 [08:32<05:36,  4.16s/it]Running Inference:  60%|██████    | 120/200 [08:40<07:01,  5.27s/it]Running Inference:  60%|██████    | 121/200 [08:43<05:55,  4.50s/it]Running Inference:  61%|██████    | 122/200 [08:46<05:19,  4.09s/it]Running Inference:  62%|██████▏   | 123/200 [08:53<06:18,  4.92s/it]Running Inference:  62%|██████▏   | 124/200 [08:54<04:56,  3.90s/it]Running Inference:  62%|██████▎   | 125/200 [08:56<04:11,  3.35s/it]Running Inference:  63%|██████▎   | 126/200 [08:59<03:45,  3.05s/it]Running Inference:  64%|██████▎   | 127/200 [09:00<03:13,  2.65s/it]Running Inference:  64%|██████▍   | 128/200 [09:03<03:10,  2.65s/it]Running Inference:  64%|██████▍   | 129/200 [09:05<03:03,  2.59s/it]Running Inference:  65%|██████▌   | 130/200 [09:08<03:07,  2.68s/it]Running Inference:  66%|██████▌   | 131/200 [09:16<04:59,  4.34s/it]Running Inference:  66%|██████▌   | 132/200 [09:24<06:03,  5.34s/it]Running Inference:  66%|██████▋   | 133/200 [09:29<05:41,  5.10s/it]Running Inference:  67%|██████▋   | 134/200 [09:31<04:39,  4.24s/it]Running Inference:  68%|██████▊   | 135/200 [09:32<03:31,  3.26s/it]Running Inference:  68%|██████▊   | 136/200 [09:39<04:43,  4.43s/it]Running Inference:  68%|██████▊   | 137/200 [09:41<03:54,  3.72s/it]Running Inference:  69%|██████▉   | 138/200 [09:44<03:26,  3.34s/it]Running Inference:  70%|██████▉   | 139/200 [09:50<04:24,  4.33s/it]Running Inference:  70%|███████   | 140/200 [09:58<05:28,  5.47s/it]Running Inference:  70%|███████   | 141/200 [10:06<05:53,  6.00s/it]Running Inference:  71%|███████   | 142/200 [10:13<06:05,  6.30s/it]Running Inference:  72%|███████▏  | 143/200 [10:20<06:15,  6.58s/it]Running Inference:  72%|███████▏  | 144/200 [10:27<06:22,  6.83s/it]Running Inference:  72%|███████▎  | 145/200 [10:29<04:52,  5.33s/it]Running Inference:  73%|███████▎  | 146/200 [10:30<03:38,  4.04s/it]Running Inference:  74%|███████▎  | 147/200 [10:35<03:48,  4.31s/it]Running Inference:  74%|███████▍  | 148/200 [10:37<03:01,  3.49s/it]Running Inference:  74%|███████▍  | 149/200 [10:40<02:51,  3.35s/it]Running Inference:  75%|███████▌  | 150/200 [10:47<03:45,  4.51s/it]Running Inference:  76%|███████▌  | 151/200 [10:51<03:39,  4.48s/it]Running Inference:  76%|███████▌  | 152/200 [10:52<02:40,  3.34s/it]Running Inference:  76%|███████▋  | 153/200 [10:55<02:32,  3.25s/it]Running Inference:  77%|███████▋  | 154/200 [11:02<03:23,  4.43s/it]Running Inference:  78%|███████▊  | 155/200 [11:04<02:42,  3.61s/it]Running Inference:  78%|███████▊  | 156/200 [11:06<02:18,  3.15s/it]Running Inference:  78%|███████▊  | 157/200 [11:07<01:48,  2.52s/it]Running Inference:  79%|███████▉  | 158/200 [11:14<02:39,  3.79s/it]Running Inference:  80%|███████▉  | 159/200 [11:18<02:38,  3.85s/it]Running Inference:  80%|████████  | 160/200 [11:24<03:09,  4.74s/it]Running Inference:  80%|████████  | 161/200 [11:31<03:29,  5.37s/it]Running Inference:  81%|████████  | 162/200 [11:33<02:43,  4.31s/it]Running Inference:  82%|████████▏ | 163/200 [11:36<02:26,  3.96s/it]Running Inference:  82%|████████▏ | 164/200 [11:44<02:59,  4.98s/it]Running Inference:  82%|████████▎ | 165/200 [11:50<03:12,  5.51s/it]Running Inference:  83%|████████▎ | 166/200 [11:57<03:21,  5.92s/it]Running Inference:  84%|████████▎ | 167/200 [11:59<02:34,  4.69s/it]Running Inference:  84%|████████▍ | 168/200 [12:07<02:59,  5.62s/it]Running Inference:  84%|████████▍ | 169/200 [12:09<02:20,  4.54s/it]Running Inference:  85%|████████▌ | 170/200 [12:16<02:41,  5.38s/it]Running Inference:  86%|████████▌ | 171/200 [12:19<02:15,  4.67s/it]Running Inference:  86%|████████▌ | 172/200 [12:22<01:50,  3.95s/it]Running Inference:  86%|████████▋ | 173/200 [12:24<01:32,  3.41s/it]Running Inference:  87%|████████▋ | 174/200 [12:31<01:57,  4.51s/it]Running Inference:  88%|████████▊ | 175/200 [12:33<01:36,  3.85s/it]Running Inference:  88%|████████▊ | 176/200 [12:40<01:53,  4.73s/it]Running Inference:  88%|████████▊ | 177/200 [12:46<02:01,  5.30s/it]Running Inference:  89%|████████▉ | 178/200 [12:50<01:44,  4.77s/it]Running Inference:  90%|████████▉ | 179/200 [12:52<01:23,  3.96s/it]Running Inference:  90%|█████████ | 180/200 [12:59<01:36,  4.81s/it]Running Inference:  90%|█████████ | 181/200 [13:06<01:45,  5.55s/it]Running Inference:  91%|█████████ | 182/200 [13:10<01:28,  4.91s/it]Running Inference:  92%|█████████▏| 183/200 [13:12<01:12,  4.24s/it]Running Inference:  92%|█████████▏| 184/200 [13:20<01:27,  5.44s/it]Running Inference:  92%|█████████▎| 185/200 [13:23<01:09,  4.64s/it]Running Inference:  93%|█████████▎| 186/200 [13:25<00:52,  3.75s/it]Running Inference:  94%|█████████▎| 187/200 [13:32<01:01,  4.76s/it]Running Inference:  94%|█████████▍| 188/200 [13:34<00:47,  3.93s/it]Running Inference:  94%|█████████▍| 189/200 [13:37<00:39,  3.57s/it]Running Inference:  95%|█████████▌| 190/200 [13:39<00:31,  3.17s/it]Running Inference:  96%|█████████▌| 191/200 [13:46<00:39,  4.35s/it]Running Inference:  96%|█████████▌| 192/200 [13:53<00:40,  5.04s/it]Running Inference:  96%|█████████▋| 193/200 [14:01<00:41,  5.95s/it]Running Inference:  97%|█████████▋| 194/200 [14:03<00:28,  4.76s/it]Running Inference:  98%|█████████▊| 195/200 [14:05<00:19,  3.86s/it]Running Inference:  98%|█████████▊| 196/200 [14:13<00:20,  5.09s/it]Running Inference:  98%|█████████▊| 197/200 [14:14<00:12,  4.00s/it]Running Inference:  99%|█████████▉| 198/200 [14:18<00:07,  3.99s/it]Running Inference: 100%|█████████▉| 199/200 [14:20<00:03,  3.45s/it]Running Inference: 100%|██████████| 200/200 [14:27<00:00,  4.54s/it]Running Inference: 100%|██████████| 200/200 [14:27<00:00,  4.34s/it]
2025-12-14 11:59:59,164 - INFO - Inference completed.
2025-12-14 11:59:59,175 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 11:59:59,175 - INFO - Calculating metrics for dataset: longbench
2025-12-14 11:59:59,308 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 11:59:59,308 - INFO - Metrics:
26.28
2025-12-14 11:59:59,310 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=samsum, ratio=0.3) on GPU 4

----------------------------------------
Task: samsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 12:00:05,852 - INFO - Set deterministic seeds to 42
2025-12-14 12:00:05,853 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 12:00:05,853 - INFO - Starting evaluation run...
2025-12-14 12:00:05,853 - INFO - Output directory set to: longbenchresult
2025-12-14 12:00:05,853 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 12:00:05,853 - INFO - KV Press 'knorm' setup.
2025-12-14 12:00:05,853 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 12:00:05,853 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.10it/s]
Device set to use cuda:0
2025-12-14 12:00:19,930 - INFO - Model pipeline loaded.
2025-12-14 12:00:19,930 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 12:00:24,489 - INFO - Dataset loaded with 200 entries.
2025-12-14 12:00:24,490 - INFO - Dataset processed with 200 entries.
2025-12-14 12:00:24,519 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<10:27,  3.16s/it]Running Inference:   1%|          | 2/200 [00:05<08:06,  2.46s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:34,  2.61s/it]Running Inference:   2%|▏         | 4/200 [00:09<07:05,  2.17s/it]Running Inference:   2%|▎         | 5/200 [00:10<05:40,  1.75s/it]Running Inference:   3%|▎         | 6/200 [00:17<11:42,  3.62s/it]Running Inference:   4%|▎         | 7/200 [00:19<09:20,  2.90s/it]Running Inference:   4%|▍         | 8/200 [00:20<08:03,  2.52s/it]Running Inference:   4%|▍         | 9/200 [00:23<07:59,  2.51s/it]Running Inference:   5%|▌         | 10/200 [00:31<13:24,  4.23s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:33<11:17,  3.59s/it]Running Inference:   6%|▌         | 12/200 [00:36<10:14,  3.27s/it]Running Inference:   6%|▋         | 13/200 [00:37<08:30,  2.73s/it]Running Inference:   7%|▋         | 14/200 [00:42<10:11,  3.29s/it]Running Inference:   8%|▊         | 15/200 [00:44<09:23,  3.04s/it]Running Inference:   8%|▊         | 16/200 [00:49<10:56,  3.57s/it]Running Inference:   8%|▊         | 17/200 [00:52<10:06,  3.31s/it]Running Inference:   9%|▉         | 18/200 [00:54<08:57,  2.95s/it]Running Inference:  10%|▉         | 19/200 [01:02<13:27,  4.46s/it]Running Inference:  10%|█         | 20/200 [01:03<10:55,  3.64s/it]Running Inference:  10%|█         | 21/200 [01:06<09:51,  3.31s/it]Running Inference:  11%|█         | 22/200 [01:11<11:21,  3.83s/it]Running Inference:  12%|█▏        | 23/200 [01:15<11:10,  3.79s/it]Running Inference:  12%|█▏        | 24/200 [01:17<09:26,  3.22s/it]Running Inference:  12%|█▎        | 25/200 [01:17<07:11,  2.47s/it]Running Inference:  13%|█▎        | 26/200 [01:25<11:22,  3.92s/it]Running Inference:  14%|█▎        | 27/200 [01:33<15:07,  5.24s/it]Running Inference:  14%|█▍        | 28/200 [01:36<12:55,  4.51s/it]Running Inference:  14%|█▍        | 29/200 [01:42<14:45,  5.18s/it]Running Inference:  15%|█▌        | 30/200 [01:45<12:13,  4.32s/it]Running Inference:  16%|█▌        | 31/200 [01:46<09:55,  3.53s/it]Running Inference:  16%|█▌        | 32/200 [01:54<13:15,  4.73s/it]Running Inference:  16%|█▋        | 33/200 [02:01<14:44,  5.29s/it]Running Inference:  17%|█▋        | 34/200 [02:03<12:28,  4.51s/it]Running Inference:  18%|█▊        | 35/200 [02:05<09:55,  3.61s/it]Running Inference:  18%|█▊        | 36/200 [02:13<13:19,  4.88s/it]Running Inference:  18%|█▊        | 37/200 [02:15<11:29,  4.23s/it]Running Inference:  19%|█▉        | 38/200 [02:18<10:13,  3.79s/it]Running Inference:  20%|█▉        | 39/200 [02:22<09:57,  3.71s/it]Running Inference:  20%|██        | 40/200 [02:30<13:22,  5.02s/it]Running Inference:  20%|██        | 41/200 [02:32<10:46,  4.06s/it]Running Inference:  21%|██        | 42/200 [02:38<12:54,  4.90s/it]Running Inference:  22%|██▏       | 43/200 [02:40<10:05,  3.86s/it]Running Inference:  22%|██▏       | 44/200 [02:47<13:01,  5.01s/it]Running Inference:  22%|██▎       | 45/200 [02:50<10:49,  4.19s/it]Running Inference:  23%|██▎       | 46/200 [02:57<13:28,  5.25s/it]Running Inference:  24%|██▎       | 47/200 [02:59<10:54,  4.28s/it]Running Inference:  24%|██▍       | 48/200 [03:02<09:32,  3.76s/it]Running Inference:  24%|██▍       | 49/200 [03:06<09:40,  3.84s/it]Running Inference:  25%|██▌       | 50/200 [03:08<07:55,  3.17s/it]Running Inference:  26%|██▌       | 51/200 [03:09<06:45,  2.72s/it]Running Inference:  26%|██▌       | 52/200 [03:17<10:39,  4.32s/it]Running Inference:  26%|██▋       | 53/200 [03:19<08:43,  3.56s/it]Running Inference:  27%|██▋       | 54/200 [03:21<07:07,  2.93s/it]Running Inference:  28%|██▊       | 55/200 [03:29<10:53,  4.51s/it]Running Inference:  28%|██▊       | 56/200 [03:31<09:00,  3.75s/it]Running Inference:  28%|██▊       | 57/200 [03:34<08:29,  3.56s/it]Running Inference:  29%|██▉       | 58/200 [03:42<11:45,  4.97s/it]Running Inference:  30%|██▉       | 59/200 [03:49<12:52,  5.48s/it]Running Inference:  30%|███       | 60/200 [03:51<10:08,  4.35s/it]Running Inference:  30%|███       | 61/200 [03:58<11:59,  5.17s/it]Running Inference:  31%|███       | 62/200 [04:05<13:08,  5.71s/it]Running Inference:  32%|███▏      | 63/200 [04:06<09:54,  4.34s/it]Running Inference:  32%|███▏      | 64/200 [04:08<08:27,  3.73s/it]Running Inference:  32%|███▎      | 65/200 [04:11<07:35,  3.37s/it]Running Inference:  33%|███▎      | 66/200 [04:13<06:53,  3.09s/it]Running Inference:  34%|███▎      | 67/200 [04:20<09:19,  4.20s/it]Running Inference:  34%|███▍      | 68/200 [04:27<10:53,  4.95s/it]Running Inference:  34%|███▍      | 69/200 [04:28<08:42,  3.99s/it]Running Inference:  35%|███▌      | 70/200 [04:36<10:56,  5.05s/it]Running Inference:  36%|███▌      | 71/200 [04:37<08:36,  4.00s/it]Running Inference:  36%|███▌      | 72/200 [04:40<07:21,  3.45s/it]Running Inference:  36%|███▋      | 73/200 [04:41<06:14,  2.95s/it]Running Inference:  37%|███▋      | 74/200 [04:43<05:38,  2.68s/it]Running Inference:  38%|███▊      | 75/200 [04:47<06:03,  2.91s/it]Running Inference:  38%|███▊      | 76/200 [04:48<05:09,  2.50s/it]Running Inference:  38%|███▊      | 77/200 [04:51<05:06,  2.49s/it]Running Inference:  39%|███▉      | 78/200 [04:53<05:02,  2.48s/it]Running Inference:  40%|███▉      | 79/200 [04:56<05:08,  2.55s/it]Running Inference:  40%|████      | 80/200 [04:57<04:21,  2.18s/it]Running Inference:  40%|████      | 81/200 [05:04<06:55,  3.49s/it]Running Inference:  41%|████      | 82/200 [05:05<05:25,  2.76s/it]Running Inference:  42%|████▏     | 83/200 [05:13<08:37,  4.42s/it]Running Inference:  42%|████▏     | 84/200 [05:21<10:43,  5.55s/it]Running Inference:  42%|████▎     | 85/200 [05:29<11:41,  6.10s/it]Running Inference:  43%|████▎     | 86/200 [05:30<08:44,  4.60s/it]Running Inference:  44%|████▎     | 87/200 [05:38<10:24,  5.53s/it]Running Inference:  44%|████▍     | 88/200 [05:45<11:09,  5.98s/it]Running Inference:  44%|████▍     | 89/200 [05:48<09:23,  5.08s/it]Running Inference:  45%|████▌     | 90/200 [05:50<07:46,  4.24s/it]Running Inference:  46%|████▌     | 91/200 [05:55<07:57,  4.38s/it]Running Inference:  46%|████▌     | 92/200 [06:02<09:33,  5.31s/it]Running Inference:  46%|████▋     | 93/200 [06:05<08:20,  4.68s/it]Running Inference:  47%|████▋     | 94/200 [06:07<06:29,  3.68s/it]Running Inference:  48%|████▊     | 95/200 [06:14<08:24,  4.80s/it]Running Inference:  48%|████▊     | 96/200 [06:22<09:46,  5.64s/it]Running Inference:  48%|████▊     | 97/200 [06:23<07:15,  4.23s/it]Running Inference:  49%|████▉     | 98/200 [06:26<06:32,  3.85s/it]Running Inference:  50%|████▉     | 99/200 [06:32<08:00,  4.76s/it]Running Inference:  50%|█████     | 100/200 [06:34<06:34,  3.94s/it]Running Inference:  50%|█████     | 101/200 [06:37<05:45,  3.49s/it]Running Inference:  51%|█████     | 102/200 [06:44<07:37,  4.67s/it]Running Inference:  52%|█████▏    | 103/200 [06:51<08:40,  5.36s/it]Running Inference:  52%|█████▏    | 104/200 [06:53<06:48,  4.26s/it]Running Inference:  52%|█████▎    | 105/200 [07:00<08:16,  5.23s/it]Running Inference:  53%|█████▎    | 106/200 [07:08<09:28,  6.05s/it]Running Inference:  54%|█████▎    | 107/200 [07:15<09:48,  6.33s/it]Running Inference:  54%|█████▍    | 108/200 [07:23<10:14,  6.68s/it]Running Inference:  55%|█████▍    | 109/200 [07:24<07:41,  5.07s/it]Running Inference:  55%|█████▌    | 110/200 [07:27<06:29,  4.33s/it]Running Inference:  56%|█████▌    | 111/200 [07:29<05:20,  3.60s/it]Running Inference:  56%|█████▌    | 112/200 [07:36<06:43,  4.59s/it]Running Inference:  56%|█████▋    | 113/200 [07:37<05:13,  3.61s/it]Running Inference:  57%|█████▋    | 114/200 [07:44<06:30,  4.54s/it]Running Inference:  57%|█████▊    | 115/200 [07:52<07:54,  5.59s/it]Running Inference:  58%|█████▊    | 116/200 [07:55<06:47,  4.85s/it]Running Inference:  58%|█████▊    | 117/200 [07:56<05:18,  3.83s/it]Running Inference:  59%|█████▉    | 118/200 [08:00<05:03,  3.70s/it]Running Inference:  60%|█████▉    | 119/200 [08:02<04:17,  3.18s/it]Running Inference:  60%|██████    | 120/200 [08:10<06:07,  4.60s/it]Running Inference:  60%|██████    | 121/200 [08:12<05:10,  3.94s/it]Running Inference:  61%|██████    | 122/200 [08:14<04:33,  3.50s/it]Running Inference:  62%|██████▏   | 123/200 [08:21<05:47,  4.52s/it]Running Inference:  62%|██████▏   | 124/200 [08:28<06:31,  5.15s/it]Running Inference:  62%|██████▎   | 125/200 [08:31<05:33,  4.45s/it]Running Inference:  63%|██████▎   | 126/200 [08:33<04:48,  3.90s/it]Running Inference:  64%|██████▎   | 127/200 [08:35<03:52,  3.19s/it]Running Inference:  64%|██████▍   | 128/200 [08:37<03:22,  2.81s/it]Running Inference:  64%|██████▍   | 129/200 [08:40<03:20,  2.82s/it]Running Inference:  65%|██████▌   | 130/200 [08:47<04:59,  4.28s/it]Running Inference:  66%|██████▌   | 131/200 [08:50<04:30,  3.92s/it]Running Inference:  66%|██████▌   | 132/200 [08:58<05:43,  5.06s/it]Running Inference:  66%|██████▋   | 133/200 [09:06<06:32,  5.86s/it]Running Inference:  67%|██████▋   | 134/200 [09:08<05:17,  4.81s/it]Running Inference:  68%|██████▊   | 135/200 [09:09<03:54,  3.61s/it]Running Inference:  68%|██████▊   | 136/200 [09:11<03:17,  3.09s/it]Running Inference:  68%|██████▊   | 137/200 [09:13<02:52,  2.74s/it]Running Inference:  69%|██████▉   | 138/200 [09:16<03:02,  2.94s/it]Running Inference:  70%|██████▉   | 139/200 [09:23<04:08,  4.07s/it]Running Inference:  70%|███████   | 140/200 [09:31<05:18,  5.30s/it]Running Inference:  70%|███████   | 141/200 [09:33<04:07,  4.20s/it]Running Inference:  71%|███████   | 142/200 [09:34<03:10,  3.29s/it]Running Inference:  72%|███████▏  | 143/200 [09:35<02:35,  2.73s/it]Running Inference:  72%|███████▏  | 144/200 [09:42<03:32,  3.80s/it]Running Inference:  72%|███████▎  | 145/200 [09:44<02:57,  3.22s/it]Running Inference:  73%|███████▎  | 146/200 [09:45<02:18,  2.57s/it]Running Inference:  74%|███████▎  | 147/200 [09:53<03:43,  4.21s/it]Running Inference:  74%|███████▍  | 148/200 [09:55<03:04,  3.54s/it]Running Inference:  74%|███████▍  | 149/200 [09:58<02:59,  3.53s/it]Running Inference:  75%|███████▌  | 150/200 [10:05<03:52,  4.64s/it]Running Inference:  76%|███████▌  | 151/200 [10:08<03:19,  4.07s/it]Running Inference:  76%|███████▌  | 152/200 [10:09<02:28,  3.10s/it]Running Inference:  76%|███████▋  | 153/200 [10:11<02:11,  2.79s/it]Running Inference:  77%|███████▋  | 154/200 [10:18<03:08,  4.10s/it]Running Inference:  78%|███████▊  | 155/200 [10:19<02:26,  3.26s/it]Running Inference:  78%|███████▊  | 156/200 [10:22<02:09,  2.94s/it]Running Inference:  78%|███████▊  | 157/200 [10:23<01:41,  2.35s/it]Running Inference:  79%|███████▉  | 158/200 [10:29<02:34,  3.68s/it]Running Inference:  80%|███████▉  | 159/200 [10:34<02:38,  3.87s/it]Running Inference:  80%|████████  | 160/200 [10:41<03:10,  4.76s/it]Running Inference:  80%|████████  | 161/200 [10:47<03:30,  5.39s/it]Running Inference:  81%|████████  | 162/200 [10:50<02:49,  4.45s/it]Running Inference:  82%|████████▏ | 163/200 [10:53<02:30,  4.06s/it]Running Inference:  82%|████████▏ | 164/200 [11:00<03:02,  5.06s/it]Running Inference:  82%|████████▎ | 165/200 [11:02<02:21,  4.06s/it]Running Inference:  83%|████████▎ | 166/200 [11:09<02:47,  4.92s/it]Running Inference:  84%|████████▎ | 167/200 [11:11<02:10,  3.96s/it]Running Inference:  84%|████████▍ | 168/200 [11:18<02:44,  5.13s/it]Running Inference:  84%|████████▍ | 169/200 [11:25<02:53,  5.58s/it]Running Inference:  85%|████████▌ | 170/200 [11:27<02:11,  4.38s/it]Running Inference:  86%|████████▌ | 171/200 [11:30<01:54,  3.96s/it]Running Inference:  86%|████████▌ | 172/200 [11:37<02:15,  4.84s/it]Running Inference:  86%|████████▋ | 173/200 [11:38<01:45,  3.93s/it]Running Inference:  87%|████████▋ | 174/200 [11:45<02:06,  4.88s/it]Running Inference:  88%|████████▊ | 175/200 [11:48<01:42,  4.11s/it]Running Inference:  88%|████████▊ | 176/200 [11:55<01:58,  4.93s/it]Running Inference:  88%|████████▊ | 177/200 [11:55<01:23,  3.63s/it]Running Inference:  89%|████████▉ | 178/200 [11:59<01:18,  3.59s/it]Running Inference:  90%|████████▉ | 179/200 [12:02<01:14,  3.54s/it]Running Inference:  90%|█████████ | 180/200 [12:09<01:30,  4.53s/it]Running Inference:  90%|█████████ | 181/200 [12:10<01:09,  3.64s/it]Running Inference:  91%|█████████ | 182/200 [12:17<01:23,  4.62s/it]Running Inference:  92%|█████████▏| 183/200 [12:21<01:13,  4.32s/it]Running Inference:  92%|█████████▏| 184/200 [12:25<01:07,  4.21s/it]Running Inference:  92%|█████████▎| 185/200 [12:27<00:55,  3.67s/it]Running Inference:  93%|█████████▎| 186/200 [12:29<00:42,  3.06s/it]Running Inference:  94%|█████████▎| 187/200 [12:30<00:32,  2.53s/it]Running Inference:  94%|█████████▍| 188/200 [12:32<00:28,  2.39s/it]Running Inference:  94%|█████████▍| 189/200 [12:41<00:46,  4.25s/it]Running Inference:  95%|█████████▌| 190/200 [12:43<00:36,  3.68s/it]Running Inference:  96%|█████████▌| 191/200 [12:50<00:42,  4.73s/it]Running Inference:  96%|█████████▌| 192/200 [12:57<00:42,  5.32s/it]Running Inference:  96%|█████████▋| 193/200 [13:05<00:43,  6.16s/it]Running Inference:  97%|█████████▋| 194/200 [13:06<00:27,  4.53s/it]Running Inference:  98%|█████████▊| 195/200 [13:08<00:18,  3.70s/it]Running Inference:  98%|█████████▊| 196/200 [13:16<00:19,  4.99s/it]Running Inference:  98%|█████████▊| 197/200 [13:17<00:11,  3.92s/it]Running Inference:  99%|█████████▉| 198/200 [13:20<00:07,  3.54s/it]Running Inference: 100%|█████████▉| 199/200 [13:27<00:04,  4.48s/it]Running Inference: 100%|██████████| 200/200 [13:29<00:00,  3.74s/it]Running Inference: 100%|██████████| 200/200 [13:29<00:00,  4.05s/it]
2025-12-14 12:13:53,578 - INFO - Inference completed.
2025-12-14 12:13:53,589 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 12:13:53,589 - INFO - Calculating metrics for dataset: longbench
2025-12-14 12:13:53,709 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 12:13:53,709 - INFO - Metrics:
26.13
2025-12-14 12:13:53,710 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=samsum, ratio=0.5) on GPU 4


========================================
LongBench Task: vcsum
========================================
----------------------------------------
Task: vcsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 12:14:00,165 - INFO - Set deterministic seeds to 42
2025-12-14 12:14:00,165 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 12:14:00,165 - INFO - Starting evaluation run...
2025-12-14 12:14:00,165 - INFO - Output directory set to: longbenchresult
2025-12-14 12:14:00,165 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 12:14:00,165 - INFO - KV Press 'knorm' setup.
2025-12-14 12:14:00,165 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 12:14:00,165 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.51it/s]
Device set to use cuda:0
2025-12-14 12:14:11,460 - INFO - Model pipeline loaded.
2025-12-14 12:14:11,460 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 12:14:20,421 - INFO - Dataset loaded with 200 entries.
2025-12-14 12:14:20,421 - INFO - Dataset processed with 200 entries.
2025-12-14 12:14:20,448 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:21:02, 24.43s/it]Running Inference:   1%|          | 2/200 [00:48<1:20:32, 24.40s/it]Running Inference:   2%|▏         | 3/200 [01:13<1:19:49, 24.31s/it]Running Inference:   2%|▏         | 4/200 [01:36<1:18:46, 24.11s/it]Running Inference:   2%|▎         | 5/200 [02:01<1:18:45, 24.24s/it]Running Inference:   3%|▎         | 6/200 [02:24<1:17:27, 23.95s/it]Running Inference:   4%|▎         | 7/200 [02:48<1:17:07, 23.98s/it]Running Inference:   4%|▍         | 8/200 [03:12<1:16:04, 23.78s/it]Running Inference:   4%|▍         | 9/200 [03:35<1:15:27, 23.70s/it]Running Inference:   5%|▌         | 10/200 [04:00<1:16:31, 24.16s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:24<1:16:00, 24.13s/it]Running Inference:   6%|▌         | 12/200 [04:48<1:14:52, 23.90s/it]Running Inference:   6%|▋         | 13/200 [05:12<1:15:03, 24.08s/it]Running Inference:   7%|▋         | 14/200 [05:35<1:13:48, 23.81s/it]Running Inference:   8%|▊         | 15/200 [05:59<1:13:03, 23.70s/it]Running Inference:   8%|▊         | 16/200 [06:24<1:13:40, 24.03s/it]Running Inference:   8%|▊         | 17/200 [06:47<1:12:42, 23.84s/it]Running Inference:   9%|▉         | 18/200 [07:13<1:14:18, 24.50s/it]Running Inference:  10%|▉         | 19/200 [07:37<1:13:22, 24.32s/it]Running Inference:  10%|█         | 20/200 [08:02<1:13:25, 24.47s/it]Running Inference:  10%|█         | 21/200 [08:26<1:12:53, 24.43s/it]Running Inference:  11%|█         | 22/200 [08:50<1:12:13, 24.34s/it]Running Inference:  12%|█▏        | 23/200 [09:14<1:11:12, 24.14s/it]Running Inference:  12%|█▏        | 24/200 [09:40<1:12:37, 24.76s/it]Running Inference:  12%|█▎        | 25/200 [10:04<1:11:23, 24.47s/it]Running Inference:  13%|█▎        | 26/200 [10:29<1:11:41, 24.72s/it]Running Inference:  14%|█▎        | 27/200 [10:53<1:10:41, 24.52s/it]Running Inference:  14%|█▍        | 28/200 [11:17<1:09:41, 24.31s/it]Running Inference:  14%|█▍        | 29/200 [11:41<1:09:10, 24.27s/it]Running Inference:  15%|█▌        | 30/200 [11:54<58:32, 20.66s/it]  Running Inference:  16%|█▌        | 31/200 [12:17<1:00:41, 21.55s/it]Running Inference:  16%|█▌        | 32/200 [12:40<1:01:38, 22.01s/it]Running Inference:  16%|█▋        | 33/200 [13:03<1:02:17, 22.38s/it]Running Inference:  17%|█▋        | 34/200 [13:27<1:02:39, 22.65s/it]Running Inference:  18%|█▊        | 35/200 [13:51<1:03:35, 23.12s/it]Running Inference:  18%|█▊        | 36/200 [14:15<1:03:41, 23.30s/it]Running Inference:  18%|█▊        | 37/200 [14:38<1:03:32, 23.39s/it]Running Inference:  19%|█▉        | 38/200 [15:04<1:04:42, 23.97s/it]Running Inference:  20%|█▉        | 39/200 [15:28<1:04:29, 24.03s/it]Running Inference:  20%|██        | 40/200 [15:53<1:05:22, 24.51s/it]Running Inference:  20%|██        | 41/200 [16:21<1:07:41, 25.54s/it]Running Inference:  21%|██        | 42/200 [16:45<1:05:59, 25.06s/it]Running Inference:  22%|██▏       | 43/200 [17:10<1:04:55, 24.81s/it]Running Inference:  22%|██▏       | 44/200 [17:35<1:05:06, 25.04s/it]Running Inference:  22%|██▎       | 45/200 [17:59<1:03:46, 24.69s/it]Running Inference:  23%|██▎       | 46/200 [18:23<1:02:29, 24.35s/it]Running Inference:  24%|██▎       | 47/200 [18:48<1:02:44, 24.60s/it]Running Inference:  24%|██▍       | 48/200 [19:11<1:01:37, 24.33s/it]Running Inference:  24%|██▍       | 49/200 [19:35<1:00:39, 24.10s/it]Running Inference:  25%|██▌       | 50/200 [19:59<59:49, 23.93s/it]  Running Inference:  26%|██▌       | 51/200 [20:23<59:52, 24.11s/it]Running Inference:  26%|██▌       | 52/200 [20:48<59:42, 24.21s/it]Running Inference:  26%|██▋       | 53/200 [21:11<59:04, 24.11s/it]Running Inference:  27%|██▋       | 54/200 [21:35<58:27, 24.02s/it]Running Inference:  28%|██▊       | 55/200 [21:59<57:51, 23.94s/it]Running Inference:  28%|██▊       | 56/200 [22:23<57:17, 23.87s/it]Running Inference:  28%|██▊       | 57/200 [22:46<56:30, 23.71s/it]Running Inference:  29%|██▉       | 58/200 [23:10<56:32, 23.89s/it]Running Inference:  30%|██▉       | 59/200 [23:35<56:26, 24.02s/it]Running Inference:  30%|███       | 60/200 [23:59<56:30, 24.22s/it]Running Inference:  30%|███       | 61/200 [24:23<55:41, 24.04s/it]Running Inference:  31%|███       | 62/200 [24:46<54:55, 23.88s/it]Running Inference:  32%|███▏      | 63/200 [25:10<54:35, 23.91s/it]Running Inference:  32%|███▏      | 64/200 [25:35<54:27, 24.02s/it]Running Inference:  32%|███▎      | 65/200 [25:59<54:15, 24.12s/it]Running Inference:  33%|███▎      | 66/200 [26:23<53:27, 23.94s/it]Running Inference:  34%|███▎      | 67/200 [26:47<53:08, 23.97s/it]Running Inference:  34%|███▍      | 68/200 [27:11<52:45, 23.98s/it]Running Inference:  34%|███▍      | 69/200 [27:36<53:08, 24.34s/it]Running Inference:  35%|███▌      | 70/200 [27:59<52:18, 24.14s/it]Running Inference:  36%|███▌      | 71/200 [28:23<51:34, 23.98s/it]Running Inference:  36%|███▌      | 72/200 [28:48<51:33, 24.16s/it]Running Inference:  36%|███▋      | 73/200 [29:12<51:11, 24.18s/it]Running Inference:  37%|███▋      | 74/200 [29:36<50:29, 24.05s/it]Running Inference:  38%|███▊      | 75/200 [30:00<49:59, 23.99s/it]Running Inference:  38%|███▊      | 76/200 [30:23<49:21, 23.88s/it]Running Inference:  38%|███▊      | 77/200 [30:48<49:50, 24.31s/it]Running Inference:  39%|███▉      | 78/200 [31:12<48:56, 24.07s/it]Running Inference:  40%|███▉      | 79/200 [31:36<48:27, 24.03s/it]Running Inference:  40%|████      | 80/200 [32:00<47:50, 23.92s/it]Running Inference:  40%|████      | 81/200 [32:24<47:30, 23.95s/it]Running Inference:  41%|████      | 82/200 [32:47<47:02, 23.92s/it]Running Inference:  42%|████▏     | 83/200 [33:11<46:34, 23.89s/it]Running Inference:  42%|████▏     | 84/200 [33:36<46:37, 24.11s/it]Running Inference:  42%|████▎     | 85/200 [34:00<46:15, 24.14s/it]Running Inference:  43%|████▎     | 86/200 [34:25<46:06, 24.27s/it]Running Inference:  44%|████▎     | 87/200 [34:49<45:30, 24.16s/it]Running Inference:  44%|████▍     | 88/200 [35:12<44:58, 24.09s/it]Running Inference:  44%|████▍     | 89/200 [35:36<44:07, 23.85s/it]Running Inference:  45%|████▌     | 90/200 [35:59<43:39, 23.81s/it]Running Inference:  46%|████▌     | 91/200 [36:24<43:43, 24.07s/it]Running Inference:  46%|████▌     | 92/200 [36:47<42:54, 23.84s/it]Running Inference:  46%|████▋     | 93/200 [37:12<43:04, 24.15s/it]Running Inference:  47%|████▋     | 94/200 [37:38<43:12, 24.46s/it]Running Inference:  48%|████▊     | 95/200 [38:03<43:22, 24.79s/it]Running Inference:  48%|████▊     | 96/200 [38:27<42:20, 24.43s/it]Running Inference:  48%|████▊     | 97/200 [38:50<41:28, 24.16s/it]Running Inference:  49%|████▉     | 98/200 [39:14<41:08, 24.20s/it]Running Inference:  50%|████▉     | 99/200 [39:38<40:32, 24.08s/it]Running Inference:  50%|█████     | 100/200 [40:03<40:15, 24.16s/it]Running Inference:  50%|█████     | 101/200 [40:28<40:36, 24.61s/it]Running Inference:  51%|█████     | 102/200 [40:56<41:49, 25.61s/it]Running Inference:  52%|█████▏    | 103/200 [41:21<41:11, 25.48s/it]Running Inference:  52%|█████▏    | 104/200 [41:45<40:05, 25.06s/it]Running Inference:  52%|█████▎    | 105/200 [42:13<40:40, 25.69s/it]Running Inference:  53%|█████▎    | 106/200 [42:37<39:25, 25.17s/it]Running Inference:  54%|█████▎    | 107/200 [43:00<38:13, 24.66s/it]Running Inference:  54%|█████▍    | 108/200 [43:24<37:34, 24.50s/it]Running Inference:  55%|█████▍    | 109/200 [43:48<36:44, 24.22s/it]Running Inference:  55%|█████▌    | 110/200 [44:12<36:19, 24.22s/it]Running Inference:  56%|█████▌    | 111/200 [44:35<35:30, 23.94s/it]Running Inference:  56%|█████▌    | 112/200 [44:59<35:10, 23.98s/it]Running Inference:  56%|█████▋    | 113/200 [45:23<34:42, 23.94s/it]Running Inference:  57%|█████▋    | 114/200 [45:47<34:14, 23.89s/it]Running Inference:  57%|█████▊    | 115/200 [46:11<33:58, 23.99s/it]Running Inference:  58%|█████▊    | 116/200 [46:35<33:32, 23.96s/it]Running Inference:  58%|█████▊    | 117/200 [46:59<33:08, 23.95s/it]Running Inference:  59%|█████▉    | 118/200 [47:22<32:26, 23.74s/it]Running Inference:  60%|█████▉    | 119/200 [47:47<32:18, 23.93s/it]Running Inference:  60%|██████    | 120/200 [48:11<32:16, 24.20s/it]Running Inference:  60%|██████    | 121/200 [48:36<32:04, 24.36s/it]Running Inference:  61%|██████    | 122/200 [48:59<31:11, 24.00s/it]Running Inference:  62%|██████▏   | 123/200 [49:25<31:18, 24.40s/it]Running Inference:  62%|██████▏   | 124/200 [49:50<31:07, 24.57s/it]Running Inference:  62%|██████▎   | 125/200 [50:13<30:25, 24.33s/it]Running Inference:  63%|██████▎   | 126/200 [50:37<29:54, 24.25s/it]Running Inference:  64%|██████▎   | 127/200 [51:02<29:46, 24.47s/it]Running Inference:  64%|██████▍   | 128/200 [51:27<29:15, 24.38s/it]Running Inference:  64%|██████▍   | 129/200 [51:51<28:46, 24.31s/it]Running Inference:  65%|██████▌   | 130/200 [52:14<27:59, 23.99s/it]Running Inference:  66%|██████▌   | 131/200 [52:38<27:35, 23.99s/it]Running Inference:  66%|██████▌   | 132/200 [53:02<27:06, 23.91s/it]Running Inference:  66%|██████▋   | 133/200 [53:26<26:52, 24.07s/it]Running Inference:  67%|██████▋   | 134/200 [53:50<26:16, 23.89s/it]Running Inference:  68%|██████▊   | 135/200 [54:13<25:50, 23.85s/it]Running Inference:  68%|██████▊   | 136/200 [54:37<25:16, 23.70s/it]Running Inference:  68%|██████▊   | 137/200 [55:01<24:54, 23.72s/it]Running Inference:  69%|██████▉   | 138/200 [55:24<24:23, 23.61s/it]Running Inference:  70%|██████▉   | 139/200 [55:49<24:22, 23.97s/it]Running Inference:  70%|███████   | 140/200 [56:13<24:13, 24.22s/it]Running Inference:  70%|███████   | 141/200 [56:37<23:42, 24.11s/it]Running Inference:  71%|███████   | 142/200 [57:01<23:15, 24.06s/it]Running Inference:  72%|███████▏  | 143/200 [57:26<22:57, 24.17s/it]Running Inference:  72%|███████▏  | 144/200 [57:50<22:28, 24.07s/it]Running Inference:  72%|███████▎  | 145/200 [58:15<22:30, 24.55s/it]Running Inference:  73%|███████▎  | 146/200 [58:39<21:53, 24.32s/it]Running Inference:  74%|███████▎  | 147/200 [59:03<21:16, 24.08s/it]Running Inference:  74%|███████▍  | 148/200 [59:26<20:45, 23.95s/it]Running Inference:  74%|███████▍  | 149/200 [59:50<20:25, 24.04s/it]Running Inference:  75%|███████▌  | 150/200 [1:00:15<20:17, 24.34s/it]Running Inference:  76%|███████▌  | 151/200 [1:00:40<20:00, 24.50s/it]Running Inference:  76%|███████▌  | 152/200 [1:01:05<19:44, 24.68s/it]Running Inference:  76%|███████▋  | 153/200 [1:01:29<19:07, 24.41s/it]Running Inference:  77%|███████▋  | 154/200 [1:01:52<18:25, 24.03s/it]Running Inference:  78%|███████▊  | 155/200 [1:02:16<17:54, 23.87s/it]Running Inference:  78%|███████▊  | 156/200 [1:02:39<17:22, 23.70s/it]Running Inference:  78%|███████▊  | 157/200 [1:03:03<16:57, 23.66s/it]Running Inference:  79%|███████▉  | 158/200 [1:03:27<16:36, 23.72s/it]Running Inference:  80%|███████▉  | 159/200 [1:03:50<16:03, 23.49s/it]Running Inference:  80%|████████  | 160/200 [1:04:15<16:02, 24.06s/it]Running Inference:  80%|████████  | 161/200 [1:04:38<15:31, 23.89s/it]Running Inference:  81%|████████  | 162/200 [1:05:03<15:17, 24.14s/it]Running Inference:  82%|████████▏ | 163/200 [1:05:29<15:17, 24.80s/it]Running Inference:  82%|████████▏ | 164/200 [1:05:53<14:41, 24.49s/it]Running Inference:  82%|████████▎ | 165/200 [1:06:18<14:24, 24.69s/it]Running Inference:  83%|████████▎ | 166/200 [1:06:42<13:44, 24.25s/it]Running Inference:  84%|████████▎ | 167/200 [1:07:05<13:12, 24.02s/it]Running Inference:  84%|████████▍ | 168/200 [1:07:29<12:45, 23.93s/it]Running Inference:  84%|████████▍ | 169/200 [1:07:52<12:17, 23.80s/it]Running Inference:  85%|████████▌ | 170/200 [1:08:16<11:53, 23.78s/it]Running Inference:  86%|████████▌ | 171/200 [1:08:39<11:24, 23.60s/it]Running Inference:  86%|████████▌ | 172/200 [1:09:03<11:03, 23.69s/it]Running Inference:  86%|████████▋ | 173/200 [1:09:28<10:51, 24.14s/it]Running Inference:  87%|████████▋ | 174/200 [1:09:52<10:23, 23.99s/it]Running Inference:  88%|████████▊ | 175/200 [1:10:16<09:58, 23.93s/it]Running Inference:  88%|████████▊ | 176/200 [1:10:39<09:29, 23.74s/it]Running Inference:  88%|████████▊ | 177/200 [1:11:02<09:02, 23.60s/it]Running Inference:  89%|████████▉ | 178/200 [1:11:26<08:36, 23.48s/it]Running Inference:  90%|████████▉ | 179/200 [1:11:50<08:18, 23.73s/it]Running Inference:  90%|█████████ | 180/200 [1:12:13<07:51, 23.57s/it]Running Inference:  90%|█████████ | 181/200 [1:12:37<07:27, 23.53s/it]Running Inference:  91%|█████████ | 182/200 [1:13:01<07:06, 23.70s/it]Running Inference:  92%|█████████▏| 183/200 [1:13:24<06:41, 23.64s/it]Running Inference:  92%|█████████▏| 184/200 [1:13:48<06:17, 23.59s/it]Running Inference:  92%|█████████▎| 185/200 [1:14:11<05:51, 23.45s/it]Running Inference:  93%|█████████▎| 186/200 [1:14:34<05:28, 23.48s/it]Running Inference:  94%|█████████▎| 187/200 [1:14:58<05:04, 23.43s/it]Running Inference:  94%|█████████▍| 188/200 [1:15:22<04:43, 23.60s/it]Running Inference:  94%|█████████▍| 189/200 [1:15:45<04:18, 23.47s/it]Running Inference:  95%|█████████▌| 190/200 [1:16:08<03:55, 23.51s/it]Running Inference:  96%|█████████▌| 191/200 [1:16:32<03:30, 23.41s/it]Running Inference:  96%|█████████▌| 192/200 [1:16:55<03:07, 23.42s/it]Running Inference:  96%|█████████▋| 193/200 [1:17:19<02:44, 23.51s/it]Running Inference:  97%|█████████▋| 194/200 [1:17:43<02:22, 23.67s/it]Running Inference:  98%|█████████▊| 195/200 [1:18:09<02:01, 24.36s/it]Running Inference:  98%|█████████▊| 196/200 [1:18:33<01:36, 24.23s/it]Running Inference:  98%|█████████▊| 197/200 [1:18:57<01:12, 24.17s/it]Running Inference:  99%|█████████▉| 198/200 [1:19:20<00:47, 23.97s/it]Running Inference: 100%|█████████▉| 199/200 [1:19:44<00:23, 23.83s/it]Running Inference: 100%|██████████| 200/200 [1:20:07<00:00, 23.81s/it]Running Inference: 100%|██████████| 200/200 [1:20:07<00:00, 24.04s/it]
2025-12-14 13:34:28,385 - INFO - Inference completed.
2025-12-14 13:34:28,398 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 13:34:28,398 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.663 seconds.
Prefix dict has been built successfully.
2025-12-14 13:34:37,299 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 13:34:37,299 - INFO - Metrics:
7.31
2025-12-14 13:34:37,300 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: knorm (task=vcsum, ratio=0.1) on GPU 4

----------------------------------------
Task: vcsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 13:34:44,491 - INFO - Set deterministic seeds to 42
2025-12-14 13:34:44,491 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 13:34:44,491 - INFO - Starting evaluation run...
2025-12-14 13:34:44,491 - INFO - Output directory set to: longbenchresult
2025-12-14 13:34:44,491 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 13:34:44,491 - INFO - KV Press 'knorm' setup.
2025-12-14 13:34:44,491 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 13:34:44,491 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.38it/s]
Device set to use cuda:0
2025-12-14 13:34:55,990 - INFO - Model pipeline loaded.
2025-12-14 13:34:55,990 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 13:35:01,569 - INFO - Dataset loaded with 200 entries.
2025-12-14 13:35:01,569 - INFO - Dataset processed with 200 entries.
2025-12-14 13:35:01,594 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:19, 23.92s/it]Running Inference:   1%|          | 2/200 [00:47<1:18:45, 23.87s/it]Running Inference:   2%|▏         | 3/200 [01:11<1:18:01, 23.77s/it]Running Inference:   2%|▏         | 4/200 [01:34<1:16:58, 23.56s/it]Running Inference:   2%|▎         | 5/200 [01:58<1:16:59, 23.69s/it]Running Inference:   3%|▎         | 6/200 [02:21<1:15:41, 23.41s/it]Running Inference:   4%|▎         | 7/200 [02:44<1:15:24, 23.45s/it]Running Inference:   4%|▍         | 8/200 [03:07<1:14:22, 23.24s/it]Running Inference:   4%|▍         | 9/200 [03:30<1:13:45, 23.17s/it]Running Inference:   5%|▌         | 10/200 [03:55<1:14:40, 23.58s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:18<1:14:12, 23.56s/it]Running Inference:   6%|▌         | 12/200 [04:41<1:13:06, 23.33s/it]Running Inference:   6%|▋         | 13/200 [05:05<1:13:20, 23.53s/it]Running Inference:   7%|▋         | 14/200 [05:28<1:12:08, 23.27s/it]Running Inference:   8%|▊         | 15/200 [05:34<55:50, 18.11s/it]  Running Inference:   8%|▊         | 16/200 [05:58<1:01:11, 19.96s/it]Running Inference:   8%|▊         | 17/200 [06:21<1:03:32, 20.83s/it]Running Inference:   9%|▉         | 18/200 [06:46<1:07:15, 22.17s/it]Running Inference:  10%|▉         | 19/200 [07:10<1:07:59, 22.54s/it]Running Inference:  10%|█         | 20/200 [07:34<1:09:13, 23.07s/it]Running Inference:  10%|█         | 21/200 [07:58<1:09:27, 23.28s/it]Running Inference:  11%|█         | 22/200 [08:21<1:09:22, 23.39s/it]Running Inference:  12%|█▏        | 23/200 [08:45<1:08:43, 23.30s/it]Running Inference:  12%|█▏        | 24/200 [09:10<1:10:12, 23.94s/it]Running Inference:  12%|█▎        | 25/200 [09:33<1:09:13, 23.74s/it]Running Inference:  13%|█▎        | 26/200 [09:58<1:09:36, 24.00s/it]Running Inference:  14%|█▎        | 27/200 [10:21<1:08:47, 23.86s/it]Running Inference:  14%|█▍        | 28/200 [10:45<1:07:57, 23.71s/it]Running Inference:  14%|█▍        | 29/200 [11:08<1:07:32, 23.70s/it]Running Inference:  15%|█▌        | 30/200 [11:31<1:06:32, 23.49s/it]Running Inference:  16%|█▌        | 31/200 [11:54<1:05:49, 23.37s/it]Running Inference:  16%|█▌        | 32/200 [12:17<1:04:46, 23.13s/it]Running Inference:  16%|█▋        | 33/200 [12:40<1:04:03, 23.01s/it]Running Inference:  17%|█▋        | 34/200 [13:03<1:03:26, 22.93s/it]Running Inference:  18%|█▊        | 35/200 [13:26<1:03:40, 23.16s/it]Running Inference:  18%|█▊        | 36/200 [13:49<1:03:18, 23.16s/it]Running Inference:  18%|█▊        | 37/200 [14:12<1:02:50, 23.13s/it]Running Inference:  19%|█▉        | 38/200 [14:37<1:03:40, 23.58s/it]Running Inference:  20%|█▉        | 39/200 [15:01<1:03:19, 23.60s/it]Running Inference:  20%|██        | 40/200 [15:26<1:03:56, 23.98s/it]Running Inference:  20%|██        | 41/200 [15:53<1:06:00, 24.91s/it]Running Inference:  21%|██        | 42/200 [16:16<1:04:24, 24.46s/it]Running Inference:  22%|██▏       | 43/200 [16:40<1:03:23, 24.22s/it]Running Inference:  22%|██▏       | 44/200 [17:05<1:03:27, 24.41s/it]Running Inference:  22%|██▎       | 45/200 [17:28<1:02:12, 24.08s/it]Running Inference:  23%|██▎       | 46/200 [17:51<1:00:59, 23.76s/it]Running Inference:  24%|██▎       | 47/200 [18:15<1:01:10, 23.99s/it]Running Inference:  24%|██▍       | 48/200 [18:39<1:00:09, 23.75s/it]Running Inference:  24%|██▍       | 49/200 [19:02<59:12, 23.53s/it]  Running Inference:  25%|██▌       | 50/200 [19:25<58:25, 23.37s/it]Running Inference:  26%|██▌       | 51/200 [19:49<58:30, 23.56s/it]Running Inference:  26%|██▌       | 52/200 [20:13<58:22, 23.67s/it]Running Inference:  26%|██▋       | 53/200 [20:36<57:43, 23.56s/it]Running Inference:  27%|██▋       | 54/200 [20:59<57:07, 23.48s/it]Running Inference:  28%|██▊       | 55/200 [21:22<56:31, 23.39s/it]Running Inference:  28%|██▊       | 56/200 [21:46<55:58, 23.32s/it]Running Inference:  28%|██▊       | 57/200 [22:08<55:09, 23.15s/it]Running Inference:  29%|██▉       | 58/200 [22:32<55:11, 23.32s/it]Running Inference:  30%|██▉       | 59/200 [22:56<55:07, 23.46s/it]Running Inference:  30%|███       | 60/200 [23:20<55:11, 23.65s/it]Running Inference:  30%|███       | 61/200 [23:43<54:22, 23.47s/it]Running Inference:  31%|███       | 62/200 [24:06<53:37, 23.32s/it]Running Inference:  32%|███▏      | 63/200 [24:29<53:19, 23.35s/it]Running Inference:  32%|███▏      | 64/200 [24:53<53:09, 23.45s/it]Running Inference:  32%|███▎      | 65/200 [25:17<52:58, 23.54s/it]Running Inference:  33%|███▎      | 66/200 [25:40<52:11, 23.37s/it]Running Inference:  34%|███▎      | 67/200 [26:03<51:54, 23.42s/it]Running Inference:  34%|███▍      | 68/200 [26:27<51:32, 23.43s/it]Running Inference:  34%|███▍      | 69/200 [26:51<51:50, 23.75s/it]Running Inference:  35%|███▌      | 70/200 [27:14<51:04, 23.57s/it]Running Inference:  36%|███▌      | 71/200 [27:37<50:20, 23.41s/it]Running Inference:  36%|███▌      | 72/200 [28:01<50:20, 23.60s/it]Running Inference:  36%|███▋      | 73/200 [28:25<49:59, 23.62s/it]Running Inference:  37%|███▋      | 74/200 [28:48<49:20, 23.50s/it]Running Inference:  38%|███▊      | 75/200 [29:12<48:50, 23.44s/it]Running Inference:  38%|███▊      | 76/200 [29:35<48:11, 23.32s/it]Running Inference:  38%|███▊      | 77/200 [29:59<48:38, 23.73s/it]Running Inference:  39%|███▉      | 78/200 [30:22<47:49, 23.52s/it]Running Inference:  40%|███▉      | 79/200 [30:46<47:20, 23.48s/it]Running Inference:  40%|████      | 80/200 [31:09<46:44, 23.37s/it]Running Inference:  40%|████      | 81/200 [31:32<46:25, 23.41s/it]Running Inference:  41%|████      | 82/200 [31:56<45:58, 23.37s/it]Running Inference:  42%|████▏     | 83/200 [32:19<45:30, 23.34s/it]Running Inference:  42%|████▏     | 84/200 [32:43<45:32, 23.56s/it]Running Inference:  42%|████▎     | 85/200 [33:07<45:11, 23.58s/it]Running Inference:  43%|████▎     | 86/200 [33:31<45:02, 23.71s/it]Running Inference:  44%|████▎     | 87/200 [33:54<44:25, 23.59s/it]Running Inference:  44%|████▍     | 88/200 [34:17<43:53, 23.51s/it]Running Inference:  44%|████▍     | 89/200 [34:40<42:59, 23.24s/it]Running Inference:  45%|████▌     | 90/200 [35:03<42:31, 23.20s/it]Running Inference:  46%|████▌     | 91/200 [35:27<42:37, 23.46s/it]Running Inference:  46%|████▌     | 92/200 [35:50<41:49, 23.24s/it]Running Inference:  46%|████▋     | 93/200 [36:14<42:00, 23.55s/it]Running Inference:  47%|████▋     | 94/200 [36:39<42:06, 23.84s/it]Running Inference:  48%|████▊     | 95/200 [37:03<42:11, 24.11s/it]Running Inference:  48%|████▊     | 96/200 [37:26<41:12, 23.77s/it]Running Inference:  48%|████▊     | 97/200 [37:49<40:23, 23.53s/it]Running Inference:  49%|████▉     | 98/200 [38:13<40:03, 23.57s/it]Running Inference:  50%|████▉     | 99/200 [38:36<39:29, 23.46s/it]Running Inference:  50%|█████     | 100/200 [39:00<39:14, 23.55s/it]Running Inference:  50%|█████     | 101/200 [39:25<39:29, 23.94s/it]Running Inference:  51%|█████     | 102/200 [39:52<40:35, 24.85s/it]Running Inference:  52%|█████▏    | 103/200 [40:16<40:01, 24.76s/it]Running Inference:  52%|█████▏    | 104/200 [40:40<38:59, 24.37s/it]Running Inference:  52%|█████▎    | 105/200 [41:06<39:29, 24.94s/it]Running Inference:  53%|█████▎    | 106/200 [41:29<38:19, 24.46s/it]Running Inference:  54%|█████▎    | 107/200 [41:52<37:10, 23.98s/it]Running Inference:  54%|█████▍    | 108/200 [42:16<36:34, 23.85s/it]Running Inference:  55%|█████▍    | 109/200 [42:39<35:46, 23.59s/it]Running Inference:  55%|█████▌    | 110/200 [43:02<35:23, 23.60s/it]Running Inference:  56%|█████▌    | 111/200 [43:25<34:35, 23.32s/it]Running Inference:  56%|█████▌    | 112/200 [43:48<34:16, 23.37s/it]Running Inference:  56%|█████▋    | 113/200 [44:12<33:50, 23.34s/it]Running Inference:  57%|█████▋    | 114/200 [44:35<33:22, 23.29s/it]Running Inference:  57%|█████▊    | 115/200 [44:59<33:07, 23.39s/it]Running Inference:  58%|█████▊    | 116/200 [45:22<32:43, 23.38s/it]Running Inference:  58%|█████▊    | 117/200 [45:45<32:19, 23.37s/it]Running Inference:  59%|█████▉    | 118/200 [46:08<31:38, 23.16s/it]Running Inference:  60%|█████▉    | 119/200 [46:32<31:31, 23.35s/it]Running Inference:  60%|██████    | 120/200 [46:56<31:30, 23.63s/it]Running Inference:  60%|██████    | 121/200 [47:20<31:19, 23.79s/it]Running Inference:  61%|██████    | 122/200 [47:43<30:28, 23.44s/it]Running Inference:  62%|██████▏   | 123/200 [48:07<30:31, 23.78s/it]Running Inference:  62%|██████▏   | 124/200 [48:32<30:21, 23.96s/it]Running Inference:  62%|██████▎   | 125/200 [48:55<29:40, 23.74s/it]Running Inference:  63%|██████▎   | 126/200 [49:18<29:10, 23.66s/it]Running Inference:  64%|██████▎   | 127/200 [49:43<29:03, 23.88s/it]Running Inference:  64%|██████▍   | 128/200 [50:06<28:33, 23.81s/it]Running Inference:  64%|██████▍   | 129/200 [50:30<28:06, 23.75s/it]Running Inference:  65%|██████▌   | 130/200 [50:53<27:19, 23.42s/it]Running Inference:  66%|██████▌   | 131/200 [51:16<26:56, 23.43s/it]Running Inference:  66%|██████▌   | 132/200 [51:39<26:27, 23.35s/it]Running Inference:  66%|██████▋   | 133/200 [52:03<26:14, 23.50s/it]Running Inference:  67%|██████▋   | 134/200 [52:26<25:39, 23.32s/it]Running Inference:  68%|██████▊   | 135/200 [52:49<25:14, 23.29s/it]Running Inference:  68%|██████▊   | 136/200 [53:12<24:40, 23.14s/it]Running Inference:  68%|██████▊   | 137/200 [53:35<24:19, 23.16s/it]Running Inference:  69%|██████▉   | 138/200 [53:58<23:49, 23.05s/it]Running Inference:  70%|██████▉   | 139/200 [54:22<23:48, 23.41s/it]Running Inference:  70%|███████   | 140/200 [54:47<23:38, 23.65s/it]Running Inference:  70%|███████   | 141/200 [55:10<23:09, 23.55s/it]Running Inference:  71%|███████   | 142/200 [55:20<18:59, 19.65s/it]Running Inference:  72%|███████▏  | 143/200 [55:44<19:52, 20.92s/it]Running Inference:  72%|███████▏  | 144/200 [56:08<20:11, 21.63s/it]Running Inference:  72%|███████▎  | 145/200 [56:32<20:43, 22.61s/it]Running Inference:  73%|███████▎  | 146/200 [56:56<20:31, 22.80s/it]Running Inference:  74%|███████▎  | 147/200 [57:19<20:11, 22.86s/it]Running Inference:  74%|███████▍  | 148/200 [57:42<19:53, 22.94s/it]Running Inference:  74%|███████▍  | 149/200 [58:06<19:41, 23.16s/it]Running Inference:  75%|███████▌  | 150/200 [58:30<19:37, 23.56s/it]Running Inference:  76%|███████▌  | 151/200 [58:54<19:25, 23.79s/it]Running Inference:  76%|███████▌  | 152/200 [59:19<19:10, 23.97s/it]Running Inference:  76%|███████▋  | 153/200 [59:42<18:38, 23.81s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:05<18:02, 23.52s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:28<17:33, 23.41s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:51<17:02, 23.24s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:14<16:38, 23.22s/it]Running Inference:  79%|███████▉  | 158/200 [1:01:38<16:18, 23.30s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:00<15:45, 23.07s/it]Running Inference:  80%|████████  | 160/200 [1:02:25<15:42, 23.57s/it]Running Inference:  80%|████████  | 161/200 [1:02:48<15:13, 23.43s/it]Running Inference:  81%|████████  | 162/200 [1:03:12<15:00, 23.71s/it]Running Inference:  82%|████████▏ | 163/200 [1:03:38<14:58, 24.29s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:01<14:24, 24.02s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:26<14:06, 24.19s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:49<13:28, 23.79s/it]Running Inference:  84%|████████▎ | 167/200 [1:05:12<12:58, 23.60s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:35<12:32, 23.52s/it]Running Inference:  84%|████████▍ | 169/200 [1:05:58<12:04, 23.38s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:22<11:41, 23.37s/it]Running Inference:  86%|████████▌ | 171/200 [1:06:45<11:12, 23.19s/it]Running Inference:  86%|████████▌ | 172/200 [1:07:08<10:52, 23.29s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:33<10:39, 23.69s/it]Running Inference:  87%|████████▋ | 174/200 [1:07:56<10:12, 23.55s/it]Running Inference:  88%|████████▊ | 175/200 [1:08:19<09:47, 23.51s/it]Running Inference:  88%|████████▊ | 176/200 [1:08:42<09:19, 23.32s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:05<08:53, 23.18s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:28<08:27, 23.06s/it]Running Inference:  90%|████████▉ | 179/200 [1:09:52<08:09, 23.32s/it]Running Inference:  90%|█████████ | 180/200 [1:10:15<07:43, 23.17s/it]Running Inference:  90%|█████████ | 181/200 [1:10:38<07:19, 23.13s/it]Running Inference:  91%|█████████ | 182/200 [1:11:01<06:59, 23.29s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:24<06:34, 23.23s/it]Running Inference:  92%|█████████▏| 184/200 [1:11:47<06:10, 23.18s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:10<05:45, 23.04s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:33<05:23, 23.07s/it]Running Inference:  94%|█████████▎| 187/200 [1:12:56<04:59, 23.03s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:20<04:38, 23.19s/it]Running Inference:  94%|█████████▍| 189/200 [1:13:43<04:13, 23.06s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:06<03:50, 23.08s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:29<03:26, 22.99s/it]Running Inference:  96%|█████████▌| 192/200 [1:14:52<03:04, 23.00s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:15<02:41, 23.09s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:38<02:19, 23.25s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:04<01:59, 23.87s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:27<01:35, 23.76s/it]Running Inference:  98%|█████████▊| 197/200 [1:16:51<01:11, 23.73s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:14<00:47, 23.59s/it]Running Inference: 100%|█████████▉| 199/200 [1:17:37<00:23, 23.46s/it]Running Inference: 100%|██████████| 200/200 [1:18:01<00:00, 23.43s/it]Running Inference: 100%|██████████| 200/200 [1:18:01<00:00, 23.41s/it]
2025-12-14 14:53:02,815 - INFO - Inference completed.
2025-12-14 14:53:02,828 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 14:53:02,828 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.662 seconds.
Prefix dict has been built successfully.
2025-12-14 14:53:12,399 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 14:53:12,399 - INFO - Metrics:
6.17
2025-12-14 14:53:12,401 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: knorm (task=vcsum, ratio=0.2) on GPU 4

----------------------------------------
Task: vcsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 14:53:19,633 - INFO - Set deterministic seeds to 42
2025-12-14 14:53:19,633 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 14:53:19,633 - INFO - Starting evaluation run...
2025-12-14 14:53:19,633 - INFO - Output directory set to: longbenchresult
2025-12-14 14:53:19,633 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 14:53:19,633 - INFO - KV Press 'knorm' setup.
2025-12-14 14:53:19,633 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 14:53:19,633 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.09it/s]
Device set to use cuda:0
2025-12-14 14:53:32,554 - INFO - Model pipeline loaded.
2025-12-14 14:53:32,554 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 14:53:38,378 - INFO - Dataset loaded with 200 entries.
2025-12-14 14:53:38,378 - INFO - Dataset processed with 200 entries.
2025-12-14 14:53:38,403 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:14, 24.20s/it]Running Inference:   1%|          | 2/200 [00:48<1:19:35, 24.12s/it]Running Inference:   2%|▏         | 3/200 [01:12<1:18:50, 24.01s/it]Running Inference:   2%|▏         | 4/200 [01:35<1:17:46, 23.81s/it]Running Inference:   2%|▎         | 5/200 [01:59<1:17:52, 23.96s/it]Running Inference:   3%|▎         | 6/200 [02:22<1:16:33, 23.68s/it]Running Inference:   4%|▎         | 7/200 [02:46<1:16:16, 23.71s/it]Running Inference:   4%|▍         | 8/200 [03:09<1:15:13, 23.51s/it]Running Inference:   4%|▍         | 9/200 [03:33<1:14:36, 23.44s/it]Running Inference:   5%|▌         | 10/200 [03:57<1:15:35, 23.87s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:21<1:15:05, 23.84s/it]Running Inference:   6%|▌         | 12/200 [04:44<1:13:59, 23.62s/it]Running Inference:   6%|▋         | 13/200 [05:09<1:14:13, 23.82s/it]Running Inference:   7%|▋         | 14/200 [05:32<1:12:59, 23.55s/it]Running Inference:   8%|▊         | 15/200 [05:55<1:12:14, 23.43s/it]Running Inference:   8%|▊         | 16/200 [06:19<1:12:51, 23.76s/it]Running Inference:   8%|▊         | 17/200 [06:42<1:11:57, 23.59s/it]Running Inference:   9%|▉         | 18/200 [07:08<1:13:05, 24.10s/it]Running Inference:  10%|▉         | 19/200 [07:31<1:12:19, 23.97s/it]Running Inference:  10%|█         | 20/200 [07:56<1:12:27, 24.16s/it]Running Inference:  10%|█         | 21/200 [08:20<1:11:58, 24.13s/it]Running Inference:  11%|█         | 22/200 [08:44<1:11:23, 24.06s/it]Running Inference:  12%|█▏        | 23/200 [09:07<1:10:22, 23.85s/it]Running Inference:  12%|█▏        | 24/200 [09:33<1:11:20, 24.32s/it]Running Inference:  12%|█▎        | 25/200 [09:56<1:10:15, 24.09s/it]Running Inference:  13%|█▎        | 26/200 [10:21<1:10:33, 24.33s/it]Running Inference:  14%|█▎        | 27/200 [10:45<1:09:40, 24.17s/it]Running Inference:  14%|█▍        | 28/200 [11:09<1:08:47, 24.00s/it]Running Inference:  14%|█▍        | 29/200 [11:32<1:08:20, 23.98s/it]Running Inference:  15%|█▌        | 30/200 [11:56<1:07:20, 23.77s/it]Running Inference:  16%|█▌        | 31/200 [12:19<1:06:36, 23.65s/it]Running Inference:  16%|█▌        | 32/200 [12:42<1:05:31, 23.40s/it]Running Inference:  16%|█▋        | 33/200 [13:05<1:04:46, 23.27s/it]Running Inference:  17%|█▋        | 34/200 [13:28<1:04:09, 23.19s/it]Running Inference:  18%|█▊        | 35/200 [13:52<1:04:24, 23.42s/it]Running Inference:  18%|█▊        | 36/200 [14:15<1:04:02, 23.43s/it]Running Inference:  18%|█▊        | 37/200 [14:39<1:03:34, 23.40s/it]Running Inference:  19%|█▉        | 38/200 [15:04<1:04:23, 23.85s/it]Running Inference:  20%|█▉        | 39/200 [15:27<1:04:03, 23.87s/it]Running Inference:  20%|██        | 40/200 [15:53<1:04:35, 24.22s/it]Running Inference:  20%|██        | 41/200 [16:19<1:06:20, 25.03s/it]Running Inference:  21%|██        | 42/200 [16:43<1:04:53, 24.64s/it]Running Inference:  22%|██▏       | 43/200 [17:07<1:03:57, 24.44s/it]Running Inference:  22%|██▏       | 44/200 [17:32<1:04:01, 24.62s/it]Running Inference:  22%|██▎       | 45/200 [17:56<1:02:49, 24.32s/it]Running Inference:  23%|██▎       | 46/200 [18:19<1:01:38, 24.01s/it]Running Inference:  24%|██▎       | 47/200 [18:44<1:01:51, 24.26s/it]Running Inference:  24%|██▍       | 48/200 [19:07<1:00:51, 24.02s/it]Running Inference:  24%|██▍       | 49/200 [19:31<59:54, 23.81s/it]  Running Inference:  25%|██▌       | 50/200 [19:54<59:06, 23.64s/it]Running Inference:  26%|██▌       | 51/200 [20:18<59:10, 23.83s/it]Running Inference:  26%|██▌       | 52/200 [20:42<59:01, 23.93s/it]Running Inference:  26%|██▋       | 53/200 [21:06<58:22, 23.83s/it]Running Inference:  27%|██▋       | 54/200 [21:30<57:46, 23.75s/it]Running Inference:  28%|██▊       | 55/200 [21:53<57:12, 23.67s/it]Running Inference:  28%|██▊       | 56/200 [22:16<56:39, 23.61s/it]Running Inference:  28%|██▊       | 57/200 [22:40<55:50, 23.43s/it]Running Inference:  29%|██▉       | 58/200 [23:04<55:54, 23.62s/it]Running Inference:  30%|██▉       | 59/200 [23:28<55:48, 23.75s/it]Running Inference:  30%|███       | 60/200 [23:52<55:53, 23.96s/it]Running Inference:  30%|███       | 61/200 [24:15<55:03, 23.77s/it]Running Inference:  31%|███       | 62/200 [24:39<54:18, 23.61s/it]Running Inference:  32%|███▏      | 63/200 [25:02<53:58, 23.64s/it]Running Inference:  32%|███▏      | 64/200 [25:26<53:49, 23.75s/it]Running Inference:  32%|███▎      | 65/200 [25:50<53:38, 23.84s/it]Running Inference:  33%|███▎      | 66/200 [26:14<52:50, 23.66s/it]Running Inference:  34%|███▎      | 67/200 [26:38<52:35, 23.73s/it]Running Inference:  34%|███▍      | 68/200 [27:01<52:13, 23.74s/it]Running Inference:  34%|███▍      | 69/200 [27:26<52:28, 24.03s/it]Running Inference:  35%|███▌      | 70/200 [27:49<51:28, 23.75s/it]Running Inference:  36%|███▌      | 71/200 [28:12<50:35, 23.53s/it]Running Inference:  36%|███▌      | 72/200 [28:22<41:36, 19.50s/it]Running Inference:  36%|███▋      | 73/200 [28:46<43:55, 20.75s/it]Running Inference:  37%|███▋      | 74/200 [29:09<45:06, 21.48s/it]Running Inference:  38%|███▊      | 75/200 [29:32<45:52, 22.02s/it]Running Inference:  38%|███▊      | 76/200 [29:55<46:07, 22.32s/it]Running Inference:  38%|███▊      | 77/200 [30:20<47:08, 22.99s/it]Running Inference:  39%|███▉      | 78/200 [30:43<46:43, 22.98s/it]Running Inference:  40%|███▉      | 79/200 [31:06<46:33, 23.09s/it]Running Inference:  40%|████      | 80/200 [31:29<46:11, 23.09s/it]Running Inference:  40%|████      | 81/200 [31:53<46:01, 23.20s/it]Running Inference:  41%|████      | 82/200 [32:16<45:40, 23.22s/it]Running Inference:  42%|████▏     | 83/200 [32:39<45:15, 23.21s/it]Running Inference:  42%|████▏     | 84/200 [33:03<45:23, 23.48s/it]Running Inference:  42%|████▎     | 85/200 [33:27<45:04, 23.52s/it]Running Inference:  43%|████▎     | 86/200 [33:51<44:57, 23.66s/it]Running Inference:  44%|████▎     | 87/200 [34:14<44:22, 23.56s/it]Running Inference:  44%|████▍     | 88/200 [34:38<43:49, 23.48s/it]Running Inference:  44%|████▍     | 89/200 [35:00<42:58, 23.23s/it]Running Inference:  45%|████▌     | 90/200 [35:23<42:34, 23.22s/it]Running Inference:  46%|████▌     | 91/200 [35:48<42:39, 23.48s/it]Running Inference:  46%|████▌     | 92/200 [36:10<41:50, 23.24s/it]Running Inference:  46%|████▋     | 93/200 [36:35<42:02, 23.57s/it]Running Inference:  47%|████▋     | 94/200 [36:59<42:07, 23.85s/it]Running Inference:  48%|████▊     | 95/200 [37:24<42:10, 24.10s/it]Running Inference:  48%|████▊     | 96/200 [37:47<41:12, 23.77s/it]Running Inference:  48%|████▊     | 97/200 [38:10<40:22, 23.52s/it]Running Inference:  49%|████▉     | 98/200 [38:33<40:04, 23.57s/it]Running Inference:  50%|████▉     | 99/200 [38:57<39:30, 23.47s/it]Running Inference:  50%|█████     | 100/200 [39:20<39:14, 23.55s/it]Running Inference:  50%|█████     | 101/200 [39:45<39:26, 23.90s/it]Running Inference:  51%|█████     | 102/200 [40:12<40:20, 24.70s/it]Running Inference:  52%|█████▏    | 103/200 [40:36<39:49, 24.64s/it]Running Inference:  52%|█████▏    | 104/200 [41:00<38:50, 24.27s/it]Running Inference:  52%|█████▎    | 105/200 [41:25<39:11, 24.75s/it]Running Inference:  53%|█████▎    | 106/200 [41:49<38:06, 24.32s/it]Running Inference:  54%|█████▎    | 107/200 [42:12<37:00, 23.88s/it]Running Inference:  54%|█████▍    | 108/200 [42:35<36:28, 23.79s/it]Running Inference:  55%|█████▍    | 109/200 [42:58<35:44, 23.57s/it]Running Inference:  55%|█████▌    | 110/200 [43:22<35:21, 23.57s/it]Running Inference:  56%|█████▌    | 111/200 [43:44<34:31, 23.28s/it]Running Inference:  56%|█████▌    | 112/200 [44:08<34:13, 23.34s/it]Running Inference:  56%|█████▋    | 113/200 [44:31<33:47, 23.31s/it]Running Inference:  57%|█████▋    | 114/200 [44:54<33:19, 23.25s/it]Running Inference:  57%|█████▊    | 115/200 [45:18<33:05, 23.36s/it]Running Inference:  58%|█████▊    | 116/200 [45:41<32:39, 23.33s/it]Running Inference:  58%|█████▊    | 117/200 [46:04<32:16, 23.33s/it]Running Inference:  59%|█████▉    | 118/200 [46:27<31:35, 23.12s/it]Running Inference:  60%|█████▉    | 119/200 [46:51<31:28, 23.32s/it]Running Inference:  60%|██████    | 120/200 [47:15<31:27, 23.59s/it]Running Inference:  60%|██████    | 121/200 [47:39<31:16, 23.76s/it]Running Inference:  61%|██████    | 122/200 [48:02<30:25, 23.40s/it]Running Inference:  62%|██████▏   | 123/200 [48:26<30:28, 23.75s/it]Running Inference:  62%|██████▏   | 124/200 [48:51<30:18, 23.92s/it]Running Inference:  62%|██████▎   | 125/200 [49:14<29:37, 23.69s/it]Running Inference:  63%|██████▎   | 126/200 [49:37<29:07, 23.62s/it]Running Inference:  64%|██████▎   | 127/200 [50:02<28:59, 23.83s/it]Running Inference:  64%|██████▍   | 128/200 [50:25<28:34, 23.81s/it]Running Inference:  64%|██████▍   | 129/200 [50:49<28:05, 23.74s/it]Running Inference:  65%|██████▌   | 130/200 [51:11<27:17, 23.40s/it]Running Inference:  66%|██████▌   | 131/200 [51:35<26:54, 23.40s/it]Running Inference:  66%|██████▌   | 132/200 [51:58<26:24, 23.30s/it]Running Inference:  66%|██████▋   | 133/200 [52:22<26:10, 23.44s/it]Running Inference:  67%|██████▋   | 134/200 [52:45<25:35, 23.26s/it]Running Inference:  68%|██████▊   | 135/200 [53:08<25:10, 23.24s/it]Running Inference:  68%|██████▊   | 136/200 [53:30<24:37, 23.08s/it]Running Inference:  68%|██████▊   | 137/200 [53:54<24:16, 23.11s/it]Running Inference:  69%|██████▉   | 138/200 [54:16<23:45, 22.99s/it]Running Inference:  70%|██████▉   | 139/200 [54:41<23:44, 23.35s/it]Running Inference:  70%|███████   | 140/200 [55:05<23:35, 23.59s/it]Running Inference:  70%|███████   | 141/200 [55:28<23:05, 23.49s/it]Running Inference:  71%|███████   | 142/200 [55:51<22:39, 23.45s/it]Running Inference:  72%|███████▏  | 143/200 [56:15<22:22, 23.56s/it]Running Inference:  72%|███████▏  | 144/200 [56:38<21:53, 23.46s/it]Running Inference:  72%|███████▎  | 145/200 [57:03<21:51, 23.84s/it]Running Inference:  73%|███████▎  | 146/200 [57:26<21:16, 23.65s/it]Running Inference:  74%|███████▎  | 147/200 [57:49<20:41, 23.43s/it]Running Inference:  74%|███████▍  | 148/200 [58:12<20:14, 23.36s/it]Running Inference:  74%|███████▍  | 149/200 [58:36<19:55, 23.44s/it]Running Inference:  75%|███████▌  | 150/200 [59:00<19:47, 23.74s/it]Running Inference:  76%|███████▌  | 151/200 [59:25<19:31, 23.90s/it]Running Inference:  76%|███████▌  | 152/200 [59:49<19:13, 24.04s/it]Running Inference:  76%|███████▋  | 153/200 [1:00:13<18:41, 23.86s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:35<18:01, 23.52s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:58<17:32, 23.38s/it]Running Inference:  78%|███████▊  | 156/200 [1:01:21<17:01, 23.21s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:44<16:36, 23.17s/it]Running Inference:  79%|███████▉  | 158/200 [1:02:08<16:16, 23.25s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:30<15:45, 23.06s/it]Running Inference:  80%|████████  | 160/200 [1:02:55<15:41, 23.53s/it]Running Inference:  80%|████████  | 161/200 [1:03:18<15:11, 23.38s/it]Running Inference:  81%|████████  | 162/200 [1:03:42<14:58, 23.65s/it]Running Inference:  82%|████████▏ | 163/200 [1:04:08<14:53, 24.15s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:31<14:20, 23.89s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:55<14:03, 24.09s/it]Running Inference:  83%|████████▎ | 166/200 [1:05:18<13:25, 23.69s/it]Running Inference:  84%|████████▎ | 167/200 [1:05:41<12:55, 23.51s/it]Running Inference:  84%|████████▍ | 168/200 [1:06:04<12:29, 23.43s/it]Running Inference:  84%|████████▍ | 169/200 [1:06:27<12:02, 23.30s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:51<11:38, 23.30s/it]Running Inference:  86%|████████▌ | 171/200 [1:07:13<11:10, 23.11s/it]Running Inference:  86%|████████▌ | 172/200 [1:07:37<10:51, 23.25s/it]Running Inference:  86%|████████▋ | 173/200 [1:08:02<10:38, 23.64s/it]Running Inference:  87%|████████▋ | 174/200 [1:08:25<10:10, 23.49s/it]Running Inference:  88%|████████▊ | 175/200 [1:08:48<09:45, 23.44s/it]Running Inference:  88%|████████▊ | 176/200 [1:09:11<09:17, 23.25s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:34<08:51, 23.12s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:56<08:26, 23.01s/it]Running Inference:  90%|████████▉ | 179/200 [1:10:20<08:08, 23.25s/it]Running Inference:  90%|█████████ | 180/200 [1:10:43<07:41, 23.09s/it]Running Inference:  90%|█████████ | 181/200 [1:11:06<07:17, 23.05s/it]Running Inference:  91%|█████████ | 182/200 [1:11:29<06:57, 23.21s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:52<06:33, 23.16s/it]Running Inference:  92%|█████████▏| 184/200 [1:12:15<06:09, 23.10s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:38<05:44, 22.96s/it]Running Inference:  93%|█████████▎| 186/200 [1:13:01<05:21, 23.00s/it]Running Inference:  94%|█████████▎| 187/200 [1:13:24<04:58, 22.96s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:48<04:37, 23.12s/it]Running Inference:  94%|█████████▍| 189/200 [1:14:10<04:12, 23.00s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:33<03:50, 23.01s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:56<03:26, 22.93s/it]Running Inference:  96%|█████████▌| 192/200 [1:15:19<03:03, 22.93s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:42<02:41, 23.01s/it]Running Inference:  97%|█████████▋| 194/200 [1:16:06<02:19, 23.18s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:31<01:58, 23.72s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:54<01:34, 23.64s/it]Running Inference:  98%|█████████▊| 197/200 [1:17:18<01:10, 23.62s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:41<00:46, 23.43s/it]Running Inference: 100%|█████████▉| 199/200 [1:18:04<00:23, 23.32s/it]Running Inference: 100%|██████████| 200/200 [1:18:27<00:00, 23.30s/it]Running Inference: 100%|██████████| 200/200 [1:18:27<00:00, 23.54s/it]
2025-12-14 16:12:05,961 - INFO - Inference completed.
2025-12-14 16:12:05,974 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 16:12:05,974 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.661 seconds.
Prefix dict has been built successfully.
2025-12-14 16:12:16,600 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 16:12:16,600 - INFO - Metrics:
4.36
2025-12-14 16:12:16,602 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: knorm (task=vcsum, ratio=0.3) on GPU 4

----------------------------------------
Task: vcsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:12:23,952 - INFO - Set deterministic seeds to 42
2025-12-14 16:12:23,952 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:12:23,952 - INFO - Starting evaluation run...
2025-12-14 16:12:23,952 - INFO - Output directory set to: longbenchresult
2025-12-14 16:12:23,952 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 16:12:23,952 - INFO - KV Press 'knorm' setup.
2025-12-14 16:12:23,952 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:12:23,952 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.29it/s]
Device set to use cuda:0
2025-12-14 16:12:38,537 - INFO - Model pipeline loaded.
2025-12-14 16:12:38,537 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 16:12:42,525 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:12:42,525 - INFO - Dataset processed with 200 entries.
2025-12-14 16:12:42,551 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:21:27, 24.56s/it]Running Inference:   1%|          | 2/200 [00:48<1:20:42, 24.46s/it]Running Inference:   2%|▏         | 3/200 [01:13<1:19:52, 24.33s/it]Running Inference:   2%|▏         | 4/200 [01:36<1:18:49, 24.13s/it]Running Inference:   2%|▎         | 5/200 [02:01<1:18:47, 24.25s/it]Running Inference:   3%|▎         | 6/200 [02:24<1:17:17, 23.91s/it]Running Inference:   4%|▎         | 7/200 [02:48<1:16:55, 23.92s/it]Running Inference:   4%|▍         | 8/200 [03:11<1:15:47, 23.69s/it]Running Inference:   4%|▍         | 9/200 [03:35<1:15:13, 23.63s/it]Running Inference:   5%|▌         | 10/200 [04:00<1:16:09, 24.05s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:24<1:15:37, 24.01s/it]Running Inference:   6%|▌         | 12/200 [04:47<1:14:28, 23.77s/it]Running Inference:   6%|▋         | 13/200 [05:11<1:14:41, 23.97s/it]Running Inference:   7%|▋         | 14/200 [05:34<1:13:29, 23.70s/it]Running Inference:   8%|▊         | 15/200 [05:58<1:12:42, 23.58s/it]Running Inference:   8%|▊         | 16/200 [06:22<1:13:19, 23.91s/it]Running Inference:   8%|▊         | 17/200 [06:46<1:12:20, 23.72s/it]Running Inference:   9%|▉         | 18/200 [07:11<1:13:30, 24.23s/it]Running Inference:  10%|▉         | 19/200 [07:35<1:12:45, 24.12s/it]Running Inference:  10%|█         | 20/200 [08:00<1:12:52, 24.29s/it]Running Inference:  10%|█         | 21/200 [08:24<1:12:23, 24.27s/it]Running Inference:  11%|█         | 22/200 [08:48<1:11:51, 24.22s/it]Running Inference:  12%|█▏        | 23/200 [09:12<1:10:58, 24.06s/it]Running Inference:  12%|█▏        | 24/200 [09:37<1:11:55, 24.52s/it]Running Inference:  12%|█▎        | 25/200 [10:01<1:10:49, 24.28s/it]Running Inference:  13%|█▎        | 26/200 [10:26<1:11:05, 24.52s/it]Running Inference:  14%|█▎        | 27/200 [10:50<1:10:16, 24.37s/it]Running Inference:  14%|█▍        | 28/200 [11:14<1:09:25, 24.22s/it]Running Inference:  14%|█▍        | 29/200 [11:38<1:08:54, 24.18s/it]Running Inference:  15%|█▌        | 30/200 [12:01<1:07:52, 23.95s/it]Running Inference:  16%|█▌        | 31/200 [12:25<1:07:06, 23.83s/it]Running Inference:  16%|█▌        | 32/200 [12:48<1:06:18, 23.68s/it]Running Inference:  16%|█▋        | 33/200 [13:12<1:05:37, 23.58s/it]Running Inference:  17%|█▋        | 34/200 [13:35<1:04:59, 23.49s/it]Running Inference:  18%|█▊        | 35/200 [13:59<1:05:08, 23.69s/it]Running Inference:  18%|█▊        | 36/200 [14:23<1:04:42, 23.67s/it]Running Inference:  18%|█▊        | 37/200 [14:46<1:04:12, 23.64s/it]Running Inference:  19%|█▉        | 38/200 [15:11<1:04:58, 24.06s/it]Running Inference:  20%|█▉        | 39/200 [15:35<1:04:36, 24.08s/it]Running Inference:  20%|██        | 40/200 [16:01<1:05:07, 24.42s/it]Running Inference:  20%|██        | 41/200 [16:27<1:06:21, 25.04s/it]Running Inference:  21%|██        | 42/200 [16:51<1:05:01, 24.69s/it]Running Inference:  22%|██▏       | 43/200 [17:15<1:04:11, 24.53s/it]Running Inference:  22%|██▏       | 44/200 [17:40<1:04:19, 24.74s/it]Running Inference:  22%|██▎       | 45/200 [18:04<1:03:09, 24.45s/it]Running Inference:  23%|██▎       | 46/200 [18:28<1:02:01, 24.16s/it]Running Inference:  24%|██▎       | 47/200 [18:53<1:02:14, 24.41s/it]Running Inference:  24%|██▍       | 48/200 [19:16<1:01:15, 24.18s/it]Running Inference:  24%|██▍       | 49/200 [19:40<1:00:19, 23.97s/it]Running Inference:  25%|██▌       | 50/200 [20:03<59:30, 23.81s/it]  Running Inference:  26%|██▌       | 51/200 [20:28<59:38, 24.01s/it]Running Inference:  26%|██▌       | 52/200 [20:52<59:33, 24.14s/it]Running Inference:  26%|██▋       | 53/200 [21:16<58:52, 24.03s/it]Running Inference:  27%|██▋       | 54/200 [21:40<58:13, 23.93s/it]Running Inference:  28%|██▊       | 55/200 [22:03<57:36, 23.84s/it]Running Inference:  28%|██▊       | 56/200 [22:27<57:02, 23.76s/it]Running Inference:  28%|██▊       | 57/200 [22:50<56:14, 23.60s/it]Running Inference:  29%|██▉       | 58/200 [23:14<56:16, 23.78s/it]Running Inference:  30%|██▉       | 59/200 [23:38<56:11, 23.91s/it]Running Inference:  30%|███       | 60/200 [24:03<56:17, 24.12s/it]Running Inference:  30%|███       | 61/200 [24:27<55:30, 23.96s/it]Running Inference:  31%|███       | 62/200 [24:50<54:43, 23.79s/it]Running Inference:  32%|███▏      | 63/200 [25:14<54:23, 23.82s/it]Running Inference:  32%|███▏      | 64/200 [25:38<54:12, 23.91s/it]Running Inference:  32%|███▎      | 65/200 [26:02<53:58, 23.99s/it]Running Inference:  33%|███▎      | 66/200 [26:26<53:11, 23.82s/it]Running Inference:  34%|███▎      | 67/200 [26:50<52:52, 23.85s/it]Running Inference:  34%|███▍      | 68/200 [27:14<52:30, 23.87s/it]Running Inference:  34%|███▍      | 69/200 [27:38<52:50, 24.21s/it]Running Inference:  35%|███▌      | 70/200 [28:02<52:01, 24.01s/it]Running Inference:  36%|███▌      | 71/200 [28:26<51:18, 23.86s/it]Running Inference:  36%|███▌      | 72/200 [28:50<51:17, 24.05s/it]Running Inference:  36%|███▋      | 73/200 [29:14<50:56, 24.06s/it]Running Inference:  37%|███▋      | 74/200 [29:38<50:19, 23.96s/it]Running Inference:  38%|███▊      | 75/200 [30:02<49:53, 23.95s/it]Running Inference:  38%|███▊      | 76/200 [30:25<49:16, 23.84s/it]Running Inference:  38%|███▊      | 77/200 [30:50<49:36, 24.20s/it]Running Inference:  39%|███▉      | 78/200 [31:14<48:43, 23.96s/it]Running Inference:  40%|███▉      | 79/200 [31:38<48:13, 23.91s/it]Running Inference:  40%|████      | 80/200 [32:01<47:36, 23.81s/it]Running Inference:  40%|████      | 81/200 [32:25<47:16, 23.84s/it]Running Inference:  41%|████      | 82/200 [32:49<46:49, 23.81s/it]Running Inference:  42%|████▏     | 83/200 [33:12<46:20, 23.76s/it]Running Inference:  42%|████▏     | 84/200 [33:37<46:22, 23.99s/it]Running Inference:  42%|████▎     | 85/200 [34:01<46:01, 24.01s/it]Running Inference:  43%|████▎     | 86/200 [34:26<45:52, 24.14s/it]Running Inference:  44%|████▎     | 87/200 [34:49<45:15, 24.03s/it]Running Inference:  44%|████▍     | 88/200 [35:13<44:44, 23.97s/it]Running Inference:  44%|████▍     | 89/200 [35:36<43:50, 23.70s/it]Running Inference:  45%|████▌     | 90/200 [36:00<43:26, 23.69s/it]Running Inference:  46%|████▌     | 91/200 [36:24<43:29, 23.94s/it]Running Inference:  46%|████▌     | 92/200 [36:48<42:41, 23.72s/it]Running Inference:  46%|████▋     | 93/200 [37:12<42:50, 24.02s/it]Running Inference:  47%|████▋     | 94/200 [37:37<42:57, 24.32s/it]Running Inference:  48%|████▊     | 95/200 [38:03<43:00, 24.58s/it]Running Inference:  48%|████▊     | 96/200 [38:26<42:02, 24.26s/it]Running Inference:  48%|████▊     | 97/200 [38:50<41:16, 24.04s/it]Running Inference:  49%|████▉     | 98/200 [39:14<40:58, 24.10s/it]Running Inference:  50%|████▉     | 99/200 [39:37<40:21, 23.98s/it]Running Inference:  50%|█████     | 100/200 [40:02<40:05, 24.05s/it]Running Inference:  50%|█████     | 101/200 [40:27<40:15, 24.40s/it]Running Inference:  51%|█████     | 102/200 [40:53<40:51, 25.02s/it]Running Inference:  52%|█████▏    | 103/200 [41:18<40:25, 25.01s/it]Running Inference:  52%|█████▏    | 104/200 [41:42<39:30, 24.69s/it]Running Inference:  52%|█████▎    | 105/200 [42:08<39:43, 25.09s/it]Running Inference:  53%|█████▎    | 106/200 [42:32<38:43, 24.71s/it]Running Inference:  54%|█████▎    | 107/200 [42:56<37:39, 24.30s/it]Running Inference:  54%|█████▍    | 108/200 [43:19<37:06, 24.20s/it]Running Inference:  55%|█████▍    | 109/200 [43:43<36:20, 23.97s/it]Running Inference:  55%|█████▌    | 110/200 [44:07<36:00, 24.01s/it]Running Inference:  56%|█████▌    | 111/200 [44:30<35:13, 23.75s/it]Running Inference:  56%|█████▌    | 112/200 [44:54<34:54, 23.80s/it]Running Inference:  56%|█████▋    | 113/200 [45:18<34:29, 23.78s/it]Running Inference:  57%|█████▋    | 114/200 [45:41<34:00, 23.73s/it]Running Inference:  57%|█████▊    | 115/200 [46:05<33:45, 23.83s/it]Running Inference:  58%|█████▊    | 116/200 [46:29<33:20, 23.81s/it]Running Inference:  58%|█████▊    | 117/200 [46:53<32:58, 23.83s/it]Running Inference:  59%|█████▉    | 118/200 [47:17<32:26, 23.73s/it]Running Inference:  60%|█████▉    | 119/200 [47:41<32:18, 23.94s/it]Running Inference:  60%|██████    | 120/200 [48:06<32:16, 24.20s/it]Running Inference:  60%|██████    | 121/200 [48:31<32:03, 24.35s/it]Running Inference:  61%|██████    | 122/200 [48:54<31:10, 23.99s/it]Running Inference:  62%|██████▏   | 123/200 [49:19<31:13, 24.33s/it]Running Inference:  62%|██████▏   | 124/200 [49:44<31:01, 24.49s/it]Running Inference:  62%|██████▎   | 125/200 [50:07<30:19, 24.26s/it]Running Inference:  63%|██████▎   | 126/200 [50:31<29:49, 24.18s/it]Running Inference:  64%|██████▎   | 127/200 [50:56<29:40, 24.38s/it]Running Inference:  64%|██████▍   | 128/200 [51:20<29:09, 24.30s/it]Running Inference:  64%|██████▍   | 129/200 [51:44<28:40, 24.23s/it]Running Inference:  65%|██████▌   | 130/200 [52:08<27:53, 23.91s/it]Running Inference:  66%|██████▌   | 131/200 [52:32<27:29, 23.91s/it]Running Inference:  66%|██████▌   | 132/200 [52:55<27:00, 23.83s/it]Running Inference:  66%|██████▋   | 133/200 [53:20<26:46, 23.98s/it]Running Inference:  67%|██████▋   | 134/200 [53:43<26:12, 23.83s/it]Running Inference:  68%|██████▊   | 135/200 [54:07<25:46, 23.80s/it]Running Inference:  68%|██████▊   | 136/200 [54:30<25:12, 23.63s/it]Running Inference:  68%|██████▊   | 137/200 [54:54<24:50, 23.66s/it]Running Inference:  69%|██████▉   | 138/200 [55:17<24:19, 23.54s/it]Running Inference:  70%|██████▉   | 139/200 [55:42<24:17, 23.89s/it]Running Inference:  70%|███████   | 140/200 [56:06<24:07, 24.12s/it]Running Inference:  70%|███████   | 141/200 [56:30<23:36, 24.01s/it]Running Inference:  71%|███████   | 142/200 [56:54<23:09, 23.97s/it]Running Inference:  72%|███████▏  | 143/200 [57:18<22:52, 24.09s/it]Running Inference:  72%|███████▏  | 144/200 [57:42<22:23, 23.99s/it]Running Inference:  72%|███████▎  | 145/200 [58:07<22:20, 24.38s/it]Running Inference:  73%|███████▎  | 146/200 [58:31<21:45, 24.18s/it]Running Inference:  74%|███████▎  | 147/200 [58:55<21:10, 23.97s/it]Running Inference:  74%|███████▍  | 148/200 [59:18<20:41, 23.87s/it]Running Inference:  74%|███████▍  | 149/200 [59:42<20:21, 23.95s/it]Running Inference:  75%|███████▌  | 150/200 [1:00:07<20:14, 24.30s/it]Running Inference:  76%|███████▌  | 151/200 [1:00:32<19:59, 24.49s/it]Running Inference:  76%|███████▌  | 152/200 [1:00:57<19:40, 24.60s/it]Running Inference:  76%|███████▋  | 153/200 [1:01:21<19:05, 24.37s/it]Running Inference:  77%|███████▋  | 154/200 [1:01:44<18:25, 24.04s/it]Running Inference:  78%|███████▊  | 155/200 [1:02:08<17:56, 23.93s/it]Running Inference:  78%|███████▊  | 156/200 [1:02:31<17:26, 23.77s/it]Running Inference:  78%|███████▊  | 157/200 [1:02:55<17:02, 23.77s/it]Running Inference:  79%|███████▉  | 158/200 [1:03:19<16:41, 23.86s/it]Running Inference:  80%|███████▉  | 159/200 [1:03:43<16:12, 23.71s/it]Running Inference:  80%|████████  | 160/200 [1:04:08<16:07, 24.18s/it]Running Inference:  80%|████████  | 161/200 [1:04:31<15:36, 24.02s/it]Running Inference:  81%|████████  | 162/200 [1:04:56<15:22, 24.29s/it]Running Inference:  82%|████████▏ | 163/200 [1:05:22<15:15, 24.74s/it]Running Inference:  82%|████████▏ | 164/200 [1:05:46<14:42, 24.53s/it]Running Inference:  82%|████████▎ | 165/200 [1:06:11<14:24, 24.70s/it]Running Inference:  83%|████████▎ | 166/200 [1:06:35<13:46, 24.30s/it]Running Inference:  84%|████████▎ | 167/200 [1:06:58<13:16, 24.13s/it]Running Inference:  84%|████████▍ | 168/200 [1:07:22<12:50, 24.08s/it]Running Inference:  84%|████████▍ | 169/200 [1:07:46<12:22, 23.94s/it]Running Inference:  85%|████████▌ | 170/200 [1:08:10<11:57, 23.91s/it]Running Inference:  86%|████████▌ | 171/200 [1:08:33<11:28, 23.72s/it]Running Inference:  86%|████████▌ | 172/200 [1:08:57<11:07, 23.83s/it]Running Inference:  86%|████████▋ | 173/200 [1:09:22<10:54, 24.23s/it]Running Inference:  87%|████████▋ | 174/200 [1:09:46<10:26, 24.08s/it]Running Inference:  88%|████████▊ | 175/200 [1:10:10<10:00, 24.04s/it]Running Inference:  88%|████████▊ | 176/200 [1:10:33<09:32, 23.85s/it]Running Inference:  88%|████████▊ | 177/200 [1:10:57<09:05, 23.70s/it]Running Inference:  89%|████████▉ | 178/200 [1:11:20<08:39, 23.62s/it]Running Inference:  90%|████████▉ | 179/200 [1:11:45<08:21, 23.86s/it]Running Inference:  90%|█████████ | 180/200 [1:12:08<07:53, 23.69s/it]Running Inference:  90%|█████████ | 181/200 [1:12:32<07:29, 23.66s/it]Running Inference:  91%|█████████ | 182/200 [1:12:56<07:08, 23.82s/it]Running Inference:  92%|█████████▏| 183/200 [1:13:19<06:43, 23.75s/it]Running Inference:  92%|█████████▏| 184/200 [1:13:43<06:19, 23.70s/it]Running Inference:  92%|█████████▎| 185/200 [1:14:06<05:53, 23.55s/it]Running Inference:  93%|█████████▎| 186/200 [1:14:30<05:30, 23.58s/it]Running Inference:  94%|█████████▎| 187/200 [1:14:53<05:05, 23.54s/it]Running Inference:  94%|█████████▍| 188/200 [1:15:17<04:44, 23.70s/it]Running Inference:  94%|█████████▍| 189/200 [1:15:41<04:19, 23.59s/it]Running Inference:  95%|█████████▌| 190/200 [1:16:04<03:56, 23.63s/it]Running Inference:  96%|█████████▌| 191/200 [1:16:28<03:31, 23.53s/it]Running Inference:  96%|█████████▌| 192/200 [1:16:51<03:08, 23.53s/it]Running Inference:  96%|█████████▋| 193/200 [1:17:15<02:45, 23.59s/it]Running Inference:  97%|█████████▋| 194/200 [1:17:39<02:22, 23.77s/it]Running Inference:  98%|█████████▊| 195/200 [1:18:05<02:01, 24.30s/it]Running Inference:  98%|█████████▊| 196/200 [1:18:29<01:36, 24.21s/it]Running Inference:  98%|█████████▊| 197/200 [1:18:53<01:12, 24.19s/it]Running Inference:  99%|█████████▉| 198/200 [1:19:16<00:48, 24.01s/it]Running Inference: 100%|█████████▉| 199/200 [1:19:40<00:23, 23.90s/it]Running Inference: 100%|██████████| 200/200 [1:20:04<00:00, 23.88s/it]Running Inference: 100%|██████████| 200/200 [1:20:04<00:00, 24.02s/it]
2025-12-14 17:32:46,883 - INFO - Inference completed.
2025-12-14 17:32:46,896 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 17:32:46,896 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.664 seconds.
Prefix dict has been built successfully.
2025-12-14 17:32:59,180 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 17:32:59,180 - INFO - Metrics:
1.73
2025-12-14 17:32:59,181 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: knorm (task=vcsum, ratio=0.5) on GPU 4


========================================
LongBench Task: passage_count
========================================
----------------------------------------
Task: passage_count | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:33:06,765 - INFO - Set deterministic seeds to 42
2025-12-14 17:33:06,765 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:33:06,765 - INFO - Starting evaluation run...
2025-12-14 17:33:06,766 - INFO - Output directory set to: longbenchresult
2025-12-14 17:33:06,766 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 17:33:06,766 - INFO - KV Press 'knorm' setup.
2025-12-14 17:33:06,766 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:33:06,766 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.62it/s]
Device set to use cuda:0
2025-12-14 17:33:19,943 - INFO - Model pipeline loaded.
2025-12-14 17:33:19,943 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 17:33:24,583 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:33:24,583 - INFO - Dataset processed with 200 entries.
2025-12-14 17:33:24,623 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:34,  1.98s/it]Running Inference:   1%|          | 2/200 [00:06<11:15,  3.41s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:28,  2.58s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:15,  1.91s/it]Running Inference:   2%|▎         | 5/200 [00:12<08:26,  2.60s/it]Running Inference:   3%|▎         | 6/200 [00:14<07:34,  2.34s/it]Running Inference:   4%|▎         | 7/200 [00:15<06:26,  2.00s/it]Running Inference:   4%|▍         | 8/200 [00:16<05:22,  1.68s/it]Running Inference:   4%|▍         | 9/200 [00:19<06:33,  2.06s/it]Running Inference:   5%|▌         | 10/200 [00:21<06:39,  2.10s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:25<07:45,  2.46s/it]Running Inference:   6%|▌         | 12/200 [00:29<09:44,  3.11s/it]Running Inference:   6%|▋         | 13/200 [00:33<10:12,  3.28s/it]Running Inference:   7%|▋         | 14/200 [00:34<08:01,  2.59s/it]Running Inference:   8%|▊         | 15/200 [00:37<08:38,  2.80s/it]Running Inference:   8%|▊         | 16/200 [00:39<07:48,  2.54s/it]Running Inference:   8%|▊         | 17/200 [00:41<06:47,  2.23s/it]Running Inference:   9%|▉         | 18/200 [00:42<06:19,  2.09s/it]Running Inference:  10%|▉         | 19/200 [00:44<05:59,  1.98s/it]Running Inference:  10%|█         | 20/200 [00:46<06:04,  2.03s/it]Running Inference:  10%|█         | 21/200 [00:49<07:00,  2.35s/it]Running Inference:  11%|█         | 22/200 [00:52<06:48,  2.30s/it]Running Inference:  12%|█▏        | 23/200 [00:54<07:05,  2.40s/it]Running Inference:  12%|█▏        | 24/200 [00:57<07:04,  2.41s/it]Running Inference:  12%|█▎        | 25/200 [01:01<08:48,  3.02s/it]Running Inference:  13%|█▎        | 26/200 [01:03<08:04,  2.79s/it]Running Inference:  14%|█▎        | 27/200 [01:05<07:13,  2.51s/it]Running Inference:  14%|█▍        | 28/200 [01:10<08:47,  3.07s/it]Running Inference:  14%|█▍        | 29/200 [01:13<08:54,  3.13s/it]Running Inference:  15%|█▌        | 30/200 [01:14<07:18,  2.58s/it]Running Inference:  16%|█▌        | 31/200 [01:16<06:19,  2.25s/it]Running Inference:  16%|█▌        | 32/200 [01:17<05:35,  2.00s/it]Running Inference:  16%|█▋        | 33/200 [01:18<04:40,  1.68s/it]Running Inference:  17%|█▋        | 34/200 [01:20<05:16,  1.91s/it]Running Inference:  18%|█▊        | 35/200 [01:23<05:33,  2.02s/it]Running Inference:  18%|█▊        | 36/200 [01:25<05:51,  2.15s/it]Running Inference:  18%|█▊        | 37/200 [01:27<05:37,  2.07s/it]Running Inference:  19%|█▉        | 38/200 [01:29<05:14,  1.94s/it]Running Inference:  20%|█▉        | 39/200 [01:31<05:42,  2.13s/it]Running Inference:  20%|██        | 40/200 [01:33<05:22,  2.02s/it]Running Inference:  20%|██        | 41/200 [01:35<05:19,  2.01s/it]Running Inference:  21%|██        | 42/200 [01:36<04:36,  1.75s/it]Running Inference:  22%|██▏       | 43/200 [01:38<04:55,  1.88s/it]Running Inference:  22%|██▏       | 44/200 [01:40<05:00,  1.92s/it]Running Inference:  22%|██▎       | 45/200 [01:43<05:41,  2.21s/it]Running Inference:  23%|██▎       | 46/200 [01:46<05:46,  2.25s/it]Running Inference:  24%|██▎       | 47/200 [01:47<04:55,  1.93s/it]Running Inference:  24%|██▍       | 48/200 [01:48<04:06,  1.62s/it]Running Inference:  24%|██▍       | 49/200 [01:49<04:02,  1.61s/it]Running Inference:  25%|██▌       | 50/200 [01:50<03:40,  1.47s/it]Running Inference:  26%|██▌       | 51/200 [01:53<04:19,  1.74s/it]Running Inference:  26%|██▌       | 52/200 [01:54<04:02,  1.64s/it]Running Inference:  26%|██▋       | 53/200 [01:57<04:51,  1.99s/it]Running Inference:  27%|██▋       | 54/200 [01:59<05:03,  2.08s/it]Running Inference:  28%|██▊       | 55/200 [02:00<04:18,  1.78s/it]Running Inference:  28%|██▊       | 56/200 [02:02<04:21,  1.81s/it]Running Inference:  28%|██▊       | 57/200 [02:04<04:31,  1.90s/it]Running Inference:  29%|██▉       | 58/200 [02:06<04:40,  1.97s/it]Running Inference:  30%|██▉       | 59/200 [02:10<05:42,  2.43s/it]Running Inference:  30%|███       | 60/200 [02:13<05:59,  2.57s/it]Running Inference:  30%|███       | 61/200 [02:14<05:12,  2.25s/it]Running Inference:  31%|███       | 62/200 [02:17<05:48,  2.52s/it]Running Inference:  32%|███▏      | 63/200 [02:21<06:22,  2.79s/it]Running Inference:  32%|███▏      | 64/200 [02:24<06:14,  2.76s/it]Running Inference:  32%|███▎      | 65/200 [02:26<05:57,  2.65s/it]Running Inference:  33%|███▎      | 66/200 [02:28<05:45,  2.58s/it]Running Inference:  34%|███▎      | 67/200 [02:30<05:08,  2.32s/it]Running Inference:  34%|███▍      | 68/200 [02:32<04:57,  2.25s/it]Running Inference:  34%|███▍      | 69/200 [02:36<05:37,  2.57s/it]Running Inference:  35%|███▌      | 70/200 [02:38<05:14,  2.42s/it]Running Inference:  36%|███▌      | 71/200 [02:40<05:20,  2.49s/it]Running Inference:  36%|███▌      | 72/200 [02:42<05:00,  2.34s/it]Running Inference:  36%|███▋      | 73/200 [02:45<05:28,  2.59s/it]Running Inference:  37%|███▋      | 74/200 [02:48<05:20,  2.54s/it]Running Inference:  38%|███▊      | 75/200 [02:50<04:45,  2.28s/it]Running Inference:  38%|███▊      | 76/200 [02:51<04:05,  1.98s/it]Running Inference:  38%|███▊      | 77/200 [02:57<06:53,  3.36s/it]Running Inference:  39%|███▉      | 78/200 [02:59<06:03,  2.98s/it]Running Inference:  40%|███▉      | 79/200 [03:02<05:37,  2.79s/it]Running Inference:  40%|████      | 80/200 [03:06<06:37,  3.31s/it]Running Inference:  40%|████      | 81/200 [03:08<05:23,  2.72s/it]Running Inference:  41%|████      | 82/200 [03:09<04:27,  2.27s/it]Running Inference:  42%|████▏     | 83/200 [03:11<04:29,  2.30s/it]Running Inference:  42%|████▏     | 84/200 [03:13<04:15,  2.20s/it]Running Inference:  42%|████▎     | 85/200 [03:16<04:37,  2.41s/it]Running Inference:  43%|████▎     | 86/200 [03:18<04:31,  2.38s/it]Running Inference:  44%|████▎     | 87/200 [03:20<04:14,  2.25s/it]Running Inference:  44%|████▍     | 88/200 [03:23<04:11,  2.24s/it]Running Inference:  44%|████▍     | 89/200 [03:25<04:29,  2.42s/it]Running Inference:  45%|████▌     | 90/200 [03:28<04:38,  2.53s/it]Running Inference:  46%|████▌     | 91/200 [03:29<03:51,  2.12s/it]Running Inference:  46%|████▌     | 92/200 [03:30<03:12,  1.78s/it]Running Inference:  46%|████▋     | 93/200 [03:32<03:04,  1.73s/it]Running Inference:  47%|████▋     | 94/200 [03:34<03:08,  1.78s/it]Running Inference:  48%|████▊     | 95/200 [03:36<03:20,  1.91s/it]Running Inference:  48%|████▊     | 96/200 [03:38<03:18,  1.91s/it]Running Inference:  48%|████▊     | 97/200 [03:40<03:33,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [03:44<04:18,  2.54s/it]Running Inference:  50%|████▉     | 99/200 [03:47<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [03:49<03:56,  2.37s/it]Running Inference:  50%|█████     | 101/200 [03:50<03:23,  2.06s/it]Running Inference:  51%|█████     | 102/200 [03:52<03:12,  1.97s/it]Running Inference:  52%|█████▏    | 103/200 [03:53<02:45,  1.71s/it]Running Inference:  52%|█████▏    | 104/200 [03:55<03:05,  1.94s/it]Running Inference:  52%|█████▎    | 105/200 [03:56<02:30,  1.59s/it]Running Inference:  53%|█████▎    | 106/200 [03:59<02:55,  1.87s/it]Running Inference:  54%|█████▎    | 107/200 [04:02<03:29,  2.25s/it]Running Inference:  54%|█████▍    | 108/200 [04:04<03:22,  2.21s/it]Running Inference:  55%|█████▍    | 109/200 [04:07<03:39,  2.42s/it]Running Inference:  55%|█████▌    | 110/200 [04:08<03:11,  2.13s/it]Running Inference:  56%|█████▌    | 111/200 [04:09<02:44,  1.85s/it]Running Inference:  56%|█████▌    | 112/200 [04:11<02:21,  1.61s/it]Running Inference:  56%|█████▋    | 113/200 [04:12<02:19,  1.60s/it]Running Inference:  57%|█████▋    | 114/200 [04:14<02:12,  1.55s/it]Running Inference:  57%|█████▊    | 115/200 [04:16<02:38,  1.86s/it]Running Inference:  58%|█████▊    | 116/200 [04:19<02:53,  2.06s/it]Running Inference:  58%|█████▊    | 117/200 [04:20<02:37,  1.90s/it]Running Inference:  59%|█████▉    | 118/200 [04:21<02:11,  1.60s/it]Running Inference:  60%|█████▉    | 119/200 [04:23<02:15,  1.67s/it]Running Inference:  60%|██████    | 120/200 [04:27<03:21,  2.52s/it]Running Inference:  60%|██████    | 121/200 [04:31<03:39,  2.77s/it]Running Inference:  61%|██████    | 122/200 [04:32<02:58,  2.29s/it]Running Inference:  62%|██████▏   | 123/200 [04:33<02:25,  1.89s/it]Running Inference:  62%|██████▏   | 124/200 [04:36<02:40,  2.11s/it]Running Inference:  62%|██████▎   | 125/200 [04:37<02:31,  2.03s/it]Running Inference:  63%|██████▎   | 126/200 [04:38<02:05,  1.69s/it]Running Inference:  64%|██████▎   | 127/200 [04:41<02:19,  1.91s/it]Running Inference:  64%|██████▍   | 128/200 [04:43<02:22,  1.99s/it]Running Inference:  64%|██████▍   | 129/200 [04:46<02:39,  2.25s/it]Running Inference:  65%|██████▌   | 130/200 [04:47<02:13,  1.91s/it]Running Inference:  66%|██████▌   | 131/200 [04:52<03:29,  3.03s/it]Running Inference:  66%|██████▌   | 132/200 [04:55<03:24,  3.00s/it]Running Inference:  66%|██████▋   | 133/200 [04:57<02:46,  2.48s/it]Running Inference:  67%|██████▋   | 134/200 [04:59<02:50,  2.58s/it]Running Inference:  68%|██████▊   | 135/200 [05:01<02:30,  2.32s/it]Running Inference:  68%|██████▊   | 136/200 [05:03<02:18,  2.16s/it]Running Inference:  68%|██████▊   | 137/200 [05:05<02:19,  2.22s/it]Running Inference:  69%|██████▉   | 138/200 [05:07<02:12,  2.14s/it]Running Inference:  70%|██████▉   | 139/200 [05:11<02:31,  2.48s/it]Running Inference:  70%|███████   | 140/200 [05:13<02:33,  2.56s/it]Running Inference:  70%|███████   | 141/200 [05:15<02:16,  2.31s/it]Running Inference:  71%|███████   | 142/200 [05:18<02:17,  2.37s/it]Running Inference:  72%|███████▏  | 143/200 [05:21<02:31,  2.67s/it]Running Inference:  72%|███████▏  | 144/200 [05:23<02:17,  2.46s/it]Running Inference:  72%|███████▎  | 145/200 [05:25<02:13,  2.43s/it]Running Inference:  73%|███████▎  | 146/200 [05:28<02:09,  2.39s/it]Running Inference:  74%|███████▎  | 147/200 [05:30<02:01,  2.28s/it]Running Inference:  74%|███████▍  | 148/200 [05:32<01:59,  2.30s/it]Running Inference:  74%|███████▍  | 149/200 [05:33<01:40,  1.97s/it]Running Inference:  75%|███████▌  | 150/200 [05:35<01:31,  1.82s/it]Running Inference:  76%|███████▌  | 151/200 [05:36<01:16,  1.57s/it]Running Inference:  76%|███████▌  | 152/200 [05:38<01:26,  1.80s/it]Running Inference:  76%|███████▋  | 153/200 [05:39<01:09,  1.47s/it]Running Inference:  77%|███████▋  | 154/200 [05:41<01:24,  1.83s/it]Running Inference:  78%|███████▊  | 155/200 [05:43<01:19,  1.76s/it]Running Inference:  78%|███████▊  | 156/200 [05:45<01:19,  1.82s/it]Running Inference:  78%|███████▊  | 157/200 [05:46<01:09,  1.63s/it]Running Inference:  79%|███████▉  | 158/200 [05:48<01:14,  1.77s/it]Running Inference:  80%|███████▉  | 159/200 [05:50<01:14,  1.81s/it]Running Inference:  80%|████████  | 160/200 [05:53<01:31,  2.29s/it]Running Inference:  80%|████████  | 161/200 [05:56<01:32,  2.37s/it]Running Inference:  81%|████████  | 162/200 [05:58<01:27,  2.29s/it]Running Inference:  82%|████████▏ | 163/200 [06:03<01:50,  2.99s/it]Running Inference:  82%|████████▏ | 164/200 [06:04<01:33,  2.60s/it]Running Inference:  82%|████████▎ | 165/200 [06:07<01:28,  2.52s/it]Running Inference:  83%|████████▎ | 166/200 [06:10<01:32,  2.73s/it]Running Inference:  84%|████████▎ | 167/200 [06:12<01:19,  2.42s/it]Running Inference:  84%|████████▍ | 168/200 [06:14<01:21,  2.53s/it]Running Inference:  84%|████████▍ | 169/200 [06:15<01:02,  2.01s/it]Running Inference:  85%|████████▌ | 170/200 [06:18<01:03,  2.11s/it]Running Inference:  86%|████████▌ | 171/200 [06:20<01:02,  2.17s/it]Running Inference:  86%|████████▌ | 172/200 [06:21<00:52,  1.87s/it]Running Inference:  86%|████████▋ | 173/200 [06:23<00:53,  1.97s/it]Running Inference:  87%|████████▋ | 174/200 [06:25<00:49,  1.89s/it]Running Inference:  88%|████████▊ | 175/200 [06:29<01:06,  2.65s/it]Running Inference:  88%|████████▊ | 176/200 [06:31<00:57,  2.39s/it]Running Inference:  88%|████████▊ | 177/200 [06:33<00:49,  2.15s/it]Running Inference:  89%|████████▉ | 178/200 [06:35<00:49,  2.25s/it]Running Inference:  90%|████████▉ | 179/200 [06:38<00:49,  2.35s/it]Running Inference:  90%|█████████ | 180/200 [06:40<00:46,  2.31s/it]Running Inference:  90%|█████████ | 181/200 [06:42<00:39,  2.06s/it]Running Inference:  91%|█████████ | 182/200 [06:44<00:37,  2.08s/it]Running Inference:  92%|█████████▏| 183/200 [06:45<00:30,  1.82s/it]Running Inference:  92%|█████████▏| 184/200 [06:48<00:36,  2.30s/it]Running Inference:  92%|█████████▎| 185/200 [06:50<00:32,  2.17s/it]Running Inference:  93%|█████████▎| 186/200 [06:55<00:42,  3.06s/it]Running Inference:  94%|█████████▎| 187/200 [06:58<00:38,  3.00s/it]Running Inference:  94%|█████████▍| 188/200 [07:01<00:34,  2.87s/it]Running Inference:  94%|█████████▍| 189/200 [07:02<00:25,  2.33s/it]Running Inference:  95%|█████████▌| 190/200 [07:05<00:25,  2.53s/it]Running Inference:  96%|█████████▌| 191/200 [07:08<00:24,  2.68s/it]Running Inference:  96%|█████████▌| 192/200 [07:09<00:17,  2.21s/it]Running Inference:  96%|█████████▋| 193/200 [07:10<00:13,  1.94s/it]Running Inference:  97%|█████████▋| 194/200 [07:13<00:12,  2.16s/it]Running Inference:  98%|█████████▊| 195/200 [07:15<00:10,  2.20s/it]Running Inference:  98%|█████████▊| 196/200 [07:17<00:07,  1.96s/it]Running Inference:  98%|█████████▊| 197/200 [07:18<00:05,  1.79s/it]Running Inference:  99%|█████████▉| 198/200 [07:19<00:03,  1.63s/it]Running Inference: 100%|█████████▉| 199/200 [07:21<00:01,  1.58s/it]Running Inference: 100%|██████████| 200/200 [07:22<00:00,  1.62s/it]Running Inference: 100%|██████████| 200/200 [07:22<00:00,  2.21s/it]
2025-12-14 17:40:47,581 - INFO - Inference completed.
2025-12-14 17:40:47,590 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 17:40:47,590 - INFO - Calculating metrics for dataset: longbench
2025-12-14 17:40:47,591 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 17:40:47,591 - INFO - Metrics:
7.7
2025-12-14 17:40:47,592 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_count, ratio=0.1) on GPU 4

----------------------------------------
Task: passage_count | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:40:54,152 - INFO - Set deterministic seeds to 42
2025-12-14 17:40:54,152 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:40:54,152 - INFO - Starting evaluation run...
2025-12-14 17:40:54,152 - INFO - Output directory set to: longbenchresult
2025-12-14 17:40:54,152 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 17:40:54,152 - INFO - KV Press 'knorm' setup.
2025-12-14 17:40:54,152 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:40:54,152 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.34it/s]
Device set to use cuda:0
2025-12-14 17:41:06,923 - INFO - Model pipeline loaded.
2025-12-14 17:41:06,923 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 17:41:15,016 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:41:15,016 - INFO - Dataset processed with 200 entries.
2025-12-14 17:41:15,055 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:14,  1.88s/it]Running Inference:   1%|          | 2/200 [00:04<07:06,  2.16s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:14,  1.90s/it]Running Inference:   2%|▏         | 4/200 [00:06<04:58,  1.52s/it]Running Inference:   2%|▎         | 5/200 [00:11<08:23,  2.58s/it]Running Inference:   3%|▎         | 6/200 [00:13<07:32,  2.33s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:25,  2.00s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:19,  1.66s/it]Running Inference:   4%|▍         | 9/200 [00:18<06:31,  2.05s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:37,  2.09s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:45,  2.46s/it]Running Inference:   6%|▌         | 12/200 [00:26<08:20,  2.66s/it]Running Inference:   6%|▋         | 13/200 [00:29<07:52,  2.53s/it]Running Inference:   7%|▋         | 14/200 [00:30<06:26,  2.08s/it]Running Inference:   8%|▊         | 15/200 [00:33<07:32,  2.44s/it]Running Inference:   8%|▊         | 16/200 [00:35<07:02,  2.30s/it]Running Inference:   8%|▊         | 17/200 [00:36<06:15,  2.05s/it]Running Inference:   9%|▉         | 18/200 [00:38<05:58,  1.97s/it]Running Inference:  10%|▉         | 19/200 [00:40<05:44,  1.90s/it]Running Inference:  10%|█         | 20/200 [00:42<05:54,  1.97s/it]Running Inference:  10%|█         | 21/200 [00:45<06:53,  2.31s/it]Running Inference:  11%|█         | 22/200 [00:47<06:46,  2.28s/it]Running Inference:  12%|█▏        | 23/200 [00:50<07:03,  2.39s/it]Running Inference:  12%|█▏        | 24/200 [00:52<07:03,  2.41s/it]Running Inference:  12%|█▎        | 25/200 [00:55<06:59,  2.40s/it]Running Inference:  13%|█▎        | 26/200 [00:57<06:49,  2.35s/it]Running Inference:  14%|█▎        | 27/200 [00:59<06:20,  2.20s/it]Running Inference:  14%|█▍        | 28/200 [01:03<08:16,  2.89s/it]Running Inference:  14%|█▍        | 29/200 [01:07<08:32,  3.00s/it]Running Inference:  15%|█▌        | 30/200 [01:08<07:03,  2.49s/it]Running Inference:  16%|█▌        | 31/200 [01:09<06:09,  2.18s/it]Running Inference:  16%|█▌        | 32/200 [01:11<05:27,  1.95s/it]Running Inference:  16%|█▋        | 33/200 [01:12<04:35,  1.65s/it]Running Inference:  17%|█▋        | 34/200 [01:14<05:13,  1.89s/it]Running Inference:  18%|█▊        | 35/200 [01:16<05:31,  2.01s/it]Running Inference:  18%|█▊        | 36/200 [01:19<05:50,  2.13s/it]Running Inference:  18%|█▊        | 37/200 [01:21<05:35,  2.06s/it]Running Inference:  19%|█▉        | 38/200 [01:22<05:13,  1.94s/it]Running Inference:  20%|█▉        | 39/200 [01:25<05:42,  2.12s/it]Running Inference:  20%|██        | 40/200 [01:27<05:19,  2.00s/it]Running Inference:  20%|██        | 41/200 [01:29<05:17,  2.00s/it]Running Inference:  21%|██        | 42/200 [01:30<04:38,  1.76s/it]Running Inference:  22%|██▏       | 43/200 [01:32<04:56,  1.89s/it]Running Inference:  22%|██▏       | 44/200 [01:34<05:01,  1.93s/it]Running Inference:  22%|██▎       | 45/200 [01:37<05:42,  2.21s/it]Running Inference:  23%|██▎       | 46/200 [01:39<05:46,  2.25s/it]Running Inference:  24%|██▎       | 47/200 [01:41<04:55,  1.93s/it]Running Inference:  24%|██▍       | 48/200 [01:41<04:06,  1.62s/it]Running Inference:  24%|██▍       | 49/200 [01:43<04:02,  1.61s/it]Running Inference:  25%|██▌       | 50/200 [01:44<03:40,  1.47s/it]Running Inference:  26%|██▌       | 51/200 [01:47<04:18,  1.74s/it]Running Inference:  26%|██▌       | 52/200 [01:48<04:02,  1.64s/it]Running Inference:  26%|██▋       | 53/200 [01:51<04:51,  1.99s/it]Running Inference:  27%|██▋       | 54/200 [01:53<05:03,  2.08s/it]Running Inference:  28%|██▊       | 55/200 [01:54<04:18,  1.78s/it]Running Inference:  28%|██▊       | 56/200 [01:56<04:21,  1.82s/it]Running Inference:  28%|██▊       | 57/200 [01:58<04:32,  1.90s/it]Running Inference:  29%|██▉       | 58/200 [02:00<04:40,  1.98s/it]Running Inference:  30%|██▉       | 59/200 [02:04<05:42,  2.43s/it]Running Inference:  30%|███       | 60/200 [02:07<05:58,  2.56s/it]Running Inference:  30%|███       | 61/200 [02:08<05:11,  2.24s/it]Running Inference:  31%|███       | 62/200 [02:11<05:46,  2.51s/it]Running Inference:  32%|███▏      | 63/200 [02:15<06:44,  2.95s/it]Running Inference:  32%|███▏      | 64/200 [02:18<06:30,  2.87s/it]Running Inference:  32%|███▎      | 65/200 [02:20<06:07,  2.72s/it]Running Inference:  33%|███▎      | 66/200 [02:23<05:52,  2.63s/it]Running Inference:  34%|███▎      | 67/200 [02:24<05:11,  2.34s/it]Running Inference:  34%|███▍      | 68/200 [02:26<05:00,  2.28s/it]Running Inference:  34%|███▍      | 69/200 [02:30<05:39,  2.59s/it]Running Inference:  35%|███▌      | 70/200 [02:32<05:16,  2.43s/it]Running Inference:  36%|███▌      | 71/200 [02:34<05:21,  2.49s/it]Running Inference:  36%|███▌      | 72/200 [02:37<05:00,  2.35s/it]Running Inference:  36%|███▋      | 73/200 [02:40<05:28,  2.59s/it]Running Inference:  37%|███▋      | 74/200 [02:42<05:19,  2.54s/it]Running Inference:  38%|███▊      | 75/200 [02:44<04:44,  2.28s/it]Running Inference:  38%|███▊      | 76/200 [02:45<04:06,  1.99s/it]Running Inference:  38%|███▊      | 77/200 [02:52<06:52,  3.35s/it]Running Inference:  39%|███▉      | 78/200 [02:54<06:02,  2.97s/it]Running Inference:  40%|███▉      | 79/200 [02:56<05:36,  2.78s/it]Running Inference:  40%|████      | 80/200 [02:58<05:21,  2.68s/it]Running Inference:  40%|████      | 81/200 [03:00<04:31,  2.28s/it]Running Inference:  41%|████      | 82/200 [03:01<03:51,  1.96s/it]Running Inference:  42%|████▏     | 83/200 [03:03<04:03,  2.08s/it]Running Inference:  42%|████▏     | 84/200 [03:05<03:57,  2.05s/it]Running Inference:  42%|████▎     | 85/200 [03:08<04:24,  2.30s/it]Running Inference:  43%|████▎     | 86/200 [03:11<04:22,  2.30s/it]Running Inference:  44%|████▎     | 87/200 [03:12<04:07,  2.19s/it]Running Inference:  44%|████▍     | 88/200 [03:15<04:06,  2.20s/it]Running Inference:  44%|████▍     | 89/200 [03:18<04:25,  2.39s/it]Running Inference:  45%|████▌     | 90/200 [03:20<04:35,  2.51s/it]Running Inference:  46%|████▌     | 91/200 [03:21<03:49,  2.11s/it]Running Inference:  46%|████▌     | 92/200 [03:22<03:11,  1.77s/it]Running Inference:  46%|████▋     | 93/200 [03:24<03:04,  1.72s/it]Running Inference:  47%|████▋     | 94/200 [03:26<03:07,  1.77s/it]Running Inference:  48%|████▊     | 95/200 [03:28<03:19,  1.90s/it]Running Inference:  48%|████▊     | 96/200 [03:30<03:19,  1.92s/it]Running Inference:  48%|████▊     | 97/200 [03:33<03:32,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [03:36<04:18,  2.53s/it]Running Inference:  50%|████▉     | 99/200 [03:39<04:28,  2.65s/it]Running Inference:  50%|█████     | 100/200 [03:41<03:56,  2.37s/it]Running Inference:  50%|█████     | 101/200 [03:42<03:23,  2.06s/it]Running Inference:  51%|█████     | 102/200 [03:44<03:12,  1.97s/it]Running Inference:  52%|█████▏    | 103/200 [03:45<02:45,  1.71s/it]Running Inference:  52%|█████▏    | 104/200 [03:47<03:03,  1.92s/it]Running Inference:  52%|█████▎    | 105/200 [03:48<02:29,  1.57s/it]Running Inference:  53%|█████▎    | 106/200 [03:51<02:54,  1.86s/it]Running Inference:  54%|█████▎    | 107/200 [03:54<03:28,  2.24s/it]Running Inference:  54%|█████▍    | 108/200 [03:56<03:22,  2.20s/it]Running Inference:  55%|█████▍    | 109/200 [03:59<03:39,  2.41s/it]Running Inference:  55%|█████▌    | 110/200 [04:00<03:12,  2.13s/it]Running Inference:  56%|█████▌    | 111/200 [04:02<02:45,  1.86s/it]Running Inference:  56%|█████▌    | 112/200 [04:03<02:21,  1.61s/it]Running Inference:  56%|█████▋    | 113/200 [04:04<02:19,  1.60s/it]Running Inference:  57%|█████▋    | 114/200 [04:06<02:13,  1.55s/it]Running Inference:  57%|█████▊    | 115/200 [04:08<02:37,  1.85s/it]Running Inference:  58%|█████▊    | 116/200 [04:11<02:52,  2.05s/it]Running Inference:  58%|█████▊    | 117/200 [04:12<02:36,  1.89s/it]Running Inference:  59%|█████▉    | 118/200 [04:13<02:10,  1.59s/it]Running Inference:  60%|█████▉    | 119/200 [04:15<02:14,  1.67s/it]Running Inference:  60%|██████    | 120/200 [04:19<03:20,  2.51s/it]Running Inference:  60%|██████    | 121/200 [04:23<03:38,  2.76s/it]Running Inference:  61%|██████    | 122/200 [04:24<02:57,  2.28s/it]Running Inference:  62%|██████▏   | 123/200 [04:25<02:25,  1.89s/it]Running Inference:  62%|██████▏   | 124/200 [04:27<02:39,  2.10s/it]Running Inference:  62%|██████▎   | 125/200 [04:29<02:31,  2.02s/it]Running Inference:  63%|██████▎   | 126/200 [04:30<02:04,  1.68s/it]Running Inference:  64%|██████▎   | 127/200 [04:33<02:19,  1.91s/it]Running Inference:  64%|██████▍   | 128/200 [04:35<02:22,  1.98s/it]Running Inference:  64%|██████▍   | 129/200 [04:38<02:39,  2.24s/it]Running Inference:  65%|██████▌   | 130/200 [04:39<02:13,  1.90s/it]Running Inference:  66%|██████▌   | 131/200 [04:42<02:44,  2.38s/it]Running Inference:  66%|██████▌   | 132/200 [04:45<02:52,  2.53s/it]Running Inference:  66%|██████▋   | 133/200 [04:46<02:24,  2.15s/it]Running Inference:  67%|██████▋   | 134/200 [04:49<02:34,  2.35s/it]Running Inference:  68%|██████▊   | 135/200 [04:51<02:20,  2.16s/it]Running Inference:  68%|██████▊   | 136/200 [04:53<02:11,  2.05s/it]Running Inference:  68%|██████▊   | 137/200 [04:55<02:14,  2.14s/it]Running Inference:  69%|██████▉   | 138/200 [04:57<02:08,  2.08s/it]Running Inference:  70%|██████▉   | 139/200 [05:00<02:28,  2.43s/it]Running Inference:  70%|███████   | 140/200 [05:03<02:31,  2.53s/it]Running Inference:  70%|███████   | 141/200 [05:05<02:14,  2.29s/it]Running Inference:  71%|███████   | 142/200 [05:07<02:16,  2.35s/it]Running Inference:  72%|███████▏  | 143/200 [05:09<02:06,  2.22s/it]Running Inference:  72%|███████▏  | 144/200 [05:11<02:00,  2.15s/it]Running Inference:  72%|███████▎  | 145/200 [05:13<02:01,  2.21s/it]Running Inference:  73%|███████▎  | 146/200 [05:16<02:00,  2.24s/it]Running Inference:  74%|███████▎  | 147/200 [05:18<01:55,  2.17s/it]Running Inference:  74%|███████▍  | 148/200 [05:20<01:55,  2.23s/it]Running Inference:  74%|███████▍  | 149/200 [05:21<01:37,  1.91s/it]Running Inference:  75%|███████▌  | 150/200 [05:23<01:28,  1.78s/it]Running Inference:  76%|███████▌  | 151/200 [05:24<01:15,  1.54s/it]Running Inference:  76%|███████▌  | 152/200 [05:26<01:25,  1.78s/it]Running Inference:  76%|███████▋  | 153/200 [05:27<01:08,  1.46s/it]Running Inference:  77%|███████▋  | 154/200 [05:29<01:24,  1.83s/it]Running Inference:  78%|███████▊  | 155/200 [05:31<01:19,  1.77s/it]Running Inference:  78%|███████▊  | 156/200 [05:33<01:20,  1.82s/it]Running Inference:  78%|███████▊  | 157/200 [05:34<01:09,  1.62s/it]Running Inference:  79%|███████▉  | 158/200 [05:36<01:13,  1.76s/it]Running Inference:  80%|███████▉  | 159/200 [05:38<01:13,  1.80s/it]Running Inference:  80%|████████  | 160/200 [05:42<01:31,  2.29s/it]Running Inference:  80%|████████  | 161/200 [05:44<01:32,  2.37s/it]Running Inference:  81%|████████  | 162/200 [05:46<01:27,  2.29s/it]Running Inference:  82%|████████▏ | 163/200 [05:49<01:34,  2.56s/it]Running Inference:  82%|████████▏ | 164/200 [05:51<01:22,  2.28s/it]Running Inference:  82%|████████▎ | 165/200 [05:53<01:20,  2.30s/it]Running Inference:  83%|████████▎ | 166/200 [05:59<01:48,  3.20s/it]Running Inference:  84%|████████▎ | 167/200 [06:00<01:30,  2.75s/it]Running Inference:  84%|████████▍ | 168/200 [06:03<01:28,  2.76s/it]Running Inference:  84%|████████▍ | 169/200 [06:04<01:07,  2.17s/it]Running Inference:  85%|████████▌ | 170/200 [06:06<01:06,  2.22s/it]Running Inference:  86%|████████▌ | 171/200 [06:09<01:05,  2.25s/it]Running Inference:  86%|████████▌ | 172/200 [06:10<00:53,  1.92s/it]Running Inference:  86%|████████▋ | 173/200 [06:12<00:54,  2.01s/it]Running Inference:  87%|████████▋ | 174/200 [06:14<00:49,  1.92s/it]Running Inference:  88%|████████▊ | 175/200 [06:16<00:50,  2.04s/it]Running Inference:  88%|████████▊ | 176/200 [06:18<00:47,  1.96s/it]Running Inference:  88%|████████▊ | 177/200 [06:19<00:42,  1.85s/it]Running Inference:  89%|████████▉ | 178/200 [06:22<00:44,  2.03s/it]Running Inference:  90%|████████▉ | 179/200 [06:24<00:45,  2.18s/it]Running Inference:  90%|█████████ | 180/200 [06:27<00:43,  2.20s/it]Running Inference:  90%|█████████ | 181/200 [06:28<00:37,  1.99s/it]Running Inference:  91%|█████████ | 182/200 [06:30<00:36,  2.03s/it]Running Inference:  92%|█████████▏| 183/200 [06:31<00:30,  1.76s/it]Running Inference:  92%|█████████▏| 184/200 [06:35<00:36,  2.26s/it]Running Inference:  92%|█████████▎| 185/200 [06:37<00:32,  2.14s/it]Running Inference:  93%|█████████▎| 186/200 [06:42<00:42,  3.03s/it]Running Inference:  94%|█████████▎| 187/200 [06:45<00:38,  2.98s/it]Running Inference:  94%|█████████▍| 188/200 [06:47<00:34,  2.86s/it]Running Inference:  94%|█████████▍| 189/200 [06:48<00:25,  2.32s/it]Running Inference:  95%|█████████▌| 190/200 [06:51<00:25,  2.53s/it]Running Inference:  96%|█████████▌| 191/200 [06:54<00:24,  2.69s/it]Running Inference:  96%|█████████▌| 192/200 [06:56<00:17,  2.23s/it]Running Inference:  96%|█████████▋| 193/200 [06:57<00:13,  1.96s/it]Running Inference:  97%|█████████▋| 194/200 [07:00<00:13,  2.17s/it]Running Inference:  98%|█████████▊| 195/200 [07:02<00:11,  2.21s/it]Running Inference:  98%|█████████▊| 196/200 [07:03<00:07,  1.97s/it]Running Inference:  98%|█████████▊| 197/200 [07:05<00:05,  1.80s/it]Running Inference:  99%|█████████▉| 198/200 [07:06<00:03,  1.64s/it]Running Inference: 100%|█████████▉| 199/200 [07:07<00:01,  1.59s/it]Running Inference: 100%|██████████| 200/200 [07:09<00:00,  1.65s/it]Running Inference: 100%|██████████| 200/200 [07:09<00:00,  2.15s/it]
2025-12-14 17:48:24,776 - INFO - Inference completed.
2025-12-14 17:48:24,784 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 17:48:24,784 - INFO - Calculating metrics for dataset: longbench
2025-12-14 17:48:24,785 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 17:48:24,785 - INFO - Metrics:
5.25
2025-12-14 17:48:24,787 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_count, ratio=0.2) on GPU 4

----------------------------------------
Task: passage_count | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:48:31,480 - INFO - Set deterministic seeds to 42
2025-12-14 17:48:31,480 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:48:31,480 - INFO - Starting evaluation run...
2025-12-14 17:48:31,480 - INFO - Output directory set to: longbenchresult
2025-12-14 17:48:31,480 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 17:48:31,480 - INFO - KV Press 'knorm' setup.
2025-12-14 17:48:31,480 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:48:31,480 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.95it/s]
Device set to use cuda:0
2025-12-14 17:48:44,660 - INFO - Model pipeline loaded.
2025-12-14 17:48:44,660 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 17:48:51,056 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:48:51,056 - INFO - Dataset processed with 200 entries.
2025-12-14 17:48:51,093 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:26,  1.94s/it]Running Inference:   1%|          | 2/200 [00:04<07:07,  2.16s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:15,  1.91s/it]Running Inference:   2%|▏         | 4/200 [00:06<04:59,  1.53s/it]Running Inference:   2%|▎         | 5/200 [00:09<06:36,  2.03s/it]Running Inference:   3%|▎         | 6/200 [00:11<06:22,  1.97s/it]Running Inference:   4%|▎         | 7/200 [00:12<05:38,  1.76s/it]Running Inference:   4%|▍         | 8/200 [00:13<04:50,  1.51s/it]Running Inference:   4%|▍         | 9/200 [00:16<06:11,  1.95s/it]Running Inference:   5%|▌         | 10/200 [00:18<06:24,  2.03s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:22<07:36,  2.42s/it]Running Inference:   6%|▌         | 12/200 [00:24<07:40,  2.45s/it]Running Inference:   6%|▋         | 13/200 [00:26<06:46,  2.17s/it]Running Inference:   7%|▋         | 14/200 [00:27<05:40,  1.83s/it]Running Inference:   8%|▊         | 15/200 [00:30<07:01,  2.28s/it]Running Inference:   8%|▊         | 16/200 [00:32<06:41,  2.18s/it]Running Inference:   8%|▊         | 17/200 [00:34<06:01,  1.97s/it]Running Inference:   9%|▉         | 18/200 [00:35<05:48,  1.91s/it]Running Inference:  10%|▉         | 19/200 [00:37<05:37,  1.87s/it]Running Inference:  10%|█         | 20/200 [00:39<05:50,  1.95s/it]Running Inference:  10%|█         | 21/200 [00:42<06:50,  2.29s/it]Running Inference:  11%|█         | 22/200 [00:45<06:44,  2.27s/it]Running Inference:  12%|█▏        | 23/200 [00:47<07:02,  2.39s/it]Running Inference:  12%|█▏        | 24/200 [00:50<07:01,  2.39s/it]Running Inference:  12%|█▎        | 25/200 [00:52<06:58,  2.39s/it]Running Inference:  13%|█▎        | 26/200 [00:54<06:48,  2.35s/it]Running Inference:  14%|█▎        | 27/200 [00:56<06:20,  2.20s/it]Running Inference:  14%|█▍        | 28/200 [01:01<08:20,  2.91s/it]Running Inference:  14%|█▍        | 29/200 [01:04<08:35,  3.01s/it]Running Inference:  15%|█▌        | 30/200 [01:05<07:05,  2.50s/it]Running Inference:  16%|█▌        | 31/200 [01:07<06:08,  2.18s/it]Running Inference:  16%|█▌        | 32/200 [01:08<05:27,  1.95s/it]Running Inference:  16%|█▋        | 33/200 [01:09<04:35,  1.65s/it]Running Inference:  17%|█▋        | 34/200 [01:12<05:13,  1.89s/it]Running Inference:  18%|█▊        | 35/200 [01:14<05:31,  2.01s/it]Running Inference:  18%|█▊        | 36/200 [01:16<05:52,  2.15s/it]Running Inference:  18%|█▊        | 37/200 [01:18<05:37,  2.07s/it]Running Inference:  19%|█▉        | 38/200 [01:20<05:15,  1.94s/it]Running Inference:  20%|█▉        | 39/200 [01:22<05:43,  2.13s/it]Running Inference:  20%|██        | 40/200 [01:24<05:20,  2.01s/it]Running Inference:  20%|██        | 41/200 [01:26<05:18,  2.00s/it]Running Inference:  21%|██        | 42/200 [01:27<04:37,  1.76s/it]Running Inference:  22%|██▏       | 43/200 [01:30<04:56,  1.89s/it]Running Inference:  22%|██▏       | 44/200 [01:32<04:59,  1.92s/it]Running Inference:  22%|██▎       | 45/200 [01:34<05:41,  2.20s/it]Running Inference:  23%|██▎       | 46/200 [01:37<05:46,  2.25s/it]Running Inference:  24%|██▎       | 47/200 [01:38<04:55,  1.93s/it]Running Inference:  24%|██▍       | 48/200 [01:39<04:08,  1.63s/it]Running Inference:  24%|██▍       | 49/200 [01:40<04:04,  1.62s/it]Running Inference:  25%|██▌       | 50/200 [01:42<03:41,  1.48s/it]Running Inference:  26%|██▌       | 51/200 [01:44<04:20,  1.74s/it]Running Inference:  26%|██▌       | 52/200 [01:45<04:03,  1.64s/it]Running Inference:  26%|██▋       | 53/200 [01:48<04:52,  1.99s/it]Running Inference:  27%|██▋       | 54/200 [01:50<05:03,  2.08s/it]Running Inference:  28%|██▊       | 55/200 [01:52<04:18,  1.79s/it]Running Inference:  28%|██▊       | 56/200 [01:53<04:22,  1.82s/it]Running Inference:  28%|██▊       | 57/200 [01:56<04:32,  1.91s/it]Running Inference:  29%|██▉       | 58/200 [01:58<04:41,  1.98s/it]Running Inference:  30%|██▉       | 59/200 [02:01<05:42,  2.43s/it]Running Inference:  30%|███       | 60/200 [02:04<05:59,  2.57s/it]Running Inference:  30%|███       | 61/200 [02:06<05:12,  2.25s/it]Running Inference:  31%|███       | 62/200 [02:07<04:21,  1.89s/it]Running Inference:  32%|███▏      | 63/200 [02:09<04:21,  1.91s/it]Running Inference:  32%|███▏      | 64/200 [02:11<04:50,  2.14s/it]Running Inference:  32%|███▎      | 65/200 [02:14<04:58,  2.21s/it]Running Inference:  33%|███▎      | 66/200 [02:16<05:04,  2.27s/it]Running Inference:  34%|███▎      | 67/200 [02:18<04:40,  2.11s/it]Running Inference:  34%|███▍      | 68/200 [02:20<04:39,  2.12s/it]Running Inference:  34%|███▍      | 69/200 [02:23<05:24,  2.48s/it]Running Inference:  35%|███▌      | 70/200 [02:25<05:06,  2.36s/it]Running Inference:  36%|███▌      | 71/200 [02:28<05:15,  2.44s/it]Running Inference:  36%|███▌      | 72/200 [02:30<04:55,  2.31s/it]Running Inference:  36%|███▋      | 73/200 [02:33<05:25,  2.57s/it]Running Inference:  37%|███▋      | 74/200 [02:36<05:17,  2.52s/it]Running Inference:  38%|███▊      | 75/200 [02:37<04:43,  2.27s/it]Running Inference:  38%|███▊      | 76/200 [02:39<04:06,  1.98s/it]Running Inference:  38%|███▊      | 77/200 [02:43<05:30,  2.69s/it]Running Inference:  39%|███▉      | 78/200 [02:45<05:05,  2.51s/it]Running Inference:  40%|███▉      | 79/200 [02:47<04:57,  2.46s/it]Running Inference:  40%|████      | 80/200 [02:50<04:54,  2.45s/it]Running Inference:  40%|████      | 81/200 [02:51<04:12,  2.12s/it]Running Inference:  41%|████      | 82/200 [02:52<03:38,  1.85s/it]Running Inference:  42%|████▏     | 83/200 [02:55<03:54,  2.01s/it]Running Inference:  42%|████▏     | 84/200 [02:57<03:51,  2.00s/it]Running Inference:  42%|████▎     | 85/200 [03:00<04:21,  2.27s/it]Running Inference:  43%|████▎     | 86/200 [03:02<04:19,  2.28s/it]Running Inference:  44%|████▎     | 87/200 [03:04<04:06,  2.18s/it]Running Inference:  44%|████▍     | 88/200 [03:06<04:05,  2.20s/it]Running Inference:  44%|████▍     | 89/200 [03:09<04:23,  2.38s/it]Running Inference:  45%|████▌     | 90/200 [03:12<04:34,  2.50s/it]Running Inference:  46%|████▌     | 91/200 [03:13<03:48,  2.10s/it]Running Inference:  46%|████▌     | 92/200 [03:14<03:10,  1.77s/it]Running Inference:  46%|████▋     | 93/200 [03:15<03:04,  1.72s/it]Running Inference:  47%|████▋     | 94/200 [03:17<03:07,  1.77s/it]Running Inference:  48%|████▊     | 95/200 [03:20<03:20,  1.91s/it]Running Inference:  48%|████▊     | 96/200 [03:21<03:18,  1.91s/it]Running Inference:  48%|████▊     | 97/200 [03:24<03:32,  2.06s/it]Running Inference:  49%|████▉     | 98/200 [03:27<04:18,  2.53s/it]Running Inference:  50%|████▉     | 99/200 [03:30<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [03:32<03:57,  2.37s/it]Running Inference:  50%|█████     | 101/200 [03:34<03:24,  2.07s/it]Running Inference:  51%|█████     | 102/200 [03:35<03:12,  1.96s/it]Running Inference:  52%|█████▏    | 103/200 [03:36<02:43,  1.69s/it]Running Inference:  52%|█████▏    | 104/200 [03:39<03:04,  1.92s/it]Running Inference:  52%|█████▎    | 105/200 [03:40<02:30,  1.58s/it]Running Inference:  53%|█████▎    | 106/200 [03:42<02:55,  1.86s/it]Running Inference:  54%|█████▎    | 107/200 [03:45<03:29,  2.25s/it]Running Inference:  54%|█████▍    | 108/200 [03:47<03:23,  2.21s/it]Running Inference:  55%|█████▍    | 109/200 [03:50<03:39,  2.41s/it]Running Inference:  55%|█████▌    | 110/200 [03:52<03:11,  2.13s/it]Running Inference:  56%|█████▌    | 111/200 [03:53<02:45,  1.86s/it]Running Inference:  56%|█████▌    | 112/200 [03:54<02:20,  1.60s/it]Running Inference:  56%|█████▋    | 113/200 [03:55<02:18,  1.59s/it]Running Inference:  57%|█████▋    | 114/200 [03:57<02:12,  1.54s/it]Running Inference:  57%|█████▊    | 115/200 [04:00<02:38,  1.86s/it]Running Inference:  58%|█████▊    | 116/200 [04:02<02:52,  2.06s/it]Running Inference:  58%|█████▊    | 117/200 [04:04<02:37,  1.90s/it]Running Inference:  59%|█████▉    | 118/200 [04:04<02:11,  1.60s/it]Running Inference:  60%|█████▉    | 119/200 [04:06<02:15,  1.67s/it]Running Inference:  60%|██████    | 120/200 [04:11<03:22,  2.53s/it]Running Inference:  60%|██████    | 121/200 [04:14<03:39,  2.78s/it]Running Inference:  61%|██████    | 122/200 [04:15<02:57,  2.28s/it]Running Inference:  62%|██████▏   | 123/200 [04:16<02:25,  1.89s/it]Running Inference:  62%|██████▏   | 124/200 [04:19<02:39,  2.10s/it]Running Inference:  62%|██████▎   | 125/200 [04:21<02:31,  2.02s/it]Running Inference:  63%|██████▎   | 126/200 [04:22<02:05,  1.69s/it]Running Inference:  64%|██████▎   | 127/200 [04:24<02:19,  1.92s/it]Running Inference:  64%|██████▍   | 128/200 [04:26<02:23,  1.99s/it]Running Inference:  64%|██████▍   | 129/200 [04:29<02:39,  2.25s/it]Running Inference:  65%|██████▌   | 130/200 [04:30<02:13,  1.91s/it]Running Inference:  66%|██████▌   | 131/200 [04:34<02:44,  2.38s/it]Running Inference:  66%|██████▌   | 132/200 [04:37<02:53,  2.55s/it]Running Inference:  66%|██████▋   | 133/200 [04:38<02:25,  2.17s/it]Running Inference:  67%|██████▋   | 134/200 [04:41<02:35,  2.36s/it]Running Inference:  68%|██████▊   | 135/200 [04:42<02:20,  2.17s/it]Running Inference:  68%|██████▊   | 136/200 [04:44<02:11,  2.06s/it]Running Inference:  68%|██████▊   | 137/200 [04:47<02:15,  2.14s/it]Running Inference:  69%|██████▉   | 138/200 [04:49<02:09,  2.09s/it]Running Inference:  70%|██████▉   | 139/200 [04:52<02:27,  2.42s/it]Running Inference:  70%|███████   | 140/200 [04:54<02:30,  2.51s/it]Running Inference:  70%|███████   | 141/200 [04:56<02:14,  2.27s/it]Running Inference:  71%|███████   | 142/200 [04:59<02:15,  2.34s/it]Running Inference:  72%|███████▏  | 143/200 [05:00<01:55,  2.02s/it]Running Inference:  72%|███████▏  | 144/200 [05:02<01:52,  2.01s/it]Running Inference:  72%|███████▎  | 145/200 [05:04<01:56,  2.12s/it]Running Inference:  73%|███████▎  | 146/200 [05:07<01:56,  2.16s/it]Running Inference:  74%|███████▎  | 147/200 [05:09<01:52,  2.12s/it]Running Inference:  74%|███████▍  | 148/200 [05:11<01:54,  2.19s/it]Running Inference:  74%|███████▍  | 149/200 [05:12<01:36,  1.89s/it]Running Inference:  75%|███████▌  | 150/200 [05:14<01:28,  1.78s/it]Running Inference:  76%|███████▌  | 151/200 [05:15<01:14,  1.53s/it]Running Inference:  76%|███████▌  | 152/200 [05:17<01:25,  1.77s/it]Running Inference:  76%|███████▋  | 153/200 [05:18<01:08,  1.45s/it]Running Inference:  77%|███████▋  | 154/200 [05:20<01:23,  1.82s/it]Running Inference:  78%|███████▊  | 155/200 [05:22<01:19,  1.76s/it]Running Inference:  78%|███████▊  | 156/200 [05:24<01:19,  1.82s/it]Running Inference:  78%|███████▊  | 157/200 [05:25<01:09,  1.62s/it]Running Inference:  79%|███████▉  | 158/200 [05:27<01:14,  1.76s/it]Running Inference:  80%|███████▉  | 159/200 [05:29<01:13,  1.80s/it]Running Inference:  80%|████████  | 160/200 [05:32<01:31,  2.29s/it]Running Inference:  80%|████████  | 161/200 [05:35<01:32,  2.37s/it]Running Inference:  81%|████████  | 162/200 [05:37<01:27,  2.29s/it]Running Inference:  82%|████████▏ | 163/200 [05:40<01:34,  2.57s/it]Running Inference:  82%|████████▏ | 164/200 [05:42<01:22,  2.29s/it]Running Inference:  82%|████████▎ | 165/200 [05:44<01:20,  2.31s/it]Running Inference:  83%|████████▎ | 166/200 [05:48<01:27,  2.58s/it]Running Inference:  84%|████████▎ | 167/200 [05:49<01:16,  2.31s/it]Running Inference:  84%|████████▍ | 168/200 [05:52<01:18,  2.46s/it]Running Inference:  84%|████████▍ | 169/200 [05:53<01:00,  1.96s/it]Running Inference:  85%|████████▌ | 170/200 [05:55<01:02,  2.07s/it]Running Inference:  86%|████████▌ | 171/200 [05:57<01:02,  2.15s/it]Running Inference:  86%|████████▌ | 172/200 [05:59<00:51,  1.85s/it]Running Inference:  86%|████████▋ | 173/200 [06:01<00:52,  1.96s/it]Running Inference:  87%|████████▋ | 174/200 [06:03<00:49,  1.89s/it]Running Inference:  88%|████████▊ | 175/200 [06:05<00:50,  2.00s/it]Running Inference:  88%|████████▊ | 176/200 [06:07<00:46,  1.94s/it]Running Inference:  88%|████████▊ | 177/200 [06:08<00:42,  1.84s/it]Running Inference:  89%|████████▉ | 178/200 [06:11<00:44,  2.02s/it]Running Inference:  90%|████████▉ | 179/200 [06:13<00:45,  2.18s/it]Running Inference:  90%|█████████ | 180/200 [06:15<00:43,  2.18s/it]Running Inference:  90%|█████████ | 181/200 [06:17<00:37,  1.98s/it]Running Inference:  91%|█████████ | 182/200 [06:19<00:36,  2.03s/it]Running Inference:  92%|█████████▏| 183/200 [06:20<00:29,  1.76s/it]Running Inference:  92%|█████████▏| 184/200 [06:24<00:36,  2.27s/it]Running Inference:  92%|█████████▎| 185/200 [06:26<00:32,  2.15s/it]Running Inference:  93%|█████████▎| 186/200 [06:31<00:42,  3.04s/it]Running Inference:  94%|█████████▎| 187/200 [06:34<00:38,  2.99s/it]Running Inference:  94%|█████████▍| 188/200 [06:36<00:34,  2.87s/it]Running Inference:  94%|█████████▍| 189/200 [06:37<00:25,  2.33s/it]Running Inference:  95%|█████████▌| 190/200 [06:40<00:25,  2.53s/it]Running Inference:  96%|█████████▌| 191/200 [06:43<00:24,  2.69s/it]Running Inference:  96%|█████████▌| 192/200 [06:44<00:17,  2.23s/it]Running Inference:  96%|█████████▋| 193/200 [06:46<00:13,  1.96s/it]Running Inference:  97%|█████████▋| 194/200 [06:48<00:13,  2.17s/it]Running Inference:  98%|█████████▊| 195/200 [06:51<00:10,  2.19s/it]Running Inference:  98%|█████████▊| 196/200 [06:52<00:07,  1.96s/it]Running Inference:  98%|█████████▊| 197/200 [06:53<00:05,  1.79s/it]Running Inference:  99%|█████████▉| 198/200 [06:55<00:03,  1.64s/it]Running Inference: 100%|█████████▉| 199/200 [06:56<00:01,  1.57s/it]Running Inference: 100%|██████████| 200/200 [06:58<00:00,  1.62s/it]Running Inference: 100%|██████████| 200/200 [06:58<00:00,  2.09s/it]
2025-12-14 17:55:49,478 - INFO - Inference completed.
2025-12-14 17:55:49,487 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 17:55:49,487 - INFO - Calculating metrics for dataset: longbench
2025-12-14 17:55:49,488 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 17:55:49,488 - INFO - Metrics:
5.75
2025-12-14 17:55:49,489 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_count, ratio=0.3) on GPU 4

----------------------------------------
Task: passage_count | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:55:55,944 - INFO - Set deterministic seeds to 42
2025-12-14 17:55:55,944 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:55:55,944 - INFO - Starting evaluation run...
2025-12-14 17:55:55,944 - INFO - Output directory set to: longbenchresult
2025-12-14 17:55:55,944 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 17:55:55,944 - INFO - KV Press 'knorm' setup.
2025-12-14 17:55:55,944 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:55:55,944 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.59it/s]
Device set to use cuda:0
2025-12-14 17:56:08,224 - INFO - Model pipeline loaded.
2025-12-14 17:56:08,224 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 17:56:15,873 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:56:15,874 - INFO - Dataset processed with 200 entries.
2025-12-14 17:56:15,916 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<06:48,  2.05s/it]Running Inference:   1%|          | 2/200 [00:04<07:19,  2.22s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:21,  1.94s/it]Running Inference:   2%|▏         | 4/200 [00:06<05:04,  1.55s/it]Running Inference:   2%|▎         | 5/200 [00:11<08:31,  2.62s/it]Running Inference:   3%|▎         | 6/200 [00:13<07:37,  2.36s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:28,  2.01s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:23,  1.69s/it]Running Inference:   4%|▍         | 9/200 [00:18<06:33,  2.06s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:39,  2.10s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:45,  2.46s/it]Running Inference:   6%|▌         | 12/200 [00:28<09:45,  3.11s/it]Running Inference:   6%|▋         | 13/200 [00:30<08:12,  2.63s/it]Running Inference:   7%|▋         | 14/200 [00:31<06:39,  2.15s/it]Running Inference:   8%|▊         | 15/200 [00:34<07:41,  2.49s/it]Running Inference:   8%|▊         | 16/200 [00:36<07:08,  2.33s/it]Running Inference:   8%|▊         | 17/200 [00:37<06:22,  2.09s/it]Running Inference:   9%|▉         | 18/200 [00:39<06:02,  1.99s/it]Running Inference:  10%|▉         | 19/200 [00:41<05:47,  1.92s/it]Running Inference:  10%|█         | 20/200 [00:43<05:56,  1.98s/it]Running Inference:  10%|█         | 21/200 [00:46<06:54,  2.32s/it]Running Inference:  11%|█         | 22/200 [00:48<06:46,  2.29s/it]Running Inference:  12%|█▏        | 23/200 [00:51<07:03,  2.39s/it]Running Inference:  12%|█▏        | 24/200 [00:53<07:03,  2.41s/it]Running Inference:  12%|█▎        | 25/200 [00:56<06:59,  2.39s/it]Running Inference:  13%|█▎        | 26/200 [00:58<06:48,  2.35s/it]Running Inference:  14%|█▎        | 27/200 [01:00<06:20,  2.20s/it]Running Inference:  14%|█▍        | 28/200 [01:04<08:18,  2.90s/it]Running Inference:  14%|█▍        | 29/200 [01:08<08:33,  3.00s/it]Running Inference:  15%|█▌        | 30/200 [01:09<07:03,  2.49s/it]Running Inference:  16%|█▌        | 31/200 [01:10<06:09,  2.18s/it]Running Inference:  16%|█▌        | 32/200 [01:12<05:27,  1.95s/it]Running Inference:  16%|█▋        | 33/200 [01:13<04:35,  1.65s/it]Running Inference:  17%|█▋        | 34/200 [01:15<05:13,  1.89s/it]Running Inference:  18%|█▊        | 35/200 [01:17<05:30,  2.01s/it]Running Inference:  18%|█▊        | 36/200 [01:20<05:50,  2.14s/it]Running Inference:  18%|█▊        | 37/200 [01:22<05:35,  2.06s/it]Running Inference:  19%|█▉        | 38/200 [01:23<05:13,  1.93s/it]Running Inference:  20%|█▉        | 39/200 [01:26<05:41,  2.12s/it]Running Inference:  20%|██        | 40/200 [01:28<05:21,  2.01s/it]Running Inference:  20%|██        | 41/200 [01:30<05:18,  2.00s/it]Running Inference:  21%|██        | 42/200 [01:31<04:39,  1.77s/it]Running Inference:  22%|██▏       | 43/200 [01:33<04:57,  1.89s/it]Running Inference:  22%|██▏       | 44/200 [01:35<05:01,  1.93s/it]Running Inference:  22%|██▎       | 45/200 [01:38<05:42,  2.21s/it]Running Inference:  23%|██▎       | 46/200 [01:40<05:46,  2.25s/it]Running Inference:  24%|██▎       | 47/200 [01:42<04:55,  1.93s/it]Running Inference:  24%|██▍       | 48/200 [01:42<04:05,  1.62s/it]Running Inference:  24%|██▍       | 49/200 [01:44<04:02,  1.61s/it]Running Inference:  25%|██▌       | 50/200 [01:45<03:39,  1.47s/it]Running Inference:  26%|██▌       | 51/200 [01:48<04:18,  1.73s/it]Running Inference:  26%|██▌       | 52/200 [01:49<04:01,  1.63s/it]Running Inference:  26%|██▋       | 53/200 [01:52<04:50,  1.98s/it]Running Inference:  27%|██▋       | 54/200 [01:54<05:02,  2.07s/it]Running Inference:  28%|██▊       | 55/200 [01:55<04:17,  1.78s/it]Running Inference:  28%|██▊       | 56/200 [01:57<04:21,  1.81s/it]Running Inference:  28%|██▊       | 57/200 [01:59<04:31,  1.90s/it]Running Inference:  29%|██▉       | 58/200 [02:01<04:40,  1.97s/it]Running Inference:  30%|██▉       | 59/200 [02:06<06:17,  2.68s/it]Running Inference:  30%|███       | 60/200 [02:08<06:23,  2.74s/it]Running Inference:  30%|███       | 61/200 [02:10<05:29,  2.37s/it]Running Inference:  31%|███       | 62/200 [02:11<04:30,  1.96s/it]Running Inference:  32%|███▏      | 63/200 [02:13<04:50,  2.12s/it]Running Inference:  32%|███▏      | 64/200 [02:16<05:10,  2.29s/it]Running Inference:  32%|███▎      | 65/200 [02:18<05:12,  2.31s/it]Running Inference:  33%|███▎      | 66/200 [02:21<05:13,  2.34s/it]Running Inference:  34%|███▎      | 67/200 [02:23<04:46,  2.15s/it]Running Inference:  34%|███▍      | 68/200 [02:25<04:43,  2.15s/it]Running Inference:  34%|███▍      | 69/200 [02:28<05:27,  2.50s/it]Running Inference:  35%|███▌      | 70/200 [02:30<05:07,  2.37s/it]Running Inference:  36%|███▌      | 71/200 [02:33<05:15,  2.44s/it]Running Inference:  36%|███▌      | 72/200 [02:35<04:56,  2.31s/it]Running Inference:  36%|███▋      | 73/200 [02:38<05:25,  2.56s/it]Running Inference:  37%|███▋      | 74/200 [02:40<05:16,  2.51s/it]Running Inference:  38%|███▊      | 75/200 [02:42<04:42,  2.26s/it]Running Inference:  38%|███▊      | 76/200 [02:43<04:05,  1.98s/it]Running Inference:  38%|███▊      | 77/200 [02:48<05:29,  2.68s/it]Running Inference:  39%|███▉      | 78/200 [02:50<05:04,  2.50s/it]Running Inference:  40%|███▉      | 79/200 [02:52<04:56,  2.45s/it]Running Inference:  40%|████      | 80/200 [02:54<04:53,  2.44s/it]Running Inference:  40%|████      | 81/200 [02:56<04:11,  2.11s/it]Running Inference:  41%|████      | 82/200 [02:57<03:37,  1.84s/it]Running Inference:  42%|████▏     | 83/200 [02:59<03:54,  2.00s/it]Running Inference:  42%|████▏     | 84/200 [03:01<03:50,  1.99s/it]Running Inference:  42%|████▎     | 85/200 [03:04<04:19,  2.26s/it]Running Inference:  43%|████▎     | 86/200 [03:07<04:18,  2.27s/it]Running Inference:  44%|████▎     | 87/200 [03:08<04:05,  2.17s/it]Running Inference:  44%|████▍     | 88/200 [03:11<04:04,  2.18s/it]Running Inference:  44%|████▍     | 89/200 [03:14<04:24,  2.38s/it]Running Inference:  45%|████▌     | 90/200 [03:16<04:34,  2.50s/it]Running Inference:  46%|████▌     | 91/200 [03:17<03:49,  2.11s/it]Running Inference:  46%|████▌     | 92/200 [03:18<03:11,  1.77s/it]Running Inference:  46%|████▋     | 93/200 [03:20<03:04,  1.72s/it]Running Inference:  47%|████▋     | 94/200 [03:22<03:07,  1.77s/it]Running Inference:  48%|████▊     | 95/200 [03:24<03:19,  1.90s/it]Running Inference:  48%|████▊     | 96/200 [03:26<03:19,  1.92s/it]Running Inference:  48%|████▊     | 97/200 [03:29<03:33,  2.08s/it]Running Inference:  49%|████▉     | 98/200 [03:32<04:18,  2.53s/it]Running Inference:  50%|████▉     | 99/200 [03:35<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [03:37<03:56,  2.36s/it]Running Inference:  50%|█████     | 101/200 [03:38<03:23,  2.06s/it]Running Inference:  51%|█████     | 102/200 [03:40<03:12,  1.97s/it]Running Inference:  52%|█████▏    | 103/200 [03:41<02:44,  1.69s/it]Running Inference:  52%|█████▏    | 104/200 [03:43<03:04,  1.92s/it]Running Inference:  52%|█████▎    | 105/200 [03:44<02:29,  1.57s/it]Running Inference:  53%|█████▎    | 106/200 [03:47<02:54,  1.85s/it]Running Inference:  54%|█████▎    | 107/200 [03:50<03:27,  2.24s/it]Running Inference:  54%|█████▍    | 108/200 [03:52<03:21,  2.19s/it]Running Inference:  55%|█████▍    | 109/200 [03:55<03:38,  2.40s/it]Running Inference:  55%|█████▌    | 110/200 [03:56<03:11,  2.13s/it]Running Inference:  56%|█████▌    | 111/200 [03:57<02:44,  1.85s/it]Running Inference:  56%|█████▌    | 112/200 [03:58<02:21,  1.61s/it]Running Inference:  56%|█████▋    | 113/200 [04:00<02:18,  1.60s/it]Running Inference:  57%|█████▋    | 114/200 [04:01<02:12,  1.54s/it]Running Inference:  57%|█████▊    | 115/200 [04:04<02:38,  1.86s/it]Running Inference:  58%|█████▊    | 116/200 [04:07<02:52,  2.05s/it]Running Inference:  58%|█████▊    | 117/200 [04:08<02:37,  1.89s/it]Running Inference:  59%|█████▉    | 118/200 [04:09<02:10,  1.60s/it]Running Inference:  60%|█████▉    | 119/200 [04:11<02:14,  1.66s/it]Running Inference:  60%|██████    | 120/200 [04:15<03:20,  2.51s/it]Running Inference:  60%|██████    | 121/200 [04:19<03:38,  2.76s/it]Running Inference:  61%|██████    | 122/200 [04:20<02:57,  2.28s/it]Running Inference:  62%|██████▏   | 123/200 [04:21<02:25,  1.88s/it]Running Inference:  62%|██████▏   | 124/200 [04:23<02:39,  2.10s/it]Running Inference:  62%|██████▎   | 125/200 [04:25<02:31,  2.02s/it]Running Inference:  63%|██████▎   | 126/200 [04:26<02:04,  1.68s/it]Running Inference:  64%|██████▎   | 127/200 [04:29<02:19,  1.91s/it]Running Inference:  64%|██████▍   | 128/200 [04:31<02:22,  1.98s/it]Running Inference:  64%|██████▍   | 129/200 [04:34<02:38,  2.24s/it]Running Inference:  65%|██████▌   | 130/200 [04:35<02:12,  1.90s/it]Running Inference:  66%|██████▌   | 131/200 [04:38<02:43,  2.37s/it]Running Inference:  66%|██████▌   | 132/200 [04:41<02:52,  2.54s/it]Running Inference:  66%|██████▋   | 133/200 [04:42<02:24,  2.16s/it]Running Inference:  67%|██████▋   | 134/200 [04:45<02:34,  2.34s/it]Running Inference:  68%|██████▊   | 135/200 [04:47<02:20,  2.16s/it]Running Inference:  68%|██████▊   | 136/200 [04:49<02:11,  2.06s/it]Running Inference:  68%|██████▊   | 137/200 [04:51<02:15,  2.14s/it]Running Inference:  69%|██████▉   | 138/200 [04:53<02:08,  2.08s/it]Running Inference:  70%|██████▉   | 139/200 [04:56<02:28,  2.43s/it]Running Inference:  70%|███████   | 140/200 [04:59<02:31,  2.53s/it]Running Inference:  70%|███████   | 141/200 [05:01<02:14,  2.28s/it]Running Inference:  71%|███████   | 142/200 [05:03<02:16,  2.35s/it]Running Inference:  72%|███████▏  | 143/200 [05:04<01:55,  2.02s/it]Running Inference:  72%|███████▏  | 144/200 [05:06<01:52,  2.01s/it]Running Inference:  72%|███████▎  | 145/200 [05:09<01:56,  2.11s/it]Running Inference:  73%|███████▎  | 146/200 [05:11<01:56,  2.17s/it]Running Inference:  74%|███████▎  | 147/200 [05:13<01:53,  2.13s/it]Running Inference:  74%|███████▍  | 148/200 [05:15<01:54,  2.20s/it]Running Inference:  74%|███████▍  | 149/200 [05:17<01:36,  1.89s/it]Running Inference:  75%|███████▌  | 150/200 [05:18<01:28,  1.76s/it]Running Inference:  76%|███████▌  | 151/200 [05:19<01:14,  1.53s/it]Running Inference:  76%|███████▌  | 152/200 [05:21<01:24,  1.77s/it]Running Inference:  76%|███████▋  | 153/200 [05:22<01:08,  1.45s/it]Running Inference:  77%|███████▋  | 154/200 [05:25<01:23,  1.82s/it]Running Inference:  78%|███████▊  | 155/200 [05:28<01:47,  2.38s/it]Running Inference:  78%|███████▊  | 156/200 [05:30<01:38,  2.25s/it]Running Inference:  78%|███████▊  | 157/200 [05:32<01:22,  1.93s/it]Running Inference:  79%|███████▉  | 158/200 [05:34<01:22,  1.98s/it]Running Inference:  80%|███████▉  | 159/200 [05:36<01:19,  1.95s/it]Running Inference:  80%|████████  | 160/200 [05:39<01:35,  2.39s/it]Running Inference:  80%|████████  | 161/200 [05:42<01:35,  2.44s/it]Running Inference:  81%|████████  | 162/200 [05:44<01:28,  2.34s/it]Running Inference:  82%|████████▏ | 163/200 [05:48<01:51,  3.01s/it]Running Inference:  82%|████████▏ | 164/200 [05:50<01:34,  2.62s/it]Running Inference:  82%|████████▎ | 165/200 [05:52<01:28,  2.53s/it]Running Inference:  83%|████████▎ | 166/200 [05:55<01:32,  2.73s/it]Running Inference:  84%|████████▎ | 167/200 [05:57<01:19,  2.41s/it]Running Inference:  84%|████████▍ | 168/200 [06:00<01:20,  2.53s/it]Running Inference:  84%|████████▍ | 169/200 [06:01<01:02,  2.00s/it]Running Inference:  85%|████████▌ | 170/200 [06:03<01:02,  2.10s/it]Running Inference:  86%|████████▌ | 171/200 [06:05<01:02,  2.16s/it]Running Inference:  86%|████████▌ | 172/200 [06:06<00:52,  1.86s/it]Running Inference:  86%|████████▋ | 173/200 [06:09<00:53,  1.97s/it]Running Inference:  87%|████████▋ | 174/200 [06:10<00:49,  1.89s/it]Running Inference:  88%|████████▊ | 175/200 [06:13<00:50,  2.01s/it]Running Inference:  88%|████████▊ | 176/200 [06:14<00:46,  1.94s/it]Running Inference:  88%|████████▊ | 177/200 [06:16<00:42,  1.85s/it]Running Inference:  89%|████████▉ | 178/200 [06:19<00:44,  2.03s/it]Running Inference:  90%|████████▉ | 179/200 [06:21<00:45,  2.19s/it]Running Inference:  90%|█████████ | 180/200 [06:23<00:43,  2.20s/it]Running Inference:  90%|█████████ | 181/200 [06:25<00:37,  1.99s/it]Running Inference:  91%|█████████ | 182/200 [06:27<00:36,  2.03s/it]Running Inference:  92%|█████████▏| 183/200 [06:28<00:30,  1.78s/it]Running Inference:  92%|█████████▏| 184/200 [06:32<00:36,  2.27s/it]Running Inference:  92%|█████████▎| 185/200 [06:33<00:32,  2.14s/it]Running Inference:  93%|█████████▎| 186/200 [06:38<00:42,  3.02s/it]Running Inference:  94%|█████████▎| 187/200 [06:41<00:38,  2.97s/it]Running Inference:  94%|█████████▍| 188/200 [06:44<00:34,  2.85s/it]Running Inference:  94%|█████████▍| 189/200 [06:45<00:25,  2.31s/it]Running Inference:  95%|█████████▌| 190/200 [06:48<00:25,  2.53s/it]Running Inference:  96%|█████████▌| 191/200 [06:51<00:24,  2.69s/it]Running Inference:  96%|█████████▌| 192/200 [06:52<00:17,  2.23s/it]Running Inference:  96%|█████████▋| 193/200 [06:54<00:13,  1.95s/it]Running Inference:  97%|█████████▋| 194/200 [06:56<00:12,  2.16s/it]Running Inference:  98%|█████████▊| 195/200 [06:58<00:10,  2.20s/it]Running Inference:  98%|█████████▊| 196/200 [07:00<00:07,  1.96s/it]Running Inference:  98%|█████████▊| 197/200 [07:01<00:05,  1.79s/it]Running Inference:  99%|█████████▉| 198/200 [07:02<00:03,  1.63s/it]Running Inference: 100%|█████████▉| 199/200 [07:04<00:01,  1.58s/it]Running Inference: 100%|██████████| 200/200 [07:06<00:00,  1.62s/it]Running Inference: 100%|██████████| 200/200 [07:06<00:00,  2.13s/it]
2025-12-14 18:03:22,107 - INFO - Inference completed.
2025-12-14 18:03:22,116 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 18:03:22,116 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:03:22,117 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 18:03:22,117 - INFO - Metrics:
4.1
2025-12-14 18:03:22,118 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_count, ratio=0.5) on GPU 4


========================================
LongBench Task: passage_retrieval_en
========================================
----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:03:28,841 - INFO - Set deterministic seeds to 42
2025-12-14 18:03:28,841 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:03:28,841 - INFO - Starting evaluation run...
2025-12-14 18:03:28,841 - INFO - Output directory set to: longbenchresult
2025-12-14 18:03:28,841 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 18:03:28,841 - INFO - KV Press 'knorm' setup.
2025-12-14 18:03:28,841 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:03:28,841 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.73it/s]
Device set to use cuda:0
2025-12-14 18:03:43,554 - INFO - Model pipeline loaded.
2025-12-14 18:03:43,554 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 18:03:49,452 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:03:49,452 - INFO - Dataset processed with 200 entries.
2025-12-14 18:03:49,487 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:45,  2.34s/it]Running Inference:   1%|          | 2/200 [00:03<06:04,  1.84s/it]Running Inference:   2%|▏         | 3/200 [00:07<09:17,  2.83s/it]Running Inference:   2%|▏         | 4/200 [00:09<07:43,  2.37s/it]Running Inference:   2%|▎         | 5/200 [00:11<07:01,  2.16s/it]Running Inference:   3%|▎         | 6/200 [00:12<06:11,  1.92s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:24,  1.99s/it]Running Inference:   4%|▍         | 8/200 [00:16<06:03,  1.89s/it]Running Inference:   4%|▍         | 9/200 [00:18<05:44,  1.80s/it]Running Inference:   5%|▌         | 10/200 [00:19<05:20,  1.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:21<05:05,  1.62s/it]Running Inference:   6%|▌         | 12/200 [00:22<05:20,  1.70s/it]Running Inference:   6%|▋         | 13/200 [00:26<07:10,  2.30s/it]Running Inference:   7%|▋         | 14/200 [00:28<06:29,  2.10s/it]Running Inference:   8%|▊         | 15/200 [00:30<06:09,  2.00s/it]Running Inference:   8%|▊         | 16/200 [00:31<05:46,  1.88s/it]Running Inference:   8%|▊         | 17/200 [00:33<05:31,  1.81s/it]Running Inference:   9%|▉         | 18/200 [00:34<05:17,  1.74s/it]Running Inference:  10%|▉         | 19/200 [00:36<05:10,  1.72s/it]Running Inference:  10%|█         | 20/200 [00:38<05:09,  1.72s/it]Running Inference:  10%|█         | 21/200 [00:40<05:14,  1.76s/it]Running Inference:  11%|█         | 22/200 [00:41<05:03,  1.71s/it]Running Inference:  12%|█▏        | 23/200 [00:43<05:04,  1.72s/it]Running Inference:  12%|█▏        | 24/200 [00:45<04:56,  1.68s/it]Running Inference:  12%|█▎        | 25/200 [00:46<04:36,  1.58s/it]Running Inference:  13%|█▎        | 26/200 [00:48<04:58,  1.72s/it]Running Inference:  14%|█▎        | 27/200 [00:50<05:14,  1.82s/it]Running Inference:  14%|█▍        | 28/200 [00:53<06:35,  2.30s/it]Running Inference:  14%|█▍        | 29/200 [00:55<05:47,  2.03s/it]Running Inference:  15%|█▌        | 30/200 [00:58<07:07,  2.52s/it]Running Inference:  16%|█▌        | 31/200 [01:02<08:16,  2.94s/it]Running Inference:  16%|█▌        | 32/200 [01:06<09:01,  3.22s/it]Running Inference:  16%|█▋        | 33/200 [01:08<07:41,  2.77s/it]Running Inference:  17%|█▋        | 34/200 [01:12<08:43,  3.15s/it]Running Inference:  18%|█▊        | 35/200 [01:14<07:29,  2.73s/it]Running Inference:  18%|█▊        | 36/200 [01:15<06:36,  2.42s/it]Running Inference:  18%|█▊        | 37/200 [01:17<05:59,  2.21s/it]Running Inference:  19%|█▉        | 38/200 [01:19<05:34,  2.06s/it]Running Inference:  20%|█▉        | 39/200 [01:20<05:03,  1.89s/it]Running Inference:  20%|██        | 40/200 [01:22<04:51,  1.82s/it]Running Inference:  20%|██        | 41/200 [01:24<04:50,  1.83s/it]Running Inference:  21%|██        | 42/200 [01:25<04:28,  1.70s/it]Running Inference:  22%|██▏       | 43/200 [01:27<04:42,  1.80s/it]Running Inference:  22%|██▏       | 44/200 [01:31<06:17,  2.42s/it]Running Inference:  22%|██▎       | 45/200 [01:33<05:32,  2.15s/it]Running Inference:  23%|██▎       | 46/200 [01:35<05:17,  2.06s/it]Running Inference:  24%|██▎       | 47/200 [01:36<04:50,  1.90s/it]Running Inference:  24%|██▍       | 48/200 [01:37<04:28,  1.76s/it]Running Inference:  24%|██▍       | 49/200 [01:39<04:32,  1.80s/it]Running Inference:  25%|██▌       | 50/200 [01:41<04:22,  1.75s/it]Running Inference:  26%|██▌       | 51/200 [01:43<04:34,  1.84s/it]Running Inference:  26%|██▌       | 52/200 [01:45<04:53,  1.98s/it]Running Inference:  26%|██▋       | 53/200 [01:47<04:47,  1.95s/it]Running Inference:  27%|██▋       | 54/200 [01:49<04:38,  1.91s/it]Running Inference:  28%|██▊       | 55/200 [01:50<04:15,  1.76s/it]Running Inference:  28%|██▊       | 56/200 [01:52<04:11,  1.75s/it]Running Inference:  28%|██▊       | 57/200 [01:56<05:23,  2.27s/it]Running Inference:  29%|██▉       | 58/200 [01:58<05:02,  2.13s/it]Running Inference:  30%|██▉       | 59/200 [01:59<04:50,  2.06s/it]Running Inference:  30%|███       | 60/200 [02:01<04:30,  1.93s/it]Running Inference:  30%|███       | 61/200 [02:03<04:13,  1.82s/it]Running Inference:  31%|███       | 62/200 [02:04<04:13,  1.84s/it]Running Inference:  32%|███▏      | 63/200 [02:06<04:01,  1.76s/it]Running Inference:  32%|███▏      | 64/200 [02:08<03:52,  1.71s/it]Running Inference:  32%|███▎      | 65/200 [02:09<03:43,  1.65s/it]Running Inference:  33%|███▎      | 66/200 [02:11<03:54,  1.75s/it]Running Inference:  34%|███▎      | 67/200 [02:15<05:01,  2.26s/it]Running Inference:  34%|███▍      | 68/200 [02:16<04:43,  2.15s/it]Running Inference:  34%|███▍      | 69/200 [02:19<05:14,  2.40s/it]Running Inference:  35%|███▌      | 70/200 [02:21<04:51,  2.24s/it]Running Inference:  36%|███▌      | 71/200 [02:23<04:35,  2.14s/it]Running Inference:  36%|███▌      | 72/200 [02:25<04:23,  2.06s/it]Running Inference:  36%|███▋      | 73/200 [02:27<04:02,  1.91s/it]Running Inference:  37%|███▋      | 74/200 [02:29<04:07,  1.96s/it]Running Inference:  38%|███▊      | 75/200 [02:30<03:48,  1.83s/it]Running Inference:  38%|███▊      | 76/200 [02:32<03:27,  1.68s/it]Running Inference:  38%|███▊      | 77/200 [02:33<03:26,  1.68s/it]Running Inference:  39%|███▉      | 78/200 [02:35<03:25,  1.68s/it]Running Inference:  40%|███▉      | 79/200 [02:37<03:22,  1.67s/it]Running Inference:  40%|████      | 80/200 [02:38<03:18,  1.65s/it]Running Inference:  40%|████      | 81/200 [02:40<03:12,  1.62s/it]Running Inference:  41%|████      | 82/200 [02:42<03:17,  1.68s/it]Running Inference:  42%|████▏     | 83/200 [02:43<03:15,  1.67s/it]Running Inference:  42%|████▏     | 84/200 [02:45<03:09,  1.63s/it]Running Inference:  42%|████▎     | 85/200 [02:48<04:17,  2.24s/it]Running Inference:  43%|████▎     | 86/200 [02:52<05:07,  2.70s/it]Running Inference:  44%|████▎     | 87/200 [02:54<04:31,  2.40s/it]Running Inference:  44%|████▍     | 88/200 [02:55<03:59,  2.13s/it]Running Inference:  44%|████▍     | 89/200 [02:57<03:35,  1.94s/it]Running Inference:  45%|████▌     | 90/200 [02:58<03:19,  1.81s/it]Running Inference:  46%|████▌     | 91/200 [03:02<04:18,  2.37s/it]Running Inference:  46%|████▌     | 92/200 [03:04<03:57,  2.20s/it]Running Inference:  46%|████▋     | 93/200 [03:07<04:38,  2.60s/it]Running Inference:  47%|████▋     | 94/200 [03:10<04:21,  2.47s/it]Running Inference:  48%|████▊     | 95/200 [03:11<03:51,  2.21s/it]Running Inference:  48%|████▊     | 96/200 [03:13<03:29,  2.01s/it]Running Inference:  48%|████▊     | 97/200 [03:16<04:19,  2.52s/it]Running Inference:  49%|████▉     | 98/200 [03:18<03:55,  2.31s/it]Running Inference:  50%|████▉     | 99/200 [03:20<03:30,  2.09s/it]Running Inference:  50%|█████     | 100/200 [03:21<03:15,  1.96s/it]Running Inference:  50%|█████     | 101/200 [03:23<02:59,  1.82s/it]Running Inference:  51%|█████     | 102/200 [03:25<02:51,  1.75s/it]Running Inference:  52%|█████▏    | 103/200 [03:28<03:44,  2.31s/it]Running Inference:  52%|█████▏    | 104/200 [03:30<03:19,  2.08s/it]Running Inference:  52%|█████▎    | 105/200 [03:32<03:10,  2.01s/it]Running Inference:  53%|█████▎    | 106/200 [03:33<02:58,  1.89s/it]Running Inference:  54%|█████▎    | 107/200 [03:35<02:43,  1.75s/it]Running Inference:  54%|█████▍    | 108/200 [03:37<02:47,  1.83s/it]Running Inference:  55%|█████▍    | 109/200 [03:39<02:48,  1.85s/it]Running Inference:  55%|█████▌    | 110/200 [03:40<02:46,  1.85s/it]Running Inference:  56%|█████▌    | 111/200 [03:42<02:34,  1.74s/it]Running Inference:  56%|█████▌    | 112/200 [03:44<02:41,  1.83s/it]Running Inference:  56%|█████▋    | 113/200 [03:46<02:36,  1.80s/it]Running Inference:  57%|█████▋    | 114/200 [03:47<02:26,  1.70s/it]Running Inference:  57%|█████▊    | 115/200 [03:49<02:22,  1.67s/it]Running Inference:  58%|█████▊    | 116/200 [03:50<02:14,  1.60s/it]Running Inference:  58%|█████▊    | 117/200 [03:52<02:09,  1.56s/it]Running Inference:  59%|█████▉    | 118/200 [03:53<02:12,  1.62s/it]Running Inference:  60%|█████▉    | 119/200 [03:55<02:16,  1.69s/it]Running Inference:  60%|██████    | 120/200 [03:57<02:19,  1.74s/it]Running Inference:  60%|██████    | 121/200 [03:59<02:15,  1.71s/it]Running Inference:  61%|██████    | 122/200 [04:01<02:15,  1.73s/it]Running Inference:  62%|██████▏   | 123/200 [04:02<02:11,  1.71s/it]Running Inference:  62%|██████▏   | 124/200 [04:04<02:11,  1.73s/it]Running Inference:  62%|██████▎   | 125/200 [04:06<02:06,  1.69s/it]Running Inference:  63%|██████▎   | 126/200 [04:08<02:14,  1.82s/it]Running Inference:  64%|██████▎   | 127/200 [04:09<02:08,  1.75s/it]Running Inference:  64%|██████▍   | 128/200 [04:11<02:00,  1.68s/it]Running Inference:  64%|██████▍   | 129/200 [04:12<01:51,  1.57s/it]Running Inference:  65%|██████▌   | 130/200 [04:14<01:51,  1.59s/it]Running Inference:  66%|██████▌   | 131/200 [04:15<01:50,  1.60s/it]Running Inference:  66%|██████▌   | 132/200 [04:17<01:54,  1.68s/it]Running Inference:  66%|██████▋   | 133/200 [04:19<01:54,  1.71s/it]Running Inference:  67%|██████▋   | 134/200 [04:21<01:52,  1.71s/it]Running Inference:  68%|██████▊   | 135/200 [04:22<01:48,  1.67s/it]Running Inference:  68%|██████▊   | 136/200 [04:24<01:42,  1.60s/it]Running Inference:  68%|██████▊   | 137/200 [04:25<01:43,  1.64s/it]Running Inference:  69%|██████▉   | 138/200 [04:29<02:20,  2.26s/it]Running Inference:  70%|██████▉   | 139/200 [04:31<02:06,  2.07s/it]Running Inference:  70%|███████   | 140/200 [04:32<01:53,  1.89s/it]Running Inference:  70%|███████   | 141/200 [04:34<01:46,  1.80s/it]Running Inference:  71%|███████   | 142/200 [04:36<01:45,  1.82s/it]Running Inference:  72%|███████▏  | 143/200 [04:40<02:17,  2.42s/it]Running Inference:  72%|███████▏  | 144/200 [04:42<02:08,  2.30s/it]Running Inference:  72%|███████▎  | 145/200 [04:43<01:51,  2.03s/it]Running Inference:  73%|███████▎  | 146/200 [04:45<01:45,  1.96s/it]Running Inference:  74%|███████▎  | 147/200 [04:46<01:34,  1.79s/it]Running Inference:  74%|███████▍  | 148/200 [04:48<01:34,  1.82s/it]Running Inference:  74%|███████▍  | 149/200 [04:50<01:30,  1.77s/it]Running Inference:  75%|███████▌  | 150/200 [04:51<01:24,  1.70s/it]Running Inference:  76%|███████▌  | 151/200 [04:53<01:28,  1.81s/it]Running Inference:  76%|███████▌  | 152/200 [04:55<01:23,  1.74s/it]Running Inference:  76%|███████▋  | 153/200 [04:58<01:48,  2.30s/it]Running Inference:  77%|███████▋  | 154/200 [05:00<01:32,  2.02s/it]Running Inference:  78%|███████▊  | 155/200 [05:02<01:26,  1.92s/it]Running Inference:  78%|███████▊  | 156/200 [05:03<01:24,  1.91s/it]Running Inference:  78%|███████▊  | 157/200 [05:05<01:18,  1.83s/it]Running Inference:  79%|███████▉  | 158/200 [05:07<01:13,  1.75s/it]Running Inference:  80%|███████▉  | 159/200 [05:08<01:08,  1.67s/it]Running Inference:  80%|████████  | 160/200 [05:10<01:03,  1.60s/it]Running Inference:  80%|████████  | 161/200 [05:11<01:01,  1.59s/it]Running Inference:  81%|████████  | 162/200 [05:13<01:01,  1.61s/it]Running Inference:  82%|████████▏ | 163/200 [05:15<01:02,  1.68s/it]Running Inference:  82%|████████▏ | 164/200 [05:16<01:01,  1.70s/it]Running Inference:  82%|████████▎ | 165/200 [05:18<01:02,  1.78s/it]Running Inference:  83%|████████▎ | 166/200 [05:20<01:01,  1.82s/it]Running Inference:  84%|████████▎ | 167/200 [05:22<00:58,  1.77s/it]Running Inference:  84%|████████▍ | 168/200 [05:24<00:56,  1.75s/it]Running Inference:  84%|████████▍ | 169/200 [05:25<00:54,  1.76s/it]Running Inference:  85%|████████▌ | 170/200 [05:28<00:56,  1.89s/it]Running Inference:  86%|████████▌ | 171/200 [05:29<00:54,  1.87s/it]Running Inference:  86%|████████▌ | 172/200 [05:31<00:49,  1.76s/it]Running Inference:  86%|████████▋ | 173/200 [05:33<00:49,  1.82s/it]Running Inference:  87%|████████▋ | 174/200 [05:35<00:47,  1.82s/it]Running Inference:  88%|████████▊ | 175/200 [05:36<00:44,  1.77s/it]Running Inference:  88%|████████▊ | 176/200 [05:38<00:39,  1.66s/it]Running Inference:  88%|████████▊ | 177/200 [05:40<00:38,  1.69s/it]Running Inference:  89%|████████▉ | 178/200 [05:41<00:39,  1.78s/it]Running Inference:  90%|████████▉ | 179/200 [05:45<00:48,  2.31s/it]Running Inference:  90%|█████████ | 180/200 [05:47<00:42,  2.10s/it]Running Inference:  90%|█████████ | 181/200 [05:50<00:48,  2.55s/it]Running Inference:  91%|█████████ | 182/200 [05:54<00:52,  2.89s/it]Running Inference:  92%|█████████▏| 183/200 [05:56<00:43,  2.53s/it]Running Inference:  92%|█████████▏| 184/200 [05:57<00:36,  2.31s/it]Running Inference:  92%|█████████▎| 185/200 [05:59<00:32,  2.14s/it]Running Inference:  93%|█████████▎| 186/200 [06:01<00:28,  2.00s/it]Running Inference:  94%|█████████▎| 187/200 [06:02<00:24,  1.90s/it]Running Inference:  94%|█████████▍| 188/200 [06:04<00:23,  1.93s/it]Running Inference:  94%|█████████▍| 189/200 [06:08<00:27,  2.46s/it]Running Inference:  95%|█████████▌| 190/200 [06:10<00:21,  2.16s/it]Running Inference:  96%|█████████▌| 191/200 [06:11<00:17,  2.00s/it]Running Inference:  96%|█████████▌| 192/200 [06:13<00:15,  1.91s/it]Running Inference:  96%|█████████▋| 193/200 [06:15<00:12,  1.85s/it]Running Inference:  97%|█████████▋| 194/200 [06:17<00:11,  1.84s/it]Running Inference:  98%|█████████▊| 195/200 [06:20<00:12,  2.44s/it]Running Inference:  98%|█████████▊| 196/200 [06:22<00:08,  2.23s/it]Running Inference:  98%|█████████▊| 197/200 [06:24<00:06,  2.09s/it]Running Inference:  99%|█████████▉| 198/200 [06:26<00:03,  1.96s/it]Running Inference: 100%|█████████▉| 199/200 [06:28<00:01,  1.98s/it]Running Inference: 100%|██████████| 200/200 [06:29<00:00,  1.84s/it]Running Inference: 100%|██████████| 200/200 [06:29<00:00,  1.95s/it]
2025-12-14 18:10:19,079 - INFO - Inference completed.
2025-12-14 18:10:19,091 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 18:10:19,091 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:10:19,092 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 18:10:19,092 - INFO - Metrics:
29.5
2025-12-14 18:10:19,093 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_retrieval_en, ratio=0.1) on GPU 4

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:10:25,639 - INFO - Set deterministic seeds to 42
2025-12-14 18:10:25,639 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:10:25,639 - INFO - Starting evaluation run...
2025-12-14 18:10:25,639 - INFO - Output directory set to: longbenchresult
2025-12-14 18:10:25,640 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 18:10:25,640 - INFO - KV Press 'knorm' setup.
2025-12-14 18:10:25,640 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:10:25,640 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.54it/s]
Device set to use cuda:0
2025-12-14 18:10:41,679 - INFO - Model pipeline loaded.
2025-12-14 18:10:41,680 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 18:10:49,798 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:10:49,798 - INFO - Dataset processed with 200 entries.
2025-12-14 18:10:49,830 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<08:01,  2.42s/it]Running Inference:   1%|          | 2/200 [00:03<06:10,  1.87s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:18,  1.92s/it]Running Inference:   2%|▏         | 4/200 [00:07<05:54,  1.81s/it]Running Inference:   2%|▎         | 5/200 [00:09<05:51,  1.80s/it]Running Inference:   3%|▎         | 6/200 [00:10<05:22,  1.66s/it]Running Inference:   4%|▎         | 7/200 [00:12<05:51,  1.82s/it]Running Inference:   4%|▍         | 8/200 [00:14<05:40,  1.77s/it]Running Inference:   4%|▍         | 9/200 [00:16<05:28,  1.72s/it]Running Inference:   5%|▌         | 10/200 [00:17<05:08,  1.63s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:19<04:57,  1.58s/it]Running Inference:   6%|▌         | 12/200 [00:20<05:14,  1.67s/it]Running Inference:   6%|▋         | 13/200 [00:22<05:09,  1.66s/it]Running Inference:   7%|▋         | 14/200 [00:24<05:05,  1.64s/it]Running Inference:   8%|▊         | 15/200 [00:25<05:10,  1.68s/it]Running Inference:   8%|▊         | 16/200 [00:27<05:05,  1.66s/it]Running Inference:   8%|▊         | 17/200 [00:29<05:02,  1.66s/it]Running Inference:   9%|▉         | 18/200 [00:30<04:55,  1.62s/it]Running Inference:  10%|▉         | 19/200 [00:32<04:55,  1.63s/it]Running Inference:  10%|█         | 20/200 [00:34<04:58,  1.66s/it]Running Inference:  10%|█         | 21/200 [00:35<05:06,  1.71s/it]Running Inference:  11%|█         | 22/200 [00:37<04:58,  1.67s/it]Running Inference:  12%|█▏        | 23/200 [00:39<04:59,  1.69s/it]Running Inference:  12%|█▏        | 24/200 [00:40<04:52,  1.66s/it]Running Inference:  12%|█▎        | 25/200 [00:42<04:34,  1.57s/it]Running Inference:  13%|█▎        | 26/200 [00:43<04:38,  1.60s/it]Running Inference:  14%|█▎        | 27/200 [00:45<05:00,  1.74s/it]Running Inference:  14%|█▍        | 28/200 [00:49<06:25,  2.24s/it]Running Inference:  14%|█▍        | 29/200 [00:50<05:40,  1.99s/it]Running Inference:  15%|█▌        | 30/200 [00:52<05:17,  1.87s/it]Running Inference:  16%|█▌        | 31/200 [00:56<07:00,  2.49s/it]Running Inference:  16%|█▌        | 32/200 [00:58<06:25,  2.29s/it]Running Inference:  16%|█▋        | 33/200 [00:59<05:53,  2.11s/it]Running Inference:  17%|█▋        | 34/200 [01:02<05:59,  2.17s/it]Running Inference:  18%|█▊        | 35/200 [01:03<05:35,  2.03s/it]Running Inference:  18%|█▊        | 36/200 [01:05<05:00,  1.83s/it]Running Inference:  18%|█▊        | 37/200 [01:06<04:52,  1.79s/it]Running Inference:  19%|█▉        | 38/200 [01:08<04:47,  1.77s/it]Running Inference:  20%|█▉        | 39/200 [01:10<04:31,  1.69s/it]Running Inference:  20%|██        | 40/200 [01:11<04:28,  1.68s/it]Running Inference:  20%|██        | 41/200 [01:13<04:34,  1.73s/it]Running Inference:  21%|██        | 42/200 [01:14<04:16,  1.62s/it]Running Inference:  22%|██▏       | 43/200 [01:16<04:33,  1.74s/it]Running Inference:  22%|██▏       | 44/200 [01:18<04:35,  1.76s/it]Running Inference:  22%|██▎       | 45/200 [01:20<04:21,  1.68s/it]Running Inference:  23%|██▎       | 46/200 [01:22<04:27,  1.74s/it]Running Inference:  24%|██▎       | 47/200 [01:23<04:15,  1.67s/it]Running Inference:  24%|██▍       | 48/200 [01:27<05:38,  2.22s/it]Running Inference:  24%|██▍       | 49/200 [01:29<05:20,  2.13s/it]Running Inference:  25%|██▌       | 50/200 [01:30<04:55,  1.97s/it]Running Inference:  26%|██▌       | 51/200 [01:32<04:43,  1.90s/it]Running Inference:  26%|██▌       | 52/200 [01:34<04:45,  1.93s/it]Running Inference:  26%|██▋       | 53/200 [01:36<04:41,  1.91s/it]Running Inference:  27%|██▋       | 54/200 [01:38<04:34,  1.88s/it]Running Inference:  28%|██▊       | 55/200 [01:39<04:12,  1.74s/it]Running Inference:  28%|██▊       | 56/200 [01:43<05:39,  2.35s/it]Running Inference:  28%|██▊       | 57/200 [01:44<04:56,  2.07s/it]Running Inference:  29%|██▉       | 58/200 [01:46<04:43,  2.00s/it]Running Inference:  30%|██▉       | 59/200 [01:48<04:36,  1.96s/it]Running Inference:  30%|███       | 60/200 [01:50<04:21,  1.87s/it]Running Inference:  30%|███       | 61/200 [01:51<04:05,  1.76s/it]Running Inference:  31%|███       | 62/200 [01:53<04:23,  1.91s/it]Running Inference:  32%|███▏      | 63/200 [01:55<04:08,  1.81s/it]Running Inference:  32%|███▏      | 64/200 [01:57<03:57,  1.74s/it]Running Inference:  32%|███▎      | 65/200 [01:58<03:46,  1.68s/it]Running Inference:  33%|███▎      | 66/200 [02:00<03:56,  1.77s/it]Running Inference:  34%|███▎      | 67/200 [02:01<03:40,  1.66s/it]Running Inference:  34%|███▍      | 68/200 [02:03<03:47,  1.72s/it]Running Inference:  34%|███▍      | 69/200 [02:05<03:51,  1.77s/it]Running Inference:  35%|███▌      | 70/200 [02:07<03:53,  1.80s/it]Running Inference:  36%|███▌      | 71/200 [02:09<03:55,  1.83s/it]Running Inference:  36%|███▌      | 72/200 [02:11<03:55,  1.84s/it]Running Inference:  36%|███▋      | 73/200 [02:12<03:41,  1.75s/it]Running Inference:  37%|███▋      | 74/200 [02:14<03:52,  1.85s/it]Running Inference:  38%|███▊      | 75/200 [02:16<03:38,  1.75s/it]Running Inference:  38%|███▊      | 76/200 [02:17<03:20,  1.62s/it]Running Inference:  38%|███▊      | 77/200 [02:19<03:21,  1.64s/it]Running Inference:  39%|███▉      | 78/200 [02:21<03:22,  1.66s/it]Running Inference:  40%|███▉      | 79/200 [02:22<03:20,  1.66s/it]Running Inference:  40%|████      | 80/200 [02:24<03:16,  1.64s/it]Running Inference:  40%|████      | 81/200 [02:25<03:10,  1.60s/it]Running Inference:  41%|████      | 82/200 [02:27<03:16,  1.66s/it]Running Inference:  42%|████▏     | 83/200 [02:29<03:14,  1.66s/it]Running Inference:  42%|████▏     | 84/200 [02:30<03:08,  1.63s/it]Running Inference:  42%|████▎     | 85/200 [02:34<04:16,  2.23s/it]Running Inference:  43%|████▎     | 86/200 [02:38<05:07,  2.69s/it]Running Inference:  44%|████▎     | 87/200 [02:40<04:31,  2.40s/it]Running Inference:  44%|████▍     | 88/200 [02:41<03:58,  2.13s/it]Running Inference:  44%|████▍     | 89/200 [02:43<03:34,  1.94s/it]Running Inference:  45%|████▌     | 90/200 [02:44<03:19,  1.81s/it]Running Inference:  46%|████▌     | 91/200 [02:48<04:18,  2.37s/it]Running Inference:  46%|████▌     | 92/200 [02:49<03:57,  2.19s/it]Running Inference:  46%|████▋     | 93/200 [02:51<03:32,  1.99s/it]Running Inference:  47%|████▋     | 94/200 [02:53<03:35,  2.04s/it]Running Inference:  48%|████▊     | 95/200 [02:55<03:19,  1.90s/it]Running Inference:  48%|████▊     | 96/200 [02:56<03:06,  1.80s/it]Running Inference:  48%|████▊     | 97/200 [02:58<03:00,  1.75s/it]Running Inference:  49%|████▉     | 98/200 [03:00<03:01,  1.78s/it]Running Inference:  50%|████▉     | 99/200 [03:01<02:53,  1.71s/it]Running Inference:  50%|█████     | 100/200 [03:03<02:49,  1.69s/it]Running Inference:  50%|█████     | 101/200 [03:04<02:41,  1.63s/it]Running Inference:  51%|█████     | 102/200 [03:06<02:39,  1.62s/it]Running Inference:  52%|█████▏    | 103/200 [03:08<02:35,  1.60s/it]Running Inference:  52%|█████▏    | 104/200 [03:09<02:32,  1.59s/it]Running Inference:  52%|█████▎    | 105/200 [03:11<02:37,  1.66s/it]Running Inference:  53%|█████▎    | 106/200 [03:13<02:35,  1.65s/it]Running Inference:  54%|█████▎    | 107/200 [03:14<02:27,  1.58s/it]Running Inference:  54%|█████▍    | 108/200 [03:16<02:37,  1.71s/it]Running Inference:  55%|█████▍    | 109/200 [03:18<02:40,  1.77s/it]Running Inference:  55%|█████▌    | 110/200 [03:20<02:41,  1.80s/it]Running Inference:  56%|█████▌    | 111/200 [03:21<02:31,  1.70s/it]Running Inference:  56%|█████▌    | 112/200 [03:23<02:26,  1.66s/it]Running Inference:  56%|█████▋    | 113/200 [03:25<02:25,  1.68s/it]Running Inference:  57%|█████▋    | 114/200 [03:26<02:19,  1.62s/it]Running Inference:  57%|█████▊    | 115/200 [03:28<02:17,  1.61s/it]Running Inference:  58%|█████▊    | 116/200 [03:29<02:10,  1.56s/it]Running Inference:  58%|█████▊    | 117/200 [03:31<02:07,  1.53s/it]Running Inference:  59%|█████▉    | 118/200 [03:32<02:10,  1.60s/it]Running Inference:  60%|█████▉    | 119/200 [03:34<02:15,  1.67s/it]Running Inference:  60%|██████    | 120/200 [03:36<02:18,  1.73s/it]Running Inference:  60%|██████    | 121/200 [03:38<02:14,  1.70s/it]Running Inference:  61%|██████    | 122/200 [03:39<02:14,  1.73s/it]Running Inference:  62%|██████▏   | 123/200 [03:41<02:11,  1.71s/it]Running Inference:  62%|██████▏   | 124/200 [03:43<02:11,  1.72s/it]Running Inference:  62%|██████▎   | 125/200 [03:45<02:06,  1.69s/it]Running Inference:  63%|██████▎   | 126/200 [03:48<02:48,  2.28s/it]Running Inference:  64%|██████▎   | 127/200 [03:50<02:31,  2.07s/it]Running Inference:  64%|██████▍   | 128/200 [03:51<02:16,  1.90s/it]Running Inference:  64%|██████▍   | 129/200 [03:55<02:46,  2.35s/it]Running Inference:  65%|██████▌   | 130/200 [03:56<02:29,  2.14s/it]Running Inference:  66%|██████▌   | 131/200 [03:58<02:16,  1.98s/it]Running Inference:  66%|██████▌   | 132/200 [04:00<02:12,  1.95s/it]Running Inference:  66%|██████▋   | 133/200 [04:02<02:07,  1.90s/it]Running Inference:  67%|██████▋   | 134/200 [04:03<02:01,  1.84s/it]Running Inference:  68%|██████▊   | 135/200 [04:05<01:54,  1.76s/it]Running Inference:  68%|██████▊   | 136/200 [04:06<01:45,  1.65s/it]Running Inference:  68%|██████▊   | 137/200 [04:08<01:45,  1.68s/it]Running Inference:  69%|██████▉   | 138/200 [04:10<01:43,  1.67s/it]Running Inference:  70%|██████▉   | 139/200 [04:11<01:41,  1.66s/it]Running Inference:  70%|███████   | 140/200 [04:13<01:36,  1.61s/it]Running Inference:  70%|███████   | 141/200 [04:14<01:34,  1.60s/it]Running Inference:  71%|███████   | 142/200 [04:16<01:37,  1.68s/it]Running Inference:  72%|███████▏  | 143/200 [04:18<01:37,  1.70s/it]Running Inference:  72%|███████▏  | 144/200 [04:22<02:15,  2.42s/it]Running Inference:  72%|███████▎  | 145/200 [04:23<01:56,  2.11s/it]Running Inference:  73%|███████▎  | 146/200 [04:25<01:48,  2.02s/it]Running Inference:  74%|███████▎  | 147/200 [04:27<01:36,  1.83s/it]Running Inference:  74%|███████▍  | 148/200 [04:28<01:35,  1.84s/it]Running Inference:  74%|███████▍  | 149/200 [04:30<01:30,  1.78s/it]Running Inference:  75%|███████▌  | 150/200 [04:32<01:25,  1.71s/it]Running Inference:  76%|███████▌  | 151/200 [04:34<01:29,  1.82s/it]Running Inference:  76%|███████▌  | 152/200 [04:37<01:53,  2.37s/it]Running Inference:  76%|███████▋  | 153/200 [04:39<01:39,  2.12s/it]Running Inference:  77%|███████▋  | 154/200 [04:40<01:27,  1.90s/it]Running Inference:  78%|███████▊  | 155/200 [04:42<01:22,  1.83s/it]Running Inference:  78%|███████▊  | 156/200 [04:44<01:21,  1.85s/it]Running Inference:  78%|███████▊  | 157/200 [04:46<01:17,  1.79s/it]Running Inference:  79%|███████▉  | 158/200 [04:47<01:12,  1.72s/it]Running Inference:  80%|███████▉  | 159/200 [04:49<01:07,  1.65s/it]Running Inference:  80%|████████  | 160/200 [04:50<01:03,  1.58s/it]Running Inference:  80%|████████  | 161/200 [04:52<01:01,  1.57s/it]Running Inference:  81%|████████  | 162/200 [04:53<01:00,  1.60s/it]Running Inference:  82%|████████▏ | 163/200 [04:55<01:01,  1.67s/it]Running Inference:  82%|████████▏ | 164/200 [04:57<01:00,  1.69s/it]Running Inference:  82%|████████▎ | 165/200 [04:59<01:02,  1.78s/it]Running Inference:  83%|████████▎ | 166/200 [05:01<01:01,  1.81s/it]Running Inference:  84%|████████▎ | 167/200 [05:02<00:58,  1.77s/it]Running Inference:  84%|████████▍ | 168/200 [05:04<00:55,  1.75s/it]Running Inference:  84%|████████▍ | 169/200 [05:06<00:54,  1.75s/it]Running Inference:  85%|████████▌ | 170/200 [05:08<00:56,  1.89s/it]Running Inference:  86%|████████▌ | 171/200 [05:10<00:54,  1.87s/it]Running Inference:  86%|████████▌ | 172/200 [05:11<00:49,  1.76s/it]Running Inference:  86%|████████▋ | 173/200 [05:13<00:49,  1.82s/it]Running Inference:  87%|████████▋ | 174/200 [05:15<00:47,  1.81s/it]Running Inference:  88%|████████▊ | 175/200 [05:19<00:59,  2.38s/it]Running Inference:  88%|████████▊ | 176/200 [05:20<00:50,  2.09s/it]Running Inference:  88%|████████▊ | 177/200 [05:22<00:45,  1.99s/it]Running Inference:  89%|████████▉ | 178/200 [05:24<00:43,  1.98s/it]Running Inference:  90%|████████▉ | 179/200 [05:25<00:38,  1.83s/it]Running Inference:  90%|█████████ | 180/200 [05:27<00:35,  1.77s/it]Running Inference:  90%|█████████ | 181/200 [05:31<00:43,  2.31s/it]Running Inference:  91%|█████████ | 182/200 [05:32<00:37,  2.10s/it]Running Inference:  92%|█████████▏| 183/200 [05:34<00:33,  1.98s/it]Running Inference:  92%|█████████▏| 184/200 [05:36<00:30,  1.93s/it]Running Inference:  92%|█████████▎| 185/200 [05:37<00:28,  1.87s/it]Running Inference:  93%|█████████▎| 186/200 [05:39<00:25,  1.81s/it]Running Inference:  94%|█████████▎| 187/200 [05:41<00:22,  1.76s/it]Running Inference:  94%|█████████▍| 188/200 [05:43<00:21,  1.83s/it]Running Inference:  94%|█████████▍| 189/200 [05:46<00:26,  2.39s/it]Running Inference:  95%|█████████▌| 190/200 [05:48<00:21,  2.10s/it]Running Inference:  96%|█████████▌| 191/200 [05:50<00:17,  1.96s/it]Running Inference:  96%|█████████▌| 192/200 [05:51<00:15,  1.88s/it]Running Inference:  96%|█████████▋| 193/200 [05:53<00:12,  1.83s/it]Running Inference:  97%|█████████▋| 194/200 [05:55<00:10,  1.82s/it]Running Inference:  98%|█████████▊| 195/200 [05:57<00:09,  1.81s/it]Running Inference:  98%|█████████▊| 196/200 [05:58<00:07,  1.79s/it]Running Inference:  98%|█████████▊| 197/200 [06:00<00:05,  1.82s/it]Running Inference:  99%|█████████▉| 198/200 [06:02<00:03,  1.77s/it]Running Inference: 100%|█████████▉| 199/200 [06:04<00:01,  1.85s/it]Running Inference: 100%|██████████| 200/200 [06:05<00:00,  1.74s/it]Running Inference: 100%|██████████| 200/200 [06:05<00:00,  1.83s/it]
2025-12-14 18:16:55,690 - INFO - Inference completed.
2025-12-14 18:16:55,701 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 18:16:55,701 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:16:55,702 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 18:16:55,702 - INFO - Metrics:
37.0
2025-12-14 18:16:55,704 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_retrieval_en, ratio=0.2) on GPU 4

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:17:02,200 - INFO - Set deterministic seeds to 42
2025-12-14 18:17:02,200 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:17:02,200 - INFO - Starting evaluation run...
2025-12-14 18:17:02,200 - INFO - Output directory set to: longbenchresult
2025-12-14 18:17:02,200 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 18:17:02,200 - INFO - KV Press 'knorm' setup.
2025-12-14 18:17:02,200 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:17:02,200 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.84it/s]
Device set to use cuda:0
2025-12-14 18:17:15,392 - INFO - Model pipeline loaded.
2025-12-14 18:17:15,392 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 18:17:23,387 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:17:23,387 - INFO - Dataset processed with 200 entries.
2025-12-14 18:17:23,420 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:44,  2.33s/it]Running Inference:   1%|          | 2/200 [00:03<06:02,  1.83s/it]Running Inference:   2%|▏         | 3/200 [00:07<09:18,  2.83s/it]Running Inference:   2%|▏         | 4/200 [00:09<07:43,  2.36s/it]Running Inference:   2%|▎         | 5/200 [00:11<07:00,  2.16s/it]Running Inference:   3%|▎         | 6/200 [00:12<06:08,  1.90s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:21,  1.98s/it]Running Inference:   4%|▍         | 8/200 [00:16<06:00,  1.88s/it]Running Inference:   4%|▍         | 9/200 [00:18<05:42,  1.79s/it]Running Inference:   5%|▌         | 10/200 [00:19<05:18,  1.68s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:04,  1.61s/it]Running Inference:   6%|▌         | 12/200 [00:22<05:18,  1.70s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:12,  1.67s/it]Running Inference:   7%|▋         | 14/200 [00:26<05:07,  1.65s/it]Running Inference:   8%|▊         | 15/200 [00:27<05:13,  1.70s/it]Running Inference:   8%|▊         | 16/200 [00:29<05:07,  1.67s/it]Running Inference:   8%|▊         | 17/200 [00:31<05:04,  1.66s/it]Running Inference:   9%|▉         | 18/200 [00:32<04:55,  1.63s/it]Running Inference:  10%|▉         | 19/200 [00:34<04:55,  1.63s/it]Running Inference:  10%|█         | 20/200 [00:36<04:58,  1.66s/it]Running Inference:  10%|█         | 21/200 [00:37<05:06,  1.71s/it]Running Inference:  11%|█         | 22/200 [00:39<04:57,  1.67s/it]Running Inference:  12%|█▏        | 23/200 [00:41<04:59,  1.69s/it]Running Inference:  12%|█▏        | 24/200 [00:42<04:52,  1.66s/it]Running Inference:  12%|█▎        | 25/200 [00:44<04:33,  1.57s/it]Running Inference:  13%|█▎        | 26/200 [00:46<04:58,  1.72s/it]Running Inference:  14%|█▎        | 27/200 [00:48<05:14,  1.82s/it]Running Inference:  14%|█▍        | 28/200 [00:51<06:36,  2.30s/it]Running Inference:  14%|█▍        | 29/200 [00:53<05:48,  2.04s/it]Running Inference:  15%|█▌        | 30/200 [00:56<07:09,  2.52s/it]Running Inference:  16%|█▌        | 31/200 [00:58<06:32,  2.32s/it]Running Inference:  16%|█▌        | 32/200 [01:00<06:05,  2.18s/it]Running Inference:  16%|█▋        | 33/200 [01:02<05:39,  2.03s/it]Running Inference:  17%|█▋        | 34/200 [01:04<05:41,  2.05s/it]Running Inference:  18%|█▊        | 35/200 [01:05<05:22,  1.96s/it]Running Inference:  18%|█▊        | 36/200 [01:07<04:50,  1.77s/it]Running Inference:  18%|█▊        | 37/200 [01:09<04:45,  1.75s/it]Running Inference:  19%|█▉        | 38/200 [01:10<04:43,  1.75s/it]Running Inference:  20%|█▉        | 39/200 [01:12<04:28,  1.67s/it]Running Inference:  20%|██        | 40/200 [01:13<04:26,  1.66s/it]Running Inference:  20%|██        | 41/200 [01:15<04:33,  1.72s/it]Running Inference:  21%|██        | 42/200 [01:17<04:16,  1.63s/it]Running Inference:  22%|██▏       | 43/200 [01:19<04:33,  1.74s/it]Running Inference:  22%|██▏       | 44/200 [01:20<04:35,  1.77s/it]Running Inference:  22%|██▎       | 45/200 [01:22<04:21,  1.68s/it]Running Inference:  23%|██▎       | 46/200 [01:24<04:29,  1.75s/it]Running Inference:  24%|██▎       | 47/200 [01:25<04:17,  1.68s/it]Running Inference:  24%|██▍       | 48/200 [01:27<04:04,  1.61s/it]Running Inference:  24%|██▍       | 49/200 [01:29<04:13,  1.68s/it]Running Inference:  25%|██▌       | 50/200 [01:32<05:43,  2.29s/it]Running Inference:  26%|██▌       | 51/200 [01:34<05:13,  2.11s/it]Running Inference:  26%|██▌       | 52/200 [01:36<05:03,  2.05s/it]Running Inference:  26%|██▋       | 53/200 [01:38<04:53,  2.00s/it]Running Inference:  27%|██▋       | 54/200 [01:40<04:48,  1.97s/it]Running Inference:  28%|██▊       | 55/200 [01:41<04:21,  1.81s/it]Running Inference:  28%|██▊       | 56/200 [01:43<04:16,  1.78s/it]Running Inference:  28%|██▊       | 57/200 [01:45<04:11,  1.76s/it]Running Inference:  29%|██▉       | 58/200 [01:46<04:12,  1.78s/it]Running Inference:  30%|██▉       | 59/200 [01:48<04:14,  1.81s/it]Running Inference:  30%|███       | 60/200 [01:50<04:05,  1.76s/it]Running Inference:  30%|███       | 61/200 [01:51<03:53,  1.68s/it]Running Inference:  31%|███       | 62/200 [01:53<03:59,  1.74s/it]Running Inference:  32%|███▏      | 63/200 [01:55<03:51,  1.69s/it]Running Inference:  32%|███▏      | 64/200 [01:57<03:45,  1.66s/it]Running Inference:  32%|███▎      | 65/200 [01:58<03:38,  1.62s/it]Running Inference:  33%|███▎      | 66/200 [02:00<03:50,  1.72s/it]Running Inference:  34%|███▎      | 67/200 [02:01<03:36,  1.63s/it]Running Inference:  34%|███▍      | 68/200 [02:03<03:44,  1.70s/it]Running Inference:  34%|███▍      | 69/200 [02:05<03:49,  1.75s/it]Running Inference:  35%|███▌      | 70/200 [02:07<03:52,  1.79s/it]Running Inference:  36%|███▌      | 71/200 [02:09<03:54,  1.82s/it]Running Inference:  36%|███▌      | 72/200 [02:11<03:55,  1.84s/it]Running Inference:  36%|███▋      | 73/200 [02:12<03:41,  1.74s/it]Running Inference:  37%|███▋      | 74/200 [02:14<03:37,  1.72s/it]Running Inference:  38%|███▊      | 75/200 [02:16<03:27,  1.66s/it]Running Inference:  38%|███▊      | 76/200 [02:17<03:13,  1.56s/it]Running Inference:  38%|███▊      | 77/200 [02:19<03:15,  1.59s/it]Running Inference:  39%|███▉      | 78/200 [02:20<03:18,  1.62s/it]Running Inference:  40%|███▉      | 79/200 [02:22<03:17,  1.63s/it]Running Inference:  40%|████      | 80/200 [02:23<03:14,  1.62s/it]Running Inference:  40%|████      | 81/200 [02:25<03:08,  1.58s/it]Running Inference:  41%|████      | 82/200 [02:27<03:14,  1.65s/it]Running Inference:  42%|████▏     | 83/200 [02:28<03:13,  1.65s/it]Running Inference:  42%|████▏     | 84/200 [02:30<03:07,  1.62s/it]Running Inference:  42%|████▎     | 85/200 [02:34<04:16,  2.23s/it]Running Inference:  43%|████▎     | 86/200 [02:37<05:08,  2.71s/it]Running Inference:  44%|████▎     | 87/200 [02:39<04:31,  2.41s/it]Running Inference:  44%|████▍     | 88/200 [02:41<03:59,  2.13s/it]Running Inference:  44%|████▍     | 89/200 [02:42<03:35,  1.94s/it]Running Inference:  45%|████▌     | 90/200 [02:44<03:19,  1.81s/it]Running Inference:  46%|████▌     | 91/200 [02:45<03:11,  1.76s/it]Running Inference:  46%|████▌     | 92/200 [02:47<03:10,  1.76s/it]Running Inference:  46%|████▋     | 93/200 [02:49<03:00,  1.68s/it]Running Inference:  47%|████▋     | 94/200 [02:50<03:02,  1.72s/it]Running Inference:  48%|████▊     | 95/200 [02:52<02:56,  1.68s/it]Running Inference:  48%|████▊     | 96/200 [02:54<02:51,  1.64s/it]Running Inference:  48%|████▊     | 97/200 [02:57<03:53,  2.27s/it]Running Inference:  49%|████▉     | 98/200 [02:59<03:38,  2.14s/it]Running Inference:  50%|████▉     | 99/200 [03:01<03:18,  1.96s/it]Running Inference:  50%|█████     | 100/200 [03:02<03:06,  1.87s/it]Running Inference:  50%|█████     | 101/200 [03:04<02:53,  1.75s/it]Running Inference:  51%|█████     | 102/200 [03:05<02:47,  1.71s/it]Running Inference:  52%|█████▏    | 103/200 [03:07<02:41,  1.66s/it]Running Inference:  52%|█████▏    | 104/200 [03:08<02:36,  1.63s/it]Running Inference:  52%|█████▎    | 105/200 [03:10<02:40,  1.69s/it]Running Inference:  53%|█████▎    | 106/200 [03:12<02:36,  1.67s/it]Running Inference:  54%|█████▎    | 107/200 [03:13<02:28,  1.60s/it]Running Inference:  54%|█████▍    | 108/200 [03:15<02:37,  1.71s/it]Running Inference:  55%|█████▍    | 109/200 [03:17<02:40,  1.77s/it]Running Inference:  55%|█████▌    | 110/200 [03:19<02:41,  1.80s/it]Running Inference:  56%|█████▌    | 111/200 [03:21<02:31,  1.70s/it]Running Inference:  56%|█████▌    | 112/200 [03:24<03:21,  2.29s/it]Running Inference:  56%|█████▋    | 113/200 [03:26<03:04,  2.12s/it]Running Inference:  57%|█████▋    | 114/200 [03:27<02:45,  1.93s/it]Running Inference:  57%|█████▊    | 115/200 [03:29<02:35,  1.83s/it]Running Inference:  58%|█████▊    | 116/200 [03:30<02:23,  1.71s/it]Running Inference:  58%|█████▊    | 117/200 [03:32<02:15,  1.64s/it]Running Inference:  59%|█████▉    | 118/200 [03:34<02:16,  1.66s/it]Running Inference:  60%|█████▉    | 119/200 [03:35<02:19,  1.72s/it]Running Inference:  60%|██████    | 120/200 [03:37<02:21,  1.76s/it]Running Inference:  60%|██████    | 121/200 [03:41<03:05,  2.35s/it]Running Inference:  61%|██████    | 122/200 [03:43<02:49,  2.18s/it]Running Inference:  62%|██████▏   | 123/200 [03:44<02:35,  2.02s/it]Running Inference:  62%|██████▏   | 124/200 [03:46<02:26,  1.93s/it]Running Inference:  62%|██████▎   | 125/200 [03:48<02:17,  1.83s/it]Running Inference:  63%|██████▎   | 126/200 [03:51<02:56,  2.39s/it]Running Inference:  64%|██████▎   | 127/200 [03:53<02:36,  2.15s/it]Running Inference:  64%|██████▍   | 128/200 [03:55<02:20,  1.95s/it]Running Inference:  64%|██████▍   | 129/200 [03:56<02:05,  1.76s/it]Running Inference:  65%|██████▌   | 130/200 [03:58<02:00,  1.72s/it]Running Inference:  66%|██████▌   | 131/200 [03:59<01:56,  1.69s/it]Running Inference:  66%|██████▌   | 132/200 [04:01<01:58,  1.74s/it]Running Inference:  66%|██████▋   | 133/200 [04:03<01:57,  1.75s/it]Running Inference:  67%|██████▋   | 134/200 [04:04<01:54,  1.73s/it]Running Inference:  68%|██████▊   | 135/200 [04:06<01:49,  1.69s/it]Running Inference:  68%|██████▊   | 136/200 [04:07<01:42,  1.60s/it]Running Inference:  68%|██████▊   | 137/200 [04:09<01:43,  1.64s/it]Running Inference:  69%|██████▉   | 138/200 [04:11<01:41,  1.64s/it]Running Inference:  70%|██████▉   | 139/200 [04:12<01:40,  1.64s/it]Running Inference:  70%|███████   | 140/200 [04:14<01:35,  1.59s/it]Running Inference:  70%|███████   | 141/200 [04:16<01:33,  1.59s/it]Running Inference:  71%|███████   | 142/200 [04:17<01:30,  1.57s/it]Running Inference:  72%|███████▏  | 143/200 [04:19<01:32,  1.62s/it]Running Inference:  72%|███████▏  | 144/200 [04:21<01:37,  1.74s/it]Running Inference:  72%|███████▎  | 145/200 [04:22<01:30,  1.64s/it]Running Inference:  73%|███████▎  | 146/200 [04:24<01:30,  1.68s/it]Running Inference:  74%|███████▎  | 147/200 [04:25<01:24,  1.59s/it]Running Inference:  74%|███████▍  | 148/200 [04:27<01:27,  1.68s/it]Running Inference:  74%|███████▍  | 149/200 [04:29<01:24,  1.66s/it]Running Inference:  75%|███████▌  | 150/200 [04:32<01:52,  2.24s/it]Running Inference:  76%|███████▌  | 151/200 [04:35<01:47,  2.20s/it]Running Inference:  76%|███████▌  | 152/200 [04:36<01:36,  2.01s/it]Running Inference:  76%|███████▋  | 153/200 [04:38<01:27,  1.87s/it]Running Inference:  77%|███████▋  | 154/200 [04:39<01:18,  1.72s/it]Running Inference:  78%|███████▊  | 155/200 [04:41<01:16,  1.71s/it]Running Inference:  78%|███████▊  | 156/200 [04:43<01:17,  1.76s/it]Running Inference:  78%|███████▊  | 157/200 [04:44<01:14,  1.73s/it]Running Inference:  79%|███████▉  | 158/200 [04:46<01:10,  1.67s/it]Running Inference:  80%|███████▉  | 159/200 [04:47<01:06,  1.62s/it]Running Inference:  80%|████████  | 160/200 [04:49<01:02,  1.56s/it]Running Inference:  80%|████████  | 161/200 [04:50<01:00,  1.56s/it]Running Inference:  81%|████████  | 162/200 [04:52<01:00,  1.59s/it]Running Inference:  82%|████████▏ | 163/200 [04:54<01:01,  1.66s/it]Running Inference:  82%|████████▏ | 164/200 [04:55<01:00,  1.68s/it]Running Inference:  82%|████████▎ | 165/200 [04:57<00:58,  1.67s/it]Running Inference:  83%|████████▎ | 166/200 [04:59<00:59,  1.74s/it]Running Inference:  84%|████████▎ | 167/200 [05:01<00:56,  1.71s/it]Running Inference:  84%|████████▍ | 168/200 [05:02<00:54,  1.71s/it]Running Inference:  84%|████████▍ | 169/200 [05:04<00:53,  1.73s/it]Running Inference:  85%|████████▌ | 170/200 [05:06<00:52,  1.76s/it]Running Inference:  86%|████████▌ | 171/200 [05:08<00:51,  1.78s/it]Running Inference:  86%|████████▌ | 172/200 [05:09<00:47,  1.69s/it]Running Inference:  86%|████████▋ | 173/200 [05:11<00:47,  1.77s/it]Running Inference:  87%|████████▋ | 174/200 [05:13<00:46,  1.78s/it]Running Inference:  88%|████████▊ | 175/200 [05:15<00:43,  1.74s/it]Running Inference:  88%|████████▊ | 176/200 [05:16<00:39,  1.64s/it]Running Inference:  88%|████████▊ | 177/200 [05:20<00:53,  2.31s/it]Running Inference:  89%|████████▉ | 178/200 [05:22<00:48,  2.21s/it]Running Inference:  90%|████████▉ | 179/200 [05:23<00:41,  1.99s/it]Running Inference:  90%|█████████ | 180/200 [05:25<00:37,  1.88s/it]Running Inference:  90%|█████████ | 181/200 [05:27<00:33,  1.77s/it]Running Inference:  91%|█████████ | 182/200 [05:28<00:31,  1.72s/it]Running Inference:  92%|█████████▏| 183/200 [05:30<00:29,  1.71s/it]Running Inference:  92%|█████████▏| 184/200 [05:32<00:27,  1.74s/it]Running Inference:  92%|█████████▎| 185/200 [05:33<00:26,  1.74s/it]Running Inference:  93%|█████████▎| 186/200 [05:35<00:24,  1.72s/it]Running Inference:  94%|█████████▎| 187/200 [05:37<00:22,  1.70s/it]Running Inference:  94%|█████████▍| 188/200 [05:39<00:21,  1.78s/it]Running Inference:  94%|█████████▍| 189/200 [05:42<00:26,  2.37s/it]Running Inference:  95%|█████████▌| 190/200 [05:44<00:20,  2.09s/it]Running Inference:  96%|█████████▌| 191/200 [05:46<00:17,  1.95s/it]Running Inference:  96%|█████████▌| 192/200 [05:47<00:15,  1.88s/it]Running Inference:  96%|█████████▋| 193/200 [05:49<00:12,  1.81s/it]Running Inference:  97%|█████████▋| 194/200 [05:51<00:10,  1.81s/it]Running Inference:  98%|█████████▊| 195/200 [05:53<00:09,  1.80s/it]Running Inference:  98%|█████████▊| 196/200 [05:54<00:07,  1.78s/it]Running Inference:  98%|█████████▊| 197/200 [05:56<00:05,  1.82s/it]Running Inference:  99%|█████████▉| 198/200 [05:58<00:03,  1.76s/it]Running Inference: 100%|█████████▉| 199/200 [06:00<00:01,  1.84s/it]Running Inference: 100%|██████████| 200/200 [06:01<00:00,  1.74s/it]Running Inference: 100%|██████████| 200/200 [06:01<00:00,  1.81s/it]
2025-12-14 18:23:25,244 - INFO - Inference completed.
2025-12-14 18:23:25,256 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 18:23:25,256 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:23:25,257 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 18:23:25,257 - INFO - Metrics:
41.5
2025-12-14 18:23:25,258 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_retrieval_en, ratio=0.3) on GPU 4

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:23:31,693 - INFO - Set deterministic seeds to 42
2025-12-14 18:23:31,693 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:23:31,693 - INFO - Starting evaluation run...
2025-12-14 18:23:31,693 - INFO - Output directory set to: longbenchresult
2025-12-14 18:23:31,693 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 18:23:31,693 - INFO - KV Press 'knorm' setup.
2025-12-14 18:23:31,693 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:23:31,693 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.12it/s]
Device set to use cuda:0
2025-12-14 18:23:45,218 - INFO - Model pipeline loaded.
2025-12-14 18:23:45,218 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 18:23:50,092 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:23:50,092 - INFO - Dataset processed with 200 entries.
2025-12-14 18:23:50,125 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:32,  2.27s/it]Running Inference:   1%|          | 2/200 [00:03<05:57,  1.81s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:14,  1.90s/it]Running Inference:   2%|▏         | 4/200 [00:07<05:51,  1.79s/it]Running Inference:   2%|▎         | 5/200 [00:09<05:48,  1.79s/it]Running Inference:   3%|▎         | 6/200 [00:10<05:23,  1.67s/it]Running Inference:   4%|▎         | 7/200 [00:12<05:52,  1.83s/it]Running Inference:   4%|▍         | 8/200 [00:14<05:41,  1.78s/it]Running Inference:   4%|▍         | 9/200 [00:16<05:28,  1.72s/it]Running Inference:   5%|▌         | 10/200 [00:17<05:05,  1.61s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<06:53,  2.19s/it]Running Inference:   6%|▌         | 12/200 [00:22<06:34,  2.10s/it]Running Inference:   6%|▋         | 13/200 [00:24<06:04,  1.95s/it]Running Inference:   7%|▋         | 14/200 [00:25<05:42,  1.84s/it]Running Inference:   8%|▊         | 15/200 [00:27<05:38,  1.83s/it]Running Inference:   8%|▊         | 16/200 [00:29<05:24,  1.76s/it]Running Inference:   8%|▊         | 17/200 [00:31<05:15,  1.72s/it]Running Inference:   9%|▉         | 18/200 [00:32<05:02,  1.66s/it]Running Inference:  10%|▉         | 19/200 [00:34<05:07,  1.70s/it]Running Inference:  10%|█         | 20/200 [00:38<06:56,  2.31s/it]Running Inference:  10%|█         | 21/200 [00:39<06:28,  2.17s/it]Running Inference:  11%|█         | 22/200 [00:41<05:54,  1.99s/it]Running Inference:  12%|█▏        | 23/200 [00:43<05:38,  1.91s/it]Running Inference:  12%|█▏        | 24/200 [00:44<05:19,  1.81s/it]Running Inference:  12%|█▎        | 25/200 [00:46<05:11,  1.78s/it]Running Inference:  13%|█▎        | 26/200 [00:48<05:13,  1.80s/it]Running Inference:  14%|█▎        | 27/200 [00:50<05:34,  1.94s/it]Running Inference:  14%|█▍        | 28/200 [00:53<06:47,  2.37s/it]Running Inference:  14%|█▍        | 29/200 [00:55<05:53,  2.07s/it]Running Inference:  15%|█▌        | 30/200 [00:57<05:34,  1.97s/it]Running Inference:  16%|█▌        | 31/200 [00:58<05:24,  1.92s/it]Running Inference:  16%|█▌        | 32/200 [01:01<05:32,  1.98s/it]Running Inference:  16%|█▋        | 33/200 [01:03<05:35,  2.01s/it]Running Inference:  17%|█▋        | 34/200 [01:05<05:31,  2.00s/it]Running Inference:  18%|█▊        | 35/200 [01:06<05:24,  1.96s/it]Running Inference:  18%|█▊        | 36/200 [01:08<04:51,  1.78s/it]Running Inference:  18%|█▊        | 37/200 [01:09<04:45,  1.75s/it]Running Inference:  19%|█▉        | 38/200 [01:11<04:42,  1.74s/it]Running Inference:  20%|█▉        | 39/200 [01:15<06:05,  2.27s/it]Running Inference:  20%|██        | 40/200 [01:16<05:33,  2.08s/it]Running Inference:  20%|██        | 41/200 [01:18<05:19,  2.01s/it]Running Inference:  21%|██        | 42/200 [01:20<05:02,  1.92s/it]Running Inference:  22%|██▏       | 43/200 [01:22<05:04,  1.94s/it]Running Inference:  22%|██▏       | 44/200 [01:24<04:56,  1.90s/it]Running Inference:  22%|██▎       | 45/200 [01:25<04:35,  1.78s/it]Running Inference:  23%|██▎       | 46/200 [01:27<04:37,  1.80s/it]Running Inference:  24%|██▎       | 47/200 [01:29<04:21,  1.71s/it]Running Inference:  24%|██▍       | 48/200 [01:30<04:12,  1.66s/it]Running Inference:  24%|██▍       | 49/200 [01:32<04:33,  1.81s/it]Running Inference:  25%|██▌       | 50/200 [01:34<04:22,  1.75s/it]Running Inference:  26%|██▌       | 51/200 [01:36<04:17,  1.73s/it]Running Inference:  26%|██▌       | 52/200 [01:37<04:23,  1.78s/it]Running Inference:  26%|██▋       | 53/200 [01:39<04:25,  1.81s/it]Running Inference:  27%|██▋       | 54/200 [01:41<04:28,  1.84s/it]Running Inference:  28%|██▊       | 55/200 [01:43<04:08,  1.71s/it]Running Inference:  28%|██▊       | 56/200 [01:45<04:17,  1.79s/it]Running Inference:  28%|██▊       | 57/200 [01:46<03:59,  1.67s/it]Running Inference:  29%|██▉       | 58/200 [01:48<04:03,  1.72s/it]Running Inference:  30%|██▉       | 59/200 [01:50<04:08,  1.76s/it]Running Inference:  30%|███       | 60/200 [01:51<04:01,  1.72s/it]Running Inference:  30%|███       | 61/200 [01:53<03:52,  1.67s/it]Running Inference:  31%|███       | 62/200 [01:55<04:13,  1.84s/it]Running Inference:  32%|███▏      | 63/200 [01:57<04:01,  1.76s/it]Running Inference:  32%|███▏      | 64/200 [02:00<05:15,  2.32s/it]Running Inference:  32%|███▎      | 65/200 [02:03<05:24,  2.41s/it]Running Inference:  33%|███▎      | 66/200 [02:05<05:04,  2.27s/it]Running Inference:  34%|███▎      | 67/200 [02:06<04:26,  2.00s/it]Running Inference:  34%|███▍      | 68/200 [02:08<04:19,  1.96s/it]Running Inference:  34%|███▍      | 69/200 [02:10<04:13,  1.93s/it]Running Inference:  35%|███▌      | 70/200 [02:12<04:08,  1.91s/it]Running Inference:  36%|███▌      | 71/200 [02:14<04:10,  1.94s/it]Running Inference:  36%|███▌      | 72/200 [02:16<04:05,  1.92s/it]Running Inference:  36%|███▋      | 73/200 [02:19<05:05,  2.41s/it]Running Inference:  37%|███▋      | 74/200 [02:21<04:35,  2.19s/it]Running Inference:  38%|███▊      | 75/200 [02:24<05:24,  2.60s/it]Running Inference:  38%|███▊      | 76/200 [02:26<04:34,  2.21s/it]Running Inference:  38%|███▊      | 77/200 [02:27<04:11,  2.05s/it]Running Inference:  39%|███▉      | 78/200 [02:29<03:56,  1.94s/it]Running Inference:  40%|███▉      | 79/200 [02:31<03:43,  1.85s/it]Running Inference:  40%|████      | 80/200 [02:33<03:46,  1.89s/it]Running Inference:  40%|████      | 81/200 [02:34<03:31,  1.78s/it]Running Inference:  41%|████      | 82/200 [02:36<03:30,  1.78s/it]Running Inference:  42%|████▏     | 83/200 [02:38<03:24,  1.75s/it]Running Inference:  42%|████▏     | 84/200 [02:39<03:14,  1.68s/it]Running Inference:  42%|████▎     | 85/200 [02:41<03:09,  1.65s/it]Running Inference:  43%|████▎     | 86/200 [02:43<03:10,  1.67s/it]Running Inference:  44%|████▎     | 87/200 [02:46<04:18,  2.29s/it]Running Inference:  44%|████▍     | 88/200 [02:48<03:49,  2.05s/it]Running Inference:  44%|████▍     | 89/200 [02:49<03:32,  1.92s/it]Running Inference:  45%|████▌     | 90/200 [02:51<03:17,  1.79s/it]Running Inference:  46%|████▌     | 91/200 [02:53<03:23,  1.87s/it]Running Inference:  46%|████▌     | 92/200 [02:55<03:18,  1.84s/it]Running Inference:  46%|████▋     | 93/200 [02:56<03:05,  1.73s/it]Running Inference:  47%|████▋     | 94/200 [02:58<03:16,  1.86s/it]Running Inference:  48%|████▊     | 95/200 [03:00<03:06,  1.77s/it]Running Inference:  48%|████▊     | 96/200 [03:01<02:57,  1.70s/it]Running Inference:  48%|████▊     | 97/200 [03:03<02:53,  1.68s/it]Running Inference:  49%|████▉     | 98/200 [03:05<02:56,  1.73s/it]Running Inference:  50%|████▉     | 99/200 [03:06<02:49,  1.67s/it]Running Inference:  50%|█████     | 100/200 [03:08<02:48,  1.69s/it]Running Inference:  50%|█████     | 101/200 [03:10<02:41,  1.63s/it]Running Inference:  51%|█████     | 102/200 [03:11<02:38,  1.62s/it]Running Inference:  52%|█████▏    | 103/200 [03:15<03:34,  2.21s/it]Running Inference:  52%|█████▏    | 104/200 [03:16<03:12,  2.01s/it]Running Inference:  52%|█████▎    | 105/200 [03:18<03:05,  1.95s/it]Running Inference:  53%|█████▎    | 106/200 [03:20<02:54,  1.85s/it]Running Inference:  54%|█████▎    | 107/200 [03:22<02:53,  1.86s/it]Running Inference:  54%|█████▍    | 108/200 [03:24<02:54,  1.90s/it]Running Inference:  55%|█████▍    | 109/200 [03:28<03:48,  2.51s/it]Running Inference:  55%|█████▌    | 110/200 [03:30<03:38,  2.43s/it]Running Inference:  56%|█████▌    | 111/200 [03:33<04:04,  2.75s/it]Running Inference:  56%|█████▌    | 112/200 [03:35<03:30,  2.39s/it]Running Inference:  56%|█████▋    | 113/200 [03:37<03:09,  2.18s/it]Running Inference:  57%|█████▋    | 114/200 [03:38<02:49,  1.97s/it]Running Inference:  57%|█████▊    | 115/200 [03:40<02:37,  1.85s/it]Running Inference:  58%|█████▊    | 116/200 [03:41<02:24,  1.72s/it]Running Inference:  58%|█████▊    | 117/200 [03:43<02:16,  1.65s/it]Running Inference:  59%|█████▉    | 118/200 [03:44<02:17,  1.67s/it]Running Inference:  60%|█████▉    | 119/200 [03:46<02:19,  1.72s/it]Running Inference:  60%|██████    | 120/200 [03:48<02:20,  1.76s/it]Running Inference:  60%|██████    | 121/200 [03:50<02:16,  1.72s/it]Running Inference:  61%|██████    | 122/200 [03:52<02:20,  1.80s/it]Running Inference:  62%|██████▏   | 123/200 [03:53<02:14,  1.75s/it]Running Inference:  62%|██████▏   | 124/200 [03:55<02:13,  1.75s/it]Running Inference:  62%|██████▎   | 125/200 [03:57<02:07,  1.70s/it]Running Inference:  63%|██████▎   | 126/200 [03:58<02:03,  1.67s/it]Running Inference:  64%|██████▎   | 127/200 [04:02<02:44,  2.26s/it]Running Inference:  64%|██████▍   | 128/200 [04:03<02:25,  2.02s/it]Running Inference:  64%|██████▍   | 129/200 [04:05<02:08,  1.81s/it]Running Inference:  65%|██████▌   | 130/200 [04:06<02:02,  1.76s/it]Running Inference:  66%|██████▌   | 131/200 [04:08<01:58,  1.71s/it]Running Inference:  66%|██████▌   | 132/200 [04:10<01:59,  1.75s/it]Running Inference:  66%|██████▋   | 133/200 [04:11<01:57,  1.76s/it]Running Inference:  67%|██████▋   | 134/200 [04:13<01:54,  1.74s/it]Running Inference:  68%|██████▊   | 135/200 [04:17<02:29,  2.30s/it]Running Inference:  68%|██████▊   | 136/200 [04:18<02:09,  2.03s/it]Running Inference:  68%|██████▊   | 137/200 [04:20<02:02,  1.94s/it]Running Inference:  69%|██████▉   | 138/200 [04:22<01:54,  1.85s/it]Running Inference:  70%|██████▉   | 139/200 [04:23<01:49,  1.80s/it]Running Inference:  70%|███████   | 140/200 [04:27<02:18,  2.31s/it]Running Inference:  70%|███████   | 141/200 [04:28<02:03,  2.09s/it]Running Inference:  71%|███████   | 142/200 [04:30<01:56,  2.01s/it]Running Inference:  72%|███████▏  | 143/200 [04:32<01:50,  1.93s/it]Running Inference:  72%|███████▏  | 144/200 [04:34<01:49,  1.96s/it]Running Inference:  72%|███████▎  | 145/200 [04:35<01:38,  1.79s/it]Running Inference:  73%|███████▎  | 146/200 [04:37<01:36,  1.79s/it]Running Inference:  74%|███████▎  | 147/200 [04:38<01:28,  1.66s/it]Running Inference:  74%|███████▍  | 148/200 [04:40<01:29,  1.72s/it]Running Inference:  74%|███████▍  | 149/200 [04:42<01:31,  1.79s/it]Running Inference:  75%|███████▌  | 150/200 [04:46<01:55,  2.32s/it]Running Inference:  76%|███████▌  | 151/200 [04:48<01:50,  2.24s/it]Running Inference:  76%|███████▌  | 152/200 [04:49<01:39,  2.06s/it]Running Inference:  76%|███████▋  | 153/200 [04:51<01:29,  1.90s/it]Running Inference:  77%|███████▋  | 154/200 [04:52<01:20,  1.74s/it]Running Inference:  78%|███████▊  | 155/200 [04:54<01:17,  1.72s/it]Running Inference:  78%|███████▊  | 156/200 [04:56<01:19,  1.81s/it]Running Inference:  78%|███████▊  | 157/200 [04:58<01:21,  1.88s/it]Running Inference:  79%|███████▉  | 158/200 [05:00<01:14,  1.78s/it]Running Inference:  80%|███████▉  | 159/200 [05:01<01:12,  1.78s/it]Running Inference:  80%|████████  | 160/200 [05:03<01:07,  1.70s/it]Running Inference:  80%|████████  | 161/200 [05:05<01:09,  1.78s/it]Running Inference:  81%|████████  | 162/200 [05:07<01:06,  1.74s/it]Running Inference:  82%|████████▏ | 163/200 [05:08<01:05,  1.77s/it]Running Inference:  82%|████████▏ | 164/200 [05:10<01:03,  1.76s/it]Running Inference:  82%|████████▎ | 165/200 [05:12<01:03,  1.82s/it]Running Inference:  83%|████████▎ | 166/200 [05:14<01:07,  1.98s/it]Running Inference:  84%|████████▎ | 167/200 [05:16<01:02,  1.88s/it]Running Inference:  84%|████████▍ | 168/200 [05:18<00:58,  1.82s/it]Running Inference:  84%|████████▍ | 169/200 [05:20<00:56,  1.81s/it]Running Inference:  85%|████████▌ | 170/200 [05:22<00:57,  1.92s/it]Running Inference:  86%|████████▌ | 171/200 [05:24<00:55,  1.93s/it]Running Inference:  86%|████████▌ | 172/200 [05:25<00:50,  1.79s/it]Running Inference:  86%|████████▋ | 173/200 [05:27<00:48,  1.79s/it]Running Inference:  87%|████████▋ | 174/200 [05:29<00:47,  1.83s/it]Running Inference:  88%|████████▊ | 175/200 [05:30<00:44,  1.77s/it]Running Inference:  88%|████████▊ | 176/200 [05:32<00:39,  1.66s/it]Running Inference:  88%|████████▊ | 177/200 [05:34<00:41,  1.80s/it]Running Inference:  89%|████████▉ | 178/200 [05:36<00:40,  1.85s/it]Running Inference:  90%|████████▉ | 179/200 [05:37<00:36,  1.74s/it]Running Inference:  90%|█████████ | 180/200 [05:39<00:33,  1.70s/it]Running Inference:  90%|█████████ | 181/200 [05:41<00:31,  1.64s/it]Running Inference:  91%|█████████ | 182/200 [05:42<00:29,  1.63s/it]Running Inference:  92%|█████████▏| 183/200 [05:44<00:28,  1.65s/it]Running Inference:  92%|█████████▏| 184/200 [05:46<00:27,  1.69s/it]Running Inference:  92%|█████████▎| 185/200 [05:48<00:27,  1.83s/it]Running Inference:  93%|█████████▎| 186/200 [05:49<00:24,  1.78s/it]Running Inference:  94%|█████████▎| 187/200 [05:51<00:22,  1.74s/it]Running Inference:  94%|█████████▍| 188/200 [05:53<00:21,  1.81s/it]Running Inference:  94%|█████████▍| 189/200 [05:55<00:19,  1.75s/it]Running Inference:  95%|█████████▌| 190/200 [05:58<00:22,  2.27s/it]Running Inference:  96%|█████████▌| 191/200 [06:00<00:19,  2.13s/it]Running Inference:  96%|█████████▌| 192/200 [06:02<00:16,  2.00s/it]Running Inference:  96%|█████████▋| 193/200 [06:03<00:13,  1.91s/it]Running Inference:  97%|█████████▋| 194/200 [06:05<00:11,  1.88s/it]Running Inference:  98%|█████████▊| 195/200 [06:07<00:09,  1.85s/it]Running Inference:  98%|█████████▊| 196/200 [06:09<00:07,  1.81s/it]Running Inference:  98%|█████████▊| 197/200 [06:11<00:05,  1.88s/it]Running Inference:  99%|█████████▉| 198/200 [06:12<00:03,  1.81s/it]Running Inference: 100%|█████████▉| 199/200 [06:16<00:02,  2.48s/it]Running Inference: 100%|██████████| 200/200 [06:18<00:00,  2.18s/it]Running Inference: 100%|██████████| 200/200 [06:18<00:00,  1.89s/it]
2025-12-14 18:30:08,568 - INFO - Inference completed.
2025-12-14 18:30:08,579 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 18:30:08,579 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:30:08,581 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 18:30:08,581 - INFO - Metrics:
33.05
2025-12-14 18:30:08,582 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_retrieval_en, ratio=0.5) on GPU 4


========================================
LongBench Task: trec
========================================
----------------------------------------
Task: trec | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:30:15,102 - INFO - Set deterministic seeds to 42
2025-12-14 18:30:15,102 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:30:15,103 - INFO - Starting evaluation run...
2025-12-14 18:30:15,103 - INFO - Output directory set to: longbenchresult
2025-12-14 18:30:15,103 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 18:30:15,103 - INFO - KV Press 'knorm' setup.
2025-12-14 18:30:15,103 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:30:15,103 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.82it/s]
Device set to use cuda:0
2025-12-14 18:30:27,467 - INFO - Model pipeline loaded.
2025-12-14 18:30:27,468 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 18:30:33,108 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:30:33,108 - INFO - Dataset processed with 200 entries.
2025-12-14 18:30:33,124 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:36,  1.39s/it]Running Inference:   1%|          | 2/200 [00:02<03:16,  1.01it/s]Running Inference:   2%|▏         | 3/200 [00:03<03:14,  1.01it/s]Running Inference:   2%|▏         | 4/200 [00:03<02:53,  1.13it/s]Running Inference:   2%|▎         | 5/200 [00:05<03:24,  1.05s/it]Running Inference:   3%|▎         | 6/200 [00:09<06:56,  2.15s/it]Running Inference:   4%|▎         | 7/200 [00:10<05:24,  1.68s/it]Running Inference:   4%|▍         | 8/200 [00:10<04:09,  1.30s/it]Running Inference:   4%|▍         | 9/200 [00:11<03:25,  1.08s/it]Running Inference:   5%|▌         | 10/200 [00:12<03:18,  1.04s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:12<03:00,  1.04it/s]Running Inference:   6%|▌         | 12/200 [00:14<03:21,  1.07s/it]Running Inference:   6%|▋         | 13/200 [00:14<02:48,  1.11it/s]Running Inference:   7%|▋         | 14/200 [00:15<03:03,  1.01it/s]Running Inference:   8%|▊         | 15/200 [00:16<02:49,  1.09it/s]Running Inference:   8%|▊         | 16/200 [00:17<02:59,  1.03it/s]Running Inference:   8%|▊         | 17/200 [00:19<03:30,  1.15s/it]Running Inference:   9%|▉         | 18/200 [00:19<02:57,  1.02it/s]Running Inference:  10%|▉         | 19/200 [00:24<06:08,  2.04s/it]Running Inference:  10%|█         | 20/200 [00:26<05:45,  1.92s/it]Running Inference:  10%|█         | 21/200 [00:26<04:37,  1.55s/it]Running Inference:  11%|█         | 22/200 [00:27<03:55,  1.32s/it]Running Inference:  12%|█▏        | 23/200 [00:28<03:07,  1.06s/it]Running Inference:  12%|█▏        | 24/200 [00:29<03:16,  1.12s/it]Running Inference:  12%|█▎        | 25/200 [00:30<03:29,  1.20s/it]Running Inference:  13%|█▎        | 26/200 [00:35<06:20,  2.19s/it]Running Inference:  14%|█▎        | 27/200 [00:36<05:28,  1.90s/it]Running Inference:  14%|█▍        | 28/200 [00:37<04:52,  1.70s/it]Running Inference:  14%|█▍        | 29/200 [00:38<03:53,  1.37s/it]Running Inference:  15%|█▌        | 30/200 [00:39<03:55,  1.38s/it]Running Inference:  16%|█▌        | 31/200 [00:40<03:45,  1.34s/it]Running Inference:  16%|█▌        | 32/200 [00:45<06:13,  2.23s/it]Running Inference:  16%|█▋        | 33/200 [00:49<07:35,  2.73s/it]Running Inference:  17%|█▋        | 34/200 [00:49<05:42,  2.06s/it]Running Inference:  18%|█▊        | 35/200 [00:50<04:48,  1.75s/it]Running Inference:  18%|█▊        | 36/200 [00:54<06:37,  2.42s/it]Running Inference:  18%|█▊        | 37/200 [00:59<08:27,  3.11s/it]Running Inference:  19%|█▉        | 38/200 [01:00<06:54,  2.56s/it]Running Inference:  20%|█▉        | 39/200 [01:01<05:24,  2.01s/it]Running Inference:  20%|██        | 40/200 [01:02<04:20,  1.63s/it]Running Inference:  20%|██        | 41/200 [01:06<06:53,  2.60s/it]Running Inference:  21%|██        | 42/200 [01:07<05:12,  1.98s/it]Running Inference:  22%|██▏       | 43/200 [01:08<04:06,  1.57s/it]Running Inference:  22%|██▏       | 44/200 [01:08<03:30,  1.35s/it]Running Inference:  22%|██▎       | 45/200 [01:10<03:35,  1.39s/it]Running Inference:  23%|██▎       | 46/200 [01:10<02:52,  1.12s/it]Running Inference:  24%|██▎       | 47/200 [01:11<02:43,  1.07s/it]Running Inference:  24%|██▍       | 48/200 [01:12<02:11,  1.16it/s]Running Inference:  24%|██▍       | 49/200 [01:13<02:10,  1.16it/s]Running Inference:  25%|██▌       | 50/200 [01:16<04:22,  1.75s/it]Running Inference:  26%|██▌       | 51/200 [01:17<03:38,  1.47s/it]Running Inference:  26%|██▌       | 52/200 [01:18<03:01,  1.23s/it]Running Inference:  26%|██▋       | 53/200 [01:20<03:47,  1.55s/it]Running Inference:  27%|██▋       | 54/200 [01:21<02:56,  1.21s/it]Running Inference:  28%|██▊       | 55/200 [01:21<02:20,  1.03it/s]Running Inference:  28%|██▊       | 56/200 [01:22<02:20,  1.03it/s]Running Inference:  28%|██▊       | 57/200 [01:26<04:26,  1.86s/it]Running Inference:  29%|██▉       | 58/200 [01:27<03:56,  1.66s/it]Running Inference:  30%|██▉       | 59/200 [01:28<03:22,  1.44s/it]Running Inference:  30%|███       | 60/200 [01:29<02:40,  1.15s/it]Running Inference:  30%|███       | 61/200 [01:30<02:47,  1.21s/it]Running Inference:  31%|███       | 62/200 [01:30<02:21,  1.03s/it]Running Inference:  32%|███▏      | 63/200 [01:31<02:18,  1.01s/it]Running Inference:  32%|███▏      | 64/200 [01:32<02:10,  1.04it/s]Running Inference:  32%|███▎      | 65/200 [01:33<01:53,  1.19it/s]Running Inference:  33%|███▎      | 66/200 [01:34<01:50,  1.22it/s]Running Inference:  34%|███▎      | 67/200 [01:34<01:41,  1.31it/s]Running Inference:  34%|███▍      | 68/200 [01:35<01:35,  1.39it/s]Running Inference:  34%|███▍      | 69/200 [01:36<01:37,  1.35it/s]Running Inference:  35%|███▌      | 70/200 [01:37<01:48,  1.20it/s]Running Inference:  36%|███▌      | 71/200 [01:38<02:05,  1.03it/s]Running Inference:  36%|███▌      | 72/200 [01:43<04:28,  2.10s/it]Running Inference:  36%|███▋      | 73/200 [01:44<03:40,  1.74s/it]Running Inference:  37%|███▋      | 74/200 [01:47<04:57,  2.36s/it]Running Inference:  38%|███▊      | 75/200 [01:48<03:51,  1.85s/it]Running Inference:  38%|███▊      | 76/200 [01:49<03:14,  1.57s/it]Running Inference:  38%|███▊      | 77/200 [01:50<02:47,  1.36s/it]Running Inference:  39%|███▉      | 78/200 [01:51<02:39,  1.31s/it]Running Inference:  40%|███▉      | 79/200 [01:52<02:11,  1.09s/it]Running Inference:  40%|████      | 80/200 [01:52<01:59,  1.00it/s]Running Inference:  40%|████      | 81/200 [01:54<02:02,  1.03s/it]Running Inference:  41%|████      | 82/200 [01:55<01:59,  1.01s/it]Running Inference:  42%|████▏     | 83/200 [01:56<01:58,  1.01s/it]Running Inference:  42%|████▏     | 84/200 [01:57<02:04,  1.07s/it]Running Inference:  42%|████▎     | 85/200 [02:01<03:52,  2.02s/it]Running Inference:  43%|████▎     | 86/200 [02:02<03:07,  1.65s/it]Running Inference:  44%|████▎     | 87/200 [02:06<04:43,  2.51s/it]Running Inference:  44%|████▍     | 88/200 [02:09<04:45,  2.55s/it]Running Inference:  44%|████▍     | 89/200 [02:10<04:05,  2.21s/it]Running Inference:  45%|████▌     | 90/200 [02:11<03:17,  1.79s/it]Running Inference:  46%|████▌     | 91/200 [02:12<02:43,  1.50s/it]Running Inference:  46%|████▌     | 92/200 [02:13<02:30,  1.39s/it]Running Inference:  46%|████▋     | 93/200 [02:14<02:21,  1.32s/it]Running Inference:  47%|████▋     | 94/200 [02:19<04:03,  2.30s/it]Running Inference:  48%|████▊     | 95/200 [02:19<03:04,  1.75s/it]Running Inference:  48%|████▊     | 96/200 [02:20<02:41,  1.55s/it]Running Inference:  48%|████▊     | 97/200 [02:22<02:43,  1.59s/it]Running Inference:  49%|████▉     | 98/200 [02:23<02:18,  1.35s/it]Running Inference:  50%|████▉     | 99/200 [02:27<03:54,  2.32s/it]Running Inference:  50%|█████     | 100/200 [02:29<03:25,  2.06s/it]Running Inference:  50%|█████     | 101/200 [02:30<02:46,  1.68s/it]Running Inference:  51%|█████     | 102/200 [02:30<02:11,  1.34s/it]Running Inference:  52%|█████▏    | 103/200 [02:31<02:06,  1.30s/it]Running Inference:  52%|█████▏    | 104/200 [02:33<01:58,  1.24s/it]Running Inference:  52%|█████▎    | 105/200 [02:34<01:55,  1.21s/it]Running Inference:  53%|█████▎    | 106/200 [02:35<01:51,  1.19s/it]Running Inference:  54%|█████▎    | 107/200 [02:36<01:45,  1.14s/it]Running Inference:  54%|█████▍    | 108/200 [02:41<03:23,  2.21s/it]Running Inference:  55%|█████▍    | 109/200 [02:42<02:45,  1.82s/it]Running Inference:  55%|█████▌    | 110/200 [02:42<02:18,  1.54s/it]Running Inference:  56%|█████▌    | 111/200 [02:43<01:48,  1.22s/it]Running Inference:  56%|█████▌    | 112/200 [02:44<01:49,  1.25s/it]Running Inference:  56%|█████▋    | 113/200 [02:45<01:39,  1.15s/it]Running Inference:  57%|█████▋    | 114/200 [02:46<01:30,  1.05s/it]Running Inference:  57%|█████▊    | 115/200 [02:47<01:19,  1.07it/s]Running Inference:  58%|█████▊    | 116/200 [02:47<01:11,  1.17it/s]Running Inference:  58%|█████▊    | 117/200 [02:52<02:51,  2.07s/it]Running Inference:  59%|█████▉    | 118/200 [02:53<02:27,  1.79s/it]Running Inference:  60%|█████▉    | 119/200 [02:58<03:33,  2.64s/it]Running Inference:  60%|██████    | 120/200 [02:59<03:04,  2.31s/it]Running Inference:  60%|██████    | 121/200 [03:00<02:16,  1.73s/it]Running Inference:  61%|██████    | 122/200 [03:01<02:08,  1.64s/it]Running Inference:  62%|██████▏   | 123/200 [03:02<01:45,  1.37s/it]Running Inference:  62%|██████▏   | 124/200 [03:03<01:41,  1.34s/it]Running Inference:  62%|██████▎   | 125/200 [03:05<01:41,  1.35s/it]Running Inference:  63%|██████▎   | 126/200 [03:09<02:37,  2.13s/it]Running Inference:  64%|██████▎   | 127/200 [03:10<02:09,  1.78s/it]Running Inference:  64%|██████▍   | 128/200 [03:10<01:41,  1.41s/it]Running Inference:  64%|██████▍   | 129/200 [03:11<01:37,  1.37s/it]Running Inference:  65%|██████▌   | 130/200 [03:16<02:45,  2.36s/it]Running Inference:  66%|██████▌   | 131/200 [03:17<02:13,  1.93s/it]Running Inference:  66%|██████▌   | 132/200 [03:18<01:58,  1.74s/it]Running Inference:  66%|██████▋   | 133/200 [03:20<01:46,  1.60s/it]Running Inference:  67%|██████▋   | 134/200 [03:20<01:20,  1.22s/it]Running Inference:  68%|██████▊   | 135/200 [03:20<01:04,  1.02it/s]Running Inference:  68%|██████▊   | 136/200 [03:21<00:58,  1.10it/s]Running Inference:  68%|██████▊   | 137/200 [03:23<01:10,  1.12s/it]Running Inference:  69%|██████▉   | 138/200 [03:27<02:01,  1.95s/it]Running Inference:  70%|██████▉   | 139/200 [03:28<01:41,  1.66s/it]Running Inference:  70%|███████   | 140/200 [03:29<01:34,  1.57s/it]Running Inference:  70%|███████   | 141/200 [03:30<01:20,  1.36s/it]Running Inference:  71%|███████   | 142/200 [03:31<01:12,  1.26s/it]Running Inference:  72%|███████▏  | 143/200 [03:32<01:14,  1.30s/it]Running Inference:  72%|███████▏  | 144/200 [03:33<01:02,  1.12s/it]Running Inference:  72%|███████▎  | 145/200 [03:37<01:46,  1.94s/it]Running Inference:  73%|███████▎  | 146/200 [03:38<01:36,  1.78s/it]Running Inference:  74%|███████▎  | 147/200 [03:39<01:17,  1.46s/it]Running Inference:  74%|███████▍  | 148/200 [03:39<01:03,  1.22s/it]Running Inference:  74%|███████▍  | 149/200 [03:40<00:51,  1.02s/it]Running Inference:  75%|███████▌  | 150/200 [03:41<00:49,  1.02it/s]Running Inference:  76%|███████▌  | 151/200 [03:42<00:55,  1.13s/it]Running Inference:  76%|███████▌  | 152/200 [03:47<01:44,  2.18s/it]Running Inference:  76%|███████▋  | 153/200 [03:48<01:20,  1.72s/it]Running Inference:  77%|███████▋  | 154/200 [03:49<01:12,  1.59s/it]Running Inference:  78%|███████▊  | 155/200 [03:50<00:58,  1.30s/it]Running Inference:  78%|███████▊  | 156/200 [03:51<00:53,  1.23s/it]Running Inference:  78%|███████▊  | 157/200 [03:52<00:54,  1.28s/it]Running Inference:  79%|███████▉  | 158/200 [03:53<00:54,  1.29s/it]Running Inference:  80%|███████▉  | 159/200 [03:58<01:33,  2.27s/it]Running Inference:  80%|████████  | 160/200 [03:59<01:11,  1.79s/it]Running Inference:  80%|████████  | 161/200 [03:59<00:57,  1.47s/it]Running Inference:  81%|████████  | 162/200 [04:00<00:48,  1.26s/it]Running Inference:  82%|████████▏ | 163/200 [04:01<00:44,  1.20s/it]Running Inference:  82%|████████▏ | 164/200 [04:03<00:46,  1.29s/it]Running Inference:  82%|████████▎ | 165/200 [04:07<01:20,  2.29s/it]Running Inference:  83%|████████▎ | 166/200 [04:08<00:59,  1.76s/it]Running Inference:  84%|████████▎ | 167/200 [04:09<00:49,  1.51s/it]Running Inference:  84%|████████▍ | 168/200 [04:13<01:18,  2.44s/it]Running Inference:  84%|████████▍ | 169/200 [04:14<00:57,  1.84s/it]Running Inference:  85%|████████▌ | 170/200 [04:14<00:44,  1.47s/it]Running Inference:  86%|████████▌ | 171/200 [04:15<00:32,  1.12s/it]Running Inference:  86%|████████▌ | 172/200 [04:19<01:00,  2.14s/it]Running Inference:  86%|████████▋ | 173/200 [04:20<00:43,  1.63s/it]Running Inference:  87%|████████▋ | 174/200 [04:24<00:59,  2.30s/it]Running Inference:  88%|████████▊ | 175/200 [04:25<00:50,  2.01s/it]Running Inference:  88%|████████▊ | 176/200 [04:26<00:43,  1.79s/it]Running Inference:  88%|████████▊ | 177/200 [04:27<00:31,  1.38s/it]Running Inference:  89%|████████▉ | 178/200 [04:28<00:29,  1.32s/it]Running Inference:  90%|████████▉ | 179/200 [04:28<00:22,  1.05s/it]Running Inference:  90%|█████████ | 180/200 [04:29<00:16,  1.19it/s]Running Inference:  90%|█████████ | 181/200 [04:29<00:15,  1.21it/s]Running Inference:  91%|█████████ | 182/200 [04:30<00:16,  1.08it/s]Running Inference:  92%|█████████▏| 183/200 [04:35<00:35,  2.06s/it]Running Inference:  92%|█████████▏| 184/200 [04:39<00:43,  2.72s/it]Running Inference:  92%|█████████▎| 185/200 [04:40<00:32,  2.17s/it]Running Inference:  93%|█████████▎| 186/200 [04:41<00:25,  1.79s/it]Running Inference:  94%|█████████▎| 187/200 [04:42<00:19,  1.53s/it]Running Inference:  94%|█████████▍| 188/200 [04:43<00:14,  1.25s/it]Running Inference:  94%|█████████▍| 189/200 [04:43<00:11,  1.08s/it]Running Inference:  95%|█████████▌| 190/200 [04:47<00:19,  1.97s/it]Running Inference:  96%|█████████▌| 191/200 [04:48<00:14,  1.59s/it]Running Inference:  96%|█████████▌| 192/200 [04:49<00:11,  1.48s/it]Running Inference:  96%|█████████▋| 193/200 [04:51<00:10,  1.44s/it]Running Inference:  97%|█████████▋| 194/200 [04:52<00:08,  1.34s/it]Running Inference:  98%|█████████▊| 195/200 [04:52<00:05,  1.12s/it]Running Inference:  98%|█████████▊| 196/200 [04:56<00:07,  1.96s/it]Running Inference:  98%|█████████▊| 197/200 [04:58<00:05,  1.72s/it]Running Inference:  99%|█████████▉| 198/200 [04:58<00:02,  1.46s/it]Running Inference: 100%|█████████▉| 199/200 [05:00<00:01,  1.48s/it]Running Inference: 100%|██████████| 200/200 [05:01<00:00,  1.34s/it]Running Inference: 100%|██████████| 200/200 [05:01<00:00,  1.51s/it]
2025-12-14 18:35:34,579 - INFO - Inference completed.
2025-12-14 18:35:34,601 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 18:35:34,601 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:35:34,603 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 18:35:34,603 - INFO - Metrics:
40.0
2025-12-14 18:35:34,604 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=trec, ratio=0.1) on GPU 4

----------------------------------------
Task: trec | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:35:41,120 - INFO - Set deterministic seeds to 42
2025-12-14 18:35:41,120 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:35:41,120 - INFO - Starting evaluation run...
2025-12-14 18:35:41,120 - INFO - Output directory set to: longbenchresult
2025-12-14 18:35:41,121 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 18:35:41,121 - INFO - KV Press 'knorm' setup.
2025-12-14 18:35:41,121 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:35:41,121 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.32it/s]
Device set to use cuda:0
2025-12-14 18:35:55,129 - INFO - Model pipeline loaded.
2025-12-14 18:35:55,129 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 18:35:59,486 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:35:59,486 - INFO - Dataset processed with 200 entries.
2025-12-14 18:35:59,499 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<05:03,  1.53s/it]Running Inference:   1%|          | 2/200 [00:02<03:24,  1.03s/it]Running Inference:   2%|▏         | 3/200 [00:03<03:20,  1.02s/it]Running Inference:   2%|▏         | 4/200 [00:03<02:57,  1.10it/s]Running Inference:   2%|▎         | 5/200 [00:05<03:26,  1.06s/it]Running Inference:   3%|▎         | 6/200 [00:09<06:55,  2.14s/it]Running Inference:   4%|▎         | 7/200 [00:10<05:23,  1.68s/it]Running Inference:   4%|▍         | 8/200 [00:10<04:08,  1.29s/it]Running Inference:   4%|▍         | 9/200 [00:11<03:22,  1.06s/it]Running Inference:   5%|▌         | 10/200 [00:12<03:15,  1.03s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:12<02:53,  1.09it/s]Running Inference:   6%|▌         | 12/200 [00:14<03:11,  1.02s/it]Running Inference:   6%|▋         | 13/200 [00:14<02:41,  1.16it/s]Running Inference:   7%|▋         | 14/200 [00:15<02:58,  1.04it/s]Running Inference:   8%|▊         | 15/200 [00:16<02:45,  1.12it/s]Running Inference:   8%|▊         | 16/200 [00:17<02:44,  1.12it/s]Running Inference:   8%|▊         | 17/200 [00:18<03:03,  1.00s/it]Running Inference:   9%|▉         | 18/200 [00:21<04:30,  1.49s/it]Running Inference:  10%|▉         | 19/200 [00:22<04:17,  1.43s/it]Running Inference:  10%|█         | 20/200 [00:24<04:28,  1.49s/it]Running Inference:  10%|█         | 21/200 [00:24<03:43,  1.25s/it]Running Inference:  11%|█         | 22/200 [00:25<03:32,  1.19s/it]Running Inference:  12%|█▏        | 23/200 [00:26<02:54,  1.02it/s]Running Inference:  12%|█▏        | 24/200 [00:27<03:04,  1.05s/it]Running Inference:  12%|█▎        | 25/200 [00:29<03:21,  1.15s/it]Running Inference:  13%|█▎        | 26/200 [00:30<03:19,  1.15s/it]Running Inference:  14%|█▎        | 27/200 [00:31<03:22,  1.17s/it]Running Inference:  14%|█▍        | 28/200 [00:33<04:22,  1.53s/it]Running Inference:  14%|█▍        | 29/200 [00:34<03:32,  1.24s/it]Running Inference:  15%|█▌        | 30/200 [00:35<03:40,  1.30s/it]Running Inference:  16%|█▌        | 31/200 [00:37<03:41,  1.31s/it]Running Inference:  16%|█▌        | 32/200 [00:41<06:11,  2.21s/it]Running Inference:  16%|█▋        | 33/200 [00:45<07:32,  2.71s/it]Running Inference:  17%|█▋        | 34/200 [00:45<05:40,  2.05s/it]Running Inference:  18%|█▊        | 35/200 [00:46<04:46,  1.74s/it]Running Inference:  18%|█▊        | 36/200 [00:47<04:12,  1.54s/it]Running Inference:  18%|█▊        | 37/200 [00:52<06:45,  2.49s/it]Running Inference:  19%|█▉        | 38/200 [00:53<05:43,  2.12s/it]Running Inference:  20%|█▉        | 39/200 [00:54<04:34,  1.70s/it]Running Inference:  20%|██        | 40/200 [00:55<03:45,  1.41s/it]Running Inference:  20%|██        | 41/200 [00:56<03:43,  1.41s/it]Running Inference:  21%|██        | 42/200 [00:57<03:04,  1.17s/it]Running Inference:  22%|██▏       | 43/200 [00:57<02:36,  1.00it/s]Running Inference:  22%|██▏       | 44/200 [00:58<02:28,  1.05it/s]Running Inference:  22%|██▎       | 45/200 [01:00<02:52,  1.11s/it]Running Inference:  23%|██▎       | 46/200 [01:00<02:21,  1.09it/s]Running Inference:  24%|██▎       | 47/200 [01:01<02:22,  1.08it/s]Running Inference:  24%|██▍       | 48/200 [01:02<01:56,  1.30it/s]Running Inference:  24%|██▍       | 49/200 [01:02<01:59,  1.26it/s]Running Inference:  25%|██▌       | 50/200 [01:03<01:44,  1.43it/s]Running Inference:  26%|██▌       | 51/200 [01:04<01:48,  1.37it/s]Running Inference:  26%|██▌       | 52/200 [01:04<01:45,  1.40it/s]Running Inference:  26%|██▋       | 53/200 [01:08<04:00,  1.64s/it]Running Inference:  27%|██▋       | 54/200 [01:09<03:06,  1.27s/it]Running Inference:  28%|██▊       | 55/200 [01:09<02:26,  1.01s/it]Running Inference:  28%|██▊       | 56/200 [01:10<02:24,  1.00s/it]Running Inference:  28%|██▊       | 57/200 [01:14<04:27,  1.87s/it]Running Inference:  29%|██▉       | 58/200 [01:15<03:57,  1.67s/it]Running Inference:  30%|██▉       | 59/200 [01:16<03:23,  1.44s/it]Running Inference:  30%|███       | 60/200 [01:17<02:40,  1.15s/it]Running Inference:  30%|███       | 61/200 [01:18<02:48,  1.21s/it]Running Inference:  31%|███       | 62/200 [01:18<02:21,  1.03s/it]Running Inference:  32%|███▏      | 63/200 [01:19<02:17,  1.01s/it]Running Inference:  32%|███▏      | 64/200 [01:20<02:08,  1.06it/s]Running Inference:  32%|███▎      | 65/200 [01:21<01:51,  1.21it/s]Running Inference:  33%|███▎      | 66/200 [01:22<01:48,  1.23it/s]Running Inference:  34%|███▎      | 67/200 [01:22<01:40,  1.32it/s]Running Inference:  34%|███▍      | 68/200 [01:23<01:34,  1.40it/s]Running Inference:  34%|███▍      | 69/200 [01:24<01:36,  1.36it/s]Running Inference:  35%|███▌      | 70/200 [01:25<01:48,  1.20it/s]Running Inference:  36%|███▌      | 71/200 [01:26<02:04,  1.04it/s]Running Inference:  36%|███▌      | 72/200 [01:27<02:16,  1.06s/it]Running Inference:  36%|███▋      | 73/200 [01:28<02:08,  1.01s/it]Running Inference:  37%|███▋      | 74/200 [01:32<03:52,  1.85s/it]Running Inference:  38%|███▊      | 75/200 [01:33<03:06,  1.49s/it]Running Inference:  38%|███▊      | 76/200 [01:33<02:44,  1.33s/it]Running Inference:  38%|███▊      | 77/200 [01:34<02:26,  1.19s/it]Running Inference:  39%|███▉      | 78/200 [01:36<02:25,  1.19s/it]Running Inference:  40%|███▉      | 79/200 [01:36<02:02,  1.01s/it]Running Inference:  40%|████      | 80/200 [01:37<01:52,  1.06it/s]Running Inference:  40%|████      | 81/200 [01:38<01:57,  1.01it/s]Running Inference:  41%|████      | 82/200 [01:39<01:59,  1.01s/it]Running Inference:  42%|████▏     | 83/200 [01:40<01:58,  1.01s/it]Running Inference:  42%|████▏     | 84/200 [01:41<02:04,  1.07s/it]Running Inference:  42%|████▎     | 85/200 [01:46<03:51,  2.01s/it]Running Inference:  43%|████▎     | 86/200 [01:46<03:10,  1.67s/it]Running Inference:  44%|████▎     | 87/200 [01:51<04:45,  2.53s/it]Running Inference:  44%|████▍     | 88/200 [01:55<05:32,  2.97s/it]Running Inference:  44%|████▍     | 89/200 [01:56<04:38,  2.51s/it]Running Inference:  45%|████▌     | 90/200 [01:57<03:39,  2.00s/it]Running Inference:  46%|████▌     | 91/200 [01:58<02:58,  1.64s/it]Running Inference:  46%|████▌     | 92/200 [01:59<02:41,  1.49s/it]Running Inference:  46%|████▋     | 93/200 [02:00<02:24,  1.35s/it]Running Inference:  47%|████▋     | 94/200 [02:01<02:16,  1.29s/it]Running Inference:  48%|████▊     | 95/200 [02:02<01:49,  1.05s/it]Running Inference:  48%|████▊     | 96/200 [02:03<01:45,  1.02s/it]Running Inference:  48%|████▊     | 97/200 [02:04<02:05,  1.21s/it]Running Inference:  49%|████▉     | 98/200 [02:05<01:51,  1.09s/it]Running Inference:  50%|████▉     | 99/200 [02:06<01:50,  1.10s/it]Running Inference:  50%|█████     | 100/200 [02:08<01:59,  1.20s/it]Running Inference:  50%|█████     | 101/200 [02:09<01:46,  1.08s/it]Running Inference:  51%|█████     | 102/200 [02:09<01:30,  1.09it/s]Running Inference:  52%|█████▏    | 103/200 [02:10<01:37,  1.01s/it]Running Inference:  52%|█████▏    | 104/200 [02:11<01:33,  1.02it/s]Running Inference:  52%|█████▎    | 105/200 [02:12<01:37,  1.03s/it]Running Inference:  53%|█████▎    | 106/200 [02:13<01:39,  1.06s/it]Running Inference:  54%|█████▎    | 107/200 [02:15<01:41,  1.09s/it]Running Inference:  54%|█████▍    | 108/200 [02:16<01:45,  1.15s/it]Running Inference:  55%|█████▍    | 109/200 [02:17<01:38,  1.08s/it]Running Inference:  55%|█████▌    | 110/200 [02:18<01:33,  1.04s/it]Running Inference:  56%|█████▌    | 111/200 [02:18<01:17,  1.14it/s]Running Inference:  56%|█████▌    | 112/200 [02:23<02:54,  1.98s/it]Running Inference:  56%|█████▋    | 113/200 [02:24<02:24,  1.66s/it]Running Inference:  57%|█████▋    | 114/200 [02:25<02:01,  1.41s/it]Running Inference:  57%|█████▊    | 115/200 [02:25<01:40,  1.18s/it]Running Inference:  58%|█████▊    | 116/200 [02:26<01:26,  1.02s/it]Running Inference:  58%|█████▊    | 117/200 [02:27<01:35,  1.15s/it]Running Inference:  59%|█████▉    | 118/200 [02:29<01:34,  1.16s/it]Running Inference:  60%|█████▉    | 119/200 [02:30<01:37,  1.20s/it]Running Inference:  60%|██████    | 120/200 [02:31<01:43,  1.30s/it]Running Inference:  60%|██████    | 121/200 [02:32<01:20,  1.02s/it]Running Inference:  61%|██████    | 122/200 [02:33<01:29,  1.15s/it]Running Inference:  62%|██████▏   | 123/200 [02:34<01:24,  1.10s/it]Running Inference:  62%|██████▏   | 124/200 [02:35<01:27,  1.15s/it]Running Inference:  62%|██████▎   | 125/200 [02:37<01:31,  1.22s/it]Running Inference:  63%|██████▎   | 126/200 [02:37<01:12,  1.02it/s]Running Inference:  64%|██████▎   | 127/200 [02:38<01:12,  1.00it/s]Running Inference:  64%|██████▍   | 128/200 [02:42<02:16,  1.89s/it]Running Inference:  64%|██████▍   | 129/200 [02:44<02:01,  1.71s/it]Running Inference:  65%|██████▌   | 130/200 [02:48<03:01,  2.59s/it]Running Inference:  66%|██████▌   | 131/200 [02:49<02:24,  2.09s/it]Running Inference:  66%|██████▌   | 132/200 [02:50<02:05,  1.85s/it]Running Inference:  66%|██████▋   | 133/200 [02:52<01:52,  1.67s/it]Running Inference:  67%|██████▋   | 134/200 [02:52<01:24,  1.28s/it]Running Inference:  68%|██████▊   | 135/200 [02:52<01:06,  1.02s/it]Running Inference:  68%|██████▊   | 136/200 [02:53<00:59,  1.07it/s]Running Inference:  68%|██████▊   | 137/200 [02:55<01:11,  1.14s/it]Running Inference:  69%|██████▉   | 138/200 [02:55<01:01,  1.00it/s]Running Inference:  70%|██████▉   | 139/200 [02:56<01:00,  1.01it/s]Running Inference:  70%|███████   | 140/200 [02:58<01:06,  1.11s/it]Running Inference:  70%|███████   | 141/200 [02:59<01:00,  1.03s/it]Running Inference:  71%|███████   | 142/200 [03:00<00:59,  1.03s/it]Running Inference:  72%|███████▏  | 143/200 [03:01<01:05,  1.14s/it]Running Inference:  72%|███████▏  | 144/200 [03:02<00:57,  1.03s/it]Running Inference:  72%|███████▎  | 145/200 [03:06<01:42,  1.87s/it]Running Inference:  73%|███████▎  | 146/200 [03:07<01:32,  1.72s/it]Running Inference:  74%|███████▎  | 147/200 [03:08<01:12,  1.36s/it]Running Inference:  74%|███████▍  | 148/200 [03:08<00:59,  1.15s/it]Running Inference:  74%|███████▍  | 149/200 [03:09<00:50,  1.02it/s]Running Inference:  75%|███████▌  | 150/200 [03:10<00:47,  1.05it/s]Running Inference:  76%|███████▌  | 151/200 [03:11<00:52,  1.07s/it]Running Inference:  76%|███████▌  | 152/200 [03:12<00:54,  1.14s/it]Running Inference:  76%|███████▋  | 153/200 [03:13<00:46,  1.01it/s]Running Inference:  77%|███████▋  | 154/200 [03:14<00:49,  1.08s/it]Running Inference:  78%|███████▊  | 155/200 [03:15<00:42,  1.05it/s]Running Inference:  78%|███████▊  | 156/200 [03:16<00:42,  1.03it/s]Running Inference:  78%|███████▊  | 157/200 [03:17<00:47,  1.10s/it]Running Inference:  79%|███████▉  | 158/200 [03:19<00:50,  1.19s/it]Running Inference:  80%|███████▉  | 159/200 [03:20<00:47,  1.16s/it]Running Inference:  80%|████████  | 160/200 [03:20<00:40,  1.01s/it]Running Inference:  80%|████████  | 161/200 [03:21<00:36,  1.08it/s]Running Inference:  81%|████████  | 162/200 [03:22<00:33,  1.13it/s]Running Inference:  82%|████████▏ | 163/200 [03:23<00:34,  1.08it/s]Running Inference:  82%|████████▏ | 164/200 [03:25<00:39,  1.10s/it]Running Inference:  82%|████████▎ | 165/200 [03:26<00:42,  1.20s/it]Running Inference:  83%|████████▎ | 166/200 [03:27<00:33,  1.00it/s]Running Inference:  84%|████████▎ | 167/200 [03:27<00:32,  1.02it/s]Running Inference:  84%|████████▍ | 168/200 [03:29<00:33,  1.05s/it]Running Inference:  84%|████████▍ | 169/200 [03:29<00:26,  1.16it/s]Running Inference:  85%|████████▌ | 170/200 [03:30<00:23,  1.29it/s]Running Inference:  86%|████████▌ | 171/200 [03:30<00:18,  1.59it/s]Running Inference:  86%|████████▌ | 172/200 [03:34<00:50,  1.79s/it]Running Inference:  86%|████████▋ | 173/200 [03:35<00:37,  1.38s/it]Running Inference:  87%|████████▋ | 174/200 [03:39<00:54,  2.11s/it]Running Inference:  88%|████████▊ | 175/200 [03:40<00:47,  1.89s/it]Running Inference:  88%|████████▊ | 176/200 [03:41<00:41,  1.71s/it]Running Inference:  88%|████████▊ | 177/200 [03:42<00:30,  1.33s/it]Running Inference:  89%|████████▉ | 178/200 [03:43<00:28,  1.28s/it]Running Inference:  90%|████████▉ | 179/200 [03:43<00:21,  1.02s/it]Running Inference:  90%|█████████ | 180/200 [03:44<00:16,  1.22it/s]Running Inference:  90%|█████████ | 181/200 [03:45<00:15,  1.24it/s]Running Inference:  91%|█████████ | 182/200 [03:46<00:17,  1.01it/s]Running Inference:  92%|█████████▏| 183/200 [03:51<00:35,  2.10s/it]Running Inference:  92%|█████████▏| 184/200 [03:55<00:43,  2.73s/it]Running Inference:  92%|█████████▎| 185/200 [03:56<00:32,  2.18s/it]Running Inference:  93%|█████████▎| 186/200 [03:57<00:25,  1.80s/it]Running Inference:  94%|█████████▎| 187/200 [03:58<00:19,  1.54s/it]Running Inference:  94%|█████████▍| 188/200 [03:58<00:15,  1.25s/it]Running Inference:  94%|█████████▍| 189/200 [03:59<00:11,  1.09s/it]Running Inference:  95%|█████████▌| 190/200 [04:03<00:19,  1.96s/it]Running Inference:  96%|█████████▌| 191/200 [04:04<00:14,  1.58s/it]Running Inference:  96%|█████████▌| 192/200 [04:05<00:11,  1.48s/it]Running Inference:  96%|█████████▋| 193/200 [04:06<00:09,  1.43s/it]Running Inference:  97%|█████████▋| 194/200 [04:07<00:07,  1.33s/it]Running Inference:  98%|█████████▊| 195/200 [04:08<00:05,  1.11s/it]Running Inference:  98%|█████████▊| 196/200 [04:08<00:03,  1.05it/s]Running Inference:  98%|█████████▊| 197/200 [04:10<00:03,  1.02s/it]Running Inference:  99%|█████████▉| 198/200 [04:10<00:01,  1.03it/s]Running Inference: 100%|█████████▉| 199/200 [04:12<00:01,  1.13s/it]Running Inference: 100%|██████████| 200/200 [04:13<00:00,  1.18s/it]Running Inference: 100%|██████████| 200/200 [04:13<00:00,  1.27s/it]
2025-12-14 18:40:13,193 - INFO - Inference completed.
2025-12-14 18:40:13,214 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 18:40:13,214 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:40:13,215 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 18:40:13,215 - INFO - Metrics:
34.5
2025-12-14 18:40:13,217 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=trec, ratio=0.2) on GPU 4

----------------------------------------
Task: trec | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:40:19,743 - INFO - Set deterministic seeds to 42
2025-12-14 18:40:19,743 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:40:19,743 - INFO - Starting evaluation run...
2025-12-14 18:40:19,743 - INFO - Output directory set to: longbenchresult
2025-12-14 18:40:19,744 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 18:40:19,744 - INFO - KV Press 'knorm' setup.
2025-12-14 18:40:19,744 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:40:19,744 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.78it/s]
Device set to use cuda:0
2025-12-14 18:40:32,051 - INFO - Model pipeline loaded.
2025-12-14 18:40:32,051 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 18:40:38,303 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:40:38,304 - INFO - Dataset processed with 200 entries.
2025-12-14 18:40:38,316 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:58,  1.50s/it]Running Inference:   1%|          | 2/200 [00:02<03:22,  1.02s/it]Running Inference:   2%|▏         | 3/200 [00:03<03:18,  1.01s/it]Running Inference:   2%|▏         | 4/200 [00:03<02:56,  1.11it/s]Running Inference:   2%|▎         | 5/200 [00:05<03:26,  1.06s/it]Running Inference:   3%|▎         | 6/200 [00:06<03:07,  1.03it/s]Running Inference:   4%|▎         | 7/200 [00:06<02:51,  1.13it/s]Running Inference:   4%|▍         | 8/200 [00:07<02:25,  1.32it/s]Running Inference:   4%|▍         | 9/200 [00:11<05:36,  1.76s/it]Running Inference:   5%|▌         | 10/200 [00:12<04:58,  1.57s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:13<04:04,  1.29s/it]Running Inference:   6%|▌         | 12/200 [00:14<04:00,  1.28s/it]Running Inference:   6%|▋         | 13/200 [00:14<03:22,  1.08s/it]Running Inference:   7%|▋         | 14/200 [00:16<03:26,  1.11s/it]Running Inference:   8%|▊         | 15/200 [00:16<03:05,  1.00s/it]Running Inference:   8%|▊         | 16/200 [00:17<02:58,  1.03it/s]Running Inference:   8%|▊         | 17/200 [00:19<03:22,  1.11s/it]Running Inference:   9%|▉         | 18/200 [00:19<03:01,  1.00it/s]Running Inference:  10%|▉         | 19/200 [00:24<06:11,  2.05s/it]Running Inference:  10%|█         | 20/200 [00:26<05:56,  1.98s/it]Running Inference:  10%|█         | 21/200 [00:26<04:45,  1.59s/it]Running Inference:  11%|█         | 22/200 [00:30<06:55,  2.33s/it]Running Inference:  12%|█▏        | 23/200 [00:31<05:12,  1.77s/it]Running Inference:  12%|█▏        | 24/200 [00:32<04:41,  1.60s/it]Running Inference:  12%|█▎        | 25/200 [00:33<04:27,  1.53s/it]Running Inference:  13%|█▎        | 26/200 [00:35<04:03,  1.40s/it]Running Inference:  14%|█▎        | 27/200 [00:36<03:52,  1.34s/it]Running Inference:  14%|█▍        | 28/200 [00:37<03:54,  1.37s/it]Running Inference:  14%|█▍        | 29/200 [00:38<03:12,  1.13s/it]Running Inference:  15%|█▌        | 30/200 [00:39<03:26,  1.22s/it]Running Inference:  16%|█▌        | 31/200 [00:41<03:32,  1.26s/it]Running Inference:  16%|█▌        | 32/200 [00:45<06:04,  2.17s/it]Running Inference:  16%|█▋        | 33/200 [00:45<04:36,  1.65s/it]Running Inference:  17%|█▋        | 34/200 [00:46<03:37,  1.31s/it]Running Inference:  18%|█▊        | 35/200 [00:50<06:06,  2.22s/it]Running Inference:  18%|█▊        | 36/200 [00:54<07:32,  2.76s/it]Running Inference:  18%|█▊        | 37/200 [00:59<09:05,  3.35s/it]Running Inference:  19%|█▉        | 38/200 [01:00<07:27,  2.76s/it]Running Inference:  20%|█▉        | 39/200 [01:01<05:46,  2.15s/it]Running Inference:  20%|██        | 40/200 [01:02<04:35,  1.72s/it]Running Inference:  20%|██        | 41/200 [01:03<04:18,  1.62s/it]Running Inference:  21%|██        | 42/200 [01:04<03:24,  1.30s/it]Running Inference:  22%|██▏       | 43/200 [01:04<02:51,  1.09s/it]Running Inference:  22%|██▏       | 44/200 [01:05<02:38,  1.02s/it]Running Inference:  22%|██▎       | 45/200 [01:06<02:50,  1.10s/it]Running Inference:  23%|██▎       | 46/200 [01:07<02:20,  1.09it/s]Running Inference:  24%|██▎       | 47/200 [01:08<02:21,  1.08it/s]Running Inference:  24%|██▍       | 48/200 [01:08<01:56,  1.31it/s]Running Inference:  24%|██▍       | 49/200 [01:09<01:59,  1.26it/s]Running Inference:  25%|██▌       | 50/200 [01:09<01:40,  1.49it/s]Running Inference:  26%|██▌       | 51/200 [01:10<01:46,  1.41it/s]Running Inference:  26%|██▌       | 52/200 [01:11<01:43,  1.43it/s]Running Inference:  26%|██▋       | 53/200 [01:15<04:01,  1.64s/it]Running Inference:  27%|██▋       | 54/200 [01:19<05:38,  2.32s/it]Running Inference:  28%|██▊       | 55/200 [01:19<04:12,  1.74s/it]Running Inference:  28%|██▊       | 56/200 [01:20<03:38,  1.51s/it]Running Inference:  28%|██▊       | 57/200 [01:21<02:53,  1.21s/it]Running Inference:  29%|██▉       | 58/200 [01:22<02:51,  1.21s/it]Running Inference:  30%|██▉       | 59/200 [01:23<02:37,  1.12s/it]Running Inference:  30%|███       | 60/200 [01:23<02:09,  1.08it/s]Running Inference:  30%|███       | 61/200 [01:24<02:25,  1.05s/it]Running Inference:  31%|███       | 62/200 [01:25<02:06,  1.09it/s]Running Inference:  32%|███▏      | 63/200 [01:26<02:07,  1.07it/s]Running Inference:  32%|███▏      | 64/200 [01:27<02:01,  1.12it/s]Running Inference:  32%|███▎      | 65/200 [01:27<01:47,  1.26it/s]Running Inference:  33%|███▎      | 66/200 [01:28<01:50,  1.21it/s]Running Inference:  34%|███▎      | 67/200 [01:29<01:38,  1.34it/s]Running Inference:  34%|███▍      | 68/200 [01:29<01:32,  1.42it/s]Running Inference:  34%|███▍      | 69/200 [01:30<01:35,  1.37it/s]Running Inference:  35%|███▌      | 70/200 [01:31<01:44,  1.25it/s]Running Inference:  36%|███▌      | 71/200 [01:33<02:01,  1.06it/s]Running Inference:  36%|███▌      | 72/200 [01:37<04:26,  2.08s/it]Running Inference:  36%|███▋      | 73/200 [01:38<03:38,  1.72s/it]Running Inference:  37%|███▋      | 74/200 [01:42<04:56,  2.36s/it]Running Inference:  38%|███▊      | 75/200 [01:43<03:50,  1.84s/it]Running Inference:  38%|███▊      | 76/200 [01:44<03:15,  1.57s/it]Running Inference:  38%|███▊      | 77/200 [01:44<02:47,  1.36s/it]Running Inference:  39%|███▉      | 78/200 [01:46<02:39,  1.31s/it]Running Inference:  40%|███▉      | 79/200 [01:46<02:11,  1.09s/it]Running Inference:  40%|████      | 80/200 [01:47<01:59,  1.00it/s]Running Inference:  40%|████      | 81/200 [01:48<02:02,  1.03s/it]Running Inference:  41%|████      | 82/200 [01:49<01:59,  1.01s/it]Running Inference:  42%|████▏     | 83/200 [01:50<01:58,  1.01s/it]Running Inference:  42%|████▏     | 84/200 [01:51<02:04,  1.07s/it]Running Inference:  42%|████▎     | 85/200 [01:52<01:58,  1.03s/it]Running Inference:  43%|████▎     | 86/200 [01:53<01:48,  1.05it/s]Running Inference:  44%|████▎     | 87/200 [01:57<03:48,  2.02s/it]Running Inference:  44%|████▍     | 88/200 [02:02<04:54,  2.63s/it]Running Inference:  44%|████▍     | 89/200 [02:06<06:04,  3.29s/it]Running Inference:  45%|████▌     | 90/200 [02:07<04:39,  2.54s/it]Running Inference:  46%|████▌     | 91/200 [02:08<03:40,  2.02s/it]Running Inference:  46%|████▌     | 92/200 [02:09<03:10,  1.76s/it]Running Inference:  46%|████▋     | 93/200 [02:14<04:34,  2.57s/it]Running Inference:  47%|████▋     | 94/200 [02:15<03:47,  2.14s/it]Running Inference:  48%|████▊     | 95/200 [02:15<02:52,  1.64s/it]Running Inference:  48%|████▊     | 96/200 [02:20<04:16,  2.46s/it]Running Inference:  48%|████▊     | 97/200 [02:21<03:49,  2.23s/it]Running Inference:  49%|████▉     | 98/200 [02:22<03:03,  1.80s/it]Running Inference:  50%|████▉     | 99/200 [02:27<04:25,  2.63s/it]Running Inference:  50%|█████     | 100/200 [02:28<03:47,  2.27s/it]Running Inference:  50%|█████     | 101/200 [02:29<03:01,  1.83s/it]Running Inference:  51%|█████     | 102/200 [02:29<02:21,  1.45s/it]Running Inference:  52%|█████▏    | 103/200 [02:31<02:13,  1.37s/it]Running Inference:  52%|█████▏    | 104/200 [02:32<01:58,  1.23s/it]Running Inference:  52%|█████▎    | 105/200 [02:33<01:54,  1.21s/it]Running Inference:  53%|█████▎    | 106/200 [02:34<01:51,  1.19s/it]Running Inference:  54%|█████▎    | 107/200 [02:35<01:52,  1.21s/it]Running Inference:  54%|█████▍    | 108/200 [02:36<01:53,  1.24s/it]Running Inference:  55%|█████▍    | 109/200 [02:37<01:43,  1.14s/it]Running Inference:  55%|█████▌    | 110/200 [02:41<03:04,  2.05s/it]Running Inference:  56%|█████▌    | 111/200 [02:42<02:35,  1.75s/it]Running Inference:  56%|█████▌    | 112/200 [02:44<02:17,  1.56s/it]Running Inference:  56%|█████▋    | 113/200 [02:45<01:58,  1.37s/it]Running Inference:  57%|█████▋    | 114/200 [02:45<01:43,  1.20s/it]Running Inference:  57%|█████▊    | 115/200 [02:46<01:29,  1.05s/it]Running Inference:  58%|█████▊    | 116/200 [02:47<01:18,  1.08it/s]Running Inference:  58%|█████▊    | 117/200 [02:48<01:29,  1.07s/it]Running Inference:  59%|█████▉    | 118/200 [02:49<01:30,  1.10s/it]Running Inference:  60%|█████▉    | 119/200 [02:51<01:33,  1.16s/it]Running Inference:  60%|██████    | 120/200 [02:52<01:41,  1.27s/it]Running Inference:  60%|██████    | 121/200 [02:52<01:19,  1.00s/it]Running Inference:  61%|██████    | 122/200 [02:54<01:28,  1.14s/it]Running Inference:  62%|██████▏   | 123/200 [02:55<01:24,  1.09s/it]Running Inference:  62%|██████▏   | 124/200 [02:56<01:26,  1.14s/it]Running Inference:  62%|██████▎   | 125/200 [02:58<01:30,  1.21s/it]Running Inference:  63%|██████▎   | 126/200 [02:58<01:11,  1.03it/s]Running Inference:  64%|██████▎   | 127/200 [02:59<01:12,  1.00it/s]Running Inference:  64%|██████▍   | 128/200 [03:00<01:01,  1.16it/s]Running Inference:  64%|██████▍   | 129/200 [03:01<01:09,  1.02it/s]Running Inference:  65%|██████▌   | 130/200 [03:06<02:26,  2.09s/it]Running Inference:  66%|██████▌   | 131/200 [03:06<02:00,  1.74s/it]Running Inference:  66%|██████▌   | 132/200 [03:08<01:49,  1.61s/it]Running Inference:  66%|██████▋   | 133/200 [03:09<01:43,  1.54s/it]Running Inference:  67%|██████▋   | 134/200 [03:09<01:17,  1.17s/it]Running Inference:  68%|██████▊   | 135/200 [03:10<01:01,  1.05it/s]Running Inference:  68%|██████▊   | 136/200 [03:11<00:56,  1.13it/s]Running Inference:  68%|██████▊   | 137/200 [03:12<01:09,  1.10s/it]Running Inference:  69%|██████▉   | 138/200 [03:13<01:00,  1.03it/s]Running Inference:  70%|██████▉   | 139/200 [03:14<00:59,  1.03it/s]Running Inference:  70%|███████   | 140/200 [03:15<01:05,  1.09s/it]Running Inference:  70%|███████   | 141/200 [03:20<02:01,  2.06s/it]Running Inference:  71%|███████   | 142/200 [03:21<01:41,  1.75s/it]Running Inference:  72%|███████▏  | 143/200 [03:22<01:33,  1.65s/it]Running Inference:  72%|███████▏  | 144/200 [03:23<01:17,  1.38s/it]Running Inference:  72%|███████▎  | 145/200 [03:27<01:56,  2.13s/it]Running Inference:  73%|███████▎  | 146/200 [03:28<01:43,  1.91s/it]Running Inference:  74%|███████▎  | 147/200 [03:29<01:21,  1.54s/it]Running Inference:  74%|███████▍  | 148/200 [03:29<01:06,  1.27s/it]Running Inference:  74%|███████▍  | 149/200 [03:30<00:54,  1.07s/it]Running Inference:  75%|███████▌  | 150/200 [03:31<00:50,  1.02s/it]Running Inference:  76%|███████▌  | 151/200 [03:32<00:56,  1.16s/it]Running Inference:  76%|███████▌  | 152/200 [03:34<00:57,  1.19s/it]Running Inference:  76%|███████▋  | 153/200 [03:34<00:50,  1.07s/it]Running Inference:  77%|███████▋  | 154/200 [03:36<00:52,  1.14s/it]Running Inference:  78%|███████▊  | 155/200 [03:36<00:44,  1.01it/s]Running Inference:  78%|███████▊  | 156/200 [03:37<00:43,  1.00it/s]Running Inference:  78%|███████▊  | 157/200 [03:39<00:47,  1.12s/it]Running Inference:  79%|███████▉  | 158/200 [03:40<00:49,  1.18s/it]Running Inference:  80%|███████▉  | 159/200 [03:41<00:47,  1.15s/it]Running Inference:  80%|████████  | 160/200 [03:42<00:40,  1.00s/it]Running Inference:  80%|████████  | 161/200 [03:42<00:35,  1.09it/s]Running Inference:  81%|████████  | 162/200 [03:43<00:33,  1.14it/s]Running Inference:  82%|████████▏ | 163/200 [03:44<00:34,  1.08it/s]Running Inference:  82%|████████▏ | 164/200 [03:46<00:39,  1.10s/it]Running Inference:  82%|████████▎ | 165/200 [03:47<00:38,  1.11s/it]Running Inference:  83%|████████▎ | 166/200 [03:47<00:31,  1.07it/s]Running Inference:  84%|████████▎ | 167/200 [03:48<00:30,  1.07it/s]Running Inference:  84%|████████▍ | 168/200 [03:50<00:31,  1.01it/s]Running Inference:  84%|████████▍ | 169/200 [03:50<00:25,  1.21it/s]Running Inference:  85%|████████▌ | 170/200 [03:51<00:22,  1.32it/s]Running Inference:  86%|████████▌ | 171/200 [03:51<00:17,  1.61it/s]Running Inference:  86%|████████▌ | 172/200 [03:52<00:20,  1.35it/s]Running Inference:  86%|████████▋ | 173/200 [03:52<00:18,  1.44it/s]Running Inference:  87%|████████▋ | 174/200 [03:56<00:42,  1.65s/it]Running Inference:  88%|████████▊ | 175/200 [03:58<00:39,  1.56s/it]Running Inference:  88%|████████▊ | 176/200 [03:59<00:35,  1.48s/it]Running Inference:  88%|████████▊ | 177/200 [03:59<00:26,  1.17s/it]Running Inference:  89%|████████▉ | 178/200 [04:01<00:25,  1.17s/it]Running Inference:  90%|████████▉ | 179/200 [04:04<00:41,  1.99s/it]Running Inference:  90%|█████████ | 180/200 [04:05<00:29,  1.49s/it]Running Inference:  90%|█████████ | 181/200 [04:06<00:24,  1.28s/it]Running Inference:  91%|█████████ | 182/200 [04:07<00:22,  1.24s/it]Running Inference:  92%|█████████▏| 183/200 [04:08<00:23,  1.38s/it]Running Inference:  92%|█████████▏| 184/200 [04:13<00:35,  2.24s/it]Running Inference:  92%|█████████▎| 185/200 [04:14<00:27,  1.84s/it]Running Inference:  93%|█████████▎| 186/200 [04:15<00:21,  1.55s/it]Running Inference:  94%|█████████▎| 187/200 [04:15<00:17,  1.37s/it]Running Inference:  94%|█████████▍| 188/200 [04:16<00:13,  1.13s/it]Running Inference:  94%|█████████▍| 189/200 [04:17<00:11,  1.00s/it]Running Inference:  95%|█████████▌| 190/200 [04:21<00:19,  1.91s/it]Running Inference:  96%|█████████▌| 191/200 [04:21<00:13,  1.54s/it]Running Inference:  96%|█████████▌| 192/200 [04:23<00:11,  1.45s/it]Running Inference:  96%|█████████▋| 193/200 [04:24<00:09,  1.41s/it]Running Inference:  97%|█████████▋| 194/200 [04:25<00:07,  1.33s/it]Running Inference:  98%|█████████▊| 195/200 [04:26<00:05,  1.11s/it]Running Inference:  98%|█████████▊| 196/200 [04:30<00:07,  1.96s/it]Running Inference:  98%|█████████▊| 197/200 [04:31<00:05,  1.73s/it]Running Inference:  99%|█████████▉| 198/200 [04:32<00:02,  1.47s/it]Running Inference: 100%|█████████▉| 199/200 [04:33<00:01,  1.48s/it]Running Inference: 100%|██████████| 200/200 [04:34<00:00,  1.34s/it]Running Inference: 100%|██████████| 200/200 [04:34<00:00,  1.37s/it]
2025-12-14 18:45:13,092 - INFO - Inference completed.
2025-12-14 18:45:13,113 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 18:45:13,113 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:45:13,114 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 18:45:13,114 - INFO - Metrics:
32.5
2025-12-14 18:45:13,116 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=trec, ratio=0.3) on GPU 4

----------------------------------------
Task: trec | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:45:19,639 - INFO - Set deterministic seeds to 42
2025-12-14 18:45:19,639 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:45:19,639 - INFO - Starting evaluation run...
2025-12-14 18:45:19,640 - INFO - Output directory set to: longbenchresult
2025-12-14 18:45:19,640 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 18:45:19,640 - INFO - KV Press 'knorm' setup.
2025-12-14 18:45:19,640 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:45:19,640 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.92it/s]
Device set to use cuda:0
2025-12-14 18:45:36,039 - INFO - Model pipeline loaded.
2025-12-14 18:45:36,040 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 18:45:41,640 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:45:41,641 - INFO - Dataset processed with 200 entries.
2025-12-14 18:45:41,654 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<16:07,  4.86s/it]Running Inference:   1%|          | 2/200 [00:06<09:51,  2.99s/it]Running Inference:   2%|▏         | 3/200 [00:07<06:47,  2.07s/it]Running Inference:   2%|▏         | 4/200 [00:08<05:01,  1.54s/it]Running Inference:   2%|▎         | 5/200 [00:09<04:52,  1.50s/it]Running Inference:   3%|▎         | 6/200 [00:10<04:07,  1.27s/it]Running Inference:   4%|▎         | 7/200 [00:11<03:30,  1.09s/it]Running Inference:   4%|▍         | 8/200 [00:15<06:15,  1.96s/it]Running Inference:   4%|▍         | 9/200 [00:15<04:49,  1.52s/it]Running Inference:   5%|▌         | 10/200 [00:19<07:36,  2.40s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:59,  1.90s/it]Running Inference:   6%|▌         | 12/200 [00:22<05:24,  1.73s/it]Running Inference:   6%|▋         | 13/200 [00:22<04:04,  1.30s/it]Running Inference:   7%|▋         | 14/200 [00:23<04:00,  1.29s/it]Running Inference:   8%|▊         | 15/200 [00:24<03:28,  1.13s/it]Running Inference:   8%|▊         | 16/200 [00:25<03:14,  1.06s/it]Running Inference:   8%|▊         | 17/200 [00:30<06:37,  2.17s/it]Running Inference:   9%|▉         | 18/200 [00:34<08:15,  2.72s/it]Running Inference:  10%|▉         | 19/200 [00:38<09:49,  3.26s/it]Running Inference:  10%|█         | 20/200 [00:40<08:27,  2.82s/it]Running Inference:  10%|█         | 21/200 [00:41<06:34,  2.20s/it]Running Inference:  11%|█         | 22/200 [00:45<08:10,  2.76s/it]Running Inference:  12%|█▏        | 23/200 [00:45<06:05,  2.06s/it]Running Inference:  12%|█▏        | 24/200 [00:46<05:17,  1.80s/it]Running Inference:  12%|█▎        | 25/200 [00:48<04:52,  1.67s/it]Running Inference:  13%|█▎        | 26/200 [00:49<04:20,  1.50s/it]Running Inference:  14%|█▎        | 27/200 [00:50<04:04,  1.41s/it]Running Inference:  14%|█▍        | 28/200 [00:51<04:05,  1.43s/it]Running Inference:  14%|█▍        | 29/200 [00:52<03:26,  1.21s/it]Running Inference:  15%|█▌        | 30/200 [00:54<03:42,  1.31s/it]Running Inference:  16%|█▌        | 31/200 [00:58<06:32,  2.32s/it]Running Inference:  16%|█▌        | 32/200 [01:03<08:09,  2.91s/it]Running Inference:  16%|█▋        | 33/200 [01:03<06:20,  2.28s/it]Running Inference:  17%|█▋        | 34/200 [01:08<07:46,  2.81s/it]Running Inference:  18%|█▊        | 35/200 [01:12<09:00,  3.28s/it]Running Inference:  18%|█▊        | 36/200 [01:16<09:31,  3.49s/it]Running Inference:  18%|█▊        | 37/200 [01:21<10:28,  3.86s/it]Running Inference:  19%|█▉        | 38/200 [01:22<08:16,  3.07s/it]Running Inference:  20%|█▉        | 39/200 [01:23<06:20,  2.37s/it]Running Inference:  20%|██        | 40/200 [01:23<04:59,  1.87s/it]Running Inference:  20%|██        | 41/200 [01:25<04:35,  1.73s/it]Running Inference:  21%|██        | 42/200 [01:29<06:23,  2.42s/it]Running Inference:  22%|██▏       | 43/200 [01:29<04:48,  1.84s/it]Running Inference:  22%|██▏       | 44/200 [01:30<04:00,  1.54s/it]Running Inference:  22%|██▎       | 45/200 [01:32<03:55,  1.52s/it]Running Inference:  23%|██▎       | 46/200 [01:32<03:07,  1.22s/it]Running Inference:  24%|██▎       | 47/200 [01:33<02:53,  1.13s/it]Running Inference:  24%|██▍       | 48/200 [01:34<02:26,  1.04it/s]Running Inference:  24%|██▍       | 49/200 [01:34<02:22,  1.06it/s]Running Inference:  25%|██▌       | 50/200 [01:35<02:00,  1.25it/s]Running Inference:  26%|██▌       | 51/200 [01:36<01:59,  1.25it/s]Running Inference:  26%|██▌       | 52/200 [01:36<01:56,  1.27it/s]Running Inference:  26%|██▋       | 53/200 [01:40<04:10,  1.70s/it]Running Inference:  27%|██▋       | 54/200 [01:41<03:17,  1.35s/it]Running Inference:  28%|██▊       | 55/200 [01:41<02:38,  1.09s/it]Running Inference:  28%|██▊       | 56/200 [01:42<02:32,  1.06s/it]Running Inference:  28%|██▊       | 57/200 [01:43<02:07,  1.12it/s]Running Inference:  29%|██▉       | 58/200 [01:47<04:45,  2.01s/it]Running Inference:  30%|██▉       | 59/200 [01:48<03:56,  1.68s/it]Running Inference:  30%|███       | 60/200 [01:49<03:04,  1.31s/it]Running Inference:  30%|███       | 61/200 [01:50<03:03,  1.32s/it]Running Inference:  31%|███       | 62/200 [01:51<02:32,  1.11s/it]Running Inference:  32%|███▏      | 63/200 [01:52<02:25,  1.06s/it]Running Inference:  32%|███▏      | 64/200 [01:53<02:15,  1.00it/s]Running Inference:  32%|███▎      | 65/200 [01:53<01:56,  1.16it/s]Running Inference:  33%|███▎      | 66/200 [01:57<04:06,  1.84s/it]Running Inference:  34%|███▎      | 67/200 [01:58<03:13,  1.45s/it]Running Inference:  34%|███▍      | 68/200 [01:58<02:37,  1.20s/it]Running Inference:  34%|███▍      | 69/200 [01:59<02:20,  1.07s/it]Running Inference:  35%|███▌      | 70/200 [02:00<02:18,  1.07s/it]Running Inference:  36%|███▌      | 71/200 [02:01<02:23,  1.12s/it]Running Inference:  36%|███▌      | 72/200 [02:03<02:31,  1.18s/it]Running Inference:  36%|███▋      | 73/200 [02:04<02:18,  1.09s/it]Running Inference:  37%|███▋      | 74/200 [02:04<01:49,  1.15it/s]Running Inference:  38%|███▊      | 75/200 [02:05<01:40,  1.24it/s]Running Inference:  38%|███▊      | 76/200 [02:06<01:44,  1.18it/s]Running Inference:  38%|███▊      | 77/200 [02:06<01:44,  1.18it/s]Running Inference:  39%|███▉      | 78/200 [02:08<02:02,  1.00s/it]Running Inference:  40%|███▉      | 79/200 [02:08<01:45,  1.14it/s]Running Inference:  40%|████      | 80/200 [02:09<01:41,  1.18it/s]Running Inference:  40%|████      | 81/200 [02:10<01:49,  1.09it/s]Running Inference:  41%|████      | 82/200 [02:15<03:54,  1.98s/it]Running Inference:  42%|████▏     | 83/200 [02:16<03:18,  1.69s/it]Running Inference:  42%|████▏     | 84/200 [02:17<02:59,  1.55s/it]Running Inference:  42%|████▎     | 85/200 [02:18<02:39,  1.39s/it]Running Inference:  43%|████▎     | 86/200 [02:19<02:18,  1.21s/it]Running Inference:  44%|████▎     | 87/200 [02:23<04:08,  2.20s/it]Running Inference:  44%|████▍     | 88/200 [02:27<05:08,  2.75s/it]Running Inference:  44%|████▍     | 89/200 [02:32<06:14,  3.37s/it]Running Inference:  45%|████▌     | 90/200 [02:33<04:47,  2.61s/it]Running Inference:  46%|████▌     | 91/200 [02:34<03:45,  2.07s/it]Running Inference:  46%|████▌     | 92/200 [02:35<03:13,  1.79s/it]Running Inference:  46%|████▋     | 93/200 [02:39<04:36,  2.59s/it]Running Inference:  47%|████▋     | 94/200 [02:40<03:47,  2.14s/it]Running Inference:  48%|████▊     | 95/200 [02:41<02:52,  1.64s/it]Running Inference:  48%|████▊     | 96/200 [02:45<04:15,  2.46s/it]Running Inference:  48%|████▊     | 97/200 [02:47<03:48,  2.22s/it]Running Inference:  49%|████▉     | 98/200 [02:48<03:02,  1.79s/it]Running Inference:  50%|████▉     | 99/200 [02:52<04:25,  2.63s/it]Running Inference:  50%|█████     | 100/200 [02:54<03:41,  2.22s/it]Running Inference:  50%|█████     | 101/200 [02:54<02:57,  1.79s/it]Running Inference:  51%|█████     | 102/200 [02:55<02:19,  1.42s/it]Running Inference:  52%|█████▏    | 103/200 [02:56<02:11,  1.35s/it]Running Inference:  52%|█████▏    | 104/200 [02:57<01:56,  1.22s/it]Running Inference:  52%|█████▎    | 105/200 [02:58<01:53,  1.20s/it]Running Inference:  53%|█████▎    | 106/200 [02:59<01:50,  1.18s/it]Running Inference:  54%|█████▎    | 107/200 [03:00<01:47,  1.16s/it]Running Inference:  54%|█████▍    | 108/200 [03:02<01:48,  1.18s/it]Running Inference:  55%|█████▍    | 109/200 [03:06<03:14,  2.14s/it]Running Inference:  55%|█████▌    | 110/200 [03:07<02:34,  1.72s/it]Running Inference:  56%|█████▌    | 111/200 [03:11<03:31,  2.37s/it]Running Inference:  56%|█████▌    | 112/200 [03:12<02:55,  2.00s/it]Running Inference:  56%|█████▋    | 113/200 [03:13<02:25,  1.67s/it]Running Inference:  57%|█████▋    | 114/200 [03:14<02:01,  1.42s/it]Running Inference:  57%|█████▊    | 115/200 [03:14<01:42,  1.20s/it]Running Inference:  58%|█████▊    | 116/200 [03:18<02:54,  2.08s/it]Running Inference:  58%|█████▊    | 117/200 [03:23<04:01,  2.91s/it]Running Inference:  59%|█████▉    | 118/200 [03:28<04:39,  3.41s/it]Running Inference:  60%|█████▉    | 119/200 [03:32<05:04,  3.76s/it]Running Inference:  60%|██████    | 120/200 [03:34<04:07,  3.09s/it]Running Inference:  60%|██████    | 121/200 [03:34<02:59,  2.27s/it]Running Inference:  61%|██████    | 122/200 [03:36<02:37,  2.02s/it]Running Inference:  62%|██████▏   | 123/200 [03:37<02:12,  1.71s/it]Running Inference:  62%|██████▏   | 124/200 [03:38<01:59,  1.57s/it]Running Inference:  62%|██████▎   | 125/200 [03:39<01:53,  1.51s/it]Running Inference:  63%|██████▎   | 126/200 [03:43<02:45,  2.23s/it]Running Inference:  64%|██████▎   | 127/200 [03:44<02:18,  1.90s/it]Running Inference:  64%|██████▍   | 128/200 [03:45<01:48,  1.50s/it]Running Inference:  64%|██████▍   | 129/200 [03:46<01:41,  1.43s/it]Running Inference:  65%|██████▌   | 130/200 [03:51<02:48,  2.40s/it]Running Inference:  66%|██████▌   | 131/200 [03:52<02:15,  1.96s/it]Running Inference:  66%|██████▌   | 132/200 [03:53<01:59,  1.76s/it]Running Inference:  66%|██████▋   | 133/200 [03:54<01:50,  1.65s/it]Running Inference:  67%|██████▋   | 134/200 [03:55<01:22,  1.24s/it]Running Inference:  68%|██████▊   | 135/200 [03:59<02:12,  2.04s/it]Running Inference:  68%|██████▊   | 136/200 [03:59<01:46,  1.66s/it]Running Inference:  68%|██████▊   | 137/200 [04:01<01:43,  1.64s/it]Running Inference:  69%|██████▉   | 138/200 [04:05<02:23,  2.31s/it]Running Inference:  70%|██████▉   | 139/200 [04:06<01:56,  1.91s/it]Running Inference:  70%|███████   | 140/200 [04:07<01:44,  1.75s/it]Running Inference:  70%|███████   | 141/200 [04:12<02:28,  2.52s/it]Running Inference:  71%|███████   | 142/200 [04:13<02:00,  2.08s/it]Running Inference:  72%|███████▏  | 143/200 [04:14<01:46,  1.88s/it]Running Inference:  72%|███████▏  | 144/200 [04:15<01:28,  1.57s/it]Running Inference:  72%|███████▎  | 145/200 [04:19<02:04,  2.27s/it]Running Inference:  73%|███████▎  | 146/200 [04:24<02:43,  3.03s/it]Running Inference:  74%|███████▎  | 147/200 [04:24<02:03,  2.33s/it]Running Inference:  74%|███████▍  | 148/200 [04:25<01:35,  1.83s/it]Running Inference:  74%|███████▍  | 149/200 [04:26<01:14,  1.46s/it]Running Inference:  75%|███████▌  | 150/200 [04:30<01:54,  2.29s/it]Running Inference:  76%|███████▌  | 151/200 [04:31<01:40,  2.05s/it]Running Inference:  76%|███████▌  | 152/200 [04:36<02:15,  2.82s/it]Running Inference:  76%|███████▋  | 153/200 [04:37<01:43,  2.21s/it]Running Inference:  77%|███████▋  | 154/200 [04:38<01:28,  1.93s/it]Running Inference:  78%|███████▊  | 155/200 [04:39<01:09,  1.55s/it]Running Inference:  78%|███████▊  | 156/200 [04:40<01:00,  1.38s/it]Running Inference:  78%|███████▊  | 157/200 [04:41<00:59,  1.38s/it]Running Inference:  79%|███████▉  | 158/200 [04:42<00:57,  1.36s/it]Running Inference:  80%|███████▉  | 159/200 [04:47<01:35,  2.32s/it]Running Inference:  80%|████████  | 160/200 [04:47<01:10,  1.77s/it]Running Inference:  80%|████████  | 161/200 [04:48<00:56,  1.46s/it]Running Inference:  81%|████████  | 162/200 [04:49<00:47,  1.25s/it]Running Inference:  82%|████████▏ | 163/200 [04:50<00:43,  1.19s/it]Running Inference:  82%|████████▏ | 164/200 [04:51<00:46,  1.28s/it]Running Inference:  82%|████████▎ | 165/200 [04:53<00:43,  1.24s/it]Running Inference:  83%|████████▎ | 166/200 [04:53<00:35,  1.04s/it]Running Inference:  84%|████████▎ | 167/200 [04:54<00:33,  1.00s/it]Running Inference:  84%|████████▍ | 168/200 [04:55<00:33,  1.04s/it]Running Inference:  84%|████████▍ | 169/200 [04:56<00:26,  1.17it/s]Running Inference:  85%|████████▌ | 170/200 [05:00<00:53,  1.79s/it]Running Inference:  86%|████████▌ | 171/200 [05:03<01:09,  2.40s/it]Running Inference:  86%|████████▌ | 172/200 [05:08<01:25,  3.04s/it]Running Inference:  86%|████████▋ | 173/200 [05:08<01:02,  2.30s/it]Running Inference:  87%|████████▋ | 174/200 [05:12<01:12,  2.77s/it]Running Inference:  88%|████████▊ | 175/200 [05:14<00:58,  2.35s/it]Running Inference:  88%|████████▊ | 176/200 [05:18<01:13,  3.07s/it]Running Inference:  88%|████████▊ | 177/200 [05:19<00:52,  2.28s/it]Running Inference:  89%|████████▉ | 178/200 [05:20<00:42,  1.95s/it]Running Inference:  90%|████████▉ | 179/200 [05:24<00:53,  2.53s/it]Running Inference:  90%|█████████ | 180/200 [05:28<00:58,  2.93s/it]Running Inference:  90%|█████████ | 181/200 [05:29<00:43,  2.28s/it]Running Inference:  91%|█████████ | 182/200 [05:33<00:53,  2.97s/it]Running Inference:  92%|█████████▏| 183/200 [05:38<00:59,  3.49s/it]Running Inference:  92%|█████████▏| 184/200 [05:42<00:59,  3.72s/it]Running Inference:  92%|█████████▎| 185/200 [05:43<00:43,  2.87s/it]Running Inference:  93%|█████████▎| 186/200 [05:44<00:31,  2.28s/it]Running Inference:  94%|█████████▎| 187/200 [05:45<00:24,  1.87s/it]Running Inference:  94%|█████████▍| 188/200 [05:49<00:30,  2.51s/it]Running Inference:  94%|█████████▍| 189/200 [05:50<00:21,  1.97s/it]Running Inference:  95%|█████████▌| 190/200 [05:54<00:25,  2.58s/it]Running Inference:  96%|█████████▌| 191/200 [05:54<00:18,  2.01s/it]Running Inference:  96%|█████████▌| 192/200 [05:59<00:22,  2.80s/it]Running Inference:  96%|█████████▋| 193/200 [06:00<00:16,  2.35s/it]Running Inference:  97%|█████████▋| 194/200 [06:01<00:12,  2.02s/it]Running Inference:  98%|█████████▊| 195/200 [06:05<00:12,  2.60s/it]Running Inference:  98%|█████████▊| 196/200 [06:09<00:11,  2.99s/it]Running Inference:  98%|█████████▊| 197/200 [06:14<00:10,  3.48s/it]Running Inference:  99%|█████████▉| 198/200 [06:15<00:05,  2.76s/it]Running Inference: 100%|█████████▉| 199/200 [06:20<00:03,  3.39s/it]Running Inference: 100%|██████████| 200/200 [06:21<00:00,  2.71s/it]Running Inference: 100%|██████████| 200/200 [06:21<00:00,  1.91s/it]
2025-12-14 18:52:03,109 - INFO - Inference completed.
2025-12-14 18:52:03,131 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 18:52:03,131 - INFO - Calculating metrics for dataset: longbench
2025-12-14 18:52:03,132 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 18:52:03,132 - INFO - Metrics:
23.5
2025-12-14 18:52:03,134 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=trec, ratio=0.5) on GPU 4


========================================
LongBench Task: dureader
========================================
----------------------------------------
Task: dureader | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:52:09,773 - INFO - Set deterministic seeds to 42
2025-12-14 18:52:09,774 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:52:09,774 - INFO - Starting evaluation run...
2025-12-14 18:52:09,774 - INFO - Output directory set to: longbenchresult
2025-12-14 18:52:09,774 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 18:52:09,774 - INFO - KV Press 'knorm' setup.
2025-12-14 18:52:09,774 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:52:09,774 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.32it/s]
Device set to use cuda:0
2025-12-14 18:52:29,606 - INFO - Model pipeline loaded.
2025-12-14 18:52:29,606 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 18:52:35,200 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:52:35,200 - INFO - Dataset processed with 200 entries.
2025-12-14 18:52:35,229 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:26,  8.87s/it]Running Inference:   1%|          | 2/200 [00:10<16:07,  4.89s/it]Running Inference:   2%|▏         | 3/200 [00:18<19:45,  6.02s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:30,  4.14s/it]Running Inference:   2%|▎         | 5/200 [00:21<11:03,  3.40s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:33,  4.81s/it]Running Inference:   4%|▎         | 7/200 [00:36<18:28,  5.74s/it]Running Inference:   4%|▍         | 8/200 [00:44<20:31,  6.41s/it]Running Inference:   4%|▍         | 9/200 [00:52<21:59,  6.91s/it]Running Inference:   5%|▌         | 10/200 [01:00<22:14,  7.02s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:07<22:13,  7.06s/it]Running Inference:   6%|▌         | 12/200 [01:14<22:17,  7.12s/it]Running Inference:   6%|▋         | 13/200 [01:21<22:16,  7.15s/it]Running Inference:   7%|▋         | 14/200 [01:29<22:26,  7.24s/it]Running Inference:   8%|▊         | 15/200 [01:36<22:22,  7.26s/it]Running Inference:   8%|▊         | 16/200 [01:43<22:04,  7.20s/it]Running Inference:   8%|▊         | 17/200 [01:50<21:58,  7.21s/it]Running Inference:   9%|▉         | 18/200 [01:58<22:19,  7.36s/it]Running Inference:  10%|▉         | 19/200 [02:05<22:11,  7.36s/it]Running Inference:  10%|█         | 20/200 [02:13<22:21,  7.45s/it]Running Inference:  10%|█         | 21/200 [02:21<22:31,  7.55s/it]Running Inference:  11%|█         | 22/200 [02:28<22:10,  7.47s/it]Running Inference:  12%|█▏        | 23/200 [02:35<21:46,  7.38s/it]Running Inference:  12%|█▏        | 24/200 [02:42<21:34,  7.36s/it]Running Inference:  12%|█▎        | 25/200 [02:49<21:07,  7.24s/it]Running Inference:  13%|█▎        | 26/200 [02:57<21:10,  7.30s/it]Running Inference:  14%|█▎        | 27/200 [03:04<21:06,  7.32s/it]Running Inference:  14%|█▍        | 28/200 [03:11<20:54,  7.29s/it]Running Inference:  14%|█▍        | 29/200 [03:19<21:03,  7.39s/it]Running Inference:  15%|█▌        | 30/200 [03:26<20:33,  7.26s/it]Running Inference:  16%|█▌        | 31/200 [03:33<20:36,  7.32s/it]Running Inference:  16%|█▌        | 32/200 [03:40<20:09,  7.20s/it]Running Inference:  16%|█▋        | 33/200 [03:48<20:10,  7.25s/it]Running Inference:  17%|█▋        | 34/200 [03:53<18:46,  6.79s/it]Running Inference:  18%|█▊        | 35/200 [04:01<18:58,  6.90s/it]Running Inference:  18%|█▊        | 36/200 [04:08<19:33,  7.16s/it]Running Inference:  18%|█▊        | 37/200 [04:16<19:57,  7.35s/it]Running Inference:  19%|█▉        | 38/200 [04:25<20:42,  7.67s/it]Running Inference:  20%|█▉        | 39/200 [04:32<20:33,  7.66s/it]Running Inference:  20%|██        | 40/200 [04:40<20:37,  7.74s/it]Running Inference:  20%|██        | 41/200 [04:45<18:04,  6.82s/it]Running Inference:  21%|██        | 42/200 [04:52<18:14,  6.92s/it]Running Inference:  22%|██▏       | 43/200 [05:00<18:45,  7.17s/it]Running Inference:  22%|██▏       | 44/200 [05:07<18:32,  7.13s/it]Running Inference:  22%|██▎       | 45/200 [05:15<18:59,  7.35s/it]Running Inference:  23%|██▎       | 46/200 [05:22<18:50,  7.34s/it]Running Inference:  24%|██▎       | 47/200 [05:29<18:44,  7.35s/it]Running Inference:  24%|██▍       | 48/200 [05:37<18:44,  7.40s/it]Running Inference:  24%|██▍       | 49/200 [05:40<15:17,  6.07s/it]Running Inference:  25%|██▌       | 50/200 [05:47<16:13,  6.49s/it]Running Inference:  26%|██▌       | 51/200 [05:55<16:51,  6.79s/it]Running Inference:  26%|██▌       | 52/200 [05:58<13:44,  5.57s/it]Running Inference:  26%|██▋       | 53/200 [06:05<15:09,  6.19s/it]Running Inference:  27%|██▋       | 54/200 [06:13<16:14,  6.67s/it]Running Inference:  28%|██▊       | 55/200 [06:20<16:23,  6.78s/it]Running Inference:  28%|██▊       | 56/200 [06:27<16:19,  6.80s/it]Running Inference:  28%|██▊       | 57/200 [06:34<16:22,  6.87s/it]Running Inference:  29%|██▉       | 58/200 [06:41<16:19,  6.90s/it]Running Inference:  30%|██▉       | 59/200 [06:44<13:52,  5.90s/it]Running Inference:  30%|███       | 60/200 [06:52<14:59,  6.43s/it]Running Inference:  30%|███       | 61/200 [07:00<15:59,  6.90s/it]Running Inference:  31%|███       | 62/200 [07:08<16:18,  7.09s/it]Running Inference:  32%|███▏      | 63/200 [07:15<16:40,  7.30s/it]Running Inference:  32%|███▏      | 64/200 [07:23<16:42,  7.37s/it]Running Inference:  32%|███▎      | 65/200 [07:31<16:51,  7.49s/it]Running Inference:  33%|███▎      | 66/200 [07:38<16:45,  7.50s/it]Running Inference:  34%|███▎      | 67/200 [07:46<16:48,  7.58s/it]Running Inference:  34%|███▍      | 68/200 [07:53<16:24,  7.46s/it]Running Inference:  34%|███▍      | 69/200 [08:01<16:30,  7.56s/it]Running Inference:  35%|███▌      | 70/200 [08:09<16:37,  7.68s/it]Running Inference:  36%|███▌      | 71/200 [08:16<16:26,  7.65s/it]Running Inference:  36%|███▌      | 72/200 [08:23<15:49,  7.42s/it]Running Inference:  36%|███▋      | 73/200 [08:31<16:02,  7.58s/it]Running Inference:  37%|███▋      | 74/200 [08:39<15:49,  7.54s/it]Running Inference:  38%|███▊      | 75/200 [08:46<15:30,  7.44s/it]Running Inference:  38%|███▊      | 76/200 [08:54<15:30,  7.51s/it]Running Inference:  38%|███▊      | 77/200 [09:01<15:25,  7.52s/it]Running Inference:  39%|███▉      | 78/200 [09:09<15:25,  7.59s/it]Running Inference:  40%|███▉      | 79/200 [09:16<15:10,  7.52s/it]Running Inference:  40%|████      | 80/200 [09:24<15:03,  7.53s/it]Running Inference:  40%|████      | 81/200 [09:31<14:41,  7.41s/it]Running Inference:  41%|████      | 82/200 [09:38<14:32,  7.39s/it]Running Inference:  42%|████▏     | 83/200 [09:45<14:15,  7.32s/it]Running Inference:  42%|████▏     | 84/200 [09:53<14:05,  7.29s/it]Running Inference:  42%|████▎     | 85/200 [10:00<13:59,  7.30s/it]Running Inference:  43%|████▎     | 86/200 [10:07<13:47,  7.26s/it]Running Inference:  44%|████▎     | 87/200 [10:14<13:32,  7.19s/it]Running Inference:  44%|████▍     | 88/200 [10:21<13:23,  7.17s/it]Running Inference:  44%|████▍     | 89/200 [10:29<13:24,  7.25s/it]Running Inference:  45%|████▌     | 90/200 [10:36<13:10,  7.18s/it]Running Inference:  46%|████▌     | 91/200 [10:43<13:11,  7.26s/it]Running Inference:  46%|████▌     | 92/200 [10:51<13:06,  7.28s/it]Running Inference:  46%|████▋     | 93/200 [10:58<13:12,  7.40s/it]Running Inference:  47%|████▋     | 94/200 [11:06<13:16,  7.52s/it]Running Inference:  48%|████▊     | 95/200 [11:13<13:06,  7.49s/it]Running Inference:  48%|████▊     | 96/200 [11:21<12:51,  7.42s/it]Running Inference:  48%|████▊     | 97/200 [11:28<12:26,  7.25s/it]Running Inference:  49%|████▉     | 98/200 [11:34<12:07,  7.13s/it]Running Inference:  50%|████▉     | 99/200 [11:42<12:12,  7.26s/it]Running Inference:  50%|█████     | 100/200 [11:49<12:00,  7.21s/it]Running Inference:  50%|█████     | 101/200 [11:56<11:53,  7.20s/it]Running Inference:  51%|█████     | 102/200 [12:03<11:46,  7.21s/it]Running Inference:  52%|█████▏    | 103/200 [12:11<11:46,  7.28s/it]Running Inference:  52%|█████▏    | 104/200 [12:18<11:38,  7.28s/it]Running Inference:  52%|█████▎    | 105/200 [12:25<11:30,  7.27s/it]Running Inference:  53%|█████▎    | 106/200 [12:33<11:24,  7.28s/it]Running Inference:  54%|█████▎    | 107/200 [12:40<11:23,  7.35s/it]Running Inference:  54%|█████▍    | 108/200 [12:47<11:10,  7.29s/it]Running Inference:  55%|█████▍    | 109/200 [12:55<11:10,  7.37s/it]Running Inference:  55%|█████▌    | 110/200 [13:03<11:28,  7.64s/it]Running Inference:  56%|█████▌    | 111/200 [13:10<11:08,  7.51s/it]Running Inference:  56%|█████▌    | 112/200 [13:17<10:46,  7.35s/it]Running Inference:  56%|█████▋    | 113/200 [13:25<10:43,  7.40s/it]Running Inference:  57%|█████▋    | 114/200 [13:32<10:36,  7.40s/it]Running Inference:  57%|█████▊    | 115/200 [13:40<10:31,  7.42s/it]Running Inference:  58%|█████▊    | 116/200 [13:47<10:26,  7.45s/it]Running Inference:  58%|█████▊    | 117/200 [13:55<10:15,  7.42s/it]Running Inference:  59%|█████▉    | 118/200 [14:03<10:28,  7.67s/it]Running Inference:  60%|█████▉    | 119/200 [14:11<10:34,  7.83s/it]Running Inference:  60%|██████    | 120/200 [14:20<10:42,  8.03s/it]Running Inference:  60%|██████    | 121/200 [14:27<10:23,  7.89s/it]Running Inference:  61%|██████    | 122/200 [14:34<09:53,  7.61s/it]Running Inference:  62%|██████▏   | 123/200 [14:42<09:50,  7.67s/it]Running Inference:  62%|██████▏   | 124/200 [14:49<09:35,  7.57s/it]Running Inference:  62%|██████▎   | 125/200 [14:56<09:15,  7.41s/it]Running Inference:  63%|██████▎   | 126/200 [15:03<09:00,  7.31s/it]Running Inference:  64%|██████▎   | 127/200 [15:11<08:55,  7.34s/it]Running Inference:  64%|██████▍   | 128/200 [15:18<08:46,  7.31s/it]Running Inference:  64%|██████▍   | 129/200 [15:25<08:37,  7.29s/it]Running Inference:  65%|██████▌   | 130/200 [15:33<08:30,  7.29s/it]Running Inference:  66%|██████▌   | 131/200 [15:40<08:19,  7.25s/it]Running Inference:  66%|██████▌   | 132/200 [15:47<08:15,  7.29s/it]Running Inference:  66%|██████▋   | 133/200 [15:54<08:09,  7.30s/it]Running Inference:  67%|██████▋   | 134/200 [16:02<08:15,  7.51s/it]Running Inference:  68%|██████▊   | 135/200 [16:10<08:02,  7.42s/it]Running Inference:  68%|██████▊   | 136/200 [16:17<08:00,  7.51s/it]Running Inference:  68%|██████▊   | 137/200 [16:25<07:47,  7.42s/it]Running Inference:  69%|██████▉   | 138/200 [16:33<07:55,  7.67s/it]Running Inference:  70%|██████▉   | 139/200 [16:40<07:45,  7.64s/it]Running Inference:  70%|███████   | 140/200 [16:47<07:24,  7.41s/it]Running Inference:  70%|███████   | 141/200 [16:55<07:20,  7.46s/it]Running Inference:  71%|███████   | 142/200 [17:02<07:12,  7.46s/it]Running Inference:  72%|███████▏  | 143/200 [17:10<07:04,  7.45s/it]Running Inference:  72%|███████▏  | 144/200 [17:17<06:50,  7.32s/it]Running Inference:  72%|███████▎  | 145/200 [17:24<06:45,  7.37s/it]Running Inference:  73%|███████▎  | 146/200 [17:32<06:37,  7.36s/it]Running Inference:  74%|███████▎  | 147/200 [17:39<06:24,  7.26s/it]Running Inference:  74%|███████▍  | 148/200 [17:47<06:29,  7.49s/it]Running Inference:  74%|███████▍  | 149/200 [17:55<06:29,  7.63s/it]Running Inference:  75%|███████▌  | 150/200 [18:02<06:16,  7.53s/it]Running Inference:  76%|███████▌  | 151/200 [18:09<06:09,  7.53s/it]Running Inference:  76%|███████▌  | 152/200 [18:17<05:54,  7.39s/it]Running Inference:  76%|███████▋  | 153/200 [18:24<05:52,  7.50s/it]Running Inference:  77%|███████▋  | 154/200 [18:31<05:38,  7.36s/it]Running Inference:  78%|███████▊  | 155/200 [18:39<05:33,  7.41s/it]Running Inference:  78%|███████▊  | 156/200 [18:46<05:21,  7.31s/it]Running Inference:  78%|███████▊  | 157/200 [18:54<05:21,  7.47s/it]Running Inference:  79%|███████▉  | 158/200 [19:01<05:09,  7.37s/it]Running Inference:  80%|███████▉  | 159/200 [19:09<05:08,  7.51s/it]Running Inference:  80%|████████  | 160/200 [19:16<04:51,  7.29s/it]Running Inference:  80%|████████  | 161/200 [19:23<04:45,  7.31s/it]Running Inference:  81%|████████  | 162/200 [19:28<04:15,  6.72s/it]Running Inference:  82%|████████▏ | 163/200 [19:37<04:26,  7.21s/it]Running Inference:  82%|████████▏ | 164/200 [19:45<04:32,  7.57s/it]Running Inference:  82%|████████▎ | 165/200 [19:52<04:23,  7.54s/it]Running Inference:  83%|████████▎ | 166/200 [20:00<04:15,  7.52s/it]Running Inference:  84%|████████▎ | 167/200 [20:07<04:07,  7.48s/it]Running Inference:  84%|████████▍ | 168/200 [20:15<04:01,  7.54s/it]Running Inference:  84%|████████▍ | 169/200 [20:23<03:53,  7.53s/it]Running Inference:  85%|████████▌ | 170/200 [20:30<03:41,  7.39s/it]Running Inference:  86%|████████▌ | 171/200 [20:37<03:38,  7.52s/it]Running Inference:  86%|████████▌ | 172/200 [20:45<03:32,  7.60s/it]Running Inference:  86%|████████▋ | 173/200 [20:53<03:23,  7.54s/it]Running Inference:  87%|████████▋ | 174/200 [20:59<03:10,  7.33s/it]Running Inference:  88%|████████▊ | 175/200 [21:06<03:00,  7.20s/it]Running Inference:  88%|████████▊ | 176/200 [21:14<02:56,  7.36s/it]Running Inference:  88%|████████▊ | 177/200 [21:22<02:51,  7.48s/it]Running Inference:  89%|████████▉ | 178/200 [21:30<02:47,  7.60s/it]Running Inference:  90%|████████▉ | 179/200 [21:36<02:34,  7.37s/it]Running Inference:  90%|█████████ | 180/200 [21:45<02:35,  7.78s/it]Running Inference:  90%|█████████ | 181/200 [21:53<02:27,  7.74s/it]Running Inference:  91%|█████████ | 182/200 [22:01<02:19,  7.73s/it]Running Inference:  92%|█████████▏| 183/200 [22:08<02:08,  7.57s/it]Running Inference:  92%|█████████▏| 184/200 [22:16<02:02,  7.67s/it]Running Inference:  92%|█████████▎| 185/200 [22:23<01:52,  7.53s/it]Running Inference:  93%|█████████▎| 186/200 [22:31<01:47,  7.70s/it]Running Inference:  94%|█████████▎| 187/200 [22:39<01:39,  7.69s/it]Running Inference:  94%|█████████▍| 188/200 [22:46<01:30,  7.55s/it]Running Inference:  94%|█████████▍| 189/200 [22:53<01:23,  7.55s/it]Running Inference:  95%|█████████▌| 190/200 [23:01<01:14,  7.44s/it]Running Inference:  96%|█████████▌| 191/200 [23:08<01:06,  7.42s/it]Running Inference:  96%|█████████▌| 192/200 [23:15<00:59,  7.41s/it]Running Inference:  96%|█████████▋| 193/200 [23:23<00:52,  7.56s/it]Running Inference:  97%|█████████▋| 194/200 [23:31<00:45,  7.62s/it]Running Inference:  98%|█████████▊| 195/200 [23:38<00:37,  7.49s/it]Running Inference:  98%|█████████▊| 196/200 [23:46<00:30,  7.54s/it]Running Inference:  98%|█████████▊| 197/200 [23:54<00:22,  7.61s/it]Running Inference:  99%|█████████▉| 198/200 [24:01<00:14,  7.41s/it]Running Inference: 100%|█████████▉| 199/200 [24:09<00:07,  7.58s/it]Running Inference: 100%|██████████| 200/200 [24:16<00:00,  7.53s/it]Running Inference: 100%|██████████| 200/200 [24:16<00:00,  7.28s/it]
2025-12-14 19:16:51,750 - INFO - Inference completed.
2025-12-14 19:16:51,761 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 19:16:51,761 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.677 seconds.
Prefix dict has been built successfully.
2025-12-14 19:16:54,196 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 19:16:54,196 - INFO - Metrics:
15.28
2025-12-14 19:16:54,197 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=dureader, ratio=0.1) on GPU 4

----------------------------------------
Task: dureader | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 19:17:00,686 - INFO - Set deterministic seeds to 42
2025-12-14 19:17:00,686 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 19:17:00,686 - INFO - Starting evaluation run...
2025-12-14 19:17:00,686 - INFO - Output directory set to: longbenchresult
2025-12-14 19:17:00,687 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 19:17:00,687 - INFO - KV Press 'knorm' setup.
2025-12-14 19:17:00,687 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 19:17:00,687 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.01it/s]
Device set to use cuda:0
2025-12-14 19:17:17,634 - INFO - Model pipeline loaded.
2025-12-14 19:17:17,634 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 19:17:30,614 - INFO - Dataset loaded with 200 entries.
2025-12-14 19:17:30,614 - INFO - Dataset processed with 200 entries.
2025-12-14 19:17:30,643 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:40,  8.95s/it]Running Inference:   1%|          | 2/200 [00:10<15:44,  4.77s/it]Running Inference:   2%|▏         | 3/200 [00:18<19:40,  5.99s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:24,  4.10s/it]Running Inference:   2%|▎         | 5/200 [00:21<10:59,  3.38s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:39,  4.84s/it]Running Inference:   4%|▎         | 7/200 [00:36<18:38,  5.79s/it]Running Inference:   4%|▍         | 8/200 [00:44<20:44,  6.48s/it]Running Inference:   4%|▍         | 9/200 [00:53<22:13,  6.98s/it]Running Inference:   5%|▌         | 10/200 [01:00<22:28,  7.10s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:07<22:26,  7.13s/it]Running Inference:   6%|▌         | 12/200 [01:14<22:32,  7.20s/it]Running Inference:   6%|▋         | 13/200 [01:22<22:31,  7.23s/it]Running Inference:   7%|▋         | 14/200 [01:29<22:41,  7.32s/it]Running Inference:   8%|▊         | 15/200 [01:37<22:37,  7.34s/it]Running Inference:   8%|▊         | 16/200 [01:44<22:20,  7.29s/it]Running Inference:   8%|▊         | 17/200 [01:51<22:14,  7.29s/it]Running Inference:   9%|▉         | 18/200 [01:59<22:34,  7.44s/it]Running Inference:  10%|▉         | 19/200 [02:06<22:25,  7.43s/it]Running Inference:  10%|█         | 20/200 [02:14<22:35,  7.53s/it]Running Inference:  10%|█         | 21/200 [02:22<22:46,  7.63s/it]Running Inference:  11%|█         | 22/200 [02:29<22:25,  7.56s/it]Running Inference:  12%|█▏        | 23/200 [02:33<18:57,  6.43s/it]Running Inference:  12%|█▏        | 24/200 [02:36<16:06,  5.49s/it]Running Inference:  12%|█▎        | 25/200 [02:43<17:23,  5.96s/it]Running Inference:  13%|█▎        | 26/200 [02:51<18:40,  6.44s/it]Running Inference:  14%|█▎        | 27/200 [02:58<19:26,  6.74s/it]Running Inference:  14%|█▍        | 28/200 [03:06<19:48,  6.91s/it]Running Inference:  14%|█▍        | 29/200 [03:13<20:22,  7.15s/it]Running Inference:  15%|█▌        | 30/200 [03:21<20:08,  7.11s/it]Running Inference:  16%|█▌        | 31/200 [03:28<20:23,  7.24s/it]Running Inference:  16%|█▌        | 32/200 [03:35<20:04,  7.17s/it]Running Inference:  16%|█▋        | 33/200 [03:43<20:13,  7.27s/it]Running Inference:  17%|█▋        | 34/200 [03:50<20:22,  7.36s/it]Running Inference:  18%|█▊        | 35/200 [03:57<20:12,  7.35s/it]Running Inference:  18%|█▊        | 36/200 [04:05<20:30,  7.50s/it]Running Inference:  18%|█▊        | 37/200 [04:13<20:43,  7.63s/it]Running Inference:  19%|█▉        | 38/200 [04:22<21:16,  7.88s/it]Running Inference:  20%|█▉        | 39/200 [04:29<21:02,  7.84s/it]Running Inference:  20%|██        | 40/200 [04:37<21:03,  7.90s/it]Running Inference:  20%|██        | 41/200 [04:45<20:24,  7.70s/it]Running Inference:  21%|██        | 42/200 [04:51<18:46,  7.13s/it]Running Inference:  22%|██▏       | 43/200 [04:58<19:12,  7.34s/it]Running Inference:  22%|██▏       | 44/200 [05:06<18:56,  7.28s/it]Running Inference:  22%|██▎       | 45/200 [05:13<19:21,  7.49s/it]Running Inference:  23%|██▎       | 46/200 [05:21<19:09,  7.46s/it]Running Inference:  24%|██▎       | 47/200 [05:28<19:02,  7.46s/it]Running Inference:  24%|██▍       | 48/200 [05:36<19:01,  7.51s/it]Running Inference:  24%|██▍       | 49/200 [05:43<18:51,  7.49s/it]Running Inference:  25%|██▌       | 50/200 [05:51<18:45,  7.51s/it]Running Inference:  26%|██▌       | 51/200 [05:59<18:40,  7.52s/it]Running Inference:  26%|██▌       | 52/200 [06:01<14:53,  6.04s/it]Running Inference:  26%|██▋       | 53/200 [06:09<16:01,  6.54s/it]Running Inference:  27%|██▋       | 54/200 [06:17<16:54,  6.95s/it]Running Inference:  28%|██▊       | 55/200 [06:24<16:54,  7.00s/it]Running Inference:  28%|██▊       | 56/200 [06:31<16:45,  6.98s/it]Running Inference:  28%|██▊       | 57/200 [06:32<12:16,  5.15s/it]Running Inference:  29%|██▉       | 58/200 [06:36<11:26,  4.83s/it]Running Inference:  30%|██▉       | 59/200 [06:40<11:18,  4.81s/it]Running Inference:  30%|███       | 60/200 [06:48<13:17,  5.70s/it]Running Inference:  30%|███       | 61/200 [06:56<14:51,  6.41s/it]Running Inference:  31%|███       | 62/200 [07:04<15:35,  6.78s/it]Running Inference:  32%|███▏      | 63/200 [07:12<16:14,  7.11s/it]Running Inference:  32%|███▏      | 64/200 [07:19<16:24,  7.24s/it]Running Inference:  32%|███▎      | 65/200 [07:27<16:41,  7.42s/it]Running Inference:  33%|███▎      | 66/200 [07:35<16:38,  7.45s/it]Running Inference:  34%|███▎      | 67/200 [07:43<16:47,  7.57s/it]Running Inference:  34%|███▍      | 68/200 [07:50<16:26,  7.48s/it]Running Inference:  34%|███▍      | 69/200 [07:58<16:35,  7.60s/it]Running Inference:  35%|███▌      | 70/200 [08:06<16:45,  7.73s/it]Running Inference:  36%|███▌      | 71/200 [08:13<16:34,  7.71s/it]Running Inference:  36%|███▌      | 72/200 [08:17<14:03,  6.59s/it]Running Inference:  36%|███▋      | 73/200 [08:25<14:51,  7.02s/it]Running Inference:  37%|███▋      | 74/200 [08:33<15:03,  7.17s/it]Running Inference:  38%|███▊      | 75/200 [08:40<15:01,  7.21s/it]Running Inference:  38%|███▊      | 76/200 [08:48<15:13,  7.37s/it]Running Inference:  38%|███▊      | 77/200 [08:56<15:16,  7.45s/it]Running Inference:  39%|███▉      | 78/200 [09:03<15:22,  7.56s/it]Running Inference:  40%|███▉      | 79/200 [09:11<15:11,  7.53s/it]Running Inference:  40%|████      | 80/200 [09:19<15:06,  7.55s/it]Running Inference:  40%|████      | 81/200 [09:26<14:46,  7.45s/it]Running Inference:  41%|████      | 82/200 [09:33<14:38,  7.45s/it]Running Inference:  42%|████▏     | 83/200 [09:40<14:23,  7.38s/it]Running Inference:  42%|████▏     | 84/200 [09:48<14:13,  7.36s/it]Running Inference:  42%|████▎     | 85/200 [09:55<14:08,  7.38s/it]Running Inference:  43%|████▎     | 86/200 [10:02<13:57,  7.34s/it]Running Inference:  44%|████▎     | 87/200 [10:10<13:44,  7.29s/it]Running Inference:  44%|████▍     | 88/200 [10:17<13:35,  7.28s/it]Running Inference:  44%|████▍     | 89/200 [10:24<13:36,  7.35s/it]Running Inference:  45%|████▌     | 90/200 [10:31<13:20,  7.28s/it]Running Inference:  46%|████▌     | 91/200 [10:39<13:21,  7.36s/it]Running Inference:  46%|████▌     | 92/200 [10:46<13:15,  7.37s/it]Running Inference:  46%|████▋     | 93/200 [10:54<13:21,  7.49s/it]Running Inference:  47%|████▋     | 94/200 [11:02<13:26,  7.60s/it]Running Inference:  48%|████▊     | 95/200 [11:10<13:15,  7.58s/it]Running Inference:  48%|████▊     | 96/200 [11:17<13:01,  7.52s/it]Running Inference:  48%|████▊     | 97/200 [11:24<12:36,  7.34s/it]Running Inference:  49%|████▉     | 98/200 [11:31<12:20,  7.26s/it]Running Inference:  50%|████▉     | 99/200 [11:39<12:24,  7.37s/it]Running Inference:  50%|█████     | 100/200 [11:46<12:11,  7.31s/it]Running Inference:  50%|█████     | 101/200 [11:53<12:03,  7.31s/it]Running Inference:  51%|█████     | 102/200 [12:00<11:56,  7.31s/it]Running Inference:  52%|█████▏    | 103/200 [12:08<11:55,  7.38s/it]Running Inference:  52%|█████▏    | 104/200 [12:15<11:47,  7.37s/it]Running Inference:  52%|█████▎    | 105/200 [12:23<11:39,  7.36s/it]Running Inference:  53%|█████▎    | 106/200 [12:30<11:33,  7.38s/it]Running Inference:  54%|█████▎    | 107/200 [12:38<11:32,  7.44s/it]Running Inference:  54%|█████▍    | 108/200 [12:45<11:19,  7.39s/it]Running Inference:  55%|█████▍    | 109/200 [12:53<11:19,  7.47s/it]Running Inference:  55%|█████▌    | 110/200 [13:01<11:36,  7.74s/it]Running Inference:  56%|█████▌    | 111/200 [13:06<10:18,  6.95s/it]Running Inference:  56%|█████▌    | 112/200 [13:13<10:14,  6.98s/it]Running Inference:  56%|█████▋    | 113/200 [13:21<10:23,  7.16s/it]Running Inference:  57%|█████▋    | 114/200 [13:28<10:25,  7.27s/it]Running Inference:  57%|█████▊    | 115/200 [13:36<10:24,  7.35s/it]Running Inference:  58%|█████▊    | 116/200 [13:43<10:23,  7.43s/it]Running Inference:  58%|█████▊    | 117/200 [13:51<10:16,  7.42s/it]Running Inference:  59%|█████▉    | 118/200 [13:59<10:30,  7.69s/it]Running Inference:  60%|█████▉    | 119/200 [14:07<10:37,  7.87s/it]Running Inference:  60%|██████    | 120/200 [14:16<10:45,  8.06s/it]Running Inference:  60%|██████    | 121/200 [14:23<10:26,  7.93s/it]Running Inference:  61%|██████    | 122/200 [14:31<09:58,  7.67s/it]Running Inference:  62%|██████▏   | 123/200 [14:38<09:55,  7.74s/it]Running Inference:  62%|██████▏   | 124/200 [14:46<09:40,  7.64s/it]Running Inference:  62%|██████▎   | 125/200 [14:53<09:21,  7.49s/it]Running Inference:  63%|██████▎   | 126/200 [15:00<09:06,  7.39s/it]Running Inference:  64%|██████▎   | 127/200 [15:08<09:01,  7.42s/it]Running Inference:  64%|██████▍   | 128/200 [15:15<08:52,  7.40s/it]Running Inference:  64%|██████▍   | 129/200 [15:22<08:44,  7.38s/it]Running Inference:  65%|██████▌   | 130/200 [15:30<08:36,  7.39s/it]Running Inference:  66%|██████▌   | 131/200 [15:37<08:26,  7.34s/it]Running Inference:  66%|██████▌   | 132/200 [15:44<08:20,  7.36s/it]Running Inference:  66%|██████▋   | 133/200 [15:52<08:14,  7.37s/it]Running Inference:  67%|██████▋   | 134/200 [16:00<08:19,  7.57s/it]Running Inference:  68%|██████▊   | 135/200 [16:07<08:06,  7.48s/it]Running Inference:  68%|██████▊   | 136/200 [16:15<08:05,  7.58s/it]Running Inference:  68%|██████▊   | 137/200 [16:22<07:51,  7.49s/it]Running Inference:  69%|██████▉   | 138/200 [16:30<07:59,  7.74s/it]Running Inference:  70%|██████▉   | 139/200 [16:38<07:50,  7.71s/it]Running Inference:  70%|███████   | 140/200 [16:45<07:29,  7.49s/it]Running Inference:  70%|███████   | 141/200 [16:53<07:25,  7.54s/it]Running Inference:  71%|███████   | 142/200 [17:00<07:17,  7.54s/it]Running Inference:  72%|███████▏  | 143/200 [17:08<07:09,  7.53s/it]Running Inference:  72%|███████▏  | 144/200 [17:15<06:54,  7.40s/it]Running Inference:  72%|███████▎  | 145/200 [17:22<06:49,  7.45s/it]Running Inference:  73%|███████▎  | 146/200 [17:30<06:41,  7.44s/it]Running Inference:  74%|███████▎  | 147/200 [17:37<06:29,  7.34s/it]Running Inference:  74%|███████▍  | 148/200 [17:45<06:34,  7.58s/it]Running Inference:  74%|███████▍  | 149/200 [17:53<06:33,  7.72s/it]Running Inference:  75%|███████▌  | 150/200 [18:01<06:20,  7.62s/it]Running Inference:  76%|███████▌  | 151/200 [18:07<06:00,  7.36s/it]Running Inference:  76%|███████▌  | 152/200 [18:14<05:50,  7.30s/it]Running Inference:  76%|███████▋  | 153/200 [18:22<05:50,  7.46s/it]Running Inference:  77%|███████▋  | 154/200 [18:25<04:42,  6.14s/it]Running Inference:  78%|███████▊  | 155/200 [18:33<04:56,  6.58s/it]Running Inference:  78%|███████▊  | 156/200 [18:40<04:57,  6.75s/it]Running Inference:  78%|███████▊  | 157/200 [18:48<05:05,  7.10s/it]Running Inference:  79%|███████▉  | 158/200 [18:55<05:00,  7.16s/it]Running Inference:  80%|███████▉  | 159/200 [19:03<05:02,  7.38s/it]Running Inference:  80%|████████  | 160/200 [19:10<04:49,  7.25s/it]Running Inference:  80%|████████  | 161/200 [19:18<04:44,  7.30s/it]Running Inference:  81%|████████  | 162/200 [19:22<04:04,  6.43s/it]Running Inference:  82%|████████▏ | 163/200 [19:30<04:19,  7.03s/it]Running Inference:  82%|████████▏ | 164/200 [19:39<04:28,  7.45s/it]Running Inference:  82%|████████▎ | 165/200 [19:46<04:21,  7.48s/it]Running Inference:  83%|████████▎ | 166/200 [19:54<04:15,  7.51s/it]Running Inference:  84%|████████▎ | 167/200 [20:01<04:07,  7.50s/it]Running Inference:  84%|████████▍ | 168/200 [20:09<04:02,  7.58s/it]Running Inference:  84%|████████▍ | 169/200 [20:17<03:55,  7.58s/it]Running Inference:  85%|████████▌ | 170/200 [20:24<03:43,  7.45s/it]Running Inference:  86%|████████▌ | 171/200 [20:32<03:40,  7.59s/it]Running Inference:  86%|████████▌ | 172/200 [20:40<03:35,  7.68s/it]Running Inference:  86%|████████▋ | 173/200 [20:47<03:25,  7.62s/it]Running Inference:  87%|████████▋ | 174/200 [20:54<03:12,  7.41s/it]Running Inference:  88%|████████▊ | 175/200 [21:01<03:02,  7.29s/it]Running Inference:  88%|████████▊ | 176/200 [21:09<02:58,  7.45s/it]Running Inference:  88%|████████▊ | 177/200 [21:17<02:54,  7.57s/it]Running Inference:  89%|████████▉ | 178/200 [21:25<02:49,  7.69s/it]Running Inference:  90%|████████▉ | 179/200 [21:32<02:36,  7.45s/it]Running Inference:  90%|█████████ | 180/200 [21:40<02:36,  7.84s/it]Running Inference:  90%|█████████ | 181/200 [21:48<02:28,  7.80s/it]Running Inference:  91%|█████████ | 182/200 [21:56<02:20,  7.80s/it]Running Inference:  92%|█████████▏| 183/200 [22:03<02:10,  7.65s/it]Running Inference:  92%|█████████▏| 184/200 [22:11<02:04,  7.76s/it]Running Inference:  92%|█████████▎| 185/200 [22:19<01:54,  7.62s/it]Running Inference:  93%|█████████▎| 186/200 [22:27<01:48,  7.78s/it]Running Inference:  94%|█████████▎| 187/200 [22:34<01:41,  7.78s/it]Running Inference:  94%|█████████▍| 188/200 [22:42<01:31,  7.64s/it]Running Inference:  94%|█████████▍| 189/200 [22:46<01:11,  6.46s/it]Running Inference:  95%|█████████▌| 190/200 [22:53<01:07,  6.70s/it]Running Inference:  96%|█████████▌| 191/200 [23:00<01:02,  6.93s/it]Running Inference:  96%|█████████▌| 192/200 [23:08<00:56,  7.09s/it]Running Inference:  96%|█████████▋| 193/200 [23:16<00:51,  7.37s/it]Running Inference:  97%|█████████▋| 194/200 [23:24<00:45,  7.50s/it]Running Inference:  98%|█████████▊| 195/200 [23:31<00:37,  7.44s/it]Running Inference:  98%|█████████▊| 196/200 [23:39<00:30,  7.53s/it]Running Inference:  98%|█████████▊| 197/200 [23:46<00:22,  7.65s/it]Running Inference:  99%|█████████▉| 198/200 [23:54<00:14,  7.47s/it]Running Inference: 100%|█████████▉| 199/200 [24:02<00:07,  7.64s/it]Running Inference: 100%|██████████| 200/200 [24:09<00:00,  7.58s/it]Running Inference: 100%|██████████| 200/200 [24:09<00:00,  7.25s/it]
2025-12-14 19:41:40,192 - INFO - Inference completed.
2025-12-14 19:41:40,204 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 19:41:40,204 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.667 seconds.
Prefix dict has been built successfully.
2025-12-14 19:41:42,710 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 19:41:42,710 - INFO - Metrics:
15.16
2025-12-14 19:41:42,712 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=dureader, ratio=0.2) on GPU 4

----------------------------------------
Task: dureader | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 19:41:49,279 - INFO - Set deterministic seeds to 42
2025-12-14 19:41:49,279 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 19:41:49,279 - INFO - Starting evaluation run...
2025-12-14 19:41:49,279 - INFO - Output directory set to: longbenchresult
2025-12-14 19:41:49,279 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 19:41:49,279 - INFO - KV Press 'knorm' setup.
2025-12-14 19:41:49,279 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 19:41:49,279 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.55it/s]
Device set to use cuda:0
2025-12-14 19:42:01,625 - INFO - Model pipeline loaded.
2025-12-14 19:42:01,625 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 19:42:08,840 - INFO - Dataset loaded with 200 entries.
2025-12-14 19:42:08,840 - INFO - Dataset processed with 200 entries.
2025-12-14 19:42:08,866 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:44,  8.97s/it]Running Inference:   1%|          | 2/200 [00:10<15:46,  4.78s/it]Running Inference:   2%|▏         | 3/200 [00:18<19:49,  6.04s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:29,  4.13s/it]Running Inference:   2%|▎         | 5/200 [00:21<11:03,  3.40s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:45,  4.87s/it]Running Inference:   4%|▎         | 7/200 [00:37<18:45,  5.83s/it]Running Inference:   4%|▍         | 8/200 [00:45<20:52,  6.52s/it]Running Inference:   4%|▍         | 9/200 [00:53<22:23,  7.04s/it]Running Inference:   5%|▌         | 10/200 [01:00<22:42,  7.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:08<22:44,  7.22s/it]Running Inference:   6%|▌         | 12/200 [01:15<22:49,  7.28s/it]Running Inference:   6%|▋         | 13/200 [01:22<22:47,  7.31s/it]Running Inference:   7%|▋         | 14/200 [01:30<22:57,  7.41s/it]Running Inference:   8%|▊         | 15/200 [01:38<22:54,  7.43s/it]Running Inference:   8%|▊         | 16/200 [01:45<22:37,  7.38s/it]Running Inference:   8%|▊         | 17/200 [01:52<22:31,  7.39s/it]Running Inference:   9%|▉         | 18/200 [02:00<22:51,  7.54s/it]Running Inference:  10%|▉         | 19/200 [02:08<22:42,  7.53s/it]Running Inference:  10%|█         | 20/200 [02:15<22:52,  7.62s/it]Running Inference:  10%|█         | 21/200 [02:23<23:02,  7.72s/it]Running Inference:  11%|█         | 22/200 [02:31<22:39,  7.64s/it]Running Inference:  12%|█▏        | 23/200 [02:35<19:22,  6.57s/it]Running Inference:  12%|█▏        | 24/200 [02:41<19:09,  6.53s/it]Running Inference:  12%|█▎        | 25/200 [02:49<19:35,  6.72s/it]Running Inference:  13%|█▎        | 26/200 [02:56<20:15,  6.98s/it]Running Inference:  14%|█▎        | 27/200 [03:04<20:36,  7.15s/it]Running Inference:  14%|█▍        | 28/200 [03:11<20:42,  7.22s/it]Running Inference:  14%|█▍        | 29/200 [03:19<21:03,  7.39s/it]Running Inference:  15%|█▌        | 30/200 [03:26<20:41,  7.30s/it]Running Inference:  16%|█▌        | 31/200 [03:34<20:50,  7.40s/it]Running Inference:  16%|█▌        | 32/200 [03:41<20:27,  7.31s/it]Running Inference:  16%|█▋        | 33/200 [03:48<20:31,  7.37s/it]Running Inference:  17%|█▋        | 34/200 [03:56<20:34,  7.44s/it]Running Inference:  18%|█▊        | 35/200 [04:03<20:22,  7.41s/it]Running Inference:  18%|█▊        | 36/200 [04:11<20:40,  7.56s/it]Running Inference:  18%|█▊        | 37/200 [04:19<20:51,  7.68s/it]Running Inference:  19%|█▉        | 38/200 [04:28<21:24,  7.93s/it]Running Inference:  20%|█▉        | 39/200 [04:35<21:09,  7.89s/it]Running Inference:  20%|██        | 40/200 [04:43<21:10,  7.94s/it]Running Inference:  20%|██        | 41/200 [04:51<20:31,  7.75s/it]Running Inference:  21%|██        | 42/200 [04:58<20:04,  7.63s/it]Running Inference:  22%|██▏       | 43/200 [05:06<20:10,  7.71s/it]Running Inference:  22%|██▏       | 44/200 [05:13<19:39,  7.56s/it]Running Inference:  22%|██▎       | 45/200 [05:21<19:54,  7.71s/it]Running Inference:  23%|██▎       | 46/200 [05:29<19:35,  7.64s/it]Running Inference:  24%|██▎       | 47/200 [05:36<19:23,  7.61s/it]Running Inference:  24%|██▍       | 48/200 [05:44<19:19,  7.63s/it]Running Inference:  24%|██▍       | 49/200 [05:51<19:07,  7.60s/it]Running Inference:  25%|██▌       | 50/200 [05:59<19:00,  7.60s/it]Running Inference:  26%|██▌       | 51/200 [06:07<18:55,  7.62s/it]Running Inference:  26%|██▌       | 52/200 [06:10<15:15,  6.19s/it]Running Inference:  26%|██▋       | 53/200 [06:17<16:20,  6.67s/it]Running Inference:  27%|██▋       | 54/200 [06:25<17:11,  7.06s/it]Running Inference:  28%|██▊       | 55/200 [06:33<17:10,  7.11s/it]Running Inference:  28%|██▊       | 56/200 [06:40<17:01,  7.09s/it]Running Inference:  28%|██▊       | 57/200 [06:40<12:27,  5.23s/it]Running Inference:  29%|██▉       | 58/200 [06:44<10:55,  4.61s/it]Running Inference:  30%|██▉       | 59/200 [06:51<13:00,  5.54s/it]Running Inference:  30%|███       | 60/200 [06:59<14:30,  6.22s/it]Running Inference:  30%|███       | 61/200 [07:07<15:46,  6.81s/it]Running Inference:  31%|███       | 62/200 [07:15<16:16,  7.08s/it]Running Inference:  32%|███▏      | 63/200 [07:23<16:46,  7.34s/it]Running Inference:  32%|███▏      | 64/200 [07:31<16:49,  7.43s/it]Running Inference:  32%|███▎      | 65/200 [07:39<17:02,  7.57s/it]Running Inference:  33%|███▎      | 66/200 [07:46<16:55,  7.58s/it]Running Inference:  34%|███▎      | 67/200 [07:54<17:02,  7.69s/it]Running Inference:  34%|███▍      | 68/200 [08:01<16:40,  7.58s/it]Running Inference:  34%|███▍      | 69/200 [08:09<16:49,  7.70s/it]Running Inference:  35%|███▌      | 70/200 [08:17<16:58,  7.83s/it]Running Inference:  36%|███▌      | 71/200 [08:25<16:46,  7.80s/it]Running Inference:  36%|███▌      | 72/200 [08:29<13:55,  6.53s/it]Running Inference:  36%|███▋      | 73/200 [08:37<14:49,  7.01s/it]Running Inference:  37%|███▋      | 74/200 [08:45<15:04,  7.18s/it]Running Inference:  38%|███▊      | 75/200 [08:52<15:06,  7.25s/it]Running Inference:  38%|███▊      | 76/200 [09:00<15:19,  7.42s/it]Running Inference:  38%|███▊      | 77/200 [09:07<15:23,  7.51s/it]Running Inference:  39%|███▉      | 78/200 [09:15<15:29,  7.62s/it]Running Inference:  40%|███▉      | 79/200 [09:23<15:19,  7.60s/it]Running Inference:  40%|████      | 80/200 [09:31<15:14,  7.62s/it]Running Inference:  40%|████      | 81/200 [09:38<14:55,  7.52s/it]Running Inference:  41%|████      | 82/200 [09:45<14:47,  7.52s/it]Running Inference:  42%|████▏     | 83/200 [09:53<14:32,  7.45s/it]Running Inference:  42%|████▏     | 84/200 [10:00<14:22,  7.44s/it]Running Inference:  42%|████▎     | 85/200 [10:08<14:17,  7.46s/it]Running Inference:  43%|████▎     | 86/200 [10:15<14:05,  7.42s/it]Running Inference:  44%|████▎     | 87/200 [10:22<13:51,  7.36s/it]Running Inference:  44%|████▍     | 88/200 [10:29<13:41,  7.34s/it]Running Inference:  44%|████▍     | 89/200 [10:37<13:43,  7.42s/it]Running Inference:  45%|████▌     | 90/200 [10:44<13:27,  7.35s/it]Running Inference:  46%|████▌     | 91/200 [10:52<13:29,  7.43s/it]Running Inference:  46%|████▌     | 92/200 [10:59<13:23,  7.44s/it]Running Inference:  46%|████▋     | 93/200 [11:07<13:29,  7.57s/it]Running Inference:  47%|████▋     | 94/200 [11:15<13:33,  7.68s/it]Running Inference:  48%|████▊     | 95/200 [11:23<13:23,  7.65s/it]Running Inference:  48%|████▊     | 96/200 [11:30<13:09,  7.59s/it]Running Inference:  48%|████▊     | 97/200 [11:37<12:43,  7.41s/it]Running Inference:  49%|████▉     | 98/200 [11:44<12:27,  7.33s/it]Running Inference:  50%|████▉     | 99/200 [11:52<12:32,  7.45s/it]Running Inference:  50%|█████     | 100/200 [11:59<12:19,  7.39s/it]Running Inference:  50%|█████     | 101/200 [12:07<12:11,  7.38s/it]Running Inference:  51%|█████     | 102/200 [12:14<12:04,  7.40s/it]Running Inference:  52%|█████▏    | 103/200 [12:22<12:03,  7.46s/it]Running Inference:  52%|█████▏    | 104/200 [12:29<11:55,  7.45s/it]Running Inference:  52%|█████▎    | 105/200 [12:36<11:46,  7.44s/it]Running Inference:  53%|█████▎    | 106/200 [12:44<11:40,  7.45s/it]Running Inference:  54%|█████▎    | 107/200 [12:52<11:38,  7.52s/it]Running Inference:  54%|█████▍    | 108/200 [12:59<11:25,  7.46s/it]Running Inference:  55%|█████▍    | 109/200 [13:07<11:25,  7.53s/it]Running Inference:  55%|█████▌    | 110/200 [13:15<11:42,  7.80s/it]Running Inference:  56%|█████▌    | 111/200 [13:22<11:22,  7.67s/it]Running Inference:  56%|█████▌    | 112/200 [13:30<11:00,  7.51s/it]Running Inference:  56%|█████▋    | 113/200 [13:37<10:57,  7.56s/it]Running Inference:  57%|█████▋    | 114/200 [13:45<10:50,  7.57s/it]Running Inference:  57%|█████▊    | 115/200 [13:52<10:44,  7.59s/it]Running Inference:  58%|█████▊    | 116/200 [14:00<10:39,  7.62s/it]Running Inference:  58%|█████▊    | 117/200 [14:04<08:55,  6.45s/it]Running Inference:  59%|█████▉    | 118/200 [14:12<09:38,  7.05s/it]Running Inference:  60%|█████▉    | 119/200 [14:21<10:03,  7.45s/it]Running Inference:  60%|██████    | 120/200 [14:29<10:23,  7.79s/it]Running Inference:  60%|██████    | 121/200 [14:37<10:13,  7.77s/it]Running Inference:  61%|██████    | 122/200 [14:44<09:51,  7.58s/it]Running Inference:  62%|██████▏   | 123/200 [14:52<09:53,  7.70s/it]Running Inference:  62%|██████▏   | 124/200 [15:00<09:40,  7.64s/it]Running Inference:  62%|██████▎   | 125/200 [15:07<09:23,  7.51s/it]Running Inference:  63%|██████▎   | 126/200 [15:14<09:09,  7.43s/it]Running Inference:  64%|██████▎   | 127/200 [15:22<09:05,  7.47s/it]Running Inference:  64%|██████▍   | 128/200 [15:29<08:56,  7.45s/it]Running Inference:  64%|██████▍   | 129/200 [15:36<08:48,  7.44s/it]Running Inference:  65%|██████▌   | 130/200 [15:44<08:40,  7.44s/it]Running Inference:  66%|██████▌   | 131/200 [15:51<08:31,  7.41s/it]Running Inference:  66%|██████▌   | 132/200 [15:59<08:25,  7.43s/it]Running Inference:  66%|██████▋   | 133/200 [16:06<08:19,  7.45s/it]Running Inference:  67%|██████▋   | 134/200 [16:14<08:25,  7.65s/it]Running Inference:  68%|██████▊   | 135/200 [16:22<08:11,  7.57s/it]Running Inference:  68%|██████▊   | 136/200 [16:30<08:10,  7.67s/it]Running Inference:  68%|██████▊   | 137/200 [16:37<07:57,  7.58s/it]Running Inference:  69%|██████▉   | 138/200 [16:45<08:04,  7.82s/it]Running Inference:  70%|██████▉   | 139/200 [16:53<07:55,  7.80s/it]Running Inference:  70%|███████   | 140/200 [17:00<07:34,  7.57s/it]Running Inference:  70%|███████   | 141/200 [17:08<07:29,  7.63s/it]Running Inference:  71%|███████   | 142/200 [17:16<07:22,  7.63s/it]Running Inference:  72%|███████▏  | 143/200 [17:23<07:14,  7.62s/it]Running Inference:  72%|███████▏  | 144/200 [17:30<06:59,  7.49s/it]Running Inference:  72%|███████▎  | 145/200 [17:38<06:54,  7.54s/it]Running Inference:  73%|███████▎  | 146/200 [17:45<06:46,  7.53s/it]Running Inference:  74%|███████▎  | 147/200 [17:53<06:33,  7.42s/it]Running Inference:  74%|███████▍  | 148/200 [18:01<06:38,  7.67s/it]Running Inference:  74%|███████▍  | 149/200 [18:09<06:38,  7.81s/it]Running Inference:  75%|███████▌  | 150/200 [18:17<06:25,  7.71s/it]Running Inference:  76%|███████▌  | 151/200 [18:24<06:17,  7.71s/it]Running Inference:  76%|███████▌  | 152/200 [18:31<06:03,  7.56s/it]Running Inference:  76%|███████▋  | 153/200 [18:39<06:00,  7.67s/it]Running Inference:  77%|███████▋  | 154/200 [18:47<05:46,  7.53s/it]Running Inference:  78%|███████▊  | 155/200 [18:54<05:41,  7.58s/it]Running Inference:  78%|███████▊  | 156/200 [19:01<05:28,  7.47s/it]Running Inference:  78%|███████▊  | 157/200 [19:10<05:28,  7.64s/it]Running Inference:  79%|███████▉  | 158/200 [19:17<05:16,  7.54s/it]Running Inference:  80%|███████▉  | 159/200 [19:25<05:14,  7.68s/it]Running Inference:  80%|████████  | 160/200 [19:30<04:42,  7.06s/it]Running Inference:  80%|████████  | 161/200 [19:38<04:40,  7.20s/it]Running Inference:  81%|████████  | 162/200 [19:42<03:58,  6.28s/it]Running Inference:  82%|████████▏ | 163/200 [19:51<04:16,  6.94s/it]Running Inference:  82%|████████▏ | 164/200 [19:59<04:27,  7.42s/it]Running Inference:  82%|████████▎ | 165/200 [20:07<04:21,  7.48s/it]Running Inference:  83%|████████▎ | 166/200 [20:14<04:16,  7.53s/it]Running Inference:  84%|████████▎ | 167/200 [20:22<04:08,  7.54s/it]Running Inference:  84%|████████▍ | 168/200 [20:30<04:04,  7.64s/it]Running Inference:  84%|████████▍ | 169/200 [20:37<03:57,  7.65s/it]Running Inference:  85%|████████▌ | 170/200 [20:45<03:45,  7.52s/it]Running Inference:  86%|████████▌ | 171/200 [20:53<03:42,  7.66s/it]Running Inference:  86%|████████▌ | 172/200 [21:01<03:37,  7.75s/it]Running Inference:  86%|████████▋ | 173/200 [21:08<03:27,  7.69s/it]Running Inference:  87%|████████▋ | 174/200 [21:15<03:14,  7.49s/it]Running Inference:  88%|████████▊ | 175/200 [21:22<03:04,  7.36s/it]Running Inference:  88%|████████▊ | 176/200 [21:30<03:00,  7.52s/it]Running Inference:  88%|████████▊ | 177/200 [21:38<02:55,  7.64s/it]Running Inference:  89%|████████▉ | 178/200 [21:46<02:50,  7.77s/it]Running Inference:  90%|████████▉ | 179/200 [21:53<02:38,  7.53s/it]Running Inference:  90%|█████████ | 180/200 [22:02<02:38,  7.91s/it]Running Inference:  90%|█████████ | 181/200 [22:10<02:29,  7.88s/it]Running Inference:  91%|█████████ | 182/200 [22:18<02:21,  7.87s/it]Running Inference:  92%|█████████▏| 183/200 [22:25<02:11,  7.72s/it]Running Inference:  92%|█████████▏| 184/200 [22:33<02:05,  7.83s/it]Running Inference:  92%|█████████▎| 185/200 [22:40<01:55,  7.69s/it]Running Inference:  93%|█████████▎| 186/200 [22:49<01:50,  7.86s/it]Running Inference:  94%|█████████▎| 187/200 [22:57<01:42,  7.85s/it]Running Inference:  94%|█████████▍| 188/200 [23:04<01:32,  7.72s/it]Running Inference:  94%|█████████▍| 189/200 [23:12<01:24,  7.71s/it]Running Inference:  95%|█████████▌| 190/200 [23:19<01:16,  7.62s/it]Running Inference:  96%|█████████▌| 191/200 [23:27<01:08,  7.60s/it]Running Inference:  96%|█████████▌| 192/200 [23:34<01:00,  7.60s/it]Running Inference:  96%|█████████▋| 193/200 [23:42<00:54,  7.74s/it]Running Inference:  97%|█████████▋| 194/200 [23:50<00:46,  7.79s/it]Running Inference:  98%|█████████▊| 195/200 [23:58<00:38,  7.66s/it]Running Inference:  98%|█████████▊| 196/200 [24:05<00:30,  7.71s/it]Running Inference:  98%|█████████▊| 197/200 [24:13<00:23,  7.78s/it]Running Inference:  99%|█████████▉| 198/200 [24:20<00:15,  7.58s/it]Running Inference: 100%|█████████▉| 199/200 [24:29<00:07,  7.75s/it]Running Inference: 100%|██████████| 200/200 [24:36<00:00,  7.70s/it]Running Inference: 100%|██████████| 200/200 [24:36<00:00,  7.38s/it]
2025-12-14 20:06:45,507 - INFO - Inference completed.
2025-12-14 20:06:45,518 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 20:06:45,518 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.672 seconds.
Prefix dict has been built successfully.
2025-12-14 20:06:48,130 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 20:06:48,130 - INFO - Metrics:
14.07
2025-12-14 20:06:48,131 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=dureader, ratio=0.3) on GPU 4

----------------------------------------
Task: dureader | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 20:06:54,792 - INFO - Set deterministic seeds to 42
2025-12-14 20:06:54,792 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 20:06:54,792 - INFO - Starting evaluation run...
2025-12-14 20:06:54,792 - INFO - Output directory set to: longbenchresult
2025-12-14 20:06:54,792 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 20:06:54,792 - INFO - KV Press 'knorm' setup.
2025-12-14 20:06:54,792 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 20:06:54,792 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.74it/s]
Device set to use cuda:0
2025-12-14 20:07:15,437 - INFO - Model pipeline loaded.
2025-12-14 20:07:15,437 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 20:07:24,767 - INFO - Dataset loaded with 200 entries.
2025-12-14 20:07:24,767 - INFO - Dataset processed with 200 entries.
2025-12-14 20:07:24,793 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:26,  8.88s/it]Running Inference:   1%|          | 2/200 [00:11<16:41,  5.06s/it]Running Inference:   2%|▏         | 3/200 [00:18<20:10,  6.14s/it]Running Inference:   2%|▏         | 4/200 [00:20<14:08,  4.33s/it]Running Inference:   2%|▎         | 5/200 [00:27<17:42,  5.45s/it]Running Inference:   3%|▎         | 6/200 [00:35<20:00,  6.19s/it]Running Inference:   4%|▎         | 7/200 [00:43<21:30,  6.69s/it]Running Inference:   4%|▍         | 8/200 [00:50<22:39,  7.08s/it]Running Inference:   4%|▍         | 9/200 [00:58<23:29,  7.38s/it]Running Inference:   5%|▌         | 10/200 [01:06<23:20,  7.37s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:13<23:03,  7.32s/it]Running Inference:   6%|▌         | 12/200 [01:20<22:55,  7.32s/it]Running Inference:   6%|▋         | 13/200 [01:28<22:45,  7.30s/it]Running Inference:   7%|▋         | 14/200 [01:35<22:49,  7.36s/it]Running Inference:   8%|▊         | 15/200 [01:42<22:41,  7.36s/it]Running Inference:   8%|▊         | 16/200 [01:50<22:21,  7.29s/it]Running Inference:   8%|▊         | 17/200 [01:57<22:14,  7.29s/it]Running Inference:   9%|▉         | 18/200 [02:05<22:33,  7.44s/it]Running Inference:  10%|▉         | 19/200 [02:12<22:23,  7.42s/it]Running Inference:  10%|█         | 20/200 [02:20<22:32,  7.51s/it]Running Inference:  10%|█         | 21/200 [02:28<22:42,  7.61s/it]Running Inference:  11%|█         | 22/200 [02:35<22:20,  7.53s/it]Running Inference:  12%|█▏        | 23/200 [02:42<21:56,  7.44s/it]Running Inference:  12%|█▏        | 24/200 [02:50<21:45,  7.42s/it]Running Inference:  12%|█▎        | 25/200 [02:57<21:17,  7.30s/it]Running Inference:  13%|█▎        | 26/200 [03:04<21:19,  7.36s/it]Running Inference:  14%|█▎        | 27/200 [03:11<21:15,  7.37s/it]Running Inference:  14%|█▍        | 28/200 [03:19<21:03,  7.35s/it]Running Inference:  14%|█▍        | 29/200 [03:26<21:12,  7.44s/it]Running Inference:  15%|█▌        | 30/200 [03:33<20:43,  7.31s/it]Running Inference:  16%|█▌        | 31/200 [03:41<20:46,  7.38s/it]Running Inference:  16%|█▌        | 32/200 [03:48<20:19,  7.26s/it]Running Inference:  16%|█▋        | 33/200 [03:55<20:20,  7.31s/it]Running Inference:  17%|█▋        | 34/200 [04:03<20:21,  7.36s/it]Running Inference:  18%|█▊        | 35/200 [04:10<20:07,  7.32s/it]Running Inference:  18%|█▊        | 36/200 [04:18<20:24,  7.47s/it]Running Inference:  18%|█▊        | 37/200 [04:26<20:35,  7.58s/it]Running Inference:  19%|█▉        | 38/200 [04:34<21:08,  7.83s/it]Running Inference:  20%|█▉        | 39/200 [04:42<20:53,  7.78s/it]Running Inference:  20%|██        | 40/200 [04:50<20:54,  7.84s/it]Running Inference:  20%|██        | 41/200 [04:57<20:15,  7.64s/it]Running Inference:  21%|██        | 42/200 [05:04<19:47,  7.51s/it]Running Inference:  22%|██▏       | 43/200 [05:12<19:52,  7.60s/it]Running Inference:  22%|██▏       | 44/200 [05:19<19:21,  7.45s/it]Running Inference:  22%|██▎       | 45/200 [05:27<19:37,  7.59s/it]Running Inference:  23%|██▎       | 46/200 [05:34<19:18,  7.52s/it]Running Inference:  24%|██▎       | 47/200 [05:42<19:06,  7.50s/it]Running Inference:  24%|██▍       | 48/200 [05:49<19:03,  7.52s/it]Running Inference:  24%|██▍       | 49/200 [05:56<18:34,  7.38s/it]Running Inference:  25%|██▌       | 50/200 [06:04<18:35,  7.44s/it]Running Inference:  26%|██▌       | 51/200 [06:11<18:29,  7.45s/it]Running Inference:  26%|██▌       | 52/200 [06:19<18:37,  7.55s/it]Running Inference:  26%|██▋       | 53/200 [06:27<18:32,  7.57s/it]Running Inference:  27%|██▋       | 54/200 [06:35<18:35,  7.64s/it]Running Inference:  28%|██▊       | 55/200 [06:42<18:00,  7.45s/it]Running Inference:  28%|██▊       | 56/200 [06:48<17:25,  7.26s/it]Running Inference:  28%|██▊       | 57/200 [06:56<17:08,  7.19s/it]Running Inference:  29%|██▉       | 58/200 [07:02<16:50,  7.12s/it]Running Inference:  30%|██▉       | 59/200 [07:09<16:01,  6.82s/it]Running Inference:  30%|███       | 60/200 [07:16<16:27,  7.06s/it]Running Inference:  30%|███       | 61/200 [07:24<16:58,  7.33s/it]Running Inference:  31%|███       | 62/200 [07:29<14:52,  6.47s/it]Running Inference:  32%|███▏      | 63/200 [07:36<15:39,  6.85s/it]Running Inference:  32%|███▏      | 64/200 [07:44<15:55,  7.03s/it]Running Inference:  32%|███▎      | 65/200 [07:52<16:17,  7.24s/it]Running Inference:  33%|███▎      | 66/200 [07:59<16:17,  7.30s/it]Running Inference:  34%|███▎      | 67/200 [08:07<16:28,  7.43s/it]Running Inference:  34%|███▍      | 68/200 [08:14<16:09,  7.34s/it]Running Inference:  34%|███▍      | 69/200 [08:22<16:19,  7.47s/it]Running Inference:  35%|███▌      | 70/200 [08:30<16:29,  7.61s/it]Running Inference:  36%|███▌      | 71/200 [08:37<16:19,  7.59s/it]Running Inference:  36%|███▌      | 72/200 [08:44<15:43,  7.37s/it]Running Inference:  36%|███▋      | 73/200 [08:52<15:57,  7.54s/it]Running Inference:  37%|███▋      | 74/200 [08:59<15:44,  7.50s/it]Running Inference:  38%|███▊      | 75/200 [09:07<15:26,  7.41s/it]Running Inference:  38%|███▊      | 76/200 [09:14<15:27,  7.48s/it]Running Inference:  38%|███▊      | 77/200 [09:22<15:21,  7.49s/it]Running Inference:  39%|███▉      | 78/200 [09:29<15:21,  7.56s/it]Running Inference:  40%|███▉      | 79/200 [09:37<15:07,  7.50s/it]Running Inference:  40%|████      | 80/200 [09:44<14:59,  7.50s/it]Running Inference:  40%|████      | 81/200 [09:51<14:38,  7.38s/it]Running Inference:  41%|████      | 82/200 [09:59<14:29,  7.37s/it]Running Inference:  42%|████▏     | 83/200 [10:06<14:13,  7.29s/it]Running Inference:  42%|████▏     | 84/200 [10:13<14:03,  7.27s/it]Running Inference:  42%|████▎     | 85/200 [10:20<13:57,  7.28s/it]Running Inference:  43%|████▎     | 86/200 [10:23<11:02,  5.81s/it]Running Inference:  44%|████▎     | 87/200 [10:30<11:37,  6.17s/it]Running Inference:  44%|████▍     | 88/200 [10:37<12:02,  6.45s/it]Running Inference:  44%|████▍     | 89/200 [10:44<12:28,  6.74s/it]Running Inference:  45%|████▌     | 90/200 [10:51<12:30,  6.82s/it]Running Inference:  46%|████▌     | 91/200 [10:59<12:43,  7.00s/it]Running Inference:  46%|████▌     | 92/200 [11:06<12:45,  7.09s/it]Running Inference:  46%|████▋     | 93/200 [11:14<12:57,  7.27s/it]Running Inference:  47%|████▋     | 94/200 [11:21<13:05,  7.41s/it]Running Inference:  48%|████▊     | 95/200 [11:29<12:57,  7.40s/it]Running Inference:  48%|████▊     | 96/200 [11:36<12:45,  7.36s/it]Running Inference:  48%|████▊     | 97/200 [11:43<12:21,  7.20s/it]Running Inference:  49%|████▉     | 98/200 [11:50<12:06,  7.13s/it]Running Inference:  50%|████▉     | 99/200 [11:57<12:12,  7.25s/it]Running Inference:  50%|█████     | 100/200 [12:04<11:59,  7.20s/it]Running Inference:  50%|█████     | 101/200 [12:07<09:23,  5.69s/it]Running Inference:  51%|█████     | 102/200 [12:14<10:02,  6.15s/it]Running Inference:  52%|█████▏    | 103/200 [12:21<10:33,  6.53s/it]Running Inference:  52%|█████▏    | 104/200 [12:28<10:46,  6.74s/it]Running Inference:  52%|█████▎    | 105/200 [12:36<10:54,  6.89s/it]Running Inference:  53%|█████▎    | 106/200 [12:43<10:58,  7.01s/it]Running Inference:  54%|█████▎    | 107/200 [12:50<11:05,  7.15s/it]Running Inference:  54%|█████▍    | 108/200 [12:58<10:57,  7.15s/it]Running Inference:  55%|█████▍    | 109/200 [13:05<11:00,  7.25s/it]Running Inference:  55%|█████▌    | 110/200 [13:13<11:19,  7.55s/it]Running Inference:  56%|█████▌    | 111/200 [13:16<09:07,  6.15s/it]Running Inference:  56%|█████▌    | 112/200 [13:23<09:22,  6.39s/it]Running Inference:  56%|█████▋    | 113/200 [13:31<09:44,  6.72s/it]Running Inference:  57%|█████▋    | 114/200 [13:38<09:55,  6.92s/it]Running Inference:  57%|█████▊    | 115/200 [13:45<10:01,  7.07s/it]Running Inference:  58%|█████▊    | 116/200 [13:53<10:04,  7.20s/it]Running Inference:  58%|█████▊    | 117/200 [14:00<10:00,  7.23s/it]Running Inference:  59%|█████▉    | 118/200 [14:08<10:16,  7.52s/it]Running Inference:  60%|█████▉    | 119/200 [14:17<10:24,  7.71s/it]Running Inference:  60%|██████    | 120/200 [14:25<10:33,  7.92s/it]Running Inference:  60%|██████    | 121/200 [14:33<10:16,  7.80s/it]Running Inference:  61%|██████    | 122/200 [14:40<09:48,  7.54s/it]Running Inference:  62%|██████▏   | 123/200 [14:47<09:46,  7.61s/it]Running Inference:  62%|██████▏   | 124/200 [14:55<09:31,  7.52s/it]Running Inference:  62%|██████▎   | 125/200 [15:02<09:13,  7.37s/it]Running Inference:  63%|██████▎   | 126/200 [15:09<08:58,  7.28s/it]Running Inference:  64%|██████▎   | 127/200 [15:16<08:53,  7.31s/it]Running Inference:  64%|██████▍   | 128/200 [15:23<08:44,  7.28s/it]Running Inference:  64%|██████▍   | 129/200 [15:30<08:35,  7.26s/it]Running Inference:  65%|██████▌   | 130/200 [15:38<08:28,  7.26s/it]Running Inference:  66%|██████▌   | 131/200 [15:45<08:18,  7.23s/it]Running Inference:  66%|██████▌   | 132/200 [15:52<08:12,  7.25s/it]Running Inference:  66%|██████▋   | 133/200 [15:59<08:06,  7.27s/it]Running Inference:  67%|██████▋   | 134/200 [16:07<08:12,  7.46s/it]Running Inference:  68%|██████▊   | 135/200 [16:15<07:59,  7.38s/it]Running Inference:  68%|██████▊   | 136/200 [16:22<07:58,  7.48s/it]Running Inference:  68%|██████▊   | 137/200 [16:29<07:45,  7.38s/it]Running Inference:  69%|██████▉   | 138/200 [16:38<07:53,  7.63s/it]Running Inference:  70%|██████▉   | 139/200 [16:45<07:44,  7.61s/it]Running Inference:  70%|███████   | 140/200 [16:52<07:23,  7.38s/it]Running Inference:  70%|███████   | 141/200 [17:00<07:18,  7.43s/it]Running Inference:  71%|███████   | 142/200 [17:07<07:11,  7.44s/it]Running Inference:  72%|███████▏  | 143/200 [17:14<07:03,  7.43s/it]Running Inference:  72%|███████▏  | 144/200 [17:22<06:48,  7.30s/it]Running Inference:  72%|███████▎  | 145/200 [17:29<06:44,  7.35s/it]Running Inference:  73%|███████▎  | 146/200 [17:36<06:36,  7.35s/it]Running Inference:  74%|███████▎  | 147/200 [17:43<06:24,  7.25s/it]Running Inference:  74%|███████▍  | 148/200 [17:51<06:29,  7.48s/it]Running Inference:  74%|███████▍  | 149/200 [17:59<06:28,  7.61s/it]Running Inference:  75%|███████▌  | 150/200 [18:07<06:15,  7.52s/it]Running Inference:  76%|███████▌  | 151/200 [18:14<06:08,  7.51s/it]Running Inference:  76%|███████▌  | 152/200 [18:21<05:53,  7.37s/it]Running Inference:  76%|███████▋  | 153/200 [18:29<05:51,  7.47s/it]Running Inference:  77%|███████▋  | 154/200 [18:36<05:37,  7.34s/it]Running Inference:  78%|███████▊  | 155/200 [18:43<05:32,  7.39s/it]Running Inference:  78%|███████▊  | 156/200 [18:50<05:20,  7.29s/it]Running Inference:  78%|███████▊  | 157/200 [18:58<05:20,  7.46s/it]Running Inference:  79%|███████▉  | 158/200 [19:05<05:09,  7.36s/it]Running Inference:  80%|███████▉  | 159/200 [19:13<05:07,  7.50s/it]Running Inference:  80%|████████  | 160/200 [19:20<04:52,  7.30s/it]Running Inference:  80%|████████  | 161/200 [19:27<04:45,  7.32s/it]Running Inference:  81%|████████  | 162/200 [19:35<04:37,  7.31s/it]Running Inference:  82%|████████▏ | 163/200 [19:43<04:41,  7.60s/it]Running Inference:  82%|████████▏ | 164/200 [19:51<04:42,  7.83s/it]Running Inference:  82%|████████▎ | 165/200 [19:59<04:30,  7.72s/it]Running Inference:  83%|████████▎ | 166/200 [20:06<04:20,  7.65s/it]Running Inference:  84%|████████▎ | 167/200 [20:14<04:10,  7.58s/it]Running Inference:  84%|████████▍ | 168/200 [20:21<04:03,  7.61s/it]Running Inference:  84%|████████▍ | 169/200 [20:29<03:55,  7.58s/it]Running Inference:  85%|████████▌ | 170/200 [20:36<03:42,  7.43s/it]Running Inference:  86%|████████▌ | 171/200 [20:44<03:38,  7.54s/it]Running Inference:  86%|████████▌ | 172/200 [20:52<03:33,  7.61s/it]Running Inference:  86%|████████▋ | 173/200 [20:59<03:23,  7.54s/it]Running Inference:  87%|████████▋ | 174/200 [21:06<03:10,  7.34s/it]Running Inference:  88%|████████▊ | 175/200 [21:13<03:00,  7.20s/it]Running Inference:  88%|████████▊ | 176/200 [21:20<02:56,  7.35s/it]Running Inference:  88%|████████▊ | 177/200 [21:28<02:52,  7.48s/it]Running Inference:  89%|████████▉ | 178/200 [21:36<02:47,  7.61s/it]Running Inference:  90%|████████▉ | 179/200 [21:43<02:34,  7.37s/it]Running Inference:  90%|█████████ | 180/200 [21:51<02:34,  7.74s/it]Running Inference:  90%|█████████ | 181/200 [21:59<02:26,  7.70s/it]Running Inference:  91%|█████████ | 182/200 [22:07<02:18,  7.69s/it]Running Inference:  92%|█████████▏| 183/200 [22:14<02:08,  7.54s/it]Running Inference:  92%|█████████▏| 184/200 [22:22<02:02,  7.64s/it]Running Inference:  92%|█████████▎| 185/200 [22:29<01:52,  7.50s/it]Running Inference:  93%|█████████▎| 186/200 [22:37<01:47,  7.67s/it]Running Inference:  94%|█████████▎| 187/200 [22:45<01:39,  7.67s/it]Running Inference:  94%|█████████▍| 188/200 [22:52<01:30,  7.54s/it]Running Inference:  94%|█████████▍| 189/200 [23:00<01:22,  7.54s/it]Running Inference:  95%|█████████▌| 190/200 [23:07<01:14,  7.43s/it]Running Inference:  96%|█████████▌| 191/200 [23:14<01:06,  7.41s/it]Running Inference:  96%|█████████▌| 192/200 [23:21<00:59,  7.39s/it]Running Inference:  96%|█████████▋| 193/200 [23:29<00:52,  7.56s/it]Running Inference:  97%|█████████▋| 194/200 [23:37<00:45,  7.62s/it]Running Inference:  98%|█████████▊| 195/200 [23:44<00:37,  7.49s/it]Running Inference:  98%|█████████▊| 196/200 [23:52<00:30,  7.54s/it]Running Inference:  98%|█████████▊| 197/200 [24:00<00:22,  7.61s/it]Running Inference:  99%|█████████▉| 198/200 [24:07<00:14,  7.43s/it]Running Inference: 100%|█████████▉| 199/200 [24:15<00:07,  7.60s/it]Running Inference: 100%|██████████| 200/200 [24:22<00:00,  7.54s/it]Running Inference: 100%|██████████| 200/200 [24:22<00:00,  7.31s/it]
2025-12-14 20:31:47,457 - INFO - Inference completed.
2025-12-14 20:31:47,468 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 20:31:47,468 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.670 seconds.
Prefix dict has been built successfully.
2025-12-14 20:31:50,382 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 20:31:50,382 - INFO - Metrics:
12.29
2025-12-14 20:31:50,383 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=dureader, ratio=0.5) on GPU 4


========================================
LongBench Task: lcc
========================================
----------------------------------------
Task: lcc | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 20:31:57,068 - INFO - Set deterministic seeds to 42
2025-12-14 20:31:57,068 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 20:31:57,068 - INFO - Starting evaluation run...
2025-12-14 20:31:57,068 - INFO - Output directory set to: longbenchresult
2025-12-14 20:31:57,068 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 20:31:57,068 - INFO - KV Press 'knorm' setup.
2025-12-14 20:31:57,068 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 20:31:57,068 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.83it/s]
Device set to use cuda:0
2025-12-14 20:32:47,920 - INFO - Model pipeline loaded.
2025-12-14 20:32:47,920 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 20:33:44,508 - INFO - Dataset loaded with 500 entries.
2025-12-14 20:33:44,508 - INFO - Dataset processed with 500 entries.
2025-12-14 20:33:44,523 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<40:14,  4.84s/it]Running Inference:   0%|          | 2/500 [00:08<35:26,  4.27s/it]Running Inference:   1%|          | 3/500 [00:13<35:35,  4.30s/it]Running Inference:   1%|          | 4/500 [00:16<34:05,  4.12s/it]Running Inference:   1%|          | 5/500 [00:17<24:48,  3.01s/it]Running Inference:   1%|          | 6/500 [00:19<19:45,  2.40s/it]Running Inference:   1%|▏         | 7/500 [00:23<23:49,  2.90s/it]Running Inference:   2%|▏         | 8/500 [00:26<26:09,  3.19s/it]Running Inference:   2%|▏         | 9/500 [00:30<27:45,  3.39s/it]Running Inference:   2%|▏         | 10/500 [00:34<28:48,  3.53s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:38<30:39,  3.76s/it]Running Inference:   2%|▏         | 12/500 [00:42<30:52,  3.80s/it]Running Inference:   3%|▎         | 13/500 [00:46<30:55,  3.81s/it]Running Inference:   3%|▎         | 14/500 [00:50<30:58,  3.82s/it]Running Inference:   3%|▎         | 15/500 [00:54<30:57,  3.83s/it]Running Inference:   3%|▎         | 16/500 [00:55<25:00,  3.10s/it]Running Inference:   3%|▎         | 17/500 [01:00<28:28,  3.54s/it]Running Inference:   4%|▎         | 18/500 [01:04<29:24,  3.66s/it]Running Inference:   4%|▍         | 19/500 [01:04<21:47,  2.72s/it]Running Inference:   4%|▍         | 20/500 [01:08<24:26,  3.06s/it]Running Inference:   4%|▍         | 21/500 [01:12<26:16,  3.29s/it]Running Inference:   4%|▍         | 22/500 [01:16<27:30,  3.45s/it]Running Inference:   5%|▍         | 23/500 [01:20<28:18,  3.56s/it]Running Inference:   5%|▍         | 24/500 [01:23<28:50,  3.64s/it]Running Inference:   5%|▌         | 25/500 [01:27<29:21,  3.71s/it]Running Inference:   5%|▌         | 26/500 [01:31<29:48,  3.77s/it]Running Inference:   5%|▌         | 27/500 [01:35<29:57,  3.80s/it]Running Inference:   6%|▌         | 28/500 [01:40<33:14,  4.23s/it]Running Inference:   6%|▌         | 29/500 [01:42<26:58,  3.44s/it]Running Inference:   6%|▌         | 30/500 [01:46<29:21,  3.75s/it]Running Inference:   6%|▌         | 31/500 [01:50<29:46,  3.81s/it]Running Inference:   6%|▋         | 32/500 [01:54<29:57,  3.84s/it]Running Inference:   7%|▋         | 33/500 [01:58<29:52,  3.84s/it]Running Inference:   7%|▋         | 34/500 [02:02<29:47,  3.84s/it]Running Inference:   7%|▋         | 35/500 [02:06<29:40,  3.83s/it]Running Inference:   7%|▋         | 36/500 [02:10<30:39,  3.96s/it]Running Inference:   7%|▋         | 37/500 [02:14<31:21,  4.06s/it]Running Inference:   8%|▊         | 38/500 [02:18<30:51,  4.01s/it]Running Inference:   8%|▊         | 39/500 [02:22<31:22,  4.08s/it]Running Inference:   8%|▊         | 40/500 [02:26<30:44,  4.01s/it]Running Inference:   8%|▊         | 41/500 [02:30<30:25,  3.98s/it]Running Inference:   8%|▊         | 42/500 [02:35<32:01,  4.20s/it]Running Inference:   9%|▊         | 43/500 [02:39<32:54,  4.32s/it]Running Inference:   9%|▉         | 44/500 [02:41<25:46,  3.39s/it]Running Inference:   9%|▉         | 45/500 [02:45<27:01,  3.56s/it]Running Inference:   9%|▉         | 46/500 [02:48<27:31,  3.64s/it]Running Inference:   9%|▉         | 47/500 [02:52<28:10,  3.73s/it]Running Inference:  10%|▉         | 48/500 [02:56<28:14,  3.75s/it]Running Inference:  10%|▉         | 49/500 [03:00<28:45,  3.83s/it]Running Inference:  10%|█         | 50/500 [03:05<30:24,  4.05s/it]Running Inference:  10%|█         | 51/500 [03:09<29:50,  3.99s/it]Running Inference:  10%|█         | 52/500 [03:12<29:31,  3.95s/it]Running Inference:  11%|█         | 53/500 [03:16<29:25,  3.95s/it]Running Inference:  11%|█         | 54/500 [03:20<29:16,  3.94s/it]Running Inference:  11%|█         | 55/500 [03:24<28:56,  3.90s/it]Running Inference:  11%|█         | 56/500 [03:25<21:47,  2.95s/it]Running Inference:  11%|█▏        | 57/500 [03:33<34:23,  4.66s/it]Running Inference:  12%|█▏        | 58/500 [03:37<32:24,  4.40s/it]Running Inference:  12%|█▏        | 59/500 [03:41<30:57,  4.21s/it]Running Inference:  12%|█▏        | 60/500 [03:45<30:02,  4.10s/it]Running Inference:  12%|█▏        | 61/500 [03:49<29:29,  4.03s/it]Running Inference:  12%|█▏        | 62/500 [03:53<29:03,  3.98s/it]Running Inference:  13%|█▎        | 63/500 [03:56<28:41,  3.94s/it]Running Inference:  13%|█▎        | 64/500 [04:00<28:20,  3.90s/it]Running Inference:  13%|█▎        | 65/500 [04:04<28:08,  3.88s/it]Running Inference:  13%|█▎        | 66/500 [04:08<28:38,  3.96s/it]Running Inference:  13%|█▎        | 67/500 [04:12<28:26,  3.94s/it]Running Inference:  14%|█▎        | 68/500 [04:16<28:13,  3.92s/it]Running Inference:  14%|█▍        | 69/500 [04:20<27:59,  3.90s/it]Running Inference:  14%|█▍        | 70/500 [04:24<27:47,  3.88s/it]Running Inference:  14%|█▍        | 71/500 [04:28<27:42,  3.87s/it]Running Inference:  14%|█▍        | 72/500 [04:32<28:05,  3.94s/it]Running Inference:  15%|█▍        | 73/500 [04:35<27:46,  3.90s/it]Running Inference:  15%|█▍        | 74/500 [04:36<20:49,  2.93s/it]Running Inference:  15%|█▌        | 75/500 [04:40<23:32,  3.32s/it]Running Inference:  15%|█▌        | 76/500 [04:42<19:22,  2.74s/it]Running Inference:  15%|█▌        | 77/500 [04:46<21:48,  3.09s/it]Running Inference:  16%|█▌        | 78/500 [04:50<24:16,  3.45s/it]Running Inference:  16%|█▌        | 79/500 [04:54<24:58,  3.56s/it]Running Inference:  16%|█▌        | 80/500 [04:58<25:36,  3.66s/it]Running Inference:  16%|█▌        | 81/500 [05:02<27:00,  3.87s/it]Running Inference:  16%|█▋        | 82/500 [05:06<28:02,  4.03s/it]Running Inference:  17%|█▋        | 83/500 [05:10<27:46,  4.00s/it]Running Inference:  17%|█▋        | 84/500 [05:14<27:31,  3.97s/it]Running Inference:  17%|█▋        | 85/500 [05:19<29:20,  4.24s/it]Running Inference:  17%|█▋        | 86/500 [05:23<28:21,  4.11s/it]Running Inference:  17%|█▋        | 87/500 [05:27<27:41,  4.02s/it]Running Inference:  18%|█▊        | 88/500 [05:31<27:15,  3.97s/it]Running Inference:  18%|█▊        | 89/500 [05:35<27:11,  3.97s/it]Running Inference:  18%|█▊        | 90/500 [05:39<27:32,  4.03s/it]Running Inference:  18%|█▊        | 91/500 [05:43<27:35,  4.05s/it]Running Inference:  18%|█▊        | 92/500 [05:47<27:16,  4.01s/it]Running Inference:  19%|█▊        | 93/500 [05:51<26:56,  3.97s/it]Running Inference:  19%|█▉        | 94/500 [05:54<26:34,  3.93s/it]Running Inference:  19%|█▉        | 95/500 [05:58<26:22,  3.91s/it]Running Inference:  19%|█▉        | 96/500 [06:02<26:21,  3.92s/it]Running Inference:  19%|█▉        | 97/500 [06:05<23:08,  3.44s/it]Running Inference:  20%|█▉        | 98/500 [06:08<23:52,  3.56s/it]Running Inference:  20%|█▉        | 99/500 [06:09<18:00,  2.70s/it]Running Inference:  20%|██        | 100/500 [06:11<17:05,  2.56s/it]Running Inference:  20%|██        | 101/500 [06:15<19:58,  3.00s/it]Running Inference:  20%|██        | 102/500 [06:19<21:31,  3.24s/it]Running Inference:  21%|██        | 103/500 [06:23<22:35,  3.41s/it]Running Inference:  21%|██        | 104/500 [06:29<27:22,  4.15s/it]Running Inference:  21%|██        | 105/500 [06:34<28:17,  4.30s/it]Running Inference:  21%|██        | 106/500 [06:39<30:25,  4.63s/it]Running Inference:  21%|██▏       | 107/500 [06:40<23:32,  3.59s/it]Running Inference:  22%|██▏       | 108/500 [06:41<17:44,  2.72s/it]Running Inference:  22%|██▏       | 109/500 [06:45<20:01,  3.07s/it]Running Inference:  22%|██▏       | 110/500 [06:49<21:39,  3.33s/it]Running Inference:  22%|██▏       | 111/500 [06:52<22:30,  3.47s/it]Running Inference:  22%|██▏       | 112/500 [06:56<23:10,  3.58s/it]Running Inference:  23%|██▎       | 113/500 [07:00<24:17,  3.77s/it]Running Inference:  23%|██▎       | 114/500 [07:06<26:59,  4.20s/it]Running Inference:  23%|██▎       | 115/500 [07:10<26:46,  4.17s/it]Running Inference:  23%|██▎       | 116/500 [07:14<26:47,  4.19s/it]Running Inference:  23%|██▎       | 117/500 [07:18<25:56,  4.06s/it]Running Inference:  24%|██▎       | 118/500 [07:22<26:38,  4.19s/it]Running Inference:  24%|██▍       | 119/500 [07:26<25:48,  4.07s/it]Running Inference:  24%|██▍       | 120/500 [07:30<26:00,  4.11s/it]Running Inference:  24%|██▍       | 121/500 [07:34<25:35,  4.05s/it]Running Inference:  24%|██▍       | 122/500 [07:38<24:58,  3.97s/it]Running Inference:  25%|██▍       | 123/500 [07:42<24:37,  3.92s/it]Running Inference:  25%|██▍       | 124/500 [07:46<24:21,  3.89s/it]Running Inference:  25%|██▌       | 125/500 [07:50<24:46,  3.96s/it]Running Inference:  25%|██▌       | 126/500 [07:53<24:24,  3.92s/it]Running Inference:  25%|██▌       | 127/500 [07:57<24:07,  3.88s/it]Running Inference:  26%|██▌       | 128/500 [08:01<24:18,  3.92s/it]Running Inference:  26%|██▌       | 129/500 [08:05<24:02,  3.89s/it]Running Inference:  26%|██▌       | 130/500 [08:09<24:04,  3.90s/it]Running Inference:  26%|██▌       | 131/500 [08:13<23:51,  3.88s/it]Running Inference:  26%|██▋       | 132/500 [08:17<23:40,  3.86s/it]Running Inference:  27%|██▋       | 133/500 [08:21<23:43,  3.88s/it]Running Inference:  27%|██▋       | 134/500 [08:24<23:38,  3.87s/it]Running Inference:  27%|██▋       | 135/500 [08:28<23:43,  3.90s/it]Running Inference:  27%|██▋       | 136/500 [08:32<23:43,  3.91s/it]Running Inference:  27%|██▋       | 137/500 [08:36<23:25,  3.87s/it]Running Inference:  28%|██▊       | 138/500 [08:40<23:32,  3.90s/it]Running Inference:  28%|██▊       | 139/500 [08:44<23:15,  3.87s/it]Running Inference:  28%|██▊       | 140/500 [08:48<23:21,  3.89s/it]Running Inference:  28%|██▊       | 141/500 [08:52<23:58,  4.01s/it]Running Inference:  28%|██▊       | 142/500 [08:56<24:05,  4.04s/it]Running Inference:  29%|██▊       | 143/500 [09:00<23:40,  3.98s/it]Running Inference:  29%|██▉       | 144/500 [09:04<23:17,  3.93s/it]Running Inference:  29%|██▉       | 145/500 [09:08<23:19,  3.94s/it]Running Inference:  29%|██▉       | 146/500 [09:12<23:28,  3.98s/it]Running Inference:  29%|██▉       | 147/500 [09:16<23:40,  4.03s/it]Running Inference:  30%|██▉       | 148/500 [09:20<23:16,  3.97s/it]Running Inference:  30%|██▉       | 149/500 [09:24<22:58,  3.93s/it]Running Inference:  30%|███       | 150/500 [09:28<22:46,  3.90s/it]Running Inference:  30%|███       | 151/500 [09:32<22:47,  3.92s/it]Running Inference:  30%|███       | 152/500 [09:35<22:36,  3.90s/it]Running Inference:  31%|███       | 153/500 [09:39<22:29,  3.89s/it]Running Inference:  31%|███       | 154/500 [09:43<22:17,  3.87s/it]Running Inference:  31%|███       | 155/500 [09:47<22:04,  3.84s/it]Running Inference:  31%|███       | 156/500 [09:51<22:00,  3.84s/it]Running Inference:  31%|███▏      | 157/500 [09:55<22:15,  3.89s/it]Running Inference:  32%|███▏      | 158/500 [09:59<22:09,  3.89s/it]Running Inference:  32%|███▏      | 159/500 [10:02<22:00,  3.87s/it]Running Inference:  32%|███▏      | 160/500 [10:06<21:49,  3.85s/it]Running Inference:  32%|███▏      | 161/500 [10:10<21:46,  3.85s/it]Running Inference:  32%|███▏      | 162/500 [10:14<21:53,  3.88s/it]Running Inference:  33%|███▎      | 163/500 [10:18<21:38,  3.85s/it]Running Inference:  33%|███▎      | 164/500 [10:22<21:34,  3.85s/it]Running Inference:  33%|███▎      | 165/500 [10:26<21:35,  3.87s/it]Running Inference:  33%|███▎      | 166/500 [10:29<21:21,  3.84s/it]Running Inference:  33%|███▎      | 167/500 [10:34<21:55,  3.95s/it]Running Inference:  34%|███▎      | 168/500 [10:37<21:44,  3.93s/it]Running Inference:  34%|███▍      | 169/500 [10:41<21:38,  3.92s/it]Running Inference:  34%|███▍      | 170/500 [10:45<21:23,  3.89s/it]Running Inference:  34%|███▍      | 171/500 [10:49<21:25,  3.91s/it]Running Inference:  34%|███▍      | 172/500 [10:53<21:12,  3.88s/it]Running Inference:  35%|███▍      | 173/500 [10:57<21:04,  3.87s/it]Running Inference:  35%|███▍      | 174/500 [11:01<21:25,  3.94s/it]Running Inference:  35%|███▌      | 175/500 [11:05<22:09,  4.09s/it]Running Inference:  35%|███▌      | 176/500 [11:09<21:43,  4.02s/it]Running Inference:  35%|███▌      | 177/500 [11:14<22:27,  4.17s/it]Running Inference:  36%|███▌      | 178/500 [11:17<21:43,  4.05s/it]Running Inference:  36%|███▌      | 179/500 [11:21<21:16,  3.98s/it]Running Inference:  36%|███▌      | 180/500 [11:25<21:06,  3.96s/it]Running Inference:  36%|███▌      | 181/500 [11:29<20:47,  3.91s/it]Running Inference:  36%|███▋      | 182/500 [11:33<20:31,  3.87s/it]Running Inference:  37%|███▋      | 183/500 [11:37<20:41,  3.92s/it]Running Inference:  37%|███▋      | 184/500 [11:41<20:38,  3.92s/it]Running Inference:  37%|███▋      | 185/500 [11:42<16:02,  3.06s/it]Running Inference:  37%|███▋      | 186/500 [11:46<17:18,  3.31s/it]Running Inference:  37%|███▋      | 187/500 [11:50<18:15,  3.50s/it]Running Inference:  38%|███▊      | 188/500 [11:53<18:41,  3.59s/it]Running Inference:  38%|███▊      | 189/500 [11:54<14:25,  2.78s/it]Running Inference:  38%|███▊      | 190/500 [11:58<16:35,  3.21s/it]Running Inference:  38%|███▊      | 191/500 [12:02<17:25,  3.38s/it]Running Inference:  38%|███▊      | 192/500 [12:06<18:09,  3.54s/it]Running Inference:  39%|███▊      | 193/500 [12:10<18:36,  3.64s/it]Running Inference:  39%|███▉      | 194/500 [12:14<18:56,  3.71s/it]Running Inference:  39%|███▉      | 195/500 [12:18<19:16,  3.79s/it]Running Inference:  39%|███▉      | 196/500 [12:22<19:24,  3.83s/it]Running Inference:  39%|███▉      | 197/500 [12:26<19:32,  3.87s/it]Running Inference:  40%|███▉      | 198/500 [12:30<19:20,  3.84s/it]Running Inference:  40%|███▉      | 199/500 [12:32<17:10,  3.42s/it]Running Inference:  40%|████      | 200/500 [12:36<17:53,  3.58s/it]Running Inference:  40%|████      | 201/500 [12:40<18:27,  3.70s/it]Running Inference:  40%|████      | 202/500 [12:44<18:33,  3.74s/it]Running Inference:  41%|████      | 203/500 [12:49<20:48,  4.20s/it]Running Inference:  41%|████      | 204/500 [12:53<20:20,  4.12s/it]Running Inference:  41%|████      | 205/500 [12:54<15:42,  3.20s/it]Running Inference:  41%|████      | 206/500 [12:58<16:30,  3.37s/it]Running Inference:  41%|████▏     | 207/500 [13:02<17:07,  3.51s/it]Running Inference:  42%|████▏     | 208/500 [13:06<18:05,  3.72s/it]Running Inference:  42%|████▏     | 209/500 [13:10<18:49,  3.88s/it]Running Inference:  42%|████▏     | 210/500 [13:14<18:39,  3.86s/it]Running Inference:  42%|████▏     | 211/500 [13:18<18:52,  3.92s/it]Running Inference:  42%|████▏     | 212/500 [13:22<18:37,  3.88s/it]Running Inference:  43%|████▎     | 213/500 [13:25<18:23,  3.85s/it]Running Inference:  43%|████▎     | 214/500 [13:29<18:19,  3.85s/it]Running Inference:  43%|████▎     | 215/500 [13:33<18:19,  3.86s/it]Running Inference:  43%|████▎     | 216/500 [13:37<18:12,  3.85s/it]Running Inference:  43%|████▎     | 217/500 [13:41<18:30,  3.93s/it]Running Inference:  44%|████▎     | 218/500 [13:45<18:36,  3.96s/it]Running Inference:  44%|████▍     | 219/500 [13:49<18:16,  3.90s/it]Running Inference:  44%|████▍     | 220/500 [13:53<18:07,  3.89s/it]Running Inference:  44%|████▍     | 221/500 [13:57<17:55,  3.85s/it]Running Inference:  44%|████▍     | 222/500 [14:00<17:46,  3.84s/it]Running Inference:  45%|████▍     | 223/500 [14:04<17:40,  3.83s/it]Running Inference:  45%|████▍     | 224/500 [14:08<17:37,  3.83s/it]Running Inference:  45%|████▌     | 225/500 [14:12<17:29,  3.81s/it]Running Inference:  45%|████▌     | 226/500 [14:16<17:22,  3.80s/it]Running Inference:  45%|████▌     | 227/500 [14:19<17:18,  3.81s/it]Running Inference:  46%|████▌     | 228/500 [14:20<12:35,  2.78s/it]Running Inference:  46%|████▌     | 229/500 [14:24<13:52,  3.07s/it]Running Inference:  46%|████▌     | 230/500 [14:27<14:48,  3.29s/it]Running Inference:  46%|████▌     | 231/500 [14:31<15:28,  3.45s/it]Running Inference:  46%|████▋     | 232/500 [14:32<11:46,  2.64s/it]Running Inference:  47%|████▋     | 233/500 [14:36<13:26,  3.02s/it]Running Inference:  47%|████▋     | 234/500 [14:40<14:31,  3.28s/it]Running Inference:  47%|████▋     | 235/500 [14:44<15:39,  3.54s/it]Running Inference:  47%|████▋     | 236/500 [14:48<15:52,  3.61s/it]Running Inference:  47%|████▋     | 237/500 [14:51<16:03,  3.66s/it]Running Inference:  48%|████▊     | 238/500 [14:55<16:11,  3.71s/it]Running Inference:  48%|████▊     | 239/500 [14:59<16:24,  3.77s/it]Running Inference:  48%|████▊     | 240/500 [15:03<16:26,  3.80s/it]Running Inference:  48%|████▊     | 241/500 [15:07<16:28,  3.81s/it]Running Inference:  48%|████▊     | 242/500 [15:08<12:57,  3.01s/it]Running Inference:  49%|████▊     | 243/500 [15:13<14:50,  3.46s/it]Running Inference:  49%|████▉     | 244/500 [15:17<15:50,  3.71s/it]Running Inference:  49%|████▉     | 245/500 [15:21<16:03,  3.78s/it]Running Inference:  49%|████▉     | 246/500 [15:22<12:21,  2.92s/it]Running Inference:  49%|████▉     | 247/500 [15:26<13:35,  3.22s/it]Running Inference:  50%|████▉     | 248/500 [15:29<14:17,  3.40s/it]Running Inference:  50%|████▉     | 249/500 [15:33<14:49,  3.54s/it]Running Inference:  50%|█████     | 250/500 [15:37<15:13,  3.65s/it]Running Inference:  50%|█████     | 251/500 [15:41<15:47,  3.80s/it]Running Inference:  50%|█████     | 252/500 [15:46<16:39,  4.03s/it]Running Inference:  51%|█████     | 253/500 [15:50<16:20,  3.97s/it]Running Inference:  51%|█████     | 254/500 [15:54<16:06,  3.93s/it]Running Inference:  51%|█████     | 255/500 [15:58<16:06,  3.95s/it]Running Inference:  51%|█████     | 256/500 [16:01<15:53,  3.91s/it]Running Inference:  51%|█████▏    | 257/500 [16:05<16:00,  3.95s/it]Running Inference:  52%|█████▏    | 258/500 [16:09<15:49,  3.92s/it]Running Inference:  52%|█████▏    | 259/500 [16:13<15:48,  3.93s/it]Running Inference:  52%|█████▏    | 260/500 [16:17<15:37,  3.91s/it]Running Inference:  52%|█████▏    | 261/500 [16:21<15:26,  3.88s/it]Running Inference:  52%|█████▏    | 262/500 [16:25<15:28,  3.90s/it]Running Inference:  53%|█████▎    | 263/500 [16:29<15:27,  3.91s/it]Running Inference:  53%|█████▎    | 264/500 [16:33<15:18,  3.89s/it]Running Inference:  53%|█████▎    | 265/500 [16:37<15:23,  3.93s/it]Running Inference:  53%|█████▎    | 266/500 [16:41<15:15,  3.91s/it]Running Inference:  53%|█████▎    | 267/500 [16:45<15:42,  4.04s/it]Running Inference:  54%|█████▎    | 268/500 [16:49<15:25,  3.99s/it]Running Inference:  54%|█████▍    | 269/500 [16:53<15:15,  3.96s/it]Running Inference:  54%|█████▍    | 270/500 [16:56<15:03,  3.93s/it]Running Inference:  54%|█████▍    | 271/500 [17:00<14:48,  3.88s/it]Running Inference:  54%|█████▍    | 272/500 [17:04<14:44,  3.88s/it]Running Inference:  55%|█████▍    | 273/500 [17:08<14:45,  3.90s/it]Running Inference:  55%|█████▍    | 274/500 [17:12<14:35,  3.87s/it]Running Inference:  55%|█████▌    | 275/500 [17:16<14:29,  3.86s/it]Running Inference:  55%|█████▌    | 276/500 [17:20<14:29,  3.88s/it]Running Inference:  55%|█████▌    | 277/500 [17:24<14:24,  3.88s/it]Running Inference:  56%|█████▌    | 278/500 [17:27<14:24,  3.89s/it]Running Inference:  56%|█████▌    | 279/500 [17:32<14:31,  3.94s/it]Running Inference:  56%|█████▌    | 280/500 [17:36<14:39,  4.00s/it]Running Inference:  56%|█████▌    | 281/500 [17:40<14:32,  3.98s/it]Running Inference:  56%|█████▋    | 282/500 [17:43<14:22,  3.96s/it]Running Inference:  57%|█████▋    | 283/500 [17:47<14:12,  3.93s/it]Running Inference:  57%|█████▋    | 284/500 [17:51<14:12,  3.95s/it]Running Inference:  57%|█████▋    | 285/500 [17:55<14:09,  3.95s/it]Running Inference:  57%|█████▋    | 286/500 [17:59<13:56,  3.91s/it]Running Inference:  57%|█████▋    | 287/500 [18:04<14:41,  4.14s/it]Running Inference:  58%|█████▊    | 288/500 [18:08<14:34,  4.12s/it]Running Inference:  58%|█████▊    | 289/500 [18:12<14:12,  4.04s/it]Running Inference:  58%|█████▊    | 290/500 [18:16<14:05,  4.03s/it]Running Inference:  58%|█████▊    | 291/500 [18:20<14:03,  4.04s/it]Running Inference:  58%|█████▊    | 292/500 [18:24<13:45,  3.97s/it]Running Inference:  59%|█████▊    | 293/500 [18:28<13:50,  4.01s/it]Running Inference:  59%|█████▉    | 294/500 [18:32<13:43,  4.00s/it]Running Inference:  59%|█████▉    | 295/500 [18:36<13:30,  3.96s/it]Running Inference:  59%|█████▉    | 296/500 [18:39<13:22,  3.93s/it]Running Inference:  59%|█████▉    | 297/500 [18:43<13:20,  3.94s/it]Running Inference:  60%|█████▉    | 298/500 [18:47<13:09,  3.91s/it]Running Inference:  60%|█████▉    | 299/500 [18:51<13:01,  3.89s/it]Running Inference:  60%|██████    | 300/500 [18:51<09:26,  2.83s/it]Running Inference:  60%|██████    | 301/500 [18:56<10:41,  3.22s/it]Running Inference:  60%|██████    | 302/500 [18:59<11:16,  3.42s/it]Running Inference:  61%|██████    | 303/500 [19:03<11:40,  3.56s/it]Running Inference:  61%|██████    | 304/500 [19:08<12:26,  3.81s/it]Running Inference:  61%|██████    | 305/500 [19:12<12:25,  3.82s/it]Running Inference:  61%|██████    | 306/500 [19:16<12:51,  3.97s/it]Running Inference:  61%|██████▏   | 307/500 [19:20<12:42,  3.95s/it]Running Inference:  62%|██████▏   | 308/500 [19:24<12:30,  3.91s/it]Running Inference:  62%|██████▏   | 309/500 [19:27<12:23,  3.89s/it]Running Inference:  62%|██████▏   | 310/500 [19:31<12:17,  3.88s/it]Running Inference:  62%|██████▏   | 311/500 [19:32<09:35,  3.04s/it]Running Inference:  62%|██████▏   | 312/500 [19:36<10:19,  3.30s/it]Running Inference:  63%|██████▎   | 313/500 [19:40<10:55,  3.50s/it]Running Inference:  63%|██████▎   | 314/500 [19:44<11:09,  3.60s/it]Running Inference:  63%|██████▎   | 315/500 [19:48<11:16,  3.66s/it]Running Inference:  63%|██████▎   | 316/500 [19:52<11:20,  3.70s/it]Running Inference:  63%|██████▎   | 317/500 [19:55<11:23,  3.73s/it]Running Inference:  64%|██████▎   | 318/500 [19:59<11:27,  3.78s/it]Running Inference:  64%|██████▍   | 319/500 [20:03<11:30,  3.81s/it]Running Inference:  64%|██████▍   | 320/500 [20:08<11:58,  3.99s/it]Running Inference:  64%|██████▍   | 321/500 [20:09<09:16,  3.11s/it]Running Inference:  64%|██████▍   | 322/500 [20:13<09:54,  3.34s/it]Running Inference:  65%|██████▍   | 323/500 [20:16<10:17,  3.49s/it]Running Inference:  65%|██████▍   | 324/500 [20:20<10:34,  3.61s/it]Running Inference:  65%|██████▌   | 325/500 [20:24<10:44,  3.68s/it]Running Inference:  65%|██████▌   | 326/500 [20:28<11:12,  3.87s/it]Running Inference:  65%|██████▌   | 327/500 [20:34<12:28,  4.33s/it]Running Inference:  66%|██████▌   | 328/500 [20:38<12:23,  4.32s/it]Running Inference:  66%|██████▌   | 329/500 [20:42<11:52,  4.17s/it]Running Inference:  66%|██████▌   | 330/500 [20:46<11:31,  4.07s/it]Running Inference:  66%|██████▌   | 331/500 [20:50<11:34,  4.11s/it]Running Inference:  66%|██████▋   | 332/500 [20:54<11:26,  4.09s/it]Running Inference:  67%|██████▋   | 333/500 [20:58<11:08,  4.00s/it]Running Inference:  67%|██████▋   | 334/500 [21:02<10:56,  3.95s/it]Running Inference:  67%|██████▋   | 335/500 [21:05<10:45,  3.91s/it]Running Inference:  67%|██████▋   | 336/500 [21:09<10:35,  3.88s/it]Running Inference:  67%|██████▋   | 337/500 [21:13<10:28,  3.85s/it]Running Inference:  68%|██████▊   | 338/500 [21:17<10:24,  3.85s/it]Running Inference:  68%|██████▊   | 339/500 [21:21<10:38,  3.96s/it]Running Inference:  68%|██████▊   | 340/500 [21:27<12:07,  4.55s/it]Running Inference:  68%|██████▊   | 341/500 [21:31<11:28,  4.33s/it]Running Inference:  68%|██████▊   | 342/500 [21:35<11:01,  4.19s/it]Running Inference:  69%|██████▊   | 343/500 [21:39<10:56,  4.18s/it]Running Inference:  69%|██████▉   | 344/500 [21:43<10:49,  4.16s/it]Running Inference:  69%|██████▉   | 345/500 [21:45<08:54,  3.45s/it]Running Inference:  69%|██████▉   | 346/500 [21:46<07:28,  2.91s/it]Running Inference:  69%|██████▉   | 347/500 [21:48<06:02,  2.37s/it]Running Inference:  70%|██████▉   | 348/500 [21:51<07:09,  2.83s/it]Running Inference:  70%|██████▉   | 349/500 [21:55<07:53,  3.13s/it]Running Inference:  70%|███████   | 350/500 [22:01<09:36,  3.84s/it]Running Inference:  70%|███████   | 351/500 [22:05<09:52,  3.98s/it]Running Inference:  70%|███████   | 352/500 [22:09<09:38,  3.91s/it]Running Inference:  71%|███████   | 353/500 [22:13<09:28,  3.87s/it]Running Inference:  71%|███████   | 354/500 [22:17<09:29,  3.90s/it]Running Inference:  71%|███████   | 355/500 [22:23<11:27,  4.74s/it]Running Inference:  71%|███████   | 356/500 [22:27<10:42,  4.46s/it]Running Inference:  71%|███████▏  | 357/500 [22:31<10:27,  4.39s/it]Running Inference:  72%|███████▏  | 358/500 [22:35<10:02,  4.24s/it]Running Inference:  72%|███████▏  | 359/500 [22:39<09:37,  4.10s/it]Running Inference:  72%|███████▏  | 360/500 [22:43<09:20,  4.01s/it]Running Inference:  72%|███████▏  | 361/500 [22:47<09:12,  3.97s/it]Running Inference:  72%|███████▏  | 362/500 [22:51<09:12,  4.00s/it]Running Inference:  73%|███████▎  | 363/500 [22:55<09:05,  3.98s/it]Running Inference:  73%|███████▎  | 364/500 [22:59<09:02,  3.99s/it]Running Inference:  73%|███████▎  | 365/500 [23:04<09:39,  4.29s/it]Running Inference:  73%|███████▎  | 366/500 [23:05<07:39,  3.43s/it]Running Inference:  73%|███████▎  | 367/500 [23:09<07:54,  3.57s/it]Running Inference:  74%|███████▎  | 368/500 [23:13<08:07,  3.69s/it]Running Inference:  74%|███████▍  | 369/500 [23:17<08:23,  3.84s/it]Running Inference:  74%|███████▍  | 370/500 [23:22<08:40,  4.00s/it]Running Inference:  74%|███████▍  | 371/500 [23:25<08:29,  3.95s/it]Running Inference:  74%|███████▍  | 372/500 [23:29<08:26,  3.96s/it]Running Inference:  75%|███████▍  | 373/500 [23:33<08:17,  3.92s/it]Running Inference:  75%|███████▍  | 374/500 [23:37<08:14,  3.92s/it]Running Inference:  75%|███████▌  | 375/500 [23:41<08:04,  3.87s/it]Running Inference:  75%|███████▌  | 376/500 [23:45<08:00,  3.87s/it]Running Inference:  75%|███████▌  | 377/500 [23:49<07:53,  3.85s/it]Running Inference:  76%|███████▌  | 378/500 [23:52<07:51,  3.86s/it]Running Inference:  76%|███████▌  | 379/500 [23:56<07:54,  3.92s/it]Running Inference:  76%|███████▌  | 380/500 [24:00<07:46,  3.89s/it]Running Inference:  76%|███████▌  | 381/500 [24:04<07:40,  3.87s/it]Running Inference:  76%|███████▋  | 382/500 [24:08<07:48,  3.97s/it]Running Inference:  77%|███████▋  | 383/500 [24:12<07:44,  3.97s/it]Running Inference:  77%|███████▋  | 384/500 [24:16<07:34,  3.92s/it]Running Inference:  77%|███████▋  | 385/500 [24:20<07:35,  3.96s/it]Running Inference:  77%|███████▋  | 386/500 [24:24<07:32,  3.97s/it]Running Inference:  77%|███████▋  | 387/500 [24:28<07:27,  3.96s/it]Running Inference:  78%|███████▊  | 388/500 [24:32<07:19,  3.92s/it]Running Inference:  78%|███████▊  | 389/500 [24:36<07:14,  3.92s/it]Running Inference:  78%|███████▊  | 390/500 [24:40<07:07,  3.88s/it]Running Inference:  78%|███████▊  | 391/500 [24:43<07:00,  3.86s/it]Running Inference:  78%|███████▊  | 392/500 [24:47<06:54,  3.84s/it]Running Inference:  79%|███████▊  | 393/500 [24:51<06:46,  3.80s/it]Running Inference:  79%|███████▉  | 394/500 [24:55<06:45,  3.82s/it]Running Inference:  79%|███████▉  | 395/500 [24:59<06:42,  3.84s/it]Running Inference:  79%|███████▉  | 396/500 [25:02<06:35,  3.81s/it]Running Inference:  79%|███████▉  | 397/500 [25:07<06:42,  3.90s/it]Running Inference:  80%|███████▉  | 398/500 [25:10<06:33,  3.86s/it]Running Inference:  80%|███████▉  | 399/500 [25:14<06:26,  3.82s/it]Running Inference:  80%|████████  | 400/500 [25:18<06:20,  3.80s/it]Running Inference:  80%|████████  | 401/500 [25:22<06:14,  3.79s/it]Running Inference:  80%|████████  | 402/500 [25:25<06:09,  3.77s/it]Running Inference:  81%|████████  | 403/500 [25:29<06:05,  3.77s/it]Running Inference:  81%|████████  | 404/500 [25:33<06:01,  3.76s/it]Running Inference:  81%|████████  | 405/500 [25:37<05:59,  3.79s/it]Running Inference:  81%|████████  | 406/500 [25:40<05:56,  3.79s/it]Running Inference:  81%|████████▏ | 407/500 [25:44<05:51,  3.78s/it]Running Inference:  82%|████████▏ | 408/500 [25:45<04:24,  2.88s/it]Running Inference:  82%|████████▏ | 409/500 [25:49<04:46,  3.15s/it]Running Inference:  82%|████████▏ | 410/500 [25:53<05:02,  3.36s/it]Running Inference:  82%|████████▏ | 411/500 [25:56<05:12,  3.51s/it]Running Inference:  82%|████████▏ | 412/500 [25:57<04:00,  2.73s/it]Running Inference:  83%|████████▎ | 413/500 [26:01<04:24,  3.04s/it]Running Inference:  83%|████████▎ | 414/500 [26:05<04:41,  3.28s/it]Running Inference:  83%|████████▎ | 415/500 [26:09<04:51,  3.43s/it]Running Inference:  83%|████████▎ | 416/500 [26:12<04:55,  3.51s/it]Running Inference:  83%|████████▎ | 417/500 [26:16<04:57,  3.58s/it]Running Inference:  84%|████████▎ | 418/500 [26:20<05:00,  3.66s/it]Running Inference:  84%|████████▍ | 419/500 [26:24<04:59,  3.69s/it]Running Inference:  84%|████████▍ | 420/500 [26:24<03:40,  2.75s/it]Running Inference:  84%|████████▍ | 421/500 [26:28<04:01,  3.06s/it]Running Inference:  84%|████████▍ | 422/500 [26:29<03:07,  2.41s/it]Running Inference:  85%|████████▍ | 423/500 [26:33<03:36,  2.81s/it]Running Inference:  85%|████████▍ | 424/500 [26:37<03:55,  3.10s/it]Running Inference:  85%|████████▌ | 425/500 [26:41<04:14,  3.39s/it]Running Inference:  85%|████████▌ | 426/500 [26:44<04:19,  3.51s/it]Running Inference:  85%|████████▌ | 427/500 [26:45<03:09,  2.59s/it]Running Inference:  86%|████████▌ | 428/500 [26:49<03:31,  2.94s/it]Running Inference:  86%|████████▌ | 429/500 [26:52<03:46,  3.18s/it]Running Inference:  86%|████████▌ | 430/500 [26:54<03:17,  2.82s/it]Running Inference:  86%|████████▌ | 431/500 [26:58<03:35,  3.12s/it]Running Inference:  86%|████████▋ | 432/500 [27:02<03:52,  3.41s/it]Running Inference:  87%|████████▋ | 433/500 [27:06<04:01,  3.60s/it]Running Inference:  87%|████████▋ | 434/500 [27:10<04:05,  3.72s/it]Running Inference:  87%|████████▋ | 435/500 [27:14<04:03,  3.74s/it]Running Inference:  87%|████████▋ | 436/500 [27:18<04:01,  3.78s/it]Running Inference:  87%|████████▋ | 437/500 [27:22<03:58,  3.79s/it]Running Inference:  88%|████████▊ | 438/500 [27:26<03:54,  3.78s/it]Running Inference:  88%|████████▊ | 439/500 [27:29<03:52,  3.81s/it]Running Inference:  88%|████████▊ | 440/500 [27:33<03:48,  3.81s/it]Running Inference:  88%|████████▊ | 441/500 [27:37<03:43,  3.79s/it]Running Inference:  88%|████████▊ | 442/500 [27:41<03:38,  3.77s/it]Running Inference:  89%|████████▊ | 443/500 [27:44<03:34,  3.77s/it]Running Inference:  89%|████████▉ | 444/500 [27:48<03:33,  3.81s/it]Running Inference:  89%|████████▉ | 445/500 [27:52<03:31,  3.84s/it]Running Inference:  89%|████████▉ | 446/500 [27:56<03:28,  3.87s/it]Running Inference:  89%|████████▉ | 447/500 [28:00<03:23,  3.84s/it]Running Inference:  90%|████████▉ | 448/500 [28:04<03:20,  3.85s/it]Running Inference:  90%|████████▉ | 449/500 [28:08<03:15,  3.84s/it]Running Inference:  90%|█████████ | 450/500 [28:11<03:11,  3.84s/it]Running Inference:  90%|█████████ | 451/500 [28:15<03:07,  3.82s/it]Running Inference:  90%|█████████ | 452/500 [28:19<03:03,  3.83s/it]Running Inference:  91%|█████████ | 453/500 [28:23<03:00,  3.85s/it]Running Inference:  91%|█████████ | 454/500 [28:27<02:56,  3.83s/it]Running Inference:  91%|█████████ | 455/500 [28:31<02:51,  3.80s/it]Running Inference:  91%|█████████ | 456/500 [28:34<02:46,  3.78s/it]Running Inference:  91%|█████████▏| 457/500 [28:38<02:43,  3.80s/it]Running Inference:  92%|█████████▏| 458/500 [28:42<02:39,  3.80s/it]Running Inference:  92%|█████████▏| 459/500 [28:46<02:39,  3.89s/it]Running Inference:  92%|█████████▏| 460/500 [28:50<02:38,  3.96s/it]Running Inference:  92%|█████████▏| 461/500 [28:54<02:33,  3.93s/it]Running Inference:  92%|█████████▏| 462/500 [28:58<02:32,  4.02s/it]Running Inference:  93%|█████████▎| 463/500 [29:02<02:25,  3.94s/it]Running Inference:  93%|█████████▎| 464/500 [29:06<02:19,  3.87s/it]Running Inference:  93%|█████████▎| 465/500 [29:09<02:14,  3.83s/it]Running Inference:  93%|█████████▎| 466/500 [29:13<02:10,  3.83s/it]Running Inference:  93%|█████████▎| 467/500 [29:17<02:05,  3.80s/it]Running Inference:  94%|█████████▎| 468/500 [29:21<02:03,  3.86s/it]Running Inference:  94%|█████████▍| 469/500 [29:25<01:58,  3.83s/it]Running Inference:  94%|█████████▍| 470/500 [29:28<01:54,  3.80s/it]Running Inference:  94%|█████████▍| 471/500 [29:32<01:51,  3.83s/it]Running Inference:  94%|█████████▍| 472/500 [29:36<01:46,  3.81s/it]Running Inference:  95%|█████████▍| 473/500 [29:40<01:45,  3.90s/it]Running Inference:  95%|█████████▍| 474/500 [29:44<01:40,  3.87s/it]Running Inference:  95%|█████████▌| 475/500 [29:48<01:36,  3.84s/it]Running Inference:  95%|█████████▌| 476/500 [29:52<01:32,  3.84s/it]Running Inference:  95%|█████████▌| 477/500 [29:55<01:28,  3.83s/it]Running Inference:  96%|█████████▌| 478/500 [29:59<01:25,  3.88s/it]Running Inference:  96%|█████████▌| 479/500 [30:04<01:27,  4.18s/it]Running Inference:  96%|█████████▌| 480/500 [30:08<01:21,  4.10s/it]Running Inference:  96%|█████████▌| 481/500 [30:13<01:19,  4.17s/it]Running Inference:  96%|█████████▋| 482/500 [30:16<01:12,  4.04s/it]Running Inference:  97%|█████████▋| 483/500 [30:20<01:07,  3.97s/it]Running Inference:  97%|█████████▋| 484/500 [30:24<01:03,  3.99s/it]Running Inference:  97%|█████████▋| 485/500 [30:29<01:05,  4.36s/it]Running Inference:  97%|█████████▋| 486/500 [30:33<00:58,  4.18s/it]Running Inference:  97%|█████████▋| 487/500 [30:37<00:52,  4.07s/it]Running Inference:  98%|█████████▊| 488/500 [30:41<00:47,  3.95s/it]Running Inference:  98%|█████████▊| 489/500 [30:45<00:43,  3.94s/it]Running Inference:  98%|█████████▊| 490/500 [30:48<00:39,  3.93s/it]Running Inference:  98%|█████████▊| 491/500 [30:52<00:34,  3.89s/it]Running Inference:  98%|█████████▊| 492/500 [30:56<00:31,  3.90s/it]Running Inference:  99%|█████████▊| 493/500 [31:00<00:26,  3.85s/it]Running Inference:  99%|█████████▉| 494/500 [31:04<00:23,  3.87s/it]Running Inference:  99%|█████████▉| 495/500 [31:08<00:19,  3.86s/it]Running Inference:  99%|█████████▉| 496/500 [31:12<00:15,  3.87s/it]Running Inference:  99%|█████████▉| 497/500 [31:12<00:08,  2.95s/it]Running Inference: 100%|█████████▉| 498/500 [31:16<00:06,  3.23s/it]Running Inference: 100%|█████████▉| 499/500 [31:20<00:03,  3.39s/it]Running Inference: 100%|██████████| 500/500 [31:24<00:00,  3.60s/it]Running Inference: 100%|██████████| 500/500 [31:24<00:00,  3.77s/it]
2025-12-14 21:05:09,172 - INFO - Inference completed.
2025-12-14 21:05:09,187 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 21:05:09,187 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:05:09,226 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 21:05:09,226 - INFO - Metrics:
21.56
2025-12-14 21:05:09,227 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lcc, ratio=0.1) on GPU 4

----------------------------------------
Task: lcc | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:05:15,671 - INFO - Set deterministic seeds to 42
2025-12-14 21:05:15,671 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:05:15,671 - INFO - Starting evaluation run...
2025-12-14 21:05:15,671 - INFO - Output directory set to: longbenchresult
2025-12-14 21:05:15,672 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 21:05:15,672 - INFO - KV Press 'knorm' setup.
2025-12-14 21:05:15,672 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:05:15,672 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.45it/s]
Device set to use cuda:0
2025-12-14 21:05:30,988 - INFO - Model pipeline loaded.
2025-12-14 21:05:30,988 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 21:05:38,333 - INFO - Dataset loaded with 500 entries.
2025-12-14 21:05:38,333 - INFO - Dataset processed with 500 entries.
2025-12-14 21:05:38,348 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<40:51,  4.91s/it]Running Inference:   0%|          | 2/500 [00:05<21:21,  2.57s/it]Running Inference:   1%|          | 3/500 [00:10<27:51,  3.36s/it]Running Inference:   1%|          | 4/500 [00:13<29:18,  3.55s/it]Running Inference:   1%|          | 5/500 [00:17<30:08,  3.65s/it]Running Inference:   1%|          | 6/500 [00:18<22:46,  2.77s/it]Running Inference:   1%|▏         | 7/500 [00:22<26:03,  3.17s/it]Running Inference:   2%|▏         | 8/500 [00:26<27:42,  3.38s/it]Running Inference:   2%|▏         | 9/500 [00:30<28:44,  3.51s/it]Running Inference:   2%|▏         | 10/500 [00:34<29:22,  3.60s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:38<31:05,  3.82s/it]Running Inference:   2%|▏         | 12/500 [00:42<31:09,  3.83s/it]Running Inference:   3%|▎         | 13/500 [00:46<31:05,  3.83s/it]Running Inference:   3%|▎         | 14/500 [00:50<31:02,  3.83s/it]Running Inference:   3%|▎         | 15/500 [00:53<30:55,  3.83s/it]Running Inference:   3%|▎         | 16/500 [00:57<30:45,  3.81s/it]Running Inference:   3%|▎         | 17/500 [01:02<32:24,  4.03s/it]Running Inference:   4%|▎         | 18/500 [01:06<32:02,  3.99s/it]Running Inference:   4%|▍         | 19/500 [01:09<31:34,  3.94s/it]Running Inference:   4%|▍         | 20/500 [01:13<31:10,  3.90s/it]Running Inference:   4%|▍         | 21/500 [01:17<30:53,  3.87s/it]Running Inference:   4%|▍         | 22/500 [01:21<30:40,  3.85s/it]Running Inference:   5%|▍         | 23/500 [01:25<30:26,  3.83s/it]Running Inference:   5%|▍         | 24/500 [01:28<30:17,  3.82s/it]Running Inference:   5%|▌         | 25/500 [01:32<30:15,  3.82s/it]Running Inference:   5%|▌         | 26/500 [01:36<30:18,  3.84s/it]Running Inference:   5%|▌         | 27/500 [01:40<30:12,  3.83s/it]Running Inference:   6%|▌         | 28/500 [01:45<33:13,  4.22s/it]Running Inference:   6%|▌         | 29/500 [01:49<32:45,  4.17s/it]Running Inference:   6%|▌         | 30/500 [01:52<29:12,  3.73s/it]Running Inference:   6%|▌         | 31/500 [01:56<29:35,  3.79s/it]Running Inference:   6%|▋         | 32/500 [02:00<29:45,  3.82s/it]Running Inference:   7%|▋         | 33/500 [02:03<29:38,  3.81s/it]Running Inference:   7%|▋         | 34/500 [02:05<25:10,  3.24s/it]Running Inference:   7%|▋         | 35/500 [02:09<26:22,  3.40s/it]Running Inference:   7%|▋         | 36/500 [02:13<28:13,  3.65s/it]Running Inference:   7%|▋         | 37/500 [02:18<29:30,  3.82s/it]Running Inference:   8%|▊         | 38/500 [02:21<29:27,  3.83s/it]Running Inference:   8%|▊         | 39/500 [02:26<30:21,  3.95s/it]Running Inference:   8%|▊         | 40/500 [02:29<29:57,  3.91s/it]Running Inference:   8%|▊         | 41/500 [02:33<29:47,  3.89s/it]Running Inference:   8%|▊         | 42/500 [02:38<31:29,  4.13s/it]Running Inference:   9%|▊         | 43/500 [02:43<32:26,  4.26s/it]Running Inference:   9%|▉         | 44/500 [02:46<31:32,  4.15s/it]Running Inference:   9%|▉         | 45/500 [02:50<30:55,  4.08s/it]Running Inference:   9%|▉         | 46/500 [02:54<30:07,  3.98s/it]Running Inference:   9%|▉         | 47/500 [02:58<29:54,  3.96s/it]Running Inference:  10%|▉         | 48/500 [03:02<29:24,  3.90s/it]Running Inference:  10%|▉         | 49/500 [03:06<29:28,  3.92s/it]Running Inference:  10%|█         | 50/500 [03:10<30:49,  4.11s/it]Running Inference:  10%|█         | 51/500 [03:14<30:02,  4.02s/it]Running Inference:  10%|█         | 52/500 [03:18<29:32,  3.96s/it]Running Inference:  11%|█         | 53/500 [03:22<29:20,  3.94s/it]Running Inference:  11%|█         | 54/500 [03:26<29:02,  3.91s/it]Running Inference:  11%|█         | 55/500 [03:29<28:41,  3.87s/it]Running Inference:  11%|█         | 56/500 [03:33<28:30,  3.85s/it]Running Inference:  11%|█▏        | 57/500 [03:42<38:46,  5.25s/it]Running Inference:  12%|█▏        | 58/500 [03:46<35:23,  4.80s/it]Running Inference:  12%|█▏        | 59/500 [03:48<29:00,  3.95s/it]Running Inference:  12%|█▏        | 60/500 [03:51<28:35,  3.90s/it]Running Inference:  12%|█▏        | 61/500 [03:55<28:23,  3.88s/it]Running Inference:  12%|█▏        | 62/500 [03:59<28:08,  3.85s/it]Running Inference:  13%|█▎        | 63/500 [04:03<27:59,  3.84s/it]Running Inference:  13%|█▎        | 64/500 [04:05<24:15,  3.34s/it]Running Inference:  13%|█▎        | 65/500 [04:06<18:15,  2.52s/it]Running Inference:  13%|█▎        | 66/500 [04:10<21:39,  3.00s/it]Running Inference:  13%|█▎        | 67/500 [04:13<23:29,  3.26s/it]Running Inference:  14%|█▎        | 68/500 [04:17<24:40,  3.43s/it]Running Inference:  14%|█▍        | 69/500 [04:21<25:24,  3.54s/it]Running Inference:  14%|█▍        | 70/500 [04:25<25:53,  3.61s/it]Running Inference:  14%|█▍        | 71/500 [04:29<26:12,  3.66s/it]Running Inference:  14%|█▍        | 72/500 [04:33<26:58,  3.78s/it]Running Inference:  15%|█▍        | 73/500 [04:37<26:56,  3.79s/it]Running Inference:  15%|█▍        | 74/500 [04:37<20:08,  2.84s/it]Running Inference:  15%|█▌        | 75/500 [04:41<22:44,  3.21s/it]Running Inference:  15%|█▌        | 76/500 [04:45<24:16,  3.44s/it]Running Inference:  15%|█▌        | 77/500 [04:49<25:08,  3.57s/it]Running Inference:  16%|█▌        | 78/500 [04:53<26:31,  3.77s/it]Running Inference:  16%|█▌        | 79/500 [04:54<19:20,  2.76s/it]Running Inference:  16%|█▌        | 80/500 [04:58<21:33,  3.08s/it]Running Inference:  16%|█▌        | 81/500 [05:02<23:57,  3.43s/it]Running Inference:  16%|█▋        | 82/500 [05:06<25:49,  3.71s/it]Running Inference:  17%|█▋        | 83/500 [05:10<26:09,  3.76s/it]Running Inference:  17%|█▋        | 84/500 [05:14<26:18,  3.80s/it]Running Inference:  17%|█▋        | 85/500 [05:19<28:23,  4.11s/it]Running Inference:  17%|█▋        | 86/500 [05:23<27:39,  4.01s/it]Running Inference:  17%|█▋        | 87/500 [05:23<20:26,  2.97s/it]Running Inference:  18%|█▊        | 88/500 [05:24<16:30,  2.40s/it]Running Inference:  18%|█▊        | 89/500 [05:28<19:32,  2.85s/it]Running Inference:  18%|█▊        | 90/500 [05:32<22:08,  3.24s/it]Running Inference:  18%|█▊        | 91/500 [05:36<23:44,  3.48s/it]Running Inference:  18%|█▊        | 92/500 [05:40<24:33,  3.61s/it]Running Inference:  19%|█▊        | 93/500 [05:44<24:57,  3.68s/it]Running Inference:  19%|█▉        | 94/500 [05:48<25:13,  3.73s/it]Running Inference:  19%|█▉        | 95/500 [05:52<25:25,  3.77s/it]Running Inference:  19%|█▉        | 96/500 [05:56<25:38,  3.81s/it]Running Inference:  19%|█▉        | 97/500 [05:59<25:36,  3.81s/it]Running Inference:  20%|█▉        | 98/500 [06:03<25:28,  3.80s/it]Running Inference:  20%|█▉        | 99/500 [06:07<25:27,  3.81s/it]Running Inference:  20%|██        | 100/500 [06:10<23:56,  3.59s/it]Running Inference:  20%|██        | 101/500 [06:14<24:42,  3.72s/it]Running Inference:  20%|██        | 102/500 [06:18<24:48,  3.74s/it]Running Inference:  21%|██        | 103/500 [06:22<24:50,  3.75s/it]Running Inference:  21%|██        | 104/500 [06:27<28:47,  4.36s/it]Running Inference:  21%|██        | 105/500 [06:32<29:12,  4.44s/it]Running Inference:  21%|██        | 106/500 [06:37<30:56,  4.71s/it]Running Inference:  21%|██▏       | 107/500 [06:41<29:29,  4.50s/it]Running Inference:  22%|██▏       | 108/500 [06:45<28:03,  4.30s/it]Running Inference:  22%|██▏       | 109/500 [06:49<27:09,  4.17s/it]Running Inference:  22%|██▏       | 110/500 [06:53<26:35,  4.09s/it]Running Inference:  22%|██▏       | 111/500 [06:57<25:55,  4.00s/it]Running Inference:  22%|██▏       | 112/500 [07:01<25:29,  3.94s/it]Running Inference:  23%|██▎       | 113/500 [07:05<25:50,  4.01s/it]Running Inference:  23%|██▎       | 114/500 [07:10<28:01,  4.36s/it]Running Inference:  23%|██▎       | 115/500 [07:14<27:26,  4.28s/it]Running Inference:  23%|██▎       | 116/500 [07:18<27:07,  4.24s/it]Running Inference:  23%|██▎       | 117/500 [07:22<26:08,  4.09s/it]Running Inference:  24%|██▎       | 118/500 [07:26<26:43,  4.20s/it]Running Inference:  24%|██▍       | 119/500 [07:30<25:47,  4.06s/it]Running Inference:  24%|██▍       | 120/500 [07:34<25:55,  4.09s/it]Running Inference:  24%|██▍       | 121/500 [07:38<25:29,  4.03s/it]Running Inference:  24%|██▍       | 122/500 [07:42<24:50,  3.94s/it]Running Inference:  25%|██▍       | 123/500 [07:46<24:27,  3.89s/it]Running Inference:  25%|██▍       | 124/500 [07:49<24:09,  3.86s/it]Running Inference:  25%|██▌       | 125/500 [07:54<24:32,  3.93s/it]Running Inference:  25%|██▌       | 126/500 [07:57<24:11,  3.88s/it]Running Inference:  25%|██▌       | 127/500 [08:01<23:55,  3.85s/it]Running Inference:  26%|██▌       | 128/500 [08:05<24:06,  3.89s/it]Running Inference:  26%|██▌       | 129/500 [08:09<23:50,  3.86s/it]Running Inference:  26%|██▌       | 130/500 [08:13<23:52,  3.87s/it]Running Inference:  26%|██▌       | 131/500 [08:17<23:40,  3.85s/it]Running Inference:  26%|██▋       | 132/500 [08:20<23:28,  3.83s/it]Running Inference:  27%|██▋       | 133/500 [08:24<23:28,  3.84s/it]Running Inference:  27%|██▋       | 134/500 [08:28<23:23,  3.84s/it]Running Inference:  27%|██▋       | 135/500 [08:32<23:30,  3.86s/it]Running Inference:  27%|██▋       | 136/500 [08:36<23:29,  3.87s/it]Running Inference:  27%|██▋       | 137/500 [08:40<23:12,  3.84s/it]Running Inference:  28%|██▊       | 138/500 [08:44<23:18,  3.86s/it]Running Inference:  28%|██▊       | 139/500 [08:47<23:03,  3.83s/it]Running Inference:  28%|██▊       | 140/500 [08:51<23:09,  3.86s/it]Running Inference:  28%|██▊       | 141/500 [08:55<23:46,  3.97s/it]Running Inference:  28%|██▊       | 142/500 [09:00<23:51,  4.00s/it]Running Inference:  29%|██▊       | 143/500 [09:03<23:26,  3.94s/it]Running Inference:  29%|██▉       | 144/500 [09:07<23:04,  3.89s/it]Running Inference:  29%|██▉       | 145/500 [09:11<23:08,  3.91s/it]Running Inference:  29%|██▉       | 146/500 [09:15<23:16,  3.94s/it]Running Inference:  29%|██▉       | 147/500 [09:19<23:27,  3.99s/it]Running Inference:  30%|██▉       | 148/500 [09:23<22:59,  3.92s/it]Running Inference:  30%|██▉       | 149/500 [09:27<22:41,  3.88s/it]Running Inference:  30%|███       | 150/500 [09:30<22:24,  3.84s/it]Running Inference:  30%|███       | 151/500 [09:34<22:26,  3.86s/it]Running Inference:  30%|███       | 152/500 [09:35<16:23,  2.83s/it]Running Inference:  31%|███       | 153/500 [09:36<14:06,  2.44s/it]Running Inference:  31%|███       | 154/500 [09:40<16:23,  2.84s/it]Running Inference:  31%|███       | 155/500 [09:44<17:56,  3.12s/it]Running Inference:  31%|███       | 156/500 [09:48<19:03,  3.32s/it]Running Inference:  31%|███▏      | 157/500 [09:52<20:09,  3.53s/it]Running Inference:  32%|███▏      | 158/500 [09:56<20:36,  3.62s/it]Running Inference:  32%|███▏      | 159/500 [09:59<20:49,  3.66s/it]Running Inference:  32%|███▏      | 160/500 [10:03<20:57,  3.70s/it]Running Inference:  32%|███▏      | 161/500 [10:07<21:08,  3.74s/it]Running Inference:  32%|███▏      | 162/500 [10:11<21:26,  3.81s/it]Running Inference:  33%|███▎      | 163/500 [10:15<21:17,  3.79s/it]Running Inference:  33%|███▎      | 164/500 [10:18<21:17,  3.80s/it]Running Inference:  33%|███▎      | 165/500 [10:22<21:30,  3.85s/it]Running Inference:  33%|███▎      | 166/500 [10:26<21:20,  3.84s/it]Running Inference:  33%|███▎      | 167/500 [10:30<21:51,  3.94s/it]Running Inference:  34%|███▎      | 168/500 [10:34<21:36,  3.91s/it]Running Inference:  34%|███▍      | 169/500 [10:38<21:30,  3.90s/it]Running Inference:  34%|███▍      | 170/500 [10:42<21:15,  3.87s/it]Running Inference:  34%|███▍      | 171/500 [10:46<21:16,  3.88s/it]Running Inference:  34%|███▍      | 172/500 [10:50<21:03,  3.85s/it]Running Inference:  35%|███▍      | 173/500 [10:53<20:56,  3.84s/it]Running Inference:  35%|███▍      | 174/500 [10:57<21:16,  3.91s/it]Running Inference:  35%|███▌      | 175/500 [11:02<21:59,  4.06s/it]Running Inference:  35%|███▌      | 176/500 [11:06<21:31,  3.99s/it]Running Inference:  35%|███▌      | 177/500 [11:10<22:16,  4.14s/it]Running Inference:  36%|███▌      | 178/500 [11:14<21:32,  4.01s/it]Running Inference:  36%|███▌      | 179/500 [11:18<21:05,  3.94s/it]Running Inference:  36%|███▌      | 180/500 [11:22<20:56,  3.93s/it]Running Inference:  36%|███▌      | 181/500 [11:25<20:37,  3.88s/it]Running Inference:  36%|███▋      | 182/500 [11:29<20:22,  3.84s/it]Running Inference:  37%|███▋      | 183/500 [11:33<20:32,  3.89s/it]Running Inference:  37%|███▋      | 184/500 [11:37<20:29,  3.89s/it]Running Inference:  37%|███▋      | 185/500 [11:38<16:02,  3.06s/it]Running Inference:  37%|███▋      | 186/500 [11:42<17:12,  3.29s/it]Running Inference:  37%|███▋      | 187/500 [11:46<18:08,  3.48s/it]Running Inference:  38%|███▊      | 188/500 [11:50<18:34,  3.57s/it]Running Inference:  38%|███▊      | 189/500 [11:51<14:27,  2.79s/it]Running Inference:  38%|███▊      | 190/500 [11:55<16:33,  3.21s/it]Running Inference:  38%|███▊      | 191/500 [11:59<17:22,  3.37s/it]Running Inference:  38%|███▊      | 192/500 [12:02<18:04,  3.52s/it]Running Inference:  39%|███▊      | 193/500 [12:06<18:28,  3.61s/it]Running Inference:  39%|███▉      | 194/500 [12:10<18:43,  3.67s/it]Running Inference:  39%|███▉      | 195/500 [12:14<18:51,  3.71s/it]Running Inference:  39%|███▉      | 196/500 [12:18<18:56,  3.74s/it]Running Inference:  39%|███▉      | 197/500 [12:22<19:10,  3.80s/it]Running Inference:  40%|███▉      | 198/500 [12:25<19:02,  3.78s/it]Running Inference:  40%|███▉      | 199/500 [12:29<18:56,  3.78s/it]Running Inference:  40%|████      | 200/500 [12:33<19:03,  3.81s/it]Running Inference:  40%|████      | 201/500 [12:37<19:13,  3.86s/it]Running Inference:  40%|████      | 202/500 [12:41<19:02,  3.83s/it]Running Inference:  41%|████      | 203/500 [12:46<21:04,  4.26s/it]Running Inference:  41%|████      | 204/500 [12:50<20:23,  4.13s/it]Running Inference:  41%|████      | 205/500 [12:54<19:47,  4.03s/it]Running Inference:  41%|████      | 206/500 [12:57<19:19,  3.94s/it]Running Inference:  41%|████▏     | 207/500 [12:58<14:59,  3.07s/it]Running Inference:  42%|████▏     | 208/500 [13:03<16:32,  3.40s/it]Running Inference:  42%|████▏     | 209/500 [13:07<17:40,  3.65s/it]Running Inference:  42%|████▏     | 210/500 [13:11<17:49,  3.69s/it]Running Inference:  42%|████▏     | 211/500 [13:15<18:12,  3.78s/it]Running Inference:  42%|████▏     | 212/500 [13:18<18:05,  3.77s/it]Running Inference:  43%|████▎     | 213/500 [13:19<13:14,  2.77s/it]Running Inference:  43%|████▎     | 214/500 [13:23<14:41,  3.08s/it]Running Inference:  43%|████▎     | 215/500 [13:23<11:26,  2.41s/it]Running Inference:  43%|████▎     | 216/500 [13:27<13:21,  2.82s/it]Running Inference:  43%|████▎     | 217/500 [13:31<15:04,  3.20s/it]Running Inference:  44%|████▎     | 218/500 [13:35<16:09,  3.44s/it]Running Inference:  44%|████▍     | 219/500 [13:39<16:32,  3.53s/it]Running Inference:  44%|████▍     | 220/500 [13:43<16:51,  3.61s/it]Running Inference:  44%|████▍     | 221/500 [13:47<16:58,  3.65s/it]Running Inference:  44%|████▍     | 222/500 [13:50<17:03,  3.68s/it]Running Inference:  45%|████▍     | 223/500 [13:54<17:08,  3.71s/it]Running Inference:  45%|████▍     | 224/500 [13:58<17:12,  3.74s/it]Running Inference:  45%|████▌     | 225/500 [14:02<17:09,  3.74s/it]Running Inference:  45%|████▌     | 226/500 [14:05<17:05,  3.74s/it]Running Inference:  45%|████▌     | 227/500 [14:09<17:04,  3.75s/it]Running Inference:  46%|████▌     | 228/500 [14:10<12:31,  2.76s/it]Running Inference:  46%|████▌     | 229/500 [14:13<13:46,  3.05s/it]Running Inference:  46%|████▌     | 230/500 [14:17<14:41,  3.27s/it]Running Inference:  46%|████▌     | 231/500 [14:21<15:19,  3.42s/it]Running Inference:  46%|████▋     | 232/500 [14:25<15:45,  3.53s/it]Running Inference:  47%|████▋     | 233/500 [14:29<16:11,  3.64s/it]Running Inference:  47%|████▋     | 234/500 [14:32<16:23,  3.70s/it]Running Inference:  47%|████▋     | 235/500 [14:37<16:55,  3.83s/it]Running Inference:  47%|████▋     | 236/500 [14:40<16:42,  3.80s/it]Running Inference:  47%|████▋     | 237/500 [14:44<16:34,  3.78s/it]Running Inference:  48%|████▊     | 238/500 [14:48<16:30,  3.78s/it]Running Inference:  48%|████▊     | 239/500 [14:49<12:54,  2.97s/it]Running Inference:  48%|████▊     | 240/500 [14:53<13:56,  3.22s/it]Running Inference:  48%|████▊     | 241/500 [14:56<14:39,  3.39s/it]Running Inference:  48%|████▊     | 242/500 [15:00<15:01,  3.49s/it]Running Inference:  49%|████▊     | 243/500 [15:05<16:12,  3.79s/it]Running Inference:  49%|████▉     | 244/500 [15:09<16:44,  3.92s/it]Running Inference:  49%|████▉     | 245/500 [15:13<16:37,  3.91s/it]Running Inference:  49%|████▉     | 246/500 [15:16<16:18,  3.85s/it]Running Inference:  49%|████▉     | 247/500 [15:20<16:17,  3.87s/it]Running Inference:  50%|████▉     | 248/500 [15:24<16:08,  3.84s/it]Running Inference:  50%|████▉     | 249/500 [15:28<16:03,  3.84s/it]Running Inference:  50%|█████     | 250/500 [15:28<11:45,  2.82s/it]Running Inference:  50%|█████     | 251/500 [15:33<13:19,  3.21s/it]Running Inference:  50%|█████     | 252/500 [15:37<14:54,  3.61s/it]Running Inference:  51%|█████     | 253/500 [15:41<15:04,  3.66s/it]Running Inference:  51%|█████     | 254/500 [15:45<15:12,  3.71s/it]Running Inference:  51%|█████     | 255/500 [15:49<15:29,  3.79s/it]Running Inference:  51%|█████     | 256/500 [15:53<15:25,  3.79s/it]Running Inference:  51%|█████▏    | 257/500 [15:57<15:37,  3.86s/it]Running Inference:  52%|█████▏    | 258/500 [16:00<15:29,  3.84s/it]Running Inference:  52%|█████▏    | 259/500 [16:04<15:24,  3.84s/it]Running Inference:  52%|█████▏    | 260/500 [16:08<15:15,  3.81s/it]Running Inference:  52%|█████▏    | 261/500 [16:12<15:05,  3.79s/it]Running Inference:  52%|█████▏    | 262/500 [16:16<15:08,  3.82s/it]Running Inference:  53%|█████▎    | 263/500 [16:19<15:05,  3.82s/it]Running Inference:  53%|█████▎    | 264/500 [16:23<14:56,  3.80s/it]Running Inference:  53%|█████▎    | 265/500 [16:27<15:04,  3.85s/it]Running Inference:  53%|█████▎    | 266/500 [16:31<14:57,  3.83s/it]Running Inference:  53%|█████▎    | 267/500 [16:35<15:26,  3.97s/it]Running Inference:  54%|█████▎    | 268/500 [16:39<15:10,  3.92s/it]Running Inference:  54%|█████▍    | 269/500 [16:43<14:59,  3.90s/it]Running Inference:  54%|█████▍    | 270/500 [16:47<14:48,  3.86s/it]Running Inference:  54%|█████▍    | 271/500 [16:50<14:35,  3.82s/it]Running Inference:  54%|█████▍    | 272/500 [16:54<14:31,  3.82s/it]Running Inference:  55%|█████▍    | 273/500 [16:58<14:32,  3.84s/it]Running Inference:  55%|█████▍    | 274/500 [17:02<14:22,  3.82s/it]Running Inference:  55%|█████▌    | 275/500 [17:06<14:18,  3.82s/it]Running Inference:  55%|█████▌    | 276/500 [17:09<14:15,  3.82s/it]Running Inference:  55%|█████▌    | 277/500 [17:13<14:10,  3.81s/it]Running Inference:  56%|█████▌    | 278/500 [17:17<14:10,  3.83s/it]Running Inference:  56%|█████▌    | 279/500 [17:21<14:17,  3.88s/it]Running Inference:  56%|█████▌    | 280/500 [17:25<14:25,  3.93s/it]Running Inference:  56%|█████▌    | 281/500 [17:29<14:19,  3.93s/it]Running Inference:  56%|█████▋    | 282/500 [17:33<14:09,  3.90s/it]Running Inference:  57%|█████▋    | 283/500 [17:37<13:58,  3.87s/it]Running Inference:  57%|█████▋    | 284/500 [17:41<13:58,  3.88s/it]Running Inference:  57%|█████▋    | 285/500 [17:44<13:55,  3.89s/it]Running Inference:  57%|█████▋    | 286/500 [17:48<13:42,  3.84s/it]Running Inference:  57%|█████▋    | 287/500 [17:53<14:27,  4.07s/it]Running Inference:  58%|█████▊    | 288/500 [17:57<14:20,  4.06s/it]Running Inference:  58%|█████▊    | 289/500 [18:01<13:59,  3.98s/it]Running Inference:  58%|█████▊    | 290/500 [18:05<13:52,  3.96s/it]Running Inference:  58%|█████▊    | 291/500 [18:09<13:49,  3.97s/it]Running Inference:  58%|█████▊    | 292/500 [18:12<13:30,  3.90s/it]Running Inference:  59%|█████▊    | 293/500 [18:16<13:28,  3.90s/it]Running Inference:  59%|█████▉    | 294/500 [18:20<13:14,  3.86s/it]Running Inference:  59%|█████▉    | 295/500 [18:24<13:06,  3.84s/it]Running Inference:  59%|█████▉    | 296/500 [18:28<13:00,  3.83s/it]Running Inference:  59%|█████▉    | 297/500 [18:31<13:00,  3.84s/it]Running Inference:  60%|█████▉    | 298/500 [18:35<12:50,  3.82s/it]Running Inference:  60%|█████▉    | 299/500 [18:39<12:42,  3.79s/it]Running Inference:  60%|██████    | 300/500 [18:43<12:33,  3.77s/it]Running Inference:  60%|██████    | 301/500 [18:47<12:46,  3.85s/it]Running Inference:  60%|██████    | 302/500 [18:51<12:40,  3.84s/it]Running Inference:  61%|██████    | 303/500 [18:54<12:34,  3.83s/it]Running Inference:  61%|██████    | 304/500 [18:59<12:59,  3.98s/it]Running Inference:  61%|██████    | 305/500 [19:02<12:41,  3.90s/it]Running Inference:  61%|██████    | 306/500 [19:07<12:57,  4.01s/it]Running Inference:  61%|██████▏   | 307/500 [19:10<12:43,  3.95s/it]Running Inference:  62%|██████▏   | 308/500 [19:14<12:26,  3.89s/it]Running Inference:  62%|██████▏   | 309/500 [19:18<12:14,  3.85s/it]Running Inference:  62%|██████▏   | 310/500 [19:18<08:55,  2.82s/it]Running Inference:  62%|██████▏   | 311/500 [19:22<09:55,  3.15s/it]Running Inference:  62%|██████▏   | 312/500 [19:26<10:29,  3.35s/it]Running Inference:  63%|██████▎   | 313/500 [19:30<10:59,  3.53s/it]Running Inference:  63%|██████▎   | 314/500 [19:34<11:09,  3.60s/it]Running Inference:  63%|██████▎   | 315/500 [19:38<11:12,  3.63s/it]Running Inference:  63%|██████▎   | 316/500 [19:41<11:13,  3.66s/it]Running Inference:  63%|██████▎   | 317/500 [19:45<11:16,  3.70s/it]Running Inference:  64%|██████▎   | 318/500 [19:49<11:18,  3.73s/it]Running Inference:  64%|██████▍   | 319/500 [19:50<08:33,  2.84s/it]Running Inference:  64%|██████▍   | 320/500 [19:54<09:50,  3.28s/it]Running Inference:  64%|██████▍   | 321/500 [19:58<10:32,  3.53s/it]Running Inference:  64%|██████▍   | 322/500 [20:02<10:43,  3.61s/it]Running Inference:  65%|██████▍   | 323/500 [20:03<08:50,  3.00s/it]Running Inference:  65%|██████▍   | 324/500 [20:07<09:30,  3.24s/it]Running Inference:  65%|██████▌   | 325/500 [20:11<09:56,  3.41s/it]Running Inference:  65%|██████▌   | 326/500 [20:15<10:34,  3.65s/it]Running Inference:  65%|██████▌   | 327/500 [20:21<11:58,  4.15s/it]Running Inference:  66%|██████▌   | 328/500 [20:25<12:00,  4.19s/it]Running Inference:  66%|██████▌   | 329/500 [20:29<11:33,  4.05s/it]Running Inference:  66%|██████▌   | 330/500 [20:32<11:12,  3.96s/it]Running Inference:  66%|██████▌   | 331/500 [20:37<11:35,  4.12s/it]Running Inference:  66%|██████▋   | 332/500 [20:41<11:26,  4.09s/it]Running Inference:  67%|██████▋   | 333/500 [20:45<11:07,  4.00s/it]Running Inference:  67%|██████▋   | 334/500 [20:48<10:53,  3.94s/it]Running Inference:  67%|██████▋   | 335/500 [20:52<10:42,  3.90s/it]Running Inference:  67%|██████▋   | 336/500 [20:56<10:31,  3.85s/it]Running Inference:  67%|██████▋   | 337/500 [21:00<10:24,  3.83s/it]Running Inference:  68%|██████▊   | 338/500 [21:04<10:19,  3.83s/it]Running Inference:  68%|██████▊   | 339/500 [21:08<10:33,  3.94s/it]Running Inference:  68%|██████▊   | 340/500 [21:14<12:03,  4.52s/it]Running Inference:  68%|██████▊   | 341/500 [21:17<11:25,  4.31s/it]Running Inference:  68%|██████▊   | 342/500 [21:21<10:59,  4.17s/it]Running Inference:  69%|██████▊   | 343/500 [21:25<10:54,  4.17s/it]Running Inference:  69%|██████▉   | 344/500 [21:30<10:53,  4.19s/it]Running Inference:  69%|██████▉   | 345/500 [21:34<10:36,  4.10s/it]Running Inference:  69%|██████▉   | 346/500 [21:38<10:27,  4.07s/it]Running Inference:  69%|██████▉   | 347/500 [21:38<07:57,  3.12s/it]Running Inference:  70%|██████▉   | 348/500 [21:42<08:28,  3.35s/it]Running Inference:  70%|██████▉   | 349/500 [21:43<06:34,  2.61s/it]Running Inference:  70%|███████   | 350/500 [21:49<09:03,  3.62s/it]Running Inference:  70%|███████   | 351/500 [21:54<09:29,  3.82s/it]Running Inference:  70%|███████   | 352/500 [21:57<09:21,  3.80s/it]Running Inference:  71%|███████   | 353/500 [22:01<09:16,  3.78s/it]Running Inference:  71%|███████   | 354/500 [22:05<09:18,  3.83s/it]Running Inference:  71%|███████   | 355/500 [22:12<11:17,  4.67s/it]Running Inference:  71%|███████   | 356/500 [22:15<10:34,  4.41s/it]Running Inference:  71%|███████▏  | 357/500 [22:20<10:21,  4.34s/it]Running Inference:  72%|███████▏  | 358/500 [22:23<09:53,  4.18s/it]Running Inference:  72%|███████▏  | 359/500 [22:27<09:31,  4.05s/it]Running Inference:  72%|███████▏  | 360/500 [22:31<09:15,  3.97s/it]Running Inference:  72%|███████▏  | 361/500 [22:35<09:08,  3.95s/it]Running Inference:  72%|███████▏  | 362/500 [22:39<09:07,  3.96s/it]Running Inference:  73%|███████▎  | 363/500 [22:43<09:01,  3.95s/it]Running Inference:  73%|███████▎  | 364/500 [22:47<08:58,  3.96s/it]Running Inference:  73%|███████▎  | 365/500 [22:52<09:35,  4.26s/it]Running Inference:  73%|███████▎  | 366/500 [22:56<09:29,  4.25s/it]Running Inference:  73%|███████▎  | 367/500 [23:00<09:10,  4.14s/it]Running Inference:  74%|███████▎  | 368/500 [23:04<09:00,  4.09s/it]Running Inference:  74%|███████▍  | 369/500 [23:08<08:59,  4.12s/it]Running Inference:  74%|███████▍  | 370/500 [23:12<09:05,  4.19s/it]Running Inference:  74%|███████▍  | 371/500 [23:16<08:45,  4.08s/it]Running Inference:  74%|███████▍  | 372/500 [23:20<08:37,  4.04s/it]Running Inference:  75%|███████▍  | 373/500 [23:24<08:25,  3.98s/it]Running Inference:  75%|███████▍  | 374/500 [23:28<08:19,  3.96s/it]Running Inference:  75%|███████▌  | 375/500 [23:32<08:07,  3.90s/it]Running Inference:  75%|███████▌  | 376/500 [23:35<08:02,  3.89s/it]Running Inference:  75%|███████▌  | 377/500 [23:39<07:55,  3.87s/it]Running Inference:  76%|███████▌  | 378/500 [23:43<07:51,  3.87s/it]Running Inference:  76%|███████▌  | 379/500 [23:47<07:53,  3.92s/it]Running Inference:  76%|███████▌  | 380/500 [23:51<07:45,  3.88s/it]Running Inference:  76%|███████▌  | 381/500 [23:55<07:39,  3.86s/it]Running Inference:  76%|███████▋  | 382/500 [23:59<07:47,  3.96s/it]Running Inference:  77%|███████▋  | 383/500 [24:03<07:44,  3.97s/it]Running Inference:  77%|███████▋  | 384/500 [24:04<06:14,  3.23s/it]Running Inference:  77%|███████▋  | 385/500 [24:05<04:56,  2.58s/it]Running Inference:  77%|███████▋  | 386/500 [24:07<04:05,  2.16s/it]Running Inference:  77%|███████▋  | 387/500 [24:11<05:03,  2.69s/it]Running Inference:  78%|███████▊  | 388/500 [24:14<05:37,  3.02s/it]Running Inference:  78%|███████▊  | 389/500 [24:18<06:04,  3.28s/it]Running Inference:  78%|███████▊  | 390/500 [24:22<06:18,  3.44s/it]Running Inference:  78%|███████▊  | 391/500 [24:26<06:26,  3.55s/it]Running Inference:  78%|███████▊  | 392/500 [24:30<06:31,  3.62s/it]Running Inference:  79%|███████▊  | 393/500 [24:33<06:30,  3.65s/it]Running Inference:  79%|███████▉  | 394/500 [24:37<06:33,  3.71s/it]Running Inference:  79%|███████▉  | 395/500 [24:41<06:34,  3.76s/it]Running Inference:  79%|███████▉  | 396/500 [24:45<06:30,  3.76s/it]Running Inference:  79%|███████▉  | 397/500 [24:49<06:38,  3.87s/it]Running Inference:  80%|███████▉  | 398/500 [24:53<06:31,  3.83s/it]Running Inference:  80%|███████▉  | 399/500 [24:56<06:24,  3.80s/it]Running Inference:  80%|████████  | 400/500 [25:00<06:19,  3.80s/it]Running Inference:  80%|████████  | 401/500 [25:04<06:15,  3.79s/it]Running Inference:  80%|████████  | 402/500 [25:08<06:09,  3.77s/it]Running Inference:  81%|████████  | 403/500 [25:12<06:06,  3.77s/it]Running Inference:  81%|████████  | 404/500 [25:15<06:01,  3.76s/it]Running Inference:  81%|████████  | 405/500 [25:19<05:59,  3.79s/it]Running Inference:  81%|████████  | 406/500 [25:23<05:56,  3.79s/it]Running Inference:  81%|████████▏ | 407/500 [25:27<05:52,  3.79s/it]Running Inference:  82%|████████▏ | 408/500 [25:30<05:48,  3.79s/it]Running Inference:  82%|████████▏ | 409/500 [25:34<05:44,  3.79s/it]Running Inference:  82%|████████▏ | 410/500 [25:38<05:42,  3.80s/it]Running Inference:  82%|████████▏ | 411/500 [25:42<05:40,  3.82s/it]Running Inference:  82%|████████▏ | 412/500 [25:43<04:33,  3.11s/it]Running Inference:  83%|████████▎ | 413/500 [25:47<04:46,  3.30s/it]Running Inference:  83%|████████▎ | 414/500 [25:51<04:56,  3.45s/it]Running Inference:  83%|████████▎ | 415/500 [25:55<05:01,  3.55s/it]Running Inference:  83%|████████▎ | 416/500 [25:58<05:02,  3.60s/it]Running Inference:  83%|████████▎ | 417/500 [26:02<05:02,  3.64s/it]Running Inference:  84%|████████▎ | 418/500 [26:06<05:03,  3.70s/it]Running Inference:  84%|████████▍ | 419/500 [26:10<05:00,  3.72s/it]Running Inference:  84%|████████▍ | 420/500 [26:14<04:57,  3.72s/it]Running Inference:  84%|████████▍ | 421/500 [26:17<04:55,  3.74s/it]Running Inference:  84%|████████▍ | 422/500 [26:18<03:32,  2.73s/it]Running Inference:  85%|████████▍ | 423/500 [26:21<03:53,  3.03s/it]Running Inference:  85%|████████▍ | 424/500 [26:25<04:07,  3.25s/it]Running Inference:  85%|████████▌ | 425/500 [26:29<04:22,  3.50s/it]Running Inference:  85%|████████▌ | 426/500 [26:33<04:25,  3.59s/it]Running Inference:  85%|████████▌ | 427/500 [26:34<03:13,  2.65s/it]Running Inference:  86%|████████▌ | 428/500 [26:37<03:34,  2.98s/it]Running Inference:  86%|████████▌ | 429/500 [26:41<03:47,  3.21s/it]Running Inference:  86%|████████▌ | 430/500 [26:45<04:05,  3.50s/it]Running Inference:  86%|████████▌ | 431/500 [26:49<04:08,  3.60s/it]Running Inference:  86%|████████▋ | 432/500 [26:53<04:14,  3.75s/it]Running Inference:  87%|████████▋ | 433/500 [26:57<04:16,  3.84s/it]Running Inference:  87%|████████▋ | 434/500 [27:01<04:16,  3.88s/it]Running Inference:  87%|████████▋ | 435/500 [27:05<04:10,  3.85s/it]Running Inference:  87%|████████▋ | 436/500 [27:09<04:06,  3.85s/it]Running Inference:  87%|████████▋ | 437/500 [27:13<04:02,  3.84s/it]Running Inference:  88%|████████▊ | 438/500 [27:16<03:56,  3.81s/it]Running Inference:  88%|████████▊ | 439/500 [27:20<03:53,  3.83s/it]Running Inference:  88%|████████▊ | 440/500 [27:24<03:49,  3.82s/it]Running Inference:  88%|████████▊ | 441/500 [27:28<03:43,  3.79s/it]Running Inference:  88%|████████▊ | 442/500 [27:31<03:38,  3.77s/it]Running Inference:  89%|████████▊ | 443/500 [27:35<03:34,  3.77s/it]Running Inference:  89%|████████▉ | 444/500 [27:36<02:36,  2.80s/it]Running Inference:  89%|████████▉ | 445/500 [27:40<02:52,  3.13s/it]Running Inference:  89%|████████▉ | 446/500 [27:44<03:02,  3.37s/it]Running Inference:  89%|████████▉ | 447/500 [27:47<03:05,  3.49s/it]Running Inference:  90%|████████▉ | 448/500 [27:51<03:07,  3.60s/it]Running Inference:  90%|████████▉ | 449/500 [27:55<03:06,  3.66s/it]Running Inference:  90%|█████████ | 450/500 [27:59<03:05,  3.71s/it]Running Inference:  90%|█████████ | 451/500 [28:00<02:17,  2.80s/it]Running Inference:  90%|█████████ | 452/500 [28:03<02:29,  3.11s/it]Running Inference:  91%|█████████ | 453/500 [28:04<01:53,  2.41s/it]Running Inference:  91%|█████████ | 454/500 [28:08<02:09,  2.82s/it]Running Inference:  91%|█████████ | 455/500 [28:12<02:19,  3.09s/it]Running Inference:  91%|█████████ | 456/500 [28:15<02:24,  3.29s/it]Running Inference:  91%|█████████▏| 457/500 [28:19<02:28,  3.45s/it]Running Inference:  92%|█████████▏| 458/500 [28:23<02:29,  3.55s/it]Running Inference:  92%|█████████▏| 459/500 [28:27<02:32,  3.71s/it]Running Inference:  92%|█████████▏| 460/500 [28:29<02:08,  3.22s/it]Running Inference:  92%|█████████▏| 461/500 [28:33<02:13,  3.42s/it]Running Inference:  92%|█████████▏| 462/500 [28:37<02:18,  3.66s/it]Running Inference:  93%|█████████▎| 463/500 [28:41<02:16,  3.68s/it]Running Inference:  93%|█████████▎| 464/500 [28:45<02:13,  3.70s/it]Running Inference:  93%|█████████▎| 465/500 [28:48<02:09,  3.71s/it]Running Inference:  93%|█████████▎| 466/500 [28:52<02:07,  3.74s/it]Running Inference:  93%|█████████▎| 467/500 [28:56<02:03,  3.73s/it]Running Inference:  94%|█████████▎| 468/500 [29:00<02:01,  3.81s/it]Running Inference:  94%|█████████▍| 469/500 [29:04<01:57,  3.79s/it]Running Inference:  94%|█████████▍| 470/500 [29:07<01:53,  3.78s/it]Running Inference:  94%|█████████▍| 471/500 [29:11<01:50,  3.82s/it]Running Inference:  94%|█████████▍| 472/500 [29:15<01:46,  3.80s/it]Running Inference:  95%|█████████▍| 473/500 [29:19<01:44,  3.88s/it]Running Inference:  95%|█████████▍| 474/500 [29:23<01:40,  3.86s/it]Running Inference:  95%|█████████▌| 475/500 [29:27<01:35,  3.84s/it]Running Inference:  95%|█████████▌| 476/500 [29:31<01:32,  3.83s/it]Running Inference:  95%|█████████▌| 477/500 [29:34<01:28,  3.83s/it]Running Inference:  96%|█████████▌| 478/500 [29:38<01:25,  3.88s/it]Running Inference:  96%|█████████▌| 479/500 [29:43<01:27,  4.18s/it]Running Inference:  96%|█████████▌| 480/500 [29:47<01:21,  4.09s/it]Running Inference:  96%|█████████▌| 481/500 [29:52<01:20,  4.24s/it]Running Inference:  96%|█████████▋| 482/500 [29:56<01:13,  4.09s/it]Running Inference:  97%|█████████▋| 483/500 [29:59<01:08,  4.02s/it]Running Inference:  97%|█████████▋| 484/500 [30:03<01:04,  4.04s/it]Running Inference:  97%|█████████▋| 485/500 [30:09<01:05,  4.39s/it]Running Inference:  97%|█████████▋| 486/500 [30:12<00:58,  4.20s/it]Running Inference:  97%|█████████▋| 487/500 [30:16<00:53,  4.10s/it]Running Inference:  98%|█████████▊| 488/500 [30:20<00:47,  3.98s/it]Running Inference:  98%|█████████▊| 489/500 [30:21<00:32,  3.00s/it]Running Inference:  98%|█████████▊| 490/500 [30:25<00:32,  3.27s/it]Running Inference:  98%|█████████▊| 491/500 [30:28<00:30,  3.43s/it]Running Inference:  98%|█████████▊| 492/500 [30:32<00:28,  3.57s/it]Running Inference:  99%|█████████▊| 493/500 [30:36<00:25,  3.63s/it]Running Inference:  99%|█████████▉| 494/500 [30:40<00:22,  3.72s/it]Running Inference:  99%|█████████▉| 495/500 [30:44<00:18,  3.75s/it]Running Inference:  99%|█████████▉| 496/500 [30:46<00:12,  3.13s/it]Running Inference:  99%|█████████▉| 497/500 [30:46<00:07,  2.35s/it]Running Inference: 100%|█████████▉| 498/500 [30:50<00:05,  2.81s/it]Running Inference: 100%|█████████▉| 499/500 [30:54<00:03,  3.09s/it]Running Inference: 100%|██████████| 500/500 [30:58<00:00,  3.39s/it]Running Inference: 100%|██████████| 500/500 [30:58<00:00,  3.72s/it]
2025-12-14 21:36:36,646 - INFO - Inference completed.
2025-12-14 21:36:36,660 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 21:36:36,660 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:36:36,662 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 21:36:36,662 - INFO - Metrics:
15.3
2025-12-14 21:36:36,664 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lcc, ratio=0.2) on GPU 4

----------------------------------------
Task: lcc | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:36:43,161 - INFO - Set deterministic seeds to 42
2025-12-14 21:36:43,162 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:36:43,162 - INFO - Starting evaluation run...
2025-12-14 21:36:43,162 - INFO - Output directory set to: longbenchresult
2025-12-14 21:36:43,162 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 21:36:43,162 - INFO - KV Press 'knorm' setup.
2025-12-14 21:36:43,162 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:36:43,162 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.73it/s]
Device set to use cuda:0
2025-12-14 21:37:00,640 - INFO - Model pipeline loaded.
2025-12-14 21:37:00,640 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 21:37:11,531 - INFO - Dataset loaded with 500 entries.
2025-12-14 21:37:11,531 - INFO - Dataset processed with 500 entries.
2025-12-14 21:37:11,545 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<39:42,  4.77s/it]Running Inference:   0%|          | 2/500 [00:06<25:00,  3.01s/it]Running Inference:   1%|          | 3/500 [00:10<29:42,  3.59s/it]Running Inference:   1%|          | 4/500 [00:14<30:20,  3.67s/it]Running Inference:   1%|          | 5/500 [00:18<30:41,  3.72s/it]Running Inference:   1%|          | 6/500 [00:22<31:30,  3.83s/it]Running Inference:   1%|▏         | 7/500 [00:26<31:33,  3.84s/it]Running Inference:   2%|▏         | 8/500 [00:30<31:17,  3.82s/it]Running Inference:   2%|▏         | 9/500 [00:33<31:06,  3.80s/it]Running Inference:   2%|▏         | 10/500 [00:37<30:57,  3.79s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:41<31:59,  3.93s/it]Running Inference:   2%|▏         | 12/500 [00:45<31:39,  3.89s/it]Running Inference:   3%|▎         | 13/500 [00:49<31:21,  3.86s/it]Running Inference:   3%|▎         | 14/500 [00:53<31:08,  3.84s/it]Running Inference:   3%|▎         | 15/500 [00:53<23:17,  2.88s/it]Running Inference:   3%|▎         | 16/500 [00:57<25:24,  3.15s/it]Running Inference:   3%|▎         | 17/500 [01:02<28:36,  3.55s/it]Running Inference:   4%|▎         | 18/500 [01:06<29:20,  3.65s/it]Running Inference:   4%|▍         | 19/500 [01:09<29:39,  3.70s/it]Running Inference:   4%|▍         | 20/500 [01:13<29:47,  3.72s/it]Running Inference:   4%|▍         | 21/500 [01:17<29:53,  3.74s/it]Running Inference:   4%|▍         | 22/500 [01:21<29:54,  3.75s/it]Running Inference:   5%|▍         | 23/500 [01:24<29:51,  3.76s/it]Running Inference:   5%|▍         | 24/500 [01:28<29:49,  3.76s/it]Running Inference:   5%|▌         | 25/500 [01:32<29:52,  3.77s/it]Running Inference:   5%|▌         | 26/500 [01:36<30:00,  3.80s/it]Running Inference:   5%|▌         | 27/500 [01:40<29:56,  3.80s/it]Running Inference:   6%|▌         | 28/500 [01:45<33:00,  4.20s/it]Running Inference:   6%|▌         | 29/500 [01:49<32:33,  4.15s/it]Running Inference:   6%|▌         | 30/500 [01:53<33:08,  4.23s/it]Running Inference:   6%|▌         | 31/500 [01:57<32:17,  4.13s/it]Running Inference:   6%|▋         | 32/500 [02:01<31:34,  4.05s/it]Running Inference:   7%|▋         | 33/500 [02:02<23:50,  3.06s/it]Running Inference:   7%|▋         | 34/500 [02:06<25:27,  3.28s/it]Running Inference:   7%|▋         | 35/500 [02:06<18:43,  2.42s/it]Running Inference:   7%|▋         | 36/500 [02:10<22:50,  2.95s/it]Running Inference:   7%|▋         | 37/500 [02:14<25:42,  3.33s/it]Running Inference:   8%|▊         | 38/500 [02:18<26:46,  3.48s/it]Running Inference:   8%|▊         | 39/500 [02:22<28:24,  3.70s/it]Running Inference:   8%|▊         | 40/500 [02:26<28:32,  3.72s/it]Running Inference:   8%|▊         | 41/500 [02:30<28:45,  3.76s/it]Running Inference:   8%|▊         | 42/500 [02:35<30:44,  4.03s/it]Running Inference:   9%|▊         | 43/500 [02:39<31:52,  4.18s/it]Running Inference:   9%|▉         | 44/500 [02:42<28:12,  3.71s/it]Running Inference:   9%|▉         | 45/500 [02:46<28:31,  3.76s/it]Running Inference:   9%|▉         | 46/500 [02:50<28:25,  3.76s/it]Running Inference:   9%|▉         | 47/500 [02:53<28:41,  3.80s/it]Running Inference:  10%|▉         | 48/500 [02:57<28:29,  3.78s/it]Running Inference:  10%|▉         | 49/500 [03:01<28:49,  3.84s/it]Running Inference:  10%|█         | 50/500 [03:06<30:21,  4.05s/it]Running Inference:  10%|█         | 51/500 [03:09<29:41,  3.97s/it]Running Inference:  10%|█         | 52/500 [03:13<29:15,  3.92s/it]Running Inference:  11%|█         | 53/500 [03:14<21:53,  2.94s/it]Running Inference:  11%|█         | 54/500 [03:18<23:47,  3.20s/it]Running Inference:  11%|█         | 55/500 [03:21<25:00,  3.37s/it]Running Inference:  11%|█         | 56/500 [03:25<26:02,  3.52s/it]Running Inference:  11%|█▏        | 57/500 [03:34<36:52,  4.99s/it]Running Inference:  12%|█▏        | 58/500 [03:38<34:01,  4.62s/it]Running Inference:  12%|█▏        | 59/500 [03:40<28:16,  3.85s/it]Running Inference:  12%|█▏        | 60/500 [03:43<28:10,  3.84s/it]Running Inference:  12%|█▏        | 61/500 [03:47<28:05,  3.84s/it]Running Inference:  12%|█▏        | 62/500 [03:51<27:51,  3.82s/it]Running Inference:  13%|█▎        | 63/500 [03:55<27:45,  3.81s/it]Running Inference:  13%|█▎        | 64/500 [03:59<27:35,  3.80s/it]Running Inference:  13%|█▎        | 65/500 [04:02<27:30,  3.79s/it]Running Inference:  13%|█▎        | 66/500 [04:06<28:04,  3.88s/it]Running Inference:  13%|█▎        | 67/500 [04:10<27:54,  3.87s/it]Running Inference:  14%|█▎        | 68/500 [04:14<27:43,  3.85s/it]Running Inference:  14%|█▍        | 69/500 [04:18<27:30,  3.83s/it]Running Inference:  14%|█▍        | 70/500 [04:22<27:19,  3.81s/it]Running Inference:  14%|█▍        | 71/500 [04:25<27:09,  3.80s/it]Running Inference:  14%|█▍        | 72/500 [04:29<27:35,  3.87s/it]Running Inference:  15%|█▍        | 73/500 [04:33<27:18,  3.84s/it]Running Inference:  15%|█▍        | 74/500 [04:34<20:23,  2.87s/it]Running Inference:  15%|█▌        | 75/500 [04:38<22:50,  3.23s/it]Running Inference:  15%|█▌        | 76/500 [04:42<24:18,  3.44s/it]Running Inference:  15%|█▌        | 77/500 [04:46<25:06,  3.56s/it]Running Inference:  16%|█▌        | 78/500 [04:50<26:28,  3.76s/it]Running Inference:  16%|█▌        | 79/500 [04:54<26:20,  3.75s/it]Running Inference:  16%|█▌        | 80/500 [04:57<26:23,  3.77s/it]Running Inference:  16%|█▌        | 81/500 [05:02<27:16,  3.91s/it]Running Inference:  16%|█▋        | 82/500 [05:06<28:04,  4.03s/it]Running Inference:  17%|█▋        | 83/500 [05:10<27:41,  3.99s/it]Running Inference:  17%|█▋        | 84/500 [05:14<27:19,  3.94s/it]Running Inference:  17%|█▋        | 85/500 [05:19<29:04,  4.20s/it]Running Inference:  17%|█▋        | 86/500 [05:22<28:10,  4.08s/it]Running Inference:  17%|█▋        | 87/500 [05:24<22:44,  3.30s/it]Running Inference:  18%|█▊        | 88/500 [05:25<18:59,  2.77s/it]Running Inference:  18%|█▊        | 89/500 [05:29<21:16,  3.11s/it]Running Inference:  18%|█▊        | 90/500 [05:33<23:22,  3.42s/it]Running Inference:  18%|█▊        | 91/500 [05:37<24:36,  3.61s/it]Running Inference:  18%|█▊        | 92/500 [05:41<25:09,  3.70s/it]Running Inference:  19%|█▊        | 93/500 [05:45<25:22,  3.74s/it]Running Inference:  19%|█▉        | 94/500 [05:49<25:25,  3.76s/it]Running Inference:  19%|█▉        | 95/500 [05:53<25:30,  3.78s/it]Running Inference:  19%|█▉        | 96/500 [05:57<25:39,  3.81s/it]Running Inference:  19%|█▉        | 97/500 [06:00<25:37,  3.82s/it]Running Inference:  20%|█▉        | 98/500 [06:04<25:28,  3.80s/it]Running Inference:  20%|█▉        | 99/500 [06:08<25:27,  3.81s/it]Running Inference:  20%|██        | 100/500 [06:12<26:07,  3.92s/it]Running Inference:  20%|██        | 101/500 [06:16<26:20,  3.96s/it]Running Inference:  20%|██        | 102/500 [06:20<26:13,  3.95s/it]Running Inference:  21%|██        | 103/500 [06:24<25:45,  3.89s/it]Running Inference:  21%|██        | 104/500 [06:26<22:51,  3.46s/it]Running Inference:  21%|██        | 105/500 [06:31<25:00,  3.80s/it]Running Inference:  21%|██        | 106/500 [06:36<28:02,  4.27s/it]Running Inference:  21%|██▏       | 107/500 [06:38<22:01,  3.36s/it]Running Inference:  22%|██▏       | 108/500 [06:42<22:59,  3.52s/it]Running Inference:  22%|██▏       | 109/500 [06:45<23:33,  3.61s/it]Running Inference:  22%|██▏       | 110/500 [06:49<24:01,  3.70s/it]Running Inference:  22%|██▏       | 111/500 [06:50<17:40,  2.73s/it]Running Inference:  22%|██▏       | 112/500 [06:54<19:40,  3.04s/it]Running Inference:  23%|██▎       | 113/500 [06:58<21:43,  3.37s/it]Running Inference:  23%|██▎       | 114/500 [07:03<25:07,  3.90s/it]Running Inference:  23%|██▎       | 115/500 [07:07<25:22,  3.96s/it]Running Inference:  23%|██▎       | 116/500 [07:11<25:38,  4.01s/it]Running Inference:  23%|██▎       | 117/500 [07:15<25:05,  3.93s/it]Running Inference:  24%|██▎       | 118/500 [07:19<25:57,  4.08s/it]Running Inference:  24%|██▍       | 119/500 [07:23<25:13,  3.97s/it]Running Inference:  24%|██▍       | 120/500 [07:27<25:30,  4.03s/it]Running Inference:  24%|██▍       | 121/500 [07:31<25:08,  3.98s/it]Running Inference:  24%|██▍       | 122/500 [07:35<24:33,  3.90s/it]Running Inference:  25%|██▍       | 123/500 [07:38<24:14,  3.86s/it]Running Inference:  25%|██▍       | 124/500 [07:42<23:59,  3.83s/it]Running Inference:  25%|██▌       | 125/500 [07:46<24:23,  3.90s/it]Running Inference:  25%|██▌       | 126/500 [07:50<24:06,  3.87s/it]Running Inference:  25%|██▌       | 127/500 [07:54<23:49,  3.83s/it]Running Inference:  26%|██▌       | 128/500 [07:58<24:00,  3.87s/it]Running Inference:  26%|██▌       | 129/500 [08:02<23:45,  3.84s/it]Running Inference:  26%|██▌       | 130/500 [08:05<23:46,  3.86s/it]Running Inference:  26%|██▌       | 131/500 [08:09<23:34,  3.83s/it]Running Inference:  26%|██▋       | 132/500 [08:13<23:21,  3.81s/it]Running Inference:  27%|██▋       | 133/500 [08:17<23:22,  3.82s/it]Running Inference:  27%|██▋       | 134/500 [08:21<23:18,  3.82s/it]Running Inference:  27%|██▋       | 135/500 [08:25<23:25,  3.85s/it]Running Inference:  27%|██▋       | 136/500 [08:28<23:23,  3.86s/it]Running Inference:  27%|██▋       | 137/500 [08:32<23:06,  3.82s/it]Running Inference:  28%|██▊       | 138/500 [08:36<23:11,  3.84s/it]Running Inference:  28%|██▊       | 139/500 [08:40<22:55,  3.81s/it]Running Inference:  28%|██▊       | 140/500 [08:44<23:00,  3.83s/it]Running Inference:  28%|██▊       | 141/500 [08:48<23:40,  3.96s/it]Running Inference:  28%|██▊       | 142/500 [08:52<23:54,  4.01s/it]Running Inference:  29%|██▊       | 143/500 [08:56<23:36,  3.97s/it]Running Inference:  29%|██▉       | 144/500 [09:00<23:12,  3.91s/it]Running Inference:  29%|██▉       | 145/500 [09:04<23:13,  3.92s/it]Running Inference:  29%|██▉       | 146/500 [09:08<23:22,  3.96s/it]Running Inference:  29%|██▉       | 147/500 [09:12<23:32,  4.00s/it]Running Inference:  30%|██▉       | 148/500 [09:16<23:01,  3.93s/it]Running Inference:  30%|██▉       | 149/500 [09:19<22:41,  3.88s/it]Running Inference:  30%|███       | 150/500 [09:23<22:24,  3.84s/it]Running Inference:  30%|███       | 151/500 [09:24<18:01,  3.10s/it]Running Inference:  30%|███       | 152/500 [09:28<19:11,  3.31s/it]Running Inference:  31%|███       | 153/500 [09:32<19:59,  3.46s/it]Running Inference:  31%|███       | 154/500 [09:36<20:30,  3.56s/it]Running Inference:  31%|███       | 155/500 [09:40<20:46,  3.61s/it]Running Inference:  31%|███       | 156/500 [09:40<15:46,  2.75s/it]Running Inference:  31%|███▏      | 157/500 [09:44<17:52,  3.13s/it]Running Inference:  32%|███▏      | 158/500 [09:48<19:00,  3.34s/it]Running Inference:  32%|███▏      | 159/500 [09:52<19:43,  3.47s/it]Running Inference:  32%|███▏      | 160/500 [09:56<20:11,  3.56s/it]Running Inference:  32%|███▏      | 161/500 [09:57<15:31,  2.75s/it]Running Inference:  32%|███▏      | 162/500 [10:00<17:28,  3.10s/it]Running Inference:  33%|███▎      | 163/500 [10:01<13:03,  2.33s/it]Running Inference:  33%|███▎      | 164/500 [10:05<15:33,  2.78s/it]Running Inference:  33%|███▎      | 165/500 [10:09<17:18,  3.10s/it]Running Inference:  33%|███▎      | 166/500 [10:12<18:19,  3.29s/it]Running Inference:  33%|███▎      | 167/500 [10:17<19:44,  3.56s/it]Running Inference:  34%|███▎      | 168/500 [10:20<20:06,  3.63s/it]Running Inference:  34%|███▍      | 169/500 [10:24<20:26,  3.71s/it]Running Inference:  34%|███▍      | 170/500 [10:28<20:30,  3.73s/it]Running Inference:  34%|███▍      | 171/500 [10:32<20:46,  3.79s/it]Running Inference:  34%|███▍      | 172/500 [10:36<20:43,  3.79s/it]Running Inference:  35%|███▍      | 173/500 [10:40<20:40,  3.79s/it]Running Inference:  35%|███▍      | 174/500 [10:44<21:04,  3.88s/it]Running Inference:  35%|███▌      | 175/500 [10:48<21:51,  4.04s/it]Running Inference:  35%|███▌      | 176/500 [10:52<21:26,  3.97s/it]Running Inference:  35%|███▌      | 177/500 [10:56<22:11,  4.12s/it]Running Inference:  36%|███▌      | 178/500 [11:00<21:27,  4.00s/it]Running Inference:  36%|███▌      | 179/500 [11:04<21:02,  3.93s/it]Running Inference:  36%|███▌      | 180/500 [11:08<20:52,  3.91s/it]Running Inference:  36%|███▌      | 181/500 [11:11<20:35,  3.87s/it]Running Inference:  36%|███▋      | 182/500 [11:15<20:19,  3.83s/it]Running Inference:  37%|███▋      | 183/500 [11:19<20:29,  3.88s/it]Running Inference:  37%|███▋      | 184/500 [11:23<20:26,  3.88s/it]Running Inference:  37%|███▋      | 185/500 [11:24<15:52,  3.02s/it]Running Inference:  37%|███▋      | 186/500 [11:28<17:04,  3.26s/it]Running Inference:  37%|███▋      | 187/500 [11:29<13:10,  2.53s/it]Running Inference:  38%|███▊      | 188/500 [11:33<15:05,  2.90s/it]Running Inference:  38%|███▊      | 189/500 [11:36<16:31,  3.19s/it]Running Inference:  38%|███▊      | 190/500 [11:41<18:01,  3.49s/it]Running Inference:  38%|███▊      | 191/500 [11:44<18:36,  3.61s/it]Running Inference:  38%|███▊      | 192/500 [11:48<19:03,  3.71s/it]Running Inference:  39%|███▊      | 193/500 [11:49<14:23,  2.81s/it]Running Inference:  39%|███▉      | 194/500 [11:53<15:52,  3.11s/it]Running Inference:  39%|███▉      | 195/500 [11:57<16:52,  3.32s/it]Running Inference:  39%|███▉      | 196/500 [12:01<17:33,  3.46s/it]Running Inference:  39%|███▉      | 197/500 [12:04<18:11,  3.60s/it]Running Inference:  40%|███▉      | 198/500 [12:05<13:43,  2.73s/it]Running Inference:  40%|███▉      | 199/500 [12:06<10:12,  2.04s/it]Running Inference:  40%|████      | 200/500 [12:09<12:58,  2.59s/it]Running Inference:  40%|████      | 201/500 [12:13<14:59,  3.01s/it]Running Inference:  40%|████      | 202/500 [12:17<16:08,  3.25s/it]Running Inference:  41%|████      | 203/500 [12:22<19:02,  3.85s/it]Running Inference:  41%|████      | 204/500 [12:26<18:57,  3.84s/it]Running Inference:  41%|████      | 205/500 [12:30<18:47,  3.82s/it]Running Inference:  41%|████      | 206/500 [12:34<18:37,  3.80s/it]Running Inference:  41%|████▏     | 207/500 [12:35<14:26,  2.96s/it]Running Inference:  42%|████▏     | 208/500 [12:36<11:37,  2.39s/it]Running Inference:  42%|████▏     | 209/500 [12:40<14:13,  2.93s/it]Running Inference:  42%|████▏     | 210/500 [12:44<15:24,  3.19s/it]Running Inference:  42%|████▏     | 211/500 [12:48<16:32,  3.43s/it]Running Inference:  42%|████▏     | 212/500 [12:49<13:00,  2.71s/it]Running Inference:  43%|████▎     | 213/500 [12:53<14:26,  3.02s/it]Running Inference:  43%|████▎     | 214/500 [12:56<15:30,  3.25s/it]Running Inference:  43%|████▎     | 215/500 [13:00<16:19,  3.44s/it]Running Inference:  43%|████▎     | 216/500 [13:04<16:45,  3.54s/it]Running Inference:  43%|████▎     | 217/500 [13:08<17:27,  3.70s/it]Running Inference:  44%|████▎     | 218/500 [13:12<17:48,  3.79s/it]Running Inference:  44%|████▍     | 219/500 [13:16<17:41,  3.78s/it]Running Inference:  44%|████▍     | 220/500 [13:20<17:40,  3.79s/it]Running Inference:  44%|████▍     | 221/500 [13:23<17:31,  3.77s/it]Running Inference:  44%|████▍     | 222/500 [13:27<17:27,  3.77s/it]Running Inference:  45%|████▍     | 223/500 [13:31<17:25,  3.78s/it]Running Inference:  45%|████▍     | 224/500 [13:35<17:24,  3.78s/it]Running Inference:  45%|████▌     | 225/500 [13:39<17:17,  3.77s/it]Running Inference:  45%|████▌     | 226/500 [13:42<17:11,  3.76s/it]Running Inference:  45%|████▌     | 227/500 [13:46<17:08,  3.77s/it]Running Inference:  46%|████▌     | 228/500 [13:50<16:58,  3.74s/it]Running Inference:  46%|████▌     | 229/500 [13:54<16:53,  3.74s/it]Running Inference:  46%|████▌     | 230/500 [13:57<16:52,  3.75s/it]Running Inference:  46%|████▌     | 231/500 [13:58<12:58,  2.89s/it]Running Inference:  46%|████▋     | 232/500 [13:59<10:12,  2.28s/it]Running Inference:  47%|████▋     | 233/500 [14:03<12:17,  2.76s/it]Running Inference:  47%|████▋     | 234/500 [14:07<13:40,  3.08s/it]Running Inference:  47%|████▋     | 235/500 [14:11<15:00,  3.40s/it]Running Inference:  47%|████▋     | 236/500 [14:15<15:21,  3.49s/it]Running Inference:  47%|████▋     | 237/500 [14:18<15:38,  3.57s/it]Running Inference:  48%|████▊     | 238/500 [14:22<15:51,  3.63s/it]Running Inference:  48%|████▊     | 239/500 [14:26<16:07,  3.71s/it]Running Inference:  48%|████▊     | 240/500 [14:30<16:11,  3.74s/it]Running Inference:  48%|████▊     | 241/500 [14:34<16:14,  3.76s/it]Running Inference:  48%|████▊     | 242/500 [14:37<16:06,  3.75s/it]Running Inference:  49%|████▊     | 243/500 [14:42<16:59,  3.97s/it]Running Inference:  49%|████▉     | 244/500 [14:46<17:16,  4.05s/it]Running Inference:  49%|████▉     | 245/500 [14:50<17:00,  4.00s/it]Running Inference:  49%|████▉     | 246/500 [14:54<16:34,  3.91s/it]Running Inference:  49%|████▉     | 247/500 [14:58<16:28,  3.91s/it]Running Inference:  50%|████▉     | 248/500 [15:01<16:17,  3.88s/it]Running Inference:  50%|████▉     | 249/500 [15:05<16:10,  3.87s/it]Running Inference:  50%|█████     | 250/500 [15:09<16:05,  3.86s/it]Running Inference:  50%|█████     | 251/500 [15:13<16:20,  3.94s/it]Running Inference:  50%|█████     | 252/500 [15:18<16:59,  4.11s/it]Running Inference:  51%|█████     | 253/500 [15:19<13:12,  3.21s/it]Running Inference:  51%|█████     | 254/500 [15:23<13:52,  3.39s/it]Running Inference:  51%|█████     | 255/500 [15:27<14:31,  3.56s/it]Running Inference:  51%|█████     | 256/500 [15:30<14:44,  3.63s/it]Running Inference:  51%|█████▏    | 257/500 [15:34<15:08,  3.74s/it]Running Inference:  52%|█████▏    | 258/500 [15:38<15:07,  3.75s/it]Running Inference:  52%|█████▏    | 259/500 [15:42<15:09,  3.77s/it]Running Inference:  52%|█████▏    | 260/500 [15:46<15:04,  3.77s/it]Running Inference:  52%|█████▏    | 261/500 [15:49<14:57,  3.75s/it]Running Inference:  52%|█████▏    | 262/500 [15:50<11:05,  2.80s/it]Running Inference:  53%|█████▎    | 263/500 [15:54<12:14,  3.10s/it]Running Inference:  53%|█████▎    | 264/500 [15:58<12:57,  3.29s/it]Running Inference:  53%|█████▎    | 265/500 [16:02<13:41,  3.49s/it]Running Inference:  53%|█████▎    | 266/500 [16:05<13:59,  3.59s/it]Running Inference:  53%|█████▎    | 267/500 [16:10<14:43,  3.79s/it]Running Inference:  54%|█████▎    | 268/500 [16:13<14:40,  3.79s/it]Running Inference:  54%|█████▍    | 269/500 [16:17<14:39,  3.81s/it]Running Inference:  54%|█████▍    | 270/500 [16:21<14:34,  3.80s/it]Running Inference:  54%|█████▍    | 271/500 [16:25<14:24,  3.77s/it]Running Inference:  54%|█████▍    | 272/500 [16:29<14:22,  3.78s/it]Running Inference:  55%|█████▍    | 273/500 [16:30<11:40,  3.09s/it]Running Inference:  55%|█████▍    | 274/500 [16:34<12:21,  3.28s/it]Running Inference:  55%|█████▌    | 275/500 [16:37<12:51,  3.43s/it]Running Inference:  55%|█████▌    | 276/500 [16:41<13:16,  3.55s/it]Running Inference:  55%|█████▌    | 277/500 [16:45<13:28,  3.63s/it]Running Inference:  56%|█████▌    | 278/500 [16:49<13:40,  3.69s/it]Running Inference:  56%|█████▌    | 279/500 [16:53<13:56,  3.79s/it]Running Inference:  56%|█████▌    | 280/500 [16:57<14:10,  3.87s/it]Running Inference:  56%|█████▌    | 281/500 [17:01<14:07,  3.87s/it]Running Inference:  56%|█████▋    | 282/500 [17:05<14:00,  3.86s/it]Running Inference:  57%|█████▋    | 283/500 [17:09<13:53,  3.84s/it]Running Inference:  57%|█████▋    | 284/500 [17:12<13:54,  3.86s/it]Running Inference:  57%|█████▋    | 285/500 [17:16<13:52,  3.87s/it]Running Inference:  57%|█████▋    | 286/500 [17:20<13:40,  3.83s/it]Running Inference:  57%|█████▋    | 287/500 [17:25<14:26,  4.07s/it]Running Inference:  58%|█████▊    | 288/500 [17:29<14:19,  4.05s/it]Running Inference:  58%|█████▊    | 289/500 [17:33<13:58,  3.97s/it]Running Inference:  58%|█████▊    | 290/500 [17:36<13:50,  3.96s/it]Running Inference:  58%|█████▊    | 291/500 [17:38<11:32,  3.31s/it]Running Inference:  58%|█████▊    | 292/500 [17:42<11:54,  3.44s/it]Running Inference:  59%|█████▊    | 293/500 [17:43<09:23,  2.72s/it]Running Inference:  59%|█████▉    | 294/500 [17:44<07:33,  2.20s/it]Running Inference:  59%|█████▉    | 295/500 [17:48<09:09,  2.68s/it]Running Inference:  59%|█████▉    | 296/500 [17:52<10:15,  3.02s/it]Running Inference:  59%|█████▉    | 297/500 [17:55<11:04,  3.27s/it]Running Inference:  60%|█████▉    | 298/500 [17:59<11:31,  3.42s/it]Running Inference:  60%|█████▉    | 299/500 [18:03<11:47,  3.52s/it]Running Inference:  60%|██████    | 300/500 [18:07<11:55,  3.58s/it]Running Inference:  60%|██████    | 301/500 [18:11<12:20,  3.72s/it]Running Inference:  60%|██████    | 302/500 [18:15<12:21,  3.74s/it]Running Inference:  61%|██████    | 303/500 [18:18<12:21,  3.76s/it]Running Inference:  61%|██████    | 304/500 [18:23<12:49,  3.93s/it]Running Inference:  61%|██████    | 305/500 [18:26<12:34,  3.87s/it]Running Inference:  61%|██████    | 306/500 [18:31<12:52,  3.98s/it]Running Inference:  61%|██████▏   | 307/500 [18:35<12:40,  3.94s/it]Running Inference:  62%|██████▏   | 308/500 [18:38<12:24,  3.88s/it]Running Inference:  62%|██████▏   | 309/500 [18:42<12:13,  3.84s/it]Running Inference:  62%|██████▏   | 310/500 [18:46<12:07,  3.83s/it]Running Inference:  62%|██████▏   | 311/500 [18:50<12:10,  3.86s/it]Running Inference:  62%|██████▏   | 312/500 [18:50<09:05,  2.90s/it]Running Inference:  63%|██████▎   | 313/500 [18:54<09:59,  3.21s/it]Running Inference:  63%|██████▎   | 314/500 [18:58<10:26,  3.37s/it]Running Inference:  63%|██████▎   | 315/500 [18:59<07:42,  2.50s/it]Running Inference:  63%|██████▎   | 316/500 [19:02<08:49,  2.88s/it]Running Inference:  63%|██████▎   | 317/500 [19:06<09:35,  3.14s/it]Running Inference:  64%|██████▎   | 318/500 [19:10<10:06,  3.33s/it]Running Inference:  64%|██████▍   | 319/500 [19:14<10:29,  3.48s/it]Running Inference:  64%|██████▍   | 320/500 [19:18<11:12,  3.74s/it]Running Inference:  64%|██████▍   | 321/500 [19:19<08:43,  2.93s/it]Running Inference:  64%|██████▍   | 322/500 [19:23<09:27,  3.19s/it]Running Inference:  65%|██████▍   | 323/500 [19:27<09:56,  3.37s/it]Running Inference:  65%|██████▍   | 324/500 [19:30<10:16,  3.50s/it]Running Inference:  65%|██████▌   | 325/500 [19:34<10:29,  3.60s/it]Running Inference:  65%|██████▌   | 326/500 [19:38<10:57,  3.78s/it]Running Inference:  65%|██████▌   | 327/500 [19:44<12:13,  4.24s/it]Running Inference:  66%|██████▌   | 328/500 [19:48<12:11,  4.25s/it]Running Inference:  66%|██████▌   | 329/500 [19:52<11:40,  4.09s/it]Running Inference:  66%|██████▌   | 330/500 [19:56<11:17,  3.99s/it]Running Inference:  66%|██████▌   | 331/500 [20:00<11:39,  4.14s/it]Running Inference:  66%|██████▋   | 332/500 [20:04<11:29,  4.10s/it]Running Inference:  67%|██████▋   | 333/500 [20:08<11:10,  4.02s/it]Running Inference:  67%|██████▋   | 334/500 [20:10<09:14,  3.34s/it]Running Inference:  67%|██████▋   | 335/500 [20:13<09:33,  3.48s/it]Running Inference:  67%|██████▋   | 336/500 [20:14<07:00,  2.56s/it]Running Inference:  67%|██████▋   | 337/500 [20:18<07:57,  2.93s/it]Running Inference:  68%|██████▊   | 338/500 [20:21<08:37,  3.20s/it]Running Inference:  68%|██████▊   | 339/500 [20:26<09:22,  3.50s/it]Running Inference:  68%|██████▊   | 340/500 [20:31<11:11,  4.19s/it]Running Inference:  68%|██████▊   | 341/500 [20:32<08:34,  3.23s/it]Running Inference:  68%|██████▊   | 342/500 [20:36<08:59,  3.42s/it]Running Inference:  69%|██████▊   | 343/500 [20:40<09:30,  3.64s/it]Running Inference:  69%|██████▉   | 344/500 [20:45<09:47,  3.77s/it]Running Inference:  69%|██████▉   | 345/500 [20:48<09:49,  3.80s/it]Running Inference:  69%|██████▉   | 346/500 [20:52<09:54,  3.86s/it]Running Inference:  69%|██████▉   | 347/500 [20:56<09:51,  3.86s/it]Running Inference:  70%|██████▉   | 348/500 [21:00<09:47,  3.86s/it]Running Inference:  70%|██████▉   | 349/500 [21:01<07:38,  3.04s/it]Running Inference:  70%|███████   | 350/500 [21:07<09:47,  3.92s/it]Running Inference:  70%|███████   | 351/500 [21:11<10:00,  4.03s/it]Running Inference:  70%|███████   | 352/500 [21:15<09:43,  3.95s/it]Running Inference:  71%|███████   | 353/500 [21:19<09:31,  3.89s/it]Running Inference:  71%|███████   | 354/500 [21:23<09:29,  3.90s/it]Running Inference:  71%|███████   | 355/500 [21:27<09:18,  3.85s/it]Running Inference:  71%|███████   | 356/500 [21:30<09:12,  3.83s/it]Running Inference:  71%|███████▏  | 357/500 [21:35<09:23,  3.94s/it]Running Inference:  72%|███████▏  | 358/500 [21:38<09:13,  3.90s/it]Running Inference:  72%|███████▏  | 359/500 [21:42<09:03,  3.85s/it]Running Inference:  72%|███████▏  | 360/500 [21:46<08:56,  3.83s/it]Running Inference:  72%|███████▏  | 361/500 [21:50<08:54,  3.85s/it]Running Inference:  72%|███████▏  | 362/500 [21:54<08:56,  3.89s/it]Running Inference:  73%|███████▎  | 363/500 [21:58<08:53,  3.90s/it]Running Inference:  73%|███████▎  | 364/500 [22:02<08:52,  3.92s/it]Running Inference:  73%|███████▎  | 365/500 [22:07<09:31,  4.23s/it]Running Inference:  73%|███████▎  | 366/500 [22:11<09:26,  4.23s/it]Running Inference:  73%|███████▎  | 367/500 [22:15<09:07,  4.12s/it]Running Inference:  74%|███████▎  | 368/500 [22:19<08:58,  4.08s/it]Running Inference:  74%|███████▍  | 369/500 [22:23<08:57,  4.10s/it]Running Inference:  74%|███████▍  | 370/500 [22:27<09:02,  4.18s/it]Running Inference:  74%|███████▍  | 371/500 [22:31<08:44,  4.06s/it]Running Inference:  74%|███████▍  | 372/500 [22:35<08:35,  4.03s/it]Running Inference:  75%|███████▍  | 373/500 [22:39<08:24,  3.97s/it]Running Inference:  75%|███████▍  | 374/500 [22:43<08:17,  3.95s/it]Running Inference:  75%|███████▌  | 375/500 [22:44<06:19,  3.03s/it]Running Inference:  75%|███████▌  | 376/500 [22:47<06:46,  3.28s/it]Running Inference:  75%|███████▌  | 377/500 [22:51<07:02,  3.44s/it]Running Inference:  76%|███████▌  | 378/500 [22:55<07:15,  3.57s/it]Running Inference:  76%|███████▌  | 379/500 [22:59<07:28,  3.70s/it]Running Inference:  76%|███████▌  | 380/500 [23:03<07:27,  3.73s/it]Running Inference:  76%|███████▌  | 381/500 [23:07<07:26,  3.75s/it]Running Inference:  76%|███████▋  | 382/500 [23:11<07:37,  3.88s/it]Running Inference:  77%|███████▋  | 383/500 [23:15<07:38,  3.92s/it]Running Inference:  77%|███████▋  | 384/500 [23:19<07:30,  3.88s/it]Running Inference:  77%|███████▋  | 385/500 [23:23<07:31,  3.93s/it]Running Inference:  77%|███████▋  | 386/500 [23:27<07:30,  3.95s/it]Running Inference:  77%|███████▋  | 387/500 [23:31<07:25,  3.94s/it]Running Inference:  78%|███████▊  | 388/500 [23:35<07:15,  3.89s/it]Running Inference:  78%|███████▊  | 389/500 [23:38<07:12,  3.89s/it]Running Inference:  78%|███████▊  | 390/500 [23:42<07:04,  3.86s/it]Running Inference:  78%|███████▊  | 391/500 [23:46<06:58,  3.84s/it]Running Inference:  78%|███████▊  | 392/500 [23:50<06:52,  3.82s/it]Running Inference:  79%|███████▊  | 393/500 [23:53<06:44,  3.78s/it]Running Inference:  79%|███████▉  | 394/500 [23:57<06:41,  3.79s/it]Running Inference:  79%|███████▉  | 395/500 [23:59<05:27,  3.12s/it]Running Inference:  79%|███████▉  | 396/500 [24:03<05:42,  3.29s/it]Running Inference:  79%|███████▉  | 397/500 [24:07<06:03,  3.53s/it]Running Inference:  80%|███████▉  | 398/500 [24:10<06:05,  3.59s/it]Running Inference:  80%|███████▉  | 399/500 [24:14<06:06,  3.63s/it]Running Inference:  80%|████████  | 400/500 [24:15<04:31,  2.71s/it]Running Inference:  80%|████████  | 401/500 [24:18<04:58,  3.02s/it]Running Inference:  80%|████████  | 402/500 [24:22<05:15,  3.22s/it]Running Inference:  81%|████████  | 403/500 [24:26<05:27,  3.38s/it]Running Inference:  81%|████████  | 404/500 [24:30<05:34,  3.48s/it]Running Inference:  81%|████████  | 405/500 [24:33<05:40,  3.58s/it]Running Inference:  81%|████████  | 406/500 [24:37<05:41,  3.64s/it]Running Inference:  81%|████████▏ | 407/500 [24:37<04:08,  2.67s/it]Running Inference:  82%|████████▏ | 408/500 [24:41<04:35,  3.00s/it]Running Inference:  82%|████████▏ | 409/500 [24:45<04:54,  3.23s/it]Running Inference:  82%|████████▏ | 410/500 [24:49<05:06,  3.41s/it]Running Inference:  82%|████████▏ | 411/500 [24:53<05:14,  3.54s/it]Running Inference:  82%|████████▏ | 412/500 [24:56<05:18,  3.61s/it]Running Inference:  83%|████████▎ | 413/500 [25:00<05:17,  3.65s/it]Running Inference:  83%|████████▎ | 414/500 [25:04<05:17,  3.69s/it]Running Inference:  83%|████████▎ | 415/500 [25:08<05:15,  3.71s/it]Running Inference:  83%|████████▎ | 416/500 [25:11<05:11,  3.70s/it]Running Inference:  83%|████████▎ | 417/500 [25:15<05:08,  3.71s/it]Running Inference:  84%|████████▎ | 418/500 [25:19<05:06,  3.74s/it]Running Inference:  84%|████████▍ | 419/500 [25:23<05:02,  3.74s/it]Running Inference:  84%|████████▍ | 420/500 [25:26<04:58,  3.74s/it]Running Inference:  84%|████████▍ | 421/500 [25:30<04:55,  3.74s/it]Running Inference:  84%|████████▍ | 422/500 [25:31<03:32,  2.73s/it]Running Inference:  85%|████████▍ | 423/500 [25:31<02:43,  2.12s/it]Running Inference:  85%|████████▍ | 424/500 [25:35<03:18,  2.61s/it]Running Inference:  85%|████████▌ | 425/500 [25:39<03:48,  3.05s/it]Running Inference:  85%|████████▌ | 426/500 [25:43<04:01,  3.26s/it]Running Inference:  85%|████████▌ | 427/500 [25:47<04:07,  3.39s/it]Running Inference:  86%|████████▌ | 428/500 [25:50<04:11,  3.49s/it]Running Inference:  86%|████████▌ | 429/500 [25:54<04:12,  3.56s/it]Running Inference:  86%|████████▌ | 430/500 [25:58<04:21,  3.74s/it]Running Inference:  86%|████████▌ | 431/500 [26:02<04:19,  3.76s/it]Running Inference:  86%|████████▋ | 432/500 [26:06<04:21,  3.85s/it]Running Inference:  87%|████████▋ | 433/500 [26:10<04:21,  3.90s/it]Running Inference:  87%|████████▋ | 434/500 [26:14<04:18,  3.91s/it]Running Inference:  87%|████████▋ | 435/500 [26:18<04:11,  3.87s/it]Running Inference:  87%|████████▋ | 436/500 [26:22<04:07,  3.86s/it]Running Inference:  87%|████████▋ | 437/500 [26:25<04:01,  3.84s/it]Running Inference:  88%|████████▊ | 438/500 [26:29<03:55,  3.80s/it]Running Inference:  88%|████████▊ | 439/500 [26:33<03:52,  3.81s/it]Running Inference:  88%|████████▊ | 440/500 [26:37<03:48,  3.80s/it]Running Inference:  88%|████████▊ | 441/500 [26:40<03:42,  3.77s/it]Running Inference:  88%|████████▊ | 442/500 [26:41<02:47,  2.88s/it]Running Inference:  89%|████████▊ | 443/500 [26:45<02:59,  3.14s/it]Running Inference:  89%|████████▉ | 444/500 [26:45<02:12,  2.36s/it]Running Inference:  89%|████████▉ | 445/500 [26:49<02:34,  2.82s/it]Running Inference:  89%|████████▉ | 446/500 [26:53<02:49,  3.14s/it]Running Inference:  89%|████████▉ | 447/500 [26:57<02:56,  3.33s/it]Running Inference:  90%|████████▉ | 448/500 [27:01<03:00,  3.48s/it]Running Inference:  90%|████████▉ | 449/500 [27:05<03:02,  3.57s/it]Running Inference:  90%|█████████ | 450/500 [27:08<03:02,  3.64s/it]Running Inference:  90%|█████████ | 451/500 [27:12<03:00,  3.68s/it]Running Inference:  90%|█████████ | 452/500 [27:16<02:58,  3.71s/it]Running Inference:  91%|█████████ | 453/500 [27:17<02:13,  2.83s/it]Running Inference:  91%|█████████ | 454/500 [27:18<01:46,  2.31s/it]Running Inference:  91%|█████████ | 455/500 [27:22<02:02,  2.73s/it]Running Inference:  91%|█████████ | 456/500 [27:25<02:13,  3.02s/it]Running Inference:  91%|█████████▏| 457/500 [27:28<02:03,  2.88s/it]Running Inference:  92%|█████████▏| 458/500 [27:32<02:12,  3.14s/it]Running Inference:  92%|█████████▏| 459/500 [27:36<02:20,  3.42s/it]Running Inference:  92%|█████████▏| 460/500 [27:38<02:00,  3.01s/it]Running Inference:  92%|█████████▏| 461/500 [27:42<02:07,  3.26s/it]Running Inference:  92%|█████████▏| 462/500 [27:46<02:14,  3.54s/it]Running Inference:  93%|█████████▎| 463/500 [27:50<02:13,  3.60s/it]Running Inference:  93%|█████████▎| 464/500 [27:53<02:10,  3.63s/it]Running Inference:  93%|█████████▎| 465/500 [27:57<02:07,  3.65s/it]Running Inference:  93%|█████████▎| 466/500 [28:01<02:05,  3.70s/it]Running Inference:  93%|█████████▎| 467/500 [28:04<02:01,  3.70s/it]Running Inference:  94%|█████████▎| 468/500 [28:08<02:00,  3.78s/it]Running Inference:  94%|█████████▍| 469/500 [28:09<01:27,  2.81s/it]Running Inference:  94%|█████████▍| 470/500 [28:13<01:32,  3.08s/it]Running Inference:  94%|█████████▍| 471/500 [28:17<01:36,  3.33s/it]Running Inference:  94%|█████████▍| 472/500 [28:20<01:36,  3.46s/it]Running Inference:  95%|█████████▍| 473/500 [28:24<01:38,  3.64s/it]Running Inference:  95%|█████████▍| 474/500 [28:28<01:35,  3.69s/it]Running Inference:  95%|█████████▌| 475/500 [28:32<01:32,  3.72s/it]Running Inference:  95%|█████████▌| 476/500 [28:36<01:30,  3.76s/it]Running Inference:  95%|█████████▌| 477/500 [28:40<01:26,  3.77s/it]Running Inference:  96%|█████████▌| 478/500 [28:44<01:24,  3.83s/it]Running Inference:  96%|█████████▌| 479/500 [28:48<01:26,  4.14s/it]Running Inference:  96%|█████████▌| 480/500 [28:52<01:21,  4.06s/it]Running Inference:  96%|█████████▌| 481/500 [28:57<01:20,  4.21s/it]Running Inference:  96%|█████████▋| 482/500 [29:01<01:13,  4.07s/it]Running Inference:  97%|█████████▋| 483/500 [29:04<01:07,  3.99s/it]Running Inference:  97%|█████████▋| 484/500 [29:07<00:55,  3.49s/it]Running Inference:  97%|█████████▋| 485/500 [29:12<00:59,  4.00s/it]Running Inference:  97%|█████████▋| 486/500 [29:16<00:54,  3.92s/it]Running Inference:  97%|█████████▋| 487/500 [29:19<00:50,  3.88s/it]Running Inference:  98%|█████████▊| 488/500 [29:23<00:45,  3.82s/it]Running Inference:  98%|█████████▊| 489/500 [29:27<00:42,  3.83s/it]Running Inference:  98%|█████████▊| 490/500 [29:31<00:38,  3.85s/it]Running Inference:  98%|█████████▊| 491/500 [29:35<00:34,  3.83s/it]Running Inference:  98%|█████████▊| 492/500 [29:39<00:30,  3.85s/it]Running Inference:  99%|█████████▊| 493/500 [29:42<00:26,  3.81s/it]Running Inference:  99%|█████████▉| 494/500 [29:46<00:23,  3.84s/it]Running Inference:  99%|█████████▉| 495/500 [29:50<00:19,  3.83s/it]Running Inference:  99%|█████████▉| 496/500 [29:54<00:15,  3.84s/it]Running Inference:  99%|█████████▉| 497/500 [29:58<00:11,  3.82s/it]Running Inference: 100%|█████████▉| 498/500 [30:01<00:07,  3.82s/it]Running Inference: 100%|█████████▉| 499/500 [30:05<00:03,  3.80s/it]Running Inference: 100%|██████████| 500/500 [30:09<00:00,  3.88s/it]Running Inference: 100%|██████████| 500/500 [30:09<00:00,  3.62s/it]
2025-12-14 22:07:21,343 - INFO - Inference completed.
2025-12-14 22:07:21,358 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 22:07:21,358 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:07:21,360 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 22:07:21,360 - INFO - Metrics:
11.36
2025-12-14 22:07:21,361 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lcc, ratio=0.3) on GPU 4

----------------------------------------
Task: lcc | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:07:27,821 - INFO - Set deterministic seeds to 42
2025-12-14 22:07:27,821 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:07:27,821 - INFO - Starting evaluation run...
2025-12-14 22:07:27,821 - INFO - Output directory set to: longbenchresult
2025-12-14 22:07:27,821 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 22:07:27,821 - INFO - KV Press 'knorm' setup.
2025-12-14 22:07:27,821 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:07:27,821 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.01it/s]
Device set to use cuda:0
2025-12-14 22:07:41,677 - INFO - Model pipeline loaded.
2025-12-14 22:07:41,677 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 22:07:46,408 - INFO - Dataset loaded with 500 entries.
2025-12-14 22:07:46,408 - INFO - Dataset processed with 500 entries.
2025-12-14 22:07:46,422 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<39:34,  4.76s/it]Running Inference:   0%|          | 2/500 [00:05<18:30,  2.23s/it]Running Inference:   1%|          | 3/500 [00:09<26:02,  3.14s/it]Running Inference:   1%|          | 4/500 [00:13<28:02,  3.39s/it]Running Inference:   1%|          | 5/500 [00:14<20:30,  2.48s/it]Running Inference:   1%|          | 6/500 [00:15<16:37,  2.02s/it]Running Inference:   1%|▏         | 7/500 [00:19<21:28,  2.61s/it]Running Inference:   2%|▏         | 8/500 [00:22<24:18,  2.97s/it]Running Inference:   2%|▏         | 9/500 [00:24<20:30,  2.51s/it]Running Inference:   2%|▏         | 10/500 [00:27<23:31,  2.88s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:32<26:45,  3.28s/it]Running Inference:   2%|▏         | 12/500 [00:35<27:57,  3.44s/it]Running Inference:   3%|▎         | 13/500 [00:39<28:41,  3.53s/it]Running Inference:   3%|▎         | 14/500 [00:43<29:13,  3.61s/it]Running Inference:   3%|▎         | 15/500 [00:47<29:31,  3.65s/it]Running Inference:   3%|▎         | 16/500 [00:50<29:37,  3.67s/it]Running Inference:   3%|▎         | 17/500 [00:55<31:30,  3.91s/it]Running Inference:   4%|▎         | 18/500 [00:59<31:16,  3.89s/it]Running Inference:   4%|▍         | 19/500 [01:03<30:56,  3.86s/it]Running Inference:   4%|▍         | 20/500 [01:06<30:35,  3.82s/it]Running Inference:   4%|▍         | 21/500 [01:10<30:21,  3.80s/it]Running Inference:   4%|▍         | 22/500 [01:14<30:11,  3.79s/it]Running Inference:   5%|▍         | 23/500 [01:18<29:57,  3.77s/it]Running Inference:   5%|▍         | 24/500 [01:21<29:46,  3.75s/it]Running Inference:   5%|▌         | 25/500 [01:25<29:46,  3.76s/it]Running Inference:   5%|▌         | 26/500 [01:29<29:51,  3.78s/it]Running Inference:   5%|▌         | 27/500 [01:33<29:46,  3.78s/it]Running Inference:   6%|▌         | 28/500 [01:38<32:47,  4.17s/it]Running Inference:   6%|▌         | 29/500 [01:42<32:18,  4.12s/it]Running Inference:   6%|▌         | 30/500 [01:46<32:53,  4.20s/it]Running Inference:   6%|▌         | 31/500 [01:50<32:01,  4.10s/it]Running Inference:   6%|▋         | 32/500 [01:54<31:19,  4.02s/it]Running Inference:   7%|▋         | 33/500 [01:58<30:36,  3.93s/it]Running Inference:   7%|▋         | 34/500 [02:00<26:10,  3.37s/it]Running Inference:   7%|▋         | 35/500 [02:03<26:55,  3.47s/it]Running Inference:   7%|▋         | 36/500 [02:07<28:28,  3.68s/it]Running Inference:   7%|▋         | 37/500 [02:12<29:33,  3.83s/it]Running Inference:   8%|▊         | 38/500 [02:15<29:23,  3.82s/it]Running Inference:   8%|▊         | 39/500 [02:20<30:10,  3.93s/it]Running Inference:   8%|▊         | 40/500 [02:23<29:42,  3.87s/it]Running Inference:   8%|▊         | 41/500 [02:27<29:29,  3.86s/it]Running Inference:   8%|▊         | 42/500 [02:32<31:10,  4.08s/it]Running Inference:   9%|▊         | 43/500 [02:36<32:04,  4.21s/it]Running Inference:   9%|▉         | 44/500 [02:40<31:10,  4.10s/it]Running Inference:   9%|▉         | 45/500 [02:44<30:30,  4.02s/it]Running Inference:   9%|▉         | 46/500 [02:48<29:41,  3.92s/it]Running Inference:   9%|▉         | 47/500 [02:52<29:27,  3.90s/it]Running Inference:  10%|▉         | 48/500 [02:55<28:55,  3.84s/it]Running Inference:  10%|▉         | 49/500 [02:59<28:59,  3.86s/it]Running Inference:  10%|█         | 50/500 [03:04<30:21,  4.05s/it]Running Inference:  10%|█         | 51/500 [03:07<29:36,  3.96s/it]Running Inference:  10%|█         | 52/500 [03:11<29:07,  3.90s/it]Running Inference:  11%|█         | 53/500 [03:12<21:30,  2.89s/it]Running Inference:  11%|█         | 54/500 [03:15<23:27,  3.16s/it]Running Inference:  11%|█         | 55/500 [03:19<24:40,  3.33s/it]Running Inference:  11%|█         | 56/500 [03:23<25:34,  3.46s/it]Running Inference:  11%|█▏        | 57/500 [03:31<36:06,  4.89s/it]Running Inference:  12%|█▏        | 58/500 [03:35<33:23,  4.53s/it]Running Inference:  12%|█▏        | 59/500 [03:39<31:25,  4.28s/it]Running Inference:  12%|█▏        | 60/500 [03:42<30:10,  4.12s/it]Running Inference:  12%|█▏        | 61/500 [03:46<29:22,  4.02s/it]Running Inference:  12%|█▏        | 62/500 [03:50<28:40,  3.93s/it]Running Inference:  13%|█▎        | 63/500 [03:51<21:38,  2.97s/it]Running Inference:  13%|█▎        | 64/500 [03:54<23:12,  3.19s/it]Running Inference:  13%|█▎        | 65/500 [03:58<24:24,  3.37s/it]Running Inference:  13%|█▎        | 66/500 [04:02<25:49,  3.57s/it]Running Inference:  13%|█▎        | 67/500 [04:06<26:16,  3.64s/it]Running Inference:  14%|█▎        | 68/500 [04:10<26:30,  3.68s/it]Running Inference:  14%|█▍        | 69/500 [04:13<26:35,  3.70s/it]Running Inference:  14%|█▍        | 70/500 [04:17<26:36,  3.71s/it]Running Inference:  14%|█▍        | 71/500 [04:21<26:33,  3.72s/it]Running Inference:  14%|█▍        | 72/500 [04:25<27:06,  3.80s/it]Running Inference:  15%|█▍        | 73/500 [04:29<26:52,  3.78s/it]Running Inference:  15%|█▍        | 74/500 [04:32<26:54,  3.79s/it]Running Inference:  15%|█▌        | 75/500 [04:36<27:20,  3.86s/it]Running Inference:  15%|█▌        | 76/500 [04:40<27:22,  3.87s/it]Running Inference:  15%|█▌        | 77/500 [04:42<22:23,  3.18s/it]Running Inference:  16%|█▌        | 78/500 [04:46<24:30,  3.48s/it]Running Inference:  16%|█▌        | 79/500 [04:50<24:53,  3.55s/it]Running Inference:  16%|█▌        | 80/500 [04:54<25:20,  3.62s/it]Running Inference:  16%|█▌        | 81/500 [04:58<26:27,  3.79s/it]Running Inference:  16%|█▋        | 82/500 [05:02<27:25,  3.94s/it]Running Inference:  17%|█▋        | 83/500 [05:03<21:13,  3.05s/it]Running Inference:  17%|█▋        | 84/500 [05:07<22:45,  3.28s/it]Running Inference:  17%|█▋        | 85/500 [05:08<18:55,  2.74s/it]Running Inference:  17%|█▋        | 86/500 [05:12<20:55,  3.03s/it]Running Inference:  17%|█▋        | 87/500 [05:16<22:18,  3.24s/it]Running Inference:  18%|█▊        | 88/500 [05:20<23:19,  3.40s/it]Running Inference:  18%|█▊        | 89/500 [05:23<24:11,  3.53s/it]Running Inference:  18%|█▊        | 90/500 [05:27<25:15,  3.70s/it]Running Inference:  18%|█▊        | 91/500 [05:28<19:22,  2.84s/it]Running Inference:  18%|█▊        | 92/500 [05:32<21:21,  3.14s/it]Running Inference:  19%|█▊        | 93/500 [05:36<22:39,  3.34s/it]Running Inference:  19%|█▉        | 94/500 [05:40<23:25,  3.46s/it]Running Inference:  19%|█▉        | 95/500 [05:40<17:42,  2.62s/it]Running Inference:  19%|█▉        | 96/500 [05:41<13:50,  2.05s/it]Running Inference:  19%|█▉        | 97/500 [05:45<17:14,  2.57s/it]Running Inference:  20%|█▉        | 98/500 [05:49<19:30,  2.91s/it]Running Inference:  20%|█▉        | 99/500 [05:52<21:05,  3.16s/it]Running Inference:  20%|██        | 100/500 [05:56<21:20,  3.20s/it]Running Inference:  20%|██        | 101/500 [06:00<22:46,  3.42s/it]Running Inference:  20%|██        | 102/500 [06:03<23:18,  3.51s/it]Running Inference:  21%|██        | 103/500 [06:07<23:40,  3.58s/it]Running Inference:  21%|██        | 104/500 [06:13<27:53,  4.23s/it]Running Inference:  21%|██        | 105/500 [06:17<28:27,  4.32s/it]Running Inference:  21%|██        | 106/500 [06:23<30:19,  4.62s/it]Running Inference:  21%|██▏       | 107/500 [06:27<28:57,  4.42s/it]Running Inference:  22%|██▏       | 108/500 [06:30<27:35,  4.22s/it]Running Inference:  22%|██▏       | 109/500 [06:32<22:23,  3.44s/it]Running Inference:  22%|██▏       | 110/500 [06:36<23:08,  3.56s/it]Running Inference:  22%|██▏       | 111/500 [06:39<23:23,  3.61s/it]Running Inference:  22%|██▏       | 112/500 [06:43<23:39,  3.66s/it]Running Inference:  23%|██▎       | 113/500 [06:47<24:27,  3.79s/it]Running Inference:  23%|██▎       | 114/500 [06:52<26:56,  4.19s/it]Running Inference:  23%|██▎       | 115/500 [06:56<26:33,  4.14s/it]Running Inference:  23%|██▎       | 116/500 [07:01<26:24,  4.13s/it]Running Inference:  23%|██▎       | 117/500 [07:04<25:30,  4.00s/it]Running Inference:  24%|██▎       | 118/500 [07:09<26:20,  4.14s/it]Running Inference:  24%|██▍       | 119/500 [07:12<25:31,  4.02s/it]Running Inference:  24%|██▍       | 120/500 [07:17<25:44,  4.07s/it]Running Inference:  24%|██▍       | 121/500 [07:21<25:20,  4.01s/it]Running Inference:  24%|██▍       | 122/500 [07:24<24:45,  3.93s/it]Running Inference:  25%|██▍       | 123/500 [07:28<24:28,  3.90s/it]Running Inference:  25%|██▍       | 124/500 [07:32<24:08,  3.85s/it]Running Inference:  25%|██▌       | 125/500 [07:33<19:25,  3.11s/it]Running Inference:  25%|██▌       | 126/500 [07:34<14:53,  2.39s/it]Running Inference:  25%|██▌       | 127/500 [07:38<17:23,  2.80s/it]Running Inference:  26%|██▌       | 128/500 [07:42<19:31,  3.15s/it]Running Inference:  26%|██▌       | 129/500 [07:45<20:37,  3.33s/it]Running Inference:  26%|██▌       | 130/500 [07:49<21:36,  3.50s/it]Running Inference:  26%|██▌       | 131/500 [07:53<22:04,  3.59s/it]Running Inference:  26%|██▋       | 132/500 [07:57<22:18,  3.64s/it]Running Inference:  27%|██▋       | 133/500 [08:01<22:37,  3.70s/it]Running Inference:  27%|██▋       | 134/500 [08:05<22:47,  3.74s/it]Running Inference:  27%|██▋       | 135/500 [08:08<23:02,  3.79s/it]Running Inference:  27%|██▋       | 136/500 [08:12<23:09,  3.82s/it]Running Inference:  27%|██▋       | 137/500 [08:16<22:56,  3.79s/it]Running Inference:  28%|██▊       | 138/500 [08:20<23:06,  3.83s/it]Running Inference:  28%|██▊       | 139/500 [08:24<22:51,  3.80s/it]Running Inference:  28%|██▊       | 140/500 [08:28<22:58,  3.83s/it]Running Inference:  28%|██▊       | 141/500 [08:32<23:36,  3.95s/it]Running Inference:  28%|██▊       | 142/500 [08:36<23:42,  3.97s/it]Running Inference:  29%|██▊       | 143/500 [08:40<23:20,  3.92s/it]Running Inference:  29%|██▉       | 144/500 [08:43<22:58,  3.87s/it]Running Inference:  29%|██▉       | 145/500 [08:47<23:01,  3.89s/it]Running Inference:  29%|██▉       | 146/500 [08:51<23:09,  3.93s/it]Running Inference:  29%|██▉       | 147/500 [08:55<23:21,  3.97s/it]Running Inference:  30%|██▉       | 148/500 [08:59<22:51,  3.90s/it]Running Inference:  30%|██▉       | 149/500 [09:03<22:33,  3.86s/it]Running Inference:  30%|███       | 150/500 [09:07<22:15,  3.82s/it]Running Inference:  30%|███       | 151/500 [09:10<22:15,  3.83s/it]Running Inference:  30%|███       | 152/500 [09:14<22:04,  3.81s/it]Running Inference:  31%|███       | 153/500 [09:18<22:03,  3.81s/it]Running Inference:  31%|███       | 154/500 [09:22<21:52,  3.79s/it]Running Inference:  31%|███       | 155/500 [09:22<16:17,  2.83s/it]Running Inference:  31%|███       | 156/500 [09:26<17:53,  3.12s/it]Running Inference:  31%|███▏      | 157/500 [09:30<19:16,  3.37s/it]Running Inference:  32%|███▏      | 158/500 [09:34<20:02,  3.52s/it]Running Inference:  32%|███▏      | 159/500 [09:38<20:21,  3.58s/it]Running Inference:  32%|███▏      | 160/500 [09:41<20:32,  3.62s/it]Running Inference:  32%|███▏      | 161/500 [09:45<20:44,  3.67s/it]Running Inference:  32%|███▏      | 162/500 [09:49<21:06,  3.75s/it]Running Inference:  33%|███▎      | 163/500 [09:53<20:57,  3.73s/it]Running Inference:  33%|███▎      | 164/500 [09:57<20:57,  3.74s/it]Running Inference:  33%|███▎      | 165/500 [09:57<15:47,  2.83s/it]Running Inference:  33%|███▎      | 166/500 [10:01<17:12,  3.09s/it]Running Inference:  33%|███▎      | 167/500 [10:05<18:52,  3.40s/it]Running Inference:  34%|███▎      | 168/500 [10:09<19:25,  3.51s/it]Running Inference:  34%|███▍      | 169/500 [10:13<19:53,  3.61s/it]Running Inference:  34%|███▍      | 170/500 [10:16<20:02,  3.64s/it]Running Inference:  34%|███▍      | 171/500 [10:20<20:20,  3.71s/it]Running Inference:  34%|███▍      | 172/500 [10:24<20:19,  3.72s/it]Running Inference:  35%|███▍      | 173/500 [10:28<20:19,  3.73s/it]Running Inference:  35%|███▍      | 174/500 [10:32<20:46,  3.83s/it]Running Inference:  35%|███▌      | 175/500 [10:36<21:33,  3.98s/it]Running Inference:  35%|███▌      | 176/500 [10:40<21:09,  3.92s/it]Running Inference:  35%|███▌      | 177/500 [10:44<21:56,  4.07s/it]Running Inference:  36%|███▌      | 178/500 [10:48<21:13,  3.95s/it]Running Inference:  36%|███▌      | 179/500 [10:52<20:47,  3.89s/it]Running Inference:  36%|███▌      | 180/500 [10:56<20:38,  3.87s/it]Running Inference:  36%|███▌      | 181/500 [10:59<20:18,  3.82s/it]Running Inference:  36%|███▋      | 182/500 [11:03<20:03,  3.78s/it]Running Inference:  37%|███▋      | 183/500 [11:07<20:13,  3.83s/it]Running Inference:  37%|███▋      | 184/500 [11:11<20:11,  3.83s/it]Running Inference:  37%|███▋      | 185/500 [11:15<20:11,  3.84s/it]Running Inference:  37%|███▋      | 186/500 [11:19<20:02,  3.83s/it]Running Inference:  37%|███▋      | 187/500 [11:22<20:02,  3.84s/it]Running Inference:  38%|███▊      | 188/500 [11:26<19:48,  3.81s/it]Running Inference:  38%|███▊      | 189/500 [11:30<19:45,  3.81s/it]Running Inference:  38%|███▊      | 190/500 [11:34<20:10,  3.91s/it]Running Inference:  38%|███▊      | 191/500 [11:38<19:48,  3.85s/it]Running Inference:  38%|███▊      | 192/500 [11:42<19:42,  3.84s/it]Running Inference:  39%|███▊      | 193/500 [11:45<19:33,  3.82s/it]Running Inference:  39%|███▉      | 194/500 [11:49<19:24,  3.81s/it]Running Inference:  39%|███▉      | 195/500 [11:53<19:17,  3.79s/it]Running Inference:  39%|███▉      | 196/500 [11:57<19:10,  3.78s/it]Running Inference:  39%|███▉      | 197/500 [12:01<19:15,  3.81s/it]Running Inference:  40%|███▉      | 198/500 [12:04<19:00,  3.78s/it]Running Inference:  40%|███▉      | 199/500 [12:05<13:49,  2.76s/it]Running Inference:  40%|████      | 200/500 [12:08<15:24,  3.08s/it]Running Inference:  40%|████      | 201/500 [12:12<16:35,  3.33s/it]Running Inference:  40%|████      | 202/500 [12:16<17:07,  3.45s/it]Running Inference:  41%|████      | 203/500 [12:21<19:39,  3.97s/it]Running Inference:  41%|████      | 204/500 [12:25<19:20,  3.92s/it]Running Inference:  41%|████      | 205/500 [12:29<19:00,  3.87s/it]Running Inference:  41%|████      | 206/500 [12:33<18:41,  3.82s/it]Running Inference:  41%|████▏     | 207/500 [12:36<18:32,  3.80s/it]Running Inference:  42%|████▏     | 208/500 [12:40<18:56,  3.89s/it]Running Inference:  42%|████▏     | 209/500 [12:45<19:17,  3.98s/it]Running Inference:  42%|████▏     | 210/500 [12:45<14:21,  2.97s/it]Running Inference:  42%|████▏     | 211/500 [12:49<15:43,  3.26s/it]Running Inference:  42%|████▏     | 212/500 [12:53<16:16,  3.39s/it]Running Inference:  43%|████▎     | 213/500 [12:56<16:36,  3.47s/it]Running Inference:  43%|████▎     | 214/500 [13:00<16:58,  3.56s/it]Running Inference:  43%|████▎     | 215/500 [13:04<17:16,  3.64s/it]Running Inference:  43%|████▎     | 216/500 [13:08<17:20,  3.66s/it]Running Inference:  43%|████▎     | 217/500 [13:12<17:47,  3.77s/it]Running Inference:  44%|████▎     | 218/500 [13:16<18:00,  3.83s/it]Running Inference:  44%|████▍     | 219/500 [13:19<17:44,  3.79s/it]Running Inference:  44%|████▍     | 220/500 [13:23<17:38,  3.78s/it]Running Inference:  44%|████▍     | 221/500 [13:27<17:26,  3.75s/it]Running Inference:  44%|████▍     | 222/500 [13:31<17:17,  3.73s/it]Running Inference:  45%|████▍     | 223/500 [13:34<17:14,  3.74s/it]Running Inference:  45%|████▍     | 224/500 [13:38<17:12,  3.74s/it]Running Inference:  45%|████▌     | 225/500 [13:42<17:04,  3.73s/it]Running Inference:  45%|████▌     | 226/500 [13:45<16:58,  3.72s/it]Running Inference:  45%|████▌     | 227/500 [13:49<16:55,  3.72s/it]Running Inference:  46%|████▌     | 228/500 [13:50<12:32,  2.77s/it]Running Inference:  46%|████▌     | 229/500 [13:53<13:43,  3.04s/it]Running Inference:  46%|████▌     | 230/500 [13:57<14:34,  3.24s/it]Running Inference:  46%|████▌     | 231/500 [13:58<11:00,  2.46s/it]Running Inference:  46%|████▋     | 232/500 [14:02<12:41,  2.84s/it]Running Inference:  47%|████▋     | 233/500 [14:05<13:58,  3.14s/it]Running Inference:  47%|████▋     | 234/500 [14:09<14:47,  3.34s/it]Running Inference:  47%|████▋     | 235/500 [14:13<15:43,  3.56s/it]Running Inference:  47%|████▋     | 236/500 [14:17<15:49,  3.60s/it]Running Inference:  47%|████▋     | 237/500 [14:21<15:54,  3.63s/it]Running Inference:  48%|████▊     | 238/500 [14:24<15:58,  3.66s/it]Running Inference:  48%|████▊     | 239/500 [14:25<12:08,  2.79s/it]Running Inference:  48%|████▊     | 240/500 [14:29<13:21,  3.08s/it]Running Inference:  48%|████▊     | 241/500 [14:33<14:11,  3.29s/it]Running Inference:  48%|████▊     | 242/500 [14:36<14:38,  3.41s/it]Running Inference:  49%|████▊     | 243/500 [14:41<15:54,  3.71s/it]Running Inference:  49%|████▉     | 244/500 [14:45<16:27,  3.86s/it]Running Inference:  49%|████▉     | 245/500 [14:49<16:22,  3.85s/it]Running Inference:  49%|████▉     | 246/500 [14:52<16:05,  3.80s/it]Running Inference:  49%|████▉     | 247/500 [14:56<16:04,  3.81s/it]Running Inference:  50%|████▉     | 248/500 [15:00<15:56,  3.80s/it]Running Inference:  50%|████▉     | 249/500 [15:04<15:52,  3.79s/it]Running Inference:  50%|█████     | 250/500 [15:08<15:50,  3.80s/it]Running Inference:  50%|█████     | 251/500 [15:12<16:05,  3.88s/it]Running Inference:  50%|█████     | 252/500 [15:16<16:45,  4.05s/it]Running Inference:  51%|█████     | 253/500 [15:20<16:17,  3.96s/it]Running Inference:  51%|█████     | 254/500 [15:24<15:59,  3.90s/it]Running Inference:  51%|█████     | 255/500 [15:28<15:58,  3.91s/it]Running Inference:  51%|█████     | 256/500 [15:31<15:42,  3.86s/it]Running Inference:  51%|█████▏    | 257/500 [15:35<15:45,  3.89s/it]Running Inference:  52%|█████▏    | 258/500 [15:39<15:29,  3.84s/it]Running Inference:  52%|█████▏    | 259/500 [15:43<15:22,  3.83s/it]Running Inference:  52%|█████▏    | 260/500 [15:47<15:12,  3.80s/it]Running Inference:  52%|█████▏    | 261/500 [15:50<15:00,  3.77s/it]Running Inference:  52%|█████▏    | 262/500 [15:51<11:07,  2.80s/it]Running Inference:  53%|█████▎    | 263/500 [15:55<12:13,  3.09s/it]Running Inference:  53%|█████▎    | 264/500 [15:58<12:53,  3.28s/it]Running Inference:  53%|█████▎    | 265/500 [16:02<13:34,  3.46s/it]Running Inference:  53%|█████▎    | 266/500 [16:06<13:50,  3.55s/it]Running Inference:  53%|█████▎    | 267/500 [16:10<14:33,  3.75s/it]Running Inference:  54%|█████▎    | 268/500 [16:14<14:30,  3.75s/it]Running Inference:  54%|█████▍    | 269/500 [16:18<14:28,  3.76s/it]Running Inference:  54%|█████▍    | 270/500 [16:21<14:22,  3.75s/it]Running Inference:  54%|█████▍    | 271/500 [16:25<14:12,  3.72s/it]Running Inference:  54%|█████▍    | 272/500 [16:29<14:11,  3.74s/it]Running Inference:  55%|█████▍    | 273/500 [16:33<14:20,  3.79s/it]Running Inference:  55%|█████▍    | 274/500 [16:36<14:10,  3.76s/it]Running Inference:  55%|█████▌    | 275/500 [16:40<14:04,  3.75s/it]Running Inference:  55%|█████▌    | 276/500 [16:44<14:02,  3.76s/it]Running Inference:  55%|█████▌    | 277/500 [16:48<14:01,  3.77s/it]Running Inference:  56%|█████▌    | 278/500 [16:52<14:01,  3.79s/it]Running Inference:  56%|█████▌    | 279/500 [16:56<14:06,  3.83s/it]Running Inference:  56%|█████▌    | 280/500 [16:56<10:51,  2.96s/it]Running Inference:  56%|█████▌    | 281/500 [17:00<11:46,  3.23s/it]Running Inference:  56%|█████▋    | 282/500 [17:04<12:20,  3.40s/it]Running Inference:  57%|█████▋    | 283/500 [17:08<12:39,  3.50s/it]Running Inference:  57%|█████▋    | 284/500 [17:12<12:59,  3.61s/it]Running Inference:  57%|█████▋    | 285/500 [17:16<13:10,  3.68s/it]Running Inference:  57%|█████▋    | 286/500 [17:19<13:07,  3.68s/it]Running Inference:  57%|█████▋    | 287/500 [17:24<13:59,  3.94s/it]Running Inference:  58%|█████▊    | 288/500 [17:28<13:57,  3.95s/it]Running Inference:  58%|█████▊    | 289/500 [17:32<13:40,  3.89s/it]Running Inference:  58%|█████▊    | 290/500 [17:35<13:35,  3.88s/it]Running Inference:  58%|█████▊    | 291/500 [17:39<13:34,  3.90s/it]Running Inference:  58%|█████▊    | 292/500 [17:43<13:17,  3.83s/it]Running Inference:  59%|█████▊    | 293/500 [17:47<13:14,  3.84s/it]Running Inference:  59%|█████▉    | 294/500 [17:51<13:01,  3.79s/it]Running Inference:  59%|█████▉    | 295/500 [17:54<12:53,  3.77s/it]Running Inference:  59%|█████▉    | 296/500 [17:58<12:48,  3.77s/it]Running Inference:  59%|█████▉    | 297/500 [18:02<12:48,  3.79s/it]Running Inference:  60%|█████▉    | 298/500 [18:06<12:39,  3.76s/it]Running Inference:  60%|█████▉    | 299/500 [18:09<12:31,  3.74s/it]Running Inference:  60%|██████    | 300/500 [18:10<09:03,  2.72s/it]Running Inference:  60%|██████    | 301/500 [18:14<10:17,  3.10s/it]Running Inference:  60%|██████    | 302/500 [18:17<10:52,  3.30s/it]Running Inference:  61%|██████    | 303/500 [18:21<11:16,  3.43s/it]Running Inference:  61%|██████    | 304/500 [18:25<12:02,  3.69s/it]Running Inference:  61%|██████    | 305/500 [18:29<11:57,  3.68s/it]Running Inference:  61%|██████    | 306/500 [18:33<12:23,  3.83s/it]Running Inference:  61%|██████▏   | 307/500 [18:37<12:16,  3.82s/it]Running Inference:  62%|██████▏   | 308/500 [18:41<12:04,  3.78s/it]Running Inference:  62%|██████▏   | 309/500 [18:44<11:56,  3.75s/it]Running Inference:  62%|██████▏   | 310/500 [18:48<11:51,  3.74s/it]Running Inference:  62%|██████▏   | 311/500 [18:52<11:53,  3.78s/it]Running Inference:  62%|██████▏   | 312/500 [18:56<11:47,  3.77s/it]Running Inference:  63%|██████▎   | 313/500 [19:00<11:49,  3.79s/it]Running Inference:  63%|██████▎   | 314/500 [19:03<11:39,  3.76s/it]Running Inference:  63%|██████▎   | 315/500 [19:07<11:29,  3.73s/it]Running Inference:  63%|██████▎   | 316/500 [19:11<11:22,  3.71s/it]Running Inference:  63%|██████▎   | 317/500 [19:14<11:17,  3.70s/it]Running Inference:  64%|██████▎   | 318/500 [19:18<11:15,  3.71s/it]Running Inference:  64%|██████▍   | 319/500 [19:18<08:17,  2.75s/it]Running Inference:  64%|██████▍   | 320/500 [19:21<07:41,  2.56s/it]Running Inference:  64%|██████▍   | 321/500 [19:25<08:58,  3.01s/it]Running Inference:  64%|██████▍   | 322/500 [19:25<06:49,  2.30s/it]Running Inference:  65%|██████▍   | 323/500 [19:29<08:02,  2.72s/it]Running Inference:  65%|██████▍   | 324/500 [19:33<08:54,  3.04s/it]Running Inference:  65%|██████▌   | 325/500 [19:37<09:29,  3.25s/it]Running Inference:  65%|██████▌   | 326/500 [19:41<10:13,  3.52s/it]Running Inference:  65%|██████▌   | 327/500 [19:46<11:39,  4.04s/it]Running Inference:  66%|██████▌   | 328/500 [19:50<11:45,  4.10s/it]Running Inference:  66%|██████▌   | 329/500 [19:54<11:18,  3.97s/it]Running Inference:  66%|██████▌   | 330/500 [19:58<10:59,  3.88s/it]Running Inference:  66%|██████▌   | 331/500 [20:02<11:23,  4.05s/it]Running Inference:  66%|██████▋   | 332/500 [20:06<11:15,  4.02s/it]Running Inference:  67%|██████▋   | 333/500 [20:10<10:57,  3.94s/it]Running Inference:  67%|██████▋   | 334/500 [20:13<10:44,  3.88s/it]Running Inference:  67%|██████▋   | 335/500 [20:17<10:33,  3.84s/it]Running Inference:  67%|██████▋   | 336/500 [20:21<10:22,  3.80s/it]Running Inference:  67%|██████▋   | 337/500 [20:25<10:15,  3.77s/it]Running Inference:  68%|██████▊   | 338/500 [20:28<10:10,  3.77s/it]Running Inference:  68%|██████▊   | 339/500 [20:32<10:24,  3.88s/it]Running Inference:  68%|██████▊   | 340/500 [20:38<11:51,  4.44s/it]Running Inference:  68%|██████▊   | 341/500 [20:39<08:51,  3.34s/it]Running Inference:  68%|██████▊   | 342/500 [20:40<06:57,  2.64s/it]Running Inference:  69%|██████▊   | 343/500 [20:44<08:02,  3.07s/it]Running Inference:  69%|██████▉   | 344/500 [20:48<08:43,  3.36s/it]Running Inference:  69%|██████▉   | 345/500 [20:52<09:02,  3.50s/it]Running Inference:  69%|██████▉   | 346/500 [20:53<07:01,  2.74s/it]Running Inference:  69%|██████▉   | 347/500 [20:54<05:32,  2.17s/it]Running Inference:  70%|██████▉   | 348/500 [20:58<06:44,  2.66s/it]Running Inference:  70%|██████▉   | 349/500 [21:01<07:32,  3.00s/it]Running Inference:  70%|███████   | 350/500 [21:07<09:41,  3.87s/it]Running Inference:  70%|███████   | 351/500 [21:11<09:52,  3.98s/it]Running Inference:  70%|███████   | 352/500 [21:15<09:35,  3.89s/it]Running Inference:  71%|███████   | 353/500 [21:19<09:23,  3.83s/it]Running Inference:  71%|███████   | 354/500 [21:23<09:22,  3.85s/it]Running Inference:  71%|███████   | 355/500 [21:29<11:13,  4.64s/it]Running Inference:  71%|███████   | 356/500 [21:33<10:29,  4.37s/it]Running Inference:  71%|███████▏  | 357/500 [21:37<10:14,  4.30s/it]Running Inference:  72%|███████▏  | 358/500 [21:41<09:46,  4.13s/it]Running Inference:  72%|███████▏  | 359/500 [21:45<09:23,  3.99s/it]Running Inference:  72%|███████▏  | 360/500 [21:48<09:07,  3.91s/it]Running Inference:  72%|███████▏  | 361/500 [21:52<09:00,  3.89s/it]Running Inference:  72%|███████▏  | 362/500 [21:56<08:58,  3.90s/it]Running Inference:  73%|███████▎  | 363/500 [22:00<08:53,  3.89s/it]Running Inference:  73%|███████▎  | 364/500 [22:04<08:50,  3.90s/it]Running Inference:  73%|███████▎  | 365/500 [22:09<09:28,  4.21s/it]Running Inference:  73%|███████▎  | 366/500 [22:13<09:23,  4.20s/it]Running Inference:  73%|███████▎  | 367/500 [22:14<07:10,  3.23s/it]Running Inference:  74%|███████▎  | 368/500 [22:18<07:34,  3.44s/it]Running Inference:  74%|███████▍  | 369/500 [22:21<07:00,  3.21s/it]Running Inference:  74%|███████▍  | 370/500 [22:25<07:40,  3.54s/it]Running Inference:  74%|███████▍  | 371/500 [22:29<07:45,  3.61s/it]Running Inference:  74%|███████▍  | 372/500 [22:33<07:54,  3.71s/it]Running Inference:  75%|███████▍  | 373/500 [22:33<05:53,  2.78s/it]Running Inference:  75%|███████▍  | 374/500 [22:37<06:31,  3.10s/it]Running Inference:  75%|███████▌  | 375/500 [22:41<06:49,  3.28s/it]Running Inference:  75%|███████▌  | 376/500 [22:44<07:06,  3.44s/it]Running Inference:  75%|███████▌  | 377/500 [22:48<07:14,  3.53s/it]Running Inference:  76%|███████▌  | 378/500 [22:52<07:21,  3.62s/it]Running Inference:  76%|███████▌  | 379/500 [22:56<07:30,  3.72s/it]Running Inference:  76%|███████▌  | 380/500 [23:00<07:28,  3.73s/it]Running Inference:  76%|███████▌  | 381/500 [23:04<07:25,  3.74s/it]Running Inference:  76%|███████▋  | 382/500 [23:08<07:35,  3.86s/it]Running Inference:  77%|███████▋  | 383/500 [23:12<07:33,  3.88s/it]Running Inference:  77%|███████▋  | 384/500 [23:12<05:44,  2.97s/it]Running Inference:  77%|███████▋  | 385/500 [23:16<06:16,  3.28s/it]Running Inference:  77%|███████▋  | 386/500 [23:19<05:42,  3.01s/it]Running Inference:  77%|███████▋  | 387/500 [23:20<04:26,  2.36s/it]Running Inference:  78%|███████▊  | 388/500 [23:20<03:22,  1.80s/it]Running Inference:  78%|███████▊  | 389/500 [23:24<04:27,  2.41s/it]Running Inference:  78%|███████▊  | 390/500 [23:28<05:09,  2.82s/it]Running Inference:  78%|███████▊  | 391/500 [23:32<05:37,  3.10s/it]Running Inference:  78%|███████▊  | 392/500 [23:35<05:54,  3.29s/it]Running Inference:  79%|███████▊  | 393/500 [23:36<04:23,  2.47s/it]Running Inference:  79%|███████▉  | 394/500 [23:40<05:04,  2.87s/it]Running Inference:  79%|███████▉  | 395/500 [23:43<05:31,  3.15s/it]Running Inference:  79%|███████▉  | 396/500 [23:47<05:44,  3.31s/it]Running Inference:  79%|███████▉  | 397/500 [23:51<06:03,  3.53s/it]Running Inference:  80%|███████▉  | 398/500 [23:55<06:04,  3.58s/it]Running Inference:  80%|███████▉  | 399/500 [23:59<06:04,  3.61s/it]Running Inference:  80%|████████  | 400/500 [23:59<04:25,  2.66s/it]Running Inference:  80%|████████  | 401/500 [24:03<04:53,  2.97s/it]Running Inference:  80%|████████  | 402/500 [24:06<05:11,  3.18s/it]Running Inference:  81%|████████  | 403/500 [24:10<05:24,  3.34s/it]Running Inference:  81%|████████  | 404/500 [24:14<05:30,  3.44s/it]Running Inference:  81%|████████  | 405/500 [24:17<05:36,  3.54s/it]Running Inference:  81%|████████  | 406/500 [24:21<05:38,  3.60s/it]Running Inference:  81%|████████▏ | 407/500 [24:22<04:04,  2.63s/it]Running Inference:  82%|████████▏ | 408/500 [24:25<04:32,  2.96s/it]Running Inference:  82%|████████▏ | 409/500 [24:29<04:50,  3.20s/it]Running Inference:  82%|████████▏ | 410/500 [24:33<05:03,  3.37s/it]Running Inference:  82%|████████▏ | 411/500 [24:37<05:12,  3.51s/it]Running Inference:  82%|████████▏ | 412/500 [24:40<05:15,  3.58s/it]Running Inference:  83%|████████▎ | 413/500 [24:44<05:14,  3.62s/it]Running Inference:  83%|████████▎ | 414/500 [24:45<04:07,  2.88s/it]Running Inference:  83%|████████▎ | 415/500 [24:49<04:26,  3.13s/it]Running Inference:  83%|████████▎ | 416/500 [24:53<04:35,  3.29s/it]Running Inference:  83%|████████▎ | 417/500 [24:54<03:41,  2.66s/it]Running Inference:  84%|████████▎ | 418/500 [24:58<04:05,  3.00s/it]Running Inference:  84%|████████▍ | 419/500 [25:01<04:19,  3.20s/it]Running Inference:  84%|████████▍ | 420/500 [25:05<04:27,  3.35s/it]Running Inference:  84%|████████▍ | 421/500 [25:09<04:32,  3.46s/it]Running Inference:  84%|████████▍ | 422/500 [25:12<04:34,  3.52s/it]Running Inference:  85%|████████▍ | 423/500 [25:16<04:35,  3.57s/it]Running Inference:  85%|████████▍ | 424/500 [25:20<04:34,  3.62s/it]Running Inference:  85%|████████▌ | 425/500 [25:24<04:40,  3.74s/it]Running Inference:  85%|████████▌ | 426/500 [25:28<04:36,  3.73s/it]Running Inference:  85%|████████▌ | 427/500 [25:31<04:30,  3.70s/it]Running Inference:  86%|████████▌ | 428/500 [25:35<04:26,  3.70s/it]Running Inference:  86%|████████▌ | 429/500 [25:39<04:22,  3.70s/it]Running Inference:  86%|████████▌ | 430/500 [25:43<04:27,  3.82s/it]Running Inference:  86%|████████▌ | 431/500 [25:43<03:21,  2.92s/it]Running Inference:  86%|████████▋ | 432/500 [25:48<03:40,  3.25s/it]Running Inference:  87%|████████▋ | 433/500 [25:51<03:52,  3.47s/it]Running Inference:  87%|████████▋ | 434/500 [25:55<03:57,  3.60s/it]Running Inference:  87%|████████▋ | 435/500 [25:59<03:56,  3.64s/it]Running Inference:  87%|████████▋ | 436/500 [26:03<03:56,  3.69s/it]Running Inference:  87%|████████▋ | 437/500 [26:07<03:53,  3.71s/it]Running Inference:  88%|████████▊ | 438/500 [26:10<03:49,  3.71s/it]Running Inference:  88%|████████▊ | 439/500 [26:14<03:48,  3.74s/it]Running Inference:  88%|████████▊ | 440/500 [26:18<03:44,  3.75s/it]Running Inference:  88%|████████▊ | 441/500 [26:22<03:39,  3.73s/it]Running Inference:  88%|████████▊ | 442/500 [26:25<03:34,  3.71s/it]Running Inference:  89%|████████▊ | 443/500 [26:29<03:31,  3.71s/it]Running Inference:  89%|████████▉ | 444/500 [26:30<02:33,  2.75s/it]Running Inference:  89%|████████▉ | 445/500 [26:33<02:49,  3.08s/it]Running Inference:  89%|████████▉ | 446/500 [26:34<02:04,  2.31s/it]Running Inference:  89%|████████▉ | 447/500 [26:38<02:24,  2.73s/it]Running Inference:  90%|████████▉ | 448/500 [26:41<02:38,  3.05s/it]Running Inference:  90%|████████▉ | 449/500 [26:45<02:46,  3.26s/it]Running Inference:  90%|█████████ | 450/500 [26:49<02:50,  3.41s/it]Running Inference:  90%|█████████ | 451/500 [26:50<02:14,  2.74s/it]Running Inference:  90%|█████████ | 452/500 [26:54<02:26,  3.05s/it]Running Inference:  91%|█████████ | 453/500 [26:55<01:50,  2.36s/it]Running Inference:  91%|█████████ | 454/500 [26:58<02:07,  2.76s/it]Running Inference:  91%|█████████ | 455/500 [27:02<02:16,  3.03s/it]Running Inference:  91%|█████████ | 456/500 [27:06<02:21,  3.23s/it]Running Inference:  91%|█████████▏| 457/500 [27:09<02:25,  3.39s/it]Running Inference:  92%|█████████▏| 458/500 [27:13<02:26,  3.49s/it]Running Inference:  92%|█████████▏| 459/500 [27:17<02:29,  3.66s/it]Running Inference:  92%|█████████▏| 460/500 [27:21<02:30,  3.77s/it]Running Inference:  92%|█████████▏| 461/500 [27:25<02:27,  3.79s/it]Running Inference:  92%|█████████▏| 462/500 [27:29<02:28,  3.91s/it]Running Inference:  93%|█████████▎| 463/500 [27:33<02:22,  3.84s/it]Running Inference:  93%|█████████▎| 464/500 [27:37<02:16,  3.79s/it]Running Inference:  93%|█████████▎| 465/500 [27:40<02:11,  3.75s/it]Running Inference:  93%|█████████▎| 466/500 [27:44<02:07,  3.76s/it]Running Inference:  93%|█████████▎| 467/500 [27:48<02:03,  3.73s/it]Running Inference:  94%|█████████▎| 468/500 [27:52<02:01,  3.79s/it]Running Inference:  94%|█████████▍| 469/500 [27:55<01:56,  3.76s/it]Running Inference:  94%|█████████▍| 470/500 [27:59<01:52,  3.74s/it]Running Inference:  94%|█████████▍| 471/500 [28:03<01:50,  3.79s/it]Running Inference:  94%|█████████▍| 472/500 [28:07<01:45,  3.77s/it]Running Inference:  95%|█████████▍| 473/500 [28:11<01:43,  3.85s/it]Running Inference:  95%|█████████▍| 474/500 [28:14<01:39,  3.82s/it]Running Inference:  95%|█████████▌| 475/500 [28:18<01:35,  3.81s/it]Running Inference:  95%|█████████▌| 476/500 [28:22<01:31,  3.80s/it]Running Inference:  95%|█████████▌| 477/500 [28:26<01:27,  3.79s/it]Running Inference:  96%|█████████▌| 478/500 [28:27<01:06,  3.01s/it]Running Inference:  96%|█████████▌| 479/500 [28:32<01:14,  3.55s/it]Running Inference:  96%|█████████▌| 480/500 [28:36<01:12,  3.64s/it]Running Inference:  96%|█████████▌| 481/500 [28:40<01:14,  3.90s/it]Running Inference:  96%|█████████▋| 482/500 [28:44<01:09,  3.84s/it]Running Inference:  97%|█████████▋| 483/500 [28:48<01:04,  3.81s/it]Running Inference:  97%|█████████▋| 484/500 [28:52<01:01,  3.86s/it]Running Inference:  97%|█████████▋| 485/500 [28:57<01:03,  4.25s/it]Running Inference:  97%|█████████▋| 486/500 [29:00<00:57,  4.08s/it]Running Inference:  97%|█████████▋| 487/500 [29:04<00:51,  3.98s/it]Running Inference:  98%|█████████▊| 488/500 [29:08<00:46,  3.89s/it]Running Inference:  98%|█████████▊| 489/500 [29:12<00:42,  3.87s/it]Running Inference:  98%|█████████▊| 490/500 [29:15<00:38,  3.86s/it]Running Inference:  98%|█████████▊| 491/500 [29:19<00:34,  3.83s/it]Running Inference:  98%|█████████▊| 492/500 [29:23<00:30,  3.83s/it]Running Inference:  99%|█████████▊| 493/500 [29:27<00:26,  3.79s/it]Running Inference:  99%|█████████▉| 494/500 [29:31<00:22,  3.81s/it]Running Inference:  99%|█████████▉| 495/500 [29:34<00:18,  3.80s/it]Running Inference:  99%|█████████▉| 496/500 [29:38<00:15,  3.81s/it]Running Inference:  99%|█████████▉| 497/500 [29:42<00:11,  3.78s/it]Running Inference: 100%|█████████▉| 498/500 [29:46<00:07,  3.79s/it]Running Inference: 100%|█████████▉| 499/500 [29:49<00:03,  3.76s/it]Running Inference: 100%|██████████| 500/500 [29:54<00:00,  3.85s/it]Running Inference: 100%|██████████| 500/500 [29:54<00:00,  3.59s/it]
2025-12-14 22:37:40,449 - INFO - Inference completed.
2025-12-14 22:37:40,465 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 22:37:40,465 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:37:40,467 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 22:37:40,467 - INFO - Metrics:
7.93
2025-12-14 22:37:40,468 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lcc, ratio=0.5) on GPU 4


========================================
LongBench Task: lsht
========================================
----------------------------------------
Task: lsht | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:37:46,913 - INFO - Set deterministic seeds to 42
2025-12-14 22:37:46,913 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:37:46,913 - INFO - Starting evaluation run...
2025-12-14 22:37:46,913 - INFO - Output directory set to: longbenchresult
2025-12-14 22:37:46,913 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 22:37:46,913 - INFO - KV Press 'knorm' setup.
2025-12-14 22:37:46,913 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:37:46,913 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.65it/s]
Device set to use cuda:0
2025-12-14 22:37:59,649 - INFO - Model pipeline loaded.
2025-12-14 22:37:59,650 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 22:38:04,895 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:38:04,895 - INFO - Dataset processed with 200 entries.
2025-12-14 22:38:04,928 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:08,  2.15s/it]Running Inference:   1%|          | 2/200 [00:04<07:22,  2.23s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:18,  1.62s/it]Running Inference:   2%|▏         | 4/200 [00:06<05:02,  1.55s/it]Running Inference:   2%|▎         | 5/200 [00:09<06:34,  2.02s/it]Running Inference:   3%|▎         | 6/200 [00:11<06:37,  2.05s/it]Running Inference:   4%|▎         | 7/200 [00:13<06:46,  2.11s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:47,  1.81s/it]Running Inference:   4%|▍         | 9/200 [00:17<06:02,  1.90s/it]Running Inference:   5%|▌         | 10/200 [00:18<05:20,  1.69s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<05:35,  1.78s/it]Running Inference:   6%|▌         | 12/200 [00:21<05:10,  1.65s/it]Running Inference:   6%|▋         | 13/200 [00:24<05:48,  1.87s/it]Running Inference:   7%|▋         | 14/200 [00:25<05:14,  1.69s/it]Running Inference:   8%|▊         | 15/200 [00:28<06:24,  2.08s/it]Running Inference:   8%|▊         | 16/200 [00:29<05:38,  1.84s/it]Running Inference:   8%|▊         | 17/200 [00:36<09:58,  3.27s/it]Running Inference:   9%|▉         | 18/200 [00:38<09:06,  3.00s/it]Running Inference:  10%|▉         | 19/200 [00:39<07:26,  2.46s/it]Running Inference:  10%|█         | 20/200 [00:42<07:17,  2.43s/it]Running Inference:  10%|█         | 21/200 [00:44<07:06,  2.39s/it]Running Inference:  11%|█         | 22/200 [00:46<06:32,  2.20s/it]Running Inference:  12%|█▏        | 23/200 [00:47<05:28,  1.86s/it]Running Inference:  12%|█▏        | 24/200 [00:53<09:18,  3.17s/it]Running Inference:  12%|█▎        | 25/200 [00:54<07:08,  2.45s/it]Running Inference:  13%|█▎        | 26/200 [01:00<10:35,  3.65s/it]Running Inference:  14%|█▎        | 27/200 [01:03<09:49,  3.40s/it]Running Inference:  14%|█▍        | 28/200 [01:04<07:43,  2.70s/it]Running Inference:  14%|█▍        | 29/200 [01:05<06:22,  2.24s/it]Running Inference:  15%|█▌        | 30/200 [01:08<06:42,  2.37s/it]Running Inference:  16%|█▌        | 31/200 [01:11<06:51,  2.43s/it]Running Inference:  16%|█▌        | 32/200 [01:18<10:35,  3.78s/it]Running Inference:  16%|█▋        | 33/200 [01:20<09:31,  3.42s/it]Running Inference:  17%|█▋        | 34/200 [01:22<08:06,  2.93s/it]Running Inference:  18%|█▊        | 35/200 [01:30<12:01,  4.37s/it]Running Inference:  18%|█▊        | 36/200 [01:32<10:02,  3.67s/it]Running Inference:  18%|█▊        | 37/200 [01:34<08:47,  3.23s/it]Running Inference:  19%|█▉        | 38/200 [01:36<07:48,  2.89s/it]Running Inference:  20%|█▉        | 39/200 [01:38<06:54,  2.57s/it]Running Inference:  20%|██        | 40/200 [01:41<07:28,  2.81s/it]Running Inference:  20%|██        | 41/200 [01:44<07:42,  2.91s/it]Running Inference:  21%|██        | 42/200 [01:46<06:51,  2.60s/it]Running Inference:  22%|██▏       | 43/200 [01:48<05:55,  2.26s/it]Running Inference:  22%|██▏       | 44/200 [01:49<05:18,  2.04s/it]Running Inference:  22%|██▎       | 45/200 [01:51<04:56,  1.91s/it]Running Inference:  23%|██▎       | 46/200 [01:57<08:15,  3.22s/it]Running Inference:  24%|██▎       | 47/200 [01:59<07:00,  2.75s/it]Running Inference:  24%|██▍       | 48/200 [02:00<05:47,  2.29s/it]Running Inference:  24%|██▍       | 49/200 [02:06<08:42,  3.46s/it]Running Inference:  25%|██▌       | 50/200 [02:08<07:08,  2.86s/it]Running Inference:  26%|██▌       | 51/200 [02:08<05:35,  2.25s/it]Running Inference:  26%|██▌       | 52/200 [02:10<05:00,  2.03s/it]Running Inference:  26%|██▋       | 53/200 [02:12<04:41,  1.91s/it]Running Inference:  27%|██▋       | 54/200 [02:13<04:36,  1.89s/it]Running Inference:  28%|██▊       | 55/200 [02:20<08:03,  3.33s/it]Running Inference:  28%|██▊       | 56/200 [02:23<07:30,  3.13s/it]Running Inference:  28%|██▊       | 57/200 [02:25<07:04,  2.97s/it]Running Inference:  29%|██▉       | 58/200 [02:31<09:07,  3.86s/it]Running Inference:  30%|██▉       | 59/200 [02:32<07:07,  3.03s/it]Running Inference:  30%|███       | 60/200 [02:35<06:41,  2.87s/it]Running Inference:  30%|███       | 61/200 [02:36<05:10,  2.23s/it]Running Inference:  31%|███       | 62/200 [02:42<07:48,  3.40s/it]Running Inference:  32%|███▏      | 63/200 [02:43<06:13,  2.73s/it]Running Inference:  32%|███▏      | 64/200 [02:44<05:01,  2.22s/it]Running Inference:  32%|███▎      | 65/200 [02:45<04:05,  1.81s/it]Running Inference:  33%|███▎      | 66/200 [02:47<04:36,  2.07s/it]Running Inference:  34%|███▎      | 67/200 [02:48<03:51,  1.74s/it]Running Inference:  34%|███▍      | 68/200 [02:49<03:12,  1.46s/it]Running Inference:  34%|███▍      | 69/200 [02:52<03:47,  1.74s/it]Running Inference:  35%|███▌      | 70/200 [02:53<03:20,  1.54s/it]Running Inference:  36%|███▌      | 71/200 [02:54<02:49,  1.32s/it]Running Inference:  36%|███▌      | 72/200 [02:55<03:04,  1.45s/it]Running Inference:  36%|███▋      | 73/200 [02:58<03:37,  1.71s/it]Running Inference:  37%|███▋      | 74/200 [03:04<06:49,  3.25s/it]Running Inference:  38%|███▊      | 75/200 [03:06<05:42,  2.74s/it]Running Inference:  38%|███▊      | 76/200 [03:08<05:27,  2.64s/it]Running Inference:  38%|███▊      | 77/200 [03:10<04:44,  2.31s/it]Running Inference:  39%|███▉      | 78/200 [03:11<04:09,  2.04s/it]Running Inference:  40%|███▉      | 79/200 [03:12<03:29,  1.73s/it]Running Inference:  40%|████      | 80/200 [03:14<03:37,  1.81s/it]Running Inference:  40%|████      | 81/200 [03:16<03:45,  1.89s/it]Running Inference:  41%|████      | 82/200 [03:20<04:29,  2.28s/it]Running Inference:  42%|████▏     | 83/200 [03:25<06:03,  3.11s/it]Running Inference:  42%|████▏     | 84/200 [03:26<05:11,  2.69s/it]Running Inference:  42%|████▎     | 85/200 [03:29<05:08,  2.68s/it]Running Inference:  43%|████▎     | 86/200 [03:31<04:28,  2.35s/it]Running Inference:  44%|████▎     | 87/200 [03:33<04:15,  2.26s/it]Running Inference:  44%|████▍     | 88/200 [03:35<04:05,  2.19s/it]Running Inference:  44%|████▍     | 89/200 [03:36<03:33,  1.93s/it]Running Inference:  45%|████▌     | 90/200 [03:38<03:44,  2.04s/it]Running Inference:  46%|████▌     | 91/200 [03:40<03:35,  1.97s/it]Running Inference:  46%|████▌     | 92/200 [03:46<05:25,  3.01s/it]Running Inference:  46%|████▋     | 93/200 [03:51<06:51,  3.85s/it]Running Inference:  47%|████▋     | 94/200 [03:53<05:28,  3.10s/it]Running Inference:  48%|████▊     | 95/200 [03:59<07:05,  4.05s/it]Running Inference:  48%|████▊     | 96/200 [04:01<06:04,  3.50s/it]Running Inference:  48%|████▊     | 97/200 [04:08<07:41,  4.48s/it]Running Inference:  49%|████▉     | 98/200 [04:09<05:57,  3.50s/it]Running Inference:  50%|████▉     | 99/200 [04:15<07:02,  4.18s/it]Running Inference:  50%|█████     | 100/200 [04:16<05:19,  3.20s/it]Running Inference:  50%|█████     | 101/200 [04:18<04:43,  2.87s/it]Running Inference:  51%|█████     | 102/200 [04:19<03:42,  2.27s/it]Running Inference:  52%|█████▏    | 103/200 [04:26<05:59,  3.70s/it]Running Inference:  52%|█████▏    | 104/200 [04:33<07:24,  4.63s/it]Running Inference:  52%|█████▎    | 105/200 [04:38<07:33,  4.77s/it]Running Inference:  53%|█████▎    | 106/200 [04:41<06:54,  4.41s/it]Running Inference:  54%|█████▎    | 107/200 [04:42<05:18,  3.42s/it]Running Inference:  54%|█████▍    | 108/200 [04:44<04:29,  2.93s/it]Running Inference:  55%|█████▍    | 109/200 [04:46<03:45,  2.47s/it]Running Inference:  55%|█████▌    | 110/200 [04:48<03:26,  2.30s/it]Running Inference:  56%|█████▌    | 111/200 [04:50<03:21,  2.26s/it]Running Inference:  56%|█████▌    | 112/200 [04:52<03:19,  2.27s/it]Running Inference:  56%|█████▋    | 113/200 [04:57<04:29,  3.10s/it]Running Inference:  57%|█████▋    | 114/200 [04:58<03:41,  2.58s/it]Running Inference:  57%|█████▊    | 115/200 [05:04<05:01,  3.55s/it]Running Inference:  58%|█████▊    | 116/200 [05:10<06:04,  4.34s/it]Running Inference:  58%|█████▊    | 117/200 [05:17<07:01,  5.07s/it]Running Inference:  59%|█████▉    | 118/200 [05:19<05:28,  4.01s/it]Running Inference:  60%|█████▉    | 119/200 [05:20<04:20,  3.21s/it]Running Inference:  60%|██████    | 120/200 [05:25<05:03,  3.80s/it]Running Inference:  60%|██████    | 121/200 [05:28<04:28,  3.40s/it]Running Inference:  61%|██████    | 122/200 [05:29<03:47,  2.91s/it]Running Inference:  62%|██████▏   | 123/200 [05:30<02:52,  2.24s/it]Running Inference:  62%|██████▏   | 124/200 [05:33<03:01,  2.39s/it]Running Inference:  62%|██████▎   | 125/200 [05:38<04:02,  3.23s/it]Running Inference:  63%|██████▎   | 126/200 [05:39<03:17,  2.67s/it]Running Inference:  64%|██████▎   | 127/200 [05:41<02:45,  2.27s/it]Running Inference:  64%|██████▍   | 128/200 [05:44<03:07,  2.60s/it]Running Inference:  64%|██████▍   | 129/200 [05:45<02:33,  2.16s/it]Running Inference:  65%|██████▌   | 130/200 [05:47<02:32,  2.18s/it]Running Inference:  66%|██████▌   | 131/200 [05:49<02:09,  1.88s/it]Running Inference:  66%|██████▌   | 132/200 [05:50<01:57,  1.73s/it]Running Inference:  66%|██████▋   | 133/200 [05:52<01:53,  1.70s/it]Running Inference:  67%|██████▋   | 134/200 [05:53<01:44,  1.59s/it]Running Inference:  68%|██████▊   | 135/200 [05:54<01:39,  1.54s/it]Running Inference:  68%|██████▊   | 136/200 [05:56<01:30,  1.42s/it]Running Inference:  68%|██████▊   | 137/200 [06:03<03:18,  3.14s/it]Running Inference:  69%|██████▉   | 138/200 [06:08<04:02,  3.91s/it]Running Inference:  70%|██████▉   | 139/200 [06:11<03:42,  3.65s/it]Running Inference:  70%|███████   | 140/200 [06:13<03:01,  3.02s/it]Running Inference:  70%|███████   | 141/200 [06:19<03:49,  3.89s/it]Running Inference:  71%|███████   | 142/200 [06:20<03:04,  3.18s/it]Running Inference:  72%|███████▏  | 143/200 [06:21<02:22,  2.50s/it]Running Inference:  72%|███████▏  | 144/200 [06:24<02:16,  2.44s/it]Running Inference:  72%|███████▎  | 145/200 [06:26<02:12,  2.40s/it]Running Inference:  73%|███████▎  | 146/200 [06:28<01:56,  2.15s/it]Running Inference:  74%|███████▎  | 147/200 [06:31<02:07,  2.41s/it]Running Inference:  74%|███████▍  | 148/200 [06:33<02:06,  2.43s/it]Running Inference:  74%|███████▍  | 149/200 [06:34<01:45,  2.07s/it]Running Inference:  75%|███████▌  | 150/200 [06:36<01:38,  1.97s/it]Running Inference:  76%|███████▌  | 151/200 [06:38<01:42,  2.10s/it]Running Inference:  76%|███████▌  | 152/200 [06:39<01:24,  1.75s/it]Running Inference:  76%|███████▋  | 153/200 [06:46<02:32,  3.24s/it]Running Inference:  77%|███████▋  | 154/200 [06:47<01:56,  2.53s/it]Running Inference:  78%|███████▊  | 155/200 [06:49<01:41,  2.26s/it]Running Inference:  78%|███████▊  | 156/200 [06:50<01:31,  2.09s/it]Running Inference:  78%|███████▊  | 157/200 [06:51<01:16,  1.77s/it]Running Inference:  79%|███████▉  | 158/200 [06:52<01:03,  1.50s/it]Running Inference:  80%|███████▉  | 159/200 [06:54<01:03,  1.56s/it]Running Inference:  80%|████████  | 160/200 [06:55<01:01,  1.55s/it]Running Inference:  80%|████████  | 161/200 [06:58<01:11,  1.82s/it]Running Inference:  81%|████████  | 162/200 [06:59<01:00,  1.59s/it]Running Inference:  82%|████████▏ | 163/200 [07:00<00:58,  1.57s/it]Running Inference:  82%|████████▏ | 164/200 [07:02<00:56,  1.56s/it]Running Inference:  82%|████████▎ | 165/200 [07:04<00:55,  1.60s/it]Running Inference:  83%|████████▎ | 166/200 [07:05<00:47,  1.39s/it]Running Inference:  84%|████████▎ | 167/200 [07:09<01:17,  2.36s/it]Running Inference:  84%|████████▍ | 168/200 [07:10<01:02,  1.95s/it]Running Inference:  84%|████████▍ | 169/200 [07:13<01:11,  2.29s/it]Running Inference:  85%|████████▌ | 170/200 [07:17<01:26,  2.88s/it]Running Inference:  86%|████████▌ | 171/200 [07:19<01:11,  2.46s/it]Running Inference:  86%|████████▌ | 172/200 [07:20<00:59,  2.11s/it]Running Inference:  86%|████████▋ | 173/200 [07:23<00:58,  2.17s/it]Running Inference:  87%|████████▋ | 174/200 [07:25<00:56,  2.15s/it]Running Inference:  88%|████████▊ | 175/200 [07:26<00:50,  2.03s/it]Running Inference:  88%|████████▊ | 176/200 [07:28<00:44,  1.86s/it]Running Inference:  88%|████████▊ | 177/200 [07:33<01:07,  2.93s/it]Running Inference:  89%|████████▉ | 178/200 [07:35<00:57,  2.60s/it]Running Inference:  90%|████████▉ | 179/200 [07:36<00:45,  2.19s/it]Running Inference:  90%|█████████ | 180/200 [07:38<00:38,  1.95s/it]Running Inference:  90%|█████████ | 181/200 [07:40<00:36,  1.94s/it]Running Inference:  91%|█████████ | 182/200 [07:42<00:37,  2.06s/it]Running Inference:  92%|█████████▏| 183/200 [07:44<00:32,  1.89s/it]Running Inference:  92%|█████████▏| 184/200 [07:48<00:40,  2.55s/it]Running Inference:  92%|█████████▎| 185/200 [07:49<00:32,  2.16s/it]Running Inference:  93%|█████████▎| 186/200 [07:51<00:31,  2.22s/it]Running Inference:  94%|█████████▎| 187/200 [07:58<00:45,  3.52s/it]Running Inference:  94%|█████████▍| 188/200 [08:05<00:55,  4.65s/it]Running Inference:  94%|█████████▍| 189/200 [08:07<00:41,  3.81s/it]Running Inference:  95%|█████████▌| 190/200 [08:08<00:31,  3.12s/it]Running Inference:  96%|█████████▌| 191/200 [08:14<00:35,  3.98s/it]Running Inference:  96%|█████████▌| 192/200 [08:15<00:23,  2.98s/it]Running Inference:  96%|█████████▋| 193/200 [08:17<00:19,  2.79s/it]Running Inference:  97%|█████████▋| 194/200 [08:23<00:21,  3.63s/it]Running Inference:  98%|█████████▊| 195/200 [08:29<00:21,  4.33s/it]Running Inference:  98%|█████████▊| 196/200 [08:36<00:19,  5.00s/it]Running Inference:  98%|█████████▊| 197/200 [08:41<00:15,  5.20s/it]Running Inference:  99%|█████████▉| 198/200 [08:43<00:08,  4.05s/it]Running Inference: 100%|█████████▉| 199/200 [08:45<00:03,  3.70s/it]Running Inference: 100%|██████████| 200/200 [08:47<00:00,  3.08s/it]Running Inference: 100%|██████████| 200/200 [08:47<00:00,  2.64s/it]
2025-12-14 22:46:52,523 - INFO - Inference completed.
2025-12-14 22:46:52,541 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 22:46:52,541 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:46:52,542 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 22:46:52,542 - INFO - Metrics:
37.75
2025-12-14 22:46:52,544 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lsht, ratio=0.1) on GPU 4

----------------------------------------
Task: lsht | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:46:59,035 - INFO - Set deterministic seeds to 42
2025-12-14 22:46:59,036 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:46:59,036 - INFO - Starting evaluation run...
2025-12-14 22:46:59,036 - INFO - Output directory set to: longbenchresult
2025-12-14 22:46:59,036 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 22:46:59,036 - INFO - KV Press 'knorm' setup.
2025-12-14 22:46:59,036 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:46:59,036 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.08it/s]
Device set to use cuda:0
2025-12-14 22:47:12,198 - INFO - Model pipeline loaded.
2025-12-14 22:47:12,198 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 22:47:17,736 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:47:17,736 - INFO - Dataset processed with 200 entries.
2025-12-14 22:47:17,771 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:09,  2.16s/it]Running Inference:   1%|          | 2/200 [00:04<07:28,  2.26s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:21,  1.63s/it]Running Inference:   2%|▏         | 4/200 [00:10<09:19,  2.86s/it]Running Inference:   2%|▎         | 5/200 [00:12<09:17,  2.86s/it]Running Inference:   3%|▎         | 6/200 [00:14<08:13,  2.54s/it]Running Inference:   4%|▎         | 7/200 [00:17<07:50,  2.44s/it]Running Inference:   4%|▍         | 8/200 [00:18<06:30,  2.04s/it]Running Inference:   4%|▍         | 9/200 [00:20<06:37,  2.08s/it]Running Inference:   5%|▌         | 10/200 [00:21<05:44,  1.81s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<05:51,  1.86s/it]Running Inference:   6%|▌         | 12/200 [00:25<05:24,  1.73s/it]Running Inference:   6%|▋         | 13/200 [00:27<05:58,  1.91s/it]Running Inference:   7%|▋         | 14/200 [00:28<05:20,  1.72s/it]Running Inference:   8%|▊         | 15/200 [00:31<06:33,  2.13s/it]Running Inference:   8%|▊         | 16/200 [00:33<05:44,  1.87s/it]Running Inference:   8%|▊         | 17/200 [00:39<09:58,  3.27s/it]Running Inference:   9%|▉         | 18/200 [00:41<09:01,  2.98s/it]Running Inference:  10%|▉         | 19/200 [00:43<07:22,  2.45s/it]Running Inference:  10%|█         | 20/200 [00:45<07:15,  2.42s/it]Running Inference:  10%|█         | 21/200 [00:47<06:58,  2.34s/it]Running Inference:  11%|█         | 22/200 [00:49<06:25,  2.16s/it]Running Inference:  12%|█▏        | 23/200 [00:50<05:23,  1.83s/it]Running Inference:  12%|█▏        | 24/200 [00:53<06:09,  2.10s/it]Running Inference:  12%|█▎        | 25/200 [00:53<04:57,  1.70s/it]Running Inference:  13%|█▎        | 26/200 [00:56<05:59,  2.07s/it]Running Inference:  14%|█▎        | 27/200 [00:59<06:36,  2.29s/it]Running Inference:  14%|█▍        | 28/200 [01:00<05:30,  1.92s/it]Running Inference:  14%|█▍        | 29/200 [01:01<04:51,  1.71s/it]Running Inference:  15%|█▌        | 30/200 [01:04<05:41,  2.01s/it]Running Inference:  16%|█▌        | 31/200 [01:07<06:08,  2.18s/it]Running Inference:  16%|█▌        | 32/200 [01:10<07:09,  2.56s/it]Running Inference:  16%|█▋        | 33/200 [01:13<07:13,  2.60s/it]Running Inference:  17%|█▋        | 34/200 [01:15<06:29,  2.35s/it]Running Inference:  18%|█▊        | 35/200 [01:22<10:50,  3.94s/it]Running Inference:  18%|█▊        | 36/200 [01:24<09:12,  3.37s/it]Running Inference:  18%|█▊        | 37/200 [01:26<08:12,  3.02s/it]Running Inference:  19%|█▉        | 38/200 [01:29<07:24,  2.74s/it]Running Inference:  20%|█▉        | 39/200 [01:30<06:37,  2.47s/it]Running Inference:  20%|██        | 40/200 [01:34<07:17,  2.73s/it]Running Inference:  20%|██        | 41/200 [01:37<07:33,  2.85s/it]Running Inference:  21%|██        | 42/200 [01:39<06:42,  2.55s/it]Running Inference:  22%|██▏       | 43/200 [01:40<05:49,  2.23s/it]Running Inference:  22%|██▏       | 44/200 [01:42<05:14,  2.02s/it]Running Inference:  22%|██▎       | 45/200 [01:43<04:57,  1.92s/it]Running Inference:  23%|██▎       | 46/200 [01:50<08:13,  3.20s/it]Running Inference:  24%|██▎       | 47/200 [01:51<06:56,  2.72s/it]Running Inference:  24%|██▍       | 48/200 [01:52<05:44,  2.26s/it]Running Inference:  24%|██▍       | 49/200 [01:55<06:09,  2.45s/it]Running Inference:  25%|██▌       | 50/200 [01:57<05:22,  2.15s/it]Running Inference:  26%|██▌       | 51/200 [01:58<04:22,  1.76s/it]Running Inference:  26%|██▌       | 52/200 [01:59<04:09,  1.68s/it]Running Inference:  26%|██▋       | 53/200 [02:01<04:05,  1.67s/it]Running Inference:  27%|██▋       | 54/200 [02:03<04:11,  1.72s/it]Running Inference:  28%|██▊       | 55/200 [02:06<05:17,  2.19s/it]Running Inference:  28%|██▊       | 56/200 [02:09<05:35,  2.33s/it]Running Inference:  28%|██▊       | 57/200 [02:11<05:43,  2.40s/it]Running Inference:  29%|██▉       | 58/200 [02:14<05:43,  2.42s/it]Running Inference:  30%|██▉       | 59/200 [02:15<04:46,  2.03s/it]Running Inference:  30%|███       | 60/200 [02:17<05:04,  2.17s/it]Running Inference:  30%|███       | 61/200 [02:18<03:58,  1.71s/it]Running Inference:  31%|███       | 62/200 [02:20<04:34,  1.99s/it]Running Inference:  32%|███▏      | 63/200 [02:22<03:58,  1.74s/it]Running Inference:  32%|███▏      | 64/200 [02:23<03:27,  1.52s/it]Running Inference:  32%|███▎      | 65/200 [02:24<03:01,  1.34s/it]Running Inference:  33%|███▎      | 66/200 [02:26<03:52,  1.73s/it]Running Inference:  34%|███▎      | 67/200 [02:27<03:20,  1.51s/it]Running Inference:  34%|███▍      | 68/200 [02:28<02:51,  1.30s/it]Running Inference:  34%|███▍      | 69/200 [02:30<03:32,  1.63s/it]Running Inference:  35%|███▌      | 70/200 [02:31<03:09,  1.46s/it]Running Inference:  36%|███▌      | 71/200 [02:32<02:39,  1.24s/it]Running Inference:  36%|███▌      | 72/200 [02:34<02:57,  1.39s/it]Running Inference:  36%|███▋      | 73/200 [02:36<03:31,  1.67s/it]Running Inference:  37%|███▋      | 74/200 [02:40<04:31,  2.16s/it]Running Inference:  38%|███▊      | 75/200 [02:41<04:06,  1.97s/it]Running Inference:  38%|███▊      | 76/200 [02:43<04:20,  2.10s/it]Running Inference:  38%|███▊      | 77/200 [02:45<03:58,  1.93s/it]Running Inference:  39%|███▉      | 78/200 [02:46<03:36,  1.78s/it]Running Inference:  40%|███▉      | 79/200 [02:47<03:06,  1.54s/it]Running Inference:  40%|████      | 80/200 [02:49<03:22,  1.68s/it]Running Inference:  40%|████      | 81/200 [02:52<03:35,  1.81s/it]Running Inference:  41%|████      | 82/200 [02:55<04:22,  2.23s/it]Running Inference:  42%|████▏     | 83/200 [02:57<04:05,  2.10s/it]Running Inference:  42%|████▏     | 84/200 [02:58<03:57,  2.05s/it]Running Inference:  42%|████▎     | 85/200 [03:01<04:16,  2.23s/it]Running Inference:  43%|████▎     | 86/200 [03:03<03:52,  2.04s/it]Running Inference:  44%|████▎     | 87/200 [03:08<05:42,  3.03s/it]Running Inference:  44%|████▍     | 88/200 [03:10<05:06,  2.74s/it]Running Inference:  44%|████▍     | 89/200 [03:11<04:16,  2.31s/it]Running Inference:  45%|████▌     | 90/200 [03:14<04:13,  2.31s/it]Running Inference:  46%|████▌     | 91/200 [03:16<03:55,  2.16s/it]Running Inference:  46%|████▌     | 92/200 [03:18<03:55,  2.18s/it]Running Inference:  46%|████▋     | 93/200 [03:20<04:05,  2.29s/it]Running Inference:  47%|████▋     | 94/200 [03:22<03:32,  2.01s/it]Running Inference:  48%|████▊     | 95/200 [03:25<03:57,  2.26s/it]Running Inference:  48%|████▊     | 96/200 [03:27<03:53,  2.25s/it]Running Inference:  48%|████▊     | 97/200 [03:33<06:09,  3.58s/it]Running Inference:  49%|████▉     | 98/200 [03:35<04:52,  2.87s/it]Running Inference:  50%|████▉     | 99/200 [03:37<04:32,  2.70s/it]Running Inference:  50%|█████     | 100/200 [03:38<03:36,  2.16s/it]Running Inference:  50%|█████     | 101/200 [03:40<03:30,  2.12s/it]Running Inference:  51%|█████     | 102/200 [03:41<02:51,  1.75s/it]Running Inference:  52%|█████▏    | 103/200 [03:48<05:21,  3.32s/it]Running Inference:  52%|█████▏    | 104/200 [03:54<06:56,  4.34s/it]Running Inference:  52%|█████▎    | 105/200 [03:56<05:37,  3.55s/it]Running Inference:  53%|█████▎    | 106/200 [04:03<07:13,  4.61s/it]Running Inference:  54%|█████▎    | 107/200 [04:04<05:31,  3.56s/it]Running Inference:  54%|█████▍    | 108/200 [04:06<04:38,  3.02s/it]Running Inference:  55%|█████▍    | 109/200 [04:08<03:51,  2.54s/it]Running Inference:  55%|█████▌    | 110/200 [04:09<03:31,  2.35s/it]Running Inference:  56%|█████▌    | 111/200 [04:12<03:24,  2.29s/it]Running Inference:  56%|█████▌    | 112/200 [04:14<03:20,  2.28s/it]Running Inference:  56%|█████▋    | 113/200 [04:16<03:02,  2.10s/it]Running Inference:  57%|█████▋    | 114/200 [04:17<02:38,  1.85s/it]Running Inference:  57%|█████▊    | 115/200 [04:19<02:52,  2.03s/it]Running Inference:  58%|█████▊    | 116/200 [04:22<03:07,  2.23s/it]Running Inference:  58%|█████▊    | 117/200 [04:25<03:32,  2.56s/it]Running Inference:  59%|█████▉    | 118/200 [04:27<03:02,  2.22s/it]Running Inference:  60%|█████▉    | 119/200 [04:28<02:38,  1.96s/it]Running Inference:  60%|██████    | 120/200 [04:30<02:32,  1.90s/it]Running Inference:  60%|██████    | 121/200 [04:32<02:43,  2.07s/it]Running Inference:  61%|██████    | 122/200 [04:34<02:34,  1.98s/it]Running Inference:  62%|██████▏   | 123/200 [04:35<02:02,  1.59s/it]Running Inference:  62%|██████▏   | 124/200 [04:37<02:26,  1.93s/it]Running Inference:  62%|██████▎   | 125/200 [04:39<02:23,  1.91s/it]Running Inference:  63%|██████▎   | 126/200 [04:41<02:13,  1.80s/it]Running Inference:  64%|██████▎   | 127/200 [04:42<02:01,  1.66s/it]Running Inference:  64%|██████▍   | 128/200 [04:46<02:36,  2.17s/it]Running Inference:  64%|██████▍   | 129/200 [04:47<02:12,  1.86s/it]Running Inference:  65%|██████▌   | 130/200 [04:49<02:16,  1.96s/it]Running Inference:  66%|██████▌   | 131/200 [04:50<01:58,  1.72s/it]Running Inference:  66%|██████▌   | 132/200 [04:51<01:50,  1.62s/it]Running Inference:  66%|██████▋   | 133/200 [04:53<01:48,  1.62s/it]Running Inference:  67%|██████▋   | 134/200 [04:54<01:41,  1.53s/it]Running Inference:  68%|██████▊   | 135/200 [04:56<01:37,  1.50s/it]Running Inference:  68%|██████▊   | 136/200 [04:57<01:26,  1.35s/it]Running Inference:  68%|██████▊   | 137/200 [05:00<02:07,  2.02s/it]Running Inference:  69%|██████▉   | 138/200 [05:03<02:10,  2.10s/it]Running Inference:  70%|██████▉   | 139/200 [05:06<02:25,  2.38s/it]Running Inference:  70%|███████   | 140/200 [05:07<02:07,  2.13s/it]Running Inference:  70%|███████   | 141/200 [05:13<03:11,  3.25s/it]Running Inference:  71%|███████   | 142/200 [05:15<02:38,  2.73s/it]Running Inference:  72%|███████▏  | 143/200 [05:16<02:04,  2.18s/it]Running Inference:  72%|███████▏  | 144/200 [05:18<02:03,  2.21s/it]Running Inference:  72%|███████▎  | 145/200 [05:20<02:03,  2.24s/it]Running Inference:  73%|███████▎  | 146/200 [05:22<01:49,  2.03s/it]Running Inference:  74%|███████▎  | 147/200 [05:25<02:03,  2.32s/it]Running Inference:  74%|███████▍  | 148/200 [05:27<02:02,  2.36s/it]Running Inference:  74%|███████▍  | 149/200 [05:28<01:41,  1.99s/it]Running Inference:  75%|███████▌  | 150/200 [05:30<01:37,  1.95s/it]Running Inference:  76%|███████▌  | 151/200 [05:33<01:42,  2.08s/it]Running Inference:  76%|███████▌  | 152/200 [05:33<01:23,  1.74s/it]Running Inference:  76%|███████▋  | 153/200 [05:37<01:41,  2.17s/it]Running Inference:  77%|███████▋  | 154/200 [05:38<01:21,  1.78s/it]Running Inference:  78%|███████▊  | 155/200 [05:39<01:18,  1.74s/it]Running Inference:  78%|███████▊  | 156/200 [05:41<01:15,  1.72s/it]Running Inference:  78%|███████▊  | 157/200 [05:42<01:05,  1.51s/it]Running Inference:  79%|███████▉  | 158/200 [05:43<00:55,  1.32s/it]Running Inference:  80%|███████▉  | 159/200 [05:44<00:58,  1.43s/it]Running Inference:  80%|████████  | 160/200 [05:46<00:58,  1.45s/it]Running Inference:  80%|████████  | 161/200 [05:48<01:08,  1.76s/it]Running Inference:  81%|████████  | 162/200 [05:49<00:58,  1.54s/it]Running Inference:  82%|████████▏ | 163/200 [05:51<00:57,  1.55s/it]Running Inference:  82%|████████▏ | 164/200 [05:53<00:55,  1.54s/it]Running Inference:  82%|████████▎ | 165/200 [05:54<00:55,  1.58s/it]Running Inference:  83%|████████▎ | 166/200 [05:55<00:47,  1.38s/it]Running Inference:  84%|████████▎ | 167/200 [05:56<00:44,  1.35s/it]Running Inference:  84%|████████▍ | 168/200 [05:58<00:41,  1.30s/it]Running Inference:  84%|████████▍ | 169/200 [06:01<00:56,  1.83s/it]Running Inference:  85%|████████▌ | 170/200 [06:02<00:47,  1.58s/it]Running Inference:  86%|████████▌ | 171/200 [06:03<00:45,  1.55s/it]Running Inference:  86%|████████▌ | 172/200 [06:04<00:41,  1.47s/it]Running Inference:  86%|████████▋ | 173/200 [06:07<00:46,  1.73s/it]Running Inference:  87%|████████▋ | 174/200 [06:09<00:47,  1.84s/it]Running Inference:  88%|████████▊ | 175/200 [06:11<00:45,  1.81s/it]Running Inference:  88%|████████▊ | 176/200 [06:12<00:41,  1.71s/it]Running Inference:  88%|████████▊ | 177/200 [06:17<01:04,  2.81s/it]Running Inference:  89%|████████▉ | 178/200 [06:19<00:55,  2.54s/it]Running Inference:  90%|████████▉ | 179/200 [06:21<00:44,  2.14s/it]Running Inference:  90%|█████████ | 180/200 [06:22<00:38,  1.91s/it]Running Inference:  90%|█████████ | 181/200 [06:24<00:36,  1.92s/it]Running Inference:  91%|█████████ | 182/200 [06:26<00:36,  2.04s/it]Running Inference:  92%|█████████▏| 183/200 [06:28<00:31,  1.87s/it]Running Inference:  92%|█████████▏| 184/200 [06:28<00:24,  1.56s/it]Running Inference:  92%|█████████▎| 185/200 [06:30<00:21,  1.47s/it]Running Inference:  93%|█████████▎| 186/200 [06:32<00:24,  1.73s/it]Running Inference:  94%|█████████▎| 187/200 [06:39<00:41,  3.15s/it]Running Inference:  94%|█████████▍| 188/200 [06:46<00:52,  4.36s/it]Running Inference:  94%|█████████▍| 189/200 [06:48<00:39,  3.58s/it]Running Inference:  95%|█████████▌| 190/200 [06:49<00:29,  2.96s/it]Running Inference:  96%|█████████▌| 191/200 [06:55<00:34,  3.84s/it]Running Inference:  96%|█████████▌| 192/200 [06:59<00:30,  3.87s/it]Running Inference:  96%|█████████▋| 193/200 [07:01<00:23,  3.39s/it]Running Inference:  97%|█████████▋| 194/200 [07:07<00:24,  4.03s/it]Running Inference:  98%|█████████▊| 195/200 [07:09<00:17,  3.57s/it]Running Inference:  98%|█████████▊| 196/200 [07:12<00:13,  3.40s/it]Running Inference:  98%|█████████▊| 197/200 [07:18<00:12,  4.07s/it]Running Inference:  99%|█████████▉| 198/200 [07:19<00:06,  3.23s/it]Running Inference: 100%|█████████▉| 199/200 [07:22<00:03,  3.12s/it]Running Inference: 100%|██████████| 200/200 [07:24<00:00,  2.68s/it]Running Inference: 100%|██████████| 200/200 [07:24<00:00,  2.22s/it]
2025-12-14 22:54:41,869 - INFO - Inference completed.
2025-12-14 22:54:41,887 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 22:54:41,887 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:54:41,888 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 22:54:41,888 - INFO - Metrics:
27.25
2025-12-14 22:54:41,890 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lsht, ratio=0.2) on GPU 4

----------------------------------------
Task: lsht | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:54:48,326 - INFO - Set deterministic seeds to 42
2025-12-14 22:54:48,326 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:54:48,327 - INFO - Starting evaluation run...
2025-12-14 22:54:48,327 - INFO - Output directory set to: longbenchresult
2025-12-14 22:54:48,327 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 22:54:48,327 - INFO - KV Press 'knorm' setup.
2025-12-14 22:54:48,327 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:54:48,327 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.21it/s]
Device set to use cuda:0
2025-12-14 22:55:00,391 - INFO - Model pipeline loaded.
2025-12-14 22:55:00,391 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 22:55:07,093 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:55:07,093 - INFO - Dataset processed with 200 entries.
2025-12-14 22:55:07,127 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:10,  2.16s/it]Running Inference:   1%|          | 2/200 [00:04<07:27,  2.26s/it]Running Inference:   2%|▏         | 3/200 [00:05<05:23,  1.64s/it]Running Inference:   2%|▏         | 4/200 [00:10<09:24,  2.88s/it]Running Inference:   2%|▎         | 5/200 [00:12<09:13,  2.84s/it]Running Inference:   3%|▎         | 6/200 [00:14<08:18,  2.57s/it]Running Inference:   4%|▎         | 7/200 [00:17<07:54,  2.46s/it]Running Inference:   4%|▍         | 8/200 [00:18<06:34,  2.06s/it]Running Inference:   4%|▍         | 9/200 [00:20<06:40,  2.10s/it]Running Inference:   5%|▌         | 10/200 [00:21<05:45,  1.82s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<05:52,  1.87s/it]Running Inference:   6%|▌         | 12/200 [00:25<05:25,  1.73s/it]Running Inference:   6%|▋         | 13/200 [00:27<05:58,  1.91s/it]Running Inference:   7%|▋         | 14/200 [00:28<05:25,  1.75s/it]Running Inference:   8%|▊         | 15/200 [00:31<06:36,  2.14s/it]Running Inference:   8%|▊         | 16/200 [00:33<05:45,  1.88s/it]Running Inference:   8%|▊         | 17/200 [00:36<06:48,  2.23s/it]Running Inference:   9%|▉         | 18/200 [00:38<07:05,  2.34s/it]Running Inference:  10%|▉         | 19/200 [00:40<06:01,  2.00s/it]Running Inference:  10%|█         | 20/200 [00:42<06:18,  2.10s/it]Running Inference:  10%|█         | 21/200 [00:44<06:18,  2.11s/it]Running Inference:  11%|█         | 22/200 [00:46<05:58,  2.01s/it]Running Inference:  12%|█▏        | 23/200 [00:47<05:04,  1.72s/it]Running Inference:  12%|█▏        | 24/200 [00:50<05:56,  2.02s/it]Running Inference:  12%|█▎        | 25/200 [00:54<07:48,  2.68s/it]Running Inference:  13%|█▎        | 26/200 [00:57<07:57,  2.75s/it]Running Inference:  14%|█▎        | 27/200 [01:00<07:58,  2.76s/it]Running Inference:  14%|█▍        | 28/200 [01:01<06:26,  2.25s/it]Running Inference:  14%|█▍        | 29/200 [01:02<05:28,  1.92s/it]Running Inference:  15%|█▌        | 30/200 [01:04<06:06,  2.16s/it]Running Inference:  16%|█▌        | 31/200 [01:07<06:25,  2.28s/it]Running Inference:  16%|█▌        | 32/200 [01:10<07:20,  2.62s/it]Running Inference:  16%|█▋        | 33/200 [01:13<07:20,  2.64s/it]Running Inference:  17%|█▋        | 34/200 [01:15<06:34,  2.38s/it]Running Inference:  18%|█▊        | 35/200 [01:22<10:51,  3.95s/it]Running Inference:  18%|█▊        | 36/200 [01:24<09:13,  3.37s/it]Running Inference:  18%|█▊        | 37/200 [01:27<08:12,  3.02s/it]Running Inference:  19%|█▉        | 38/200 [01:29<07:24,  2.74s/it]Running Inference:  20%|█▉        | 39/200 [01:31<06:41,  2.49s/it]Running Inference:  20%|██        | 40/200 [01:34<07:18,  2.74s/it]Running Inference:  20%|██        | 41/200 [01:37<07:34,  2.86s/it]Running Inference:  21%|██        | 42/200 [01:39<06:45,  2.56s/it]Running Inference:  22%|██▏       | 43/200 [01:41<05:51,  2.24s/it]Running Inference:  22%|██▏       | 44/200 [01:42<05:15,  2.02s/it]Running Inference:  22%|██▎       | 45/200 [01:44<04:57,  1.92s/it]Running Inference:  23%|██▎       | 46/200 [01:50<08:12,  3.20s/it]Running Inference:  24%|██▎       | 47/200 [01:52<06:58,  2.73s/it]Running Inference:  24%|██▍       | 48/200 [01:53<05:45,  2.27s/it]Running Inference:  24%|██▍       | 49/200 [01:55<06:02,  2.40s/it]Running Inference:  25%|██▌       | 50/200 [01:57<05:16,  2.11s/it]Running Inference:  26%|██▌       | 51/200 [01:58<04:16,  1.72s/it]Running Inference:  26%|██▌       | 52/200 [01:59<04:05,  1.66s/it]Running Inference:  26%|██▋       | 53/200 [02:01<04:03,  1.65s/it]Running Inference:  27%|██▋       | 54/200 [02:03<04:13,  1.74s/it]Running Inference:  28%|██▊       | 55/200 [02:09<07:42,  3.19s/it]Running Inference:  28%|██▊       | 56/200 [02:12<07:19,  3.05s/it]Running Inference:  28%|██▊       | 57/200 [02:15<06:55,  2.91s/it]Running Inference:  29%|██▉       | 58/200 [02:17<06:37,  2.80s/it]Running Inference:  30%|██▉       | 59/200 [02:18<05:23,  2.29s/it]Running Inference:  30%|███       | 60/200 [02:21<05:29,  2.35s/it]Running Inference:  30%|███       | 61/200 [02:25<06:38,  2.86s/it]Running Inference:  31%|███       | 62/200 [02:28<06:28,  2.82s/it]Running Inference:  32%|███▏      | 63/200 [02:29<05:18,  2.32s/it]Running Inference:  32%|███▏      | 64/200 [02:30<04:25,  1.96s/it]Running Inference:  32%|███▎      | 65/200 [02:34<05:58,  2.66s/it]Running Inference:  33%|███▎      | 66/200 [02:37<05:54,  2.64s/it]Running Inference:  34%|███▎      | 67/200 [02:38<04:45,  2.14s/it]Running Inference:  34%|███▍      | 68/200 [02:39<03:50,  1.75s/it]Running Inference:  34%|███▍      | 69/200 [02:41<04:13,  1.94s/it]Running Inference:  35%|███▌      | 70/200 [02:42<03:38,  1.68s/it]Running Inference:  36%|███▌      | 71/200 [02:43<03:02,  1.41s/it]Running Inference:  36%|███▌      | 72/200 [02:45<03:13,  1.51s/it]Running Inference:  36%|███▋      | 73/200 [02:47<03:42,  1.75s/it]Running Inference:  37%|███▋      | 74/200 [02:50<04:37,  2.21s/it]Running Inference:  38%|███▊      | 75/200 [02:52<04:10,  2.00s/it]Running Inference:  38%|███▊      | 76/200 [02:54<04:22,  2.12s/it]Running Inference:  38%|███▊      | 77/200 [02:56<03:59,  1.95s/it]Running Inference:  39%|███▉      | 78/200 [02:57<03:39,  1.80s/it]Running Inference:  40%|███▉      | 79/200 [02:58<03:08,  1.56s/it]Running Inference:  40%|████      | 80/200 [03:00<03:23,  1.69s/it]Running Inference:  40%|████      | 81/200 [03:02<03:36,  1.82s/it]Running Inference:  41%|████      | 82/200 [03:05<04:22,  2.23s/it]Running Inference:  42%|████▏     | 83/200 [03:07<04:05,  2.10s/it]Running Inference:  42%|████▏     | 84/200 [03:09<03:57,  2.05s/it]Running Inference:  42%|████▎     | 85/200 [03:12<04:16,  2.23s/it]Running Inference:  43%|████▎     | 86/200 [03:13<03:52,  2.04s/it]Running Inference:  44%|████▎     | 87/200 [03:19<05:43,  3.04s/it]Running Inference:  44%|████▍     | 88/200 [03:21<05:07,  2.74s/it]Running Inference:  44%|████▍     | 89/200 [03:22<04:17,  2.32s/it]Running Inference:  45%|████▌     | 90/200 [03:24<04:14,  2.32s/it]Running Inference:  46%|████▌     | 91/200 [03:26<03:58,  2.19s/it]Running Inference:  46%|████▌     | 92/200 [03:32<05:40,  3.15s/it]Running Inference:  46%|████▋     | 93/200 [03:34<05:17,  2.97s/it]Running Inference:  47%|████▋     | 94/200 [03:36<04:22,  2.48s/it]Running Inference:  48%|████▊     | 95/200 [03:38<04:31,  2.59s/it]Running Inference:  48%|████▊     | 96/200 [03:41<04:16,  2.47s/it]Running Inference:  48%|████▊     | 97/200 [03:44<04:39,  2.72s/it]Running Inference:  49%|████▉     | 98/200 [03:45<03:50,  2.26s/it]Running Inference:  50%|████▉     | 99/200 [03:47<03:49,  2.28s/it]Running Inference:  50%|█████     | 100/200 [03:48<03:07,  1.88s/it]Running Inference:  50%|█████     | 101/200 [03:50<03:11,  1.94s/it]Running Inference:  51%|█████     | 102/200 [03:51<02:38,  1.62s/it]Running Inference:  52%|█████▏    | 103/200 [03:55<03:30,  2.17s/it]Running Inference:  52%|█████▏    | 104/200 [03:58<04:01,  2.51s/it]Running Inference:  52%|█████▎    | 105/200 [04:00<03:35,  2.27s/it]Running Inference:  53%|█████▎    | 106/200 [04:07<05:47,  3.70s/it]Running Inference:  54%|█████▎    | 107/200 [04:08<04:31,  2.92s/it]Running Inference:  54%|█████▍    | 108/200 [04:10<03:56,  2.57s/it]Running Inference:  55%|█████▍    | 109/200 [04:11<03:21,  2.22s/it]Running Inference:  55%|█████▌    | 110/200 [04:13<03:10,  2.12s/it]Running Inference:  56%|█████▌    | 111/200 [04:15<03:10,  2.13s/it]Running Inference:  56%|█████▌    | 112/200 [04:17<03:11,  2.17s/it]Running Inference:  56%|█████▋    | 113/200 [04:19<02:55,  2.02s/it]Running Inference:  57%|█████▋    | 114/200 [04:20<02:37,  1.83s/it]Running Inference:  57%|█████▊    | 115/200 [04:23<02:51,  2.02s/it]Running Inference:  58%|█████▊    | 116/200 [04:29<04:32,  3.24s/it]Running Inference:  58%|█████▊    | 117/200 [04:32<04:30,  3.26s/it]Running Inference:  59%|█████▉    | 118/200 [04:34<03:44,  2.74s/it]Running Inference:  60%|█████▉    | 119/200 [04:35<03:00,  2.23s/it]Running Inference:  60%|██████    | 120/200 [04:37<02:47,  2.09s/it]Running Inference:  60%|██████    | 121/200 [04:39<02:55,  2.22s/it]Running Inference:  61%|██████    | 122/200 [04:41<02:43,  2.09s/it]Running Inference:  62%|██████▏   | 123/200 [04:42<02:08,  1.67s/it]Running Inference:  62%|██████▏   | 124/200 [04:44<02:30,  1.98s/it]Running Inference:  62%|██████▎   | 125/200 [04:46<02:25,  1.94s/it]Running Inference:  63%|██████▎   | 126/200 [04:48<02:14,  1.82s/it]Running Inference:  64%|██████▎   | 127/200 [04:49<02:01,  1.67s/it]Running Inference:  64%|██████▍   | 128/200 [04:52<02:37,  2.19s/it]Running Inference:  64%|██████▍   | 129/200 [04:53<02:12,  1.86s/it]Running Inference:  65%|██████▌   | 130/200 [04:56<02:16,  1.96s/it]Running Inference:  66%|██████▌   | 131/200 [04:57<01:59,  1.74s/it]Running Inference:  66%|██████▌   | 132/200 [04:58<01:50,  1.63s/it]Running Inference:  66%|██████▋   | 133/200 [05:00<01:49,  1.63s/it]Running Inference:  67%|██████▋   | 134/200 [05:01<01:41,  1.54s/it]Running Inference:  68%|██████▊   | 135/200 [05:03<01:37,  1.50s/it]Running Inference:  68%|██████▊   | 136/200 [05:04<01:28,  1.38s/it]Running Inference:  68%|██████▊   | 137/200 [05:07<02:08,  2.03s/it]Running Inference:  69%|██████▉   | 138/200 [05:10<02:10,  2.11s/it]Running Inference:  70%|██████▉   | 139/200 [05:13<02:28,  2.43s/it]Running Inference:  70%|███████   | 140/200 [05:14<02:09,  2.16s/it]Running Inference:  70%|███████   | 141/200 [05:17<02:14,  2.28s/it]Running Inference:  71%|███████   | 142/200 [05:18<01:58,  2.05s/it]Running Inference:  72%|███████▏  | 143/200 [05:19<01:37,  1.71s/it]Running Inference:  72%|███████▏  | 144/200 [05:22<01:45,  1.88s/it]Running Inference:  72%|███████▎  | 145/200 [05:24<01:50,  2.01s/it]Running Inference:  73%|███████▎  | 146/200 [05:25<01:40,  1.87s/it]Running Inference:  74%|███████▎  | 147/200 [05:28<01:57,  2.22s/it]Running Inference:  74%|███████▍  | 148/200 [05:31<01:57,  2.26s/it]Running Inference:  74%|███████▍  | 149/200 [05:32<01:43,  2.03s/it]Running Inference:  75%|███████▌  | 150/200 [05:34<01:37,  1.94s/it]Running Inference:  76%|███████▌  | 151/200 [05:36<01:41,  2.08s/it]Running Inference:  76%|███████▌  | 152/200 [05:37<01:23,  1.74s/it]Running Inference:  76%|███████▋  | 153/200 [05:41<01:41,  2.17s/it]Running Inference:  77%|███████▋  | 154/200 [05:41<01:23,  1.80s/it]Running Inference:  78%|███████▊  | 155/200 [05:43<01:18,  1.75s/it]Running Inference:  78%|███████▊  | 156/200 [05:45<01:14,  1.69s/it]Running Inference:  78%|███████▊  | 157/200 [05:46<01:04,  1.50s/it]Running Inference:  79%|███████▉  | 158/200 [05:47<00:54,  1.31s/it]Running Inference:  80%|███████▉  | 159/200 [05:48<00:58,  1.42s/it]Running Inference:  80%|████████  | 160/200 [05:50<00:57,  1.45s/it]Running Inference:  80%|████████  | 161/200 [05:52<01:08,  1.75s/it]Running Inference:  81%|████████  | 162/200 [05:53<00:59,  1.56s/it]Running Inference:  82%|████████▏ | 163/200 [05:55<00:57,  1.56s/it]Running Inference:  82%|████████▏ | 164/200 [05:56<00:55,  1.55s/it]Running Inference:  82%|████████▎ | 165/200 [05:58<00:55,  1.59s/it]Running Inference:  83%|████████▎ | 166/200 [05:59<00:48,  1.41s/it]Running Inference:  84%|████████▎ | 167/200 [06:00<00:44,  1.35s/it]Running Inference:  84%|████████▍ | 168/200 [06:02<00:41,  1.31s/it]Running Inference:  84%|████████▍ | 169/200 [06:05<00:56,  1.83s/it]Running Inference:  85%|████████▌ | 170/200 [06:06<00:47,  1.59s/it]Running Inference:  86%|████████▌ | 171/200 [06:07<00:45,  1.55s/it]Running Inference:  86%|████████▌ | 172/200 [06:12<01:08,  2.46s/it]Running Inference:  86%|████████▋ | 173/200 [06:14<01:05,  2.42s/it]Running Inference:  87%|████████▋ | 174/200 [06:16<01:00,  2.32s/it]Running Inference:  88%|████████▊ | 175/200 [06:18<00:53,  2.14s/it]Running Inference:  88%|████████▊ | 176/200 [06:19<00:46,  1.94s/it]Running Inference:  88%|████████▊ | 177/200 [06:25<01:08,  2.99s/it]Running Inference:  89%|████████▉ | 178/200 [06:26<00:57,  2.63s/it]Running Inference:  90%|████████▉ | 179/200 [06:28<00:46,  2.21s/it]Running Inference:  90%|█████████ | 180/200 [06:29<00:39,  1.96s/it]Running Inference:  90%|█████████ | 181/200 [06:34<00:56,  2.95s/it]Running Inference:  91%|█████████ | 182/200 [06:37<00:49,  2.76s/it]Running Inference:  92%|█████████▏| 183/200 [06:38<00:40,  2.37s/it]Running Inference:  92%|█████████▏| 184/200 [06:42<00:46,  2.89s/it]Running Inference:  92%|█████████▎| 185/200 [06:43<00:35,  2.40s/it]Running Inference:  93%|█████████▎| 186/200 [06:46<00:33,  2.38s/it]Running Inference:  94%|█████████▎| 187/200 [06:49<00:33,  2.57s/it]Running Inference:  94%|█████████▍| 188/200 [06:53<00:35,  2.92s/it]Running Inference:  94%|█████████▍| 189/200 [06:58<00:39,  3.60s/it]Running Inference:  95%|█████████▌| 190/200 [06:59<00:29,  2.97s/it]Running Inference:  96%|█████████▌| 191/200 [07:02<00:25,  2.88s/it]Running Inference:  96%|█████████▌| 192/200 [07:04<00:21,  2.65s/it]Running Inference:  96%|█████████▋| 193/200 [07:06<00:17,  2.53s/it]Running Inference:  97%|█████████▋| 194/200 [07:09<00:14,  2.47s/it]Running Inference:  98%|█████████▊| 195/200 [07:11<00:12,  2.48s/it]Running Inference:  98%|█████████▊| 196/200 [07:14<00:10,  2.64s/it]Running Inference:  98%|█████████▊| 197/200 [07:20<00:10,  3.55s/it]Running Inference:  99%|█████████▉| 198/200 [07:21<00:05,  2.88s/it]Running Inference: 100%|█████████▉| 199/200 [07:24<00:02,  2.86s/it]Running Inference: 100%|██████████| 200/200 [07:26<00:00,  2.48s/it]Running Inference: 100%|██████████| 200/200 [07:26<00:00,  2.23s/it]
2025-12-14 23:02:33,182 - INFO - Inference completed.
2025-12-14 23:02:33,202 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 23:02:33,202 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:02:33,203 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 23:02:33,203 - INFO - Metrics:
25.0
2025-12-14 23:02:33,204 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lsht, ratio=0.3) on GPU 4

----------------------------------------
Task: lsht | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:02:39,910 - INFO - Set deterministic seeds to 42
2025-12-14 23:02:39,910 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:02:39,910 - INFO - Starting evaluation run...
2025-12-14 23:02:39,910 - INFO - Output directory set to: longbenchresult
2025-12-14 23:02:39,910 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 23:02:39,910 - INFO - KV Press 'knorm' setup.
2025-12-14 23:02:39,910 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:02:39,910 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.60it/s]
Device set to use cuda:0
2025-12-14 23:02:54,849 - INFO - Model pipeline loaded.
2025-12-14 23:02:54,849 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 23:03:03,784 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:03:03,784 - INFO - Dataset processed with 200 entries.
2025-12-14 23:03:03,819 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<18:16,  5.51s/it]Running Inference:   1%|          | 2/200 [00:07<12:00,  3.64s/it]Running Inference:   2%|▏         | 3/200 [00:08<07:51,  2.39s/it]Running Inference:   2%|▏         | 4/200 [00:10<06:34,  2.01s/it]Running Inference:   2%|▎         | 5/200 [00:16<11:30,  3.54s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:46,  3.02s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:52,  2.76s/it]Running Inference:   4%|▍         | 8/200 [00:21<07:13,  2.26s/it]Running Inference:   4%|▍         | 9/200 [00:24<07:05,  2.23s/it]Running Inference:   5%|▌         | 10/200 [00:25<06:03,  1.91s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:27<06:04,  1.93s/it]Running Inference:   6%|▌         | 12/200 [00:28<05:37,  1.80s/it]Running Inference:   6%|▋         | 13/200 [00:31<06:06,  1.96s/it]Running Inference:   7%|▋         | 14/200 [00:32<05:28,  1.77s/it]Running Inference:   8%|▊         | 15/200 [00:35<06:30,  2.11s/it]Running Inference:   8%|▊         | 16/200 [00:36<05:41,  1.86s/it]Running Inference:   8%|▊         | 17/200 [00:39<06:44,  2.21s/it]Running Inference:   9%|▉         | 18/200 [00:41<06:45,  2.23s/it]Running Inference:  10%|▉         | 19/200 [00:46<08:58,  2.98s/it]Running Inference:  10%|█         | 20/200 [00:52<11:26,  3.82s/it]Running Inference:  10%|█         | 21/200 [00:54<09:51,  3.30s/it]Running Inference:  11%|█         | 22/200 [00:56<08:25,  2.84s/it]Running Inference:  12%|█▏        | 23/200 [00:57<06:44,  2.29s/it]Running Inference:  12%|█▏        | 24/200 [00:59<07:05,  2.42s/it]Running Inference:  12%|█▎        | 25/200 [01:04<08:41,  2.98s/it]Running Inference:  13%|█▎        | 26/200 [01:07<08:38,  2.98s/it]Running Inference:  14%|█▎        | 27/200 [01:09<08:25,  2.92s/it]Running Inference:  14%|█▍        | 28/200 [01:10<06:45,  2.36s/it]Running Inference:  14%|█▍        | 29/200 [01:12<05:41,  1.99s/it]Running Inference:  15%|█▌        | 30/200 [01:14<06:10,  2.18s/it]Running Inference:  16%|█▌        | 31/200 [01:17<06:27,  2.29s/it]Running Inference:  16%|█▌        | 32/200 [01:20<07:21,  2.63s/it]Running Inference:  16%|█▋        | 33/200 [01:23<07:20,  2.64s/it]Running Inference:  17%|█▋        | 34/200 [01:25<07:00,  2.53s/it]Running Inference:  18%|█▊        | 35/200 [01:29<08:22,  3.04s/it]Running Inference:  18%|█▊        | 36/200 [01:31<07:28,  2.74s/it]Running Inference:  18%|█▊        | 37/200 [01:37<09:47,  3.60s/it]Running Inference:  19%|█▉        | 38/200 [01:39<08:29,  3.15s/it]Running Inference:  20%|█▉        | 39/200 [01:41<07:30,  2.80s/it]Running Inference:  20%|██        | 40/200 [01:44<07:53,  2.96s/it]Running Inference:  20%|██        | 41/200 [01:48<07:58,  3.01s/it]Running Inference:  21%|██        | 42/200 [01:49<07:01,  2.67s/it]Running Inference:  22%|██▏       | 43/200 [01:51<06:02,  2.31s/it]Running Inference:  22%|██▏       | 44/200 [01:52<05:22,  2.07s/it]Running Inference:  22%|██▎       | 45/200 [01:54<04:58,  1.92s/it]Running Inference:  23%|██▎       | 46/200 [02:00<08:17,  3.23s/it]Running Inference:  24%|██▎       | 47/200 [02:02<06:59,  2.74s/it]Running Inference:  24%|██▍       | 48/200 [02:03<05:49,  2.30s/it]Running Inference:  24%|██▍       | 49/200 [02:06<06:05,  2.42s/it]Running Inference:  25%|██▌       | 50/200 [02:07<05:23,  2.15s/it]Running Inference:  26%|██▌       | 51/200 [02:08<04:21,  1.75s/it]Running Inference:  26%|██▌       | 52/200 [02:10<04:08,  1.68s/it]Running Inference:  26%|██▋       | 53/200 [02:11<04:04,  1.66s/it]Running Inference:  27%|██▋       | 54/200 [02:13<04:14,  1.74s/it]Running Inference:  28%|██▊       | 55/200 [02:17<05:19,  2.21s/it]Running Inference:  28%|██▊       | 56/200 [02:19<05:36,  2.34s/it]Running Inference:  28%|██▊       | 57/200 [02:22<05:43,  2.40s/it]Running Inference:  29%|██▉       | 58/200 [02:28<08:12,  3.47s/it]Running Inference:  30%|██▉       | 59/200 [02:32<08:52,  3.78s/it]Running Inference:  30%|███       | 60/200 [02:35<07:54,  3.39s/it]Running Inference:  30%|███       | 61/200 [02:39<08:22,  3.62s/it]Running Inference:  31%|███       | 62/200 [02:42<07:41,  3.34s/it]Running Inference:  32%|███▏      | 63/200 [02:43<06:08,  2.69s/it]Running Inference:  32%|███▏      | 64/200 [02:44<04:57,  2.19s/it]Running Inference:  32%|███▎      | 65/200 [02:48<06:24,  2.85s/it]Running Inference:  33%|███▎      | 66/200 [02:51<06:11,  2.77s/it]Running Inference:  34%|███▎      | 67/200 [02:52<04:57,  2.23s/it]Running Inference:  34%|███▍      | 68/200 [02:53<04:02,  1.83s/it]Running Inference:  34%|███▍      | 69/200 [02:55<04:26,  2.04s/it]Running Inference:  35%|███▌      | 70/200 [02:56<03:46,  1.75s/it]Running Inference:  36%|███▌      | 71/200 [02:57<03:08,  1.46s/it]Running Inference:  36%|███▌      | 72/200 [02:59<03:17,  1.54s/it]Running Inference:  36%|███▋      | 73/200 [03:04<05:55,  2.80s/it]Running Inference:  37%|███▋      | 74/200 [03:08<06:09,  2.94s/it]Running Inference:  38%|███▊      | 75/200 [03:13<07:22,  3.54s/it]Running Inference:  38%|███▊      | 76/200 [03:15<06:35,  3.19s/it]Running Inference:  38%|███▊      | 77/200 [03:20<07:38,  3.73s/it]Running Inference:  39%|███▉      | 78/200 [03:21<06:10,  3.04s/it]Running Inference:  40%|███▉      | 79/200 [03:22<04:51,  2.41s/it]Running Inference:  40%|████      | 80/200 [03:24<04:34,  2.29s/it]Running Inference:  40%|████      | 81/200 [03:26<04:24,  2.22s/it]Running Inference:  41%|████      | 82/200 [03:30<04:54,  2.50s/it]Running Inference:  42%|████▏     | 83/200 [03:31<04:27,  2.29s/it]Running Inference:  42%|████▏     | 84/200 [03:33<04:06,  2.12s/it]Running Inference:  42%|████▎     | 85/200 [03:36<04:22,  2.28s/it]Running Inference:  43%|████▎     | 86/200 [03:37<03:55,  2.07s/it]Running Inference:  44%|████▎     | 87/200 [03:43<05:48,  3.09s/it]Running Inference:  44%|████▍     | 88/200 [03:45<05:08,  2.76s/it]Running Inference:  44%|████▍     | 89/200 [03:46<04:20,  2.35s/it]Running Inference:  45%|████▌     | 90/200 [03:48<04:16,  2.33s/it]Running Inference:  46%|████▌     | 91/200 [03:54<05:51,  3.23s/it]Running Inference:  46%|████▌     | 92/200 [03:59<07:02,  3.91s/it]Running Inference:  46%|████▋     | 93/200 [04:02<06:12,  3.48s/it]Running Inference:  47%|████▋     | 94/200 [04:03<05:00,  2.84s/it]Running Inference:  48%|████▊     | 95/200 [04:06<04:57,  2.84s/it]Running Inference:  48%|████▊     | 96/200 [04:08<04:34,  2.64s/it]Running Inference:  48%|████▊     | 97/200 [04:11<04:51,  2.83s/it]Running Inference:  49%|████▉     | 98/200 [04:13<03:58,  2.34s/it]Running Inference:  50%|████▉     | 99/200 [04:15<03:55,  2.33s/it]Running Inference:  50%|█████     | 100/200 [04:16<03:11,  1.91s/it]Running Inference:  50%|█████     | 101/200 [04:18<03:12,  1.95s/it]Running Inference:  51%|█████     | 102/200 [04:19<02:39,  1.63s/it]Running Inference:  52%|█████▏    | 103/200 [04:22<03:30,  2.17s/it]Running Inference:  52%|█████▏    | 104/200 [04:25<04:00,  2.50s/it]Running Inference:  52%|█████▎    | 105/200 [04:27<03:37,  2.28s/it]Running Inference:  53%|█████▎    | 106/200 [04:31<04:09,  2.66s/it]Running Inference:  54%|█████▎    | 107/200 [04:32<03:25,  2.21s/it]Running Inference:  54%|█████▍    | 108/200 [04:34<03:10,  2.07s/it]Running Inference:  55%|█████▍    | 109/200 [04:35<02:50,  1.87s/it]Running Inference:  55%|█████▌    | 110/200 [04:37<02:51,  1.90s/it]Running Inference:  56%|█████▌    | 111/200 [04:39<02:56,  1.98s/it]Running Inference:  56%|█████▌    | 112/200 [04:41<03:00,  2.05s/it]Running Inference:  56%|█████▋    | 113/200 [04:43<02:47,  1.93s/it]Running Inference:  57%|█████▋    | 114/200 [04:48<03:59,  2.78s/it]Running Inference:  57%|█████▊    | 115/200 [04:50<03:47,  2.68s/it]Running Inference:  58%|█████▊    | 116/200 [04:53<03:47,  2.70s/it]Running Inference:  58%|█████▊    | 117/200 [05:00<05:24,  3.91s/it]Running Inference:  59%|█████▉    | 118/200 [05:01<04:23,  3.22s/it]Running Inference:  60%|█████▉    | 119/200 [05:02<03:28,  2.57s/it]Running Inference:  60%|██████    | 120/200 [05:04<03:06,  2.33s/it]Running Inference:  60%|██████    | 121/200 [05:07<03:10,  2.41s/it]Running Inference:  61%|██████    | 122/200 [05:12<04:11,  3.22s/it]Running Inference:  62%|██████▏   | 123/200 [05:13<03:09,  2.46s/it]Running Inference:  62%|██████▏   | 124/200 [05:15<03:15,  2.57s/it]Running Inference:  62%|██████▎   | 125/200 [05:17<02:57,  2.37s/it]Running Inference:  63%|██████▎   | 126/200 [05:19<02:34,  2.09s/it]Running Inference:  64%|██████▎   | 127/200 [05:20<02:15,  1.86s/it]Running Inference:  64%|██████▍   | 128/200 [05:27<03:59,  3.33s/it]Running Inference:  64%|██████▍   | 129/200 [05:28<03:07,  2.63s/it]Running Inference:  65%|██████▌   | 130/200 [05:30<02:54,  2.49s/it]Running Inference:  66%|██████▌   | 131/200 [05:31<02:25,  2.11s/it]Running Inference:  66%|██████▌   | 132/200 [05:33<02:06,  1.87s/it]Running Inference:  66%|██████▋   | 133/200 [05:34<02:02,  1.82s/it]Running Inference:  67%|██████▋   | 134/200 [05:36<01:50,  1.67s/it]Running Inference:  68%|██████▊   | 135/200 [05:37<01:43,  1.59s/it]Running Inference:  68%|██████▊   | 136/200 [05:42<02:39,  2.49s/it]Running Inference:  68%|██████▊   | 137/200 [05:45<02:56,  2.81s/it]Running Inference:  69%|██████▉   | 138/200 [05:47<02:45,  2.67s/it]Running Inference:  70%|██████▉   | 139/200 [05:51<02:49,  2.78s/it]Running Inference:  70%|███████   | 140/200 [05:52<02:24,  2.40s/it]Running Inference:  70%|███████   | 141/200 [05:55<02:24,  2.45s/it]Running Inference:  71%|███████   | 142/200 [05:56<02:06,  2.19s/it]Running Inference:  72%|███████▏  | 143/200 [05:57<01:43,  1.81s/it]Running Inference:  72%|███████▏  | 144/200 [05:59<01:47,  1.91s/it]Running Inference:  72%|███████▎  | 145/200 [06:02<01:51,  2.02s/it]Running Inference:  73%|███████▎  | 146/200 [06:03<01:41,  1.88s/it]Running Inference:  74%|███████▎  | 147/200 [06:06<01:56,  2.20s/it]Running Inference:  74%|███████▍  | 148/200 [06:08<01:56,  2.25s/it]Running Inference:  74%|███████▍  | 149/200 [06:10<01:39,  1.95s/it]Running Inference:  75%|███████▌  | 150/200 [06:15<02:24,  2.89s/it]Running Inference:  76%|███████▌  | 151/200 [06:17<02:14,  2.74s/it]Running Inference:  76%|███████▌  | 152/200 [06:18<01:45,  2.20s/it]Running Inference:  76%|███████▋  | 153/200 [06:21<01:56,  2.48s/it]Running Inference:  77%|███████▋  | 154/200 [06:22<01:33,  2.04s/it]Running Inference:  78%|███████▊  | 155/200 [06:24<01:26,  1.92s/it]Running Inference:  78%|███████▊  | 156/200 [06:25<01:19,  1.81s/it]Running Inference:  78%|███████▊  | 157/200 [06:26<01:07,  1.57s/it]Running Inference:  79%|███████▉  | 158/200 [06:27<00:56,  1.35s/it]Running Inference:  80%|███████▉  | 159/200 [06:29<00:59,  1.45s/it]Running Inference:  80%|████████  | 160/200 [06:30<00:58,  1.47s/it]Running Inference:  80%|████████  | 161/200 [06:33<01:08,  1.76s/it]Running Inference:  81%|████████  | 162/200 [06:34<00:59,  1.57s/it]Running Inference:  82%|████████▏ | 163/200 [06:36<00:57,  1.56s/it]Running Inference:  82%|████████▏ | 164/200 [06:37<00:55,  1.55s/it]Running Inference:  82%|████████▎ | 165/200 [06:39<00:55,  1.59s/it]Running Inference:  83%|████████▎ | 166/200 [06:40<00:47,  1.39s/it]Running Inference:  84%|████████▎ | 167/200 [06:41<00:45,  1.38s/it]Running Inference:  84%|████████▍ | 168/200 [06:42<00:40,  1.28s/it]Running Inference:  84%|████████▍ | 169/200 [06:45<00:56,  1.81s/it]Running Inference:  85%|████████▌ | 170/200 [06:46<00:45,  1.52s/it]Running Inference:  86%|████████▌ | 171/200 [06:47<00:43,  1.50s/it]Running Inference:  86%|████████▌ | 172/200 [06:49<00:40,  1.45s/it]Running Inference:  86%|████████▋ | 173/200 [06:51<00:46,  1.71s/it]Running Inference:  87%|████████▋ | 174/200 [06:53<00:47,  1.83s/it]Running Inference:  88%|████████▊ | 175/200 [06:55<00:45,  1.82s/it]Running Inference:  88%|████████▊ | 176/200 [06:56<00:41,  1.72s/it]Running Inference:  88%|████████▊ | 177/200 [07:02<01:05,  2.85s/it]Running Inference:  89%|████████▉ | 178/200 [07:04<00:56,  2.57s/it]Running Inference:  90%|████████▉ | 179/200 [07:05<00:45,  2.16s/it]Running Inference:  90%|█████████ | 180/200 [07:10<00:59,  2.96s/it]Running Inference:  90%|█████████ | 181/200 [07:12<00:50,  2.67s/it]Running Inference:  91%|█████████ | 182/200 [07:14<00:46,  2.57s/it]Running Inference:  92%|█████████▏| 183/200 [07:16<00:38,  2.28s/it]Running Inference:  92%|█████████▏| 184/200 [07:17<00:29,  1.83s/it]Running Inference:  92%|█████████▎| 185/200 [07:18<00:24,  1.66s/it]Running Inference:  93%|█████████▎| 186/200 [07:20<00:26,  1.90s/it]Running Inference:  94%|█████████▎| 187/200 [07:23<00:29,  2.24s/it]Running Inference:  94%|█████████▍| 188/200 [07:27<00:32,  2.71s/it]Running Inference:  94%|█████████▍| 189/200 [07:29<00:26,  2.44s/it]Running Inference:  95%|█████████▌| 190/200 [07:31<00:21,  2.19s/it]Running Inference:  96%|█████████▌| 191/200 [07:33<00:20,  2.33s/it]Running Inference:  96%|█████████▌| 192/200 [07:37<00:22,  2.85s/it]Running Inference:  96%|█████████▋| 193/200 [07:40<00:18,  2.67s/it]Running Inference:  97%|█████████▋| 194/200 [07:45<00:21,  3.57s/it]Running Inference:  98%|█████████▊| 195/200 [07:48<00:16,  3.28s/it]Running Inference:  98%|█████████▊| 196/200 [07:51<00:12,  3.19s/it]Running Inference:  98%|█████████▊| 197/200 [07:53<00:08,  2.91s/it]Running Inference:  99%|█████████▉| 198/200 [07:58<00:06,  3.47s/it]Running Inference: 100%|█████████▉| 199/200 [08:01<00:03,  3.28s/it]Running Inference: 100%|██████████| 200/200 [08:02<00:00,  2.75s/it]Running Inference: 100%|██████████| 200/200 [08:02<00:00,  2.41s/it]
2025-12-14 23:11:06,521 - INFO - Inference completed.
2025-12-14 23:11:06,539 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 23:11:06,540 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:11:06,541 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 23:11:06,541 - INFO - Metrics:
19.25
2025-12-14 23:11:06,542 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=lsht, ratio=0.5) on GPU 4


========================================
LongBench Task: multifieldqa_zh
========================================
----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:11:13,025 - INFO - Set deterministic seeds to 42
2025-12-14 23:11:13,025 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:11:13,025 - INFO - Starting evaluation run...
2025-12-14 23:11:13,025 - INFO - Output directory set to: longbenchresult
2025-12-14 23:11:13,025 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 23:11:13,025 - INFO - KV Press 'knorm' setup.
2025-12-14 23:11:13,025 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:11:13,025 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.67it/s]
Device set to use cuda:0
2025-12-14 23:11:26,238 - INFO - Model pipeline loaded.
2025-12-14 23:11:26,239 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 23:11:31,373 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:11:31,373 - INFO - Dataset processed with 200 entries.
2025-12-14 23:11:31,383 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:03<07:51,  3.34s/it]Running Inference:   1%|▏         | 2/142 [00:11<14:11,  6.08s/it]Running Inference:   2%|▏         | 3/142 [00:13<09:39,  4.17s/it]Running Inference:   3%|▎         | 4/142 [00:13<06:21,  2.77s/it]Running Inference:   4%|▎         | 5/142 [00:17<07:21,  3.22s/it]Running Inference:   4%|▍         | 6/142 [00:18<05:15,  2.32s/it]Running Inference:   5%|▍         | 7/142 [00:19<04:30,  2.01s/it]Running Inference:   6%|▌         | 8/142 [00:21<04:04,  1.82s/it]Running Inference:   6%|▋         | 9/142 [00:25<05:45,  2.60s/it]Running Inference:   7%|▋         | 10/142 [00:31<08:07,  3.70s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:33<06:54,  3.16s/it]Running Inference:   8%|▊         | 12/142 [00:34<05:29,  2.53s/it]Running Inference:   9%|▉         | 13/142 [00:36<04:40,  2.17s/it]Running Inference:  10%|▉         | 14/142 [00:40<06:02,  2.83s/it]Running Inference:  11%|█         | 15/142 [00:41<04:42,  2.22s/it]Running Inference:  11%|█▏        | 16/142 [00:46<06:25,  3.06s/it]Running Inference:  12%|█▏        | 17/142 [00:50<07:25,  3.56s/it]Running Inference:  13%|█▎        | 18/142 [00:54<07:30,  3.63s/it]Running Inference:  13%|█▎        | 19/142 [00:59<08:05,  3.95s/it]Running Inference:  14%|█▍        | 20/142 [01:00<06:03,  2.98s/it]Running Inference:  15%|█▍        | 21/142 [01:05<07:18,  3.63s/it]Running Inference:  15%|█▌        | 22/142 [01:06<05:51,  2.93s/it]Running Inference:  16%|█▌        | 23/142 [01:07<04:26,  2.24s/it]Running Inference:  17%|█▋        | 24/142 [01:11<05:22,  2.74s/it]Running Inference:  18%|█▊        | 25/142 [01:11<04:13,  2.17s/it]Running Inference:  18%|█▊        | 26/142 [01:16<05:20,  2.76s/it]Running Inference:  19%|█▉        | 27/142 [01:19<05:49,  3.04s/it]Running Inference:  20%|█▉        | 28/142 [01:24<06:37,  3.49s/it]Running Inference:  20%|██        | 29/142 [01:26<05:42,  3.03s/it]Running Inference:  21%|██        | 30/142 [01:30<06:12,  3.32s/it]Running Inference:  22%|██▏       | 31/142 [01:32<05:14,  2.84s/it]Running Inference:  23%|██▎       | 32/142 [01:36<05:57,  3.25s/it]Running Inference:  23%|██▎       | 33/142 [01:36<04:22,  2.40s/it]Running Inference:  24%|██▍       | 34/142 [01:37<03:41,  2.05s/it]Running Inference:  25%|██▍       | 35/142 [01:41<04:44,  2.66s/it]Running Inference:  25%|██▌       | 36/142 [01:42<03:47,  2.15s/it]Running Inference:  26%|██▌       | 37/142 [01:44<03:14,  1.85s/it]Running Inference:  27%|██▋       | 38/142 [01:49<05:05,  2.94s/it]Running Inference:  27%|██▋       | 39/142 [01:53<05:40,  3.30s/it]Running Inference:  28%|██▊       | 40/142 [01:57<05:52,  3.46s/it]Running Inference:  29%|██▉       | 41/142 [01:58<04:33,  2.71s/it]Running Inference:  30%|██▉       | 42/142 [01:59<03:50,  2.31s/it]Running Inference:  30%|███       | 43/142 [02:03<04:35,  2.78s/it]Running Inference:  31%|███       | 44/142 [02:07<05:07,  3.14s/it]Running Inference:  32%|███▏      | 45/142 [02:11<05:31,  3.42s/it]Running Inference:  32%|███▏      | 46/142 [02:12<04:11,  2.62s/it]Running Inference:  33%|███▎      | 47/142 [02:16<04:48,  3.03s/it]Running Inference:  34%|███▍      | 48/142 [02:17<03:37,  2.31s/it]Running Inference:  35%|███▍      | 49/142 [02:17<02:50,  1.84s/it]Running Inference:  35%|███▌      | 50/142 [02:22<03:58,  2.59s/it]Running Inference:  36%|███▌      | 51/142 [02:26<04:31,  2.98s/it]Running Inference:  37%|███▋      | 52/142 [02:27<03:39,  2.44s/it]Running Inference:  37%|███▋      | 53/142 [02:28<02:50,  1.91s/it]Running Inference:  38%|███▊      | 54/142 [02:28<02:19,  1.59s/it]Running Inference:  39%|███▊      | 55/142 [02:30<02:27,  1.69s/it]Running Inference:  39%|███▉      | 56/142 [02:35<03:31,  2.46s/it]Running Inference:  40%|████      | 57/142 [02:38<04:03,  2.87s/it]Running Inference:  41%|████      | 58/142 [02:39<03:05,  2.20s/it]Running Inference:  42%|████▏     | 59/142 [02:40<02:43,  1.98s/it]Running Inference:  42%|████▏     | 60/142 [02:41<02:16,  1.66s/it]Running Inference:  43%|████▎     | 61/142 [02:46<03:16,  2.43s/it]Running Inference:  44%|████▎     | 62/142 [02:50<03:49,  2.87s/it]Running Inference:  44%|████▍     | 63/142 [02:51<03:21,  2.55s/it]Running Inference:  45%|████▌     | 64/142 [02:56<04:13,  3.24s/it]Running Inference:  46%|████▌     | 65/142 [03:00<04:26,  3.46s/it]Running Inference:  46%|████▋     | 66/142 [03:05<04:57,  3.91s/it]Running Inference:  47%|████▋     | 67/142 [03:07<03:57,  3.17s/it]Running Inference:  48%|████▊     | 68/142 [03:07<02:56,  2.38s/it]Running Inference:  49%|████▊     | 69/142 [03:12<03:57,  3.26s/it]Running Inference:  49%|████▉     | 70/142 [03:14<03:27,  2.89s/it]Running Inference:  50%|█████     | 71/142 [03:15<02:40,  2.25s/it]Running Inference:  51%|█████     | 72/142 [03:19<03:11,  2.74s/it]Running Inference:  51%|█████▏    | 73/142 [03:20<02:36,  2.26s/it]Running Inference:  52%|█████▏    | 74/142 [03:22<02:19,  2.06s/it]Running Inference:  53%|█████▎    | 75/142 [03:26<02:53,  2.58s/it]Running Inference:  54%|█████▎    | 76/142 [03:27<02:20,  2.12s/it]Running Inference:  54%|█████▍    | 77/142 [03:28<02:00,  1.86s/it]Running Inference:  55%|█████▍    | 78/142 [03:32<02:37,  2.46s/it]Running Inference:  56%|█████▌    | 79/142 [03:36<03:16,  3.12s/it]Running Inference:  56%|█████▋    | 80/142 [03:38<02:35,  2.52s/it]Running Inference:  57%|█████▋    | 81/142 [03:39<02:14,  2.20s/it]Running Inference:  58%|█████▊    | 82/142 [03:43<02:41,  2.69s/it]Running Inference:  58%|█████▊    | 83/142 [03:50<04:03,  4.12s/it]Running Inference:  59%|█████▉    | 84/142 [03:51<03:04,  3.18s/it]Running Inference:  60%|█████▉    | 85/142 [03:52<02:17,  2.42s/it]Running Inference:  61%|██████    | 86/142 [03:56<02:38,  2.83s/it]Running Inference:  61%|██████▏   | 87/142 [03:56<02:02,  2.22s/it]Running Inference:  62%|██████▏   | 88/142 [03:58<01:42,  1.90s/it]Running Inference:  63%|██████▎   | 89/142 [03:59<01:32,  1.74s/it]Running Inference:  63%|██████▎   | 90/142 [04:01<01:29,  1.72s/it]Running Inference:  64%|██████▍   | 91/142 [04:05<02:11,  2.57s/it]Running Inference:  65%|██████▍   | 92/142 [04:06<01:44,  2.09s/it]Running Inference:  65%|██████▌   | 93/142 [04:11<02:20,  2.87s/it]Running Inference:  66%|██████▌   | 94/142 [04:12<01:50,  2.31s/it]Running Inference:  67%|██████▋   | 95/142 [04:14<01:43,  2.20s/it]Running Inference:  68%|██████▊   | 96/142 [04:18<02:11,  2.87s/it]Running Inference:  68%|██████▊   | 97/142 [04:19<01:39,  2.20s/it]Running Inference:  69%|██████▉   | 98/142 [04:20<01:17,  1.76s/it]Running Inference:  70%|██████▉   | 99/142 [04:24<01:48,  2.52s/it]Running Inference:  70%|███████   | 100/142 [04:28<02:04,  2.95s/it]Running Inference:  71%|███████   | 101/142 [04:29<01:33,  2.29s/it]Running Inference:  72%|███████▏  | 102/142 [04:30<01:17,  1.94s/it]Running Inference:  73%|███████▎  | 103/142 [04:34<01:47,  2.76s/it]Running Inference:  73%|███████▎  | 104/142 [04:39<02:02,  3.21s/it]Running Inference:  74%|███████▍  | 105/142 [04:43<02:07,  3.45s/it]Running Inference:  75%|███████▍  | 106/142 [04:55<03:35,  5.97s/it]Running Inference:  75%|███████▌  | 107/142 [04:59<03:13,  5.54s/it]Running Inference:  76%|███████▌  | 108/142 [05:00<02:20,  4.12s/it]Running Inference:  77%|███████▋  | 109/142 [05:04<02:15,  4.10s/it]Running Inference:  77%|███████▋  | 110/142 [05:06<01:51,  3.49s/it]Running Inference:  78%|███████▊  | 111/142 [05:07<01:25,  2.77s/it]Running Inference:  79%|███████▉  | 112/142 [05:11<01:31,  3.07s/it]Running Inference:  80%|███████▉  | 113/142 [05:12<01:07,  2.34s/it]Running Inference:  80%|████████  | 114/142 [05:12<00:53,  1.93s/it]Running Inference:  81%|████████  | 115/142 [05:13<00:41,  1.55s/it]Running Inference:  82%|████████▏ | 116/142 [05:14<00:33,  1.27s/it]Running Inference:  82%|████████▏ | 117/142 [05:15<00:31,  1.25s/it]Running Inference:  83%|████████▎ | 118/142 [05:16<00:29,  1.24s/it]Running Inference:  84%|████████▍ | 119/142 [05:17<00:22,  1.03it/s]Running Inference:  85%|████████▍ | 120/142 [05:18<00:27,  1.24s/it]Running Inference:  85%|████████▌ | 121/142 [05:20<00:29,  1.42s/it]Running Inference:  86%|████████▌ | 122/142 [05:21<00:26,  1.33s/it]Running Inference:  87%|████████▋ | 123/142 [05:25<00:39,  2.08s/it]Running Inference:  87%|████████▋ | 124/142 [05:26<00:29,  1.63s/it]Running Inference:  88%|████████▊ | 125/142 [05:27<00:24,  1.44s/it]Running Inference:  89%|████████▊ | 126/142 [05:31<00:36,  2.25s/it]Running Inference:  89%|████████▉ | 127/142 [05:35<00:42,  2.83s/it]Running Inference:  90%|█████████ | 128/142 [05:35<00:29,  2.09s/it]Running Inference:  91%|█████████ | 129/142 [05:36<00:21,  1.63s/it]Running Inference:  92%|█████████▏| 130/142 [05:37<00:18,  1.56s/it]Running Inference:  92%|█████████▏| 131/142 [05:38<00:14,  1.35s/it]Running Inference:  93%|█████████▎| 132/142 [05:39<00:11,  1.11s/it]Running Inference:  94%|█████████▎| 133/142 [05:41<00:14,  1.57s/it]Running Inference:  94%|█████████▍| 134/142 [05:43<00:11,  1.44s/it]Running Inference:  95%|█████████▌| 135/142 [05:46<00:15,  2.17s/it]Running Inference:  96%|█████████▌| 136/142 [05:51<00:17,  2.96s/it]Running Inference:  96%|█████████▋| 137/142 [05:53<00:12,  2.48s/it]Running Inference:  97%|█████████▋| 138/142 [05:53<00:07,  1.87s/it]Running Inference:  98%|█████████▊| 139/142 [05:54<00:05,  1.69s/it]Running Inference:  99%|█████████▊| 140/142 [05:59<00:04,  2.45s/it]Running Inference:  99%|█████████▉| 141/142 [06:00<00:02,  2.12s/it]Running Inference: 100%|██████████| 142/142 [06:04<00:00,  2.71s/it]Running Inference: 100%|██████████| 142/142 [06:04<00:00,  2.57s/it]
2025-12-14 23:17:35,909 - INFO - Inference completed.
2025-12-14 23:17:35,917 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 23:17:35,917 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.662 seconds.
Prefix dict has been built successfully.
2025-12-14 23:17:36,657 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 23:17:36,657 - INFO - Metrics:
28.91
2025-12-14 23:17:36,658 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_zh, ratio=0.1) on GPU 4

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:17:43,199 - INFO - Set deterministic seeds to 42
2025-12-14 23:17:43,199 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:17:43,199 - INFO - Starting evaluation run...
2025-12-14 23:17:43,199 - INFO - Output directory set to: longbenchresult
2025-12-14 23:17:43,199 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 23:17:43,199 - INFO - KV Press 'knorm' setup.
2025-12-14 23:17:43,199 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:17:43,199 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.62it/s]
Device set to use cuda:0
2025-12-14 23:17:55,174 - INFO - Model pipeline loaded.
2025-12-14 23:17:55,174 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 23:18:03,073 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:18:03,073 - INFO - Dataset processed with 200 entries.
2025-12-14 23:18:03,083 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:03<07:44,  3.29s/it]Running Inference:   1%|▏         | 2/142 [00:11<14:07,  6.06s/it]Running Inference:   2%|▏         | 3/142 [00:12<09:07,  3.94s/it]Running Inference:   3%|▎         | 4/142 [00:16<09:08,  3.98s/it]Running Inference:   4%|▎         | 5/142 [00:20<09:07,  3.99s/it]Running Inference:   4%|▍         | 6/142 [00:21<06:19,  2.79s/it]Running Inference:   5%|▍         | 7/142 [00:22<05:09,  2.29s/it]Running Inference:   6%|▌         | 8/142 [00:27<07:10,  3.22s/it]Running Inference:   6%|▋         | 9/142 [00:35<10:04,  4.55s/it]Running Inference:   7%|▋         | 10/142 [00:43<12:27,  5.67s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:45<09:55,  4.54s/it]Running Inference:   8%|▊         | 12/142 [00:49<09:34,  4.42s/it]Running Inference:   9%|▉         | 13/142 [00:50<07:29,  3.49s/it]Running Inference:  10%|▉         | 14/142 [00:52<06:22,  2.99s/it]Running Inference:  11%|█         | 15/142 [00:53<05:08,  2.43s/it]Running Inference:  11%|█▏        | 16/142 [00:58<06:18,  3.00s/it]Running Inference:  12%|█▏        | 17/142 [01:02<07:26,  3.57s/it]Running Inference:  13%|█▎        | 18/142 [01:06<07:30,  3.64s/it]Running Inference:  13%|█▎        | 19/142 [01:11<08:05,  3.95s/it]Running Inference:  14%|█▍        | 20/142 [01:12<06:08,  3.02s/it]Running Inference:  15%|█▍        | 21/142 [01:17<07:41,  3.81s/it]Running Inference:  15%|█▌        | 22/142 [01:22<07:51,  3.93s/it]Running Inference:  16%|█▌        | 23/142 [01:22<05:49,  2.94s/it]Running Inference:  17%|█▋        | 24/142 [01:25<05:40,  2.88s/it]Running Inference:  18%|█▊        | 25/142 [01:29<06:05,  3.13s/it]Running Inference:  18%|█▊        | 26/142 [01:33<06:38,  3.44s/it]Running Inference:  19%|█▉        | 27/142 [01:37<06:43,  3.51s/it]Running Inference:  20%|█▉        | 28/142 [01:41<07:14,  3.81s/it]Running Inference:  20%|██        | 29/142 [01:43<06:09,  3.27s/it]Running Inference:  21%|██        | 30/142 [01:47<06:31,  3.49s/it]Running Inference:  22%|██▏       | 31/142 [01:48<05:08,  2.78s/it]Running Inference:  23%|██▎       | 32/142 [01:50<04:32,  2.48s/it]Running Inference:  23%|██▎       | 33/142 [01:50<03:22,  1.86s/it]Running Inference:  24%|██▍       | 34/142 [01:54<04:26,  2.47s/it]Running Inference:  25%|██▍       | 35/142 [01:56<03:57,  2.22s/it]Running Inference:  25%|██▌       | 36/142 [01:57<03:17,  1.87s/it]Running Inference:  26%|██▌       | 37/142 [01:58<02:53,  1.66s/it]Running Inference:  27%|██▋       | 38/142 [02:06<06:18,  3.64s/it]Running Inference:  27%|██▋       | 39/142 [02:11<06:31,  3.80s/it]Running Inference:  28%|██▊       | 40/142 [02:14<06:28,  3.80s/it]Running Inference:  29%|██▉       | 41/142 [02:16<05:05,  3.03s/it]Running Inference:  30%|██▉       | 42/142 [02:17<04:17,  2.57s/it]Running Inference:  30%|███       | 43/142 [02:21<04:54,  2.97s/it]Running Inference:  31%|███       | 44/142 [02:23<04:24,  2.70s/it]Running Inference:  32%|███▏      | 45/142 [02:27<05:09,  3.19s/it]Running Inference:  32%|███▏      | 46/142 [02:28<03:56,  2.46s/it]Running Inference:  33%|███▎      | 47/142 [02:29<03:12,  2.03s/it]Running Inference:  34%|███▍      | 48/142 [02:30<02:30,  1.60s/it]Running Inference:  35%|███▍      | 49/142 [02:31<02:04,  1.34s/it]Running Inference:  35%|███▌      | 50/142 [02:32<02:11,  1.43s/it]Running Inference:  36%|███▌      | 51/142 [02:36<03:18,  2.18s/it]Running Inference:  37%|███▋      | 52/142 [02:37<02:49,  1.88s/it]Running Inference:  37%|███▋      | 53/142 [02:38<02:22,  1.60s/it]Running Inference:  38%|███▊      | 54/142 [02:39<02:00,  1.37s/it]Running Inference:  39%|███▊      | 55/142 [02:41<02:21,  1.63s/it]Running Inference:  39%|███▉      | 56/142 [02:43<02:19,  1.62s/it]Running Inference:  40%|████      | 57/142 [02:47<03:14,  2.28s/it]Running Inference:  41%|████      | 58/142 [02:51<03:52,  2.77s/it]Running Inference:  42%|████▏     | 59/142 [02:52<03:20,  2.41s/it]Running Inference:  42%|████▏     | 60/142 [02:53<02:41,  1.97s/it]Running Inference:  43%|████▎     | 61/142 [02:57<03:34,  2.65s/it]Running Inference:  44%|████▎     | 62/142 [03:01<04:01,  3.02s/it]Running Inference:  44%|████▍     | 63/142 [03:06<04:44,  3.60s/it]Running Inference:  45%|████▌     | 64/142 [03:14<06:29,  4.99s/it]Running Inference:  46%|████▌     | 65/142 [03:15<04:46,  3.72s/it]Running Inference:  46%|████▋     | 66/142 [03:20<05:13,  4.12s/it]Running Inference:  47%|████▋     | 67/142 [03:22<04:08,  3.31s/it]Running Inference:  48%|████▊     | 68/142 [03:22<03:03,  2.49s/it]Running Inference:  49%|████▊     | 69/142 [03:28<04:06,  3.38s/it]Running Inference:  49%|████▉     | 70/142 [03:30<03:34,  2.98s/it]Running Inference:  50%|█████     | 71/142 [03:31<02:47,  2.35s/it]Running Inference:  51%|█████     | 72/142 [03:35<03:16,  2.81s/it]Running Inference:  51%|█████▏    | 73/142 [03:36<02:39,  2.31s/it]Running Inference:  52%|█████▏    | 74/142 [03:37<02:22,  2.09s/it]Running Inference:  53%|█████▎    | 75/142 [03:41<02:54,  2.61s/it]Running Inference:  54%|█████▎    | 76/142 [03:45<03:15,  2.96s/it]Running Inference:  54%|█████▍    | 77/142 [03:46<02:31,  2.33s/it]Running Inference:  55%|█████▍    | 78/142 [03:50<02:58,  2.79s/it]Running Inference:  56%|█████▌    | 79/142 [03:54<03:30,  3.35s/it]Running Inference:  56%|█████▋    | 80/142 [03:55<02:42,  2.62s/it]Running Inference:  57%|█████▋    | 81/142 [03:57<02:17,  2.26s/it]Running Inference:  58%|█████▊    | 82/142 [04:00<02:43,  2.73s/it]Running Inference:  58%|█████▊    | 83/142 [04:05<03:05,  3.15s/it]Running Inference:  59%|█████▉    | 84/142 [04:09<03:23,  3.51s/it]Running Inference:  60%|█████▉    | 85/142 [04:10<02:31,  2.65s/it]Running Inference:  61%|██████    | 86/142 [04:13<02:47,  3.00s/it]Running Inference:  61%|██████▏   | 87/142 [04:14<02:08,  2.34s/it]Running Inference:  62%|██████▏   | 88/142 [04:15<01:46,  1.97s/it]Running Inference:  63%|██████▎   | 89/142 [04:20<02:24,  2.73s/it]Running Inference:  63%|██████▎   | 90/142 [04:22<02:11,  2.53s/it]Running Inference:  64%|██████▍   | 91/142 [04:26<02:40,  3.15s/it]Running Inference:  65%|██████▍   | 92/142 [04:31<02:51,  3.44s/it]Running Inference:  65%|██████▌   | 93/142 [04:35<03:07,  3.84s/it]Running Inference:  66%|██████▌   | 94/142 [04:36<02:23,  2.98s/it]Running Inference:  67%|██████▋   | 95/142 [04:42<02:55,  3.74s/it]Running Inference:  68%|██████▊   | 96/142 [04:46<03:01,  3.95s/it]Running Inference:  68%|██████▊   | 97/142 [04:47<02:13,  2.96s/it]Running Inference:  69%|██████▉   | 98/142 [04:48<01:45,  2.39s/it]Running Inference:  70%|██████▉   | 99/142 [04:52<02:08,  2.99s/it]Running Inference:  70%|███████   | 100/142 [04:56<02:19,  3.31s/it]Running Inference:  71%|███████   | 101/142 [04:57<01:44,  2.54s/it]Running Inference:  72%|███████▏  | 102/142 [04:58<01:24,  2.12s/it]Running Inference:  73%|███████▎  | 103/142 [05:03<01:55,  2.97s/it]Running Inference:  73%|███████▎  | 104/142 [05:08<02:07,  3.36s/it]Running Inference:  74%|███████▍  | 105/142 [05:12<02:11,  3.56s/it]Running Inference:  75%|███████▍  | 106/142 [05:27<04:12,  7.00s/it]Running Inference:  75%|███████▌  | 107/142 [05:31<03:39,  6.28s/it]Running Inference:  76%|███████▌  | 108/142 [05:32<02:39,  4.68s/it]Running Inference:  77%|███████▋  | 109/142 [05:36<02:28,  4.49s/it]Running Inference:  77%|███████▋  | 110/142 [05:38<01:57,  3.66s/it]Running Inference:  78%|███████▊  | 111/142 [05:42<01:54,  3.69s/it]Running Inference:  79%|███████▉  | 112/142 [05:44<01:37,  3.25s/it]Running Inference:  80%|███████▉  | 113/142 [05:45<01:12,  2.51s/it]Running Inference:  80%|████████  | 114/142 [05:46<00:57,  2.05s/it]Running Inference:  81%|████████  | 115/142 [05:46<00:44,  1.64s/it]Running Inference:  82%|████████▏ | 116/142 [05:47<00:36,  1.39s/it]Running Inference:  82%|████████▏ | 117/142 [05:48<00:33,  1.32s/it]Running Inference:  83%|████████▎ | 118/142 [05:50<00:31,  1.29s/it]Running Inference:  84%|████████▍ | 119/142 [05:50<00:23,  1.01s/it]Running Inference:  85%|████████▍ | 120/142 [05:52<00:27,  1.27s/it]Running Inference:  85%|████████▌ | 121/142 [05:54<00:30,  1.44s/it]Running Inference:  86%|████████▌ | 122/142 [05:55<00:26,  1.33s/it]Running Inference:  87%|████████▋ | 123/142 [05:58<00:39,  2.09s/it]Running Inference:  87%|████████▋ | 124/142 [05:59<00:29,  1.64s/it]Running Inference:  88%|████████▊ | 125/142 [06:00<00:24,  1.44s/it]Running Inference:  89%|████████▊ | 126/142 [06:04<00:36,  2.26s/it]Running Inference:  89%|████████▉ | 127/142 [06:05<00:27,  1.86s/it]Running Inference:  90%|█████████ | 128/142 [06:09<00:34,  2.44s/it]Running Inference:  91%|█████████ | 129/142 [06:13<00:37,  2.92s/it]Running Inference:  92%|█████████▏| 130/142 [06:14<00:29,  2.46s/it]Running Inference:  92%|█████████▏| 131/142 [06:15<00:21,  1.97s/it]Running Inference:  93%|█████████▎| 132/142 [06:19<00:25,  2.56s/it]Running Inference:  94%|█████████▎| 133/142 [06:27<00:37,  4.21s/it]Running Inference:  94%|█████████▍| 134/142 [06:28<00:26,  3.29s/it]Running Inference:  95%|█████████▌| 135/142 [06:29<00:18,  2.61s/it]Running Inference:  96%|█████████▌| 136/142 [06:34<00:19,  3.29s/it]Running Inference:  96%|█████████▋| 137/142 [06:36<00:13,  2.71s/it]Running Inference:  97%|█████████▋| 138/142 [06:37<00:08,  2.16s/it]Running Inference:  98%|█████████▊| 139/142 [06:38<00:05,  1.89s/it]Running Inference:  99%|█████████▊| 140/142 [06:42<00:05,  2.59s/it]Running Inference:  99%|█████████▉| 141/142 [06:43<00:02,  2.25s/it]Running Inference: 100%|██████████| 142/142 [06:48<00:00,  2.80s/it]Running Inference: 100%|██████████| 142/142 [06:48<00:00,  2.87s/it]
2025-12-14 23:24:51,114 - INFO - Inference completed.
2025-12-14 23:24:51,122 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.20/predictions.csv
2025-12-14 23:24:51,122 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.674 seconds.
Prefix dict has been built successfully.
2025-12-14 23:24:51,886 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.20/metrics.json
2025-12-14 23:24:51,886 - INFO - Metrics:
25.01
2025-12-14 23:24:51,887 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_zh, ratio=0.2) on GPU 4

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:24:58,426 - INFO - Set deterministic seeds to 42
2025-12-14 23:24:58,426 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:24:58,426 - INFO - Starting evaluation run...
2025-12-14 23:24:58,426 - INFO - Output directory set to: longbenchresult
2025-12-14 23:24:58,426 - INFO - Set KnormPress compression_ratio to 0.3
2025-12-14 23:24:58,426 - INFO - KV Press 'knorm' setup.
2025-12-14 23:24:58,426 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:24:58,426 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.33it/s]
Device set to use cuda:0
2025-12-14 23:25:12,396 - INFO - Model pipeline loaded.
2025-12-14 23:25:12,396 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 23:25:18,831 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:25:18,831 - INFO - Dataset processed with 200 entries.
2025-12-14 23:25:18,841 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:03<07:48,  3.32s/it]Running Inference:   1%|▏         | 2/142 [00:11<14:10,  6.07s/it]Running Inference:   2%|▏         | 3/142 [00:12<09:08,  3.95s/it]Running Inference:   3%|▎         | 4/142 [00:13<06:34,  2.86s/it]Running Inference:   4%|▎         | 5/142 [00:17<07:28,  3.27s/it]Running Inference:   4%|▍         | 6/142 [00:21<07:51,  3.47s/it]Running Inference:   5%|▍         | 7/142 [00:23<06:11,  2.75s/it]Running Inference:   6%|▌         | 8/142 [00:28<07:47,  3.49s/it]Running Inference:   6%|▋         | 9/142 [00:35<10:28,  4.72s/it]Running Inference:   7%|▋         | 10/142 [00:43<12:43,  5.78s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:48<11:48,  5.41s/it]Running Inference:   8%|▊         | 12/142 [00:49<08:52,  4.09s/it]Running Inference:   9%|▉         | 13/142 [00:51<07:23,  3.44s/it]Running Inference:  10%|▉         | 14/142 [00:56<08:30,  3.99s/it]Running Inference:  11%|█         | 15/142 [00:57<06:36,  3.12s/it]Running Inference:  11%|█▏        | 16/142 [01:02<07:19,  3.49s/it]Running Inference:  12%|█▏        | 17/142 [01:10<10:06,  4.85s/it]Running Inference:  13%|█▎        | 18/142 [01:13<09:22,  4.53s/it]Running Inference:  13%|█▎        | 19/142 [01:18<09:22,  4.58s/it]Running Inference:  14%|█▍        | 20/142 [01:19<06:58,  3.43s/it]Running Inference:  15%|█▍        | 21/142 [01:31<12:00,  5.95s/it]Running Inference:  15%|█▌        | 22/142 [01:35<10:51,  5.43s/it]Running Inference:  16%|█▌        | 23/142 [01:35<07:54,  3.99s/it]Running Inference:  17%|█▋        | 24/142 [01:39<07:47,  3.96s/it]Running Inference:  18%|█▊        | 25/142 [01:43<07:34,  3.88s/it]Running Inference:  18%|█▊        | 26/142 [01:47<07:39,  3.96s/it]Running Inference:  19%|█▉        | 27/142 [01:51<07:26,  3.88s/it]Running Inference:  20%|█▉        | 28/142 [01:53<06:12,  3.27s/it]Running Inference:  20%|██        | 29/142 [01:55<05:26,  2.89s/it]Running Inference:  21%|██        | 30/142 [01:59<06:00,  3.22s/it]Running Inference:  22%|██▏       | 31/142 [02:00<05:07,  2.77s/it]Running Inference:  23%|██▎       | 32/142 [02:05<05:52,  3.20s/it]Running Inference:  23%|██▎       | 33/142 [02:05<04:27,  2.45s/it]Running Inference:  24%|██▍       | 34/142 [02:09<05:11,  2.88s/it]Running Inference:  25%|██▍       | 35/142 [02:13<05:46,  3.24s/it]Running Inference:  25%|██▌       | 36/142 [02:14<04:33,  2.58s/it]Running Inference:  26%|██▌       | 37/142 [02:16<03:49,  2.18s/it]Running Inference:  27%|██▋       | 38/142 [02:24<06:56,  4.00s/it]Running Inference:  27%|██▋       | 39/142 [02:28<06:56,  4.05s/it]Running Inference:  28%|██▊       | 40/142 [02:32<06:46,  3.99s/it]Running Inference:  29%|██▉       | 41/142 [02:36<06:46,  4.02s/it]Running Inference:  30%|██▉       | 42/142 [02:41<07:05,  4.25s/it]Running Inference:  30%|███       | 43/142 [02:45<06:48,  4.13s/it]Running Inference:  31%|███       | 44/142 [02:48<06:37,  4.06s/it]Running Inference:  32%|███▏      | 45/142 [02:53<06:40,  4.13s/it]Running Inference:  32%|███▏      | 46/142 [02:54<04:59,  3.12s/it]Running Inference:  33%|███▎      | 47/142 [02:58<05:22,  3.39s/it]Running Inference:  34%|███▍      | 48/142 [02:58<04:01,  2.57s/it]Running Inference:  35%|███▍      | 49/142 [03:02<04:34,  2.95s/it]Running Inference:  35%|███▌      | 50/142 [03:04<03:55,  2.56s/it]Running Inference:  36%|███▌      | 51/142 [03:08<04:30,  2.97s/it]Running Inference:  37%|███▋      | 52/142 [03:12<05:05,  3.40s/it]Running Inference:  37%|███▋      | 53/142 [03:13<03:56,  2.66s/it]Running Inference:  38%|███▊      | 54/142 [03:14<03:05,  2.11s/it]Running Inference:  39%|███▊      | 55/142 [03:18<03:59,  2.75s/it]Running Inference:  39%|███▉      | 56/142 [03:22<04:34,  3.20s/it]Running Inference:  40%|████      | 57/142 [03:26<04:47,  3.38s/it]Running Inference:  41%|████      | 58/142 [03:30<04:56,  3.53s/it]Running Inference:  42%|████▏     | 59/142 [03:31<04:03,  2.93s/it]Running Inference:  42%|████▏     | 60/142 [03:32<03:11,  2.34s/it]Running Inference:  43%|████▎     | 61/142 [03:37<03:54,  2.90s/it]Running Inference:  44%|████▎     | 62/142 [03:41<04:15,  3.20s/it]Running Inference:  44%|████▍     | 63/142 [03:43<03:45,  2.86s/it]Running Inference:  45%|████▌     | 64/142 [03:48<04:43,  3.64s/it]Running Inference:  46%|████▌     | 65/142 [03:52<04:47,  3.73s/it]Running Inference:  46%|████▋     | 66/142 [03:57<05:13,  4.12s/it]Running Inference:  47%|████▋     | 67/142 [04:01<05:07,  4.10s/it]Running Inference:  48%|████▊     | 68/142 [04:02<03:44,  3.04s/it]Running Inference:  49%|████▊     | 69/142 [04:09<05:18,  4.36s/it]Running Inference:  49%|████▉     | 70/142 [04:11<04:25,  3.68s/it]Running Inference:  50%|█████     | 71/142 [04:12<03:22,  2.85s/it]Running Inference:  51%|█████     | 72/142 [04:16<03:40,  3.15s/it]Running Inference:  51%|█████▏    | 73/142 [04:17<02:55,  2.55s/it]Running Inference:  52%|█████▏    | 74/142 [04:19<02:33,  2.25s/it]Running Inference:  53%|█████▎    | 75/142 [04:22<03:02,  2.72s/it]Running Inference:  54%|█████▎    | 76/142 [04:26<03:20,  3.04s/it]Running Inference:  54%|█████▍    | 77/142 [04:27<02:34,  2.38s/it]Running Inference:  55%|█████▍    | 78/142 [04:28<01:59,  1.87s/it]Running Inference:  56%|█████▌    | 79/142 [04:32<02:49,  2.69s/it]Running Inference:  56%|█████▋    | 80/142 [04:33<02:16,  2.20s/it]Running Inference:  57%|█████▋    | 81/142 [04:35<01:59,  1.96s/it]Running Inference:  58%|█████▊    | 82/142 [04:39<02:31,  2.52s/it]Running Inference:  58%|█████▊    | 83/142 [04:46<03:55,  3.99s/it]Running Inference:  59%|█████▉    | 84/142 [04:50<03:57,  4.09s/it]Running Inference:  60%|█████▉    | 85/142 [04:51<02:54,  3.07s/it]Running Inference:  61%|██████    | 86/142 [04:55<03:04,  3.29s/it]Running Inference:  61%|██████▏   | 87/142 [04:56<02:19,  2.54s/it]Running Inference:  62%|██████▏   | 88/142 [05:00<02:45,  3.07s/it]Running Inference:  63%|██████▎   | 89/142 [05:04<03:03,  3.46s/it]Running Inference:  63%|██████▎   | 90/142 [05:09<03:22,  3.89s/it]Running Inference:  64%|██████▍   | 91/142 [05:11<02:39,  3.12s/it]Running Inference:  65%|██████▍   | 92/142 [05:15<02:50,  3.41s/it]Running Inference:  65%|██████▌   | 93/142 [05:19<03:07,  3.82s/it]Running Inference:  66%|██████▌   | 94/142 [05:21<02:25,  3.02s/it]Running Inference:  67%|██████▋   | 95/142 [05:29<03:34,  4.57s/it]Running Inference:  68%|██████▊   | 96/142 [05:33<03:28,  4.52s/it]Running Inference:  68%|██████▊   | 97/142 [05:37<03:16,  4.36s/it]Running Inference:  69%|██████▉   | 98/142 [05:38<02:27,  3.36s/it]Running Inference:  70%|██████▉   | 99/142 [05:42<02:36,  3.63s/it]Running Inference:  70%|███████   | 100/142 [05:46<02:36,  3.73s/it]Running Inference:  71%|███████   | 101/142 [05:50<02:35,  3.80s/it]Running Inference:  72%|███████▏  | 102/142 [05:52<02:00,  3.00s/it]Running Inference:  73%|███████▎  | 103/142 [05:59<02:48,  4.33s/it]Running Inference:  73%|███████▎  | 104/142 [06:03<02:43,  4.30s/it]Running Inference:  74%|███████▍  | 105/142 [06:11<03:13,  5.24s/it]Running Inference:  75%|███████▍  | 106/142 [06:26<04:53,  8.14s/it]Running Inference:  75%|███████▌  | 107/142 [06:30<04:06,  7.05s/it]Running Inference:  76%|███████▌  | 108/142 [06:31<02:57,  5.22s/it]Running Inference:  77%|███████▋  | 109/142 [06:35<02:40,  4.86s/it]Running Inference:  77%|███████▋  | 110/142 [06:43<03:03,  5.72s/it]Running Inference:  78%|███████▊  | 111/142 [06:46<02:38,  5.13s/it]Running Inference:  79%|███████▉  | 112/142 [06:50<02:21,  4.71s/it]Running Inference:  80%|███████▉  | 113/142 [06:54<02:10,  4.50s/it]Running Inference:  80%|████████  | 114/142 [06:55<01:36,  3.43s/it]Running Inference:  81%|████████  | 115/142 [06:56<01:10,  2.60s/it]Running Inference:  82%|████████▏ | 116/142 [06:57<00:53,  2.05s/it]Running Inference:  82%|████████▏ | 117/142 [06:58<00:44,  1.79s/it]Running Inference:  83%|████████▎ | 118/142 [06:59<00:38,  1.62s/it]Running Inference:  84%|████████▍ | 119/142 [07:03<00:52,  2.27s/it]Running Inference:  85%|████████▍ | 120/142 [07:05<00:47,  2.15s/it]Running Inference:  85%|████████▌ | 121/142 [07:07<00:44,  2.12s/it]Running Inference:  86%|████████▌ | 122/142 [07:11<00:53,  2.66s/it]Running Inference:  87%|████████▋ | 123/142 [07:14<00:57,  3.01s/it]Running Inference:  87%|████████▋ | 124/142 [07:15<00:41,  2.28s/it]Running Inference:  88%|████████▊ | 125/142 [07:16<00:32,  1.90s/it]Running Inference:  89%|████████▊ | 126/142 [07:20<00:41,  2.57s/it]Running Inference:  89%|████████▉ | 127/142 [07:21<00:31,  2.07s/it]Running Inference:  90%|█████████ | 128/142 [07:22<00:22,  1.63s/it]Running Inference:  91%|█████████ | 129/142 [07:26<00:30,  2.32s/it]Running Inference:  92%|█████████▏| 130/142 [07:27<00:24,  2.04s/it]Running Inference:  92%|█████████▏| 131/142 [07:28<00:18,  1.68s/it]Running Inference:  93%|█████████▎| 132/142 [07:32<00:23,  2.35s/it]Running Inference:  94%|█████████▎| 133/142 [07:34<00:21,  2.42s/it]Running Inference:  94%|█████████▍| 134/142 [07:35<00:16,  2.04s/it]Running Inference:  95%|█████████▌| 135/142 [07:39<00:17,  2.56s/it]Running Inference:  96%|█████████▌| 136/142 [07:44<00:19,  3.30s/it]Running Inference:  96%|█████████▋| 137/142 [07:45<00:13,  2.66s/it]Running Inference:  97%|█████████▋| 138/142 [07:46<00:08,  2.15s/it]Running Inference:  98%|█████████▊| 139/142 [07:51<00:08,  2.80s/it]Running Inference:  99%|█████████▊| 140/142 [07:55<00:06,  3.22s/it]Running Inference:  99%|█████████▉| 141/142 [07:59<00:03,  3.56s/it]Running Inference: 100%|██████████| 142/142 [08:03<00:00,  3.72s/it]Running Inference: 100%|██████████| 142/142 [08:03<00:00,  3.41s/it]
2025-12-14 23:33:22,688 - INFO - Inference completed.
2025-12-14 23:33:22,696 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.30/predictions.csv
2025-12-14 23:33:22,696 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.667 seconds.
Prefix dict has been built successfully.
2025-12-14 23:33:23,467 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.30/metrics.json
2025-12-14 23:33:23,467 - INFO - Metrics:
19.17
2025-12-14 23:33:23,468 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_zh, ratio=0.3) on GPU 4

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:33:29,908 - INFO - Set deterministic seeds to 42
2025-12-14 23:33:29,908 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:33:29,908 - INFO - Starting evaluation run...
2025-12-14 23:33:29,908 - INFO - Output directory set to: longbenchresult
2025-12-14 23:33:29,908 - INFO - Set KnormPress compression_ratio to 0.5
2025-12-14 23:33:29,908 - INFO - KV Press 'knorm' setup.
2025-12-14 23:33:29,908 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:33:29,908 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.75it/s]
Device set to use cuda:0
2025-12-14 23:33:43,703 - INFO - Model pipeline loaded.
2025-12-14 23:33:43,703 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 23:33:53,895 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:33:53,896 - INFO - Dataset processed with 200 entries.
2025-12-14 23:33:53,906 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:03<07:37,  3.25s/it]Running Inference:   1%|▏         | 2/142 [00:11<14:04,  6.03s/it]Running Inference:   2%|▏         | 3/142 [00:15<12:31,  5.41s/it]Running Inference:   3%|▎         | 4/142 [00:19<11:09,  4.85s/it]Running Inference:   4%|▎         | 5/142 [00:23<10:22,  4.54s/it]Running Inference:   4%|▍         | 6/142 [00:24<07:06,  3.13s/it]Running Inference:   5%|▍         | 7/142 [00:28<07:54,  3.51s/it]Running Inference:   6%|▌         | 8/142 [00:30<06:43,  3.01s/it]Running Inference:   6%|▋         | 9/142 [00:37<09:45,  4.41s/it]Running Inference:   7%|▋         | 10/142 [00:42<10:04,  4.58s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:47<09:59,  4.57s/it]Running Inference:   8%|▊         | 12/142 [00:51<09:46,  4.51s/it]Running Inference:   9%|▉         | 13/142 [00:56<09:39,  4.49s/it]Running Inference:  10%|▉         | 14/142 [01:01<09:47,  4.59s/it]Running Inference:  11%|█         | 15/142 [01:02<07:29,  3.54s/it]Running Inference:  11%|█▏        | 16/142 [01:06<07:56,  3.78s/it]Running Inference:  12%|█▏        | 17/142 [01:14<10:32,  5.06s/it]Running Inference:  13%|█▎        | 18/142 [01:15<07:36,  3.68s/it]Running Inference:  13%|█▎        | 19/142 [01:19<08:09,  3.98s/it]Running Inference:  14%|█▍        | 20/142 [01:20<06:18,  3.10s/it]Running Inference:  15%|█▍        | 21/142 [01:32<11:30,  5.71s/it]Running Inference:  15%|█▌        | 22/142 [01:36<10:35,  5.30s/it]Running Inference:  16%|█▌        | 23/142 [01:37<07:44,  3.91s/it]Running Inference:  17%|█▋        | 24/142 [01:41<07:40,  3.90s/it]Running Inference:  18%|█▊        | 25/142 [01:42<05:40,  2.91s/it]Running Inference:  18%|█▊        | 26/142 [01:46<06:20,  3.28s/it]Running Inference:  19%|█▉        | 27/142 [01:49<06:32,  3.41s/it]Running Inference:  20%|█▉        | 28/142 [01:51<05:26,  2.86s/it]Running Inference:  20%|██        | 29/142 [01:53<04:52,  2.59s/it]Running Inference:  21%|██        | 30/142 [01:57<05:37,  3.01s/it]Running Inference:  22%|██▏       | 31/142 [01:58<04:25,  2.39s/it]Running Inference:  23%|██▎       | 32/142 [02:02<05:22,  2.93s/it]Running Inference:  23%|██▎       | 33/142 [02:06<05:50,  3.22s/it]Running Inference:  24%|██▍       | 34/142 [02:10<06:08,  3.42s/it]Running Inference:  25%|██▍       | 35/142 [02:14<06:26,  3.61s/it]Running Inference:  25%|██▌       | 36/142 [02:15<05:00,  2.84s/it]Running Inference:  26%|██▌       | 37/142 [02:16<04:03,  2.32s/it]Running Inference:  27%|██▋       | 38/142 [02:24<07:06,  4.10s/it]Running Inference:  27%|██▋       | 39/142 [02:29<07:03,  4.11s/it]Running Inference:  28%|██▊       | 40/142 [02:32<06:49,  4.02s/it]Running Inference:  29%|██▉       | 41/142 [02:36<06:48,  4.04s/it]Running Inference:  30%|██▉       | 42/142 [02:42<07:19,  4.39s/it]Running Inference:  30%|███       | 43/142 [02:46<07:00,  4.24s/it]Running Inference:  31%|███       | 44/142 [02:49<06:47,  4.16s/it]Running Inference:  32%|███▏      | 45/142 [02:54<07:03,  4.36s/it]Running Inference:  32%|███▏      | 46/142 [02:55<05:19,  3.32s/it]Running Inference:  33%|███▎      | 47/142 [02:59<05:34,  3.52s/it]Running Inference:  34%|███▍      | 48/142 [03:03<05:38,  3.60s/it]Running Inference:  35%|███▍      | 49/142 [03:07<05:40,  3.66s/it]Running Inference:  35%|███▌      | 50/142 [03:11<05:55,  3.86s/it]Running Inference:  36%|███▌      | 51/142 [03:15<05:52,  3.87s/it]Running Inference:  37%|███▋      | 52/142 [03:17<04:50,  3.22s/it]Running Inference:  37%|███▋      | 53/142 [03:18<03:45,  2.54s/it]Running Inference:  38%|███▊      | 54/142 [03:22<04:29,  3.06s/it]Running Inference:  39%|███▊      | 55/142 [03:26<04:57,  3.42s/it]Running Inference:  39%|███▉      | 56/142 [03:27<03:52,  2.71s/it]Running Inference:  40%|████      | 57/142 [03:31<04:18,  3.04s/it]Running Inference:  41%|████      | 58/142 [03:35<04:37,  3.30s/it]Running Inference:  42%|████▏     | 59/142 [03:37<04:03,  2.93s/it]Running Inference:  42%|████▏     | 60/142 [03:38<03:03,  2.24s/it]Running Inference:  43%|████▎     | 61/142 [03:39<02:42,  2.00s/it]Running Inference:  44%|████▎     | 62/142 [03:43<03:25,  2.57s/it]Running Inference:  44%|████▍     | 63/142 [03:45<03:20,  2.54s/it]Running Inference:  45%|████▌     | 64/142 [03:53<05:17,  4.07s/it]Running Inference:  46%|████▌     | 65/142 [03:57<05:11,  4.04s/it]Running Inference:  46%|████▋     | 66/142 [04:02<05:30,  4.34s/it]Running Inference:  47%|████▋     | 67/142 [04:06<05:19,  4.26s/it]Running Inference:  48%|████▊     | 68/142 [04:07<03:49,  3.10s/it]Running Inference:  49%|████▊     | 69/142 [04:14<05:21,  4.40s/it]Running Inference:  49%|████▉     | 70/142 [04:15<03:59,  3.33s/it]Running Inference:  50%|█████     | 71/142 [04:19<04:13,  3.58s/it]Running Inference:  51%|█████     | 72/142 [04:23<04:16,  3.67s/it]Running Inference:  51%|█████▏    | 73/142 [04:24<03:24,  2.96s/it]Running Inference:  52%|█████▏    | 74/142 [04:26<02:57,  2.61s/it]Running Inference:  53%|█████▎    | 75/142 [04:30<03:18,  2.97s/it]Running Inference:  54%|█████▎    | 76/142 [04:34<03:31,  3.21s/it]Running Inference:  54%|█████▍    | 77/142 [04:34<02:42,  2.50s/it]Running Inference:  55%|█████▍    | 78/142 [04:38<03:06,  2.92s/it]Running Inference:  56%|█████▌    | 79/142 [04:43<03:36,  3.43s/it]Running Inference:  56%|█████▋    | 80/142 [04:47<03:41,  3.57s/it]Running Inference:  57%|█████▋    | 81/142 [04:48<02:58,  2.92s/it]Running Inference:  58%|█████▊    | 82/142 [04:52<03:11,  3.20s/it]Running Inference:  58%|█████▊    | 83/142 [04:57<03:31,  3.58s/it]Running Inference:  59%|█████▉    | 84/142 [05:01<03:40,  3.80s/it]Running Inference:  60%|█████▉    | 85/142 [05:02<02:43,  2.87s/it]Running Inference:  61%|██████    | 86/142 [05:05<02:56,  3.15s/it]Running Inference:  61%|██████▏   | 87/142 [05:09<03:07,  3.42s/it]Running Inference:  62%|██████▏   | 88/142 [05:14<03:19,  3.69s/it]Running Inference:  63%|██████▎   | 89/142 [05:15<02:39,  3.00s/it]Running Inference:  63%|██████▎   | 90/142 [05:20<03:03,  3.53s/it]Running Inference:  64%|██████▍   | 91/142 [05:28<04:03,  4.77s/it]Running Inference:  65%|██████▍   | 92/142 [05:32<03:48,  4.57s/it]Running Inference:  65%|██████▌   | 93/142 [05:40<04:33,  5.59s/it]Running Inference:  66%|██████▌   | 94/142 [05:44<04:05,  5.11s/it]Running Inference:  67%|██████▋   | 95/142 [05:51<04:38,  5.92s/it]Running Inference:  68%|██████▊   | 96/142 [05:56<04:11,  5.47s/it]Running Inference:  68%|██████▊   | 97/142 [06:00<03:46,  5.03s/it]Running Inference:  69%|██████▉   | 98/142 [06:01<02:48,  3.82s/it]Running Inference:  70%|██████▉   | 99/142 [06:05<02:50,  3.96s/it]Running Inference:  70%|███████   | 100/142 [06:09<02:46,  3.96s/it]Running Inference:  71%|███████   | 101/142 [06:13<02:43,  3.99s/it]Running Inference:  72%|███████▏  | 102/142 [06:14<02:05,  3.13s/it]Running Inference:  73%|███████▎  | 103/142 [06:22<02:52,  4.42s/it]Running Inference:  73%|███████▎  | 104/142 [06:26<02:45,  4.36s/it]Running Inference:  74%|███████▍  | 105/142 [06:34<03:17,  5.33s/it]Running Inference:  75%|███████▍  | 106/142 [06:48<04:54,  8.19s/it]Running Inference:  75%|███████▌  | 107/142 [06:53<04:07,  7.08s/it]Running Inference:  76%|███████▌  | 108/142 [06:54<02:58,  5.26s/it]Running Inference:  77%|███████▋  | 109/142 [06:58<02:41,  4.89s/it]Running Inference:  77%|███████▋  | 110/142 [07:06<03:03,  5.73s/it]Running Inference:  78%|███████▊  | 111/142 [07:09<02:39,  5.13s/it]Running Inference:  79%|███████▉  | 112/142 [07:13<02:21,  4.70s/it]Running Inference:  80%|███████▉  | 113/142 [07:14<01:41,  3.49s/it]Running Inference:  80%|████████  | 114/142 [07:18<01:41,  3.64s/it]Running Inference:  81%|████████  | 115/142 [07:19<01:15,  2.81s/it]Running Inference:  82%|████████▏ | 116/142 [07:19<00:57,  2.20s/it]Running Inference:  82%|████████▏ | 117/142 [07:21<00:47,  1.90s/it]Running Inference:  83%|████████▎ | 118/142 [07:22<00:43,  1.80s/it]Running Inference:  84%|████████▍ | 119/142 [07:26<00:54,  2.39s/it]Running Inference:  85%|████████▍ | 120/142 [07:28<00:49,  2.23s/it]Running Inference:  85%|████████▌ | 121/142 [07:29<00:41,  1.99s/it]Running Inference:  86%|████████▌ | 122/142 [07:33<00:51,  2.56s/it]Running Inference:  87%|████████▋ | 123/142 [07:37<00:55,  2.94s/it]Running Inference:  87%|████████▋ | 124/142 [07:38<00:42,  2.33s/it]Running Inference:  88%|████████▊ | 125/142 [07:42<00:48,  2.86s/it]Running Inference:  89%|████████▊ | 126/142 [07:43<00:36,  2.27s/it]Running Inference:  89%|████████▉ | 127/142 [07:44<00:27,  1.82s/it]Running Inference:  90%|█████████ | 128/142 [07:47<00:33,  2.40s/it]Running Inference:  91%|█████████ | 129/142 [07:48<00:26,  2.01s/it]Running Inference:  92%|█████████▏| 130/142 [07:53<00:33,  2.80s/it]Running Inference:  92%|█████████▏| 131/142 [07:57<00:34,  3.10s/it]Running Inference:  93%|█████████▎| 132/142 [08:01<00:33,  3.33s/it]Running Inference:  94%|█████████▎| 133/142 [08:12<00:51,  5.76s/it]Running Inference:  94%|█████████▍| 134/142 [08:13<00:34,  4.37s/it]Running Inference:  95%|█████████▌| 135/142 [08:17<00:29,  4.20s/it]Running Inference:  96%|█████████▌| 136/142 [08:25<00:32,  5.38s/it]Running Inference:  96%|█████████▋| 137/142 [08:29<00:24,  4.99s/it]Running Inference:  97%|█████████▋| 138/142 [08:30<00:15,  3.75s/it]Running Inference:  98%|█████████▊| 139/142 [08:31<00:09,  3.03s/it]Running Inference:  99%|█████████▊| 140/142 [08:32<00:04,  2.38s/it]Running Inference:  99%|█████████▉| 141/142 [08:37<00:02,  2.97s/it]Running Inference: 100%|██████████| 142/142 [08:41<00:00,  3.30s/it]Running Inference: 100%|██████████| 142/142 [08:41<00:00,  3.67s/it]
2025-12-14 23:42:35,205 - INFO - Inference completed.
2025-12-14 23:42:35,214 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.50/predictions.csv
2025-12-14 23:42:35,214 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.662 seconds.
Prefix dict has been built successfully.
2025-12-14 23:42:35,986 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__knorm__0.50/metrics.json
2025-12-14 23:42:35,986 - INFO - Metrics:
14.77
2025-12-14 23:42:35,987 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=multifieldqa_zh, ratio=0.5) on GPU 4


========================================
LongBench Task: passage_retrieval_zh
========================================
----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:42:42,440 - INFO - Set deterministic seeds to 42
2025-12-14 23:42:42,440 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:42:42,440 - INFO - Starting evaluation run...
2025-12-14 23:42:42,440 - INFO - Output directory set to: longbenchresult
2025-12-14 23:42:42,440 - INFO - Set KnormPress compression_ratio to 0.1
2025-12-14 23:42:42,440 - INFO - KV Press 'knorm' setup.
2025-12-14 23:42:42,440 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:42:42,440 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.64it/s]
Device set to use cuda:0
2025-12-14 23:42:55,111 - INFO - Model pipeline loaded.
2025-12-14 23:42:55,111 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 23:42:58,722 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:42:58,722 - INFO - Dataset processed with 200 entries.
2025-12-14 23:42:58,734 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:39,  3.52s/it]Running Inference:   1%|          | 2/200 [00:04<05:58,  1.81s/it]Running Inference:   2%|▏         | 3/200 [00:06<07:24,  2.26s/it]Running Inference:   2%|▏         | 4/200 [00:09<08:00,  2.45s/it]Running Inference:   2%|▎         | 5/200 [00:10<05:47,  1.78s/it]Running Inference:   3%|▎         | 6/200 [00:13<06:48,  2.11s/it]Running Inference:   4%|▎         | 7/200 [00:13<05:18,  1.65s/it]Running Inference:   4%|▍         | 8/200 [00:14<04:11,  1.31s/it]Running Inference:   4%|▍         | 9/200 [00:14<03:24,  1.07s/it]Running Inference:   5%|▌         | 10/200 [00:17<04:58,  1.57s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<06:07,  1.94s/it]Running Inference:   6%|▌         | 12/200 [00:21<04:56,  1.58s/it]Running Inference:   6%|▋         | 13/200 [00:21<04:01,  1.29s/it]Running Inference:   7%|▋         | 14/200 [00:22<03:18,  1.07s/it]Running Inference:   8%|▊         | 15/200 [00:22<02:47,  1.10it/s]Running Inference:   8%|▊         | 16/200 [00:23<02:41,  1.14it/s]Running Inference:   8%|▊         | 17/200 [00:24<02:22,  1.28it/s]Running Inference:   9%|▉         | 18/200 [00:24<02:11,  1.38it/s]Running Inference:  10%|▉         | 19/200 [00:25<02:07,  1.42it/s]Running Inference:  10%|█         | 20/200 [00:25<02:01,  1.48it/s]Running Inference:  10%|█         | 21/200 [00:26<01:57,  1.52it/s]Running Inference:  11%|█         | 22/200 [00:27<01:57,  1.52it/s]Running Inference:  12%|█▏        | 23/200 [00:29<03:46,  1.28s/it]Running Inference:  12%|█▏        | 24/200 [00:30<03:06,  1.06s/it]Running Inference:  12%|█▎        | 25/200 [00:31<02:43,  1.07it/s]Running Inference:  13%|█▎        | 26/200 [00:31<02:25,  1.19it/s]Running Inference:  14%|█▎        | 27/200 [00:32<02:08,  1.35it/s]Running Inference:  14%|█▍        | 28/200 [00:32<02:00,  1.43it/s]Running Inference:  14%|█▍        | 29/200 [00:33<01:51,  1.53it/s]Running Inference:  15%|█▌        | 30/200 [00:36<03:36,  1.27s/it]Running Inference:  16%|█▌        | 31/200 [00:38<04:49,  1.71s/it]Running Inference:  16%|█▌        | 32/200 [00:39<03:50,  1.37s/it]Running Inference:  16%|█▋        | 33/200 [00:40<03:48,  1.37s/it]Running Inference:  17%|█▋        | 34/200 [00:41<03:07,  1.13s/it]Running Inference:  18%|█▊        | 35/200 [00:42<02:42,  1.02it/s]Running Inference:  18%|█▊        | 36/200 [00:43<03:00,  1.10s/it]Running Inference:  18%|█▊        | 37/200 [00:44<02:38,  1.03it/s]Running Inference:  19%|█▉        | 38/200 [00:44<02:16,  1.19it/s]Running Inference:  20%|█▉        | 39/200 [00:45<02:05,  1.28it/s]Running Inference:  20%|██        | 40/200 [00:45<01:54,  1.40it/s]Running Inference:  20%|██        | 41/200 [00:46<01:52,  1.41it/s]Running Inference:  21%|██        | 42/200 [00:47<01:44,  1.51it/s]Running Inference:  22%|██▏       | 43/200 [00:47<01:42,  1.53it/s]Running Inference:  22%|██▏       | 44/200 [00:48<01:40,  1.55it/s]Running Inference:  22%|██▎       | 45/200 [00:49<02:03,  1.26it/s]Running Inference:  23%|██▎       | 46/200 [00:50<01:48,  1.42it/s]Running Inference:  24%|██▎       | 47/200 [00:52<03:21,  1.32s/it]Running Inference:  24%|██▍       | 48/200 [00:53<03:14,  1.28s/it]Running Inference:  24%|██▍       | 49/200 [00:56<04:19,  1.72s/it]Running Inference:  25%|██▌       | 50/200 [00:57<03:25,  1.37s/it]Running Inference:  26%|██▌       | 51/200 [00:57<02:52,  1.16s/it]Running Inference:  26%|██▌       | 52/200 [01:00<04:00,  1.62s/it]Running Inference:  26%|██▋       | 53/200 [01:01<03:15,  1.33s/it]Running Inference:  27%|██▋       | 54/200 [01:04<04:16,  1.76s/it]Running Inference:  28%|██▊       | 55/200 [01:04<03:24,  1.41s/it]Running Inference:  28%|██▊       | 56/200 [01:07<04:18,  1.80s/it]Running Inference:  28%|██▊       | 57/200 [01:07<03:21,  1.41s/it]Running Inference:  29%|██▉       | 58/200 [01:08<02:49,  1.19s/it]Running Inference:  30%|██▉       | 59/200 [01:11<03:57,  1.68s/it]Running Inference:  30%|███       | 60/200 [01:11<03:09,  1.36s/it]Running Inference:  30%|███       | 61/200 [01:12<02:34,  1.11s/it]Running Inference:  31%|███       | 62/200 [01:13<02:15,  1.02it/s]Running Inference:  32%|███▏      | 63/200 [01:13<01:57,  1.17it/s]Running Inference:  32%|███▏      | 64/200 [01:16<03:13,  1.42s/it]Running Inference:  32%|███▎      | 65/200 [01:19<04:04,  1.81s/it]Running Inference:  33%|███▎      | 66/200 [01:19<03:15,  1.46s/it]Running Inference:  34%|███▎      | 67/200 [01:20<02:44,  1.24s/it]Running Inference:  34%|███▍      | 68/200 [01:21<02:16,  1.03s/it]Running Inference:  34%|███▍      | 69/200 [01:21<01:58,  1.11it/s]Running Inference:  35%|███▌      | 70/200 [01:24<03:13,  1.49s/it]Running Inference:  36%|███▌      | 71/200 [01:27<03:59,  1.86s/it]Running Inference:  36%|███▌      | 72/200 [01:30<04:34,  2.14s/it]Running Inference:  36%|███▋      | 73/200 [01:30<03:30,  1.65s/it]Running Inference:  37%|███▋      | 74/200 [01:31<02:55,  1.40s/it]Running Inference:  38%|███▊      | 75/200 [01:31<02:23,  1.15s/it]Running Inference:  38%|███▊      | 76/200 [01:34<03:20,  1.62s/it]Running Inference:  38%|███▊      | 77/200 [01:35<02:40,  1.31s/it]Running Inference:  39%|███▉      | 78/200 [01:35<02:11,  1.08s/it]Running Inference:  40%|███▉      | 79/200 [01:36<01:50,  1.10it/s]Running Inference:  40%|████      | 80/200 [01:36<01:37,  1.23it/s]Running Inference:  40%|████      | 81/200 [01:38<01:52,  1.06it/s]Running Inference:  41%|████      | 82/200 [01:38<01:42,  1.15it/s]Running Inference:  42%|████▏     | 83/200 [01:39<01:32,  1.26it/s]Running Inference:  42%|████▏     | 84/200 [01:40<01:23,  1.39it/s]Running Inference:  42%|████▎     | 85/200 [01:41<01:40,  1.14it/s]Running Inference:  43%|████▎     | 86/200 [01:41<01:27,  1.30it/s]Running Inference:  44%|████▎     | 87/200 [01:42<01:18,  1.43it/s]Running Inference:  44%|████▍     | 88/200 [01:44<02:24,  1.29s/it]Running Inference:  44%|████▍     | 89/200 [01:47<03:13,  1.74s/it]Running Inference:  45%|████▌     | 90/200 [01:50<03:45,  2.05s/it]Running Inference:  46%|████▌     | 91/200 [01:51<02:57,  1.62s/it]Running Inference:  46%|████▌     | 92/200 [01:52<02:42,  1.50s/it]Running Inference:  46%|████▋     | 93/200 [01:53<02:33,  1.44s/it]Running Inference:  47%|████▋     | 94/200 [01:54<02:05,  1.18s/it]Running Inference:  48%|████▊     | 95/200 [01:54<01:44,  1.00it/s]Running Inference:  48%|████▊     | 96/200 [01:55<01:31,  1.13it/s]Running Inference:  48%|████▊     | 97/200 [01:56<01:22,  1.25it/s]Running Inference:  49%|████▉     | 98/200 [01:58<02:18,  1.36s/it]Running Inference:  50%|████▉     | 99/200 [01:59<01:54,  1.14s/it]Running Inference:  50%|█████     | 100/200 [02:00<01:40,  1.00s/it]Running Inference:  50%|█████     | 101/200 [02:00<01:25,  1.16it/s]Running Inference:  51%|█████     | 102/200 [02:03<02:22,  1.46s/it]Running Inference:  52%|█████▏    | 103/200 [02:03<01:54,  1.18s/it]Running Inference:  52%|█████▏    | 104/200 [02:06<02:39,  1.66s/it]Running Inference:  52%|█████▎    | 105/200 [02:07<02:07,  1.34s/it]Running Inference:  53%|█████▎    | 106/200 [02:07<01:44,  1.11s/it]Running Inference:  54%|█████▎    | 107/200 [02:08<01:27,  1.06it/s]Running Inference:  54%|█████▍    | 108/200 [02:09<01:17,  1.19it/s]Running Inference:  55%|█████▍    | 109/200 [02:09<01:08,  1.33it/s]Running Inference:  55%|█████▌    | 110/200 [02:10<01:07,  1.34it/s]Running Inference:  56%|█████▌    | 111/200 [02:13<02:00,  1.36s/it]Running Inference:  56%|█████▌    | 112/200 [02:15<02:39,  1.81s/it]Running Inference:  56%|█████▋    | 113/200 [02:18<03:00,  2.07s/it]Running Inference:  57%|█████▋    | 114/200 [02:19<02:21,  1.65s/it]Running Inference:  57%|█████▊    | 115/200 [02:19<01:52,  1.32s/it]Running Inference:  58%|█████▊    | 116/200 [02:20<01:30,  1.07s/it]Running Inference:  58%|█████▊    | 117/200 [02:21<01:19,  1.05it/s]Running Inference:  59%|█████▉    | 118/200 [02:21<01:10,  1.16it/s]Running Inference:  60%|█████▉    | 119/200 [02:24<01:56,  1.44s/it]Running Inference:  60%|██████    | 120/200 [02:24<01:32,  1.15s/it]Running Inference:  60%|██████    | 121/200 [02:25<01:17,  1.02it/s]Running Inference:  61%|██████    | 122/200 [02:26<01:06,  1.17it/s]Running Inference:  62%|██████▏   | 123/200 [02:26<00:58,  1.32it/s]Running Inference:  62%|██████▏   | 124/200 [02:27<00:54,  1.40it/s]Running Inference:  62%|██████▎   | 125/200 [02:27<00:49,  1.50it/s]Running Inference:  63%|██████▎   | 126/200 [02:29<01:07,  1.09it/s]Running Inference:  64%|██████▎   | 127/200 [02:29<01:01,  1.19it/s]Running Inference:  64%|██████▍   | 128/200 [02:30<00:54,  1.33it/s]Running Inference:  64%|██████▍   | 129/200 [02:31<00:50,  1.41it/s]Running Inference:  65%|██████▌   | 130/200 [02:31<00:48,  1.46it/s]Running Inference:  66%|██████▌   | 131/200 [02:32<00:47,  1.46it/s]Running Inference:  66%|██████▌   | 132/200 [02:33<00:51,  1.33it/s]Running Inference:  66%|██████▋   | 133/200 [02:34<00:48,  1.38it/s]Running Inference:  67%|██████▋   | 134/200 [02:34<00:46,  1.42it/s]Running Inference:  68%|██████▊   | 135/200 [02:37<01:25,  1.32s/it]Running Inference:  68%|██████▊   | 136/200 [02:37<01:09,  1.09s/it]Running Inference:  68%|██████▊   | 137/200 [02:38<00:58,  1.07it/s]Running Inference:  69%|██████▉   | 138/200 [02:39<00:50,  1.23it/s]Running Inference:  70%|██████▉   | 139/200 [02:39<00:44,  1.36it/s]Running Inference:  70%|███████   | 140/200 [02:42<01:21,  1.36s/it]Running Inference:  70%|███████   | 141/200 [02:43<01:06,  1.13s/it]Running Inference:  71%|███████   | 142/200 [02:43<00:55,  1.04it/s]Running Inference:  72%|███████▏  | 143/200 [02:44<00:48,  1.19it/s]Running Inference:  72%|███████▏  | 144/200 [02:44<00:41,  1.35it/s]Running Inference:  72%|███████▎  | 145/200 [02:45<00:37,  1.47it/s]Running Inference:  73%|███████▎  | 146/200 [02:45<00:33,  1.62it/s]Running Inference:  74%|███████▎  | 147/200 [02:46<00:32,  1.61it/s]Running Inference:  74%|███████▍  | 148/200 [02:46<00:31,  1.63it/s]Running Inference:  74%|███████▍  | 149/200 [02:49<01:02,  1.23s/it]Running Inference:  75%|███████▌  | 150/200 [02:50<00:52,  1.04s/it]Running Inference:  76%|███████▌  | 151/200 [02:52<01:14,  1.51s/it]Running Inference:  76%|███████▌  | 152/200 [02:53<01:00,  1.26s/it]Running Inference:  76%|███████▋  | 153/200 [02:53<00:49,  1.05s/it]Running Inference:  77%|███████▋  | 154/200 [02:54<00:41,  1.11it/s]Running Inference:  78%|███████▊  | 155/200 [02:57<01:07,  1.49s/it]Running Inference:  78%|███████▊  | 156/200 [03:00<01:21,  1.86s/it]Running Inference:  78%|███████▊  | 157/200 [03:00<01:03,  1.47s/it]Running Inference:  79%|███████▉  | 158/200 [03:01<00:51,  1.23s/it]Running Inference:  80%|███████▉  | 159/200 [03:01<00:42,  1.05s/it]Running Inference:  80%|████████  | 160/200 [03:02<00:36,  1.09it/s]Running Inference:  80%|████████  | 161/200 [03:03<00:32,  1.19it/s]Running Inference:  81%|████████  | 162/200 [03:03<00:27,  1.36it/s]Running Inference:  82%|████████▏ | 163/200 [03:04<00:25,  1.43it/s]Running Inference:  82%|████████▏ | 164/200 [03:04<00:24,  1.48it/s]Running Inference:  82%|████████▎ | 165/200 [03:07<00:45,  1.29s/it]Running Inference:  83%|████████▎ | 166/200 [03:08<00:37,  1.10s/it]Running Inference:  84%|████████▎ | 167/200 [03:08<00:31,  1.05it/s]Running Inference:  84%|████████▍ | 168/200 [03:09<00:27,  1.16it/s]Running Inference:  84%|████████▍ | 169/200 [03:12<00:43,  1.40s/it]Running Inference:  85%|████████▌ | 170/200 [03:15<00:54,  1.80s/it]Running Inference:  86%|████████▌ | 171/200 [03:17<00:59,  2.05s/it]Running Inference:  86%|████████▌ | 172/200 [03:18<00:45,  1.62s/it]Running Inference:  86%|████████▋ | 173/200 [03:18<00:34,  1.26s/it]Running Inference:  87%|████████▋ | 174/200 [03:19<00:27,  1.06s/it]Running Inference:  88%|████████▊ | 175/200 [03:19<00:23,  1.09it/s]Running Inference:  88%|████████▊ | 176/200 [03:20<00:19,  1.21it/s]Running Inference:  88%|████████▊ | 177/200 [03:23<00:32,  1.40s/it]Running Inference:  89%|████████▉ | 178/200 [03:25<00:39,  1.80s/it]Running Inference:  90%|████████▉ | 179/200 [03:26<00:30,  1.43s/it]Running Inference:  90%|█████████ | 180/200 [03:29<00:37,  1.86s/it]Running Inference:  90%|█████████ | 181/200 [03:29<00:28,  1.48s/it]Running Inference:  91%|█████████ | 182/200 [03:30<00:22,  1.25s/it]Running Inference:  92%|█████████▏| 183/200 [03:31<00:18,  1.06s/it]Running Inference:  92%|█████████▏| 184/200 [03:31<00:14,  1.10it/s]Running Inference:  92%|█████████▎| 185/200 [03:34<00:21,  1.44s/it]Running Inference:  93%|█████████▎| 186/200 [03:35<00:16,  1.19s/it]Running Inference:  94%|█████████▎| 187/200 [03:37<00:21,  1.68s/it]Running Inference:  94%|█████████▍| 188/200 [03:38<00:16,  1.34s/it]Running Inference:  94%|█████████▍| 189/200 [03:41<00:19,  1.75s/it]Running Inference:  95%|█████████▌| 190/200 [03:43<00:20,  2.05s/it]Running Inference:  96%|█████████▌| 191/200 [03:44<00:14,  1.62s/it]Running Inference:  96%|█████████▌| 192/200 [03:45<00:10,  1.30s/it]Running Inference:  96%|█████████▋| 193/200 [03:45<00:07,  1.08s/it]Running Inference:  97%|█████████▋| 194/200 [03:48<00:09,  1.60s/it]Running Inference:  98%|█████████▊| 195/200 [03:49<00:06,  1.39s/it]Running Inference:  98%|█████████▊| 196/200 [03:49<00:04,  1.13s/it]Running Inference:  98%|█████████▊| 197/200 [03:52<00:04,  1.64s/it]Running Inference:  99%|█████████▉| 198/200 [03:53<00:02,  1.29s/it]Running Inference: 100%|█████████▉| 199/200 [03:53<00:01,  1.08s/it]Running Inference: 100%|██████████| 200/200 [03:54<00:00,  1.08it/s]Running Inference: 100%|██████████| 200/200 [03:54<00:00,  1.17s/it]
2025-12-14 23:46:53,164 - INFO - Inference completed.
2025-12-14 23:46:53,174 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__knorm__0.10/predictions.csv
2025-12-14 23:46:53,174 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:46:53,175 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__knorm__0.10/metrics.json
2025-12-14 23:46:53,175 - INFO - Metrics:
0.5
2025-12-14 23:46:53,177 - INFO - Evaluation run completed successfully.
✓ Completed: knorm (task=passage_retrieval_zh, ratio=0.1) on GPU 4

----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:46:59,590 - INFO - Set deterministic seeds to 42
2025-12-14 23:46:59,590 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "knorm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:46:59,590 - INFO - Starting evaluation run...
2025-12-14 23:46:59,590 - INFO - Output directory set to: longbenchresult
2025-12-14 23:46:59,590 - INFO - Set KnormPress compression_ratio to 0.2
2025-12-14 23:46:59,591 - INFO - KV Press 'knorm' setup.
2025-12-14 23:46:59,591 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:46:59,591 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.68it/s]
Device set to use cuda:0
2025-12-14 23:47:13,492 - INFO - Model pipeline loaded.
2025-12-14 23:47:13,492 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
Using the latest cached version of the dataset since Xnhyacinth/LongBench couldn't be found on the Hugging Face Hub
Traceback (most recent call last):
  File "/data/kvpress-main1/evaluation/evaluate.py", line 685, in <module>
    Fire(CliEntryPoint)
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 568, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/evaluation/evaluate.py", line 681, in __call__
    runner.run_evaluation()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 631, in run_evaluation
    self._load_and_prepare_dataset()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 411, in _load_and_prepare_dataset
    df = load_dataset(DATASET_REGISTRY[dataset_name], data_dir=data_dir, split="test").to_pandas()
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2606, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2314, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 140, in __init__
    config_name, version, hash = _find_hash_in_cache(
                                 ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 65, in _find_hash_in_cache
    raise ValueError(
ValueError: Couldn't find cache for Xnhyacinth/LongBench for config 'default-data_dir=passage_retrieval_zh'
Available configs in the cache: ['2wikimqa', '2wikimqa_e', 'default-0065b3a180de55c0', 'default-098a7eb91a184203', 'default-0b7076831c003298', 'default-24f61eabf2a87b99', 'default-24fadcfe8fa8f3a5', 'default-4e16e465b7b19fe5', 'default-515182422f5c01c8', 'default-531e246ba9fed12a', 'default-5984d2c88b13ebba', 'default-5e4ea58ffb882d49', 'default-65a1875e7b162d76', 'default-7a8b9ba40e031066', 'default-8d4d4887aac16609', 'default-988b39b640b83efb', 'default-9d6e0826941c237c', 'default-a17dcc93f20cb815', 'default-a9c51f35339386df', 'default-afe1d98c024b9964', 'default-b1151d9592e7a1a3', 'default-dce78539c6759811', 'default-f744668059827d3f', 'dureader', 'gov_report', 'gov_report_e', 'hotpotqa', 'hotpotqa_e', 'lcc', 'lcc_e', 'lsht', 'multi_news', 'multi_news_e', 'multifieldqa_en', 'multifieldqa_en_e', 'multifieldqa_zh', 'musique', 'narrativeqa', 'passage_count', 'passage_count_e', 'passage_retrieval_en', 'passage_retrieval_en_e', 'passage_retrieval_zh', 'qasper', 'qasper_e', 'qmsum', 'repobench-p', 'repobench-p_e', 'samsum', 'samsum_e', 'trec', 'trec_e', 'triviaqa', 'triviaqa_e', 'vcsum']
