==========================================
Method: streaming_llm
GPU: 2
Start Time: Sat Dec 13 05:54:11 PM CST 2025
==========================================

========================================
LongBench Task: 2wikimqa
========================================
----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 17:54:16,650 - INFO - Set deterministic seeds to 42
2025-12-13 17:54:16,650 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 17:54:16,650 - INFO - Starting evaluation run...
2025-12-13 17:54:16,651 - INFO - Output directory set to: longbenchresult
2025-12-13 17:54:16,651 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 17:54:16,651 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 17:54:16,651 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 17:54:16,651 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.57it/s]
Device set to use cuda:0
2025-12-13 17:54:30,943 - INFO - Model pipeline loaded.
2025-12-13 17:54:30,943 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 17:54:36,064 - INFO - Dataset loaded with 200 entries.
2025-12-13 17:54:36,064 - INFO - Dataset processed with 200 entries.
2025-12-13 17:54:36,085 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<13:54,  4.19s/it]Running Inference:   1%|          | 2/200 [00:07<11:20,  3.44s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:51,  3.30s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:55,  3.04s/it]Running Inference:   2%|▎         | 5/200 [00:17<11:09,  3.43s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:05,  2.81s/it]Running Inference:   4%|▎         | 7/200 [00:19<07:13,  2.24s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:06,  2.54s/it]Running Inference:   4%|▍         | 9/200 [00:26<08:49,  2.77s/it]Running Inference:   5%|▌         | 10/200 [00:26<06:45,  2.14s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:30<08:03,  2.56s/it]Running Inference:   6%|▌         | 12/200 [00:32<07:11,  2.29s/it]Running Inference:   6%|▋         | 13/200 [00:35<08:16,  2.66s/it]Running Inference:   7%|▋         | 14/200 [00:36<06:15,  2.02s/it]Running Inference:   8%|▊         | 15/200 [00:37<05:12,  1.69s/it]Running Inference:   8%|▊         | 16/200 [00:37<04:28,  1.46s/it]Running Inference:   8%|▊         | 17/200 [00:40<05:08,  1.68s/it]Running Inference:   9%|▉         | 18/200 [00:41<04:27,  1.47s/it]Running Inference:  10%|▉         | 19/200 [00:43<05:35,  1.86s/it]Running Inference:  10%|█         | 20/200 [00:44<04:09,  1.39s/it]Running Inference:  10%|█         | 21/200 [00:46<05:15,  1.76s/it]Running Inference:  11%|█         | 22/200 [00:49<06:09,  2.08s/it]Running Inference:  12%|█▏        | 23/200 [00:53<07:17,  2.47s/it]Running Inference:  12%|█▏        | 24/200 [00:56<08:15,  2.81s/it]Running Inference:  12%|█▎        | 25/200 [00:59<08:29,  2.91s/it]Running Inference:  13%|█▎        | 26/200 [01:02<08:25,  2.91s/it]Running Inference:  14%|█▎        | 27/200 [01:05<08:10,  2.84s/it]Running Inference:  14%|█▍        | 28/200 [01:06<06:21,  2.22s/it]Running Inference:  14%|█▍        | 29/200 [01:08<06:44,  2.37s/it]Running Inference:  15%|█▌        | 30/200 [01:09<04:55,  1.74s/it]Running Inference:  16%|█▌        | 31/200 [01:11<05:04,  1.80s/it]Running Inference:  16%|█▌        | 32/200 [01:12<04:39,  1.66s/it]Running Inference:  16%|█▋        | 33/200 [01:14<05:16,  1.90s/it]Running Inference:  17%|█▋        | 34/200 [01:17<05:59,  2.17s/it]Running Inference:  18%|█▊        | 35/200 [01:20<06:32,  2.38s/it]Running Inference:  18%|█▊        | 36/200 [01:21<05:16,  1.93s/it]Running Inference:  18%|█▊        | 37/200 [01:24<06:09,  2.27s/it]Running Inference:  19%|█▉        | 38/200 [01:25<05:28,  2.03s/it]Running Inference:  20%|█▉        | 39/200 [01:28<06:06,  2.28s/it]Running Inference:  20%|██        | 40/200 [01:30<05:47,  2.17s/it]Running Inference:  20%|██        | 41/200 [01:31<04:59,  1.88s/it]Running Inference:  21%|██        | 42/200 [01:33<04:39,  1.77s/it]Running Inference:  22%|██▏       | 43/200 [01:34<03:55,  1.50s/it]Running Inference:  22%|██▏       | 44/200 [01:34<03:12,  1.23s/it]Running Inference:  22%|██▎       | 45/200 [01:35<02:58,  1.15s/it]Running Inference:  23%|██▎       | 46/200 [01:37<03:12,  1.25s/it]Running Inference:  24%|██▎       | 47/200 [01:41<05:23,  2.11s/it]Running Inference:  24%|██▍       | 48/200 [01:42<04:28,  1.77s/it]Running Inference:  24%|██▍       | 49/200 [01:45<05:24,  2.15s/it]Running Inference:  25%|██▌       | 50/200 [01:48<05:51,  2.34s/it]Running Inference:  26%|██▌       | 51/200 [01:50<05:40,  2.29s/it]Running Inference:  26%|██▌       | 52/200 [01:52<05:43,  2.32s/it]Running Inference:  26%|██▋       | 53/200 [01:55<05:55,  2.42s/it]Running Inference:  27%|██▋       | 54/200 [01:56<05:10,  2.13s/it]Running Inference:  28%|██▊       | 55/200 [01:57<04:12,  1.74s/it]Running Inference:  28%|██▊       | 56/200 [01:58<03:48,  1.59s/it]Running Inference:  28%|██▊       | 57/200 [01:59<03:20,  1.41s/it]Running Inference:  29%|██▉       | 58/200 [02:01<03:18,  1.40s/it]Running Inference:  30%|██▉       | 59/200 [02:04<04:12,  1.79s/it]Running Inference:  30%|███       | 60/200 [02:05<03:38,  1.56s/it]Running Inference:  30%|███       | 61/200 [02:07<04:26,  1.92s/it]Running Inference:  31%|███       | 62/200 [02:09<04:23,  1.91s/it]Running Inference:  32%|███▏      | 63/200 [02:09<03:13,  1.41s/it]Running Inference:  32%|███▏      | 64/200 [02:13<04:54,  2.16s/it]Running Inference:  32%|███▎      | 65/200 [02:15<04:13,  1.88s/it]Running Inference:  33%|███▎      | 66/200 [02:18<05:07,  2.30s/it]Running Inference:  34%|███▎      | 67/200 [02:21<05:39,  2.55s/it]Running Inference:  34%|███▍      | 68/200 [02:24<06:07,  2.78s/it]Running Inference:  34%|███▍      | 69/200 [02:25<05:00,  2.29s/it]Running Inference:  35%|███▌      | 70/200 [02:26<04:08,  1.91s/it]Running Inference:  36%|███▌      | 71/200 [02:27<03:15,  1.51s/it]Running Inference:  36%|███▌      | 72/200 [02:32<05:22,  2.52s/it]Running Inference:  36%|███▋      | 73/200 [02:35<05:34,  2.64s/it]Running Inference:  37%|███▋      | 74/200 [02:37<05:11,  2.47s/it]Running Inference:  38%|███▊      | 75/200 [02:38<04:17,  2.06s/it]Running Inference:  38%|███▊      | 76/200 [02:39<03:42,  1.80s/it]Running Inference:  38%|███▊      | 77/200 [02:40<03:13,  1.58s/it]Running Inference:  39%|███▉      | 78/200 [02:44<04:13,  2.08s/it]Running Inference:  40%|███▉      | 79/200 [02:47<04:44,  2.35s/it]Running Inference:  40%|████      | 80/200 [02:50<05:21,  2.68s/it]Running Inference:  40%|████      | 81/200 [02:53<05:20,  2.69s/it]Running Inference:  41%|████      | 82/200 [02:56<05:28,  2.79s/it]Running Inference:  42%|████▏     | 83/200 [02:57<04:40,  2.40s/it]Running Inference:  42%|████▏     | 84/200 [02:58<03:46,  1.96s/it]Running Inference:  42%|████▎     | 85/200 [03:01<04:20,  2.26s/it]Running Inference:  43%|████▎     | 86/200 [03:05<05:09,  2.72s/it]Running Inference:  44%|████▎     | 87/200 [03:08<05:07,  2.72s/it]Running Inference:  44%|████▍     | 88/200 [03:11<05:11,  2.78s/it]Running Inference:  44%|████▍     | 89/200 [03:13<05:10,  2.79s/it]Running Inference:  45%|████▌     | 90/200 [03:16<05:14,  2.86s/it]Running Inference:  46%|████▌     | 91/200 [03:19<05:06,  2.82s/it]Running Inference:  46%|████▌     | 92/200 [03:22<05:08,  2.86s/it]Running Inference:  46%|████▋     | 93/200 [03:24<04:36,  2.59s/it]Running Inference:  47%|████▋     | 94/200 [03:24<03:25,  1.94s/it]Running Inference:  48%|████▊     | 95/200 [03:25<02:44,  1.57s/it]Running Inference:  48%|████▊     | 96/200 [03:28<03:21,  1.94s/it]Running Inference:  48%|████▊     | 97/200 [03:30<03:30,  2.04s/it]Running Inference:  49%|████▉     | 98/200 [03:31<02:56,  1.73s/it]Running Inference:  50%|████▉     | 99/200 [03:33<02:50,  1.69s/it]Running Inference:  50%|█████     | 100/200 [03:34<02:47,  1.67s/it]Running Inference:  50%|█████     | 101/200 [03:37<03:16,  1.99s/it]Running Inference:  51%|█████     | 102/200 [03:40<03:37,  2.22s/it]Running Inference:  52%|█████▏    | 103/200 [03:43<03:49,  2.37s/it]Running Inference:  52%|█████▏    | 104/200 [03:46<04:01,  2.52s/it]Running Inference:  52%|█████▎    | 105/200 [03:48<04:09,  2.63s/it]Running Inference:  53%|█████▎    | 106/200 [03:49<03:18,  2.11s/it]Running Inference:  54%|█████▎    | 107/200 [03:52<03:36,  2.32s/it]Running Inference:  54%|█████▍    | 108/200 [03:55<03:48,  2.48s/it]Running Inference:  55%|█████▍    | 109/200 [03:56<03:02,  2.00s/it]Running Inference:  55%|█████▌    | 110/200 [03:57<02:48,  1.87s/it]Running Inference:  56%|█████▌    | 111/200 [03:58<02:14,  1.51s/it]Running Inference:  56%|█████▌    | 112/200 [03:59<02:06,  1.43s/it]Running Inference:  56%|█████▋    | 113/200 [04:00<01:54,  1.31s/it]Running Inference:  57%|█████▋    | 114/200 [04:01<01:38,  1.15s/it]Running Inference:  57%|█████▊    | 115/200 [04:03<01:57,  1.38s/it]Running Inference:  58%|█████▊    | 116/200 [04:06<02:27,  1.75s/it]Running Inference:  58%|█████▊    | 117/200 [04:08<02:43,  1.98s/it]Running Inference:  59%|█████▉    | 118/200 [04:09<02:09,  1.58s/it]Running Inference:  60%|█████▉    | 119/200 [04:12<02:46,  2.05s/it]Running Inference:  60%|██████    | 120/200 [04:15<03:04,  2.31s/it]Running Inference:  60%|██████    | 121/200 [04:18<03:16,  2.49s/it]Running Inference:  61%|██████    | 122/200 [04:18<02:29,  1.92s/it]Running Inference:  62%|██████▏   | 123/200 [04:21<02:47,  2.17s/it]Running Inference:  62%|██████▏   | 124/200 [04:23<02:37,  2.07s/it]Running Inference:  62%|██████▎   | 125/200 [04:25<02:24,  1.93s/it]Running Inference:  63%|██████▎   | 126/200 [04:27<02:39,  2.16s/it]Running Inference:  64%|██████▎   | 127/200 [04:30<02:43,  2.25s/it]Running Inference:  64%|██████▍   | 128/200 [04:31<02:24,  2.01s/it]Running Inference:  64%|██████▍   | 129/200 [04:33<02:11,  1.85s/it]Running Inference:  65%|██████▌   | 130/200 [04:33<01:47,  1.54s/it]Running Inference:  66%|██████▌   | 131/200 [04:36<02:14,  1.95s/it]Running Inference:  66%|██████▌   | 132/200 [04:40<02:43,  2.41s/it]Running Inference:  66%|██████▋   | 133/200 [04:41<02:23,  2.14s/it]Running Inference:  67%|██████▋   | 134/200 [04:44<02:38,  2.40s/it]Running Inference:  68%|██████▊   | 135/200 [04:49<03:12,  2.96s/it]Running Inference:  68%|██████▊   | 136/200 [04:49<02:26,  2.29s/it]Running Inference:  68%|██████▊   | 137/200 [04:52<02:35,  2.47s/it]Running Inference:  69%|██████▉   | 138/200 [04:53<02:06,  2.04s/it]Running Inference:  70%|██████▉   | 139/200 [04:57<02:39,  2.62s/it]Running Inference:  70%|███████   | 140/200 [04:59<02:17,  2.30s/it]Running Inference:  70%|███████   | 141/200 [05:03<02:40,  2.72s/it]Running Inference:  71%|███████   | 142/200 [05:05<02:41,  2.78s/it]Running Inference:  72%|███████▏  | 143/200 [05:08<02:37,  2.76s/it]Running Inference:  72%|███████▏  | 144/200 [05:11<02:36,  2.80s/it]Running Inference:  72%|███████▎  | 145/200 [05:15<02:49,  3.09s/it]Running Inference:  73%|███████▎  | 146/200 [05:18<02:45,  3.06s/it]Running Inference:  74%|███████▎  | 147/200 [05:21<02:38,  2.98s/it]Running Inference:  74%|███████▍  | 148/200 [05:23<02:31,  2.92s/it]Running Inference:  74%|███████▍  | 149/200 [05:24<01:59,  2.34s/it]Running Inference:  75%|███████▌  | 150/200 [05:26<01:41,  2.04s/it]Running Inference:  76%|███████▌  | 151/200 [05:26<01:13,  1.51s/it]Running Inference:  76%|███████▌  | 152/200 [05:29<01:28,  1.85s/it]Running Inference:  76%|███████▋  | 153/200 [05:31<01:38,  2.10s/it]Running Inference:  77%|███████▋  | 154/200 [05:34<01:46,  2.31s/it]Running Inference:  78%|███████▊  | 155/200 [05:37<01:50,  2.45s/it]Running Inference:  78%|███████▊  | 156/200 [05:40<01:59,  2.72s/it]Running Inference:  78%|███████▊  | 157/200 [05:41<01:27,  2.04s/it]Running Inference:  79%|███████▉  | 158/200 [05:43<01:34,  2.26s/it]Running Inference:  80%|███████▉  | 159/200 [05:47<01:51,  2.71s/it]Running Inference:  80%|████████  | 160/200 [05:51<01:59,  3.00s/it]Running Inference:  80%|████████  | 161/200 [05:53<01:51,  2.86s/it]Running Inference:  81%|████████  | 162/200 [05:56<01:42,  2.70s/it]Running Inference:  82%|████████▏ | 163/200 [05:57<01:23,  2.26s/it]Running Inference:  82%|████████▏ | 164/200 [06:00<01:26,  2.39s/it]Running Inference:  82%|████████▎ | 165/200 [06:02<01:20,  2.30s/it]Running Inference:  83%|████████▎ | 166/200 [06:04<01:15,  2.22s/it]Running Inference:  84%|████████▎ | 167/200 [06:06<01:10,  2.13s/it]Running Inference:  84%|████████▍ | 168/200 [06:06<00:53,  1.67s/it]Running Inference:  84%|████████▍ | 169/200 [06:08<00:51,  1.65s/it]Running Inference:  85%|████████▌ | 170/200 [06:11<01:01,  2.05s/it]Running Inference:  86%|████████▌ | 171/200 [06:12<00:51,  1.79s/it]Running Inference:  86%|████████▌ | 172/200 [06:13<00:43,  1.55s/it]Running Inference:  86%|████████▋ | 173/200 [06:16<00:53,  1.97s/it]Running Inference:  87%|████████▋ | 174/200 [06:19<00:57,  2.22s/it]Running Inference:  88%|████████▊ | 175/200 [06:23<01:08,  2.74s/it]Running Inference:  88%|████████▊ | 176/200 [06:26<01:07,  2.81s/it]Running Inference:  88%|████████▊ | 177/200 [06:28<01:04,  2.78s/it]Running Inference:  89%|████████▉ | 178/200 [06:31<00:56,  2.59s/it]Running Inference:  90%|████████▉ | 179/200 [06:34<01:01,  2.94s/it]Running Inference:  90%|█████████ | 180/200 [06:36<00:49,  2.45s/it]Running Inference:  90%|█████████ | 181/200 [06:37<00:38,  2.02s/it]Running Inference:  91%|█████████ | 182/200 [06:38<00:34,  1.94s/it]Running Inference:  92%|█████████▏| 183/200 [06:40<00:30,  1.81s/it]Running Inference:  92%|█████████▏| 184/200 [06:41<00:24,  1.51s/it]Running Inference:  92%|█████████▎| 185/200 [06:43<00:26,  1.75s/it]Running Inference:  93%|█████████▎| 186/200 [06:44<00:20,  1.43s/it]Running Inference:  94%|█████████▎| 187/200 [06:47<00:25,  1.93s/it]Running Inference:  94%|█████████▍| 188/200 [06:50<00:26,  2.18s/it]Running Inference:  94%|█████████▍| 189/200 [06:52<00:26,  2.38s/it]Running Inference:  95%|█████████▌| 190/200 [06:53<00:17,  1.76s/it]Running Inference:  96%|█████████▌| 191/200 [06:56<00:21,  2.34s/it]Running Inference:  96%|█████████▌| 192/200 [06:58<00:15,  1.98s/it]Running Inference:  96%|█████████▋| 193/200 [07:01<00:17,  2.50s/it]Running Inference:  97%|█████████▋| 194/200 [07:02<00:11,  1.89s/it]Running Inference:  98%|█████████▊| 195/200 [07:05<00:11,  2.25s/it]Running Inference:  98%|█████████▊| 196/200 [07:06<00:07,  1.91s/it]Running Inference:  98%|█████████▊| 197/200 [07:07<00:05,  1.68s/it]Running Inference:  99%|█████████▉| 198/200 [07:08<00:02,  1.39s/it]Running Inference: 100%|█████████▉| 199/200 [07:09<00:01,  1.21s/it]Running Inference: 100%|██████████| 200/200 [07:10<00:00,  1.12s/it]Running Inference: 100%|██████████| 200/200 [07:10<00:00,  2.15s/it]
2025-12-13 18:01:46,153 - INFO - Inference completed.
2025-12-13 18:01:46,163 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 18:01:46,163 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:01:46,168 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 18:01:46,168 - INFO - Metrics:
19.93
2025-12-13 18:01:46,170 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa, ratio=0.1) on GPU 2

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:01:52,535 - INFO - Set deterministic seeds to 42
2025-12-13 18:01:52,535 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:01:52,535 - INFO - Starting evaluation run...
2025-12-13 18:01:52,536 - INFO - Output directory set to: longbenchresult
2025-12-13 18:01:52,536 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 18:01:52,536 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 18:01:52,536 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:01:52,536 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.52it/s]
Device set to use cuda:0
2025-12-13 18:02:04,508 - INFO - Model pipeline loaded.
2025-12-13 18:02:04,509 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:02:08,546 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:02:08,546 - INFO - Dataset processed with 200 entries.
2025-12-13 18:02:08,565 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<13:48,  4.16s/it]Running Inference:   1%|          | 2/200 [00:07<11:16,  3.42s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:44,  3.27s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:50,  3.01s/it]Running Inference:   2%|▎         | 5/200 [00:16<11:06,  3.42s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:03,  2.80s/it]Running Inference:   4%|▎         | 7/200 [00:19<07:12,  2.24s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:06,  2.53s/it]Running Inference:   4%|▍         | 9/200 [00:26<08:48,  2.77s/it]Running Inference:   5%|▌         | 10/200 [00:26<06:39,  2.10s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:30<07:58,  2.53s/it]Running Inference:   6%|▌         | 12/200 [00:31<07:08,  2.28s/it]Running Inference:   6%|▋         | 13/200 [00:35<08:14,  2.65s/it]Running Inference:   7%|▋         | 14/200 [00:35<06:14,  2.01s/it]Running Inference:   8%|▊         | 15/200 [00:36<05:11,  1.69s/it]Running Inference:   8%|▊         | 16/200 [00:37<04:27,  1.45s/it]Running Inference:   8%|▊         | 17/200 [00:39<05:07,  1.68s/it]Running Inference:   9%|▉         | 18/200 [00:40<04:26,  1.47s/it]Running Inference:  10%|▉         | 19/200 [00:43<05:36,  1.86s/it]Running Inference:  10%|█         | 20/200 [00:43<04:09,  1.39s/it]Running Inference:  10%|█         | 21/200 [00:46<05:10,  1.74s/it]Running Inference:  11%|█         | 22/200 [00:49<06:06,  2.06s/it]Running Inference:  12%|█▏        | 23/200 [00:52<07:15,  2.46s/it]Running Inference:  12%|█▏        | 24/200 [00:55<07:31,  2.56s/it]Running Inference:  12%|█▎        | 25/200 [00:58<08:00,  2.74s/it]Running Inference:  13%|█▎        | 26/200 [01:01<08:04,  2.79s/it]Running Inference:  14%|█▎        | 27/200 [01:03<07:33,  2.62s/it]Running Inference:  14%|█▍        | 28/200 [01:04<05:54,  2.06s/it]Running Inference:  14%|█▍        | 29/200 [01:07<06:25,  2.26s/it]Running Inference:  15%|█▌        | 30/200 [01:07<04:41,  1.66s/it]Running Inference:  16%|█▌        | 31/200 [01:09<04:57,  1.76s/it]Running Inference:  16%|█▌        | 32/200 [01:11<04:43,  1.69s/it]Running Inference:  16%|█▋        | 33/200 [01:13<05:38,  2.03s/it]Running Inference:  17%|█▋        | 34/200 [01:16<06:14,  2.26s/it]Running Inference:  18%|█▊        | 35/200 [01:19<06:43,  2.45s/it]Running Inference:  18%|█▊        | 36/200 [01:20<05:24,  1.98s/it]Running Inference:  18%|█▊        | 37/200 [01:21<04:43,  1.74s/it]Running Inference:  19%|█▉        | 38/200 [01:23<04:28,  1.66s/it]Running Inference:  20%|█▉        | 39/200 [01:25<05:25,  2.02s/it]Running Inference:  20%|██        | 40/200 [01:28<06:05,  2.28s/it]Running Inference:  20%|██        | 41/200 [01:30<05:12,  1.96s/it]Running Inference:  21%|██        | 42/200 [01:31<04:48,  1.83s/it]Running Inference:  22%|██▏       | 43/200 [01:32<04:03,  1.55s/it]Running Inference:  22%|██▏       | 44/200 [01:33<03:18,  1.27s/it]Running Inference:  22%|██▎       | 45/200 [01:34<03:03,  1.18s/it]Running Inference:  23%|██▎       | 46/200 [01:37<04:24,  1.72s/it]Running Inference:  24%|██▎       | 47/200 [01:40<05:24,  2.12s/it]Running Inference:  24%|██▍       | 48/200 [01:41<04:29,  1.77s/it]Running Inference:  24%|██▍       | 49/200 [01:42<03:55,  1.56s/it]Running Inference:  25%|██▌       | 50/200 [01:44<04:50,  1.94s/it]Running Inference:  26%|██▌       | 51/200 [01:47<04:58,  2.00s/it]Running Inference:  26%|██▌       | 52/200 [01:49<05:14,  2.13s/it]Running Inference:  26%|██▋       | 53/200 [01:52<05:36,  2.29s/it]Running Inference:  27%|██▋       | 54/200 [01:53<04:56,  2.03s/it]Running Inference:  28%|██▊       | 55/200 [01:54<04:02,  1.68s/it]Running Inference:  28%|██▊       | 56/200 [01:55<03:42,  1.54s/it]Running Inference:  28%|██▊       | 57/200 [01:58<04:33,  1.91s/it]Running Inference:  29%|██▉       | 58/200 [01:59<04:08,  1.75s/it]Running Inference:  30%|██▉       | 59/200 [02:02<04:47,  2.04s/it]Running Inference:  30%|███       | 60/200 [02:03<04:06,  1.76s/it]Running Inference:  30%|███       | 61/200 [02:06<04:45,  2.05s/it]Running Inference:  31%|███       | 62/200 [02:08<04:36,  2.00s/it]Running Inference:  32%|███▏      | 63/200 [02:08<03:22,  1.48s/it]Running Inference:  32%|███▏      | 64/200 [02:10<03:40,  1.62s/it]Running Inference:  32%|███▎      | 65/200 [02:11<03:23,  1.50s/it]Running Inference:  33%|███▎      | 66/200 [02:14<03:56,  1.77s/it]Running Inference:  34%|███▎      | 67/200 [02:17<04:49,  2.18s/it]Running Inference:  34%|███▍      | 68/200 [02:20<05:32,  2.52s/it]Running Inference:  34%|███▍      | 69/200 [02:21<04:36,  2.11s/it]Running Inference:  35%|███▌      | 70/200 [02:22<03:51,  1.78s/it]Running Inference:  36%|███▌      | 71/200 [02:23<03:03,  1.42s/it]Running Inference:  36%|███▌      | 72/200 [02:26<04:09,  1.95s/it]Running Inference:  36%|███▋      | 73/200 [02:27<03:33,  1.68s/it]Running Inference:  37%|███▋      | 74/200 [02:29<03:46,  1.80s/it]Running Inference:  38%|███▊      | 75/200 [02:30<03:18,  1.59s/it]Running Inference:  38%|███▊      | 76/200 [02:32<03:38,  1.76s/it]Running Inference:  38%|███▊      | 77/200 [02:33<03:10,  1.55s/it]Running Inference:  39%|███▉      | 78/200 [02:37<04:12,  2.07s/it]Running Inference:  40%|███▉      | 79/200 [02:40<04:43,  2.34s/it]Running Inference:  40%|████      | 80/200 [02:43<05:20,  2.67s/it]Running Inference:  40%|████      | 81/200 [02:46<05:22,  2.71s/it]Running Inference:  41%|████      | 82/200 [02:49<05:29,  2.80s/it]Running Inference:  42%|████▏     | 83/200 [02:50<04:41,  2.41s/it]Running Inference:  42%|████▏     | 84/200 [02:53<04:58,  2.57s/it]Running Inference:  42%|████▎     | 85/200 [02:56<05:10,  2.70s/it]Running Inference:  43%|████▎     | 86/200 [03:00<05:44,  3.02s/it]Running Inference:  44%|████▎     | 87/200 [03:03<05:31,  2.94s/it]Running Inference:  44%|████▍     | 88/200 [03:06<05:28,  2.93s/it]Running Inference:  44%|████▍     | 89/200 [03:09<05:21,  2.90s/it]Running Inference:  45%|████▌     | 90/200 [03:10<04:15,  2.32s/it]Running Inference:  46%|████▌     | 91/200 [03:12<04:26,  2.44s/it]Running Inference:  46%|████▌     | 92/200 [03:15<04:40,  2.60s/it]Running Inference:  46%|████▋     | 93/200 [03:18<04:55,  2.77s/it]Running Inference:  47%|████▋     | 94/200 [03:19<03:38,  2.06s/it]Running Inference:  48%|████▊     | 95/200 [03:20<02:53,  1.66s/it]Running Inference:  48%|████▊     | 96/200 [03:22<03:28,  2.00s/it]Running Inference:  48%|████▊     | 97/200 [03:25<03:34,  2.09s/it]Running Inference:  49%|████▉     | 98/200 [03:28<03:59,  2.35s/it]Running Inference:  50%|████▉     | 99/200 [03:30<04:10,  2.48s/it]Running Inference:  50%|█████     | 100/200 [03:32<03:46,  2.27s/it]Running Inference:  50%|█████     | 101/200 [03:35<03:57,  2.40s/it]Running Inference:  51%|█████     | 102/200 [03:38<04:06,  2.51s/it]Running Inference:  52%|█████▏    | 103/200 [03:40<04:09,  2.57s/it]Running Inference:  52%|█████▏    | 104/200 [03:43<04:15,  2.66s/it]Running Inference:  52%|█████▎    | 105/200 [03:44<03:21,  2.12s/it]Running Inference:  53%|█████▎    | 106/200 [03:45<02:46,  1.77s/it]Running Inference:  54%|█████▎    | 107/200 [03:48<03:08,  2.03s/it]Running Inference:  54%|█████▍    | 108/200 [03:51<03:29,  2.28s/it]Running Inference:  55%|█████▍    | 109/200 [03:52<02:56,  1.94s/it]Running Inference:  55%|█████▌    | 110/200 [03:53<02:44,  1.82s/it]Running Inference:  56%|█████▌    | 111/200 [03:54<02:11,  1.47s/it]Running Inference:  56%|█████▌    | 112/200 [03:57<02:47,  1.90s/it]Running Inference:  56%|█████▋    | 113/200 [03:58<02:22,  1.64s/it]Running Inference:  57%|█████▋    | 114/200 [04:01<02:51,  1.99s/it]Running Inference:  57%|█████▊    | 115/200 [04:03<02:47,  1.97s/it]Running Inference:  58%|█████▊    | 116/200 [04:05<03:01,  2.16s/it]Running Inference:  58%|█████▊    | 117/200 [04:09<03:44,  2.71s/it]Running Inference:  59%|█████▉    | 118/200 [04:10<02:51,  2.10s/it]Running Inference:  60%|█████▉    | 119/200 [04:13<03:15,  2.41s/it]Running Inference:  60%|██████    | 120/200 [04:16<03:25,  2.56s/it]Running Inference:  60%|██████    | 121/200 [04:19<03:33,  2.70s/it]Running Inference:  61%|██████    | 122/200 [04:20<02:41,  2.07s/it]Running Inference:  62%|██████▏   | 123/200 [04:22<02:55,  2.28s/it]Running Inference:  62%|██████▏   | 124/200 [04:24<02:38,  2.08s/it]Running Inference:  62%|██████▎   | 125/200 [04:29<03:41,  2.96s/it]Running Inference:  63%|██████▎   | 126/200 [04:32<03:35,  2.91s/it]Running Inference:  64%|██████▎   | 127/200 [04:34<03:23,  2.79s/it]Running Inference:  64%|██████▍   | 128/200 [04:37<03:20,  2.78s/it]Running Inference:  64%|██████▍   | 129/200 [04:38<02:49,  2.39s/it]Running Inference:  65%|██████▌   | 130/200 [04:39<02:14,  1.92s/it]Running Inference:  66%|██████▌   | 131/200 [04:42<02:32,  2.21s/it]Running Inference:  66%|██████▌   | 132/200 [04:45<02:46,  2.45s/it]Running Inference:  66%|██████▋   | 133/200 [04:47<02:25,  2.17s/it]Running Inference:  67%|██████▋   | 134/200 [04:50<02:39,  2.42s/it]Running Inference:  68%|██████▊   | 135/200 [04:54<03:12,  2.97s/it]Running Inference:  68%|██████▊   | 136/200 [04:57<03:04,  2.89s/it]Running Inference:  68%|██████▊   | 137/200 [05:00<03:01,  2.88s/it]Running Inference:  69%|██████▉   | 138/200 [05:01<02:24,  2.33s/it]Running Inference:  70%|██████▉   | 139/200 [05:05<02:52,  2.84s/it]Running Inference:  70%|███████   | 140/200 [05:06<02:27,  2.45s/it]Running Inference:  70%|███████   | 141/200 [05:10<02:47,  2.83s/it]Running Inference:  71%|███████   | 142/200 [05:13<02:46,  2.86s/it]Running Inference:  72%|███████▏  | 143/200 [05:16<02:40,  2.82s/it]Running Inference:  72%|███████▏  | 144/200 [05:18<02:39,  2.85s/it]Running Inference:  72%|███████▎  | 145/200 [05:22<02:52,  3.13s/it]Running Inference:  73%|███████▎  | 146/200 [05:24<02:27,  2.73s/it]Running Inference:  74%|███████▎  | 147/200 [05:27<02:25,  2.75s/it]Running Inference:  74%|███████▍  | 148/200 [05:30<02:23,  2.76s/it]Running Inference:  74%|███████▍  | 149/200 [05:32<02:21,  2.77s/it]Running Inference:  75%|███████▌  | 150/200 [05:34<02:01,  2.42s/it]Running Inference:  76%|███████▌  | 151/200 [05:34<01:27,  1.78s/it]Running Inference:  76%|███████▌  | 152/200 [05:37<01:37,  2.04s/it]Running Inference:  76%|███████▋  | 153/200 [05:40<01:45,  2.24s/it]Running Inference:  77%|███████▋  | 154/200 [05:42<01:50,  2.41s/it]Running Inference:  78%|███████▊  | 155/200 [05:45<01:53,  2.53s/it]Running Inference:  78%|███████▊  | 156/200 [05:49<02:02,  2.78s/it]Running Inference:  78%|███████▊  | 157/200 [05:49<01:29,  2.08s/it]Running Inference:  79%|███████▉  | 158/200 [05:52<01:36,  2.29s/it]Running Inference:  80%|███████▉  | 159/200 [05:56<01:52,  2.74s/it]Running Inference:  80%|████████  | 160/200 [05:57<01:35,  2.38s/it]Running Inference:  80%|████████  | 161/200 [06:00<01:34,  2.43s/it]Running Inference:  81%|████████  | 162/200 [06:02<01:30,  2.39s/it]Running Inference:  82%|████████▏ | 163/200 [06:03<01:14,  2.01s/it]Running Inference:  82%|████████▏ | 164/200 [06:06<01:19,  2.22s/it]Running Inference:  82%|████████▎ | 165/200 [06:07<01:02,  1.79s/it]Running Inference:  83%|████████▎ | 166/200 [06:09<01:03,  1.86s/it]Running Inference:  84%|████████▎ | 167/200 [06:11<01:01,  1.87s/it]Running Inference:  84%|████████▍ | 168/200 [06:11<00:49,  1.55s/it]Running Inference:  84%|████████▍ | 169/200 [06:13<00:48,  1.57s/it]Running Inference:  85%|████████▌ | 170/200 [06:16<00:59,  1.98s/it]Running Inference:  86%|████████▌ | 171/200 [06:17<00:50,  1.73s/it]Running Inference:  86%|████████▌ | 172/200 [06:18<00:42,  1.51s/it]Running Inference:  86%|████████▋ | 173/200 [06:20<00:42,  1.59s/it]Running Inference:  87%|████████▋ | 174/200 [06:23<00:50,  1.95s/it]Running Inference:  88%|████████▊ | 175/200 [06:27<01:03,  2.56s/it]Running Inference:  88%|████████▊ | 176/200 [06:30<01:04,  2.67s/it]Running Inference:  88%|████████▊ | 177/200 [06:32<01:01,  2.66s/it]Running Inference:  89%|████████▉ | 178/200 [06:34<00:55,  2.50s/it]Running Inference:  90%|████████▉ | 179/200 [06:38<01:00,  2.88s/it]Running Inference:  90%|█████████ | 180/200 [06:39<00:48,  2.41s/it]Running Inference:  90%|█████████ | 181/200 [06:40<00:37,  1.99s/it]Running Inference:  91%|█████████ | 182/200 [06:42<00:34,  1.92s/it]Running Inference:  92%|█████████▏| 183/200 [06:44<00:30,  1.80s/it]Running Inference:  92%|█████████▏| 184/200 [06:44<00:23,  1.48s/it]Running Inference:  92%|█████████▎| 185/200 [06:47<00:25,  1.73s/it]Running Inference:  93%|█████████▎| 186/200 [06:47<00:19,  1.42s/it]Running Inference:  94%|█████████▎| 187/200 [06:51<00:24,  1.92s/it]Running Inference:  94%|█████████▍| 188/200 [06:52<00:21,  1.78s/it]Running Inference:  94%|█████████▍| 189/200 [06:55<00:23,  2.10s/it]Running Inference:  95%|█████████▌| 190/200 [06:55<00:15,  1.57s/it]Running Inference:  96%|█████████▌| 191/200 [06:59<00:19,  2.21s/it]Running Inference:  96%|█████████▌| 192/200 [07:02<00:19,  2.44s/it]Running Inference:  96%|█████████▋| 193/200 [07:06<00:19,  2.83s/it]Running Inference:  97%|█████████▋| 194/200 [07:06<00:12,  2.12s/it]Running Inference:  98%|█████████▊| 195/200 [07:11<00:15,  3.06s/it]Running Inference:  98%|█████████▊| 196/200 [07:12<00:09,  2.47s/it]Running Inference:  98%|█████████▊| 197/200 [07:13<00:06,  2.07s/it]Running Inference:  99%|█████████▉| 198/200 [07:14<00:03,  1.63s/it]Running Inference: 100%|█████████▉| 199/200 [07:15<00:01,  1.38s/it]Running Inference: 100%|██████████| 200/200 [07:16<00:00,  1.23s/it]Running Inference: 100%|██████████| 200/200 [07:16<00:00,  2.18s/it]
2025-12-13 18:09:24,855 - INFO - Inference completed.
2025-12-13 18:09:24,864 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 18:09:24,864 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:09:24,869 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 18:09:24,869 - INFO - Metrics:
18.68
2025-12-13 18:09:24,871 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa, ratio=0.2) on GPU 2

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:09:31,256 - INFO - Set deterministic seeds to 42
2025-12-13 18:09:31,256 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:09:31,256 - INFO - Starting evaluation run...
2025-12-13 18:09:31,256 - INFO - Output directory set to: longbenchresult
2025-12-13 18:09:31,256 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 18:09:31,256 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 18:09:31,256 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:09:31,256 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.55it/s]
Device set to use cuda:0
2025-12-13 18:09:43,257 - INFO - Model pipeline loaded.
2025-12-13 18:09:43,257 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:09:49,023 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:09:49,023 - INFO - Dataset processed with 200 entries.
2025-12-13 18:09:49,043 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:05,  4.25s/it]Running Inference:   1%|          | 2/200 [00:07<11:20,  3.44s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:44,  3.27s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:50,  3.01s/it]Running Inference:   2%|▎         | 5/200 [00:15<09:21,  2.88s/it]Running Inference:   3%|▎         | 6/200 [00:17<07:54,  2.44s/it]Running Inference:   4%|▎         | 7/200 [00:18<06:25,  2.00s/it]Running Inference:   4%|▍         | 8/200 [00:21<07:33,  2.36s/it]Running Inference:   4%|▍         | 9/200 [00:24<08:24,  2.64s/it]Running Inference:   5%|▌         | 10/200 [00:25<06:22,  2.01s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:28<07:46,  2.47s/it]Running Inference:   6%|▌         | 12/200 [00:30<06:59,  2.23s/it]Running Inference:   6%|▋         | 13/200 [00:33<08:07,  2.61s/it]Running Inference:   7%|▋         | 14/200 [00:34<06:08,  1.98s/it]Running Inference:   8%|▊         | 15/200 [00:35<05:07,  1.66s/it]Running Inference:   8%|▊         | 16/200 [00:36<04:24,  1.44s/it]Running Inference:   8%|▊         | 17/200 [00:38<05:05,  1.67s/it]Running Inference:   9%|▉         | 18/200 [00:39<04:25,  1.46s/it]Running Inference:  10%|▉         | 19/200 [00:42<05:33,  1.84s/it]Running Inference:  10%|█         | 20/200 [00:42<04:07,  1.38s/it]Running Inference:  10%|█         | 21/200 [00:44<05:13,  1.75s/it]Running Inference:  11%|█         | 22/200 [00:47<06:07,  2.06s/it]Running Inference:  12%|█▏        | 23/200 [00:51<07:14,  2.45s/it]Running Inference:  12%|█▏        | 24/200 [00:53<07:29,  2.56s/it]Running Inference:  12%|█▎        | 25/200 [00:55<06:08,  2.11s/it]Running Inference:  13%|█▎        | 26/200 [00:57<06:45,  2.33s/it]Running Inference:  14%|█▎        | 27/200 [01:00<07:00,  2.43s/it]Running Inference:  14%|█▍        | 28/200 [01:01<05:32,  1.93s/it]Running Inference:  14%|█▍        | 29/200 [01:03<06:09,  2.16s/it]Running Inference:  15%|█▌        | 30/200 [01:04<04:41,  1.66s/it]Running Inference:  16%|█▌        | 31/200 [01:06<04:56,  1.76s/it]Running Inference:  16%|█▌        | 32/200 [01:07<04:42,  1.68s/it]Running Inference:  16%|█▋        | 33/200 [01:10<05:36,  2.02s/it]Running Inference:  17%|█▋        | 34/200 [01:13<06:12,  2.24s/it]Running Inference:  18%|█▊        | 35/200 [01:16<06:40,  2.43s/it]Running Inference:  18%|█▊        | 36/200 [01:17<05:22,  1.97s/it]Running Inference:  18%|█▊        | 37/200 [01:18<04:42,  1.73s/it]Running Inference:  19%|█▉        | 38/200 [01:19<04:27,  1.65s/it]Running Inference:  20%|█▉        | 39/200 [01:22<05:23,  2.01s/it]Running Inference:  20%|██        | 40/200 [01:25<06:02,  2.27s/it]Running Inference:  20%|██        | 41/200 [01:28<06:28,  2.44s/it]Running Inference:  21%|██        | 42/200 [01:29<05:41,  2.16s/it]Running Inference:  22%|██▏       | 43/200 [01:30<04:40,  1.79s/it]Running Inference:  22%|██▏       | 44/200 [01:31<03:43,  1.43s/it]Running Inference:  22%|██▎       | 45/200 [01:32<03:41,  1.43s/it]Running Inference:  23%|██▎       | 46/200 [01:35<04:50,  1.88s/it]Running Inference:  24%|██▎       | 47/200 [01:38<05:41,  2.24s/it]Running Inference:  24%|██▍       | 48/200 [01:40<04:58,  1.97s/it]Running Inference:  24%|██▍       | 49/200 [01:41<04:15,  1.69s/it]Running Inference:  25%|██▌       | 50/200 [01:44<05:02,  2.02s/it]Running Inference:  26%|██▌       | 51/200 [01:46<05:12,  2.10s/it]Running Inference:  26%|██▌       | 52/200 [01:48<05:23,  2.19s/it]Running Inference:  26%|██▋       | 53/200 [01:51<05:41,  2.32s/it]Running Inference:  27%|██▋       | 54/200 [01:52<05:00,  2.06s/it]Running Inference:  28%|██▊       | 55/200 [01:53<04:04,  1.69s/it]Running Inference:  28%|██▊       | 56/200 [01:54<03:43,  1.55s/it]Running Inference:  28%|██▊       | 57/200 [01:56<04:02,  1.69s/it]Running Inference:  29%|██▉       | 58/200 [01:58<03:46,  1.59s/it]Running Inference:  30%|██▉       | 59/200 [01:59<03:11,  1.36s/it]Running Inference:  30%|███       | 60/200 [02:02<04:17,  1.84s/it]Running Inference:  30%|███       | 61/200 [02:04<04:51,  2.10s/it]Running Inference:  31%|███       | 62/200 [02:06<04:40,  2.03s/it]Running Inference:  32%|███▏      | 63/200 [02:06<03:24,  1.50s/it]Running Inference:  32%|███▏      | 64/200 [02:10<05:01,  2.22s/it]Running Inference:  32%|███▎      | 65/200 [02:12<04:19,  1.92s/it]Running Inference:  33%|███▎      | 66/200 [02:14<04:31,  2.03s/it]Running Inference:  34%|███▎      | 67/200 [02:17<05:12,  2.35s/it]Running Inference:  34%|███▍      | 68/200 [02:20<05:47,  2.63s/it]Running Inference:  34%|███▍      | 69/200 [02:21<04:46,  2.19s/it]Running Inference:  35%|███▌      | 70/200 [02:24<05:13,  2.41s/it]Running Inference:  36%|███▌      | 71/200 [02:27<05:18,  2.47s/it]Running Inference:  36%|███▌      | 72/200 [02:30<05:41,  2.67s/it]Running Inference:  36%|███▋      | 73/200 [02:31<04:37,  2.18s/it]Running Inference:  37%|███▋      | 74/200 [02:33<04:31,  2.15s/it]Running Inference:  38%|███▊      | 75/200 [02:34<03:49,  1.84s/it]Running Inference:  38%|███▊      | 76/200 [02:37<04:31,  2.19s/it]Running Inference:  38%|███▊      | 77/200 [02:38<03:48,  1.86s/it]Running Inference:  39%|███▉      | 78/200 [02:42<04:40,  2.30s/it]Running Inference:  40%|███▉      | 79/200 [02:45<05:02,  2.50s/it]Running Inference:  40%|████      | 80/200 [02:48<05:22,  2.69s/it]Running Inference:  40%|████      | 81/200 [02:50<05:13,  2.63s/it]Running Inference:  41%|████      | 82/200 [02:53<05:24,  2.75s/it]Running Inference:  42%|████▏     | 83/200 [02:55<04:37,  2.37s/it]Running Inference:  42%|████▏     | 84/200 [02:58<04:55,  2.55s/it]Running Inference:  42%|████▎     | 85/200 [03:01<05:06,  2.67s/it]Running Inference:  43%|████▎     | 86/200 [03:04<05:41,  3.00s/it]Running Inference:  44%|████▎     | 87/200 [03:07<05:29,  2.91s/it]Running Inference:  44%|████▍     | 88/200 [03:10<05:25,  2.91s/it]Running Inference:  44%|████▍     | 89/200 [03:13<05:19,  2.87s/it]Running Inference:  45%|████▌     | 90/200 [03:14<04:13,  2.30s/it]Running Inference:  46%|████▌     | 91/200 [03:17<04:23,  2.42s/it]Running Inference:  46%|████▌     | 92/200 [03:19<04:38,  2.58s/it]Running Inference:  46%|████▋     | 93/200 [03:23<04:53,  2.74s/it]Running Inference:  47%|████▋     | 94/200 [03:23<03:36,  2.05s/it]Running Inference:  48%|████▊     | 95/200 [03:25<03:44,  2.14s/it]Running Inference:  48%|████▊     | 96/200 [03:28<04:02,  2.33s/it]Running Inference:  48%|████▊     | 97/200 [03:30<03:57,  2.31s/it]Running Inference:  49%|████▉     | 98/200 [03:33<04:14,  2.50s/it]Running Inference:  50%|████▉     | 99/200 [03:35<03:32,  2.11s/it]Running Inference:  50%|█████     | 100/200 [03:37<03:30,  2.10s/it]Running Inference:  50%|█████     | 101/200 [03:39<03:45,  2.28s/it]Running Inference:  51%|█████     | 102/200 [03:42<03:57,  2.42s/it]Running Inference:  52%|█████▏    | 103/200 [03:45<04:02,  2.50s/it]Running Inference:  52%|█████▏    | 104/200 [03:48<04:10,  2.61s/it]Running Inference:  52%|█████▎    | 105/200 [03:48<03:16,  2.07s/it]Running Inference:  53%|█████▎    | 106/200 [03:49<02:42,  1.73s/it]Running Inference:  54%|█████▎    | 107/200 [03:53<03:29,  2.25s/it]Running Inference:  54%|█████▍    | 108/200 [03:56<03:43,  2.42s/it]Running Inference:  55%|█████▍    | 109/200 [03:57<03:12,  2.11s/it]Running Inference:  55%|█████▌    | 110/200 [03:59<02:54,  1.94s/it]Running Inference:  56%|█████▌    | 111/200 [03:59<02:18,  1.56s/it]Running Inference:  56%|█████▌    | 112/200 [04:02<02:52,  1.96s/it]Running Inference:  56%|█████▋    | 113/200 [04:03<02:25,  1.68s/it]Running Inference:  57%|█████▋    | 114/200 [04:06<02:52,  2.01s/it]Running Inference:  57%|█████▊    | 115/200 [04:08<02:48,  1.98s/it]Running Inference:  58%|█████▊    | 116/200 [04:12<03:34,  2.56s/it]Running Inference:  58%|█████▊    | 117/200 [04:16<04:07,  2.98s/it]Running Inference:  59%|█████▉    | 118/200 [04:16<03:07,  2.29s/it]Running Inference:  60%|█████▉    | 119/200 [04:20<03:25,  2.54s/it]Running Inference:  60%|██████    | 120/200 [04:22<03:31,  2.64s/it]Running Inference:  60%|██████    | 121/200 [04:25<03:34,  2.72s/it]Running Inference:  61%|██████    | 122/200 [04:26<02:42,  2.08s/it]Running Inference:  62%|██████▏   | 123/200 [04:29<02:55,  2.27s/it]Running Inference:  62%|██████▏   | 124/200 [04:32<03:08,  2.48s/it]Running Inference:  62%|██████▎   | 125/200 [04:34<03:04,  2.45s/it]Running Inference:  63%|██████▎   | 126/200 [04:37<03:06,  2.52s/it]Running Inference:  64%|██████▎   | 127/200 [04:38<02:45,  2.26s/it]Running Inference:  64%|██████▍   | 128/200 [04:41<02:47,  2.32s/it]Running Inference:  64%|██████▍   | 129/200 [04:42<02:26,  2.06s/it]Running Inference:  65%|██████▌   | 130/200 [04:43<01:58,  1.69s/it]Running Inference:  66%|██████▌   | 131/200 [04:46<02:20,  2.04s/it]Running Inference:  66%|██████▌   | 132/200 [04:49<02:41,  2.38s/it]Running Inference:  66%|██████▋   | 133/200 [04:51<02:21,  2.12s/it]Running Inference:  67%|██████▋   | 134/200 [04:54<02:37,  2.39s/it]Running Inference:  68%|██████▊   | 135/200 [04:58<03:11,  2.94s/it]Running Inference:  68%|██████▊   | 136/200 [05:00<02:48,  2.63s/it]Running Inference:  68%|██████▊   | 137/200 [05:03<02:50,  2.70s/it]Running Inference:  69%|██████▉   | 138/200 [05:04<02:16,  2.21s/it]Running Inference:  70%|██████▉   | 139/200 [05:08<02:48,  2.76s/it]Running Inference:  70%|███████   | 140/200 [05:09<02:24,  2.40s/it]Running Inference:  70%|███████   | 141/200 [05:13<02:44,  2.79s/it]Running Inference:  71%|███████   | 142/200 [05:14<02:11,  2.27s/it]Running Inference:  72%|███████▏  | 143/200 [05:16<02:10,  2.29s/it]Running Inference:  72%|███████▏  | 144/200 [05:19<02:18,  2.48s/it]Running Inference:  72%|███████▎  | 145/200 [05:23<02:37,  2.86s/it]Running Inference:  73%|███████▎  | 146/200 [05:25<02:16,  2.53s/it]Running Inference:  74%|███████▎  | 147/200 [05:28<02:18,  2.62s/it]Running Inference:  74%|███████▍  | 148/200 [05:30<02:18,  2.66s/it]Running Inference:  74%|███████▍  | 149/200 [05:33<02:17,  2.69s/it]Running Inference:  75%|███████▌  | 150/200 [05:36<02:20,  2.81s/it]Running Inference:  76%|███████▌  | 151/200 [05:38<02:00,  2.47s/it]Running Inference:  76%|███████▌  | 152/200 [05:41<02:00,  2.52s/it]Running Inference:  76%|███████▋  | 153/200 [05:43<02:00,  2.57s/it]Running Inference:  77%|███████▋  | 154/200 [05:46<02:00,  2.63s/it]Running Inference:  78%|███████▊  | 155/200 [05:49<02:00,  2.68s/it]Running Inference:  78%|███████▊  | 156/200 [05:50<01:41,  2.30s/it]Running Inference:  78%|███████▊  | 157/200 [05:51<01:14,  1.74s/it]Running Inference:  79%|███████▉  | 158/200 [05:53<01:26,  2.05s/it]Running Inference:  80%|███████▉  | 159/200 [05:57<01:44,  2.56s/it]Running Inference:  80%|████████  | 160/200 [05:59<01:29,  2.24s/it]Running Inference:  80%|████████  | 161/200 [06:01<01:30,  2.33s/it]Running Inference:  81%|████████  | 162/200 [06:04<01:28,  2.33s/it]Running Inference:  82%|████████▏ | 163/200 [06:05<01:13,  2.00s/it]Running Inference:  82%|████████▏ | 164/200 [06:07<01:19,  2.21s/it]Running Inference:  82%|████████▎ | 165/200 [06:08<01:02,  1.78s/it]Running Inference:  83%|████████▎ | 166/200 [06:12<01:22,  2.43s/it]Running Inference:  84%|████████▎ | 167/200 [06:14<01:15,  2.27s/it]Running Inference:  84%|████████▍ | 168/200 [06:15<00:56,  1.75s/it]Running Inference:  84%|████████▍ | 169/200 [06:16<00:52,  1.71s/it]Running Inference:  85%|████████▌ | 170/200 [06:19<01:02,  2.08s/it]Running Inference:  86%|████████▌ | 171/200 [06:22<01:07,  2.32s/it]Running Inference:  86%|████████▌ | 172/200 [06:25<01:09,  2.49s/it]Running Inference:  86%|████████▋ | 173/200 [06:28<01:10,  2.62s/it]Running Inference:  87%|████████▋ | 174/200 [06:31<01:09,  2.67s/it]Running Inference:  88%|████████▊ | 175/200 [06:35<01:16,  3.05s/it]Running Inference:  88%|████████▊ | 176/200 [06:37<01:08,  2.84s/it]Running Inference:  88%|████████▊ | 177/200 [06:40<01:04,  2.78s/it]Running Inference:  89%|████████▉ | 178/200 [06:42<00:56,  2.59s/it]Running Inference:  90%|████████▉ | 179/200 [06:45<01:01,  2.93s/it]Running Inference:  90%|█████████ | 180/200 [06:47<00:48,  2.44s/it]Running Inference:  90%|█████████ | 181/200 [06:48<00:38,  2.02s/it]Running Inference:  91%|█████████ | 182/200 [06:50<00:34,  1.93s/it]Running Inference:  92%|█████████▏| 183/200 [06:52<00:37,  2.21s/it]Running Inference:  92%|█████████▏| 184/200 [06:53<00:28,  1.77s/it]Running Inference:  92%|█████████▎| 185/200 [06:55<00:28,  1.93s/it]Running Inference:  93%|█████████▎| 186/200 [06:56<00:21,  1.55s/it]Running Inference:  94%|█████████▎| 187/200 [06:59<00:26,  2.01s/it]Running Inference:  94%|█████████▍| 188/200 [07:01<00:22,  1.84s/it]Running Inference:  94%|█████████▍| 189/200 [07:03<00:23,  2.14s/it]Running Inference:  95%|█████████▌| 190/200 [07:04<00:15,  1.59s/it]Running Inference:  96%|█████████▌| 191/200 [07:07<00:19,  2.22s/it]Running Inference:  96%|█████████▌| 192/200 [07:10<00:19,  2.45s/it]Running Inference:  96%|█████████▋| 193/200 [07:14<00:19,  2.82s/it]Running Inference:  97%|█████████▋| 194/200 [07:15<00:12,  2.11s/it]Running Inference:  98%|█████████▊| 195/200 [07:18<00:12,  2.41s/it]Running Inference:  98%|█████████▊| 196/200 [07:19<00:08,  2.02s/it]Running Inference:  98%|█████████▊| 197/200 [07:20<00:05,  1.75s/it]Running Inference:  99%|█████████▉| 198/200 [07:20<00:02,  1.38s/it]Running Inference: 100%|█████████▉| 199/200 [07:21<00:01,  1.20s/it]Running Inference: 100%|██████████| 200/200 [07:22<00:00,  1.11s/it]Running Inference: 100%|██████████| 200/200 [07:22<00:00,  2.21s/it]
2025-12-13 18:17:11,679 - INFO - Inference completed.
2025-12-13 18:17:11,687 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-13 18:17:11,688 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:17:11,693 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-13 18:17:11,693 - INFO - Metrics:
17.73
2025-12-13 18:17:11,694 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa, ratio=0.3) on GPU 2

----------------------------------------
Task: 2wikimqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:17:18,109 - INFO - Set deterministic seeds to 42
2025-12-13 18:17:18,109 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:17:18,109 - INFO - Starting evaluation run...
2025-12-13 18:17:18,109 - INFO - Output directory set to: longbenchresult
2025-12-13 18:17:18,109 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-13 18:17:18,109 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 18:17:18,109 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:17:18,109 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.86it/s]
Device set to use cuda:0
2025-12-13 18:17:31,581 - INFO - Model pipeline loaded.
2025-12-13 18:17:31,581 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa)
2025-12-13 18:17:37,718 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:17:37,718 - INFO - Dataset processed with 200 entries.
2025-12-13 18:17:37,737 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<13:47,  4.16s/it]Running Inference:   1%|          | 2/200 [00:07<11:17,  3.42s/it]Running Inference:   2%|▏         | 3/200 [00:10<10:44,  3.27s/it]Running Inference:   2%|▏         | 4/200 [00:12<09:50,  3.01s/it]Running Inference:   2%|▎         | 5/200 [00:15<09:20,  2.88s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:25,  2.92s/it]Running Inference:   4%|▎         | 7/200 [00:21<09:32,  2.96s/it]Running Inference:   4%|▍         | 8/200 [00:24<09:40,  3.02s/it]Running Inference:   4%|▍         | 9/200 [00:27<09:51,  3.10s/it]Running Inference:   5%|▌         | 10/200 [00:28<07:20,  2.32s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:31<08:26,  2.68s/it]Running Inference:   6%|▌         | 12/200 [00:35<09:09,  2.92s/it]Running Inference:   6%|▋         | 13/200 [00:38<09:37,  3.09s/it]Running Inference:   7%|▋         | 14/200 [00:39<07:11,  2.32s/it]Running Inference:   8%|▊         | 15/200 [00:40<05:51,  1.90s/it]Running Inference:   8%|▊         | 16/200 [00:41<04:55,  1.60s/it]Running Inference:   8%|▊         | 17/200 [00:44<06:43,  2.20s/it]Running Inference:   9%|▉         | 18/200 [00:45<05:33,  1.83s/it]Running Inference:  10%|▉         | 19/200 [00:48<06:22,  2.11s/it]Running Inference:  10%|█         | 20/200 [00:48<04:41,  1.56s/it]Running Inference:  10%|█         | 21/200 [00:51<05:38,  1.89s/it]Running Inference:  11%|█         | 22/200 [00:54<06:26,  2.17s/it]Running Inference:  12%|█▏        | 23/200 [00:57<07:27,  2.53s/it]Running Inference:  12%|█▏        | 24/200 [01:00<07:41,  2.62s/it]Running Inference:  12%|█▎        | 25/200 [01:01<06:17,  2.15s/it]Running Inference:  13%|█▎        | 26/200 [01:04<06:52,  2.37s/it]Running Inference:  14%|█▎        | 27/200 [01:07<07:05,  2.46s/it]Running Inference:  14%|█▍        | 28/200 [01:07<05:32,  1.93s/it]Running Inference:  14%|█▍        | 29/200 [01:10<06:09,  2.16s/it]Running Inference:  15%|█▌        | 30/200 [01:10<04:30,  1.59s/it]Running Inference:  16%|█▌        | 31/200 [01:12<04:49,  1.71s/it]Running Inference:  16%|█▌        | 32/200 [01:14<04:37,  1.65s/it]Running Inference:  16%|█▋        | 33/200 [01:16<04:42,  1.69s/it]Running Inference:  17%|█▋        | 34/200 [01:18<05:35,  2.02s/it]Running Inference:  18%|█▊        | 35/200 [01:21<06:15,  2.28s/it]Running Inference:  18%|█▊        | 36/200 [01:22<05:04,  1.86s/it]Running Inference:  18%|█▊        | 37/200 [01:23<04:29,  1.66s/it]Running Inference:  19%|█▉        | 38/200 [01:25<04:19,  1.60s/it]Running Inference:  20%|█▉        | 39/200 [01:28<05:18,  1.98s/it]Running Inference:  20%|██        | 40/200 [01:31<05:59,  2.25s/it]Running Inference:  20%|██        | 41/200 [01:32<05:08,  1.94s/it]Running Inference:  21%|██        | 42/200 [01:35<05:51,  2.23s/it]Running Inference:  22%|██▏       | 43/200 [01:36<04:47,  1.83s/it]Running Inference:  22%|██▏       | 44/200 [01:36<03:50,  1.48s/it]Running Inference:  22%|██▎       | 45/200 [01:37<03:15,  1.26s/it]Running Inference:  23%|██▎       | 46/200 [01:39<03:26,  1.34s/it]Running Inference:  24%|██▎       | 47/200 [01:42<04:43,  1.86s/it]Running Inference:  24%|██▍       | 48/200 [01:44<05:27,  2.16s/it]Running Inference:  24%|██▍       | 49/200 [01:46<05:16,  2.09s/it]Running Inference:  25%|██▌       | 50/200 [01:49<05:45,  2.31s/it]Running Inference:  26%|██▌       | 51/200 [01:51<05:41,  2.29s/it]Running Inference:  26%|██▌       | 52/200 [01:54<05:44,  2.33s/it]Running Inference:  26%|██▋       | 53/200 [01:57<05:56,  2.43s/it]Running Inference:  27%|██▋       | 54/200 [01:59<06:11,  2.55s/it]Running Inference:  28%|██▊       | 55/200 [02:00<04:54,  2.03s/it]Running Inference:  28%|██▊       | 56/200 [02:01<04:18,  1.79s/it]Running Inference:  28%|██▊       | 57/200 [02:02<03:34,  1.50s/it]Running Inference:  29%|██▉       | 58/200 [02:04<03:27,  1.46s/it]Running Inference:  30%|██▉       | 59/200 [02:05<03:26,  1.47s/it]Running Inference:  30%|███       | 60/200 [02:08<04:28,  1.92s/it]Running Inference:  30%|███       | 61/200 [02:11<04:59,  2.16s/it]Running Inference:  31%|███       | 62/200 [02:13<05:07,  2.23s/it]Running Inference:  32%|███▏      | 63/200 [02:13<03:43,  1.63s/it]Running Inference:  32%|███▏      | 64/200 [02:17<05:14,  2.31s/it]Running Inference:  32%|███▎      | 65/200 [02:18<03:57,  1.76s/it]Running Inference:  33%|███▎      | 66/200 [02:20<04:25,  1.98s/it]Running Inference:  34%|███▎      | 67/200 [02:23<05:08,  2.32s/it]Running Inference:  34%|███▍      | 68/200 [02:27<05:45,  2.62s/it]Running Inference:  34%|███▍      | 69/200 [02:28<04:45,  2.18s/it]Running Inference:  35%|███▌      | 70/200 [02:31<05:13,  2.41s/it]Running Inference:  36%|███▌      | 71/200 [02:33<05:19,  2.47s/it]Running Inference:  36%|███▌      | 72/200 [02:37<05:42,  2.68s/it]Running Inference:  36%|███▋      | 73/200 [02:38<05:06,  2.42s/it]Running Inference:  37%|███▋      | 74/200 [02:40<04:51,  2.32s/it]Running Inference:  38%|███▊      | 75/200 [02:42<04:09,  2.00s/it]Running Inference:  38%|███▊      | 76/200 [02:43<03:34,  1.73s/it]Running Inference:  38%|███▊      | 77/200 [02:44<03:07,  1.53s/it]Running Inference:  39%|███▉      | 78/200 [02:46<03:41,  1.82s/it]Running Inference:  40%|███▉      | 79/200 [02:49<04:01,  2.00s/it]Running Inference:  40%|████      | 80/200 [02:52<04:47,  2.39s/it]Running Inference:  40%|████      | 81/200 [02:54<04:34,  2.30s/it]Running Inference:  41%|████      | 82/200 [02:57<04:56,  2.51s/it]Running Inference:  42%|████▏     | 83/200 [03:01<05:32,  2.84s/it]Running Inference:  42%|████▏     | 84/200 [03:04<05:30,  2.85s/it]Running Inference:  42%|████▎     | 85/200 [03:05<04:30,  2.35s/it]Running Inference:  43%|████▎     | 86/200 [03:09<05:16,  2.77s/it]Running Inference:  44%|████▎     | 87/200 [03:11<05:11,  2.76s/it]Running Inference:  44%|████▍     | 88/200 [03:14<05:13,  2.80s/it]Running Inference:  44%|████▍     | 89/200 [03:17<05:10,  2.80s/it]Running Inference:  45%|████▌     | 90/200 [03:20<05:14,  2.86s/it]Running Inference:  46%|████▌     | 91/200 [03:21<04:24,  2.42s/it]Running Inference:  46%|████▌     | 92/200 [03:24<04:29,  2.49s/it]Running Inference:  46%|████▋     | 93/200 [03:27<04:47,  2.69s/it]Running Inference:  47%|████▋     | 94/200 [03:28<03:37,  2.06s/it]Running Inference:  48%|████▊     | 95/200 [03:29<02:53,  1.65s/it]Running Inference:  48%|████▊     | 96/200 [03:31<03:27,  2.00s/it]Running Inference:  48%|████▊     | 97/200 [03:34<03:34,  2.08s/it]Running Inference:  49%|████▉     | 98/200 [03:35<03:23,  1.99s/it]Running Inference:  50%|████▉     | 99/200 [03:36<02:45,  1.64s/it]Running Inference:  50%|█████     | 100/200 [03:39<03:18,  1.98s/it]Running Inference:  50%|█████     | 101/200 [03:42<03:37,  2.20s/it]Running Inference:  51%|█████     | 102/200 [03:44<03:51,  2.37s/it]Running Inference:  52%|█████▏    | 103/200 [03:45<03:00,  1.86s/it]Running Inference:  52%|█████▏    | 104/200 [03:48<03:27,  2.16s/it]Running Inference:  52%|█████▎    | 105/200 [03:49<02:57,  1.87s/it]Running Inference:  53%|█████▎    | 106/200 [03:50<02:28,  1.58s/it]Running Inference:  54%|█████▎    | 107/200 [03:52<02:49,  1.82s/it]Running Inference:  54%|█████▍    | 108/200 [03:55<03:15,  2.13s/it]Running Inference:  55%|█████▍    | 109/200 [03:57<02:53,  1.90s/it]Running Inference:  55%|█████▌    | 110/200 [03:58<02:41,  1.80s/it]Running Inference:  56%|█████▌    | 111/200 [03:59<02:09,  1.46s/it]Running Inference:  56%|█████▌    | 112/200 [04:02<02:46,  1.89s/it]Running Inference:  56%|█████▋    | 113/200 [04:03<02:21,  1.63s/it]Running Inference:  57%|█████▋    | 114/200 [04:06<02:50,  1.98s/it]Running Inference:  57%|█████▊    | 115/200 [04:08<02:46,  1.96s/it]Running Inference:  58%|█████▊    | 116/200 [04:11<03:33,  2.55s/it]Running Inference:  58%|█████▊    | 117/200 [04:15<04:06,  2.97s/it]Running Inference:  59%|█████▉    | 118/200 [04:16<03:07,  2.28s/it]Running Inference:  60%|█████▉    | 119/200 [04:19<03:26,  2.55s/it]Running Inference:  60%|██████    | 120/200 [04:22<03:33,  2.66s/it]Running Inference:  60%|██████    | 121/200 [04:25<03:36,  2.74s/it]Running Inference:  61%|██████    | 122/200 [04:26<02:47,  2.14s/it]Running Inference:  62%|██████▏   | 123/200 [04:29<02:59,  2.32s/it]Running Inference:  62%|██████▏   | 124/200 [04:30<02:31,  2.00s/it]Running Inference:  62%|██████▎   | 125/200 [04:32<02:41,  2.16s/it]Running Inference:  63%|██████▎   | 126/200 [04:35<02:51,  2.32s/it]Running Inference:  64%|██████▎   | 127/200 [04:37<02:35,  2.13s/it]Running Inference:  64%|██████▍   | 128/200 [04:38<02:21,  1.96s/it]Running Inference:  64%|██████▍   | 129/200 [04:40<02:08,  1.81s/it]Running Inference:  65%|██████▌   | 130/200 [04:43<02:30,  2.14s/it]Running Inference:  66%|██████▌   | 131/200 [04:46<02:43,  2.36s/it]Running Inference:  66%|██████▌   | 132/200 [04:49<02:56,  2.59s/it]Running Inference:  66%|██████▋   | 133/200 [04:50<02:32,  2.28s/it]Running Inference:  67%|██████▋   | 134/200 [04:53<02:44,  2.50s/it]Running Inference:  68%|██████▊   | 135/200 [04:57<03:15,  3.01s/it]Running Inference:  68%|██████▊   | 136/200 [04:59<02:51,  2.68s/it]Running Inference:  68%|██████▊   | 137/200 [05:01<02:28,  2.35s/it]Running Inference:  69%|██████▉   | 138/200 [05:04<02:35,  2.51s/it]Running Inference:  70%|██████▉   | 139/200 [05:06<02:29,  2.45s/it]Running Inference:  70%|███████   | 140/200 [05:08<02:10,  2.18s/it]Running Inference:  70%|███████   | 141/200 [05:11<02:35,  2.63s/it]Running Inference:  71%|███████   | 142/200 [05:13<02:20,  2.42s/it]Running Inference:  72%|███████▏  | 143/200 [05:14<01:53,  1.99s/it]Running Inference:  72%|███████▏  | 144/200 [05:17<02:06,  2.25s/it]Running Inference:  72%|███████▎  | 145/200 [05:21<02:28,  2.69s/it]Running Inference:  73%|███████▎  | 146/200 [05:24<02:30,  2.78s/it]Running Inference:  74%|███████▎  | 147/200 [05:26<02:21,  2.67s/it]Running Inference:  74%|███████▍  | 148/200 [05:29<02:19,  2.69s/it]Running Inference:  74%|███████▍  | 149/200 [05:32<02:17,  2.71s/it]Running Inference:  75%|███████▌  | 150/200 [05:35<02:20,  2.81s/it]Running Inference:  76%|███████▌  | 151/200 [05:36<01:55,  2.35s/it]Running Inference:  76%|███████▌  | 152/200 [05:39<01:56,  2.43s/it]Running Inference:  76%|███████▋  | 153/200 [05:41<01:57,  2.50s/it]Running Inference:  77%|███████▋  | 154/200 [05:44<01:58,  2.58s/it]Running Inference:  78%|███████▊  | 155/200 [05:45<01:35,  2.12s/it]Running Inference:  78%|███████▊  | 156/200 [05:47<01:24,  1.91s/it]Running Inference:  78%|███████▊  | 157/200 [05:47<01:03,  1.47s/it]Running Inference:  79%|███████▉  | 158/200 [05:50<01:17,  1.85s/it]Running Inference:  80%|███████▉  | 159/200 [05:52<01:21,  2.00s/it]Running Inference:  80%|████████  | 160/200 [05:55<01:28,  2.22s/it]Running Inference:  80%|████████  | 161/200 [05:57<01:25,  2.20s/it]Running Inference:  81%|████████  | 162/200 [05:58<01:13,  1.93s/it]Running Inference:  82%|████████▏ | 163/200 [05:59<00:58,  1.57s/it]Running Inference:  82%|████████▏ | 164/200 [06:02<01:08,  1.90s/it]Running Inference:  82%|████████▎ | 165/200 [06:02<00:54,  1.56s/it]Running Inference:  83%|████████▎ | 166/200 [06:06<01:17,  2.28s/it]Running Inference:  84%|████████▎ | 167/200 [06:08<01:11,  2.16s/it]Running Inference:  84%|████████▍ | 168/200 [06:09<00:53,  1.67s/it]Running Inference:  84%|████████▍ | 169/200 [06:11<00:58,  1.89s/it]Running Inference:  85%|████████▌ | 170/200 [06:14<01:04,  2.13s/it]Running Inference:  86%|████████▌ | 171/200 [06:17<01:08,  2.35s/it]Running Inference:  86%|████████▌ | 172/200 [06:20<01:10,  2.51s/it]Running Inference:  86%|████████▋ | 173/200 [06:23<01:10,  2.63s/it]Running Inference:  87%|████████▋ | 174/200 [06:25<01:09,  2.67s/it]Running Inference:  88%|████████▊ | 175/200 [06:29<01:16,  3.04s/it]Running Inference:  88%|████████▊ | 176/200 [06:32<01:11,  3.00s/it]Running Inference:  88%|████████▊ | 177/200 [06:35<01:06,  2.89s/it]Running Inference:  89%|████████▉ | 178/200 [06:37<00:58,  2.66s/it]Running Inference:  90%|████████▉ | 179/200 [06:41<01:02,  2.97s/it]Running Inference:  90%|█████████ | 180/200 [06:42<00:49,  2.47s/it]Running Inference:  90%|█████████ | 181/200 [06:44<00:46,  2.47s/it]Running Inference:  91%|█████████ | 182/200 [06:46<00:40,  2.26s/it]Running Inference:  92%|█████████▏| 183/200 [06:49<00:43,  2.56s/it]Running Inference:  92%|█████████▏| 184/200 [06:50<00:33,  2.09s/it]Running Inference:  92%|█████████▎| 185/200 [06:53<00:32,  2.15s/it]Running Inference:  93%|█████████▎| 186/200 [06:53<00:23,  1.71s/it]Running Inference:  94%|█████████▎| 187/200 [06:56<00:27,  2.11s/it]Running Inference:  94%|█████████▍| 188/200 [06:58<00:22,  1.91s/it]Running Inference:  94%|█████████▍| 189/200 [06:59<00:17,  1.59s/it]Running Inference:  95%|█████████▌| 190/200 [06:59<00:12,  1.21s/it]Running Inference:  96%|█████████▌| 191/200 [07:02<00:14,  1.62s/it]Running Inference:  96%|█████████▌| 192/200 [07:05<00:16,  2.02s/it]Running Inference:  96%|█████████▋| 193/200 [07:08<00:17,  2.52s/it]Running Inference:  97%|█████████▋| 194/200 [07:09<00:11,  1.90s/it]Running Inference:  98%|█████████▊| 195/200 [07:12<00:11,  2.25s/it]Running Inference:  98%|█████████▊| 196/200 [07:13<00:07,  1.91s/it]Running Inference:  98%|█████████▊| 197/200 [07:14<00:05,  1.67s/it]Running Inference:  99%|█████████▉| 198/200 [07:16<00:03,  1.92s/it]Running Inference: 100%|█████████▉| 199/200 [07:17<00:01,  1.58s/it]Running Inference: 100%|██████████| 200/200 [07:18<00:00,  1.45s/it]Running Inference: 100%|██████████| 200/200 [07:18<00:00,  2.19s/it]
2025-12-13 18:24:56,663 - INFO - Inference completed.
2025-12-13 18:24:56,671 - INFO - Results saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-13 18:24:56,671 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:24:56,677 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-13 18:24:56,677 - INFO - Metrics:
16.31
2025-12-13 18:24:56,678 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa, ratio=0.5) on GPU 2


========================================
LongBench Task: hotpotqa
========================================
----------------------------------------
Task: hotpotqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:25:03,102 - INFO - Set deterministic seeds to 42
2025-12-13 18:25:03,102 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:25:03,102 - INFO - Starting evaluation run...
2025-12-13 18:25:03,102 - INFO - Output directory set to: longbenchresult
2025-12-13 18:25:03,102 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 18:25:03,102 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 18:25:03,102 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:25:03,102 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.51it/s]
Device set to use cuda:0
2025-12-13 18:25:15,826 - INFO - Model pipeline loaded.
2025-12-13 18:25:15,826 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:25:21,810 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:25:21,811 - INFO - Dataset processed with 200 entries.
2025-12-13 18:25:21,846 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<17:02,  5.14s/it]Running Inference:   1%|          | 2/200 [00:05<08:11,  2.48s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:10,  2.49s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:49,  2.39s/it]Running Inference:   2%|▎         | 5/200 [00:11<06:00,  1.85s/it]Running Inference:   3%|▎         | 6/200 [00:14<07:31,  2.33s/it]Running Inference:   4%|▎         | 7/200 [00:16<07:08,  2.22s/it]Running Inference:   4%|▍         | 8/200 [00:17<06:04,  1.90s/it]Running Inference:   4%|▍         | 9/200 [00:19<05:52,  1.85s/it]Running Inference:   5%|▌         | 10/200 [00:21<06:01,  1.90s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<05:34,  1.77s/it]Running Inference:   6%|▌         | 12/200 [00:24<05:01,  1.60s/it]Running Inference:   6%|▋         | 13/200 [00:26<05:57,  1.91s/it]Running Inference:   7%|▋         | 14/200 [00:29<06:09,  1.99s/it]Running Inference:   8%|▊         | 15/200 [00:33<08:12,  2.66s/it]Running Inference:   8%|▊         | 16/200 [00:35<07:55,  2.59s/it]Running Inference:   8%|▊         | 17/200 [00:39<09:22,  3.08s/it]Running Inference:   9%|▉         | 18/200 [00:42<08:28,  2.79s/it]Running Inference:  10%|▉         | 19/200 [00:42<06:33,  2.18s/it]Running Inference:  10%|█         | 20/200 [00:44<05:40,  1.89s/it]Running Inference:  10%|█         | 21/200 [00:44<04:36,  1.54s/it]Running Inference:  11%|█         | 22/200 [00:47<05:33,  1.88s/it]Running Inference:  12%|█▏        | 23/200 [00:49<05:48,  1.97s/it]Running Inference:  12%|█▏        | 24/200 [00:53<07:40,  2.62s/it]Running Inference:  12%|█▎        | 25/200 [00:55<06:33,  2.25s/it]Running Inference:  13%|█▎        | 26/200 [00:55<05:07,  1.77s/it]Running Inference:  14%|█▎        | 27/200 [00:59<06:48,  2.36s/it]Running Inference:  14%|█▍        | 28/200 [01:02<07:02,  2.46s/it]Running Inference:  14%|█▍        | 29/200 [01:06<08:30,  2.99s/it]Running Inference:  15%|█▌        | 30/200 [01:10<09:36,  3.39s/it]Running Inference:  16%|█▌        | 31/200 [01:12<07:46,  2.76s/it]Running Inference:  16%|█▌        | 32/200 [01:16<09:00,  3.22s/it]Running Inference:  16%|█▋        | 33/200 [01:18<08:13,  2.96s/it]Running Inference:  17%|█▋        | 34/200 [01:20<07:31,  2.72s/it]Running Inference:  18%|█▊        | 35/200 [01:23<07:04,  2.57s/it]Running Inference:  18%|█▊        | 36/200 [01:25<06:51,  2.51s/it]Running Inference:  18%|█▊        | 37/200 [01:27<06:39,  2.45s/it]Running Inference:  19%|█▉        | 38/200 [01:29<06:24,  2.37s/it]Running Inference:  20%|█▉        | 39/200 [01:32<06:36,  2.46s/it]Running Inference:  20%|██        | 40/200 [01:35<06:55,  2.60s/it]Running Inference:  20%|██        | 41/200 [01:37<06:30,  2.46s/it]Running Inference:  21%|██        | 42/200 [01:39<06:11,  2.35s/it]Running Inference:  22%|██▏       | 43/200 [01:40<05:15,  2.01s/it]Running Inference:  22%|██▏       | 44/200 [01:45<06:51,  2.64s/it]Running Inference:  22%|██▎       | 45/200 [01:49<07:53,  3.05s/it]Running Inference:  23%|██▎       | 46/200 [01:53<08:30,  3.32s/it]Running Inference:  24%|██▎       | 47/200 [01:57<09:08,  3.59s/it]Running Inference:  24%|██▍       | 48/200 [01:59<07:49,  3.09s/it]Running Inference:  24%|██▍       | 49/200 [02:03<08:28,  3.37s/it]Running Inference:  25%|██▌       | 50/200 [02:04<07:08,  2.86s/it]Running Inference:  26%|██▌       | 51/200 [02:06<06:08,  2.47s/it]Running Inference:  26%|██▌       | 52/200 [02:10<07:16,  2.95s/it]Running Inference:  26%|██▋       | 53/200 [02:13<07:15,  2.96s/it]Running Inference:  27%|██▋       | 54/200 [02:15<06:33,  2.69s/it]Running Inference:  28%|██▊       | 55/200 [02:19<07:32,  3.12s/it]Running Inference:  28%|██▊       | 56/200 [02:21<06:49,  2.84s/it]Running Inference:  28%|██▊       | 57/200 [02:25<07:18,  3.07s/it]Running Inference:  29%|██▉       | 58/200 [02:27<06:14,  2.64s/it]Running Inference:  30%|██▉       | 59/200 [02:29<05:47,  2.46s/it]Running Inference:  30%|███       | 60/200 [02:33<06:56,  2.98s/it]Running Inference:  30%|███       | 61/200 [02:37<07:36,  3.28s/it]Running Inference:  31%|███       | 62/200 [02:40<07:29,  3.26s/it]Running Inference:  32%|███▏      | 63/200 [02:44<07:38,  3.35s/it]Running Inference:  32%|███▏      | 64/200 [02:48<08:10,  3.61s/it]Running Inference:  32%|███▎      | 65/200 [02:50<07:06,  3.16s/it]Running Inference:  33%|███▎      | 66/200 [02:52<06:19,  2.83s/it]Running Inference:  34%|███▎      | 67/200 [02:55<06:39,  3.01s/it]Running Inference:  34%|███▍      | 68/200 [02:57<05:49,  2.65s/it]Running Inference:  34%|███▍      | 69/200 [03:00<06:06,  2.80s/it]Running Inference:  35%|███▌      | 70/200 [03:04<06:34,  3.04s/it]Running Inference:  36%|███▌      | 71/200 [03:06<06:05,  2.83s/it]Running Inference:  36%|███▌      | 72/200 [03:09<05:52,  2.75s/it]Running Inference:  36%|███▋      | 73/200 [03:12<06:04,  2.87s/it]Running Inference:  37%|███▋      | 74/200 [03:16<06:31,  3.11s/it]Running Inference:  38%|███▊      | 75/200 [03:17<05:33,  2.66s/it]Running Inference:  38%|███▊      | 76/200 [03:21<06:09,  2.98s/it]Running Inference:  38%|███▊      | 77/200 [03:22<04:47,  2.34s/it]Running Inference:  39%|███▉      | 78/200 [03:24<04:37,  2.27s/it]Running Inference:  40%|███▉      | 79/200 [03:26<04:16,  2.12s/it]Running Inference:  40%|████      | 80/200 [03:27<03:28,  1.74s/it]Running Inference:  40%|████      | 81/200 [03:31<04:56,  2.49s/it]Running Inference:  41%|████      | 82/200 [03:33<04:28,  2.27s/it]Running Inference:  42%|████▏     | 83/200 [03:34<03:54,  2.00s/it]Running Inference:  42%|████▏     | 84/200 [03:37<04:13,  2.18s/it]Running Inference:  42%|████▎     | 85/200 [03:39<04:31,  2.36s/it]Running Inference:  43%|████▎     | 86/200 [03:42<04:29,  2.37s/it]Running Inference:  44%|████▎     | 87/200 [03:44<04:15,  2.26s/it]Running Inference:  44%|████▍     | 88/200 [03:45<03:45,  2.01s/it]Running Inference:  44%|████▍     | 89/200 [03:50<05:00,  2.71s/it]Running Inference:  45%|████▌     | 90/200 [03:51<04:29,  2.45s/it]Running Inference:  46%|████▌     | 91/200 [03:52<03:35,  1.98s/it]Running Inference:  46%|████▌     | 92/200 [03:55<04:06,  2.28s/it]Running Inference:  46%|████▋     | 93/200 [03:57<03:42,  2.08s/it]Running Inference:  47%|████▋     | 94/200 [04:00<04:28,  2.53s/it]Running Inference:  48%|████▊     | 95/200 [04:02<03:57,  2.26s/it]Running Inference:  48%|████▊     | 96/200 [04:04<03:51,  2.22s/it]Running Inference:  48%|████▊     | 97/200 [04:06<03:35,  2.10s/it]Running Inference:  49%|████▉     | 98/200 [04:10<04:35,  2.71s/it]Running Inference:  50%|████▉     | 99/200 [04:12<04:07,  2.46s/it]Running Inference:  50%|█████     | 100/200 [04:14<03:59,  2.39s/it]Running Inference:  50%|█████     | 101/200 [04:17<03:59,  2.42s/it]Running Inference:  51%|█████     | 102/200 [04:19<03:46,  2.31s/it]Running Inference:  52%|█████▏    | 103/200 [04:20<03:00,  1.86s/it]Running Inference:  52%|█████▏    | 104/200 [04:22<03:00,  1.88s/it]Running Inference:  52%|█████▎    | 105/200 [04:23<02:45,  1.75s/it]Running Inference:  53%|█████▎    | 106/200 [04:25<02:51,  1.83s/it]Running Inference:  54%|█████▎    | 107/200 [04:28<03:18,  2.14s/it]Running Inference:  54%|█████▍    | 108/200 [04:29<02:36,  1.71s/it]Running Inference:  55%|█████▍    | 109/200 [04:30<02:28,  1.63s/it]Running Inference:  55%|█████▌    | 110/200 [04:31<02:21,  1.58s/it]Running Inference:  56%|█████▌    | 111/200 [04:34<02:42,  1.82s/it]Running Inference:  56%|█████▌    | 112/200 [04:38<03:41,  2.51s/it]Running Inference:  56%|█████▋    | 113/200 [04:40<03:35,  2.47s/it]Running Inference:  57%|█████▋    | 114/200 [04:43<03:25,  2.39s/it]Running Inference:  57%|█████▊    | 115/200 [04:45<03:19,  2.35s/it]Running Inference:  58%|█████▊    | 116/200 [04:48<03:26,  2.46s/it]Running Inference:  58%|█████▊    | 117/200 [04:50<03:32,  2.56s/it]Running Inference:  59%|█████▉    | 118/200 [04:53<03:22,  2.47s/it]Running Inference:  60%|█████▉    | 119/200 [04:54<02:49,  2.09s/it]Running Inference:  60%|██████    | 120/200 [04:58<03:31,  2.64s/it]Running Inference:  60%|██████    | 121/200 [05:02<04:07,  3.13s/it]Running Inference:  61%|██████    | 122/200 [05:04<03:40,  2.82s/it]Running Inference:  62%|██████▏   | 123/200 [05:06<03:22,  2.63s/it]Running Inference:  62%|██████▏   | 124/200 [05:09<03:16,  2.59s/it]Running Inference:  62%|██████▎   | 125/200 [05:11<03:01,  2.42s/it]Running Inference:  63%|██████▎   | 126/200 [05:13<02:54,  2.36s/it]Running Inference:  64%|██████▎   | 127/200 [05:14<02:16,  1.87s/it]Running Inference:  64%|██████▍   | 128/200 [05:18<03:08,  2.62s/it]Running Inference:  64%|██████▍   | 129/200 [05:20<02:54,  2.46s/it]Running Inference:  65%|██████▌   | 130/200 [05:22<02:38,  2.27s/it]Running Inference:  66%|██████▌   | 131/200 [05:26<03:10,  2.76s/it]Running Inference:  66%|██████▌   | 132/200 [05:27<02:41,  2.37s/it]Running Inference:  66%|██████▋   | 133/200 [05:29<02:19,  2.08s/it]Running Inference:  67%|██████▋   | 134/200 [05:32<02:39,  2.41s/it]Running Inference:  68%|██████▊   | 135/200 [05:34<02:28,  2.29s/it]Running Inference:  68%|██████▊   | 136/200 [05:38<03:02,  2.86s/it]Running Inference:  68%|██████▊   | 137/200 [05:40<02:46,  2.64s/it]Running Inference:  69%|██████▉   | 138/200 [05:42<02:36,  2.52s/it]Running Inference:  70%|██████▉   | 139/200 [05:47<03:04,  3.03s/it]Running Inference:  70%|███████   | 140/200 [05:50<02:57,  2.96s/it]Running Inference:  70%|███████   | 141/200 [05:52<02:41,  2.73s/it]Running Inference:  71%|███████   | 142/200 [05:56<03:02,  3.15s/it]Running Inference:  72%|███████▏  | 143/200 [05:57<02:23,  2.52s/it]Running Inference:  72%|███████▏  | 144/200 [05:58<02:02,  2.19s/it]Running Inference:  72%|███████▎  | 145/200 [05:59<01:30,  1.64s/it]Running Inference:  73%|███████▎  | 146/200 [06:03<02:10,  2.41s/it]Running Inference:  74%|███████▎  | 147/200 [06:05<02:04,  2.35s/it]Running Inference:  74%|███████▍  | 148/200 [06:09<02:24,  2.78s/it]Running Inference:  74%|███████▍  | 149/200 [06:09<01:48,  2.12s/it]Running Inference:  75%|███████▌  | 150/200 [06:11<01:36,  1.93s/it]Running Inference:  76%|███████▌  | 151/200 [06:14<01:55,  2.35s/it]Running Inference:  76%|███████▌  | 152/200 [06:18<02:15,  2.83s/it]Running Inference:  76%|███████▋  | 153/200 [06:20<02:00,  2.57s/it]Running Inference:  77%|███████▋  | 154/200 [06:23<01:54,  2.49s/it]Running Inference:  78%|███████▊  | 155/200 [06:25<01:54,  2.55s/it]Running Inference:  78%|███████▊  | 156/200 [06:26<01:33,  2.12s/it]Running Inference:  78%|███████▊  | 157/200 [06:28<01:22,  1.91s/it]Running Inference:  79%|███████▉  | 158/200 [06:31<01:31,  2.18s/it]Running Inference:  80%|███████▉  | 159/200 [06:33<01:31,  2.22s/it]Running Inference:  80%|████████  | 160/200 [06:37<01:51,  2.80s/it]Running Inference:  80%|████████  | 161/200 [06:40<01:48,  2.79s/it]Running Inference:  81%|████████  | 162/200 [06:40<01:20,  2.12s/it]Running Inference:  82%|████████▏ | 163/200 [06:44<01:37,  2.63s/it]Running Inference:  82%|████████▏ | 164/200 [06:46<01:29,  2.48s/it]Running Inference:  82%|████████▎ | 165/200 [06:50<01:44,  2.98s/it]Running Inference:  83%|████████▎ | 166/200 [06:54<01:50,  3.26s/it]Running Inference:  84%|████████▎ | 167/200 [06:56<01:34,  2.88s/it]Running Inference:  84%|████████▍ | 168/200 [07:00<01:42,  3.22s/it]Running Inference:  84%|████████▍ | 169/200 [07:03<01:30,  2.92s/it]Running Inference:  85%|████████▌ | 170/200 [07:05<01:25,  2.84s/it]Running Inference:  86%|████████▌ | 171/200 [07:08<01:20,  2.79s/it]Running Inference:  86%|████████▌ | 172/200 [07:12<01:29,  3.21s/it]Running Inference:  86%|████████▋ | 173/200 [07:14<01:17,  2.87s/it]Running Inference:  87%|████████▋ | 174/200 [07:18<01:21,  3.13s/it]Running Inference:  88%|████████▊ | 175/200 [07:20<01:11,  2.85s/it]Running Inference:  88%|████████▊ | 176/200 [07:22<01:03,  2.63s/it]Running Inference:  88%|████████▊ | 177/200 [07:27<01:12,  3.14s/it]Running Inference:  89%|████████▉ | 178/200 [07:31<01:16,  3.48s/it]Running Inference:  90%|████████▉ | 179/200 [07:33<01:05,  3.12s/it]Running Inference:  90%|█████████ | 180/200 [07:37<01:05,  3.25s/it]Running Inference:  90%|█████████ | 181/200 [07:39<00:55,  2.94s/it]Running Inference:  91%|█████████ | 182/200 [07:41<00:46,  2.61s/it]Running Inference:  92%|█████████▏| 183/200 [07:44<00:49,  2.93s/it]Running Inference:  92%|█████████▏| 184/200 [07:48<00:51,  3.25s/it]Running Inference:  92%|█████████▎| 185/200 [07:50<00:41,  2.73s/it]Running Inference:  93%|█████████▎| 186/200 [07:54<00:43,  3.14s/it]Running Inference:  94%|█████████▎| 187/200 [07:56<00:35,  2.74s/it]Running Inference:  94%|█████████▍| 188/200 [07:59<00:33,  2.76s/it]Running Inference:  94%|█████████▍| 189/200 [08:01<00:28,  2.59s/it]Running Inference:  95%|█████████▌| 190/200 [08:04<00:27,  2.75s/it]Running Inference:  96%|█████████▌| 191/200 [08:06<00:23,  2.56s/it]Running Inference:  96%|█████████▌| 192/200 [08:07<00:17,  2.19s/it]Running Inference:  96%|█████████▋| 193/200 [08:12<00:19,  2.85s/it]Running Inference:  97%|█████████▋| 194/200 [08:15<00:18,  3.06s/it]Running Inference:  98%|█████████▊| 195/200 [08:18<00:15,  3.06s/it]Running Inference:  98%|█████████▊| 196/200 [08:21<00:11,  2.79s/it]Running Inference:  98%|█████████▊| 197/200 [08:22<00:07,  2.37s/it]Running Inference:  99%|█████████▉| 198/200 [08:26<00:05,  2.79s/it]Running Inference: 100%|█████████▉| 199/200 [08:28<00:02,  2.67s/it]Running Inference: 100%|██████████| 200/200 [08:29<00:00,  2.01s/it]Running Inference: 100%|██████████| 200/200 [08:29<00:00,  2.55s/it]
2025-12-13 18:33:50,906 - INFO - Inference completed.
2025-12-13 18:33:50,914 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 18:33:50,914 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:33:50,919 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 18:33:50,919 - INFO - Metrics:
30.67
2025-12-13 18:33:50,920 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=hotpotqa, ratio=0.1) on GPU 2

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:33:57,288 - INFO - Set deterministic seeds to 42
2025-12-13 18:33:57,288 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:33:57,288 - INFO - Starting evaluation run...
2025-12-13 18:33:57,288 - INFO - Output directory set to: longbenchresult
2025-12-13 18:33:57,288 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 18:33:57,289 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 18:33:57,289 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:33:57,289 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.99it/s]
Device set to use cuda:0
2025-12-13 18:34:09,023 - INFO - Model pipeline loaded.
2025-12-13 18:34:09,023 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:34:15,823 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:34:15,823 - INFO - Dataset processed with 200 entries.
2025-12-13 18:34:15,856 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<16:32,  4.99s/it]Running Inference:   1%|          | 2/200 [00:05<08:02,  2.44s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:04,  2.46s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:46,  2.38s/it]Running Inference:   2%|▎         | 5/200 [00:11<05:58,  1.84s/it]Running Inference:   3%|▎         | 6/200 [00:14<07:25,  2.30s/it]Running Inference:   4%|▎         | 7/200 [00:16<07:03,  2.19s/it]Running Inference:   4%|▍         | 8/200 [00:17<06:00,  1.88s/it]Running Inference:   4%|▍         | 9/200 [00:19<05:49,  1.83s/it]Running Inference:   5%|▌         | 10/200 [00:21<05:57,  1.88s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:22<05:31,  1.76s/it]Running Inference:   6%|▌         | 12/200 [00:24<04:59,  1.59s/it]Running Inference:   6%|▋         | 13/200 [00:28<07:14,  2.32s/it]Running Inference:   7%|▋         | 14/200 [00:30<07:03,  2.28s/it]Running Inference:   8%|▊         | 15/200 [00:33<07:48,  2.53s/it]Running Inference:   8%|▊         | 16/200 [00:35<07:29,  2.44s/it]Running Inference:   8%|▊         | 17/200 [00:39<09:01,  2.96s/it]Running Inference:   9%|▉         | 18/200 [00:41<08:12,  2.71s/it]Running Inference:  10%|▉         | 19/200 [00:42<06:23,  2.12s/it]Running Inference:  10%|█         | 20/200 [00:43<05:33,  1.85s/it]Running Inference:  10%|█         | 21/200 [00:44<04:30,  1.51s/it]Running Inference:  11%|█         | 22/200 [00:47<05:25,  1.83s/it]Running Inference:  12%|█▏        | 23/200 [00:49<05:41,  1.93s/it]Running Inference:  12%|█▏        | 24/200 [00:53<07:32,  2.57s/it]Running Inference:  12%|█▎        | 25/200 [00:54<06:27,  2.21s/it]Running Inference:  13%|█▎        | 26/200 [00:55<05:02,  1.74s/it]Running Inference:  14%|█▎        | 27/200 [00:58<06:37,  2.30s/it]Running Inference:  14%|█▍        | 28/200 [01:02<07:55,  2.76s/it]Running Inference:  14%|█▍        | 29/200 [01:06<09:05,  3.19s/it]Running Inference:  15%|█▌        | 30/200 [01:11<09:57,  3.52s/it]Running Inference:  16%|█▌        | 31/200 [01:12<08:00,  2.84s/it]Running Inference:  16%|█▌        | 32/200 [01:16<09:07,  3.26s/it]Running Inference:  16%|█▋        | 33/200 [01:20<09:51,  3.54s/it]Running Inference:  17%|█▋        | 34/200 [01:23<09:02,  3.27s/it]Running Inference:  18%|█▊        | 35/200 [01:27<09:46,  3.56s/it]Running Inference:  18%|█▊        | 36/200 [01:30<08:44,  3.20s/it]Running Inference:  18%|█▊        | 37/200 [01:33<08:49,  3.25s/it]Running Inference:  19%|█▉        | 38/200 [01:35<07:54,  2.93s/it]Running Inference:  20%|█▉        | 39/200 [01:38<07:44,  2.89s/it]Running Inference:  20%|██        | 40/200 [01:40<07:19,  2.75s/it]Running Inference:  20%|██        | 41/200 [01:43<06:47,  2.56s/it]Running Inference:  21%|██        | 42/200 [01:45<06:22,  2.42s/it]Running Inference:  22%|██▏       | 43/200 [01:46<05:23,  2.06s/it]Running Inference:  22%|██▏       | 44/200 [01:50<06:55,  2.66s/it]Running Inference:  22%|██▎       | 45/200 [01:53<06:50,  2.65s/it]Running Inference:  23%|██▎       | 46/200 [01:56<07:46,  3.03s/it]Running Inference:  24%|██▎       | 47/200 [02:01<08:36,  3.37s/it]Running Inference:  24%|██▍       | 48/200 [02:02<07:02,  2.78s/it]Running Inference:  24%|██▍       | 49/200 [02:06<07:55,  3.15s/it]Running Inference:  25%|██▌       | 50/200 [02:08<06:44,  2.70s/it]Running Inference:  26%|██▌       | 51/200 [02:09<05:52,  2.36s/it]Running Inference:  26%|██▌       | 52/200 [02:12<06:04,  2.47s/it]Running Inference:  26%|██▋       | 53/200 [02:15<06:24,  2.61s/it]Running Inference:  27%|██▋       | 54/200 [02:17<05:57,  2.45s/it]Running Inference:  28%|██▊       | 55/200 [02:20<06:29,  2.69s/it]Running Inference:  28%|██▊       | 56/200 [02:24<07:01,  2.93s/it]Running Inference:  28%|██▊       | 57/200 [02:27<07:25,  3.12s/it]Running Inference:  29%|██▉       | 58/200 [02:31<07:41,  3.25s/it]Running Inference:  30%|██▉       | 59/200 [02:32<06:13,  2.65s/it]Running Inference:  30%|███       | 60/200 [02:36<07:14,  3.10s/it]Running Inference:  30%|███       | 61/200 [02:38<06:33,  2.83s/it]Running Inference:  31%|███       | 62/200 [02:43<07:27,  3.24s/it]Running Inference:  32%|███▏      | 63/200 [02:46<07:35,  3.33s/it]Running Inference:  32%|███▏      | 64/200 [02:49<07:00,  3.09s/it]Running Inference:  32%|███▎      | 65/200 [02:51<06:17,  2.80s/it]Running Inference:  33%|███▎      | 66/200 [02:53<05:45,  2.58s/it]Running Inference:  34%|███▎      | 67/200 [02:56<06:15,  2.82s/it]Running Inference:  34%|███▍      | 68/200 [02:59<05:57,  2.71s/it]Running Inference:  34%|███▍      | 69/200 [03:02<06:13,  2.85s/it]Running Inference:  35%|███▌      | 70/200 [03:06<06:38,  3.06s/it]Running Inference:  36%|███▌      | 71/200 [03:08<06:07,  2.85s/it]Running Inference:  36%|███▌      | 72/200 [03:10<05:42,  2.67s/it]Running Inference:  36%|███▋      | 73/200 [03:14<06:37,  3.13s/it]Running Inference:  37%|███▋      | 74/200 [03:16<05:40,  2.70s/it]Running Inference:  38%|███▊      | 75/200 [03:18<04:57,  2.38s/it]Running Inference:  38%|███▊      | 76/200 [03:22<05:50,  2.83s/it]Running Inference:  38%|███▊      | 77/200 [03:22<04:34,  2.23s/it]Running Inference:  39%|███▉      | 78/200 [03:26<05:32,  2.72s/it]Running Inference:  40%|███▉      | 79/200 [03:28<04:53,  2.43s/it]Running Inference:  40%|████      | 80/200 [03:29<03:54,  1.96s/it]Running Inference:  40%|████      | 81/200 [03:33<05:13,  2.63s/it]Running Inference:  41%|████      | 82/200 [03:35<04:39,  2.37s/it]Running Inference:  42%|████▏     | 83/200 [03:36<04:02,  2.07s/it]Running Inference:  42%|████▏     | 84/200 [03:40<04:53,  2.53s/it]Running Inference:  42%|████▎     | 85/200 [03:43<04:58,  2.60s/it]Running Inference:  43%|████▎     | 86/200 [03:45<04:48,  2.53s/it]Running Inference:  44%|████▎     | 87/200 [03:47<04:28,  2.38s/it]Running Inference:  44%|████▍     | 88/200 [03:48<03:54,  2.09s/it]Running Inference:  44%|████▍     | 89/200 [03:51<03:58,  2.15s/it]Running Inference:  45%|████▌     | 90/200 [03:52<03:45,  2.05s/it]Running Inference:  46%|████▌     | 91/200 [03:53<03:05,  1.70s/it]Running Inference:  46%|████▌     | 92/200 [03:58<04:28,  2.48s/it]Running Inference:  46%|████▋     | 93/200 [03:59<03:57,  2.22s/it]Running Inference:  47%|████▋     | 94/200 [04:03<04:37,  2.62s/it]Running Inference:  48%|████▊     | 95/200 [04:04<04:03,  2.32s/it]Running Inference:  48%|████▊     | 96/200 [04:07<03:55,  2.26s/it]Running Inference:  48%|████▊     | 97/200 [04:08<03:33,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [04:11<03:58,  2.34s/it]Running Inference:  50%|████▉     | 99/200 [04:13<03:42,  2.20s/it]Running Inference:  50%|█████     | 100/200 [04:15<03:41,  2.21s/it]Running Inference:  50%|█████     | 101/200 [04:18<03:46,  2.29s/it]Running Inference:  51%|█████     | 102/200 [04:20<03:37,  2.22s/it]Running Inference:  52%|█████▏    | 103/200 [04:21<02:53,  1.79s/it]Running Inference:  52%|█████▏    | 104/200 [04:23<02:56,  1.84s/it]Running Inference:  52%|█████▎    | 105/200 [04:24<02:42,  1.71s/it]Running Inference:  53%|█████▎    | 106/200 [04:26<02:49,  1.80s/it]Running Inference:  54%|█████▎    | 107/200 [04:29<03:16,  2.11s/it]Running Inference:  54%|█████▍    | 108/200 [04:30<02:36,  1.70s/it]Running Inference:  55%|█████▍    | 109/200 [04:31<02:28,  1.63s/it]Running Inference:  55%|█████▌    | 110/200 [04:32<02:21,  1.57s/it]Running Inference:  56%|█████▌    | 111/200 [04:35<02:41,  1.82s/it]Running Inference:  56%|█████▌    | 112/200 [04:39<03:39,  2.50s/it]Running Inference:  56%|█████▋    | 113/200 [04:41<03:34,  2.46s/it]Running Inference:  57%|█████▋    | 114/200 [04:43<03:24,  2.38s/it]Running Inference:  57%|█████▊    | 115/200 [04:46<03:18,  2.34s/it]Running Inference:  58%|█████▊    | 116/200 [04:48<03:25,  2.45s/it]Running Inference:  58%|█████▊    | 117/200 [04:51<03:30,  2.54s/it]Running Inference:  59%|█████▉    | 118/200 [04:53<03:20,  2.45s/it]Running Inference:  60%|█████▉    | 119/200 [04:55<02:48,  2.07s/it]Running Inference:  60%|██████    | 120/200 [04:58<03:28,  2.61s/it]Running Inference:  60%|██████    | 121/200 [05:03<04:03,  3.08s/it]Running Inference:  61%|██████    | 122/200 [05:05<03:37,  2.78s/it]Running Inference:  62%|██████▏   | 123/200 [05:09<04:05,  3.19s/it]Running Inference:  62%|██████▏   | 124/200 [05:11<03:46,  2.98s/it]Running Inference:  62%|██████▎   | 125/200 [05:13<03:21,  2.69s/it]Running Inference:  63%|██████▎   | 126/200 [05:16<03:08,  2.55s/it]Running Inference:  64%|██████▎   | 127/200 [05:16<02:26,  2.00s/it]Running Inference:  64%|██████▍   | 128/200 [05:21<03:13,  2.69s/it]Running Inference:  64%|██████▍   | 129/200 [05:23<02:58,  2.51s/it]Running Inference:  65%|██████▌   | 130/200 [05:25<02:40,  2.30s/it]Running Inference:  66%|██████▌   | 131/200 [05:28<03:10,  2.77s/it]Running Inference:  66%|██████▌   | 132/200 [05:30<02:40,  2.36s/it]Running Inference:  66%|██████▋   | 133/200 [05:31<02:18,  2.06s/it]Running Inference:  67%|██████▋   | 134/200 [05:33<02:20,  2.13s/it]Running Inference:  68%|██████▊   | 135/200 [05:35<02:16,  2.09s/it]Running Inference:  68%|██████▊   | 136/200 [05:40<02:52,  2.69s/it]Running Inference:  68%|██████▊   | 137/200 [05:42<02:38,  2.51s/it]Running Inference:  69%|██████▉   | 138/200 [05:44<02:30,  2.43s/it]Running Inference:  70%|██████▉   | 139/200 [05:48<02:59,  2.95s/it]Running Inference:  70%|███████   | 140/200 [05:51<02:53,  2.90s/it]Running Inference:  70%|███████   | 141/200 [05:53<02:38,  2.69s/it]Running Inference:  71%|███████   | 142/200 [05:57<03:01,  3.13s/it]Running Inference:  72%|███████▏  | 143/200 [05:58<02:16,  2.40s/it]Running Inference:  72%|███████▏  | 144/200 [05:59<01:55,  2.06s/it]Running Inference:  72%|███████▎  | 145/200 [06:00<01:37,  1.78s/it]Running Inference:  73%|███████▎  | 146/200 [06:04<02:14,  2.50s/it]Running Inference:  74%|███████▎  | 147/200 [06:07<02:07,  2.41s/it]Running Inference:  74%|███████▍  | 148/200 [06:11<02:29,  2.88s/it]Running Inference:  74%|███████▍  | 149/200 [06:11<01:51,  2.19s/it]Running Inference:  75%|███████▌  | 150/200 [06:13<01:38,  1.98s/it]Running Inference:  76%|███████▌  | 151/200 [06:16<01:55,  2.36s/it]Running Inference:  76%|███████▌  | 152/200 [06:20<02:15,  2.81s/it]Running Inference:  76%|███████▋  | 153/200 [06:22<02:00,  2.56s/it]Running Inference:  77%|███████▋  | 154/200 [06:24<01:54,  2.48s/it]Running Inference:  78%|███████▊  | 155/200 [06:27<01:53,  2.53s/it]Running Inference:  78%|███████▊  | 156/200 [06:28<01:33,  2.12s/it]Running Inference:  78%|███████▊  | 157/200 [06:29<01:22,  1.91s/it]Running Inference:  79%|███████▉  | 158/200 [06:32<01:30,  2.16s/it]Running Inference:  80%|███████▉  | 159/200 [06:34<01:30,  2.20s/it]Running Inference:  80%|████████  | 160/200 [06:38<01:50,  2.76s/it]Running Inference:  80%|████████  | 161/200 [06:41<01:47,  2.76s/it]Running Inference:  81%|████████  | 162/200 [06:42<01:19,  2.10s/it]Running Inference:  82%|████████▏ | 163/200 [06:44<01:20,  2.17s/it]Running Inference:  82%|████████▏ | 164/200 [06:46<01:17,  2.15s/it]Running Inference:  82%|████████▎ | 165/200 [06:50<01:35,  2.73s/it]Running Inference:  83%|████████▎ | 166/200 [06:54<01:44,  3.06s/it]Running Inference:  84%|████████▎ | 167/200 [06:56<01:30,  2.74s/it]Running Inference:  84%|████████▍ | 168/200 [07:00<01:39,  3.10s/it]Running Inference:  84%|████████▍ | 169/200 [07:03<01:39,  3.20s/it]Running Inference:  85%|████████▌ | 170/200 [07:06<01:30,  3.03s/it]Running Inference:  86%|████████▌ | 171/200 [07:10<01:32,  3.18s/it]Running Inference:  86%|████████▌ | 172/200 [07:14<01:36,  3.46s/it]Running Inference:  86%|████████▋ | 173/200 [07:16<01:22,  3.05s/it]Running Inference:  87%|████████▋ | 174/200 [07:19<01:24,  3.23s/it]Running Inference:  88%|████████▊ | 175/200 [07:22<01:12,  2.92s/it]Running Inference:  88%|████████▊ | 176/200 [07:24<01:04,  2.67s/it]Running Inference:  88%|████████▊ | 177/200 [07:28<01:12,  3.15s/it]Running Inference:  89%|████████▉ | 178/200 [07:32<01:16,  3.46s/it]Running Inference:  90%|████████▉ | 179/200 [07:34<01:05,  3.10s/it]Running Inference:  90%|█████████ | 180/200 [07:38<01:04,  3.22s/it]Running Inference:  90%|█████████ | 181/200 [07:40<00:55,  2.92s/it]Running Inference:  91%|█████████ | 182/200 [07:42<00:46,  2.59s/it]Running Inference:  92%|█████████▏| 183/200 [07:45<00:43,  2.58s/it]Running Inference:  92%|█████████▏| 184/200 [07:49<00:47,  2.99s/it]Running Inference:  92%|█████████▎| 185/200 [07:50<00:38,  2.55s/it]Running Inference:  93%|█████████▎| 186/200 [07:54<00:41,  2.99s/it]Running Inference:  94%|█████████▎| 187/200 [07:56<00:34,  2.63s/it]Running Inference:  94%|█████████▍| 188/200 [07:59<00:32,  2.67s/it]Running Inference:  94%|█████████▍| 189/200 [08:01<00:27,  2.51s/it]Running Inference:  95%|█████████▌| 190/200 [08:03<00:23,  2.37s/it]Running Inference:  96%|█████████▌| 191/200 [08:05<00:20,  2.29s/it]Running Inference:  96%|█████████▌| 192/200 [08:06<00:16,  2.00s/it]Running Inference:  96%|█████████▋| 193/200 [08:11<00:18,  2.70s/it]Running Inference:  97%|█████████▋| 194/200 [08:14<00:17,  2.95s/it]Running Inference:  98%|█████████▊| 195/200 [08:17<00:14,  2.96s/it]Running Inference:  98%|█████████▊| 196/200 [08:19<00:10,  2.73s/it]Running Inference:  98%|█████████▊| 197/200 [08:21<00:06,  2.33s/it]Running Inference:  99%|█████████▉| 198/200 [08:24<00:04,  2.49s/it]Running Inference: 100%|█████████▉| 199/200 [08:26<00:02,  2.46s/it]Running Inference: 100%|██████████| 200/200 [08:26<00:00,  1.87s/it]Running Inference: 100%|██████████| 200/200 [08:26<00:00,  2.53s/it]
2025-12-13 18:42:42,786 - INFO - Inference completed.
2025-12-13 18:42:42,794 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 18:42:42,794 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:42:42,798 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 18:42:42,799 - INFO - Metrics:
30.2
2025-12-13 18:42:42,800 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=hotpotqa, ratio=0.2) on GPU 2

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:42:49,229 - INFO - Set deterministic seeds to 42
2025-12-13 18:42:49,229 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:42:49,229 - INFO - Starting evaluation run...
2025-12-13 18:42:49,229 - INFO - Output directory set to: longbenchresult
2025-12-13 18:42:49,229 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 18:42:49,229 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 18:42:49,229 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:42:49,229 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.64it/s]
Device set to use cuda:0
2025-12-13 18:43:02,249 - INFO - Model pipeline loaded.
2025-12-13 18:43:02,249 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:43:07,671 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:43:07,671 - INFO - Dataset processed with 200 entries.
2025-12-13 18:43:07,704 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<16:36,  5.01s/it]Running Inference:   1%|          | 2/200 [00:05<08:03,  2.44s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:04,  2.46s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:45,  2.37s/it]Running Inference:   2%|▎         | 5/200 [00:11<05:57,  1.83s/it]Running Inference:   3%|▎         | 6/200 [00:14<07:25,  2.30s/it]Running Inference:   4%|▎         | 7/200 [00:16<07:03,  2.19s/it]Running Inference:   4%|▍         | 8/200 [00:17<06:00,  1.88s/it]Running Inference:   4%|▍         | 9/200 [00:19<05:50,  1.83s/it]Running Inference:   5%|▌         | 10/200 [00:20<04:58,  1.57s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:21<04:51,  1.54s/it]Running Inference:   6%|▌         | 12/200 [00:23<04:40,  1.49s/it]Running Inference:   6%|▋         | 13/200 [00:27<06:59,  2.25s/it]Running Inference:   7%|▋         | 14/200 [00:29<06:52,  2.22s/it]Running Inference:   8%|▊         | 15/200 [00:33<08:39,  2.81s/it]Running Inference:   8%|▊         | 16/200 [00:35<08:13,  2.68s/it]Running Inference:   8%|▊         | 17/200 [00:40<09:31,  3.12s/it]Running Inference:   9%|▉         | 18/200 [00:42<08:34,  2.83s/it]Running Inference:  10%|▉         | 19/200 [00:42<06:38,  2.20s/it]Running Inference:  10%|█         | 20/200 [00:44<05:43,  1.91s/it]Running Inference:  10%|█         | 21/200 [00:44<04:37,  1.55s/it]Running Inference:  11%|█         | 22/200 [00:46<04:47,  1.62s/it]Running Inference:  12%|█▏        | 23/200 [00:50<06:24,  2.17s/it]Running Inference:  12%|█▏        | 24/200 [00:54<08:02,  2.74s/it]Running Inference:  12%|█▎        | 25/200 [00:57<08:31,  2.92s/it]Running Inference:  13%|█▎        | 26/200 [00:58<06:51,  2.37s/it]Running Inference:  14%|█▎        | 27/200 [01:02<07:52,  2.73s/it]Running Inference:  14%|█▍        | 28/200 [01:06<08:47,  3.07s/it]Running Inference:  14%|█▍        | 29/200 [01:10<09:41,  3.40s/it]Running Inference:  15%|█▌        | 30/200 [01:14<10:23,  3.67s/it]Running Inference:  16%|█▌        | 31/200 [01:15<08:18,  2.95s/it]Running Inference:  16%|█▌        | 32/200 [01:19<09:19,  3.33s/it]Running Inference:  16%|█▋        | 33/200 [01:22<08:27,  3.04s/it]Running Inference:  17%|█▋        | 34/200 [01:24<07:40,  2.78s/it]Running Inference:  18%|█▊        | 35/200 [01:28<08:50,  3.21s/it]Running Inference:  18%|█▊        | 36/200 [01:31<08:04,  2.96s/it]Running Inference:  18%|█▊        | 37/200 [01:34<08:21,  3.08s/it]Running Inference:  19%|█▉        | 38/200 [01:36<07:35,  2.81s/it]Running Inference:  20%|█▉        | 39/200 [01:38<07:09,  2.67s/it]Running Inference:  20%|██        | 40/200 [01:41<06:59,  2.62s/it]Running Inference:  20%|██        | 41/200 [01:44<07:04,  2.67s/it]Running Inference:  21%|██        | 42/200 [01:46<06:34,  2.50s/it]Running Inference:  22%|██▏       | 43/200 [01:47<05:31,  2.11s/it]Running Inference:  22%|██▏       | 44/200 [01:51<07:01,  2.70s/it]Running Inference:  22%|██▎       | 45/200 [01:54<06:55,  2.68s/it]Running Inference:  23%|██▎       | 46/200 [01:58<07:49,  3.05s/it]Running Inference:  24%|██▎       | 47/200 [02:02<08:39,  3.39s/it]Running Inference:  24%|██▍       | 48/200 [02:03<07:04,  2.80s/it]Running Inference:  24%|██▍       | 49/200 [02:07<07:36,  3.02s/it]Running Inference:  25%|██▌       | 50/200 [02:08<06:31,  2.61s/it]Running Inference:  26%|██▌       | 51/200 [02:10<05:42,  2.30s/it]Running Inference:  26%|██▌       | 52/200 [02:12<05:38,  2.28s/it]Running Inference:  26%|██▋       | 53/200 [02:15<06:05,  2.49s/it]Running Inference:  27%|██▋       | 54/200 [02:17<05:44,  2.36s/it]Running Inference:  28%|██▊       | 55/200 [02:20<05:42,  2.36s/it]Running Inference:  28%|██▊       | 56/200 [02:23<06:29,  2.70s/it]Running Inference:  28%|██▊       | 57/200 [02:27<07:02,  2.96s/it]Running Inference:  29%|██▉       | 58/200 [02:29<06:45,  2.85s/it]Running Inference:  30%|██▉       | 59/200 [02:33<06:55,  2.95s/it]Running Inference:  30%|███       | 60/200 [02:37<07:43,  3.31s/it]Running Inference:  30%|███       | 61/200 [02:39<06:57,  3.01s/it]Running Inference:  31%|███       | 62/200 [02:41<06:25,  2.80s/it]Running Inference:  32%|███▏      | 63/200 [02:45<06:52,  3.01s/it]Running Inference:  32%|███▏      | 64/200 [02:49<07:37,  3.36s/it]Running Inference:  32%|███▎      | 65/200 [02:51<06:43,  2.99s/it]Running Inference:  33%|███▎      | 66/200 [02:53<06:02,  2.71s/it]Running Inference:  34%|███▎      | 67/200 [02:57<06:27,  2.91s/it]Running Inference:  34%|███▍      | 68/200 [03:00<07:02,  3.20s/it]Running Inference:  34%|███▍      | 69/200 [03:02<05:49,  2.67s/it]Running Inference:  35%|███▌      | 70/200 [03:05<06:21,  2.94s/it]Running Inference:  36%|███▌      | 71/200 [03:08<05:56,  2.76s/it]Running Inference:  36%|███▌      | 72/200 [03:10<05:29,  2.57s/it]Running Inference:  36%|███▋      | 73/200 [03:14<06:27,  3.05s/it]Running Inference:  37%|███▋      | 74/200 [03:16<05:33,  2.65s/it]Running Inference:  38%|███▊      | 75/200 [03:17<04:52,  2.34s/it]Running Inference:  38%|███▊      | 76/200 [03:21<05:30,  2.66s/it]Running Inference:  38%|███▊      | 77/200 [03:22<04:20,  2.12s/it]Running Inference:  39%|███▉      | 78/200 [03:24<04:18,  2.11s/it]Running Inference:  40%|███▉      | 79/200 [03:26<04:02,  2.00s/it]Running Inference:  40%|████      | 80/200 [03:26<03:17,  1.65s/it]Running Inference:  40%|████      | 81/200 [03:29<03:47,  1.91s/it]Running Inference:  41%|████      | 82/200 [03:31<03:40,  1.87s/it]Running Inference:  42%|████▏     | 83/200 [03:32<03:20,  1.72s/it]Running Inference:  42%|████▏     | 84/200 [03:36<04:24,  2.28s/it]Running Inference:  42%|████▎     | 85/200 [03:40<05:30,  2.87s/it]Running Inference:  43%|████▎     | 86/200 [03:42<05:10,  2.73s/it]Running Inference:  44%|████▎     | 87/200 [03:44<04:44,  2.51s/it]Running Inference:  44%|████▍     | 88/200 [03:46<04:05,  2.19s/it]Running Inference:  44%|████▍     | 89/200 [03:48<04:05,  2.21s/it]Running Inference:  45%|████▌     | 90/200 [03:50<03:50,  2.10s/it]Running Inference:  46%|████▌     | 91/200 [03:51<03:08,  1.73s/it]Running Inference:  46%|████▌     | 92/200 [03:55<04:30,  2.50s/it]Running Inference:  46%|████▋     | 93/200 [03:57<03:59,  2.24s/it]Running Inference:  47%|████▋     | 94/200 [04:00<04:38,  2.63s/it]Running Inference:  48%|████▊     | 95/200 [04:02<04:04,  2.33s/it]Running Inference:  48%|████▊     | 96/200 [04:04<03:55,  2.27s/it]Running Inference:  48%|████▊     | 97/200 [04:05<03:33,  2.07s/it]Running Inference:  49%|████▉     | 98/200 [04:08<03:58,  2.34s/it]Running Inference:  50%|████▉     | 99/200 [04:10<03:42,  2.20s/it]Running Inference:  50%|█████     | 100/200 [04:13<03:40,  2.21s/it]Running Inference:  50%|█████     | 101/200 [04:15<03:46,  2.29s/it]Running Inference:  51%|█████     | 102/200 [04:17<03:37,  2.22s/it]Running Inference:  52%|█████▏    | 103/200 [04:18<02:53,  1.79s/it]Running Inference:  52%|█████▏    | 104/200 [04:20<02:56,  1.83s/it]Running Inference:  52%|█████▎    | 105/200 [04:22<02:56,  1.86s/it]Running Inference:  53%|█████▎    | 106/200 [04:24<02:58,  1.90s/it]Running Inference:  54%|█████▎    | 107/200 [04:27<03:23,  2.19s/it]Running Inference:  54%|█████▍    | 108/200 [04:27<02:39,  1.74s/it]Running Inference:  55%|█████▍    | 109/200 [04:29<02:30,  1.66s/it]Running Inference:  55%|█████▌    | 110/200 [04:30<02:22,  1.59s/it]Running Inference:  56%|█████▌    | 111/200 [04:33<02:42,  1.83s/it]Running Inference:  56%|█████▌    | 112/200 [04:37<03:40,  2.51s/it]Running Inference:  56%|█████▋    | 113/200 [04:39<03:34,  2.47s/it]Running Inference:  57%|█████▋    | 114/200 [04:42<03:32,  2.47s/it]Running Inference:  57%|█████▊    | 115/200 [04:44<03:24,  2.40s/it]Running Inference:  58%|█████▊    | 116/200 [04:48<03:58,  2.84s/it]Running Inference:  58%|█████▊    | 117/200 [04:50<03:56,  2.84s/it]Running Inference:  59%|█████▉    | 118/200 [04:53<03:38,  2.66s/it]Running Inference:  60%|█████▉    | 119/200 [04:54<02:59,  2.22s/it]Running Inference:  60%|██████    | 120/200 [04:58<03:37,  2.71s/it]Running Inference:  60%|██████    | 121/200 [05:02<04:09,  3.15s/it]Running Inference:  61%|██████    | 122/200 [05:04<03:41,  2.84s/it]Running Inference:  62%|██████▏   | 123/200 [05:06<03:23,  2.64s/it]Running Inference:  62%|██████▏   | 124/200 [05:09<03:17,  2.60s/it]Running Inference:  62%|██████▎   | 125/200 [05:11<03:02,  2.43s/it]Running Inference:  63%|██████▎   | 126/200 [05:13<02:54,  2.36s/it]Running Inference:  64%|██████▎   | 127/200 [05:15<02:53,  2.38s/it]Running Inference:  64%|██████▍   | 128/200 [05:20<03:32,  2.95s/it]Running Inference:  64%|██████▍   | 129/200 [05:22<03:11,  2.69s/it]Running Inference:  65%|██████▌   | 130/200 [05:24<02:49,  2.42s/it]Running Inference:  66%|██████▌   | 131/200 [05:27<03:16,  2.85s/it]Running Inference:  66%|██████▌   | 132/200 [05:31<03:19,  2.93s/it]Running Inference:  66%|██████▋   | 133/200 [05:32<02:44,  2.45s/it]Running Inference:  67%|██████▋   | 134/200 [05:34<02:38,  2.40s/it]Running Inference:  68%|██████▊   | 135/200 [05:36<02:28,  2.28s/it]Running Inference:  68%|██████▊   | 136/200 [05:40<03:00,  2.83s/it]Running Inference:  68%|██████▊   | 137/200 [05:42<02:43,  2.59s/it]Running Inference:  69%|██████▉   | 138/200 [05:45<02:33,  2.48s/it]Running Inference:  70%|██████▉   | 139/200 [05:49<03:02,  2.98s/it]Running Inference:  70%|███████   | 140/200 [05:51<02:55,  2.93s/it]Running Inference:  70%|███████   | 141/200 [05:54<02:39,  2.70s/it]Running Inference:  71%|███████   | 142/200 [05:58<03:00,  3.11s/it]Running Inference:  72%|███████▏  | 143/200 [05:58<02:16,  2.39s/it]Running Inference:  72%|███████▏  | 144/200 [05:59<01:47,  1.92s/it]Running Inference:  72%|███████▎  | 145/200 [06:01<01:40,  1.82s/it]Running Inference:  73%|███████▎  | 146/200 [06:05<02:16,  2.52s/it]Running Inference:  74%|███████▎  | 147/200 [06:07<02:07,  2.40s/it]Running Inference:  74%|███████▍  | 148/200 [06:10<02:18,  2.67s/it]Running Inference:  74%|███████▍  | 149/200 [06:11<01:43,  2.04s/it]Running Inference:  75%|███████▌  | 150/200 [06:12<01:33,  1.87s/it]Running Inference:  76%|███████▌  | 151/200 [06:16<01:52,  2.29s/it]Running Inference:  76%|███████▌  | 152/200 [06:20<02:12,  2.76s/it]Running Inference:  76%|███████▋  | 153/200 [06:22<01:58,  2.52s/it]Running Inference:  77%|███████▋  | 154/200 [06:24<01:52,  2.45s/it]Running Inference:  78%|███████▊  | 155/200 [06:28<02:12,  2.94s/it]Running Inference:  78%|███████▊  | 156/200 [06:29<01:45,  2.39s/it]Running Inference:  78%|███████▊  | 157/200 [06:30<01:30,  2.10s/it]Running Inference:  79%|███████▉  | 158/200 [06:33<01:36,  2.30s/it]Running Inference:  80%|███████▉  | 159/200 [06:35<01:33,  2.29s/it]Running Inference:  80%|████████  | 160/200 [06:40<01:52,  2.82s/it]Running Inference:  80%|████████  | 161/200 [06:44<02:06,  3.23s/it]Running Inference:  81%|████████  | 162/200 [06:44<01:32,  2.43s/it]Running Inference:  82%|████████▏ | 163/200 [06:48<01:44,  2.83s/it]Running Inference:  82%|████████▏ | 164/200 [06:50<01:34,  2.61s/it]Running Inference:  82%|████████▎ | 165/200 [06:54<01:47,  3.06s/it]Running Inference:  83%|████████▎ | 166/200 [06:56<01:33,  2.76s/it]Running Inference:  84%|████████▎ | 167/200 [06:58<01:23,  2.53s/it]Running Inference:  84%|████████▍ | 168/200 [07:02<01:34,  2.96s/it]Running Inference:  84%|████████▍ | 169/200 [07:06<01:43,  3.33s/it]Running Inference:  85%|████████▌ | 170/200 [07:10<01:45,  3.53s/it]Running Inference:  86%|████████▌ | 171/200 [07:14<01:42,  3.54s/it]Running Inference:  86%|████████▌ | 172/200 [07:18<01:44,  3.72s/it]Running Inference:  86%|████████▋ | 173/200 [07:20<01:27,  3.22s/it]Running Inference:  87%|████████▋ | 174/200 [07:23<01:18,  3.02s/it]Running Inference:  88%|████████▊ | 175/200 [07:25<01:09,  2.77s/it]Running Inference:  88%|████████▊ | 176/200 [07:27<01:01,  2.57s/it]Running Inference:  88%|████████▊ | 177/200 [07:31<01:10,  3.08s/it]Running Inference:  89%|████████▉ | 178/200 [07:36<01:15,  3.43s/it]Running Inference:  90%|████████▉ | 179/200 [07:38<01:04,  3.08s/it]Running Inference:  90%|█████████ | 180/200 [07:41<01:04,  3.20s/it]Running Inference:  90%|█████████ | 181/200 [07:43<00:55,  2.90s/it]Running Inference:  91%|█████████ | 182/200 [07:45<00:46,  2.58s/it]Running Inference:  92%|█████████▏| 183/200 [07:47<00:38,  2.27s/it]Running Inference:  92%|█████████▏| 184/200 [07:51<00:44,  2.78s/it]Running Inference:  92%|█████████▎| 185/200 [07:52<00:36,  2.41s/it]Running Inference:  93%|█████████▎| 186/200 [07:56<00:40,  2.89s/it]Running Inference:  94%|█████████▎| 187/200 [07:58<00:33,  2.56s/it]Running Inference:  94%|█████████▍| 188/200 [08:01<00:31,  2.62s/it]Running Inference:  94%|█████████▍| 189/200 [08:03<00:27,  2.48s/it]Running Inference:  95%|█████████▌| 190/200 [08:05<00:23,  2.36s/it]Running Inference:  96%|█████████▌| 191/200 [08:07<00:20,  2.28s/it]Running Inference:  96%|█████████▌| 192/200 [08:11<00:20,  2.61s/it]Running Inference:  96%|█████████▋| 193/200 [08:15<00:21,  3.13s/it]Running Inference:  97%|█████████▋| 194/200 [08:18<00:19,  3.23s/it]Running Inference:  98%|█████████▊| 195/200 [08:21<00:15,  3.16s/it]Running Inference:  98%|█████████▊| 196/200 [08:24<00:11,  2.87s/it]Running Inference:  98%|█████████▊| 197/200 [08:25<00:07,  2.42s/it]Running Inference:  99%|█████████▉| 198/200 [08:28<00:05,  2.56s/it]Running Inference: 100%|█████████▉| 199/200 [08:32<00:03,  3.06s/it]Running Inference: 100%|██████████| 200/200 [08:33<00:00,  2.29s/it]Running Inference: 100%|██████████| 200/200 [08:33<00:00,  2.57s/it]
2025-12-13 18:51:40,856 - INFO - Inference completed.
2025-12-13 18:51:40,864 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-13 18:51:40,864 - INFO - Calculating metrics for dataset: longbench
2025-12-13 18:51:40,868 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-13 18:51:40,868 - INFO - Metrics:
29.79
2025-12-13 18:51:40,870 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=hotpotqa, ratio=0.3) on GPU 2

----------------------------------------
Task: hotpotqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 18:51:47,294 - INFO - Set deterministic seeds to 42
2025-12-13 18:51:47,294 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "hotpotqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 18:51:47,294 - INFO - Starting evaluation run...
2025-12-13 18:51:47,294 - INFO - Output directory set to: longbenchresult
2025-12-13 18:51:47,294 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-13 18:51:47,294 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 18:51:47,294 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 18:51:47,294 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.65it/s]
Device set to use cuda:0
2025-12-13 18:52:00,256 - INFO - Model pipeline loaded.
2025-12-13 18:52:00,256 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: hotpotqa)
2025-12-13 18:52:03,845 - INFO - Dataset loaded with 200 entries.
2025-12-13 18:52:03,845 - INFO - Dataset processed with 200 entries.
2025-12-13 18:52:03,880 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<16:53,  5.09s/it]Running Inference:   1%|          | 2/200 [00:05<08:10,  2.48s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:10,  2.49s/it]Running Inference:   2%|▏         | 4/200 [00:10<07:48,  2.39s/it]Running Inference:   2%|▎         | 5/200 [00:11<06:00,  1.85s/it]Running Inference:   3%|▎         | 6/200 [00:13<05:59,  1.85s/it]Running Inference:   4%|▎         | 7/200 [00:15<06:05,  1.90s/it]Running Inference:   4%|▍         | 8/200 [00:16<05:21,  1.68s/it]Running Inference:   4%|▍         | 9/200 [00:18<05:23,  1.69s/it]Running Inference:   5%|▌         | 10/200 [00:19<04:40,  1.47s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:20<04:38,  1.47s/it]Running Inference:   6%|▌         | 12/200 [00:21<04:22,  1.40s/it]Running Inference:   6%|▋         | 13/200 [00:25<06:48,  2.19s/it]Running Inference:   7%|▋         | 14/200 [00:27<06:45,  2.18s/it]Running Inference:   8%|▊         | 15/200 [00:31<07:41,  2.49s/it]Running Inference:   8%|▊         | 16/200 [00:33<07:36,  2.48s/it]Running Inference:   8%|▊         | 17/200 [00:37<09:07,  2.99s/it]Running Inference:   9%|▉         | 18/200 [00:39<08:17,  2.73s/it]Running Inference:  10%|▉         | 19/200 [00:40<06:26,  2.13s/it]Running Inference:  10%|█         | 20/200 [00:41<05:37,  1.87s/it]Running Inference:  10%|█         | 21/200 [00:42<04:33,  1.53s/it]Running Inference:  11%|█         | 22/200 [00:44<04:43,  1.59s/it]Running Inference:  12%|█▏        | 23/200 [00:47<06:23,  2.16s/it]Running Inference:  12%|█▏        | 24/200 [00:52<08:04,  2.75s/it]Running Inference:  12%|█▎        | 25/200 [00:54<07:57,  2.73s/it]Running Inference:  13%|█▎        | 26/200 [00:55<06:27,  2.23s/it]Running Inference:  14%|█▎        | 27/200 [00:59<07:39,  2.66s/it]Running Inference:  14%|█▍        | 28/200 [01:03<08:40,  3.03s/it]Running Inference:  14%|█▍        | 29/200 [01:07<09:39,  3.39s/it]Running Inference:  15%|█▌        | 30/200 [01:11<10:23,  3.67s/it]Running Inference:  16%|█▌        | 31/200 [01:13<08:18,  2.95s/it]Running Inference:  16%|█▌        | 32/200 [01:17<09:21,  3.34s/it]Running Inference:  16%|█▋        | 33/200 [01:19<08:28,  3.05s/it]Running Inference:  17%|█▋        | 34/200 [01:22<08:03,  2.91s/it]Running Inference:  18%|█▊        | 35/200 [01:26<09:07,  3.32s/it]Running Inference:  18%|█▊        | 36/200 [01:29<08:17,  3.03s/it]Running Inference:  18%|█▊        | 37/200 [01:32<08:32,  3.14s/it]Running Inference:  19%|█▉        | 38/200 [01:34<07:42,  2.85s/it]Running Inference:  20%|█▉        | 39/200 [01:36<07:14,  2.70s/it]Running Inference:  20%|██        | 40/200 [01:39<07:02,  2.64s/it]Running Inference:  20%|██        | 41/200 [01:42<07:08,  2.69s/it]Running Inference:  21%|██        | 42/200 [01:44<06:37,  2.51s/it]Running Inference:  22%|██▏       | 43/200 [01:47<07:07,  2.72s/it]Running Inference:  22%|██▏       | 44/200 [01:51<08:09,  3.14s/it]Running Inference:  22%|██▎       | 45/200 [01:55<08:48,  3.41s/it]Running Inference:  23%|██▎       | 46/200 [01:59<09:04,  3.53s/it]Running Inference:  24%|██▎       | 47/200 [02:03<09:32,  3.74s/it]Running Inference:  24%|██▍       | 48/200 [02:05<07:42,  3.04s/it]Running Inference:  24%|██▍       | 49/200 [02:08<07:43,  3.07s/it]Running Inference:  25%|██▌       | 50/200 [02:09<06:37,  2.65s/it]Running Inference:  26%|██▌       | 51/200 [02:11<05:50,  2.35s/it]Running Inference:  26%|██▌       | 52/200 [02:13<05:43,  2.32s/it]Running Inference:  26%|██▋       | 53/200 [02:14<04:44,  1.94s/it]Running Inference:  27%|██▋       | 54/200 [02:16<04:48,  1.97s/it]Running Inference:  28%|██▊       | 55/200 [02:19<04:52,  2.02s/it]Running Inference:  28%|██▊       | 56/200 [02:21<05:00,  2.09s/it]Running Inference:  28%|██▊       | 57/200 [02:24<05:37,  2.36s/it]Running Inference:  29%|██▉       | 58/200 [02:25<05:04,  2.14s/it]Running Inference:  30%|██▉       | 59/200 [02:29<05:47,  2.47s/it]Running Inference:  30%|███       | 60/200 [02:32<06:18,  2.70s/it]Running Inference:  30%|███       | 61/200 [02:34<05:51,  2.53s/it]Running Inference:  31%|███       | 62/200 [02:36<05:40,  2.47s/it]Running Inference:  32%|███▏      | 63/200 [02:40<06:22,  2.79s/it]Running Inference:  32%|███▏      | 64/200 [02:44<07:18,  3.23s/it]Running Inference:  32%|███▎      | 65/200 [02:46<06:35,  2.93s/it]Running Inference:  33%|███▎      | 66/200 [02:48<05:57,  2.67s/it]Running Inference:  34%|███▎      | 67/200 [02:50<05:03,  2.28s/it]Running Inference:  34%|███▍      | 68/200 [02:52<04:42,  2.14s/it]Running Inference:  34%|███▍      | 69/200 [02:53<04:12,  1.93s/it]Running Inference:  35%|███▌      | 70/200 [02:57<05:16,  2.43s/it]Running Inference:  36%|███▌      | 71/200 [02:59<05:11,  2.41s/it]Running Inference:  36%|███▌      | 72/200 [03:01<04:58,  2.33s/it]Running Inference:  36%|███▋      | 73/200 [03:05<06:08,  2.90s/it]Running Inference:  37%|███▋      | 74/200 [03:09<06:35,  3.14s/it]Running Inference:  38%|███▊      | 75/200 [03:11<05:35,  2.69s/it]Running Inference:  38%|███▊      | 76/200 [03:14<05:53,  2.85s/it]Running Inference:  38%|███▊      | 77/200 [03:15<04:36,  2.25s/it]Running Inference:  39%|███▉      | 78/200 [03:19<05:35,  2.75s/it]Running Inference:  40%|███▉      | 79/200 [03:20<04:49,  2.39s/it]Running Inference:  40%|████      | 80/200 [03:21<03:50,  1.92s/it]Running Inference:  40%|████      | 81/200 [03:24<04:21,  2.20s/it]Running Inference:  41%|████      | 82/200 [03:26<04:04,  2.07s/it]Running Inference:  42%|████▏     | 83/200 [03:27<03:37,  1.86s/it]Running Inference:  42%|████▏     | 84/200 [03:31<04:37,  2.39s/it]Running Inference:  42%|████▎     | 85/200 [03:35<05:41,  2.97s/it]Running Inference:  43%|████▎     | 86/200 [03:37<05:18,  2.79s/it]Running Inference:  44%|████▎     | 87/200 [03:41<05:54,  3.14s/it]Running Inference:  44%|████▍     | 88/200 [03:43<04:54,  2.63s/it]Running Inference:  44%|████▍     | 89/200 [03:45<04:40,  2.53s/it]Running Inference:  45%|████▌     | 90/200 [03:47<04:15,  2.33s/it]Running Inference:  46%|████▌     | 91/200 [03:48<03:26,  1.89s/it]Running Inference:  46%|████▌     | 92/200 [03:50<03:43,  2.07s/it]Running Inference:  46%|████▋     | 93/200 [03:54<04:33,  2.55s/it]Running Inference:  47%|████▋     | 94/200 [03:57<04:57,  2.80s/it]Running Inference:  48%|████▊     | 95/200 [03:59<04:17,  2.45s/it]Running Inference:  48%|████▊     | 96/200 [04:01<04:04,  2.35s/it]Running Inference:  48%|████▊     | 97/200 [04:03<03:40,  2.14s/it]Running Inference:  49%|████▉     | 98/200 [04:05<03:48,  2.24s/it]Running Inference:  50%|████▉     | 99/200 [04:07<03:35,  2.13s/it]Running Inference:  50%|█████     | 100/200 [04:11<04:34,  2.75s/it]Running Inference:  50%|█████     | 101/200 [04:14<04:24,  2.67s/it]Running Inference:  51%|█████     | 102/200 [04:16<04:03,  2.48s/it]Running Inference:  52%|█████▏    | 103/200 [04:17<03:12,  1.98s/it]Running Inference:  52%|█████▏    | 104/200 [04:19<03:08,  1.97s/it]Running Inference:  52%|█████▎    | 105/200 [04:22<03:48,  2.40s/it]Running Inference:  53%|█████▎    | 106/200 [04:24<03:34,  2.28s/it]Running Inference:  54%|█████▎    | 107/200 [04:27<03:49,  2.46s/it]Running Inference:  54%|█████▍    | 108/200 [04:28<02:57,  1.93s/it]Running Inference:  55%|█████▍    | 109/200 [04:29<02:43,  1.79s/it]Running Inference:  55%|█████▌    | 110/200 [04:31<02:31,  1.69s/it]Running Inference:  56%|█████▌    | 111/200 [04:35<03:37,  2.45s/it]Running Inference:  56%|█████▌    | 112/200 [04:39<04:19,  2.95s/it]Running Inference:  56%|█████▋    | 113/200 [04:41<04:01,  2.78s/it]Running Inference:  57%|█████▋    | 114/200 [04:44<03:50,  2.69s/it]Running Inference:  57%|█████▊    | 115/200 [04:46<03:37,  2.55s/it]Running Inference:  58%|█████▊    | 116/200 [04:50<04:08,  2.96s/it]Running Inference:  58%|█████▊    | 117/200 [04:52<03:51,  2.79s/it]Running Inference:  59%|█████▉    | 118/200 [04:56<04:23,  3.21s/it]Running Inference:  60%|█████▉    | 119/200 [04:59<04:14,  3.14s/it]Running Inference:  60%|██████    | 120/200 [05:03<04:29,  3.37s/it]Running Inference:  60%|██████    | 121/200 [05:06<04:00,  3.04s/it]Running Inference:  61%|██████    | 122/200 [05:08<03:35,  2.76s/it]Running Inference:  62%|██████▏   | 123/200 [05:10<03:17,  2.57s/it]Running Inference:  62%|██████▏   | 124/200 [05:12<03:11,  2.51s/it]Running Inference:  62%|██████▎   | 125/200 [05:14<02:57,  2.37s/it]Running Inference:  63%|██████▎   | 126/200 [05:16<02:51,  2.32s/it]Running Inference:  64%|██████▎   | 127/200 [05:17<02:14,  1.84s/it]Running Inference:  64%|██████▍   | 128/200 [05:22<03:05,  2.58s/it]Running Inference:  64%|██████▍   | 129/200 [05:24<02:53,  2.44s/it]Running Inference:  65%|██████▌   | 130/200 [05:25<02:37,  2.24s/it]Running Inference:  66%|██████▌   | 131/200 [05:29<02:54,  2.53s/it]Running Inference:  66%|██████▌   | 132/200 [05:32<03:07,  2.76s/it]Running Inference:  66%|██████▋   | 133/200 [05:33<02:36,  2.34s/it]Running Inference:  67%|██████▋   | 134/200 [05:36<02:32,  2.32s/it]Running Inference:  68%|██████▊   | 135/200 [05:38<02:24,  2.22s/it]Running Inference:  68%|██████▊   | 136/200 [05:42<02:58,  2.79s/it]Running Inference:  68%|██████▊   | 137/200 [05:44<02:41,  2.57s/it]Running Inference:  69%|██████▉   | 138/200 [05:46<02:33,  2.48s/it]Running Inference:  70%|██████▉   | 139/200 [05:50<03:02,  2.99s/it]Running Inference:  70%|███████   | 140/200 [05:54<03:15,  3.25s/it]Running Inference:  70%|███████   | 141/200 [05:56<02:52,  2.93s/it]Running Inference:  71%|███████   | 142/200 [06:00<03:09,  3.27s/it]Running Inference:  72%|███████▏  | 143/200 [06:03<02:51,  3.02s/it]Running Inference:  72%|███████▏  | 144/200 [06:03<02:11,  2.36s/it]Running Inference:  72%|███████▎  | 145/200 [06:05<01:49,  1.99s/it]Running Inference:  73%|███████▎  | 146/200 [06:07<01:55,  2.14s/it]Running Inference:  74%|███████▎  | 147/200 [06:09<01:52,  2.13s/it]Running Inference:  74%|███████▍  | 148/200 [06:12<01:54,  2.20s/it]Running Inference:  74%|███████▍  | 149/200 [06:12<01:27,  1.71s/it]Running Inference:  75%|███████▌  | 150/200 [06:14<01:21,  1.64s/it]Running Inference:  76%|███████▌  | 151/200 [06:17<01:39,  2.02s/it]Running Inference:  76%|███████▌  | 152/200 [06:20<02:03,  2.58s/it]Running Inference:  76%|███████▋  | 153/200 [06:22<01:52,  2.40s/it]Running Inference:  77%|███████▋  | 154/200 [06:25<01:48,  2.36s/it]Running Inference:  78%|███████▊  | 155/200 [06:29<02:09,  2.88s/it]Running Inference:  78%|███████▊  | 156/200 [06:32<02:09,  2.95s/it]Running Inference:  78%|███████▊  | 157/200 [06:33<01:47,  2.49s/it]Running Inference:  79%|███████▉  | 158/200 [06:35<01:29,  2.13s/it]Running Inference:  80%|███████▉  | 159/200 [06:38<01:44,  2.55s/it]Running Inference:  80%|████████  | 160/200 [06:41<01:49,  2.75s/it]Running Inference:  80%|████████  | 161/200 [06:45<01:54,  2.93s/it]Running Inference:  81%|████████  | 162/200 [06:45<01:24,  2.22s/it]Running Inference:  82%|████████▏ | 163/200 [06:49<01:39,  2.69s/it]Running Inference:  82%|████████▏ | 164/200 [06:51<01:30,  2.51s/it]Running Inference:  82%|████████▎ | 165/200 [06:55<01:44,  2.99s/it]Running Inference:  83%|████████▎ | 166/200 [06:59<01:50,  3.25s/it]Running Inference:  84%|████████▎ | 167/200 [07:01<01:35,  2.91s/it]Running Inference:  84%|████████▍ | 168/200 [07:05<01:43,  3.23s/it]Running Inference:  84%|████████▍ | 169/200 [07:09<01:49,  3.52s/it]Running Inference:  85%|████████▌ | 170/200 [07:13<01:46,  3.54s/it]Running Inference:  86%|████████▌ | 171/200 [07:17<01:43,  3.58s/it]Running Inference:  86%|████████▌ | 172/200 [07:19<01:33,  3.33s/it]Running Inference:  86%|████████▋ | 173/200 [07:21<01:19,  2.95s/it]Running Inference:  87%|████████▋ | 174/200 [07:25<01:22,  3.18s/it]Running Inference:  88%|████████▊ | 175/200 [07:27<01:11,  2.88s/it]Running Inference:  88%|████████▊ | 176/200 [07:29<01:03,  2.64s/it]Running Inference:  88%|████████▊ | 177/200 [07:34<01:12,  3.14s/it]Running Inference:  89%|████████▉ | 178/200 [07:38<01:16,  3.46s/it]Running Inference:  90%|████████▉ | 179/200 [07:40<01:05,  3.10s/it]Running Inference:  90%|█████████ | 180/200 [07:44<01:04,  3.23s/it]Running Inference:  90%|█████████ | 181/200 [07:46<00:55,  2.92s/it]Running Inference:  91%|█████████ | 182/200 [07:48<00:46,  2.59s/it]Running Inference:  92%|█████████▏| 183/200 [07:49<00:38,  2.28s/it]Running Inference:  92%|█████████▏| 184/200 [07:53<00:44,  2.78s/it]Running Inference:  92%|█████████▎| 185/200 [07:57<00:45,  3.01s/it]Running Inference:  93%|█████████▎| 186/200 [08:01<00:46,  3.32s/it]Running Inference:  94%|█████████▎| 187/200 [08:05<00:45,  3.50s/it]Running Inference:  94%|█████████▍| 188/200 [08:08<00:39,  3.29s/it]Running Inference:  94%|█████████▍| 189/200 [08:10<00:32,  2.94s/it]Running Inference:  95%|█████████▌| 190/200 [08:13<00:29,  2.99s/it]Running Inference:  96%|█████████▌| 191/200 [08:17<00:29,  3.28s/it]Running Inference:  96%|█████████▌| 192/200 [08:18<00:21,  2.70s/it]Running Inference:  96%|█████████▋| 193/200 [08:22<00:22,  3.19s/it]Running Inference:  97%|█████████▋| 194/200 [08:26<00:18,  3.17s/it]Running Inference:  98%|█████████▊| 195/200 [08:29<00:15,  3.12s/it]Running Inference:  98%|█████████▊| 196/200 [08:31<00:11,  2.83s/it]Running Inference:  98%|█████████▊| 197/200 [08:32<00:07,  2.40s/it]Running Inference:  99%|█████████▉| 198/200 [08:35<00:05,  2.57s/it]Running Inference: 100%|█████████▉| 199/200 [08:39<00:03,  3.08s/it]Running Inference: 100%|██████████| 200/200 [08:40<00:00,  2.31s/it]Running Inference: 100%|██████████| 200/200 [08:40<00:00,  2.60s/it]
2025-12-13 19:00:44,249 - INFO - Inference completed.
2025-12-13 19:00:44,257 - INFO - Results saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-13 19:00:44,257 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:00:44,262 - INFO - Metrics saved to longbenchresult/longbench__hotpotqa__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-13 19:00:44,262 - INFO - Metrics:
26.9
2025-12-13 19:00:44,264 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=hotpotqa, ratio=0.5) on GPU 2


========================================
LongBench Task: multifieldqa_en
========================================
----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:00:50,766 - INFO - Set deterministic seeds to 42
2025-12-13 19:00:50,766 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:00:50,766 - INFO - Starting evaluation run...
2025-12-13 19:00:50,766 - INFO - Output directory set to: longbenchresult
2025-12-13 19:00:50,766 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 19:00:50,766 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:00:50,766 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:00:50,766 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.98it/s]
Device set to use cuda:0
2025-12-13 19:01:03,065 - INFO - Model pipeline loaded.
2025-12-13 19:01:03,065 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:01:08,616 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:01:08,616 - INFO - Dataset processed with 150 entries.
2025-12-13 19:01:08,628 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:03<05:45,  3.11s/it]Running Inference:   2%|▏         | 2/112 [00:03<03:05,  1.69s/it]Running Inference:   3%|▎         | 3/112 [00:05<02:58,  1.64s/it]Running Inference:   4%|▎         | 4/112 [00:08<03:41,  2.05s/it]Running Inference:   4%|▍         | 5/112 [00:12<04:56,  2.77s/it]Running Inference:   5%|▌         | 6/112 [00:14<04:54,  2.78s/it]Running Inference:   6%|▋         | 7/112 [00:19<05:44,  3.28s/it]Running Inference:   7%|▋         | 8/112 [00:23<06:21,  3.67s/it]Running Inference:   8%|▊         | 9/112 [00:24<04:53,  2.85s/it]Running Inference:   9%|▉         | 10/112 [00:29<05:58,  3.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:31<05:02,  2.99s/it]Running Inference:  11%|█         | 12/112 [00:35<05:34,  3.35s/it]Running Inference:  12%|█▏        | 13/112 [00:37<04:30,  2.74s/it]Running Inference:  12%|█▎        | 14/112 [00:40<05:01,  3.08s/it]Running Inference:  13%|█▎        | 15/112 [00:41<03:50,  2.37s/it]Running Inference:  14%|█▍        | 16/112 [00:42<03:01,  1.89s/it]Running Inference:  15%|█▌        | 17/112 [00:42<02:18,  1.45s/it]Running Inference:  16%|█▌        | 18/112 [00:47<03:41,  2.36s/it]Running Inference:  17%|█▋        | 19/112 [00:51<04:42,  3.04s/it]Running Inference:  18%|█▊        | 20/112 [00:53<04:07,  2.69s/it]Running Inference:  19%|█▉        | 21/112 [00:55<03:48,  2.51s/it]Running Inference:  20%|█▉        | 22/112 [00:57<03:23,  2.26s/it]Running Inference:  21%|██        | 23/112 [00:59<02:58,  2.00s/it]Running Inference:  21%|██▏       | 24/112 [01:00<02:32,  1.73s/it]Running Inference:  22%|██▏       | 25/112 [01:01<02:11,  1.51s/it]Running Inference:  23%|██▎       | 26/112 [01:05<03:28,  2.42s/it]Running Inference:  24%|██▍       | 27/112 [01:10<04:36,  3.25s/it]Running Inference:  25%|██▌       | 28/112 [01:12<03:43,  2.66s/it]Running Inference:  26%|██▌       | 29/112 [01:13<03:13,  2.33s/it]Running Inference:  27%|██▋       | 30/112 [01:17<03:57,  2.89s/it]Running Inference:  28%|██▊       | 31/112 [01:18<03:09,  2.34s/it]Running Inference:  29%|██▊       | 32/112 [01:20<02:56,  2.21s/it]Running Inference:  29%|██▉       | 33/112 [01:25<03:51,  2.93s/it]Running Inference:  30%|███       | 34/112 [01:27<03:27,  2.66s/it]Running Inference:  31%|███▏      | 35/112 [01:31<04:06,  3.21s/it]Running Inference:  32%|███▏      | 36/112 [01:32<03:03,  2.42s/it]Running Inference:  33%|███▎      | 37/112 [01:32<02:16,  1.82s/it]Running Inference:  34%|███▍      | 38/112 [01:33<01:55,  1.56s/it]Running Inference:  35%|███▍      | 39/112 [01:35<01:50,  1.52s/it]Running Inference:  36%|███▌      | 40/112 [01:36<01:47,  1.49s/it]Running Inference:  37%|███▋      | 41/112 [01:37<01:37,  1.38s/it]Running Inference:  38%|███▊      | 42/112 [01:42<02:35,  2.22s/it]Running Inference:  38%|███▊      | 43/112 [01:45<03:00,  2.62s/it]Running Inference:  39%|███▉      | 44/112 [01:49<03:32,  3.12s/it]Running Inference:  40%|████      | 45/112 [01:54<03:51,  3.46s/it]Running Inference:  41%|████      | 46/112 [01:55<03:03,  2.78s/it]Running Inference:  42%|████▏     | 47/112 [01:56<02:27,  2.26s/it]Running Inference:  43%|████▎     | 48/112 [01:58<02:16,  2.13s/it]Running Inference:  44%|████▍     | 49/112 [01:59<01:56,  1.84s/it]Running Inference:  45%|████▍     | 50/112 [02:00<01:46,  1.72s/it]Running Inference:  46%|████▌     | 51/112 [02:01<01:33,  1.53s/it]Running Inference:  46%|████▋     | 52/112 [02:05<02:10,  2.17s/it]Running Inference:  47%|████▋     | 53/112 [02:07<02:09,  2.19s/it]Running Inference:  48%|████▊     | 54/112 [02:09<01:59,  2.06s/it]Running Inference:  49%|████▉     | 55/112 [02:12<02:04,  2.18s/it]Running Inference:  50%|█████     | 56/112 [02:12<01:37,  1.75s/it]Running Inference:  51%|█████     | 57/112 [02:14<01:43,  1.88s/it]Running Inference:  52%|█████▏    | 58/112 [02:17<01:55,  2.14s/it]Running Inference:  53%|█████▎    | 59/112 [02:21<02:17,  2.59s/it]Running Inference:  54%|█████▎    | 60/112 [02:22<01:46,  2.05s/it]Running Inference:  54%|█████▍    | 61/112 [02:25<02:01,  2.38s/it]Running Inference:  55%|█████▌    | 62/112 [02:27<01:49,  2.20s/it]Running Inference:  56%|█████▋    | 63/112 [02:31<02:18,  2.83s/it]Running Inference:  57%|█████▋    | 64/112 [02:35<02:33,  3.19s/it]Running Inference:  58%|█████▊    | 65/112 [02:37<02:11,  2.81s/it]Running Inference:  59%|█████▉    | 66/112 [02:41<02:29,  3.24s/it]Running Inference:  60%|█████▉    | 67/112 [02:43<02:14,  2.98s/it]Running Inference:  61%|██████    | 68/112 [02:45<01:53,  2.57s/it]Running Inference:  62%|██████▏   | 69/112 [02:46<01:29,  2.09s/it]Running Inference:  62%|██████▎   | 70/112 [02:47<01:10,  1.68s/it]Running Inference:  63%|██████▎   | 71/112 [02:48<00:58,  1.42s/it]Running Inference:  64%|██████▍   | 72/112 [02:49<00:56,  1.42s/it]Running Inference:  65%|██████▌   | 73/112 [02:51<00:57,  1.47s/it]Running Inference:  66%|██████▌   | 74/112 [02:54<01:20,  2.11s/it]Running Inference:  67%|██████▋   | 75/112 [03:00<01:58,  3.19s/it]Running Inference:  68%|██████▊   | 76/112 [03:00<01:25,  2.36s/it]Running Inference:  69%|██████▉   | 77/112 [03:01<01:09,  2.00s/it]Running Inference:  70%|██████▉   | 78/112 [03:07<01:43,  3.04s/it]Running Inference:  71%|███████   | 79/112 [03:08<01:22,  2.50s/it]Running Inference:  71%|███████▏  | 80/112 [03:10<01:13,  2.31s/it]Running Inference:  72%|███████▏  | 81/112 [03:12<01:03,  2.06s/it]Running Inference:  73%|███████▎  | 82/112 [03:13<00:55,  1.86s/it]Running Inference:  74%|███████▍  | 83/112 [03:23<02:03,  4.27s/it]Running Inference:  75%|███████▌  | 84/112 [03:28<02:05,  4.49s/it]Running Inference:  76%|███████▌  | 85/112 [03:28<01:27,  3.25s/it]Running Inference:  77%|███████▋  | 86/112 [03:29<01:05,  2.51s/it]Running Inference:  78%|███████▊  | 87/112 [03:33<01:16,  3.08s/it]Running Inference:  79%|███████▊  | 88/112 [03:38<01:26,  3.59s/it]Running Inference:  79%|███████▉  | 89/112 [03:44<01:40,  4.36s/it]Running Inference:  80%|████████  | 90/112 [03:46<01:19,  3.61s/it]Running Inference:  81%|████████▏ | 91/112 [03:51<01:21,  3.89s/it]Running Inference:  82%|████████▏ | 92/112 [03:53<01:07,  3.38s/it]Running Inference:  83%|████████▎ | 93/112 [03:57<01:07,  3.55s/it]Running Inference:  84%|████████▍ | 94/112 [03:59<00:55,  3.07s/it]Running Inference:  85%|████████▍ | 95/112 [04:01<00:46,  2.72s/it]Running Inference:  86%|████████▌ | 96/112 [04:02<00:34,  2.17s/it]Running Inference:  87%|████████▋ | 97/112 [04:04<00:35,  2.39s/it]Running Inference:  88%|████████▊ | 98/112 [04:07<00:32,  2.29s/it]Running Inference:  88%|████████▊ | 99/112 [04:09<00:28,  2.20s/it]Running Inference:  89%|████████▉ | 100/112 [04:17<00:48,  4.03s/it]Running Inference:  90%|█████████ | 101/112 [04:18<00:36,  3.28s/it]Running Inference:  91%|█████████ | 102/112 [04:23<00:36,  3.66s/it]Running Inference:  92%|█████████▏| 103/112 [04:25<00:28,  3.15s/it]Running Inference:  93%|█████████▎| 104/112 [04:26<00:19,  2.49s/it]Running Inference:  94%|█████████▍| 105/112 [04:29<00:17,  2.57s/it]Running Inference:  95%|█████████▍| 106/112 [04:33<00:19,  3.23s/it]Running Inference:  96%|█████████▌| 107/112 [04:37<00:16,  3.24s/it]Running Inference:  96%|█████████▋| 108/112 [04:44<00:18,  4.59s/it]Running Inference:  97%|█████████▋| 109/112 [04:46<00:11,  3.80s/it]Running Inference:  98%|█████████▊| 110/112 [04:49<00:06,  3.33s/it]Running Inference:  99%|█████████▉| 111/112 [04:51<00:02,  2.98s/it]Running Inference: 100%|██████████| 112/112 [04:52<00:00,  2.39s/it]Running Inference: 100%|██████████| 112/112 [04:52<00:00,  2.61s/it]
2025-12-13 19:06:00,846 - INFO - Inference completed.
2025-12-13 19:06:00,854 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 19:06:00,854 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:06:00,859 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 19:06:00,859 - INFO - Metrics:
27.14
2025-12-13 19:06:00,861 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_en, ratio=0.1) on GPU 2

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:06:07,300 - INFO - Set deterministic seeds to 42
2025-12-13 19:06:07,300 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:06:07,300 - INFO - Starting evaluation run...
2025-12-13 19:06:07,300 - INFO - Output directory set to: longbenchresult
2025-12-13 19:06:07,300 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 19:06:07,301 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:06:07,301 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:06:07,301 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.70it/s]
Device set to use cuda:0
2025-12-13 19:06:26,501 - INFO - Model pipeline loaded.
2025-12-13 19:06:26,501 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:06:31,311 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:06:31,311 - INFO - Dataset processed with 150 entries.
2025-12-13 19:06:31,324 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<05:03,  2.74s/it]Running Inference:   2%|▏         | 2/112 [00:03<02:47,  1.53s/it]Running Inference:   3%|▎         | 3/112 [00:06<03:53,  2.15s/it]Running Inference:   4%|▎         | 4/112 [00:09<04:24,  2.45s/it]Running Inference:   4%|▍         | 5/112 [00:10<03:25,  1.92s/it]Running Inference:   5%|▌         | 6/112 [00:12<03:50,  2.18s/it]Running Inference:   6%|▋         | 7/112 [00:15<04:11,  2.40s/it]Running Inference:   7%|▋         | 8/112 [00:18<04:28,  2.58s/it]Running Inference:   8%|▊         | 9/112 [00:21<04:32,  2.64s/it]Running Inference:   9%|▉         | 10/112 [00:26<05:43,  3.36s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:27<04:40,  2.78s/it]Running Inference:  11%|█         | 12/112 [00:32<05:19,  3.19s/it]Running Inference:  12%|█▏        | 13/112 [00:33<04:20,  2.63s/it]Running Inference:  12%|█▎        | 14/112 [00:37<04:53,  2.99s/it]Running Inference:  13%|█▎        | 15/112 [00:37<03:44,  2.31s/it]Running Inference:  14%|█▍        | 16/112 [00:38<02:57,  1.85s/it]Running Inference:  15%|█▌        | 17/112 [00:39<02:15,  1.42s/it]Running Inference:  16%|█▌        | 18/112 [00:43<03:39,  2.33s/it]Running Inference:  17%|█▋        | 19/112 [00:48<04:39,  3.01s/it]Running Inference:  18%|█▊        | 20/112 [00:50<04:05,  2.67s/it]Running Inference:  19%|█▉        | 21/112 [00:54<05:04,  3.34s/it]Running Inference:  20%|█▉        | 22/112 [00:56<04:23,  2.93s/it]Running Inference:  21%|██        | 23/112 [00:58<03:40,  2.47s/it]Running Inference:  21%|██▏       | 24/112 [00:59<03:00,  2.05s/it]Running Inference:  22%|██▏       | 25/112 [01:00<02:30,  1.74s/it]Running Inference:  23%|██▎       | 26/112 [01:04<03:40,  2.57s/it]Running Inference:  24%|██▍       | 27/112 [01:07<03:45,  2.65s/it]Running Inference:  25%|██▌       | 28/112 [01:09<03:07,  2.24s/it]Running Inference:  26%|██▌       | 29/112 [01:10<02:40,  1.93s/it]Running Inference:  27%|██▋       | 30/112 [01:12<02:54,  2.13s/it]Running Inference:  28%|██▊       | 31/112 [01:13<02:26,  1.80s/it]Running Inference:  29%|██▊       | 32/112 [01:19<03:48,  2.85s/it]Running Inference:  29%|██▉       | 33/112 [01:24<04:37,  3.51s/it]Running Inference:  30%|███       | 34/112 [01:26<03:59,  3.07s/it]Running Inference:  31%|███▏      | 35/112 [01:30<04:28,  3.48s/it]Running Inference:  32%|███▏      | 36/112 [01:31<03:18,  2.61s/it]Running Inference:  33%|███▎      | 37/112 [01:31<02:26,  1.96s/it]Running Inference:  34%|███▍      | 38/112 [01:32<02:02,  1.65s/it]Running Inference:  35%|███▍      | 39/112 [01:34<01:55,  1.58s/it]Running Inference:  36%|███▌      | 40/112 [01:38<02:57,  2.47s/it]Running Inference:  37%|███▋      | 41/112 [01:39<02:25,  2.04s/it]Running Inference:  38%|███▊      | 42/112 [01:45<03:45,  3.22s/it]Running Inference:  38%|███▊      | 43/112 [01:47<03:12,  2.78s/it]Running Inference:  39%|███▉      | 44/112 [01:51<03:39,  3.23s/it]Running Inference:  40%|████      | 45/112 [01:55<03:56,  3.53s/it]Running Inference:  41%|████      | 46/112 [01:57<03:06,  2.83s/it]Running Inference:  42%|████▏     | 47/112 [01:58<02:29,  2.30s/it]Running Inference:  43%|████▎     | 48/112 [01:59<02:17,  2.15s/it]Running Inference:  44%|████▍     | 49/112 [02:01<01:57,  1.86s/it]Running Inference:  45%|████▍     | 50/112 [02:02<01:48,  1.74s/it]Running Inference:  46%|████▌     | 51/112 [02:03<01:32,  1.51s/it]Running Inference:  46%|████▋     | 52/112 [02:06<01:54,  1.92s/it]Running Inference:  47%|████▋     | 53/112 [02:08<01:57,  1.99s/it]Running Inference:  48%|████▊     | 54/112 [02:10<01:51,  1.92s/it]Running Inference:  49%|████▉     | 55/112 [02:12<01:58,  2.08s/it]Running Inference:  50%|█████     | 56/112 [02:13<01:33,  1.68s/it]Running Inference:  51%|█████     | 57/112 [02:15<01:30,  1.64s/it]Running Inference:  52%|█████▏    | 58/112 [02:17<01:47,  1.99s/it]Running Inference:  53%|█████▎    | 59/112 [02:21<02:04,  2.35s/it]Running Inference:  54%|█████▎    | 60/112 [02:25<02:27,  2.83s/it]Running Inference:  54%|█████▍    | 61/112 [02:27<02:10,  2.56s/it]Running Inference:  55%|█████▌    | 62/112 [02:28<01:58,  2.37s/it]Running Inference:  56%|█████▋    | 63/112 [02:33<02:23,  2.94s/it]Running Inference:  57%|█████▋    | 64/112 [02:37<02:42,  3.38s/it]Running Inference:  58%|█████▊    | 65/112 [02:39<02:16,  2.91s/it]Running Inference:  59%|█████▉    | 66/112 [02:44<02:50,  3.70s/it]Running Inference:  60%|█████▉    | 67/112 [02:46<02:20,  3.12s/it]Running Inference:  61%|██████    | 68/112 [02:48<01:56,  2.64s/it]Running Inference:  62%|██████▏   | 69/112 [02:49<01:31,  2.14s/it]Running Inference:  62%|██████▎   | 70/112 [02:49<01:11,  1.71s/it]Running Inference:  63%|██████▎   | 71/112 [02:50<00:58,  1.43s/it]Running Inference:  64%|██████▍   | 72/112 [02:52<00:56,  1.41s/it]Running Inference:  65%|██████▌   | 73/112 [02:54<01:02,  1.60s/it]Running Inference:  66%|██████▌   | 74/112 [02:57<01:23,  2.19s/it]Running Inference:  67%|██████▋   | 75/112 [03:03<01:59,  3.24s/it]Running Inference:  68%|██████▊   | 76/112 [03:03<01:26,  2.40s/it]Running Inference:  69%|██████▉   | 77/112 [03:04<01:10,  2.02s/it]Running Inference:  70%|██████▉   | 78/112 [03:11<01:53,  3.34s/it]Running Inference:  71%|███████   | 79/112 [03:12<01:29,  2.71s/it]Running Inference:  71%|███████▏  | 80/112 [03:14<01:15,  2.36s/it]Running Inference:  72%|███████▏  | 81/112 [03:15<01:04,  2.09s/it]Running Inference:  73%|███████▎  | 82/112 [03:17<00:56,  1.89s/it]Running Inference:  74%|███████▍  | 83/112 [03:21<01:18,  2.70s/it]Running Inference:  75%|███████▌  | 84/112 [03:26<01:35,  3.40s/it]Running Inference:  76%|███████▌  | 85/112 [03:26<01:07,  2.48s/it]Running Inference:  77%|███████▋  | 86/112 [03:28<00:58,  2.24s/it]Running Inference:  78%|███████▊  | 87/112 [03:30<00:51,  2.05s/it]Running Inference:  79%|███████▊  | 88/112 [03:35<01:08,  2.87s/it]Running Inference:  79%|███████▉  | 89/112 [03:39<01:19,  3.47s/it]Running Inference:  80%|████████  | 90/112 [03:41<01:05,  2.98s/it]Running Inference:  81%|████████▏ | 91/112 [03:45<01:06,  3.16s/it]Running Inference:  82%|████████▏ | 92/112 [03:47<00:57,  2.86s/it]Running Inference:  83%|████████▎ | 93/112 [03:51<01:00,  3.18s/it]Running Inference:  84%|████████▍ | 94/112 [03:52<00:47,  2.62s/it]Running Inference:  85%|████████▍ | 95/112 [03:54<00:40,  2.41s/it]Running Inference:  86%|████████▌ | 96/112 [03:55<00:31,  1.95s/it]Running Inference:  87%|████████▋ | 97/112 [03:58<00:33,  2.23s/it]Running Inference:  88%|████████▊ | 98/112 [04:00<00:30,  2.21s/it]Running Inference:  88%|████████▊ | 99/112 [04:02<00:27,  2.15s/it]Running Inference:  89%|████████▉ | 100/112 [04:11<00:48,  4.05s/it]Running Inference:  90%|█████████ | 101/112 [04:12<00:36,  3.29s/it]Running Inference:  91%|█████████ | 102/112 [04:17<00:36,  3.66s/it]Running Inference:  92%|█████████▏| 103/112 [04:19<00:28,  3.15s/it]Running Inference:  93%|█████████▎| 104/112 [04:19<00:19,  2.49s/it]Running Inference:  94%|█████████▍| 105/112 [04:22<00:17,  2.54s/it]Running Inference:  95%|█████████▍| 106/112 [04:24<00:13,  2.32s/it]Running Inference:  96%|█████████▌| 107/112 [04:29<00:14,  3.00s/it]Running Inference:  96%|█████████▋| 108/112 [04:36<00:17,  4.41s/it]Running Inference:  97%|█████████▋| 109/112 [04:38<00:11,  3.67s/it]Running Inference:  98%|█████████▊| 110/112 [04:40<00:06,  3.15s/it]Running Inference:  99%|█████████▉| 111/112 [04:42<00:02,  2.85s/it]Running Inference: 100%|██████████| 112/112 [04:43<00:00,  2.30s/it]Running Inference: 100%|██████████| 112/112 [04:43<00:00,  2.53s/it]
2025-12-13 19:11:15,134 - INFO - Inference completed.
2025-12-13 19:11:15,142 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 19:11:15,142 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:11:15,147 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 19:11:15,147 - INFO - Metrics:
24.05
2025-12-13 19:11:15,149 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_en, ratio=0.2) on GPU 2

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:11:21,700 - INFO - Set deterministic seeds to 42
2025-12-13 19:11:21,700 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:11:21,700 - INFO - Starting evaluation run...
2025-12-13 19:11:21,700 - INFO - Output directory set to: longbenchresult
2025-12-13 19:11:21,700 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 19:11:21,700 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:11:21,700 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:11:21,700 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.48it/s]
Device set to use cuda:0
2025-12-13 19:11:37,019 - INFO - Model pipeline loaded.
2025-12-13 19:11:37,020 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:11:42,123 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:11:42,124 - INFO - Dataset processed with 150 entries.
2025-12-13 19:11:42,136 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<03:50,  2.07s/it]Running Inference:   2%|▏         | 2/112 [00:02<02:13,  1.21s/it]Running Inference:   3%|▎         | 3/112 [00:04<03:02,  1.67s/it]Running Inference:   4%|▎         | 4/112 [00:06<02:55,  1.63s/it]Running Inference:   4%|▍         | 5/112 [00:07<02:29,  1.39s/it]Running Inference:   5%|▌         | 6/112 [00:10<03:13,  1.82s/it]Running Inference:   6%|▋         | 7/112 [00:12<03:45,  2.14s/it]Running Inference:   7%|▋         | 8/112 [00:15<04:09,  2.40s/it]Running Inference:   8%|▊         | 9/112 [00:18<04:28,  2.61s/it]Running Inference:   9%|▉         | 10/112 [00:23<05:36,  3.30s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:25<04:36,  2.74s/it]Running Inference:  11%|█         | 12/112 [00:28<04:35,  2.76s/it]Running Inference:  12%|█▏        | 13/112 [00:29<03:49,  2.32s/it]Running Inference:  12%|█▎        | 14/112 [00:33<04:30,  2.76s/it]Running Inference:  13%|█▎        | 15/112 [00:33<03:22,  2.08s/it]Running Inference:  14%|█▍        | 16/112 [00:34<02:41,  1.69s/it]Running Inference:  15%|█▌        | 17/112 [00:34<02:04,  1.31s/it]Running Inference:  16%|█▌        | 18/112 [00:39<03:30,  2.24s/it]Running Inference:  17%|█▋        | 19/112 [00:43<04:32,  2.93s/it]Running Inference:  18%|█▊        | 20/112 [00:45<04:00,  2.61s/it]Running Inference:  19%|█▉        | 21/112 [00:49<04:18,  2.84s/it]Running Inference:  20%|█▉        | 22/112 [00:50<03:51,  2.57s/it]Running Inference:  21%|██        | 23/112 [00:52<03:17,  2.22s/it]Running Inference:  21%|██▏       | 24/112 [00:53<02:44,  1.87s/it]Running Inference:  22%|██▏       | 25/112 [00:54<02:19,  1.61s/it]Running Inference:  23%|██▎       | 26/112 [00:58<03:32,  2.47s/it]Running Inference:  24%|██▍       | 27/112 [01:01<03:30,  2.48s/it]Running Inference:  25%|██▌       | 28/112 [01:02<02:57,  2.11s/it]Running Inference:  26%|██▌       | 29/112 [01:03<02:33,  1.84s/it]Running Inference:  27%|██▋       | 30/112 [01:06<02:56,  2.15s/it]Running Inference:  28%|██▊       | 31/112 [01:07<02:27,  1.82s/it]Running Inference:  29%|██▊       | 32/112 [01:13<03:47,  2.85s/it]Running Inference:  29%|██▉       | 33/112 [01:15<03:29,  2.66s/it]Running Inference:  30%|███       | 34/112 [01:17<03:12,  2.47s/it]Running Inference:  31%|███▏      | 35/112 [01:21<03:54,  3.04s/it]Running Inference:  32%|███▏      | 36/112 [01:22<02:54,  2.30s/it]Running Inference:  33%|███▎      | 37/112 [01:22<02:10,  1.74s/it]Running Inference:  34%|███▍      | 38/112 [01:23<01:50,  1.50s/it]Running Inference:  35%|███▍      | 39/112 [01:25<01:48,  1.49s/it]Running Inference:  36%|███▌      | 40/112 [01:26<01:50,  1.53s/it]Running Inference:  37%|███▋      | 41/112 [01:27<01:38,  1.38s/it]Running Inference:  38%|███▊      | 42/112 [01:34<03:21,  2.87s/it]Running Inference:  38%|███▊      | 43/112 [01:35<02:55,  2.54s/it]Running Inference:  39%|███▉      | 44/112 [01:40<03:31,  3.11s/it]Running Inference:  40%|████      | 45/112 [01:44<03:49,  3.43s/it]Running Inference:  41%|████      | 46/112 [01:45<03:01,  2.76s/it]Running Inference:  42%|████▏     | 47/112 [01:46<02:25,  2.24s/it]Running Inference:  43%|████▎     | 48/112 [01:48<02:13,  2.09s/it]Running Inference:  44%|████▍     | 49/112 [01:50<02:02,  1.95s/it]Running Inference:  45%|████▍     | 50/112 [01:51<01:51,  1.79s/it]Running Inference:  46%|████▌     | 51/112 [01:52<01:34,  1.54s/it]Running Inference:  46%|████▋     | 52/112 [01:56<02:12,  2.20s/it]Running Inference:  47%|████▋     | 53/112 [01:58<02:07,  2.16s/it]Running Inference:  48%|████▊     | 54/112 [01:59<01:58,  2.04s/it]Running Inference:  49%|████▉     | 55/112 [02:02<02:02,  2.16s/it]Running Inference:  50%|█████     | 56/112 [02:03<01:36,  1.73s/it]Running Inference:  51%|█████     | 57/112 [02:04<01:29,  1.63s/it]Running Inference:  52%|█████▏    | 58/112 [02:07<01:46,  1.98s/it]Running Inference:  53%|█████▎    | 59/112 [02:11<02:20,  2.64s/it]Running Inference:  54%|█████▎    | 60/112 [02:12<01:48,  2.09s/it]Running Inference:  54%|█████▍    | 61/112 [02:14<01:52,  2.21s/it]Running Inference:  55%|█████▌    | 62/112 [02:16<01:42,  2.04s/it]Running Inference:  56%|█████▋    | 63/112 [02:20<02:12,  2.70s/it]Running Inference:  57%|█████▋    | 64/112 [02:25<02:36,  3.25s/it]Running Inference:  58%|█████▊    | 65/112 [02:27<02:12,  2.81s/it]Running Inference:  59%|█████▉    | 66/112 [02:33<02:56,  3.83s/it]Running Inference:  60%|█████▉    | 67/112 [02:34<02:24,  3.22s/it]Running Inference:  61%|██████    | 68/112 [02:39<02:32,  3.46s/it]Running Inference:  62%|██████▏   | 69/112 [02:39<01:56,  2.71s/it]Running Inference:  62%|██████▎   | 70/112 [02:40<01:28,  2.11s/it]Running Inference:  63%|██████▎   | 71/112 [02:41<01:09,  1.70s/it]Running Inference:  64%|██████▍   | 72/112 [02:42<01:03,  1.60s/it]Running Inference:  65%|██████▌   | 73/112 [02:44<01:05,  1.68s/it]Running Inference:  66%|██████▌   | 74/112 [02:48<01:25,  2.24s/it]Running Inference:  67%|██████▋   | 75/112 [02:53<02:00,  3.25s/it]Running Inference:  68%|██████▊   | 76/112 [02:54<01:26,  2.40s/it]Running Inference:  69%|██████▉   | 77/112 [02:55<01:10,  2.02s/it]Running Inference:  70%|██████▉   | 78/112 [03:01<01:52,  3.31s/it]Running Inference:  71%|███████   | 79/112 [03:02<01:28,  2.68s/it]Running Inference:  71%|███████▏  | 80/112 [03:04<01:17,  2.42s/it]Running Inference:  72%|███████▏  | 81/112 [03:06<01:08,  2.19s/it]Running Inference:  73%|███████▎  | 82/112 [03:07<00:58,  1.96s/it]Running Inference:  74%|███████▍  | 83/112 [03:17<02:02,  4.23s/it]Running Inference:  75%|███████▌  | 84/112 [03:22<02:04,  4.45s/it]Running Inference:  76%|███████▌  | 85/112 [03:22<01:26,  3.22s/it]Running Inference:  77%|███████▋  | 86/112 [03:23<01:08,  2.65s/it]Running Inference:  78%|███████▊  | 87/112 [03:26<01:03,  2.55s/it]Running Inference:  79%|███████▊  | 88/112 [03:31<01:17,  3.21s/it]Running Inference:  79%|███████▉  | 89/112 [03:35<01:23,  3.64s/it]Running Inference:  80%|████████  | 90/112 [03:37<01:08,  3.10s/it]Running Inference:  81%|████████▏ | 91/112 [03:41<01:13,  3.51s/it]Running Inference:  82%|████████▏ | 92/112 [03:44<01:02,  3.10s/it]Running Inference:  83%|████████▎ | 93/112 [03:48<01:03,  3.34s/it]Running Inference:  84%|████████▍ | 94/112 [03:49<00:49,  2.73s/it]Running Inference:  85%|████████▍ | 95/112 [03:53<00:55,  3.24s/it]Running Inference:  86%|████████▌ | 96/112 [03:54<00:40,  2.52s/it]Running Inference:  87%|████████▋ | 97/112 [03:57<00:38,  2.55s/it]Running Inference:  88%|████████▊ | 98/112 [03:59<00:34,  2.46s/it]Running Inference:  88%|████████▊ | 99/112 [04:01<00:29,  2.31s/it]Running Inference:  89%|████████▉ | 100/112 [04:09<00:47,  3.98s/it]Running Inference:  90%|█████████ | 101/112 [04:10<00:35,  3.24s/it]Running Inference:  91%|█████████ | 102/112 [04:15<00:36,  3.61s/it]Running Inference:  92%|█████████▏| 103/112 [04:17<00:28,  3.11s/it]Running Inference:  93%|█████████▎| 104/112 [04:18<00:19,  2.46s/it]Running Inference:  94%|█████████▍| 105/112 [04:19<00:14,  2.12s/it]Running Inference:  95%|█████████▍| 106/112 [04:21<00:12,  2.02s/it]Running Inference:  96%|█████████▌| 107/112 [04:25<00:13,  2.73s/it]Running Inference:  96%|█████████▋| 108/112 [04:28<00:11,  2.89s/it]Running Inference:  97%|█████████▋| 109/112 [04:30<00:07,  2.60s/it]Running Inference:  98%|█████████▊| 110/112 [04:33<00:04,  2.47s/it]Running Inference:  99%|█████████▉| 111/112 [04:35<00:02,  2.37s/it]Running Inference: 100%|██████████| 112/112 [04:36<00:00,  1.97s/it]Running Inference: 100%|██████████| 112/112 [04:36<00:00,  2.47s/it]
2025-12-13 19:16:18,345 - INFO - Inference completed.
2025-12-13 19:16:18,352 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-13 19:16:18,352 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:16:18,357 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-13 19:16:18,357 - INFO - Metrics:
23.21
2025-12-13 19:16:18,359 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_en, ratio=0.3) on GPU 2

----------------------------------------
Task: multifieldqa_en | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:16:24,721 - INFO - Set deterministic seeds to 42
2025-12-13 19:16:24,721 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:16:24,721 - INFO - Starting evaluation run...
2025-12-13 19:16:24,721 - INFO - Output directory set to: longbenchresult
2025-12-13 19:16:24,722 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-13 19:16:24,722 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:16:24,722 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:16:24,722 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.93it/s]
Device set to use cuda:0
2025-12-13 19:16:36,806 - INFO - Model pipeline loaded.
2025-12-13 19:16:36,806 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_en)
2025-12-13 19:16:42,041 - INFO - Dataset loaded with 150 entries.
2025-12-13 19:16:42,041 - INFO - Dataset processed with 150 entries.
2025-12-13 19:16:42,053 - INFO - Starting inference...
Running Inference:   0%|          | 0/112 [00:00<?, ?it/s]Running Inference:   1%|          | 1/112 [00:02<03:50,  2.07s/it]Running Inference:   2%|▏         | 2/112 [00:02<02:21,  1.29s/it]Running Inference:   3%|▎         | 3/112 [00:05<03:07,  1.72s/it]Running Inference:   4%|▎         | 4/112 [00:06<03:04,  1.71s/it]Running Inference:   4%|▍         | 5/112 [00:07<02:38,  1.48s/it]Running Inference:   5%|▌         | 6/112 [00:11<04:03,  2.29s/it]Running Inference:   6%|▋         | 7/112 [00:14<04:28,  2.56s/it]Running Inference:   7%|▋         | 8/112 [00:19<05:29,  3.17s/it]Running Inference:   8%|▊         | 9/112 [00:21<04:50,  2.82s/it]Running Inference:   9%|▉         | 10/112 [00:24<05:10,  3.05s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  10%|▉         | 11/112 [00:26<04:18,  2.56s/it]Running Inference:  11%|█         | 12/112 [00:28<04:16,  2.57s/it]Running Inference:  12%|█▏        | 13/112 [00:30<03:30,  2.13s/it]Running Inference:  12%|█▎        | 14/112 [00:33<04:19,  2.65s/it]Running Inference:  13%|█▎        | 15/112 [00:34<03:14,  2.01s/it]Running Inference:  14%|█▍        | 16/112 [00:35<02:36,  1.63s/it]Running Inference:  15%|█▌        | 17/112 [00:35<02:00,  1.27s/it]Running Inference:  16%|█▌        | 18/112 [00:39<03:13,  2.06s/it]Running Inference:  17%|█▋        | 19/112 [00:44<04:22,  2.82s/it]Running Inference:  18%|█▊        | 20/112 [00:45<03:53,  2.54s/it]Running Inference:  19%|█▉        | 21/112 [00:48<03:44,  2.46s/it]Running Inference:  20%|█▉        | 22/112 [00:50<03:27,  2.31s/it]Running Inference:  21%|██        | 23/112 [00:51<03:01,  2.04s/it]Running Inference:  21%|██▏       | 24/112 [00:52<02:40,  1.83s/it]Running Inference:  22%|██▏       | 25/112 [00:53<02:17,  1.58s/it]Running Inference:  23%|██▎       | 26/112 [00:58<03:31,  2.46s/it]Running Inference:  24%|██▍       | 27/112 [01:01<03:49,  2.70s/it]Running Inference:  25%|██▌       | 28/112 [01:02<03:10,  2.27s/it]Running Inference:  26%|██▌       | 29/112 [01:04<02:42,  1.96s/it]Running Inference:  27%|██▋       | 30/112 [01:07<03:05,  2.26s/it]Running Inference:  28%|██▊       | 31/112 [01:08<02:33,  1.89s/it]Running Inference:  29%|██▊       | 32/112 [01:12<03:33,  2.67s/it]Running Inference:  29%|██▉       | 33/112 [01:14<03:20,  2.54s/it]Running Inference:  30%|███       | 34/112 [01:16<02:55,  2.25s/it]Running Inference:  31%|███▏      | 35/112 [01:20<03:43,  2.91s/it]Running Inference:  32%|███▏      | 36/112 [01:21<02:47,  2.20s/it]Running Inference:  33%|███▎      | 37/112 [01:21<02:06,  1.69s/it]Running Inference:  34%|███▍      | 38/112 [01:26<02:56,  2.39s/it]Running Inference:  35%|███▍      | 39/112 [01:28<02:46,  2.29s/it]Running Inference:  36%|███▌      | 40/112 [01:32<03:31,  2.93s/it]Running Inference:  37%|███▋      | 41/112 [01:36<03:57,  3.34s/it]Running Inference:  38%|███▊      | 42/112 [01:41<04:31,  3.88s/it]Running Inference:  38%|███▊      | 43/112 [01:43<03:44,  3.25s/it]Running Inference:  39%|███▉      | 44/112 [01:48<04:07,  3.63s/it]Running Inference:  40%|████      | 45/112 [01:52<04:15,  3.82s/it]Running Inference:  41%|████      | 46/112 [01:53<03:19,  3.02s/it]Running Inference:  42%|████▏     | 47/112 [01:54<02:37,  2.43s/it]Running Inference:  43%|████▎     | 48/112 [01:56<02:24,  2.25s/it]Running Inference:  44%|████▍     | 49/112 [01:58<02:10,  2.07s/it]Running Inference:  45%|████▍     | 50/112 [01:59<02:02,  1.98s/it]Running Inference:  46%|████▌     | 51/112 [02:01<01:43,  1.70s/it]Running Inference:  46%|████▋     | 52/112 [02:04<02:18,  2.30s/it]Running Inference:  47%|████▋     | 53/112 [02:06<02:13,  2.26s/it]Running Inference:  48%|████▊     | 54/112 [02:11<02:55,  3.02s/it]Running Inference:  49%|████▉     | 55/112 [02:14<02:47,  2.94s/it]Running Inference:  50%|█████     | 56/112 [02:15<02:07,  2.28s/it]Running Inference:  51%|█████     | 57/112 [02:16<01:53,  2.06s/it]Running Inference:  52%|█████▏    | 58/112 [02:21<02:28,  2.76s/it]Running Inference:  53%|█████▎    | 59/112 [02:22<01:58,  2.24s/it]Running Inference:  54%|█████▎    | 60/112 [02:22<01:33,  1.81s/it]Running Inference:  54%|█████▍    | 61/112 [02:25<01:37,  1.91s/it]Running Inference:  55%|█████▌    | 62/112 [02:26<01:31,  1.83s/it]Running Inference:  56%|█████▋    | 63/112 [02:28<01:33,  1.91s/it]Running Inference:  57%|█████▋    | 64/112 [02:33<02:08,  2.68s/it]Running Inference:  58%|█████▊    | 65/112 [02:35<01:53,  2.42s/it]Running Inference:  59%|█████▉    | 66/112 [02:39<02:13,  2.90s/it]Running Inference:  60%|█████▉    | 67/112 [02:40<01:55,  2.57s/it]Running Inference:  61%|██████    | 68/112 [02:42<01:34,  2.15s/it]Running Inference:  62%|██████▏   | 69/112 [02:43<01:16,  1.78s/it]Running Inference:  62%|██████▎   | 70/112 [02:43<01:01,  1.46s/it]Running Inference:  63%|██████▎   | 71/112 [02:44<00:53,  1.30s/it]Running Inference:  64%|██████▍   | 72/112 [02:46<00:52,  1.32s/it]Running Inference:  65%|██████▌   | 73/112 [02:47<00:57,  1.48s/it]Running Inference:  66%|██████▌   | 74/112 [02:50<01:07,  1.78s/it]Running Inference:  67%|██████▋   | 75/112 [02:53<01:17,  2.10s/it]Running Inference:  68%|██████▊   | 76/112 [02:53<00:58,  1.63s/it]Running Inference:  69%|██████▉   | 77/112 [02:54<00:51,  1.48s/it]Running Inference:  70%|██████▉   | 78/112 [03:00<01:32,  2.72s/it]Running Inference:  71%|███████   | 79/112 [03:01<01:14,  2.26s/it]Running Inference:  71%|███████▏  | 80/112 [03:03<01:05,  2.04s/it]Running Inference:  72%|███████▏  | 81/112 [03:04<01:00,  1.95s/it]Running Inference:  73%|███████▎  | 82/112 [03:06<00:53,  1.79s/it]Running Inference:  74%|███████▍  | 83/112 [03:14<01:44,  3.61s/it]Running Inference:  75%|███████▌  | 84/112 [03:19<01:52,  4.03s/it]Running Inference:  76%|███████▌  | 85/112 [03:19<01:18,  2.92s/it]Running Inference:  77%|███████▋  | 86/112 [03:20<01:03,  2.44s/it]Running Inference:  78%|███████▊  | 87/112 [03:21<00:50,  2.04s/it]Running Inference:  79%|███████▊  | 88/112 [03:26<01:08,  2.86s/it]Running Inference:  79%|███████▉  | 89/112 [03:34<01:36,  4.18s/it]Running Inference:  80%|████████  | 90/112 [03:35<01:16,  3.48s/it]Running Inference:  81%|████████▏ | 91/112 [03:37<01:04,  3.07s/it]Running Inference:  82%|████████▏ | 92/112 [03:40<00:56,  2.80s/it]Running Inference:  83%|████████▎ | 93/112 [03:44<00:59,  3.14s/it]Running Inference:  84%|████████▍ | 94/112 [03:45<00:46,  2.59s/it]Running Inference:  85%|████████▍ | 95/112 [03:49<00:53,  3.16s/it]Running Inference:  86%|████████▌ | 96/112 [03:50<00:39,  2.49s/it]Running Inference:  87%|████████▋ | 97/112 [03:56<00:50,  3.35s/it]Running Inference:  88%|████████▊ | 98/112 [03:58<00:41,  2.96s/it]Running Inference:  88%|████████▊ | 99/112 [04:00<00:34,  2.63s/it]Running Inference:  89%|████████▉ | 100/112 [04:08<00:50,  4.25s/it]Running Inference:  90%|█████████ | 101/112 [04:09<00:38,  3.47s/it]Running Inference:  91%|█████████ | 102/112 [04:14<00:37,  3.78s/it]Running Inference:  92%|█████████▏| 103/112 [04:16<00:29,  3.24s/it]Running Inference:  93%|█████████▎| 104/112 [04:18<00:23,  2.88s/it]Running Inference:  94%|█████████▍| 105/112 [04:19<00:17,  2.46s/it]Running Inference:  95%|█████████▍| 106/112 [04:21<00:13,  2.26s/it]Running Inference:  96%|█████████▌| 107/112 [04:26<00:15,  3.04s/it]Running Inference:  96%|█████████▋| 108/112 [04:29<00:12,  3.12s/it]Running Inference:  97%|█████████▋| 109/112 [04:32<00:08,  2.99s/it]Running Inference:  98%|█████████▊| 110/112 [04:34<00:05,  2.63s/it]Running Inference:  99%|█████████▉| 111/112 [04:36<00:02,  2.48s/it]Running Inference: 100%|██████████| 112/112 [04:37<00:00,  2.00s/it]Running Inference: 100%|██████████| 112/112 [04:37<00:00,  2.48s/it]
2025-12-13 19:21:19,288 - INFO - Inference completed.
2025-12-13 19:21:19,296 - INFO - Results saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-13 19:21:19,296 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:21:19,301 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_en__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-13 19:21:19,301 - INFO - Metrics:
21.14
2025-12-13 19:21:19,302 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_en, ratio=0.5) on GPU 2


========================================
LongBench Task: musique
========================================
----------------------------------------
Task: musique | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:21:25,710 - INFO - Set deterministic seeds to 42
2025-12-13 19:21:25,710 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:21:25,710 - INFO - Starting evaluation run...
2025-12-13 19:21:25,710 - INFO - Output directory set to: longbenchresult
2025-12-13 19:21:25,711 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 19:21:25,711 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:21:25,711 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:21:25,711 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.94it/s]
Device set to use cuda:0
2025-12-13 19:21:38,717 - INFO - Model pipeline loaded.
2025-12-13 19:21:38,717 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:21:44,735 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:21:44,735 - INFO - Dataset processed with 200 entries.
2025-12-13 19:21:44,778 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<15:14,  4.60s/it]Running Inference:   1%|          | 2/200 [00:07<10:57,  3.32s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:20,  2.85s/it]Running Inference:   2%|▏         | 4/200 [00:13<11:14,  3.44s/it]Running Inference:   2%|▎         | 5/200 [00:17<11:44,  3.61s/it]Running Inference:   3%|▎         | 6/200 [00:21<12:19,  3.81s/it]Running Inference:   4%|▎         | 7/200 [00:26<12:47,  3.98s/it]Running Inference:   4%|▍         | 8/200 [00:28<11:02,  3.45s/it]Running Inference:   4%|▍         | 9/200 [00:32<11:52,  3.73s/it]Running Inference:   5%|▌         | 10/200 [00:37<12:21,  3.90s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:38<10:23,  3.30s/it]Running Inference:   6%|▌         | 12/200 [00:43<11:13,  3.58s/it]Running Inference:   6%|▋         | 13/200 [00:45<10:15,  3.29s/it]Running Inference:   7%|▋         | 14/200 [00:50<11:11,  3.61s/it]Running Inference:   8%|▊         | 15/200 [00:54<11:49,  3.83s/it]Running Inference:   8%|▊         | 16/200 [00:58<12:03,  3.93s/it]Running Inference:   8%|▊         | 17/200 [01:02<12:18,  4.04s/it]Running Inference:   9%|▉         | 18/200 [01:07<12:19,  4.06s/it]Running Inference:  10%|▉         | 19/200 [01:11<12:24,  4.11s/it]Running Inference:  10%|█         | 20/200 [01:15<12:26,  4.15s/it]Running Inference:  10%|█         | 21/200 [01:19<12:25,  4.16s/it]Running Inference:  11%|█         | 22/200 [01:23<12:17,  4.14s/it]Running Inference:  12%|█▏        | 23/200 [01:26<10:30,  3.56s/it]Running Inference:  12%|█▏        | 24/200 [01:27<08:38,  2.95s/it]Running Inference:  12%|█▎        | 25/200 [01:31<09:41,  3.32s/it]Running Inference:  13%|█▎        | 26/200 [01:36<10:28,  3.61s/it]Running Inference:  14%|█▎        | 27/200 [01:40<10:57,  3.80s/it]Running Inference:  14%|█▍        | 28/200 [01:44<11:18,  3.94s/it]Running Inference:  14%|█▍        | 29/200 [01:48<11:29,  4.03s/it]Running Inference:  15%|█▌        | 30/200 [01:53<11:35,  4.09s/it]Running Inference:  16%|█▌        | 31/200 [01:55<09:58,  3.54s/it]Running Inference:  16%|█▌        | 32/200 [01:59<10:03,  3.59s/it]Running Inference:  16%|█▋        | 33/200 [02:03<10:31,  3.78s/it]Running Inference:  17%|█▋        | 34/200 [02:07<10:43,  3.87s/it]Running Inference:  18%|█▊        | 35/200 [02:11<10:51,  3.95s/it]Running Inference:  18%|█▊        | 36/200 [02:14<10:27,  3.82s/it]Running Inference:  18%|█▊        | 37/200 [02:17<09:07,  3.36s/it]Running Inference:  19%|█▉        | 38/200 [02:19<08:32,  3.17s/it]Running Inference:  20%|█▉        | 39/200 [02:23<08:58,  3.34s/it]Running Inference:  20%|██        | 40/200 [02:27<09:38,  3.62s/it]Running Inference:  20%|██        | 41/200 [02:32<10:00,  3.77s/it]Running Inference:  21%|██        | 42/200 [02:34<09:03,  3.44s/it]Running Inference:  22%|██▏       | 43/200 [02:38<08:59,  3.43s/it]Running Inference:  22%|██▏       | 44/200 [02:42<09:35,  3.69s/it]Running Inference:  22%|██▎       | 45/200 [02:46<09:50,  3.81s/it]Running Inference:  23%|██▎       | 46/200 [02:48<08:33,  3.34s/it]Running Inference:  24%|██▎       | 47/200 [02:51<08:21,  3.28s/it]Running Inference:  24%|██▍       | 48/200 [02:56<09:03,  3.58s/it]Running Inference:  24%|██▍       | 49/200 [03:00<09:31,  3.79s/it]Running Inference:  25%|██▌       | 50/200 [03:04<09:20,  3.74s/it]Running Inference:  26%|██▌       | 51/200 [03:10<10:54,  4.39s/it]Running Inference:  26%|██▌       | 52/200 [03:14<10:45,  4.36s/it]Running Inference:  26%|██▋       | 53/200 [03:18<10:32,  4.30s/it]Running Inference:  27%|██▋       | 54/200 [03:21<09:36,  3.95s/it]Running Inference:  28%|██▊       | 55/200 [03:24<08:31,  3.53s/it]Running Inference:  28%|██▊       | 56/200 [03:28<08:58,  3.74s/it]Running Inference:  28%|██▊       | 57/200 [03:32<09:20,  3.92s/it]Running Inference:  29%|██▉       | 58/200 [03:34<08:05,  3.42s/it]Running Inference:  30%|██▉       | 59/200 [03:37<07:10,  3.05s/it]Running Inference:  30%|███       | 60/200 [03:41<07:55,  3.40s/it]Running Inference:  30%|███       | 61/200 [03:44<07:42,  3.33s/it]Running Inference:  31%|███       | 62/200 [03:46<06:48,  2.96s/it]Running Inference:  32%|███▏      | 63/200 [03:50<07:38,  3.35s/it]Running Inference:  32%|███▏      | 64/200 [03:55<08:13,  3.63s/it]Running Inference:  32%|███▎      | 65/200 [03:58<08:16,  3.68s/it]Running Inference:  33%|███▎      | 66/200 [04:03<08:36,  3.86s/it]Running Inference:  34%|███▎      | 67/200 [04:05<07:29,  3.38s/it]Running Inference:  34%|███▍      | 68/200 [04:09<07:57,  3.61s/it]Running Inference:  34%|███▍      | 69/200 [04:11<06:58,  3.19s/it]Running Inference:  35%|███▌      | 70/200 [04:16<07:34,  3.50s/it]Running Inference:  36%|███▌      | 71/200 [04:20<07:59,  3.72s/it]Running Inference:  36%|███▌      | 72/200 [04:22<07:06,  3.33s/it]Running Inference:  36%|███▋      | 73/200 [04:25<06:37,  3.13s/it]Running Inference:  37%|███▋      | 74/200 [04:29<07:17,  3.48s/it]Running Inference:  38%|███▊      | 75/200 [04:34<07:48,  3.74s/it]Running Inference:  38%|███▊      | 76/200 [04:38<08:02,  3.89s/it]Running Inference:  38%|███▊      | 77/200 [04:39<06:35,  3.21s/it]Running Inference:  39%|███▉      | 78/200 [04:42<05:55,  2.91s/it]Running Inference:  40%|███▉      | 79/200 [04:45<06:02,  2.99s/it]Running Inference:  40%|████      | 80/200 [04:47<05:29,  2.74s/it]Running Inference:  40%|████      | 81/200 [04:49<05:14,  2.64s/it]Running Inference:  41%|████      | 82/200 [04:54<06:11,  3.15s/it]Running Inference:  42%|████▏     | 83/200 [04:58<06:40,  3.43s/it]Running Inference:  42%|████▏     | 84/200 [05:02<07:00,  3.63s/it]Running Inference:  42%|████▎     | 85/200 [05:06<07:17,  3.80s/it]Running Inference:  43%|████▎     | 86/200 [05:10<07:25,  3.91s/it]Running Inference:  44%|████▎     | 87/200 [05:15<07:34,  4.02s/it]Running Inference:  44%|████▍     | 88/200 [05:19<07:36,  4.08s/it]Running Inference:  44%|████▍     | 89/200 [05:21<06:29,  3.51s/it]Running Inference:  45%|████▌     | 90/200 [05:25<06:50,  3.73s/it]Running Inference:  46%|████▌     | 91/200 [05:29<06:43,  3.70s/it]Running Inference:  46%|████▌     | 92/200 [05:32<06:33,  3.64s/it]Running Inference:  46%|████▋     | 93/200 [05:35<05:44,  3.22s/it]Running Inference:  47%|████▋     | 94/200 [05:39<06:11,  3.50s/it]Running Inference:  48%|████▊     | 95/200 [05:43<06:29,  3.71s/it]Running Inference:  48%|████▊     | 96/200 [05:46<06:15,  3.61s/it]Running Inference:  48%|████▊     | 97/200 [05:49<05:32,  3.23s/it]Running Inference:  49%|████▉     | 98/200 [05:51<05:01,  2.95s/it]Running Inference:  50%|████▉     | 99/200 [05:55<05:28,  3.25s/it]Running Inference:  50%|█████     | 100/200 [05:59<05:46,  3.47s/it]Running Inference:  50%|█████     | 101/200 [06:03<06:00,  3.64s/it]Running Inference:  51%|█████     | 102/200 [06:05<05:14,  3.21s/it]Running Inference:  52%|█████▏    | 103/200 [06:09<05:38,  3.49s/it]Running Inference:  52%|█████▏    | 104/200 [06:12<05:05,  3.18s/it]Running Inference:  52%|█████▎    | 105/200 [06:16<05:34,  3.52s/it]Running Inference:  53%|█████▎    | 106/200 [06:20<05:50,  3.73s/it]Running Inference:  54%|█████▎    | 107/200 [06:24<05:55,  3.83s/it]Running Inference:  54%|█████▍    | 108/200 [06:26<05:06,  3.33s/it]Running Inference:  55%|█████▍    | 109/200 [06:31<05:30,  3.63s/it]Running Inference:  55%|█████▌    | 110/200 [06:34<05:12,  3.47s/it]Running Inference:  56%|█████▌    | 111/200 [06:38<05:21,  3.62s/it]Running Inference:  56%|█████▌    | 112/200 [06:40<04:37,  3.15s/it]Running Inference:  56%|█████▋    | 113/200 [06:44<05:01,  3.47s/it]Running Inference:  57%|█████▋    | 114/200 [06:48<05:16,  3.68s/it]Running Inference:  57%|█████▊    | 115/200 [06:52<05:25,  3.83s/it]Running Inference:  58%|█████▊    | 116/200 [06:56<05:04,  3.63s/it]Running Inference:  58%|█████▊    | 117/200 [07:00<05:14,  3.79s/it]Running Inference:  59%|█████▉    | 118/200 [07:02<04:29,  3.28s/it]Running Inference:  60%|█████▉    | 119/200 [07:04<04:05,  3.03s/it]Running Inference:  60%|██████    | 120/200 [07:07<03:43,  2.80s/it]Running Inference:  60%|██████    | 121/200 [07:09<03:27,  2.63s/it]Running Inference:  61%|██████    | 122/200 [07:11<03:14,  2.49s/it]Running Inference:  62%|██████▏   | 123/200 [07:14<03:29,  2.72s/it]Running Inference:  62%|██████▏   | 124/200 [07:18<04:00,  3.16s/it]Running Inference:  62%|██████▎   | 125/200 [07:22<04:04,  3.26s/it]Running Inference:  63%|██████▎   | 126/200 [07:26<04:23,  3.56s/it]Running Inference:  64%|██████▎   | 127/200 [07:29<03:52,  3.18s/it]Running Inference:  64%|██████▍   | 128/200 [07:33<04:10,  3.48s/it]Running Inference:  64%|██████▍   | 129/200 [07:36<04:03,  3.43s/it]Running Inference:  65%|██████▌   | 130/200 [07:40<04:16,  3.66s/it]Running Inference:  66%|██████▌   | 131/200 [07:42<03:42,  3.23s/it]Running Inference:  66%|██████▌   | 132/200 [07:47<03:59,  3.52s/it]Running Inference:  66%|██████▋   | 133/200 [07:49<03:27,  3.10s/it]Running Inference:  67%|██████▋   | 134/200 [07:50<02:47,  2.54s/it]Running Inference:  68%|██████▊   | 135/200 [07:53<02:52,  2.65s/it]Running Inference:  68%|██████▊   | 136/200 [07:57<03:21,  3.14s/it]Running Inference:  68%|██████▊   | 137/200 [08:01<03:38,  3.47s/it]Running Inference:  69%|██████▉   | 138/200 [08:06<03:47,  3.67s/it]Running Inference:  70%|██████▉   | 139/200 [08:10<03:54,  3.84s/it]Running Inference:  70%|███████   | 140/200 [08:14<03:56,  3.94s/it]Running Inference:  70%|███████   | 141/200 [08:18<03:58,  4.04s/it]Running Inference:  71%|███████   | 142/200 [08:22<03:56,  4.09s/it]Running Inference:  72%|███████▏  | 143/200 [08:25<03:34,  3.77s/it]Running Inference:  72%|███████▏  | 144/200 [08:28<03:07,  3.35s/it]Running Inference:  72%|███████▎  | 145/200 [08:32<03:16,  3.58s/it]Running Inference:  73%|███████▎  | 146/200 [08:34<02:51,  3.18s/it]Running Inference:  74%|███████▎  | 147/200 [08:38<03:05,  3.50s/it]Running Inference:  74%|███████▍  | 148/200 [08:43<03:13,  3.73s/it]Running Inference:  74%|███████▍  | 149/200 [08:47<03:17,  3.88s/it]Running Inference:  75%|███████▌  | 150/200 [08:49<02:51,  3.42s/it]Running Inference:  76%|███████▌  | 151/200 [08:51<02:29,  3.05s/it]Running Inference:  76%|███████▌  | 152/200 [08:56<02:43,  3.40s/it]Running Inference:  76%|███████▋  | 153/200 [09:00<02:52,  3.67s/it]Running Inference:  77%|███████▋  | 154/200 [09:02<02:29,  3.25s/it]Running Inference:  78%|███████▊  | 155/200 [09:06<02:38,  3.53s/it]Running Inference:  78%|███████▊  | 156/200 [09:08<02:12,  3.02s/it]Running Inference:  78%|███████▊  | 157/200 [09:13<02:26,  3.40s/it]Running Inference:  79%|███████▉  | 158/200 [09:15<02:08,  3.06s/it]Running Inference:  80%|███████▉  | 159/200 [09:19<02:13,  3.26s/it]Running Inference:  80%|████████  | 160/200 [09:23<02:21,  3.53s/it]Running Inference:  80%|████████  | 161/200 [09:25<02:01,  3.12s/it]Running Inference:  81%|████████  | 162/200 [09:29<02:11,  3.46s/it]Running Inference:  82%|████████▏ | 163/200 [09:33<02:16,  3.68s/it]Running Inference:  82%|████████▏ | 164/200 [09:36<02:00,  3.36s/it]Running Inference:  82%|████████▎ | 165/200 [09:40<02:06,  3.60s/it]Running Inference:  83%|████████▎ | 166/200 [09:44<02:08,  3.78s/it]Running Inference:  84%|████████▎ | 167/200 [09:47<01:51,  3.37s/it]Running Inference:  84%|████████▍ | 168/200 [09:51<01:56,  3.64s/it]Running Inference:  84%|████████▍ | 169/200 [09:55<01:58,  3.82s/it]Running Inference:  85%|████████▌ | 170/200 [09:59<01:58,  3.94s/it]Running Inference:  86%|████████▌ | 171/200 [10:04<01:56,  4.01s/it]Running Inference:  86%|████████▌ | 172/200 [10:08<01:53,  4.07s/it]Running Inference:  86%|████████▋ | 173/200 [10:11<01:45,  3.90s/it]Running Inference:  87%|████████▋ | 174/200 [10:16<01:43,  3.99s/it]Running Inference:  88%|████████▊ | 175/200 [10:20<01:41,  4.06s/it]Running Inference:  88%|████████▊ | 176/200 [10:24<01:39,  4.14s/it]Running Inference:  88%|████████▊ | 177/200 [10:28<01:35,  4.14s/it]Running Inference:  89%|████████▉ | 178/200 [10:32<01:31,  4.18s/it]Running Inference:  90%|████████▉ | 179/200 [10:35<01:15,  3.57s/it]Running Inference:  90%|█████████ | 180/200 [10:37<01:01,  3.07s/it]Running Inference:  90%|█████████ | 181/200 [10:39<00:53,  2.81s/it]Running Inference:  91%|█████████ | 182/200 [10:42<00:54,  3.00s/it]Running Inference:  92%|█████████▏| 183/200 [10:46<00:56,  3.34s/it]Running Inference:  92%|█████████▏| 184/200 [10:51<00:57,  3.61s/it]Running Inference:  92%|█████████▎| 185/200 [10:53<00:47,  3.18s/it]Running Inference:  93%|█████████▎| 186/200 [10:57<00:48,  3.49s/it]Running Inference:  94%|█████████▎| 187/200 [10:59<00:40,  3.13s/it]Running Inference:  94%|█████████▍| 188/200 [11:01<00:32,  2.69s/it]Running Inference:  94%|█████████▍| 189/200 [11:05<00:34,  3.18s/it]Running Inference:  95%|█████████▌| 190/200 [11:07<00:28,  2.81s/it]Running Inference:  96%|█████████▌| 191/200 [11:11<00:29,  3.23s/it]Running Inference:  96%|█████████▌| 192/200 [11:16<00:28,  3.53s/it]Running Inference:  96%|█████████▋| 193/200 [11:20<00:26,  3.74s/it]Running Inference:  97%|█████████▋| 194/200 [11:24<00:23,  3.88s/it]Running Inference:  98%|█████████▊| 195/200 [11:28<00:20,  4.01s/it]Running Inference:  98%|█████████▊| 196/200 [11:31<00:14,  3.54s/it]Running Inference:  98%|█████████▊| 197/200 [11:34<00:10,  3.40s/it]Running Inference:  99%|█████████▉| 198/200 [11:38<00:07,  3.65s/it]Running Inference: 100%|█████████▉| 199/200 [11:41<00:03,  3.53s/it]Running Inference: 100%|██████████| 200/200 [11:45<00:00,  3.46s/it]Running Inference: 100%|██████████| 200/200 [11:45<00:00,  3.53s/it]
2025-12-13 19:33:29,965 - INFO - Inference completed.
2025-12-13 19:33:29,974 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 19:33:29,974 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:33:29,982 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 19:33:29,982 - INFO - Metrics:
13.69
2025-12-13 19:33:29,983 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=musique, ratio=0.1) on GPU 2

----------------------------------------
Task: musique | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:33:36,642 - INFO - Set deterministic seeds to 42
2025-12-13 19:33:36,643 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:33:36,643 - INFO - Starting evaluation run...
2025-12-13 19:33:36,643 - INFO - Output directory set to: longbenchresult
2025-12-13 19:33:36,643 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 19:33:36,643 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:33:36,643 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:33:36,643 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.75it/s]
Device set to use cuda:0
2025-12-13 19:33:49,881 - INFO - Model pipeline loaded.
2025-12-13 19:33:49,881 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:33:55,158 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:33:55,158 - INFO - Dataset processed with 200 entries.
2025-12-13 19:33:55,200 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<15:35,  4.70s/it]Running Inference:   1%|          | 2/200 [00:07<11:05,  3.36s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:25,  2.87s/it]Running Inference:   2%|▏         | 4/200 [00:13<11:17,  3.45s/it]Running Inference:   2%|▎         | 5/200 [00:17<12:07,  3.73s/it]Running Inference:   3%|▎         | 6/200 [00:22<12:34,  3.89s/it]Running Inference:   4%|▎         | 7/200 [00:25<11:45,  3.65s/it]Running Inference:   4%|▍         | 8/200 [00:27<10:20,  3.23s/it]Running Inference:   4%|▍         | 9/200 [00:31<11:18,  3.55s/it]Running Inference:   5%|▌         | 10/200 [00:36<11:56,  3.77s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:38<10:04,  3.20s/it]Running Inference:   6%|▌         | 12/200 [00:42<11:01,  3.52s/it]Running Inference:   6%|▋         | 13/200 [00:44<10:07,  3.25s/it]Running Inference:   7%|▋         | 14/200 [00:47<09:41,  3.13s/it]Running Inference:   8%|▊         | 15/200 [00:52<10:46,  3.49s/it]Running Inference:   8%|▊         | 16/200 [00:56<11:19,  3.69s/it]Running Inference:   8%|▊         | 17/200 [01:00<11:40,  3.83s/it]Running Inference:   9%|▉         | 18/200 [01:04<11:53,  3.92s/it]Running Inference:  10%|▉         | 19/200 [01:08<12:07,  4.02s/it]Running Inference:  10%|█         | 20/200 [01:13<12:14,  4.08s/it]Running Inference:  10%|█         | 21/200 [01:17<12:17,  4.12s/it]Running Inference:  11%|█         | 22/200 [01:21<12:12,  4.12s/it]Running Inference:  12%|█▏        | 23/200 [01:23<10:25,  3.53s/it]Running Inference:  12%|█▏        | 24/200 [01:25<08:35,  2.93s/it]Running Inference:  12%|█▎        | 25/200 [01:29<09:39,  3.31s/it]Running Inference:  13%|█▎        | 26/200 [01:33<10:26,  3.60s/it]Running Inference:  14%|█▎        | 27/200 [01:37<10:56,  3.79s/it]Running Inference:  14%|█▍        | 28/200 [01:42<11:16,  3.94s/it]Running Inference:  14%|█▍        | 29/200 [01:46<11:28,  4.03s/it]Running Inference:  15%|█▌        | 30/200 [01:48<09:52,  3.49s/it]Running Inference:  16%|█▌        | 31/200 [01:50<08:46,  3.12s/it]Running Inference:  16%|█▌        | 32/200 [01:54<09:14,  3.30s/it]Running Inference:  16%|█▋        | 33/200 [01:57<08:51,  3.18s/it]Running Inference:  17%|█▋        | 34/200 [02:01<09:33,  3.46s/it]Running Inference:  18%|█▊        | 35/200 [02:05<09:51,  3.58s/it]Running Inference:  18%|█▊        | 36/200 [02:09<10:04,  3.69s/it]Running Inference:  18%|█▊        | 37/200 [02:13<10:29,  3.86s/it]Running Inference:  19%|█▉        | 38/200 [02:17<10:36,  3.93s/it]Running Inference:  20%|█▉        | 39/200 [02:21<10:24,  3.88s/it]Running Inference:  20%|██        | 40/200 [02:25<10:30,  3.94s/it]Running Inference:  20%|██        | 41/200 [02:29<10:37,  4.01s/it]Running Inference:  21%|██        | 42/200 [02:33<10:45,  4.09s/it]Running Inference:  22%|██▏       | 43/200 [02:36<09:20,  3.57s/it]Running Inference:  22%|██▏       | 44/200 [02:38<08:25,  3.24s/it]Running Inference:  22%|██▎       | 45/200 [02:42<09:01,  3.50s/it]Running Inference:  23%|██▎       | 46/200 [02:45<07:59,  3.11s/it]Running Inference:  24%|██▎       | 47/200 [02:48<08:01,  3.15s/it]Running Inference:  24%|██▍       | 48/200 [02:52<08:46,  3.46s/it]Running Inference:  24%|██▍       | 49/200 [02:56<09:20,  3.71s/it]Running Inference:  25%|██▌       | 50/200 [03:01<09:41,  3.88s/it]Running Inference:  26%|██▌       | 51/200 [03:05<09:53,  3.99s/it]Running Inference:  26%|██▌       | 52/200 [03:09<10:03,  4.08s/it]Running Inference:  26%|██▋       | 53/200 [03:13<10:04,  4.12s/it]Running Inference:  27%|██▋       | 54/200 [03:18<10:05,  4.15s/it]Running Inference:  28%|██▊       | 55/200 [03:20<08:52,  3.67s/it]Running Inference:  28%|██▊       | 56/200 [03:23<07:54,  3.30s/it]Running Inference:  28%|██▊       | 57/200 [03:27<08:36,  3.61s/it]Running Inference:  29%|██▉       | 58/200 [03:29<07:35,  3.21s/it]Running Inference:  30%|██▉       | 59/200 [03:31<06:49,  2.91s/it]Running Inference:  30%|███       | 60/200 [03:36<07:42,  3.30s/it]Running Inference:  30%|███       | 61/200 [03:39<07:33,  3.26s/it]Running Inference:  31%|███       | 62/200 [03:42<07:11,  3.12s/it]Running Inference:  32%|███▏      | 63/200 [03:46<07:54,  3.46s/it]Running Inference:  32%|███▏      | 64/200 [03:50<08:24,  3.71s/it]Running Inference:  32%|███▎      | 65/200 [03:52<07:04,  3.15s/it]Running Inference:  33%|███▎      | 66/200 [03:56<07:48,  3.49s/it]Running Inference:  34%|███▎      | 67/200 [03:58<06:55,  3.12s/it]Running Inference:  34%|███▍      | 68/200 [04:03<07:34,  3.44s/it]Running Inference:  34%|███▍      | 69/200 [04:05<06:42,  3.07s/it]Running Inference:  35%|███▌      | 70/200 [04:08<06:22,  2.94s/it]Running Inference:  36%|███▌      | 71/200 [04:12<07:10,  3.34s/it]Running Inference:  36%|███▌      | 72/200 [04:14<06:32,  3.07s/it]Running Inference:  36%|███▋      | 73/200 [04:18<06:38,  3.14s/it]Running Inference:  37%|███▋      | 74/200 [04:22<07:19,  3.49s/it]Running Inference:  38%|███▊      | 75/200 [04:26<07:49,  3.76s/it]Running Inference:  38%|███▊      | 76/200 [04:30<08:03,  3.90s/it]Running Inference:  38%|███▊      | 77/200 [04:32<06:36,  3.22s/it]Running Inference:  39%|███▉      | 78/200 [04:34<05:55,  2.92s/it]Running Inference:  40%|███▉      | 79/200 [04:37<05:46,  2.86s/it]Running Inference:  40%|████      | 80/200 [04:39<05:18,  2.65s/it]Running Inference:  40%|████      | 81/200 [04:42<05:06,  2.58s/it]Running Inference:  41%|████      | 82/200 [04:46<06:06,  3.11s/it]Running Inference:  42%|████▏     | 83/200 [04:49<05:50,  3.00s/it]Running Inference:  42%|████▏     | 84/200 [04:53<06:27,  3.34s/it]Running Inference:  42%|████▎     | 85/200 [04:57<06:54,  3.60s/it]Running Inference:  43%|████▎     | 86/200 [05:01<07:11,  3.78s/it]Running Inference:  44%|████▎     | 87/200 [05:06<07:24,  3.94s/it]Running Inference:  44%|████▍     | 88/200 [05:10<07:30,  4.02s/it]Running Inference:  44%|████▍     | 89/200 [05:12<06:25,  3.47s/it]Running Inference:  45%|████▌     | 90/200 [05:16<06:47,  3.71s/it]Running Inference:  46%|████▌     | 91/200 [05:20<06:57,  3.83s/it]Running Inference:  46%|████▌     | 92/200 [05:23<06:08,  3.41s/it]Running Inference:  46%|████▋     | 93/200 [05:25<05:27,  3.06s/it]Running Inference:  47%|████▋     | 94/200 [05:29<06:00,  3.40s/it]Running Inference:  48%|████▊     | 95/200 [05:33<06:22,  3.64s/it]Running Inference:  48%|████▊     | 96/200 [05:37<06:11,  3.57s/it]Running Inference:  48%|████▊     | 97/200 [05:39<05:30,  3.21s/it]Running Inference:  49%|████▉     | 98/200 [05:41<04:59,  2.93s/it]Running Inference:  50%|████▉     | 99/200 [05:45<05:28,  3.25s/it]Running Inference:  50%|█████     | 100/200 [05:49<05:48,  3.48s/it]Running Inference:  50%|█████     | 101/200 [05:52<05:26,  3.30s/it]Running Inference:  51%|█████     | 102/200 [05:55<04:51,  2.97s/it]Running Inference:  52%|█████▏    | 103/200 [05:59<05:22,  3.33s/it]Running Inference:  52%|█████▏    | 104/200 [06:01<04:52,  3.04s/it]Running Inference:  52%|█████▎    | 105/200 [06:05<05:25,  3.43s/it]Running Inference:  53%|█████▎    | 106/200 [06:10<05:45,  3.67s/it]Running Inference:  54%|█████▎    | 107/200 [06:14<05:52,  3.79s/it]Running Inference:  54%|█████▍    | 108/200 [06:16<05:04,  3.31s/it]Running Inference:  55%|█████▍    | 109/200 [06:20<05:29,  3.62s/it]Running Inference:  55%|█████▌    | 110/200 [06:24<05:42,  3.81s/it]Running Inference:  56%|█████▌    | 111/200 [06:28<05:43,  3.86s/it]Running Inference:  56%|█████▌    | 112/200 [06:31<04:58,  3.39s/it]Running Inference:  56%|█████▋    | 113/200 [06:35<05:16,  3.64s/it]Running Inference:  57%|█████▋    | 114/200 [06:39<05:27,  3.81s/it]Running Inference:  57%|█████▊    | 115/200 [06:43<05:33,  3.93s/it]Running Inference:  58%|█████▊    | 116/200 [06:47<05:10,  3.69s/it]Running Inference:  58%|█████▊    | 117/200 [06:51<05:19,  3.84s/it]Running Inference:  59%|█████▉    | 118/200 [06:53<04:32,  3.32s/it]Running Inference:  60%|█████▉    | 119/200 [06:55<04:05,  3.04s/it]Running Inference:  60%|██████    | 120/200 [06:57<03:44,  2.81s/it]Running Inference:  60%|██████    | 121/200 [07:00<03:28,  2.64s/it]Running Inference:  61%|██████    | 122/200 [07:02<03:14,  2.50s/it]Running Inference:  62%|██████▏   | 123/200 [07:06<03:54,  3.04s/it]Running Inference:  62%|██████▏   | 124/200 [07:10<04:18,  3.40s/it]Running Inference:  62%|██████▎   | 125/200 [07:14<04:17,  3.43s/it]Running Inference:  63%|██████▎   | 126/200 [07:18<04:32,  3.68s/it]Running Inference:  64%|██████▎   | 127/200 [07:22<04:38,  3.81s/it]Running Inference:  64%|██████▍   | 128/200 [07:27<04:42,  3.93s/it]Running Inference:  64%|██████▍   | 129/200 [07:30<04:25,  3.74s/it]Running Inference:  65%|██████▌   | 130/200 [07:34<04:31,  3.88s/it]Running Inference:  66%|██████▌   | 131/200 [07:36<03:53,  3.38s/it]Running Inference:  66%|██████▌   | 132/200 [07:39<03:29,  3.07s/it]Running Inference:  66%|██████▋   | 133/200 [07:41<03:06,  2.79s/it]Running Inference:  67%|██████▋   | 134/200 [07:42<02:33,  2.32s/it]Running Inference:  68%|██████▊   | 135/200 [07:46<03:10,  2.93s/it]Running Inference:  68%|██████▊   | 136/200 [07:51<03:33,  3.34s/it]Running Inference:  68%|██████▊   | 137/200 [07:55<03:47,  3.62s/it]Running Inference:  69%|██████▉   | 138/200 [07:59<03:54,  3.78s/it]Running Inference:  70%|██████▉   | 139/200 [08:03<03:59,  3.92s/it]Running Inference:  70%|███████   | 140/200 [08:08<04:00,  4.01s/it]Running Inference:  70%|███████   | 141/200 [08:12<04:01,  4.09s/it]Running Inference:  71%|███████   | 142/200 [08:16<03:59,  4.12s/it]Running Inference:  72%|███████▏  | 143/200 [08:19<03:43,  3.91s/it]Running Inference:  72%|███████▏  | 144/200 [08:21<02:59,  3.21s/it]Running Inference:  72%|███████▎  | 145/200 [08:25<03:12,  3.49s/it]Running Inference:  73%|███████▎  | 146/200 [08:27<02:48,  3.12s/it]Running Inference:  74%|███████▎  | 147/200 [08:32<03:03,  3.46s/it]Running Inference:  74%|███████▍  | 148/200 [08:36<03:12,  3.71s/it]Running Inference:  74%|███████▍  | 149/200 [08:40<03:17,  3.87s/it]Running Inference:  75%|███████▌  | 150/200 [08:44<03:18,  3.98s/it]Running Inference:  76%|███████▌  | 151/200 [08:47<02:48,  3.43s/it]Running Inference:  76%|███████▌  | 152/200 [08:51<02:56,  3.68s/it]Running Inference:  76%|███████▋  | 153/200 [08:55<03:01,  3.86s/it]Running Inference:  77%|███████▋  | 154/200 [08:57<02:35,  3.39s/it]Running Inference:  78%|███████▊  | 155/200 [09:02<02:43,  3.63s/it]Running Inference:  78%|███████▊  | 156/200 [09:03<02:15,  3.09s/it]Running Inference:  78%|███████▊  | 157/200 [09:08<02:28,  3.46s/it]Running Inference:  79%|███████▉  | 158/200 [09:11<02:22,  3.39s/it]Running Inference:  80%|███████▉  | 159/200 [09:15<02:23,  3.49s/it]Running Inference:  80%|████████  | 160/200 [09:19<02:28,  3.70s/it]Running Inference:  80%|████████  | 161/200 [09:21<02:06,  3.24s/it]Running Inference:  81%|████████  | 162/200 [09:25<02:14,  3.55s/it]Running Inference:  82%|████████▏ | 163/200 [09:30<02:18,  3.75s/it]Running Inference:  82%|████████▏ | 164/200 [09:32<02:02,  3.41s/it]Running Inference:  82%|████████▎ | 165/200 [09:36<02:07,  3.64s/it]Running Inference:  83%|████████▎ | 166/200 [09:41<02:09,  3.82s/it]Running Inference:  84%|████████▎ | 167/200 [09:44<02:03,  3.76s/it]Running Inference:  84%|████████▍ | 168/200 [09:48<02:05,  3.91s/it]Running Inference:  84%|████████▍ | 169/200 [09:53<02:04,  4.01s/it]Running Inference:  85%|████████▌ | 170/200 [09:57<02:02,  4.08s/it]Running Inference:  86%|████████▌ | 171/200 [10:01<01:59,  4.11s/it]Running Inference:  86%|████████▌ | 172/200 [10:05<01:56,  4.15s/it]Running Inference:  86%|████████▋ | 173/200 [10:09<01:47,  3.96s/it]Running Inference:  87%|████████▋ | 174/200 [10:13<01:45,  4.05s/it]Running Inference:  88%|████████▊ | 175/200 [10:17<01:42,  4.11s/it]Running Inference:  88%|████████▊ | 176/200 [10:22<01:40,  4.18s/it]Running Inference:  88%|████████▊ | 177/200 [10:26<01:36,  4.18s/it]Running Inference:  89%|████████▉ | 178/200 [10:30<01:32,  4.22s/it]Running Inference:  90%|████████▉ | 179/200 [10:32<01:15,  3.60s/it]Running Inference:  90%|█████████ | 180/200 [10:34<01:01,  3.10s/it]Running Inference:  90%|█████████ | 181/200 [10:37<00:53,  2.83s/it]Running Inference:  91%|█████████ | 182/200 [10:40<00:54,  3.02s/it]Running Inference:  92%|█████████▏| 183/200 [10:44<00:55,  3.27s/it]Running Inference:  92%|█████████▏| 184/200 [10:48<00:57,  3.56s/it]Running Inference:  92%|█████████▎| 185/200 [10:50<00:47,  3.15s/it]Running Inference:  93%|█████████▎| 186/200 [10:54<00:48,  3.48s/it]Running Inference:  94%|█████████▎| 187/200 [10:57<00:40,  3.12s/it]Running Inference:  94%|█████████▍| 188/200 [10:58<00:32,  2.69s/it]Running Inference:  94%|█████████▍| 189/200 [11:03<00:34,  3.18s/it]Running Inference:  95%|█████████▌| 190/200 [11:07<00:34,  3.41s/it]Running Inference:  96%|█████████▌| 191/200 [11:11<00:32,  3.66s/it]Running Inference:  96%|█████████▌| 192/200 [11:15<00:30,  3.84s/it]Running Inference:  96%|█████████▋| 193/200 [11:19<00:27,  3.96s/it]Running Inference:  97%|█████████▋| 194/200 [11:24<00:24,  4.05s/it]Running Inference:  98%|█████████▊| 195/200 [11:28<00:20,  4.13s/it]Running Inference:  98%|█████████▊| 196/200 [11:30<00:14,  3.63s/it]Running Inference:  98%|█████████▊| 197/200 [11:34<00:10,  3.46s/it]Running Inference:  99%|█████████▉| 198/200 [11:37<00:07,  3.57s/it]Running Inference: 100%|█████████▉| 199/200 [11:42<00:03,  3.76s/it]Running Inference: 100%|██████████| 200/200 [11:45<00:00,  3.61s/it]Running Inference: 100%|██████████| 200/200 [11:45<00:00,  3.53s/it]
2025-12-13 19:45:40,570 - INFO - Inference completed.
2025-12-13 19:45:40,579 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 19:45:40,579 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:45:40,587 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 19:45:40,587 - INFO - Metrics:
11.71
2025-12-13 19:45:40,588 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=musique, ratio=0.2) on GPU 2

----------------------------------------
Task: musique | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:45:46,952 - INFO - Set deterministic seeds to 42
2025-12-13 19:45:46,952 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:45:46,953 - INFO - Starting evaluation run...
2025-12-13 19:45:46,953 - INFO - Output directory set to: longbenchresult
2025-12-13 19:45:46,953 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 19:45:46,953 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:45:46,953 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:45:46,953 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.49it/s]
Device set to use cuda:0
2025-12-13 19:46:02,962 - INFO - Model pipeline loaded.
2025-12-13 19:46:02,962 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:46:09,367 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:46:09,367 - INFO - Dataset processed with 200 entries.
2025-12-13 19:46:09,408 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<15:17,  4.61s/it]Running Inference:   1%|          | 2/200 [00:07<10:57,  3.32s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:20,  2.85s/it]Running Inference:   2%|▏         | 4/200 [00:13<11:12,  3.43s/it]Running Inference:   2%|▎         | 5/200 [00:17<12:03,  3.71s/it]Running Inference:   3%|▎         | 6/200 [00:20<10:24,  3.22s/it]Running Inference:   4%|▎         | 7/200 [00:23<10:17,  3.20s/it]Running Inference:   4%|▍         | 8/200 [00:25<09:20,  2.92s/it]Running Inference:   4%|▍         | 9/200 [00:29<10:35,  3.33s/it]Running Inference:   5%|▌         | 10/200 [00:34<11:25,  3.61s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:35<09:43,  3.09s/it]Running Inference:   6%|▌         | 12/200 [00:40<10:44,  3.43s/it]Running Inference:   6%|▋         | 13/200 [00:42<09:52,  3.17s/it]Running Inference:   7%|▋         | 14/200 [00:45<09:31,  3.07s/it]Running Inference:   8%|▊         | 15/200 [00:49<10:37,  3.45s/it]Running Inference:   8%|▊         | 16/200 [00:54<11:12,  3.66s/it]Running Inference:   8%|▊         | 17/200 [00:58<11:41,  3.83s/it]Running Inference:   9%|▉         | 18/200 [01:02<11:52,  3.92s/it]Running Inference:  10%|▉         | 19/200 [01:05<11:28,  3.80s/it]Running Inference:  10%|█         | 20/200 [01:10<11:46,  3.92s/it]Running Inference:  10%|█         | 21/200 [01:12<10:25,  3.49s/it]Running Inference:  11%|█         | 22/200 [01:16<10:53,  3.67s/it]Running Inference:  12%|█▏        | 23/200 [01:18<09:29,  3.22s/it]Running Inference:  12%|█▏        | 24/200 [01:20<07:56,  2.71s/it]Running Inference:  12%|█▎        | 25/200 [01:24<09:12,  3.16s/it]Running Inference:  13%|█▎        | 26/200 [01:28<10:06,  3.48s/it]Running Inference:  14%|█▎        | 27/200 [01:33<10:40,  3.70s/it]Running Inference:  14%|█▍        | 28/200 [01:37<11:04,  3.87s/it]Running Inference:  14%|█▍        | 29/200 [01:41<11:18,  3.97s/it]Running Inference:  15%|█▌        | 30/200 [01:43<09:45,  3.44s/it]Running Inference:  16%|█▌        | 31/200 [01:46<08:43,  3.10s/it]Running Inference:  16%|█▌        | 32/200 [01:49<09:10,  3.28s/it]Running Inference:  16%|█▋        | 33/200 [01:53<09:53,  3.56s/it]Running Inference:  17%|█▋        | 34/200 [01:58<10:16,  3.71s/it]Running Inference:  18%|█▊        | 35/200 [02:01<10:17,  3.74s/it]Running Inference:  18%|█▊        | 36/200 [02:06<10:42,  3.91s/it]Running Inference:  18%|█▊        | 37/200 [02:10<10:54,  4.02s/it]Running Inference:  19%|█▉        | 38/200 [02:14<10:50,  4.02s/it]Running Inference:  20%|█▉        | 39/200 [02:18<10:58,  4.09s/it]Running Inference:  20%|██        | 40/200 [02:20<09:25,  3.54s/it]Running Inference:  20%|██        | 41/200 [02:25<09:51,  3.72s/it]Running Inference:  21%|██        | 42/200 [02:27<08:56,  3.39s/it]Running Inference:  22%|██▏       | 43/200 [02:30<08:04,  3.08s/it]Running Inference:  22%|██▏       | 44/200 [02:32<07:32,  2.90s/it]Running Inference:  22%|██▎       | 45/200 [02:36<08:23,  3.25s/it]Running Inference:  23%|██▎       | 46/200 [02:38<07:32,  2.94s/it]Running Inference:  24%|██▎       | 47/200 [02:42<07:52,  3.09s/it]Running Inference:  24%|██▍       | 48/200 [02:46<08:38,  3.41s/it]Running Inference:  24%|██▍       | 49/200 [02:50<09:12,  3.66s/it]Running Inference:  25%|██▌       | 50/200 [02:54<09:35,  3.84s/it]Running Inference:  26%|██▌       | 51/200 [02:59<09:48,  3.95s/it]Running Inference:  26%|██▌       | 52/200 [03:03<09:59,  4.05s/it]Running Inference:  26%|██▋       | 53/200 [03:07<10:00,  4.08s/it]Running Inference:  27%|██▋       | 54/200 [03:11<10:01,  4.12s/it]Running Inference:  28%|██▊       | 55/200 [03:14<08:49,  3.65s/it]Running Inference:  28%|██▊       | 56/200 [03:17<08:28,  3.53s/it]Running Inference:  28%|██▊       | 57/200 [03:21<08:59,  3.77s/it]Running Inference:  29%|██▉       | 58/200 [03:23<07:29,  3.16s/it]Running Inference:  30%|██▉       | 59/200 [03:25<06:44,  2.87s/it]Running Inference:  30%|███       | 60/200 [03:30<07:37,  3.27s/it]Running Inference:  30%|███       | 61/200 [03:33<07:29,  3.23s/it]Running Inference:  31%|███       | 62/200 [03:35<07:07,  3.10s/it]Running Inference:  32%|███▏      | 63/200 [03:38<06:47,  2.97s/it]Running Inference:  32%|███▏      | 64/200 [03:42<07:37,  3.36s/it]Running Inference:  32%|███▎      | 65/200 [03:44<06:31,  2.90s/it]Running Inference:  33%|███▎      | 66/200 [03:49<07:24,  3.32s/it]Running Inference:  34%|███▎      | 67/200 [03:51<06:38,  3.00s/it]Running Inference:  34%|███▍      | 68/200 [03:55<07:05,  3.22s/it]Running Inference:  34%|███▍      | 69/200 [03:59<07:42,  3.53s/it]Running Inference:  35%|███▌      | 70/200 [04:01<07:02,  3.25s/it]Running Inference:  36%|███▌      | 71/200 [04:06<07:37,  3.55s/it]Running Inference:  36%|███▌      | 72/200 [04:08<06:50,  3.21s/it]Running Inference:  36%|███▋      | 73/200 [04:11<06:50,  3.23s/it]Running Inference:  37%|███▋      | 74/200 [04:16<07:27,  3.55s/it]Running Inference:  38%|███▊      | 75/200 [04:20<07:53,  3.79s/it]Running Inference:  38%|███▊      | 76/200 [04:24<08:05,  3.92s/it]Running Inference:  38%|███▊      | 77/200 [04:26<06:37,  3.23s/it]Running Inference:  39%|███▉      | 78/200 [04:28<05:56,  2.92s/it]Running Inference:  40%|███▉      | 79/200 [04:31<05:50,  2.90s/it]Running Inference:  40%|████      | 80/200 [04:33<05:21,  2.68s/it]Running Inference:  40%|████      | 81/200 [04:35<05:08,  2.60s/it]Running Inference:  41%|████      | 82/200 [04:40<06:07,  3.11s/it]Running Inference:  42%|████▏     | 83/200 [04:42<05:50,  3.00s/it]Running Inference:  42%|████▏     | 84/200 [04:47<06:26,  3.33s/it]Running Inference:  42%|████▎     | 85/200 [04:51<06:53,  3.59s/it]Running Inference:  43%|████▎     | 86/200 [04:53<06:17,  3.31s/it]Running Inference:  44%|████▎     | 87/200 [04:56<06:01,  3.20s/it]Running Inference:  44%|████▍     | 88/200 [05:01<06:31,  3.50s/it]Running Inference:  44%|████▍     | 89/200 [05:03<05:44,  3.10s/it]Running Inference:  45%|████▌     | 90/200 [05:07<06:18,  3.44s/it]Running Inference:  46%|████▌     | 91/200 [05:11<06:37,  3.64s/it]Running Inference:  46%|████▌     | 92/200 [05:14<05:53,  3.27s/it]Running Inference:  46%|████▋     | 93/200 [05:16<05:16,  2.96s/it]Running Inference:  47%|████▋     | 94/200 [05:20<05:52,  3.32s/it]Running Inference:  48%|████▊     | 95/200 [05:24<06:16,  3.58s/it]Running Inference:  48%|████▊     | 96/200 [05:27<06:06,  3.52s/it]Running Inference:  48%|████▊     | 97/200 [05:30<05:26,  3.17s/it]Running Inference:  49%|████▉     | 98/200 [05:32<04:56,  2.91s/it]Running Inference:  50%|████▉     | 99/200 [05:36<05:26,  3.23s/it]Running Inference:  50%|█████     | 100/200 [05:40<05:45,  3.46s/it]Running Inference:  50%|█████     | 101/200 [05:43<05:20,  3.24s/it]Running Inference:  51%|█████     | 102/200 [05:45<04:46,  2.93s/it]Running Inference:  52%|█████▏    | 103/200 [05:49<05:19,  3.29s/it]Running Inference:  52%|█████▏    | 104/200 [05:52<04:49,  3.02s/it]Running Inference:  52%|█████▎    | 105/200 [05:56<05:23,  3.40s/it]Running Inference:  53%|█████▎    | 106/200 [06:00<05:35,  3.57s/it]Running Inference:  54%|█████▎    | 107/200 [06:04<05:45,  3.72s/it]Running Inference:  54%|█████▍    | 108/200 [06:06<04:59,  3.25s/it]Running Inference:  55%|█████▍    | 109/200 [06:10<05:25,  3.57s/it]Running Inference:  55%|█████▌    | 110/200 [06:14<05:36,  3.73s/it]Running Inference:  56%|█████▌    | 111/200 [06:18<05:38,  3.80s/it]Running Inference:  56%|█████▌    | 112/200 [06:21<04:54,  3.35s/it]Running Inference:  56%|█████▋    | 113/200 [06:25<05:13,  3.61s/it]Running Inference:  57%|█████▋    | 114/200 [06:29<05:25,  3.78s/it]Running Inference:  57%|█████▊    | 115/200 [06:33<05:31,  3.90s/it]Running Inference:  58%|█████▊    | 116/200 [06:35<04:42,  3.37s/it]Running Inference:  58%|█████▊    | 117/200 [06:39<04:42,  3.40s/it]Running Inference:  59%|█████▉    | 118/200 [06:41<04:07,  3.01s/it]Running Inference:  60%|█████▉    | 119/200 [06:43<03:50,  2.84s/it]Running Inference:  60%|██████    | 120/200 [06:46<03:35,  2.70s/it]Running Inference:  60%|██████    | 121/200 [06:48<03:22,  2.56s/it]Running Inference:  61%|██████    | 122/200 [06:50<03:10,  2.44s/it]Running Inference:  62%|██████▏   | 123/200 [06:54<03:50,  3.00s/it]Running Inference:  62%|██████▏   | 124/200 [06:59<04:15,  3.36s/it]Running Inference:  62%|██████▎   | 125/200 [07:02<04:15,  3.40s/it]Running Inference:  63%|██████▎   | 126/200 [07:06<04:30,  3.65s/it]Running Inference:  64%|██████▎   | 127/200 [07:10<04:32,  3.73s/it]Running Inference:  64%|██████▍   | 128/200 [07:15<04:38,  3.87s/it]Running Inference:  64%|██████▍   | 129/200 [07:18<04:22,  3.69s/it]Running Inference:  65%|██████▌   | 130/200 [07:22<04:29,  3.85s/it]Running Inference:  66%|██████▌   | 131/200 [07:24<03:52,  3.37s/it]Running Inference:  66%|██████▌   | 132/200 [07:27<03:28,  3.06s/it]Running Inference:  66%|██████▋   | 133/200 [07:29<03:06,  2.78s/it]Running Inference:  67%|██████▋   | 134/200 [07:30<02:33,  2.32s/it]Running Inference:  68%|██████▊   | 135/200 [07:34<03:10,  2.92s/it]Running Inference:  68%|██████▊   | 136/200 [07:39<03:33,  3.33s/it]Running Inference:  68%|██████▊   | 137/200 [07:43<03:46,  3.60s/it]Running Inference:  69%|██████▉   | 138/200 [07:47<03:53,  3.76s/it]Running Inference:  70%|██████▉   | 139/200 [07:51<03:58,  3.90s/it]Running Inference:  70%|███████   | 140/200 [07:55<03:59,  3.99s/it]Running Inference:  70%|███████   | 141/200 [08:00<03:59,  4.06s/it]Running Inference:  71%|███████   | 142/200 [08:04<03:57,  4.10s/it]Running Inference:  72%|███████▏  | 143/200 [08:07<03:42,  3.90s/it]Running Inference:  72%|███████▏  | 144/200 [08:10<03:13,  3.45s/it]Running Inference:  72%|███████▎  | 145/200 [08:14<03:20,  3.65s/it]Running Inference:  73%|███████▎  | 146/200 [08:16<02:54,  3.23s/it]Running Inference:  74%|███████▎  | 147/200 [08:20<03:07,  3.53s/it]Running Inference:  74%|███████▍  | 148/200 [08:25<03:14,  3.75s/it]Running Inference:  74%|███████▍  | 149/200 [08:29<03:18,  3.89s/it]Running Inference:  75%|███████▌  | 150/200 [08:32<03:06,  3.73s/it]Running Inference:  76%|███████▌  | 151/200 [08:34<02:40,  3.27s/it]Running Inference:  76%|███████▌  | 152/200 [08:38<02:50,  3.55s/it]Running Inference:  76%|███████▋  | 153/200 [08:43<02:57,  3.77s/it]Running Inference:  77%|███████▋  | 154/200 [08:47<02:59,  3.90s/it]Running Inference:  78%|███████▊  | 155/200 [08:51<03:00,  4.01s/it]Running Inference:  78%|███████▊  | 156/200 [08:55<02:52,  3.92s/it]Running Inference:  78%|███████▊  | 157/200 [08:59<02:53,  4.03s/it]Running Inference:  79%|███████▉  | 158/200 [09:02<02:39,  3.79s/it]Running Inference:  80%|███████▉  | 159/200 [09:06<02:35,  3.80s/it]Running Inference:  80%|████████  | 160/200 [09:10<02:36,  3.91s/it]Running Inference:  80%|████████  | 161/200 [09:13<02:11,  3.38s/it]Running Inference:  81%|████████  | 162/200 [09:17<02:17,  3.62s/it]Running Inference:  82%|████████▏ | 163/200 [09:21<02:19,  3.77s/it]Running Inference:  82%|████████▏ | 164/200 [09:23<02:01,  3.38s/it]Running Inference:  82%|████████▎ | 165/200 [09:27<02:06,  3.60s/it]Running Inference:  83%|████████▎ | 166/200 [09:32<02:08,  3.77s/it]Running Inference:  84%|████████▎ | 167/200 [09:35<02:05,  3.79s/it]Running Inference:  84%|████████▍ | 168/200 [09:40<02:05,  3.91s/it]Running Inference:  84%|████████▍ | 169/200 [09:44<02:03,  3.99s/it]Running Inference:  85%|████████▌ | 170/200 [09:48<02:01,  4.04s/it]Running Inference:  86%|████████▌ | 171/200 [09:52<01:58,  4.07s/it]Running Inference:  86%|████████▌ | 172/200 [09:56<01:54,  4.09s/it]Running Inference:  86%|████████▋ | 173/200 [10:00<01:45,  3.91s/it]Running Inference:  87%|████████▋ | 174/200 [10:04<01:43,  3.98s/it]Running Inference:  88%|████████▊ | 175/200 [10:08<01:40,  4.04s/it]Running Inference:  88%|████████▊ | 176/200 [10:12<01:38,  4.10s/it]Running Inference:  88%|████████▊ | 177/200 [10:16<01:34,  4.11s/it]Running Inference:  89%|████████▉ | 178/200 [10:21<01:30,  4.13s/it]Running Inference:  90%|████████▉ | 179/200 [10:23<01:14,  3.54s/it]Running Inference:  90%|█████████ | 180/200 [10:25<01:00,  3.04s/it]Running Inference:  90%|█████████ | 181/200 [10:27<00:53,  2.79s/it]Running Inference:  91%|█████████ | 182/200 [10:30<00:53,  2.97s/it]Running Inference:  92%|█████████▏| 183/200 [10:34<00:56,  3.31s/it]Running Inference:  92%|█████████▏| 184/200 [10:39<00:57,  3.57s/it]Running Inference:  92%|█████████▎| 185/200 [10:41<00:47,  3.15s/it]Running Inference:  93%|█████████▎| 186/200 [10:45<00:48,  3.45s/it]Running Inference:  94%|█████████▎| 187/200 [10:47<00:40,  3.10s/it]Running Inference:  94%|█████████▍| 188/200 [10:49<00:32,  2.68s/it]Running Inference:  94%|█████████▍| 189/200 [10:53<00:34,  3.15s/it]Running Inference:  95%|█████████▌| 190/200 [10:55<00:27,  2.79s/it]Running Inference:  96%|█████████▌| 191/200 [10:59<00:28,  3.20s/it]Running Inference:  96%|█████████▌| 192/200 [11:03<00:27,  3.49s/it]Running Inference:  96%|█████████▋| 193/200 [11:08<00:25,  3.70s/it]Running Inference:  97%|█████████▋| 194/200 [11:12<00:23,  3.84s/it]Running Inference:  98%|█████████▊| 195/200 [11:16<00:19,  3.96s/it]Running Inference:  98%|█████████▊| 196/200 [11:18<00:13,  3.50s/it]Running Inference:  98%|█████████▊| 197/200 [11:21<00:10,  3.35s/it]Running Inference:  99%|█████████▉| 198/200 [11:24<00:06,  3.07s/it]Running Inference: 100%|█████████▉| 199/200 [11:27<00:03,  3.17s/it]Running Inference: 100%|██████████| 200/200 [11:30<00:00,  3.18s/it]Running Inference: 100%|██████████| 200/200 [11:30<00:00,  3.45s/it]
2025-12-13 19:57:40,404 - INFO - Inference completed.
2025-12-13 19:57:40,413 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-13 19:57:40,413 - INFO - Calculating metrics for dataset: longbench
2025-12-13 19:57:40,421 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-13 19:57:40,421 - INFO - Metrics:
12.05
2025-12-13 19:57:40,422 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=musique, ratio=0.3) on GPU 2

----------------------------------------
Task: musique | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 19:57:46,768 - INFO - Set deterministic seeds to 42
2025-12-13 19:57:46,768 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "musique",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 19:57:46,768 - INFO - Starting evaluation run...
2025-12-13 19:57:46,768 - INFO - Output directory set to: longbenchresult
2025-12-13 19:57:46,768 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-13 19:57:46,768 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 19:57:46,768 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 19:57:46,768 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.86it/s]
Device set to use cuda:0
2025-12-13 19:58:00,052 - INFO - Model pipeline loaded.
2025-12-13 19:58:00,052 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: musique)
2025-12-13 19:58:04,002 - INFO - Dataset loaded with 200 entries.
2025-12-13 19:58:04,002 - INFO - Dataset processed with 200 entries.
2025-12-13 19:58:04,045 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<16:31,  4.98s/it]Running Inference:   1%|          | 2/200 [00:08<14:00,  4.24s/it]Running Inference:   2%|▏         | 3/200 [00:12<13:41,  4.17s/it]Running Inference:   2%|▏         | 4/200 [00:17<13:49,  4.23s/it]Running Inference:   2%|▎         | 5/200 [00:21<13:42,  4.22s/it]Running Inference:   3%|▎         | 6/200 [00:25<13:34,  4.20s/it]Running Inference:   4%|▎         | 7/200 [00:29<13:36,  4.23s/it]Running Inference:   4%|▍         | 8/200 [00:32<11:34,  3.62s/it]Running Inference:   4%|▍         | 9/200 [00:36<12:07,  3.81s/it]Running Inference:   5%|▌         | 10/200 [00:40<12:28,  3.94s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:42<10:26,  3.31s/it]Running Inference:   6%|▌         | 12/200 [00:46<11:13,  3.58s/it]Running Inference:   6%|▋         | 13/200 [00:49<10:15,  3.29s/it]Running Inference:   7%|▋         | 14/200 [00:51<09:38,  3.11s/it]Running Inference:   8%|▊         | 15/200 [00:56<10:42,  3.47s/it]Running Inference:   8%|▊         | 16/200 [01:00<11:15,  3.67s/it]Running Inference:   8%|▊         | 17/200 [01:04<11:43,  3.84s/it]Running Inference:   9%|▉         | 18/200 [01:08<11:53,  3.92s/it]Running Inference:  10%|▉         | 19/200 [01:12<11:42,  3.88s/it]Running Inference:  10%|█         | 20/200 [01:16<11:55,  3.97s/it]Running Inference:  10%|█         | 21/200 [01:19<10:31,  3.53s/it]Running Inference:  11%|█         | 22/200 [01:23<10:57,  3.69s/it]Running Inference:  12%|█▏        | 23/200 [01:25<09:32,  3.23s/it]Running Inference:  12%|█▏        | 24/200 [01:26<07:58,  2.72s/it]Running Inference:  12%|█▎        | 25/200 [01:30<08:47,  3.01s/it]Running Inference:  13%|█▎        | 26/200 [01:34<09:48,  3.38s/it]Running Inference:  14%|█▎        | 27/200 [01:39<10:30,  3.65s/it]Running Inference:  14%|█▍        | 28/200 [01:43<10:59,  3.83s/it]Running Inference:  14%|█▍        | 29/200 [01:47<11:14,  3.95s/it]Running Inference:  15%|█▌        | 30/200 [01:49<09:43,  3.43s/it]Running Inference:  16%|█▌        | 31/200 [01:52<08:41,  3.09s/it]Running Inference:  16%|█▌        | 32/200 [01:55<09:11,  3.28s/it]Running Inference:  16%|█▋        | 33/200 [02:00<09:54,  3.56s/it]Running Inference:  17%|█▋        | 34/200 [02:04<10:16,  3.71s/it]Running Inference:  18%|█▊        | 35/200 [02:07<09:35,  3.49s/it]Running Inference:  18%|█▊        | 36/200 [02:09<08:36,  3.15s/it]Running Inference:  18%|█▊        | 37/200 [02:13<09:06,  3.35s/it]Running Inference:  19%|█▉        | 38/200 [02:17<09:35,  3.55s/it]Running Inference:  20%|█▉        | 39/200 [02:19<08:23,  3.13s/it]Running Inference:  20%|██        | 40/200 [02:23<09:03,  3.40s/it]Running Inference:  20%|██        | 41/200 [02:27<09:36,  3.63s/it]Running Inference:  21%|██        | 42/200 [02:30<08:49,  3.35s/it]Running Inference:  22%|██▏       | 43/200 [02:33<08:48,  3.36s/it]Running Inference:  22%|██▏       | 44/200 [02:38<09:27,  3.64s/it]Running Inference:  22%|██▎       | 45/200 [02:40<08:23,  3.25s/it]Running Inference:  23%|██▎       | 46/200 [02:44<09:07,  3.55s/it]Running Inference:  24%|██▎       | 47/200 [02:47<08:44,  3.43s/it]Running Inference:  24%|██▍       | 48/200 [02:51<09:16,  3.66s/it]Running Inference:  24%|██▍       | 49/200 [02:55<09:08,  3.63s/it]Running Inference:  25%|██▌       | 50/200 [02:59<09:32,  3.82s/it]Running Inference:  26%|██▌       | 51/200 [03:04<09:47,  3.94s/it]Running Inference:  26%|██▌       | 52/200 [03:08<09:58,  4.04s/it]Running Inference:  26%|██▋       | 53/200 [03:12<10:00,  4.09s/it]Running Inference:  27%|██▋       | 54/200 [03:16<10:02,  4.12s/it]Running Inference:  28%|██▊       | 55/200 [03:19<08:48,  3.65s/it]Running Inference:  28%|██▊       | 56/200 [03:23<09:11,  3.83s/it]Running Inference:  28%|██▊       | 57/200 [03:27<09:28,  3.98s/it]Running Inference:  29%|██▉       | 58/200 [03:29<07:49,  3.31s/it]Running Inference:  30%|██▉       | 59/200 [03:31<06:59,  2.97s/it]Running Inference:  30%|███       | 60/200 [03:35<07:47,  3.34s/it]Running Inference:  30%|███       | 61/200 [03:39<07:36,  3.28s/it]Running Inference:  31%|███       | 62/200 [03:41<07:12,  3.14s/it]Running Inference:  32%|███▏      | 63/200 [03:44<06:50,  3.00s/it]Running Inference:  32%|███▏      | 64/200 [03:46<06:20,  2.80s/it]Running Inference:  32%|███▎      | 65/200 [03:50<06:34,  2.92s/it]Running Inference:  33%|███▎      | 66/200 [03:54<07:25,  3.33s/it]Running Inference:  34%|███▎      | 67/200 [03:56<06:39,  3.00s/it]Running Inference:  34%|███▍      | 68/200 [04:00<07:05,  3.22s/it]Running Inference:  34%|███▍      | 69/200 [04:03<06:55,  3.17s/it]Running Inference:  35%|███▌      | 70/200 [04:07<07:32,  3.48s/it]Running Inference:  36%|███▌      | 71/200 [04:11<07:58,  3.71s/it]Running Inference:  36%|███▌      | 72/200 [04:14<07:04,  3.32s/it]Running Inference:  36%|███▋      | 73/200 [04:17<07:00,  3.31s/it]Running Inference:  37%|███▋      | 74/200 [04:21<07:33,  3.60s/it]Running Inference:  38%|███▊      | 75/200 [04:26<07:57,  3.82s/it]Running Inference:  38%|███▊      | 76/200 [04:30<08:08,  3.94s/it]Running Inference:  38%|███▊      | 77/200 [04:32<06:39,  3.25s/it]Running Inference:  39%|███▉      | 78/200 [04:34<05:57,  2.93s/it]Running Inference:  40%|███▉      | 79/200 [04:36<05:46,  2.87s/it]Running Inference:  40%|████      | 80/200 [04:39<05:21,  2.68s/it]Running Inference:  40%|████      | 81/200 [04:41<05:08,  2.59s/it]Running Inference:  41%|████      | 82/200 [04:45<06:07,  3.11s/it]Running Inference:  42%|████▏     | 83/200 [04:49<06:23,  3.27s/it]Running Inference:  42%|████▏     | 84/200 [04:53<06:49,  3.53s/it]Running Inference:  42%|████▎     | 85/200 [04:57<07:09,  3.74s/it]Running Inference:  43%|████▎     | 86/200 [05:02<07:20,  3.86s/it]Running Inference:  44%|████▎     | 87/200 [05:05<07:03,  3.74s/it]Running Inference:  44%|████▍     | 88/200 [05:09<07:14,  3.88s/it]Running Inference:  44%|████▍     | 89/200 [05:11<06:13,  3.37s/it]Running Inference:  45%|████▌     | 90/200 [05:16<06:38,  3.62s/it]Running Inference:  46%|████▌     | 91/200 [05:20<06:51,  3.77s/it]Running Inference:  46%|████▌     | 92/200 [05:22<06:00,  3.34s/it]Running Inference:  46%|████▋     | 93/200 [05:24<05:21,  3.00s/it]Running Inference:  47%|████▋     | 94/200 [05:28<05:55,  3.35s/it]Running Inference:  48%|████▊     | 95/200 [05:31<05:31,  3.15s/it]Running Inference:  48%|████▊     | 96/200 [05:35<05:34,  3.22s/it]Running Inference:  48%|████▊     | 97/200 [05:37<05:04,  2.96s/it]Running Inference:  49%|████▉     | 98/200 [05:39<04:41,  2.76s/it]Running Inference:  50%|████▉     | 99/200 [05:41<04:23,  2.61s/it]Running Inference:  50%|█████     | 100/200 [05:45<05:01,  3.02s/it]Running Inference:  50%|█████     | 101/200 [05:48<04:49,  2.93s/it]Running Inference:  51%|█████     | 102/200 [05:52<05:24,  3.32s/it]Running Inference:  52%|█████▏    | 103/200 [05:56<05:45,  3.56s/it]Running Inference:  52%|█████▏    | 104/200 [05:59<05:07,  3.20s/it]Running Inference:  52%|█████▎    | 105/200 [06:03<05:35,  3.53s/it]Running Inference:  53%|█████▎    | 106/200 [06:07<05:43,  3.65s/it]Running Inference:  54%|█████▎    | 107/200 [06:11<05:50,  3.77s/it]Running Inference:  54%|█████▍    | 108/200 [06:13<05:02,  3.29s/it]Running Inference:  55%|█████▍    | 109/200 [06:18<05:27,  3.59s/it]Running Inference:  55%|█████▌    | 110/200 [06:22<05:40,  3.78s/it]Running Inference:  56%|█████▌    | 111/200 [06:25<05:10,  3.49s/it]Running Inference:  56%|█████▌    | 112/200 [06:27<04:35,  3.13s/it]Running Inference:  56%|█████▋    | 113/200 [06:31<04:59,  3.45s/it]Running Inference:  57%|█████▋    | 114/200 [06:35<05:15,  3.66s/it]Running Inference:  57%|█████▊    | 115/200 [06:39<05:24,  3.81s/it]Running Inference:  58%|█████▊    | 116/200 [06:42<04:37,  3.31s/it]Running Inference:  58%|█████▊    | 117/200 [06:45<04:31,  3.27s/it]Running Inference:  59%|█████▉    | 118/200 [06:49<04:50,  3.54s/it]Running Inference:  60%|█████▉    | 119/200 [06:51<04:19,  3.21s/it]Running Inference:  60%|██████    | 120/200 [06:54<03:55,  2.95s/it]Running Inference:  60%|██████    | 121/200 [06:56<03:35,  2.73s/it]Running Inference:  61%|██████    | 122/200 [06:58<03:19,  2.56s/it]Running Inference:  62%|██████▏   | 123/200 [07:02<03:43,  2.90s/it]Running Inference:  62%|██████▏   | 124/200 [07:05<03:54,  3.09s/it]Running Inference:  62%|██████▎   | 125/200 [07:07<03:14,  2.59s/it]Running Inference:  63%|██████▎   | 126/200 [07:11<03:47,  3.08s/it]Running Inference:  64%|██████▎   | 127/200 [07:13<03:26,  2.82s/it]Running Inference:  64%|██████▍   | 128/200 [07:17<03:52,  3.23s/it]Running Inference:  64%|██████▍   | 129/200 [07:20<03:33,  3.01s/it]Running Inference:  65%|██████▌   | 130/200 [07:24<03:55,  3.37s/it]Running Inference:  66%|██████▌   | 131/200 [07:26<03:29,  3.03s/it]Running Inference:  66%|██████▌   | 132/200 [07:29<03:12,  2.83s/it]Running Inference:  66%|██████▋   | 133/200 [07:31<02:56,  2.64s/it]Running Inference:  67%|██████▋   | 134/200 [07:32<02:26,  2.22s/it]Running Inference:  68%|██████▊   | 135/200 [07:36<03:05,  2.85s/it]Running Inference:  68%|██████▊   | 136/200 [07:41<03:29,  3.27s/it]Running Inference:  68%|██████▊   | 137/200 [07:45<03:44,  3.56s/it]Running Inference:  69%|██████▉   | 138/200 [07:49<03:51,  3.73s/it]Running Inference:  70%|██████▉   | 139/200 [07:53<03:50,  3.77s/it]Running Inference:  70%|███████   | 140/200 [07:57<03:53,  3.89s/it]Running Inference:  70%|███████   | 141/200 [08:01<03:55,  3.99s/it]Running Inference:  71%|███████   | 142/200 [08:04<03:37,  3.75s/it]Running Inference:  72%|███████▏  | 143/200 [08:09<03:41,  3.88s/it]Running Inference:  72%|███████▏  | 144/200 [08:11<03:11,  3.41s/it]Running Inference:  72%|███████▎  | 145/200 [08:15<03:19,  3.62s/it]Running Inference:  73%|███████▎  | 146/200 [08:17<02:52,  3.20s/it]Running Inference:  74%|███████▎  | 147/200 [08:21<03:05,  3.51s/it]Running Inference:  74%|███████▍  | 148/200 [08:26<03:13,  3.72s/it]Running Inference:  74%|███████▍  | 149/200 [08:30<03:14,  3.81s/it]Running Inference:  75%|███████▌  | 150/200 [08:33<03:03,  3.67s/it]Running Inference:  76%|███████▌  | 151/200 [08:35<02:37,  3.22s/it]Running Inference:  76%|███████▌  | 152/200 [08:38<02:24,  3.02s/it]Running Inference:  76%|███████▋  | 153/200 [08:42<02:39,  3.39s/it]Running Inference:  77%|███████▋  | 154/200 [08:46<02:46,  3.63s/it]Running Inference:  78%|███████▊  | 155/200 [08:48<02:23,  3.20s/it]Running Inference:  78%|███████▊  | 156/200 [08:52<02:27,  3.34s/it]Running Inference:  78%|███████▊  | 157/200 [08:56<02:35,  3.62s/it]Running Inference:  79%|███████▉  | 158/200 [09:00<02:26,  3.50s/it]Running Inference:  80%|███████▉  | 159/200 [09:04<02:31,  3.70s/it]Running Inference:  80%|████████  | 160/200 [09:08<02:33,  3.84s/it]Running Inference:  80%|████████  | 161/200 [09:10<02:13,  3.43s/it]Running Inference:  81%|████████  | 162/200 [09:15<02:19,  3.67s/it]Running Inference:  82%|████████▏ | 163/200 [09:19<02:21,  3.82s/it]Running Inference:  82%|████████▏ | 164/200 [09:21<02:02,  3.41s/it]Running Inference:  82%|████████▎ | 165/200 [09:25<02:07,  3.63s/it]Running Inference:  83%|████████▎ | 166/200 [09:30<02:09,  3.79s/it]Running Inference:  84%|████████▎ | 167/200 [09:33<02:02,  3.72s/it]Running Inference:  84%|████████▍ | 168/200 [09:37<02:03,  3.87s/it]Running Inference:  84%|████████▍ | 169/200 [09:42<02:03,  3.97s/it]Running Inference:  85%|████████▌ | 170/200 [09:46<02:01,  4.04s/it]Running Inference:  86%|████████▌ | 171/200 [09:50<01:58,  4.07s/it]Running Inference:  86%|████████▌ | 172/200 [09:54<01:54,  4.10s/it]Running Inference:  86%|████████▋ | 173/200 [09:58<01:45,  3.92s/it]Running Inference:  87%|████████▋ | 174/200 [10:00<01:32,  3.56s/it]Running Inference:  88%|████████▊ | 175/200 [10:02<01:18,  3.13s/it]Running Inference:  88%|████████▊ | 176/200 [10:05<01:08,  2.85s/it]Running Inference:  88%|████████▊ | 177/200 [10:09<01:14,  3.24s/it]Running Inference:  89%|████████▉ | 178/200 [10:13<01:15,  3.42s/it]Running Inference:  90%|████████▉ | 179/200 [10:15<01:03,  3.04s/it]Running Inference:  90%|█████████ | 180/200 [10:17<00:53,  2.69s/it]Running Inference:  90%|█████████ | 181/200 [10:19<00:48,  2.55s/it]Running Inference:  91%|█████████ | 182/200 [10:23<00:54,  3.05s/it]Running Inference:  92%|█████████▏| 183/200 [10:25<00:47,  2.79s/it]Running Inference:  92%|█████████▏| 184/200 [10:29<00:51,  3.21s/it]Running Inference:  92%|█████████▎| 185/200 [10:34<00:52,  3.50s/it]Running Inference:  93%|█████████▎| 186/200 [10:38<00:51,  3.71s/it]Running Inference:  94%|█████████▎| 187/200 [10:40<00:42,  3.27s/it]Running Inference:  94%|█████████▍| 188/200 [10:42<00:33,  2.77s/it]Running Inference:  94%|█████████▍| 189/200 [10:45<00:31,  2.87s/it]Running Inference:  95%|█████████▌| 190/200 [10:49<00:31,  3.18s/it]Running Inference:  96%|█████████▌| 191/200 [10:53<00:31,  3.49s/it]Running Inference:  96%|█████████▌| 192/200 [10:57<00:29,  3.70s/it]Running Inference:  96%|█████████▋| 193/200 [10:59<00:22,  3.27s/it]Running Inference:  97%|█████████▋| 194/200 [11:03<00:21,  3.55s/it]Running Inference:  98%|█████████▊| 195/200 [11:08<00:18,  3.76s/it]Running Inference:  98%|█████████▊| 196/200 [11:10<00:13,  3.36s/it]Running Inference:  98%|█████████▊| 197/200 [11:13<00:09,  3.26s/it]Running Inference:  99%|█████████▉| 198/200 [11:17<00:06,  3.48s/it]Running Inference: 100%|█████████▉| 199/200 [11:21<00:03,  3.46s/it]Running Inference: 100%|██████████| 200/200 [11:24<00:00,  3.38s/it]Running Inference: 100%|██████████| 200/200 [11:24<00:00,  3.42s/it]
2025-12-13 20:09:28,373 - INFO - Inference completed.
2025-12-13 20:09:28,381 - INFO - Results saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-13 20:09:28,382 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:09:28,389 - INFO - Metrics saved to longbenchresult/longbench__musique__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-13 20:09:28,389 - INFO - Metrics:
9.82
2025-12-13 20:09:28,391 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=musique, ratio=0.5) on GPU 2


========================================
LongBench Task: narrativeqa
========================================
----------------------------------------
Task: narrativeqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:09:34,874 - INFO - Set deterministic seeds to 42
2025-12-13 20:09:34,874 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:09:34,874 - INFO - Starting evaluation run...
2025-12-13 20:09:34,874 - INFO - Output directory set to: longbenchresult
2025-12-13 20:09:34,874 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 20:09:34,874 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:09:34,874 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:09:34,874 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 100.19it/s]
Device set to use cuda:0
2025-12-13 20:10:16,024 - INFO - Model pipeline loaded.
2025-12-13 20:10:16,025 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:10:20,601 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:10:20,601 - INFO - Dataset processed with 200 entries.
2025-12-13 20:10:20,625 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:21<06:40, 21.08s/it]Running Inference:  10%|█         | 2/20 [00:30<04:20, 14.48s/it]Running Inference:  15%|█▌        | 3/20 [00:53<05:07, 18.10s/it]Running Inference:  20%|██        | 4/20 [01:20<05:48, 21.77s/it]Running Inference:  25%|██▌       | 5/20 [01:52<06:22, 25.52s/it]Running Inference:  30%|███       | 6/20 [02:23<06:21, 27.22s/it]Running Inference:  35%|███▌      | 7/20 [02:45<05:32, 25.58s/it]Running Inference:  40%|████      | 8/20 [03:03<04:36, 23.01s/it]Running Inference:  45%|████▌     | 9/20 [03:11<03:22, 18.37s/it]Running Inference:  50%|█████     | 10/20 [03:36<03:24, 20.41s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [03:52<02:52, 19.12s/it]Running Inference:  60%|██████    | 12/20 [04:09<02:27, 18.40s/it]Running Inference:  65%|██████▌   | 13/20 [04:18<01:49, 15.69s/it]Running Inference:  70%|███████   | 14/20 [04:25<01:18, 13.04s/it]Running Inference:  75%|███████▌  | 15/20 [04:33<00:57, 11.49s/it]Running Inference:  80%|████████  | 16/20 [04:49<00:51, 12.83s/it]Running Inference:  85%|████████▌ | 17/20 [05:10<00:46, 15.39s/it]Running Inference:  90%|█████████ | 18/20 [05:16<00:25, 12.58s/it]Running Inference:  95%|█████████▌| 19/20 [05:22<00:10, 10.49s/it]Running Inference: 100%|██████████| 20/20 [05:33<00:00, 10.73s/it]Running Inference: 100%|██████████| 20/20 [05:33<00:00, 16.69s/it]
2025-12-13 20:15:54,396 - INFO - Inference completed.
2025-12-13 20:15:54,404 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 20:15:54,404 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:15:54,409 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 20:15:54,409 - INFO - Metrics:
15.88
2025-12-13 20:15:54,411 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=narrativeqa, ratio=0.1) on GPU 2

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:16:00,886 - INFO - Set deterministic seeds to 42
2025-12-13 20:16:00,886 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:16:00,886 - INFO - Starting evaluation run...
2025-12-13 20:16:00,886 - INFO - Output directory set to: longbenchresult
2025-12-13 20:16:00,886 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 20:16:00,886 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:16:00,886 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:16:00,886 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.72it/s]
Device set to use cuda:0
2025-12-13 20:16:14,763 - INFO - Model pipeline loaded.
2025-12-13 20:16:14,763 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:16:18,488 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:16:18,488 - INFO - Dataset processed with 200 entries.
2025-12-13 20:16:18,513 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:21<06:57, 21.99s/it]Running Inference:  10%|█         | 2/20 [00:31<04:26, 14.78s/it]Running Inference:  15%|█▌        | 3/20 [01:05<06:39, 23.52s/it]Running Inference:  20%|██        | 4/20 [01:38<07:14, 27.14s/it]Running Inference:  25%|██▌       | 5/20 [02:04<06:39, 26.63s/it]Running Inference:  30%|███       | 6/20 [02:33<06:25, 27.51s/it]Running Inference:  35%|███▌      | 7/20 [03:04<06:13, 28.71s/it]Running Inference:  40%|████      | 8/20 [03:21<04:59, 24.92s/it]Running Inference:  45%|████▌     | 9/20 [03:35<03:56, 21.48s/it]Running Inference:  50%|█████     | 10/20 [04:04<04:00, 24.01s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [04:14<02:57, 19.67s/it]Running Inference:  60%|██████    | 12/20 [04:35<02:39, 19.94s/it]Running Inference:  65%|██████▌   | 13/20 [04:55<02:20, 20.11s/it]Running Inference:  70%|███████   | 14/20 [05:02<01:36, 16.07s/it]Running Inference:  75%|███████▌  | 15/20 [05:10<01:07, 13.52s/it]Running Inference:  80%|████████  | 16/20 [05:15<00:44, 11.18s/it]Running Inference:  85%|████████▌ | 17/20 [05:48<00:52, 17.64s/it]Running Inference:  90%|█████████ | 18/20 [05:55<00:28, 14.40s/it]Running Inference:  95%|█████████▌| 19/20 [06:00<00:11, 11.67s/it]Running Inference: 100%|██████████| 20/20 [06:11<00:00, 11.29s/it]Running Inference: 100%|██████████| 20/20 [06:11<00:00, 18.55s/it]
2025-12-13 20:22:29,589 - INFO - Inference completed.
2025-12-13 20:22:29,597 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 20:22:29,598 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:22:29,603 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 20:22:29,603 - INFO - Metrics:
15.1
2025-12-13 20:22:29,604 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=narrativeqa, ratio=0.2) on GPU 2

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:22:36,069 - INFO - Set deterministic seeds to 42
2025-12-13 20:22:36,070 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:22:36,070 - INFO - Starting evaluation run...
2025-12-13 20:22:36,070 - INFO - Output directory set to: longbenchresult
2025-12-13 20:22:36,070 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 20:22:36,070 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:22:36,070 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:22:36,070 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.50it/s]
Device set to use cuda:0
2025-12-13 20:22:48,334 - INFO - Model pipeline loaded.
2025-12-13 20:22:48,334 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:22:53,794 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:22:53,794 - INFO - Dataset processed with 200 entries.
2025-12-13 20:22:53,818 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:14<04:26, 14.01s/it]Running Inference:  10%|█         | 2/20 [00:24<03:36, 12.01s/it]Running Inference:  15%|█▌        | 3/20 [00:40<03:50, 13.57s/it]Running Inference:  20%|██        | 4/20 [01:11<05:32, 20.76s/it]Running Inference:  25%|██▌       | 5/20 [01:44<06:16, 25.09s/it]Running Inference:  30%|███       | 6/20 [02:13<06:08, 26.32s/it]Running Inference:  35%|███▌      | 7/20 [02:39<05:40, 26.20s/it]Running Inference:  40%|████      | 8/20 [02:56<04:38, 23.22s/it]Running Inference:  45%|████▌     | 9/20 [03:11<03:47, 20.65s/it]Running Inference:  50%|█████     | 10/20 [03:37<03:43, 22.35s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [03:48<02:49, 18.82s/it]Running Inference:  60%|██████    | 12/20 [04:03<02:21, 17.64s/it]Running Inference:  65%|██████▌   | 13/20 [04:28<02:19, 19.99s/it]Running Inference:  70%|███████   | 14/20 [04:35<01:36, 16.02s/it]Running Inference:  75%|███████▌  | 15/20 [04:43<01:08, 13.72s/it]Running Inference:  80%|████████  | 16/20 [04:55<00:52, 13.25s/it]Running Inference:  85%|████████▌ | 17/20 [05:15<00:45, 15.23s/it]Running Inference:  90%|█████████ | 18/20 [05:22<00:25, 12.72s/it]Running Inference:  95%|█████████▌| 19/20 [05:27<00:10, 10.39s/it]Running Inference: 100%|██████████| 20/20 [05:46<00:00, 13.02s/it]Running Inference: 100%|██████████| 20/20 [05:46<00:00, 17.33s/it]
2025-12-13 20:28:40,466 - INFO - Inference completed.
2025-12-13 20:28:40,474 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-13 20:28:40,474 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:28:40,480 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-13 20:28:40,480 - INFO - Metrics:
14.31
2025-12-13 20:28:40,481 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=narrativeqa, ratio=0.3) on GPU 2

----------------------------------------
Task: narrativeqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:28:46,949 - INFO - Set deterministic seeds to 42
2025-12-13 20:28:46,950 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "narrativeqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:28:46,950 - INFO - Starting evaluation run...
2025-12-13 20:28:46,950 - INFO - Output directory set to: longbenchresult
2025-12-13 20:28:46,950 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-13 20:28:46,950 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:28:46,950 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:28:46,950 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.68it/s]
Device set to use cuda:0
2025-12-13 20:29:01,592 - INFO - Model pipeline loaded.
2025-12-13 20:29:01,592 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: narrativeqa)
2025-12-13 20:29:06,103 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:29:06,103 - INFO - Dataset processed with 200 entries.
2025-12-13 20:29:06,127 - INFO - Starting inference...
Running Inference:   0%|          | 0/20 [00:00<?, ?it/s]Running Inference:   5%|▌         | 1/20 [00:20<06:25, 20.31s/it]Running Inference:  10%|█         | 2/20 [00:40<06:04, 20.24s/it]Running Inference:  15%|█▌        | 3/20 [01:02<05:57, 21.06s/it]Running Inference:  20%|██        | 4/20 [01:26<05:57, 22.36s/it]Running Inference:  25%|██▌       | 5/20 [01:59<06:32, 26.15s/it]Running Inference:  30%|███       | 6/20 [02:22<05:50, 25.02s/it]Running Inference:  35%|███▌      | 7/20 [02:41<05:00, 23.13s/it]Running Inference:  40%|████      | 8/20 [02:57<04:07, 20.63s/it]Running Inference:  45%|████▌     | 9/20 [03:03<02:58, 16.18s/it]Running Inference:  50%|█████     | 10/20 [03:26<03:02, 18.22s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  55%|█████▌    | 11/20 [03:31<02:07, 14.11s/it]Running Inference:  60%|██████    | 12/20 [03:57<02:23, 18.00s/it]Running Inference:  65%|██████▌   | 13/20 [04:11<01:57, 16.77s/it]Running Inference:  70%|███████   | 14/20 [04:36<01:53, 19.00s/it]Running Inference:  75%|███████▌  | 15/20 [04:43<01:17, 15.53s/it]Running Inference:  80%|████████  | 16/20 [04:51<00:52, 13.13s/it]Running Inference:  85%|████████▌ | 17/20 [05:10<00:44, 14.93s/it]Running Inference:  90%|█████████ | 18/20 [05:19<00:26, 13.20s/it]Running Inference:  95%|█████████▌| 19/20 [05:24<00:10, 10.74s/it]Running Inference: 100%|██████████| 20/20 [05:39<00:00, 12.08s/it]Running Inference: 100%|██████████| 20/20 [05:39<00:00, 16.98s/it]
2025-12-13 20:34:45,759 - INFO - Inference completed.
2025-12-13 20:34:45,767 - INFO - Results saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-13 20:34:45,767 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:34:45,773 - INFO - Metrics saved to longbenchresult/longbench__narrativeqa__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-13 20:34:45,773 - INFO - Metrics:
12.77
2025-12-13 20:34:45,774 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=narrativeqa, ratio=0.5) on GPU 2


========================================
LongBench Task: qasper
========================================
----------------------------------------
Task: qasper | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:34:52,083 - INFO - Set deterministic seeds to 42
2025-12-13 20:34:52,083 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:34:52,083 - INFO - Starting evaluation run...
2025-12-13 20:34:52,083 - INFO - Output directory set to: longbenchresult
2025-12-13 20:34:52,083 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 20:34:52,083 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:34:52,083 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:34:52,083 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.46it/s]
Device set to use cuda:0
2025-12-13 20:35:07,182 - INFO - Model pipeline loaded.
2025-12-13 20:35:07,182 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:35:24,425 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:35:24,425 - INFO - Dataset processed with 200 entries.
2025-12-13 20:35:24,439 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:03<08:30,  3.47s/it]Running Inference:   1%|▏         | 2/148 [00:03<04:03,  1.67s/it]Running Inference:   2%|▏         | 3/148 [00:04<03:14,  1.34s/it]Running Inference:   3%|▎         | 4/148 [00:08<05:06,  2.13s/it]Running Inference:   3%|▎         | 5/148 [00:11<05:46,  2.43s/it]Running Inference:   4%|▍         | 6/148 [00:14<06:16,  2.65s/it]Running Inference:   5%|▍         | 7/148 [00:15<04:54,  2.09s/it]Running Inference:   5%|▌         | 8/148 [00:16<04:06,  1.76s/it]Running Inference:   6%|▌         | 9/148 [00:18<04:30,  1.94s/it]Running Inference:   7%|▋         | 10/148 [00:19<04:03,  1.77s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:22<04:36,  2.02s/it]Running Inference:   8%|▊         | 12/148 [00:24<04:39,  2.05s/it]Running Inference:   9%|▉         | 13/148 [00:26<04:39,  2.07s/it]Running Inference:   9%|▉         | 14/148 [00:29<04:54,  2.20s/it]Running Inference:  10%|█         | 15/148 [00:30<04:09,  1.87s/it]Running Inference:  11%|█         | 16/148 [00:35<06:36,  3.01s/it]Running Inference:  11%|█▏        | 17/148 [00:37<05:38,  2.58s/it]Running Inference:  12%|█▏        | 18/148 [00:39<05:14,  2.42s/it]Running Inference:  13%|█▎        | 19/148 [00:40<04:24,  2.05s/it]Running Inference:  14%|█▎        | 20/148 [00:41<03:43,  1.74s/it]Running Inference:  14%|█▍        | 21/148 [00:44<03:59,  1.89s/it]Running Inference:  15%|█▍        | 22/148 [00:47<04:55,  2.35s/it]Running Inference:  16%|█▌        | 23/148 [00:48<03:54,  1.87s/it]Running Inference:  16%|█▌        | 24/148 [00:49<03:28,  1.68s/it]Running Inference:  17%|█▋        | 25/148 [00:52<04:01,  1.96s/it]Running Inference:  18%|█▊        | 26/148 [00:58<06:32,  3.22s/it]Running Inference:  18%|█▊        | 27/148 [00:59<05:05,  2.52s/it]Running Inference:  19%|█▉        | 28/148 [00:59<04:01,  2.02s/it]Running Inference:  20%|█▉        | 29/148 [01:07<07:19,  3.70s/it]Running Inference:  20%|██        | 30/148 [01:08<05:48,  2.95s/it]Running Inference:  21%|██        | 31/148 [01:09<04:40,  2.39s/it]Running Inference:  22%|██▏       | 32/148 [01:11<04:23,  2.27s/it]Running Inference:  22%|██▏       | 33/148 [01:12<03:38,  1.90s/it]Running Inference:  23%|██▎       | 34/148 [01:14<03:21,  1.77s/it]Running Inference:  24%|██▎       | 35/148 [01:15<03:02,  1.61s/it]Running Inference:  24%|██▍       | 36/148 [01:22<05:57,  3.19s/it]Running Inference:  25%|██▌       | 37/148 [01:24<05:12,  2.81s/it]Running Inference:  26%|██▌       | 38/148 [01:25<03:59,  2.17s/it]Running Inference:  26%|██▋       | 39/148 [01:25<02:57,  1.63s/it]Running Inference:  27%|██▋       | 40/148 [01:26<02:40,  1.49s/it]Running Inference:  28%|██▊       | 41/148 [01:28<02:48,  1.57s/it]Running Inference:  28%|██▊       | 42/148 [01:29<02:44,  1.56s/it]Running Inference:  29%|██▉       | 43/148 [01:32<03:05,  1.76s/it]Running Inference:  30%|██▉       | 44/148 [01:34<03:18,  1.90s/it]Running Inference:  30%|███       | 45/148 [01:35<03:04,  1.79s/it]Running Inference:  31%|███       | 46/148 [01:39<03:43,  2.19s/it]Running Inference:  32%|███▏      | 47/148 [01:41<03:34,  2.12s/it]Running Inference:  32%|███▏      | 48/148 [01:41<02:50,  1.71s/it]Running Inference:  33%|███▎      | 49/148 [01:52<07:15,  4.40s/it]Running Inference:  34%|███▍      | 50/148 [01:53<05:41,  3.48s/it]Running Inference:  34%|███▍      | 51/148 [01:55<04:44,  2.93s/it]Running Inference:  35%|███▌      | 52/148 [01:56<03:50,  2.40s/it]Running Inference:  36%|███▌      | 53/148 [01:57<03:16,  2.06s/it]Running Inference:  36%|███▋      | 54/148 [01:58<02:46,  1.77s/it]Running Inference:  37%|███▋      | 55/148 [01:59<02:15,  1.46s/it]Running Inference:  38%|███▊      | 56/148 [02:01<02:15,  1.47s/it]Running Inference:  39%|███▊      | 57/148 [02:02<02:02,  1.35s/it]Running Inference:  39%|███▉      | 58/148 [02:03<01:59,  1.33s/it]Running Inference:  40%|███▉      | 59/148 [02:04<01:58,  1.33s/it]Running Inference:  41%|████      | 60/148 [02:06<02:01,  1.38s/it]Running Inference:  41%|████      | 61/148 [02:08<02:16,  1.57s/it]Running Inference:  42%|████▏     | 62/148 [02:09<02:04,  1.44s/it]Running Inference:  43%|████▎     | 63/148 [02:11<02:14,  1.59s/it]Running Inference:  43%|████▎     | 64/148 [02:14<02:56,  2.10s/it]Running Inference:  44%|████▍     | 65/148 [02:17<03:05,  2.24s/it]Running Inference:  45%|████▍     | 66/148 [02:18<02:27,  1.79s/it]Running Inference:  45%|████▌     | 67/148 [02:20<02:42,  2.01s/it]Running Inference:  46%|████▌     | 68/148 [02:21<02:10,  1.63s/it]Running Inference:  47%|████▋     | 69/148 [02:23<02:10,  1.66s/it]Running Inference:  47%|████▋     | 70/148 [02:25<02:24,  1.85s/it]Running Inference:  48%|████▊     | 71/148 [02:32<04:31,  3.53s/it]Running Inference:  49%|████▊     | 72/148 [02:35<04:19,  3.41s/it]Running Inference:  49%|████▉     | 73/148 [02:37<03:43,  2.99s/it]Running Inference:  50%|█████     | 74/148 [02:39<03:15,  2.65s/it]Running Inference:  51%|█████     | 75/148 [02:41<03:02,  2.50s/it]Running Inference:  51%|█████▏    | 76/148 [02:43<02:49,  2.35s/it]Running Inference:  52%|█████▏    | 77/148 [02:45<02:39,  2.25s/it]Running Inference:  53%|█████▎    | 78/148 [02:47<02:25,  2.08s/it]Running Inference:  53%|█████▎    | 79/148 [02:48<02:05,  1.82s/it]Running Inference:  54%|█████▍    | 80/148 [02:50<02:01,  1.79s/it]Running Inference:  55%|█████▍    | 81/148 [02:52<02:03,  1.84s/it]Running Inference:  55%|█████▌    | 82/148 [02:53<01:51,  1.68s/it]Running Inference:  56%|█████▌    | 83/148 [02:54<01:34,  1.46s/it]Running Inference:  57%|█████▋    | 84/148 [02:55<01:28,  1.38s/it]Running Inference:  57%|█████▋    | 85/148 [02:57<01:25,  1.36s/it]Running Inference:  58%|█████▊    | 86/148 [02:58<01:26,  1.39s/it]Running Inference:  59%|█████▉    | 87/148 [03:00<01:31,  1.50s/it]Running Inference:  59%|█████▉    | 88/148 [03:01<01:27,  1.46s/it]Running Inference:  60%|██████    | 89/148 [03:03<01:26,  1.46s/it]Running Inference:  61%|██████    | 90/148 [03:04<01:13,  1.26s/it]Running Inference:  61%|██████▏   | 91/148 [03:05<01:11,  1.26s/it]Running Inference:  62%|██████▏   | 92/148 [03:06<01:01,  1.11s/it]Running Inference:  63%|██████▎   | 93/148 [03:09<01:30,  1.64s/it]Running Inference:  64%|██████▎   | 94/148 [03:10<01:31,  1.69s/it]Running Inference:  64%|██████▍   | 95/148 [03:12<01:30,  1.70s/it]Running Inference:  65%|██████▍   | 96/148 [03:13<01:15,  1.45s/it]Running Inference:  66%|██████▌   | 97/148 [03:14<01:02,  1.22s/it]Running Inference:  66%|██████▌   | 98/148 [03:16<01:17,  1.55s/it]Running Inference:  67%|██████▋   | 99/148 [03:19<01:40,  2.06s/it]Running Inference:  68%|██████▊   | 100/148 [03:21<01:29,  1.86s/it]Running Inference:  68%|██████▊   | 101/148 [03:22<01:26,  1.85s/it]Running Inference:  69%|██████▉   | 102/148 [03:27<01:59,  2.60s/it]Running Inference:  70%|██████▉   | 103/148 [03:28<01:43,  2.31s/it]Running Inference:  70%|███████   | 104/148 [03:30<01:33,  2.12s/it]Running Inference:  71%|███████   | 105/148 [03:34<01:59,  2.77s/it]Running Inference:  72%|███████▏  | 106/148 [03:37<01:59,  2.85s/it]Running Inference:  72%|███████▏  | 107/148 [03:38<01:27,  2.13s/it]Running Inference:  73%|███████▎  | 108/148 [03:39<01:18,  1.97s/it]Running Inference:  74%|███████▎  | 109/148 [03:40<01:02,  1.59s/it]Running Inference:  74%|███████▍  | 110/148 [03:42<01:04,  1.69s/it]Running Inference:  75%|███████▌  | 111/148 [03:43<00:55,  1.50s/it]Running Inference:  76%|███████▌  | 112/148 [03:44<00:46,  1.28s/it]Running Inference:  76%|███████▋  | 113/148 [03:46<00:50,  1.44s/it]Running Inference:  77%|███████▋  | 114/148 [03:50<01:14,  2.18s/it]Running Inference:  78%|███████▊  | 115/148 [03:50<00:56,  1.71s/it]Running Inference:  78%|███████▊  | 116/148 [03:51<00:49,  1.56s/it]Running Inference:  79%|███████▉  | 117/148 [03:52<00:43,  1.41s/it]Running Inference:  80%|███████▉  | 118/148 [03:54<00:41,  1.40s/it]Running Inference:  80%|████████  | 119/148 [03:56<00:45,  1.56s/it]Running Inference:  81%|████████  | 120/148 [03:58<00:51,  1.84s/it]Running Inference:  82%|████████▏ | 121/148 [04:01<00:53,  1.98s/it]Running Inference:  82%|████████▏ | 122/148 [04:01<00:42,  1.64s/it]Running Inference:  83%|████████▎ | 123/148 [04:05<00:53,  2.15s/it]Running Inference:  84%|████████▍ | 124/148 [04:07<00:48,  2.04s/it]Running Inference:  84%|████████▍ | 125/148 [04:12<01:13,  3.21s/it]Running Inference:  85%|████████▌ | 126/148 [04:16<01:12,  3.28s/it]Running Inference:  86%|████████▌ | 127/148 [04:17<00:58,  2.78s/it]Running Inference:  86%|████████▋ | 128/148 [04:19<00:45,  2.28s/it]Running Inference:  87%|████████▋ | 129/148 [04:19<00:34,  1.82s/it]Running Inference:  88%|████████▊ | 130/148 [04:26<00:58,  3.24s/it]Running Inference:  89%|████████▊ | 131/148 [04:29<00:53,  3.12s/it]Running Inference:  89%|████████▉ | 132/148 [04:31<00:43,  2.74s/it]Running Inference:  90%|████████▉ | 133/148 [04:33<00:38,  2.56s/it]Running Inference:  91%|█████████ | 134/148 [04:41<01:00,  4.34s/it]Running Inference:  91%|█████████ | 135/148 [04:42<00:42,  3.29s/it]Running Inference:  92%|█████████▏| 136/148 [04:43<00:32,  2.67s/it]Running Inference:  93%|█████████▎| 137/148 [04:45<00:27,  2.51s/it]Running Inference:  93%|█████████▎| 138/148 [04:48<00:25,  2.57s/it]Running Inference:  94%|█████████▍| 139/148 [04:49<00:18,  2.03s/it]Running Inference:  95%|█████████▍| 140/148 [04:50<00:13,  1.66s/it]Running Inference:  95%|█████████▌| 141/148 [04:52<00:13,  1.91s/it]Running Inference:  96%|█████████▌| 142/148 [04:53<00:09,  1.61s/it]Running Inference:  97%|█████████▋| 143/148 [04:55<00:08,  1.63s/it]Running Inference:  97%|█████████▋| 144/148 [05:01<00:12,  3.12s/it]Running Inference:  98%|█████████▊| 145/148 [05:08<00:12,  4.19s/it]Running Inference:  99%|█████████▊| 146/148 [05:11<00:07,  3.75s/it]Running Inference:  99%|█████████▉| 147/148 [05:12<00:02,  2.96s/it]Running Inference: 100%|██████████| 148/148 [05:13<00:00,  2.38s/it]Running Inference: 100%|██████████| 148/148 [05:13<00:00,  2.12s/it]
2025-12-13 20:40:37,894 - INFO - Inference completed.
2025-12-13 20:40:37,903 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 20:40:37,903 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:40:37,914 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 20:40:37,914 - INFO - Metrics:
28.36
2025-12-13 20:40:37,916 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qasper, ratio=0.1) on GPU 2

----------------------------------------
Task: qasper | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:40:44,358 - INFO - Set deterministic seeds to 42
2025-12-13 20:40:44,359 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:40:44,359 - INFO - Starting evaluation run...
2025-12-13 20:40:44,359 - INFO - Output directory set to: longbenchresult
2025-12-13 20:40:44,359 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 20:40:44,359 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:40:44,359 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:40:44,359 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.89it/s]
Device set to use cuda:0
2025-12-13 20:40:59,607 - INFO - Model pipeline loaded.
2025-12-13 20:40:59,607 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:41:26,733 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:41:26,733 - INFO - Dataset processed with 200 entries.
2025-12-13 20:41:26,747 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:03<09:29,  3.88s/it]Running Inference:   1%|▏         | 2/148 [00:04<04:28,  1.84s/it]Running Inference:   2%|▏         | 3/148 [00:05<03:29,  1.45s/it]Running Inference:   3%|▎         | 4/148 [00:08<05:21,  2.24s/it]Running Inference:   3%|▎         | 5/148 [00:11<05:58,  2.50s/it]Running Inference:   4%|▍         | 6/148 [00:14<06:16,  2.65s/it]Running Inference:   5%|▍         | 7/148 [00:15<04:54,  2.09s/it]Running Inference:   5%|▌         | 8/148 [00:16<04:08,  1.78s/it]Running Inference:   6%|▌         | 9/148 [00:19<04:32,  1.96s/it]Running Inference:   7%|▋         | 10/148 [00:20<04:06,  1.78s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:22<04:24,  1.93s/it]Running Inference:   8%|▊         | 12/148 [00:25<04:39,  2.05s/it]Running Inference:   9%|▉         | 13/148 [00:26<04:28,  1.99s/it]Running Inference:   9%|▉         | 14/148 [00:34<08:21,  3.74s/it]Running Inference:  10%|█         | 15/148 [00:35<06:33,  2.96s/it]Running Inference:  11%|█         | 16/148 [00:40<07:57,  3.62s/it]Running Inference:  11%|█▏        | 17/148 [00:43<06:57,  3.19s/it]Running Inference:  12%|█▏        | 18/148 [00:45<06:11,  2.86s/it]Running Inference:  13%|█▎        | 19/148 [00:46<05:09,  2.40s/it]Running Inference:  14%|█▎        | 20/148 [00:47<04:14,  1.99s/it]Running Inference:  14%|█▍        | 21/148 [00:49<04:21,  2.06s/it]Running Inference:  15%|█▍        | 22/148 [00:53<05:11,  2.47s/it]Running Inference:  16%|█▌        | 23/148 [00:54<04:16,  2.05s/it]Running Inference:  16%|█▌        | 24/148 [00:55<03:28,  1.68s/it]Running Inference:  17%|█▋        | 25/148 [00:57<03:36,  1.76s/it]Running Inference:  18%|█▊        | 26/148 [01:02<05:59,  2.95s/it]Running Inference:  18%|█▊        | 27/148 [01:03<04:42,  2.33s/it]Running Inference:  19%|█▉        | 28/148 [01:04<03:46,  1.88s/it]Running Inference:  20%|█▉        | 29/148 [01:06<04:02,  2.04s/it]Running Inference:  20%|██        | 30/148 [01:08<03:31,  1.79s/it]Running Inference:  21%|██        | 31/148 [01:09<03:05,  1.58s/it]Running Inference:  22%|██▏       | 32/148 [01:11<03:14,  1.68s/it]Running Inference:  22%|██▏       | 33/148 [01:12<02:50,  1.48s/it]Running Inference:  23%|██▎       | 34/148 [01:13<02:47,  1.47s/it]Running Inference:  24%|██▎       | 35/148 [01:15<02:53,  1.53s/it]Running Inference:  24%|██▍       | 36/148 [01:22<05:52,  3.14s/it]Running Inference:  25%|██▌       | 37/148 [01:23<04:39,  2.52s/it]Running Inference:  26%|██▌       | 38/148 [01:24<03:39,  1.99s/it]Running Inference:  26%|██▋       | 39/148 [01:24<02:43,  1.50s/it]Running Inference:  27%|██▋       | 40/148 [01:25<02:22,  1.32s/it]Running Inference:  28%|██▊       | 41/148 [01:27<02:36,  1.46s/it]Running Inference:  28%|██▊       | 42/148 [01:28<02:36,  1.48s/it]Running Inference:  29%|██▉       | 43/148 [01:30<03:04,  1.76s/it]Running Inference:  30%|██▉       | 44/148 [01:32<02:56,  1.70s/it]Running Inference:  30%|███       | 45/148 [01:34<02:49,  1.64s/it]Running Inference:  31%|███       | 46/148 [01:35<02:52,  1.69s/it]Running Inference:  32%|███▏      | 47/148 [01:37<02:59,  1.78s/it]Running Inference:  32%|███▏      | 48/148 [01:38<02:37,  1.58s/it]Running Inference:  33%|███▎      | 49/148 [01:50<07:20,  4.45s/it]Running Inference:  34%|███▍      | 50/148 [01:51<05:51,  3.59s/it]Running Inference:  34%|███▍      | 51/148 [01:53<04:53,  3.03s/it]Running Inference:  35%|███▌      | 52/148 [01:54<03:57,  2.47s/it]Running Inference:  36%|███▌      | 53/148 [01:55<03:19,  2.10s/it]Running Inference:  36%|███▋      | 54/148 [01:57<02:52,  1.83s/it]Running Inference:  37%|███▋      | 55/148 [01:57<02:19,  1.50s/it]Running Inference:  38%|███▊      | 56/148 [01:59<02:23,  1.56s/it]Running Inference:  39%|███▊      | 57/148 [02:00<02:08,  1.42s/it]Running Inference:  39%|███▉      | 58/148 [02:01<02:03,  1.37s/it]Running Inference:  40%|███▉      | 59/148 [02:03<02:01,  1.36s/it]Running Inference:  41%|████      | 60/148 [02:04<02:03,  1.40s/it]Running Inference:  41%|████      | 61/148 [02:06<02:17,  1.58s/it]Running Inference:  42%|████▏     | 62/148 [02:07<02:05,  1.45s/it]Running Inference:  43%|████▎     | 63/148 [02:09<02:10,  1.53s/it]Running Inference:  43%|████▎     | 64/148 [02:12<02:52,  2.05s/it]Running Inference:  44%|████▍     | 65/148 [02:15<03:02,  2.20s/it]Running Inference:  45%|████▍     | 66/148 [02:15<02:22,  1.74s/it]Running Inference:  45%|████▌     | 67/148 [02:19<03:09,  2.34s/it]Running Inference:  46%|████▌     | 68/148 [02:20<02:28,  1.86s/it]Running Inference:  47%|████▋     | 69/148 [02:21<02:09,  1.64s/it]Running Inference:  47%|████▋     | 70/148 [02:24<02:26,  1.88s/it]Running Inference:  48%|████▊     | 71/148 [02:25<02:16,  1.77s/it]Running Inference:  49%|████▊     | 72/148 [02:28<02:45,  2.18s/it]Running Inference:  49%|████▉     | 73/148 [02:30<02:39,  2.12s/it]Running Inference:  50%|█████     | 74/148 [02:32<02:24,  1.95s/it]Running Inference:  51%|█████     | 75/148 [02:34<02:26,  2.01s/it]Running Inference:  51%|█████▏    | 76/148 [02:36<02:22,  1.98s/it]Running Inference:  52%|█████▏    | 77/148 [02:38<02:21,  2.00s/it]Running Inference:  53%|█████▎    | 78/148 [02:40<02:13,  1.90s/it]Running Inference:  53%|█████▎    | 79/148 [02:40<01:47,  1.56s/it]Running Inference:  54%|█████▍    | 80/148 [02:42<01:45,  1.55s/it]Running Inference:  55%|█████▍    | 81/148 [02:43<01:43,  1.55s/it]Running Inference:  55%|█████▌    | 82/148 [02:45<01:37,  1.48s/it]Running Inference:  56%|█████▌    | 83/148 [02:46<01:25,  1.32s/it]Running Inference:  57%|█████▋    | 84/148 [02:47<01:32,  1.45s/it]Running Inference:  57%|█████▋    | 85/148 [02:49<01:28,  1.41s/it]Running Inference:  58%|█████▊    | 86/148 [02:50<01:28,  1.43s/it]Running Inference:  59%|█████▉    | 87/148 [02:52<01:38,  1.61s/it]Running Inference:  59%|█████▉    | 88/148 [02:54<01:31,  1.52s/it]Running Inference:  60%|██████    | 89/148 [02:55<01:28,  1.51s/it]Running Inference:  61%|██████    | 90/148 [02:56<01:15,  1.29s/it]Running Inference:  61%|██████▏   | 91/148 [02:57<01:12,  1.28s/it]Running Inference:  62%|██████▏   | 92/148 [02:58<01:02,  1.12s/it]Running Inference:  63%|██████▎   | 93/148 [03:00<01:14,  1.36s/it]Running Inference:  64%|██████▎   | 94/148 [03:01<01:20,  1.49s/it]Running Inference:  64%|██████▍   | 95/148 [03:03<01:14,  1.41s/it]Running Inference:  65%|██████▍   | 96/148 [03:10<02:37,  3.04s/it]Running Inference:  66%|██████▌   | 97/148 [03:11<02:07,  2.50s/it]Running Inference:  66%|██████▌   | 98/148 [03:13<02:02,  2.45s/it]Running Inference:  67%|██████▋   | 99/148 [03:16<02:11,  2.69s/it]Running Inference:  68%|██████▊   | 100/148 [03:18<01:50,  2.30s/it]Running Inference:  68%|██████▊   | 101/148 [03:20<01:41,  2.16s/it]Running Inference:  69%|██████▉   | 102/148 [03:24<02:06,  2.75s/it]Running Inference:  70%|██████▉   | 103/148 [03:25<01:49,  2.42s/it]Running Inference:  70%|███████   | 104/148 [03:26<01:27,  2.00s/it]Running Inference:  71%|███████   | 105/148 [03:34<02:37,  3.67s/it]Running Inference:  72%|███████▏  | 106/148 [03:37<02:25,  3.47s/it]Running Inference:  72%|███████▏  | 107/148 [03:37<01:45,  2.57s/it]Running Inference:  73%|███████▎  | 108/148 [03:40<01:46,  2.66s/it]Running Inference:  74%|███████▎  | 109/148 [03:41<01:21,  2.08s/it]Running Inference:  74%|███████▍  | 110/148 [03:42<01:08,  1.81s/it]Running Inference:  75%|███████▌  | 111/148 [03:43<01:00,  1.63s/it]Running Inference:  76%|███████▌  | 112/148 [03:44<00:47,  1.32s/it]Running Inference:  76%|███████▋  | 113/148 [03:45<00:48,  1.37s/it]Running Inference:  77%|███████▋  | 114/148 [03:49<01:12,  2.12s/it]Running Inference:  78%|███████▊  | 115/148 [03:50<00:54,  1.66s/it]Running Inference:  78%|███████▊  | 116/148 [03:51<00:48,  1.52s/it]Running Inference:  79%|███████▉  | 117/148 [03:52<00:44,  1.44s/it]Running Inference:  80%|███████▉  | 118/148 [03:54<00:42,  1.41s/it]Running Inference:  80%|████████  | 119/148 [03:56<00:45,  1.56s/it]Running Inference:  81%|████████  | 120/148 [03:58<00:51,  1.83s/it]Running Inference:  82%|████████▏ | 121/148 [04:00<00:52,  1.95s/it]Running Inference:  82%|████████▏ | 122/148 [04:01<00:41,  1.60s/it]Running Inference:  83%|████████▎ | 123/148 [04:03<00:45,  1.81s/it]Running Inference:  84%|████████▍ | 124/148 [04:05<00:41,  1.75s/it]Running Inference:  84%|████████▍ | 125/148 [04:15<01:36,  4.19s/it]Running Inference:  85%|████████▌ | 126/148 [04:18<01:26,  3.93s/it]Running Inference:  86%|████████▌ | 127/148 [04:20<01:05,  3.13s/it]Running Inference:  86%|████████▋ | 128/148 [04:20<00:48,  2.44s/it]Running Inference:  87%|████████▋ | 129/148 [04:21<00:36,  1.93s/it]Running Inference:  88%|████████▊ | 130/148 [04:27<00:58,  3.26s/it]Running Inference:  89%|████████▊ | 131/148 [04:30<00:52,  3.09s/it]Running Inference:  89%|████████▉ | 132/148 [04:32<00:44,  2.79s/it]Running Inference:  90%|████████▉ | 133/148 [04:34<00:38,  2.58s/it]Running Inference:  91%|█████████ | 134/148 [04:42<00:55,  3.99s/it]Running Inference:  91%|█████████ | 135/148 [04:42<00:39,  3.05s/it]Running Inference:  92%|█████████▏| 136/148 [04:44<00:29,  2.50s/it]Running Inference:  93%|█████████▎| 137/148 [04:46<00:27,  2.46s/it]Running Inference:  93%|█████████▎| 138/148 [04:49<00:25,  2.52s/it]Running Inference:  94%|█████████▍| 139/148 [04:49<00:17,  1.99s/it]Running Inference:  95%|█████████▍| 140/148 [04:51<00:15,  1.88s/it]Running Inference:  95%|█████████▌| 141/148 [04:54<00:14,  2.07s/it]Running Inference:  96%|█████████▌| 142/148 [04:54<00:09,  1.63s/it]Running Inference:  97%|█████████▋| 143/148 [04:56<00:08,  1.62s/it]Running Inference:  97%|█████████▋| 144/148 [04:58<00:06,  1.67s/it]Running Inference:  98%|█████████▊| 145/148 [04:59<00:04,  1.48s/it]Running Inference:  99%|█████████▊| 146/148 [05:01<00:03,  1.70s/it]Running Inference:  99%|█████████▉| 147/148 [05:02<00:01,  1.52s/it]Running Inference: 100%|██████████| 148/148 [05:03<00:00,  1.37s/it]Running Inference: 100%|██████████| 148/148 [05:03<00:00,  2.05s/it]
2025-12-13 20:46:30,186 - INFO - Inference completed.
2025-12-13 20:46:30,195 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 20:46:30,195 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:46:30,206 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 20:46:30,206 - INFO - Metrics:
26.08
2025-12-13 20:46:30,207 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qasper, ratio=0.2) on GPU 2

----------------------------------------
Task: qasper | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:46:36,598 - INFO - Set deterministic seeds to 42
2025-12-13 20:46:36,598 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:46:36,599 - INFO - Starting evaluation run...
2025-12-13 20:46:36,599 - INFO - Output directory set to: longbenchresult
2025-12-13 20:46:36,599 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 20:46:36,599 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:46:36,599 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:46:36,599 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.55it/s]
Device set to use cuda:0
2025-12-13 20:46:49,558 - INFO - Model pipeline loaded.
2025-12-13 20:46:49,558 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:46:58,974 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:46:58,974 - INFO - Dataset processed with 200 entries.
2025-12-13 20:46:58,988 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:04<10:21,  4.23s/it]Running Inference:   1%|▏         | 2/148 [00:04<04:49,  1.98s/it]Running Inference:   2%|▏         | 3/148 [00:05<03:39,  1.52s/it]Running Inference:   3%|▎         | 4/148 [00:08<05:13,  2.18s/it]Running Inference:   3%|▎         | 5/148 [00:11<05:56,  2.49s/it]Running Inference:   4%|▍         | 6/148 [00:19<09:55,  4.20s/it]Running Inference:   5%|▍         | 7/148 [00:20<07:43,  3.28s/it]Running Inference:   5%|▌         | 8/148 [00:21<06:08,  2.63s/it]Running Inference:   6%|▌         | 9/148 [00:24<05:52,  2.54s/it]Running Inference:   7%|▋         | 10/148 [00:26<05:33,  2.42s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:28<05:24,  2.37s/it]Running Inference:   8%|▊         | 12/148 [00:30<05:16,  2.33s/it]Running Inference:   9%|▉         | 13/148 [00:32<04:54,  2.18s/it]Running Inference:   9%|▉         | 14/148 [00:35<05:08,  2.30s/it]Running Inference:  10%|█         | 15/148 [00:36<04:17,  1.94s/it]Running Inference:  11%|█         | 16/148 [00:44<08:35,  3.90s/it]Running Inference:  11%|█▏        | 17/148 [00:46<07:17,  3.34s/it]Running Inference:  12%|█▏        | 18/148 [00:49<06:25,  2.97s/it]Running Inference:  13%|█▎        | 19/148 [00:50<05:19,  2.48s/it]Running Inference:  14%|█▎        | 20/148 [00:51<04:21,  2.04s/it]Running Inference:  14%|█▍        | 21/148 [00:53<04:26,  2.10s/it]Running Inference:  15%|█▍        | 22/148 [00:56<05:04,  2.42s/it]Running Inference:  16%|█▌        | 23/148 [00:57<04:11,  2.01s/it]Running Inference:  16%|█▌        | 24/148 [00:58<03:24,  1.65s/it]Running Inference:  17%|█▋        | 25/148 [01:00<03:34,  1.74s/it]Running Inference:  18%|█▊        | 26/148 [01:05<05:29,  2.70s/it]Running Inference:  18%|█▊        | 27/148 [01:06<04:21,  2.16s/it]Running Inference:  19%|█▉        | 28/148 [01:07<03:31,  1.77s/it]Running Inference:  20%|█▉        | 29/148 [01:14<06:54,  3.48s/it]Running Inference:  20%|██        | 30/148 [01:16<05:30,  2.80s/it]Running Inference:  21%|██        | 31/148 [01:17<04:28,  2.29s/it]Running Inference:  22%|██▏       | 32/148 [01:18<04:04,  2.10s/it]Running Inference:  22%|██▏       | 33/148 [01:19<03:24,  1.78s/it]Running Inference:  23%|██▎       | 34/148 [01:21<03:11,  1.68s/it]Running Inference:  24%|██▎       | 35/148 [01:23<03:11,  1.69s/it]Running Inference:  24%|██▍       | 36/148 [01:30<06:09,  3.30s/it]Running Inference:  25%|██▌       | 37/148 [01:31<04:51,  2.62s/it]Running Inference:  26%|██▌       | 38/148 [01:31<03:44,  2.04s/it]Running Inference:  26%|██▋       | 39/148 [01:32<02:47,  1.54s/it]Running Inference:  27%|██▋       | 40/148 [01:33<02:25,  1.35s/it]Running Inference:  28%|██▊       | 41/148 [01:34<02:38,  1.48s/it]Running Inference:  28%|██▊       | 42/148 [01:36<02:27,  1.39s/it]Running Inference:  29%|██▉       | 43/148 [01:38<03:00,  1.72s/it]Running Inference:  30%|██▉       | 44/148 [01:40<03:10,  1.83s/it]Running Inference:  30%|███       | 45/148 [01:42<02:57,  1.73s/it]Running Inference:  31%|███       | 46/148 [01:43<02:58,  1.75s/it]Running Inference:  32%|███▏      | 47/148 [01:45<02:50,  1.69s/it]Running Inference:  32%|███▏      | 48/148 [01:46<02:32,  1.52s/it]Running Inference:  33%|███▎      | 49/148 [01:57<07:20,  4.45s/it]Running Inference:  34%|███▍      | 50/148 [01:59<05:48,  3.56s/it]Running Inference:  34%|███▍      | 51/148 [02:00<04:49,  2.98s/it]Running Inference:  35%|███▌      | 52/148 [02:02<03:53,  2.44s/it]Running Inference:  36%|███▌      | 53/148 [02:03<03:17,  2.08s/it]Running Inference:  36%|███▋      | 54/148 [02:04<02:44,  1.75s/it]Running Inference:  37%|███▋      | 55/148 [02:05<02:14,  1.45s/it]Running Inference:  38%|███▊      | 56/148 [02:06<02:16,  1.49s/it]Running Inference:  39%|███▊      | 57/148 [02:07<02:03,  1.36s/it]Running Inference:  39%|███▉      | 58/148 [02:09<02:07,  1.41s/it]Running Inference:  40%|███▉      | 59/148 [02:10<02:01,  1.36s/it]Running Inference:  41%|████      | 60/148 [02:11<02:02,  1.39s/it]Running Inference:  41%|████      | 61/148 [02:13<02:17,  1.58s/it]Running Inference:  42%|████▏     | 62/148 [02:15<02:04,  1.45s/it]Running Inference:  43%|████▎     | 63/148 [02:16<02:09,  1.53s/it]Running Inference:  43%|████▎     | 64/148 [02:20<03:13,  2.31s/it]Running Inference:  44%|████▍     | 65/148 [02:24<03:31,  2.54s/it]Running Inference:  45%|████▍     | 66/148 [02:24<02:47,  2.05s/it]Running Inference:  45%|████▌     | 67/148 [02:28<03:27,  2.56s/it]Running Inference:  46%|████▌     | 68/148 [02:29<02:41,  2.01s/it]Running Inference:  47%|████▋     | 69/148 [02:30<02:21,  1.78s/it]Running Inference:  47%|████▋     | 70/148 [02:33<02:32,  1.95s/it]Running Inference:  48%|████▊     | 71/148 [02:34<02:20,  1.82s/it]Running Inference:  49%|████▊     | 72/148 [02:36<02:18,  1.82s/it]Running Inference:  49%|████▉     | 73/148 [02:38<02:20,  1.87s/it]Running Inference:  50%|█████     | 74/148 [02:39<02:11,  1.78s/it]Running Inference:  51%|█████     | 75/148 [02:42<02:19,  1.90s/it]Running Inference:  51%|█████▏    | 76/148 [02:44<02:19,  1.94s/it]Running Inference:  52%|█████▏    | 77/148 [02:46<02:19,  1.97s/it]Running Inference:  53%|█████▎    | 78/148 [02:47<02:11,  1.88s/it]Running Inference:  53%|█████▎    | 79/148 [02:48<01:46,  1.55s/it]Running Inference:  54%|█████▍    | 80/148 [02:50<01:44,  1.54s/it]Running Inference:  55%|█████▍    | 81/148 [02:52<01:49,  1.63s/it]Running Inference:  55%|█████▌    | 82/148 [02:53<01:41,  1.54s/it]Running Inference:  56%|█████▌    | 83/148 [02:54<01:28,  1.36s/it]Running Inference:  57%|█████▋    | 84/148 [02:56<01:35,  1.49s/it]Running Inference:  57%|█████▋    | 85/148 [02:57<01:36,  1.54s/it]Running Inference:  58%|█████▊    | 86/148 [02:59<01:34,  1.52s/it]Running Inference:  59%|█████▉    | 87/148 [03:01<01:44,  1.72s/it]Running Inference:  59%|█████▉    | 88/148 [03:02<01:35,  1.60s/it]Running Inference:  60%|██████    | 89/148 [03:04<01:32,  1.56s/it]Running Inference:  61%|██████    | 90/148 [03:04<01:17,  1.33s/it]Running Inference:  61%|██████▏   | 91/148 [03:06<01:20,  1.41s/it]Running Inference:  62%|██████▏   | 92/148 [03:07<01:11,  1.28s/it]Running Inference:  63%|██████▎   | 93/148 [03:09<01:17,  1.41s/it]Running Inference:  64%|██████▎   | 94/148 [03:11<01:22,  1.53s/it]Running Inference:  64%|██████▍   | 95/148 [03:12<01:23,  1.58s/it]Running Inference:  65%|██████▍   | 96/148 [03:13<01:11,  1.37s/it]Running Inference:  66%|██████▌   | 97/148 [03:14<01:08,  1.34s/it]Running Inference:  66%|██████▌   | 98/148 [03:17<01:21,  1.63s/it]Running Inference:  67%|██████▋   | 99/148 [03:20<01:43,  2.11s/it]Running Inference:  68%|██████▊   | 100/148 [03:21<01:32,  1.93s/it]Running Inference:  68%|██████▊   | 101/148 [03:23<01:29,  1.90s/it]Running Inference:  69%|██████▉   | 102/148 [03:28<02:01,  2.65s/it]Running Inference:  70%|██████▉   | 103/148 [03:30<01:50,  2.46s/it]Running Inference:  70%|███████   | 104/148 [03:31<01:36,  2.20s/it]Running Inference:  71%|███████   | 105/148 [03:39<02:44,  3.83s/it]Running Inference:  72%|███████▏  | 106/148 [03:42<02:30,  3.58s/it]Running Inference:  72%|███████▏  | 107/148 [03:42<01:48,  2.66s/it]Running Inference:  73%|███████▎  | 108/148 [03:45<01:48,  2.72s/it]Running Inference:  74%|███████▎  | 109/148 [03:46<01:22,  2.12s/it]Running Inference:  74%|███████▍  | 110/148 [03:47<01:09,  1.84s/it]Running Inference:  75%|███████▌  | 111/148 [03:48<00:59,  1.61s/it]Running Inference:  76%|███████▌  | 112/148 [03:49<00:46,  1.30s/it]Running Inference:  76%|███████▋  | 113/148 [03:50<00:48,  1.38s/it]Running Inference:  77%|███████▋  | 114/148 [03:54<01:11,  2.10s/it]Running Inference:  78%|███████▊  | 115/148 [03:55<00:54,  1.65s/it]Running Inference:  78%|███████▊  | 116/148 [03:56<00:48,  1.52s/it]Running Inference:  79%|███████▉  | 117/148 [03:57<00:44,  1.44s/it]Running Inference:  80%|███████▉  | 118/148 [03:59<00:42,  1.42s/it]Running Inference:  80%|████████  | 119/148 [04:01<00:46,  1.59s/it]Running Inference:  81%|████████  | 120/148 [04:02<00:45,  1.62s/it]Running Inference:  82%|████████▏ | 121/148 [04:05<00:49,  1.82s/it]Running Inference:  82%|████████▏ | 122/148 [04:05<00:38,  1.48s/it]Running Inference:  83%|████████▎ | 123/148 [04:07<00:42,  1.69s/it]Running Inference:  84%|████████▍ | 124/148 [04:09<00:41,  1.71s/it]Running Inference:  84%|████████▍ | 125/148 [04:17<01:21,  3.56s/it]Running Inference:  85%|████████▌ | 126/148 [04:20<01:17,  3.52s/it]Running Inference:  86%|████████▌ | 127/148 [04:22<00:59,  2.85s/it]Running Inference:  86%|████████▋ | 128/148 [04:23<00:44,  2.24s/it]Running Inference:  87%|████████▋ | 129/148 [04:24<00:36,  1.90s/it]Running Inference:  88%|████████▊ | 130/148 [04:30<00:59,  3.30s/it]Running Inference:  89%|████████▊ | 131/148 [04:33<00:53,  3.14s/it]Running Inference:  89%|████████▉ | 132/148 [04:40<01:09,  4.35s/it]Running Inference:  90%|████████▉ | 133/148 [04:42<00:55,  3.69s/it]Running Inference:  91%|█████████ | 134/148 [04:50<01:07,  4.83s/it]Running Inference:  91%|█████████ | 135/148 [04:51<00:47,  3.64s/it]Running Inference:  92%|█████████▏| 136/148 [04:52<00:34,  2.92s/it]Running Inference:  93%|█████████▎| 137/148 [04:54<00:30,  2.76s/it]Running Inference:  93%|█████████▎| 138/148 [04:56<00:25,  2.50s/it]Running Inference:  94%|█████████▍| 139/148 [04:57<00:17,  1.96s/it]Running Inference:  95%|█████████▍| 140/148 [04:59<00:14,  1.87s/it]Running Inference:  95%|█████████▌| 141/148 [05:01<00:15,  2.17s/it]Running Inference:  96%|█████████▌| 142/148 [05:02<00:10,  1.70s/it]Running Inference:  97%|█████████▋| 143/148 [05:04<00:08,  1.68s/it]Running Inference:  97%|█████████▋| 144/148 [05:05<00:06,  1.52s/it]Running Inference:  98%|█████████▊| 145/148 [05:06<00:04,  1.38s/it]Running Inference:  99%|█████████▊| 146/148 [05:08<00:03,  1.58s/it]Running Inference:  99%|█████████▉| 147/148 [05:10<00:01,  1.71s/it]Running Inference: 100%|██████████| 148/148 [05:11<00:00,  1.50s/it]Running Inference: 100%|██████████| 148/148 [05:11<00:00,  2.10s/it]
2025-12-13 20:52:10,488 - INFO - Inference completed.
2025-12-13 20:52:10,498 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-13 20:52:10,498 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:52:10,509 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-13 20:52:10,509 - INFO - Metrics:
23.43
2025-12-13 20:52:10,511 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qasper, ratio=0.3) on GPU 2

----------------------------------------
Task: qasper | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:52:16,892 - INFO - Set deterministic seeds to 42
2025-12-13 20:52:16,892 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qasper",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:52:16,892 - INFO - Starting evaluation run...
2025-12-13 20:52:16,892 - INFO - Output directory set to: longbenchresult
2025-12-13 20:52:16,893 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-13 20:52:16,893 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:52:16,893 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:52:16,893 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.74it/s]
Device set to use cuda:0
2025-12-13 20:52:32,356 - INFO - Model pipeline loaded.
2025-12-13 20:52:32,356 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qasper)
2025-12-13 20:52:42,755 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:52:42,755 - INFO - Dataset processed with 200 entries.
2025-12-13 20:52:42,769 - INFO - Starting inference...
Running Inference:   0%|          | 0/148 [00:00<?, ?it/s]Running Inference:   1%|          | 1/148 [00:04<10:56,  4.47s/it]Running Inference:   1%|▏         | 2/148 [00:04<05:07,  2.10s/it]Running Inference:   2%|▏         | 3/148 [00:05<03:48,  1.57s/it]Running Inference:   3%|▎         | 4/148 [00:08<04:33,  1.90s/it]Running Inference:   3%|▎         | 5/148 [00:11<05:27,  2.29s/it]Running Inference:   4%|▍         | 6/148 [00:14<06:20,  2.68s/it]Running Inference:   5%|▍         | 7/148 [00:21<09:19,  3.97s/it]Running Inference:   5%|▌         | 8/148 [00:22<07:03,  3.02s/it]Running Inference:   6%|▌         | 9/148 [00:24<06:10,  2.66s/it]Running Inference:   7%|▋         | 10/148 [00:25<05:10,  2.25s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   7%|▋         | 11/148 [00:27<05:07,  2.24s/it]Running Inference:   8%|▊         | 12/148 [00:29<04:55,  2.17s/it]Running Inference:   9%|▉         | 13/148 [00:31<04:29,  2.00s/it]Running Inference:   9%|▉         | 14/148 [00:33<04:50,  2.17s/it]Running Inference:  10%|█         | 15/148 [00:34<04:02,  1.82s/it]Running Inference:  11%|█         | 16/148 [00:43<08:17,  3.77s/it]Running Inference:  11%|█▏        | 17/148 [00:45<07:08,  3.27s/it]Running Inference:  12%|█▏        | 18/148 [00:47<06:21,  2.93s/it]Running Inference:  13%|█▎        | 19/148 [00:48<05:10,  2.41s/it]Running Inference:  14%|█▎        | 20/148 [00:49<04:17,  2.01s/it]Running Inference:  14%|█▍        | 21/148 [00:51<04:14,  2.00s/it]Running Inference:  15%|█▍        | 22/148 [00:55<05:22,  2.56s/it]Running Inference:  16%|█▌        | 23/148 [00:56<04:23,  2.11s/it]Running Inference:  16%|█▌        | 24/148 [00:57<03:32,  1.71s/it]Running Inference:  17%|█▋        | 25/148 [00:59<04:02,  1.97s/it]Running Inference:  18%|█▊        | 26/148 [01:07<07:26,  3.66s/it]Running Inference:  18%|█▊        | 27/148 [01:08<05:41,  2.83s/it]Running Inference:  19%|█▉        | 28/148 [01:09<04:26,  2.22s/it]Running Inference:  20%|█▉        | 29/148 [01:21<10:31,  5.30s/it]Running Inference:  20%|██        | 30/148 [01:22<08:00,  4.08s/it]Running Inference:  21%|██        | 31/148 [01:24<06:11,  3.18s/it]Running Inference:  22%|██▏       | 32/148 [01:25<05:14,  2.71s/it]Running Inference:  22%|██▏       | 33/148 [01:27<04:26,  2.31s/it]Running Inference:  23%|██▎       | 34/148 [01:28<03:52,  2.04s/it]Running Inference:  24%|██▎       | 35/148 [01:29<03:20,  1.77s/it]Running Inference:  24%|██▍       | 36/148 [01:31<03:29,  1.87s/it]Running Inference:  25%|██▌       | 37/148 [01:32<02:59,  1.62s/it]Running Inference:  26%|██▌       | 38/148 [01:33<02:32,  1.39s/it]Running Inference:  26%|██▋       | 39/148 [01:33<01:57,  1.08s/it]Running Inference:  27%|██▋       | 40/148 [01:34<01:39,  1.08it/s]Running Inference:  28%|██▊       | 41/148 [01:37<02:52,  1.61s/it]Running Inference:  28%|██▊       | 42/148 [01:38<02:36,  1.47s/it]Running Inference:  29%|██▉       | 43/148 [01:41<02:58,  1.70s/it]Running Inference:  30%|██▉       | 44/148 [01:43<03:06,  1.80s/it]Running Inference:  30%|███       | 45/148 [01:44<02:54,  1.69s/it]Running Inference:  31%|███       | 46/148 [01:46<02:44,  1.61s/it]Running Inference:  32%|███▏      | 47/148 [01:47<02:42,  1.61s/it]Running Inference:  32%|███▏      | 48/148 [01:48<02:29,  1.50s/it]Running Inference:  33%|███▎      | 49/148 [01:59<07:06,  4.31s/it]Running Inference:  34%|███▍      | 50/148 [02:01<05:37,  3.45s/it]Running Inference:  34%|███▍      | 51/148 [02:03<04:56,  3.06s/it]Running Inference:  35%|███▌      | 52/148 [02:04<04:03,  2.53s/it]Running Inference:  36%|███▌      | 53/148 [02:11<05:53,  3.72s/it]Running Inference:  36%|███▋      | 54/148 [02:12<04:35,  2.93s/it]Running Inference:  37%|███▋      | 55/148 [02:12<03:30,  2.27s/it]Running Inference:  38%|███▊      | 56/148 [02:14<03:07,  2.04s/it]Running Inference:  39%|███▊      | 57/148 [02:15<02:48,  1.85s/it]Running Inference:  39%|███▉      | 58/148 [02:17<02:37,  1.75s/it]Running Inference:  40%|███▉      | 59/148 [02:18<02:21,  1.59s/it]Running Inference:  41%|████      | 60/148 [02:20<02:16,  1.56s/it]Running Inference:  41%|████      | 61/148 [02:22<02:27,  1.69s/it]Running Inference:  42%|████▏     | 62/148 [02:23<02:11,  1.53s/it]Running Inference:  43%|████▎     | 63/148 [02:24<01:59,  1.40s/it]Running Inference:  43%|████▎     | 64/148 [02:27<02:42,  1.94s/it]Running Inference:  44%|████▍     | 65/148 [02:30<03:05,  2.24s/it]Running Inference:  45%|████▍     | 66/148 [02:31<02:27,  1.80s/it]Running Inference:  45%|████▌     | 67/148 [02:33<02:35,  1.91s/it]Running Inference:  46%|████▌     | 68/148 [02:34<02:04,  1.56s/it]Running Inference:  47%|████▋     | 69/148 [02:35<01:55,  1.46s/it]Running Inference:  47%|████▋     | 70/148 [02:37<02:17,  1.77s/it]Running Inference:  48%|████▊     | 71/148 [02:39<02:09,  1.69s/it]Running Inference:  49%|████▊     | 72/148 [02:41<02:10,  1.71s/it]Running Inference:  49%|████▉     | 73/148 [02:43<02:14,  1.79s/it]Running Inference:  50%|█████     | 74/148 [02:44<02:10,  1.76s/it]Running Inference:  51%|█████     | 75/148 [02:46<02:14,  1.84s/it]Running Inference:  51%|█████▏    | 76/148 [02:48<02:13,  1.85s/it]Running Inference:  52%|█████▏    | 77/148 [02:50<02:11,  1.86s/it]Running Inference:  53%|█████▎    | 78/148 [02:51<02:00,  1.72s/it]Running Inference:  53%|█████▎    | 79/148 [02:58<03:35,  3.13s/it]Running Inference:  54%|█████▍    | 80/148 [03:00<03:06,  2.74s/it]Running Inference:  55%|█████▍    | 81/148 [03:02<02:46,  2.48s/it]Running Inference:  55%|█████▌    | 82/148 [03:03<02:21,  2.15s/it]Running Inference:  56%|█████▌    | 83/148 [03:04<01:55,  1.78s/it]Running Inference:  57%|█████▋    | 84/148 [03:06<01:53,  1.77s/it]Running Inference:  57%|█████▋    | 85/148 [03:07<01:41,  1.61s/it]Running Inference:  58%|█████▊    | 86/148 [03:08<01:30,  1.45s/it]Running Inference:  59%|█████▉    | 87/148 [03:15<03:14,  3.19s/it]Running Inference:  59%|█████▉    | 88/148 [03:17<02:38,  2.63s/it]Running Inference:  60%|██████    | 89/148 [03:18<02:07,  2.16s/it]Running Inference:  61%|██████    | 90/148 [03:18<01:39,  1.71s/it]Running Inference:  61%|██████▏   | 91/148 [03:19<01:26,  1.51s/it]Running Inference:  62%|██████▏   | 92/148 [03:20<01:15,  1.34s/it]Running Inference:  63%|██████▎   | 93/148 [03:22<01:19,  1.44s/it]Running Inference:  64%|██████▎   | 94/148 [03:24<01:23,  1.54s/it]Running Inference:  64%|██████▍   | 95/148 [03:25<01:18,  1.49s/it]Running Inference:  65%|██████▍   | 96/148 [03:26<01:07,  1.30s/it]Running Inference:  66%|██████▌   | 97/148 [03:27<01:05,  1.28s/it]Running Inference:  66%|██████▌   | 98/148 [03:29<01:18,  1.58s/it]Running Inference:  67%|██████▋   | 99/148 [03:30<01:08,  1.40s/it]Running Inference:  68%|██████▊   | 100/148 [03:32<01:02,  1.31s/it]Running Inference:  68%|██████▊   | 101/148 [03:33<01:05,  1.39s/it]Running Inference:  69%|██████▉   | 102/148 [03:37<01:40,  2.18s/it]Running Inference:  70%|██████▉   | 103/148 [03:39<01:34,  2.09s/it]Running Inference:  70%|███████   | 104/148 [03:41<01:24,  1.93s/it]Running Inference:  71%|███████   | 105/148 [03:43<01:29,  2.08s/it]Running Inference:  72%|███████▏  | 106/148 [03:46<01:38,  2.35s/it]Running Inference:  72%|███████▏  | 107/148 [03:46<01:13,  1.79s/it]Running Inference:  73%|███████▎  | 108/148 [03:47<01:01,  1.53s/it]Running Inference:  74%|███████▎  | 109/148 [03:48<00:50,  1.28s/it]Running Inference:  74%|███████▍  | 110/148 [03:49<00:47,  1.25s/it]Running Inference:  75%|███████▌  | 111/148 [03:51<00:52,  1.42s/it]Running Inference:  76%|███████▌  | 112/148 [03:52<00:42,  1.17s/it]Running Inference:  76%|███████▋  | 113/148 [03:53<00:44,  1.28s/it]Running Inference:  77%|███████▋  | 114/148 [03:57<01:05,  1.93s/it]Running Inference:  78%|███████▊  | 115/148 [03:57<00:50,  1.53s/it]Running Inference:  78%|███████▊  | 116/148 [03:58<00:45,  1.41s/it]Running Inference:  79%|███████▉  | 117/148 [04:00<00:42,  1.38s/it]Running Inference:  80%|███████▉  | 118/148 [04:01<00:40,  1.35s/it]Running Inference:  80%|████████  | 119/148 [04:03<00:44,  1.52s/it]Running Inference:  81%|████████  | 120/148 [04:04<00:41,  1.50s/it]Running Inference:  82%|████████▏ | 121/148 [04:07<00:46,  1.72s/it]Running Inference:  82%|████████▏ | 122/148 [04:07<00:36,  1.40s/it]Running Inference:  83%|████████▎ | 123/148 [04:10<00:46,  1.84s/it]Running Inference:  84%|████████▍ | 124/148 [04:11<00:40,  1.69s/it]Running Inference:  84%|████████▍ | 125/148 [04:14<00:43,  1.89s/it]Running Inference:  85%|████████▌ | 126/148 [04:17<00:47,  2.17s/it]Running Inference:  86%|████████▌ | 127/148 [04:18<00:39,  1.90s/it]Running Inference:  86%|████████▋ | 128/148 [04:19<00:31,  1.58s/it]Running Inference:  87%|████████▋ | 129/148 [04:20<00:27,  1.42s/it]Running Inference:  88%|████████▊ | 130/148 [04:22<00:28,  1.56s/it]Running Inference:  89%|████████▊ | 131/148 [04:24<00:31,  1.85s/it]Running Inference:  89%|████████▉ | 132/148 [04:26<00:28,  1.77s/it]Running Inference:  90%|████████▉ | 133/148 [04:34<00:53,  3.58s/it]Running Inference:  91%|█████████ | 134/148 [04:47<01:31,  6.52s/it]Running Inference:  91%|█████████ | 135/148 [04:48<01:03,  4.88s/it]Running Inference:  92%|█████████▏| 136/148 [04:49<00:45,  3.79s/it]Running Inference:  93%|█████████▎| 137/148 [04:51<00:34,  3.15s/it]Running Inference:  93%|█████████▎| 138/148 [04:56<00:36,  3.70s/it]Running Inference:  94%|█████████▍| 139/148 [04:57<00:25,  2.80s/it]Running Inference:  95%|█████████▍| 140/148 [04:58<00:19,  2.44s/it]Running Inference:  95%|█████████▌| 141/148 [05:01<00:18,  2.64s/it]Running Inference:  96%|█████████▌| 142/148 [05:02<00:12,  2.15s/it]Running Inference:  97%|█████████▋| 143/148 [05:04<00:09,  1.92s/it]Running Inference:  97%|█████████▋| 144/148 [05:10<00:13,  3.28s/it]Running Inference:  98%|█████████▊| 145/148 [05:13<00:09,  3.24s/it]Running Inference:  99%|█████████▊| 146/148 [05:16<00:05,  2.96s/it]Running Inference:  99%|█████████▉| 147/148 [05:17<00:02,  2.38s/it]Running Inference: 100%|██████████| 148/148 [05:18<00:00,  2.03s/it]Running Inference: 100%|██████████| 148/148 [05:18<00:00,  2.15s/it]
2025-12-13 20:58:01,086 - INFO - Inference completed.
2025-12-13 20:58:01,095 - INFO - Results saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-13 20:58:01,095 - INFO - Calculating metrics for dataset: longbench
2025-12-13 20:58:01,106 - INFO - Metrics saved to longbenchresult/longbench__qasper__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-13 20:58:01,106 - INFO - Metrics:
21.03
2025-12-13 20:58:01,108 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qasper, ratio=0.5) on GPU 2


========================================
LongBench Task: triviaqa
========================================
----------------------------------------
Task: triviaqa | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 20:58:07,481 - INFO - Set deterministic seeds to 42
2025-12-13 20:58:07,481 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 20:58:07,481 - INFO - Starting evaluation run...
2025-12-13 20:58:07,482 - INFO - Output directory set to: longbenchresult
2025-12-13 20:58:07,482 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 20:58:07,482 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 20:58:07,482 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 20:58:07,482 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.19it/s]
Device set to use cuda:0
2025-12-13 20:58:29,289 - INFO - Model pipeline loaded.
2025-12-13 20:58:29,290 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 20:58:36,064 - INFO - Dataset loaded with 200 entries.
2025-12-13 20:58:36,065 - INFO - Dataset processed with 200 entries.
2025-12-13 20:58:36,094 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:45,  2.94s/it]Running Inference:   1%|          | 2/200 [00:04<07:18,  2.21s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:18,  2.53s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:10,  2.50s/it]Running Inference:   2%|▎         | 5/200 [00:10<05:46,  1.78s/it]Running Inference:   3%|▎         | 6/200 [00:11<04:39,  1.44s/it]Running Inference:   4%|▎         | 7/200 [00:11<03:30,  1.09s/it]Running Inference:   4%|▍         | 8/200 [00:14<04:55,  1.54s/it]Running Inference:   4%|▍         | 9/200 [00:15<04:41,  1.47s/it]Running Inference:   5%|▌         | 10/200 [00:17<04:44,  1.50s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:19<06:02,  1.92s/it]Running Inference:   6%|▌         | 12/200 [00:23<07:19,  2.34s/it]Running Inference:   6%|▋         | 13/200 [00:24<06:19,  2.03s/it]Running Inference:   7%|▋         | 14/200 [00:29<09:04,  2.93s/it]Running Inference:   8%|▊         | 15/200 [00:31<08:30,  2.76s/it]Running Inference:   8%|▊         | 16/200 [00:34<08:37,  2.81s/it]Running Inference:   8%|▊         | 17/200 [00:35<06:33,  2.15s/it]Running Inference:   9%|▉         | 18/200 [00:36<05:11,  1.71s/it]Running Inference:  10%|▉         | 19/200 [00:39<06:18,  2.09s/it]Running Inference:  10%|█         | 20/200 [00:40<05:43,  1.91s/it]Running Inference:  10%|█         | 21/200 [00:41<04:26,  1.49s/it]Running Inference:  11%|█         | 22/200 [00:42<04:20,  1.47s/it]Running Inference:  12%|█▏        | 23/200 [00:45<05:13,  1.77s/it]Running Inference:  12%|█▏        | 24/200 [00:47<05:37,  1.92s/it]Running Inference:  12%|█▎        | 25/200 [00:48<05:21,  1.84s/it]Running Inference:  13%|█▎        | 26/200 [00:50<04:53,  1.69s/it]Running Inference:  14%|█▎        | 27/200 [00:50<03:54,  1.35s/it]Running Inference:  14%|█▍        | 28/200 [00:51<03:18,  1.15s/it]Running Inference:  14%|█▍        | 29/200 [00:53<04:03,  1.43s/it]Running Inference:  15%|█▌        | 30/200 [00:56<04:54,  1.73s/it]Running Inference:  16%|█▌        | 31/200 [00:57<04:38,  1.65s/it]Running Inference:  16%|█▌        | 32/200 [01:01<06:26,  2.30s/it]Running Inference:  16%|█▋        | 33/200 [01:03<06:01,  2.16s/it]Running Inference:  17%|█▋        | 34/200 [01:04<05:21,  1.94s/it]Running Inference:  18%|█▊        | 35/200 [01:05<04:44,  1.73s/it]Running Inference:  18%|█▊        | 36/200 [01:09<06:44,  2.47s/it]Running Inference:  18%|█▊        | 37/200 [01:13<07:18,  2.69s/it]Running Inference:  19%|█▉        | 38/200 [01:17<08:57,  3.32s/it]Running Inference:  20%|█▉        | 39/200 [01:18<06:40,  2.49s/it]Running Inference:  20%|██        | 40/200 [01:21<07:11,  2.70s/it]Running Inference:  20%|██        | 41/200 [01:23<06:18,  2.38s/it]Running Inference:  21%|██        | 42/200 [01:25<06:28,  2.46s/it]Running Inference:  22%|██▏       | 43/200 [01:26<04:49,  1.85s/it]Running Inference:  22%|██▏       | 44/200 [01:28<04:42,  1.81s/it]Running Inference:  22%|██▎       | 45/200 [01:28<03:36,  1.40s/it]Running Inference:  23%|██▎       | 46/200 [01:31<04:45,  1.85s/it]Running Inference:  24%|██▎       | 47/200 [01:32<04:07,  1.62s/it]Running Inference:  24%|██▍       | 48/200 [01:35<05:18,  2.09s/it]Running Inference:  24%|██▍       | 49/200 [01:37<05:18,  2.11s/it]Running Inference:  25%|██▌       | 50/200 [01:40<05:45,  2.30s/it]Running Inference:  26%|██▌       | 51/200 [01:41<04:32,  1.83s/it]Running Inference:  26%|██▌       | 52/200 [01:43<04:47,  1.94s/it]Running Inference:  26%|██▋       | 53/200 [01:44<03:40,  1.50s/it]Running Inference:  27%|██▋       | 54/200 [01:44<02:57,  1.21s/it]Running Inference:  28%|██▊       | 55/200 [01:45<02:52,  1.19s/it]Running Inference:  28%|██▊       | 56/200 [01:49<04:35,  1.92s/it]Running Inference:  28%|██▊       | 57/200 [01:51<05:02,  2.12s/it]Running Inference:  29%|██▉       | 58/200 [01:53<04:43,  2.00s/it]Running Inference:  30%|██▉       | 59/200 [01:57<06:19,  2.69s/it]Running Inference:  30%|███       | 60/200 [01:59<05:37,  2.41s/it]Running Inference:  30%|███       | 61/200 [02:04<07:14,  3.12s/it]Running Inference:  31%|███       | 62/200 [02:07<07:10,  3.12s/it]Running Inference:  32%|███▏      | 63/200 [02:08<05:19,  2.33s/it]Running Inference:  32%|███▏      | 64/200 [02:08<04:18,  1.90s/it]Running Inference:  32%|███▎      | 65/200 [02:12<05:25,  2.41s/it]Running Inference:  33%|███▎      | 66/200 [02:14<05:14,  2.35s/it]Running Inference:  34%|███▎      | 67/200 [02:16<04:55,  2.22s/it]Running Inference:  34%|███▍      | 68/200 [02:19<05:21,  2.43s/it]Running Inference:  34%|███▍      | 69/200 [02:22<05:17,  2.42s/it]Running Inference:  35%|███▌      | 70/200 [02:24<05:23,  2.49s/it]Running Inference:  36%|███▌      | 71/200 [02:27<05:35,  2.60s/it]Running Inference:  36%|███▌      | 72/200 [02:31<06:38,  3.11s/it]Running Inference:  36%|███▋      | 73/200 [02:33<05:31,  2.61s/it]Running Inference:  37%|███▋      | 74/200 [02:36<05:32,  2.64s/it]Running Inference:  38%|███▊      | 75/200 [02:36<04:07,  1.98s/it]Running Inference:  38%|███▊      | 76/200 [02:38<04:04,  1.97s/it]Running Inference:  38%|███▊      | 77/200 [02:40<04:25,  2.16s/it]Running Inference:  39%|███▉      | 78/200 [02:41<03:15,  1.61s/it]Running Inference:  40%|███▉      | 79/200 [02:43<03:29,  1.73s/it]Running Inference:  40%|████      | 80/200 [02:45<03:41,  1.85s/it]Running Inference:  40%|████      | 81/200 [02:48<04:14,  2.14s/it]Running Inference:  41%|████      | 82/200 [02:48<03:13,  1.64s/it]Running Inference:  42%|████▏     | 83/200 [02:52<04:16,  2.20s/it]Running Inference:  42%|████▏     | 84/200 [02:55<04:42,  2.44s/it]Running Inference:  42%|████▎     | 85/200 [02:56<04:07,  2.15s/it]Running Inference:  43%|████▎     | 86/200 [03:00<05:00,  2.64s/it]Running Inference:  44%|████▎     | 87/200 [03:03<05:18,  2.82s/it]Running Inference:  44%|████▍     | 88/200 [03:05<04:41,  2.51s/it]Running Inference:  44%|████▍     | 89/200 [03:06<03:52,  2.10s/it]Running Inference:  45%|████▌     | 90/200 [03:08<03:37,  1.98s/it]Running Inference:  46%|████▌     | 91/200 [03:12<04:39,  2.56s/it]Running Inference:  46%|████▌     | 92/200 [03:12<03:35,  2.00s/it]Running Inference:  46%|████▋     | 93/200 [03:15<04:02,  2.27s/it]Running Inference:  47%|████▋     | 94/200 [03:17<03:46,  2.13s/it]Running Inference:  48%|████▊     | 95/200 [03:22<05:00,  2.86s/it]Running Inference:  48%|████▊     | 96/200 [03:24<04:40,  2.70s/it]Running Inference:  48%|████▊     | 97/200 [03:27<04:57,  2.89s/it]Running Inference:  49%|████▉     | 98/200 [03:30<04:36,  2.71s/it]Running Inference:  50%|████▉     | 99/200 [03:33<04:52,  2.90s/it]Running Inference:  50%|█████     | 100/200 [03:34<03:59,  2.40s/it]Running Inference:  50%|█████     | 101/200 [03:37<03:55,  2.37s/it]Running Inference:  51%|█████     | 102/200 [03:39<03:50,  2.36s/it]Running Inference:  52%|█████▏    | 103/200 [03:41<03:39,  2.26s/it]Running Inference:  52%|█████▏    | 104/200 [03:42<03:11,  1.99s/it]Running Inference:  52%|█████▎    | 105/200 [03:45<03:19,  2.10s/it]Running Inference:  53%|█████▎    | 106/200 [03:48<03:44,  2.39s/it]Running Inference:  54%|█████▎    | 107/200 [03:52<04:34,  2.95s/it]Running Inference:  54%|█████▍    | 108/200 [03:53<03:40,  2.40s/it]Running Inference:  55%|█████▍    | 109/200 [03:54<02:57,  1.95s/it]Running Inference:  55%|█████▌    | 110/200 [03:55<02:17,  1.53s/it]Running Inference:  56%|█████▌    | 111/200 [03:57<02:30,  1.69s/it]Running Inference:  56%|█████▌    | 112/200 [04:00<03:11,  2.18s/it]Running Inference:  56%|█████▋    | 113/200 [04:03<03:33,  2.46s/it]Running Inference:  57%|█████▋    | 114/200 [04:08<04:29,  3.14s/it]Running Inference:  57%|█████▊    | 115/200 [04:10<03:51,  2.73s/it]Running Inference:  58%|█████▊    | 116/200 [04:10<02:50,  2.03s/it]Running Inference:  58%|█████▊    | 117/200 [04:13<03:25,  2.47s/it]Running Inference:  59%|█████▉    | 118/200 [04:16<03:32,  2.59s/it]Running Inference:  60%|█████▉    | 119/200 [04:18<03:02,  2.25s/it]Running Inference:  60%|██████    | 120/200 [04:20<02:55,  2.20s/it]Running Inference:  60%|██████    | 121/200 [04:20<02:10,  1.65s/it]Running Inference:  61%|██████    | 122/200 [04:22<02:07,  1.64s/it]Running Inference:  62%|██████▏   | 123/200 [04:22<01:38,  1.28s/it]Running Inference:  62%|██████▏   | 124/200 [04:23<01:25,  1.13s/it]Running Inference:  62%|██████▎   | 125/200 [04:25<01:43,  1.38s/it]Running Inference:  63%|██████▎   | 126/200 [04:28<02:10,  1.76s/it]Running Inference:  64%|██████▎   | 127/200 [04:30<02:26,  2.00s/it]Running Inference:  64%|██████▍   | 128/200 [04:31<01:57,  1.63s/it]Running Inference:  64%|██████▍   | 129/200 [04:32<01:42,  1.45s/it]Running Inference:  65%|██████▌   | 130/200 [04:33<01:26,  1.23s/it]Running Inference:  66%|██████▌   | 131/200 [04:34<01:34,  1.37s/it]Running Inference:  66%|██████▌   | 132/200 [04:36<01:44,  1.54s/it]Running Inference:  66%|██████▋   | 133/200 [04:39<02:12,  1.97s/it]Running Inference:  67%|██████▋   | 134/200 [04:41<02:08,  1.95s/it]Running Inference:  68%|██████▊   | 135/200 [04:44<02:30,  2.31s/it]Running Inference:  68%|██████▊   | 136/200 [04:48<02:45,  2.59s/it]Running Inference:  68%|██████▊   | 137/200 [04:49<02:24,  2.29s/it]Running Inference:  69%|██████▉   | 138/200 [04:52<02:33,  2.47s/it]Running Inference:  70%|██████▉   | 139/200 [04:55<02:33,  2.51s/it]Running Inference:  70%|███████   | 140/200 [04:57<02:22,  2.37s/it]Running Inference:  70%|███████   | 141/200 [04:59<02:22,  2.41s/it]Running Inference:  71%|███████   | 142/200 [05:02<02:25,  2.52s/it]Running Inference:  72%|███████▏  | 143/200 [05:05<02:26,  2.56s/it]Running Inference:  72%|███████▏  | 144/200 [05:06<02:05,  2.23s/it]Running Inference:  72%|███████▎  | 145/200 [05:07<01:47,  1.96s/it]Running Inference:  73%|███████▎  | 146/200 [05:10<01:55,  2.14s/it]Running Inference:  74%|███████▎  | 147/200 [05:11<01:27,  1.66s/it]Running Inference:  74%|███████▍  | 148/200 [05:15<02:02,  2.35s/it]Running Inference:  74%|███████▍  | 149/200 [05:18<02:21,  2.78s/it]Running Inference:  75%|███████▌  | 150/200 [05:19<01:50,  2.22s/it]Running Inference:  76%|███████▌  | 151/200 [05:23<02:12,  2.70s/it]Running Inference:  76%|███████▌  | 152/200 [05:24<01:43,  2.16s/it]Running Inference:  76%|███████▋  | 153/200 [05:27<01:46,  2.28s/it]Running Inference:  77%|███████▋  | 154/200 [05:30<01:55,  2.51s/it]Running Inference:  78%|███████▊  | 155/200 [05:33<02:10,  2.89s/it]Running Inference:  78%|███████▊  | 156/200 [05:36<02:01,  2.76s/it]Running Inference:  78%|███████▊  | 157/200 [05:40<02:22,  3.31s/it]Running Inference:  79%|███████▉  | 158/200 [05:42<01:59,  2.86s/it]Running Inference:  80%|███████▉  | 159/200 [05:44<01:38,  2.41s/it]Running Inference:  80%|████████  | 160/200 [05:45<01:20,  2.00s/it]Running Inference:  80%|████████  | 161/200 [05:46<01:15,  1.93s/it]Running Inference:  81%|████████  | 162/200 [05:48<01:11,  1.88s/it]Running Inference:  82%|████████▏ | 163/200 [05:50<01:12,  1.95s/it]Running Inference:  82%|████████▏ | 164/200 [05:51<00:55,  1.55s/it]Running Inference:  82%|████████▎ | 165/200 [05:55<01:17,  2.22s/it]Running Inference:  83%|████████▎ | 166/200 [05:57<01:13,  2.17s/it]Running Inference:  84%|████████▎ | 167/200 [05:58<01:02,  1.91s/it]Running Inference:  84%|████████▍ | 168/200 [06:01<01:12,  2.25s/it]Running Inference:  84%|████████▍ | 169/200 [06:02<00:57,  1.87s/it]Running Inference:  85%|████████▌ | 170/200 [06:03<00:45,  1.52s/it]Running Inference:  86%|████████▌ | 171/200 [06:04<00:39,  1.35s/it]Running Inference:  86%|████████▌ | 172/200 [06:05<00:41,  1.48s/it]Running Inference:  86%|████████▋ | 173/200 [06:07<00:41,  1.53s/it]Running Inference:  87%|████████▋ | 174/200 [06:09<00:42,  1.64s/it]Running Inference:  88%|████████▊ | 175/200 [06:10<00:36,  1.44s/it]Running Inference:  88%|████████▊ | 176/200 [06:11<00:32,  1.33s/it]Running Inference:  88%|████████▊ | 177/200 [06:13<00:35,  1.54s/it]Running Inference:  89%|████████▉ | 178/200 [06:16<00:41,  1.88s/it]Running Inference:  90%|████████▉ | 179/200 [06:17<00:32,  1.57s/it]Running Inference:  90%|█████████ | 180/200 [06:18<00:29,  1.48s/it]Running Inference:  90%|█████████ | 181/200 [06:19<00:26,  1.41s/it]Running Inference:  91%|█████████ | 182/200 [06:20<00:22,  1.25s/it]Running Inference:  92%|█████████▏| 183/200 [06:25<00:38,  2.27s/it]Running Inference:  92%|█████████▏| 184/200 [06:28<00:40,  2.51s/it]Running Inference:  92%|█████████▎| 185/200 [06:29<00:30,  2.03s/it]Running Inference:  93%|█████████▎| 186/200 [06:32<00:34,  2.50s/it]Running Inference:  94%|█████████▎| 187/200 [06:36<00:36,  2.80s/it]Running Inference:  94%|█████████▍| 188/200 [06:39<00:36,  3.02s/it]Running Inference:  94%|█████████▍| 189/200 [06:40<00:24,  2.24s/it]Running Inference:  95%|█████████▌| 190/200 [06:41<00:21,  2.10s/it]Running Inference:  96%|█████████▌| 191/200 [06:43<00:18,  2.03s/it]Running Inference:  96%|█████████▌| 192/200 [06:46<00:16,  2.08s/it]Running Inference:  96%|█████████▋| 193/200 [06:47<00:14,  2.01s/it]Running Inference:  97%|█████████▋| 194/200 [06:50<00:13,  2.18s/it]Running Inference:  98%|█████████▊| 195/200 [06:53<00:11,  2.30s/it]Running Inference:  98%|█████████▊| 196/200 [06:54<00:07,  1.90s/it]Running Inference:  98%|█████████▊| 197/200 [06:57<00:07,  2.39s/it]Running Inference:  99%|█████████▉| 198/200 [07:01<00:05,  2.76s/it]Running Inference: 100%|█████████▉| 199/200 [07:01<00:02,  2.10s/it]Running Inference: 100%|██████████| 200/200 [07:02<00:00,  1.78s/it]Running Inference: 100%|██████████| 200/200 [07:02<00:00,  2.11s/it]
2025-12-13 21:05:38,879 - INFO - Inference completed.
2025-12-13 21:05:38,902 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 21:05:38,902 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:05:38,937 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 21:05:38,937 - INFO - Metrics:
39.26
2025-12-13 21:05:38,939 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=triviaqa, ratio=0.1) on GPU 2

----------------------------------------
Task: triviaqa | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:05:45,353 - INFO - Set deterministic seeds to 42
2025-12-13 21:05:45,353 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:05:45,353 - INFO - Starting evaluation run...
2025-12-13 21:05:45,354 - INFO - Output directory set to: longbenchresult
2025-12-13 21:05:45,354 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 21:05:45,354 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 21:05:45,354 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:05:45,354 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.01it/s]
Device set to use cuda:0
2025-12-13 21:06:32,887 - INFO - Model pipeline loaded.
2025-12-13 21:06:32,887 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 21:06:40,056 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:06:40,056 - INFO - Dataset processed with 200 entries.
2025-12-13 21:06:40,084 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:42,  2.93s/it]Running Inference:   1%|          | 2/200 [00:04<07:17,  2.21s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:27,  2.57s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:14,  2.53s/it]Running Inference:   2%|▎         | 5/200 [00:10<05:50,  1.79s/it]Running Inference:   3%|▎         | 6/200 [00:11<04:42,  1.46s/it]Running Inference:   4%|▎         | 7/200 [00:11<03:32,  1.10s/it]Running Inference:   4%|▍         | 8/200 [00:14<05:03,  1.58s/it]Running Inference:   4%|▍         | 9/200 [00:15<04:47,  1.51s/it]Running Inference:   5%|▌         | 10/200 [00:17<04:53,  1.54s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:19<05:26,  1.73s/it]Running Inference:   6%|▌         | 12/200 [00:22<06:54,  2.20s/it]Running Inference:   6%|▋         | 13/200 [00:23<05:52,  1.89s/it]Running Inference:   7%|▋         | 14/200 [00:29<09:23,  3.03s/it]Running Inference:   8%|▊         | 15/200 [00:31<08:44,  2.84s/it]Running Inference:   8%|▊         | 16/200 [00:34<08:47,  2.87s/it]Running Inference:   8%|▊         | 17/200 [00:35<06:45,  2.22s/it]Running Inference:   9%|▉         | 18/200 [00:36<05:20,  1.76s/it]Running Inference:  10%|▉         | 19/200 [00:39<06:27,  2.14s/it]Running Inference:  10%|█         | 20/200 [00:40<05:51,  1.96s/it]Running Inference:  10%|█         | 21/200 [00:43<06:20,  2.12s/it]Running Inference:  11%|█         | 22/200 [00:44<05:40,  1.91s/it]Running Inference:  12%|█▏        | 23/200 [00:47<06:19,  2.14s/it]Running Inference:  12%|█▏        | 24/200 [00:49<06:21,  2.17s/it]Running Inference:  12%|█▎        | 25/200 [00:51<05:51,  2.01s/it]Running Inference:  13%|█▎        | 26/200 [00:52<05:15,  1.81s/it]Running Inference:  14%|█▎        | 27/200 [00:53<04:09,  1.44s/it]Running Inference:  14%|█▍        | 28/200 [00:54<03:56,  1.37s/it]Running Inference:  14%|█▍        | 29/200 [00:56<04:30,  1.58s/it]Running Inference:  15%|█▌        | 30/200 [00:59<05:17,  1.87s/it]Running Inference:  16%|█▌        | 31/200 [01:00<04:54,  1.74s/it]Running Inference:  16%|█▌        | 32/200 [01:04<06:42,  2.40s/it]Running Inference:  16%|█▋        | 33/200 [01:06<06:12,  2.23s/it]Running Inference:  17%|█▋        | 34/200 [01:07<05:29,  1.98s/it]Running Inference:  18%|█▊        | 35/200 [01:08<04:50,  1.76s/it]Running Inference:  18%|█▊        | 36/200 [01:13<06:53,  2.52s/it]Running Inference:  18%|█▊        | 37/200 [01:16<07:28,  2.75s/it]Running Inference:  19%|█▉        | 38/200 [01:21<09:06,  3.38s/it]Running Inference:  20%|█▉        | 39/200 [01:21<06:48,  2.53s/it]Running Inference:  20%|██        | 40/200 [01:25<07:16,  2.73s/it]Running Inference:  20%|██        | 41/200 [01:26<06:11,  2.34s/it]Running Inference:  21%|██        | 42/200 [01:29<06:27,  2.45s/it]Running Inference:  22%|██▏       | 43/200 [01:29<04:49,  1.85s/it]Running Inference:  22%|██▏       | 44/200 [01:31<04:43,  1.81s/it]Running Inference:  22%|██▎       | 45/200 [01:31<03:36,  1.40s/it]Running Inference:  23%|██▎       | 46/200 [01:34<04:49,  1.88s/it]Running Inference:  24%|██▎       | 47/200 [01:35<04:10,  1.64s/it]Running Inference:  24%|██▍       | 48/200 [01:39<05:20,  2.11s/it]Running Inference:  24%|██▍       | 49/200 [01:41<05:21,  2.13s/it]Running Inference:  25%|██▌       | 50/200 [01:44<05:47,  2.32s/it]Running Inference:  26%|██▌       | 51/200 [01:44<04:34,  1.84s/it]Running Inference:  26%|██▌       | 52/200 [01:47<04:48,  1.95s/it]Running Inference:  26%|██▋       | 53/200 [01:47<03:41,  1.51s/it]Running Inference:  27%|██▋       | 54/200 [01:48<02:58,  1.22s/it]Running Inference:  28%|██▊       | 55/200 [01:49<02:53,  1.19s/it]Running Inference:  28%|██▊       | 56/200 [01:51<03:42,  1.54s/it]Running Inference:  28%|██▊       | 57/200 [01:54<04:25,  1.86s/it]Running Inference:  29%|██▉       | 58/200 [01:55<04:18,  1.82s/it]Running Inference:  30%|██▉       | 59/200 [02:00<06:04,  2.58s/it]Running Inference:  30%|███       | 60/200 [02:02<05:27,  2.34s/it]Running Inference:  30%|███       | 61/200 [02:06<07:09,  3.09s/it]Running Inference:  31%|███       | 62/200 [02:10<07:34,  3.29s/it]Running Inference:  32%|███▏      | 63/200 [02:13<06:59,  3.06s/it]Running Inference:  32%|███▏      | 64/200 [02:14<05:28,  2.41s/it]Running Inference:  32%|███▎      | 65/200 [02:18<06:44,  3.00s/it]Running Inference:  33%|███▎      | 66/200 [02:22<07:31,  3.37s/it]Running Inference:  34%|███▎      | 67/200 [02:24<06:30,  2.94s/it]Running Inference:  34%|███▍      | 68/200 [02:27<06:31,  2.96s/it]Running Inference:  34%|███▍      | 69/200 [02:30<06:06,  2.80s/it]Running Inference:  35%|███▌      | 70/200 [02:32<05:52,  2.71s/it]Running Inference:  36%|███▌      | 71/200 [02:35<05:54,  2.75s/it]Running Inference:  36%|███▌      | 72/200 [02:39<06:54,  3.24s/it]Running Inference:  36%|███▋      | 73/200 [02:41<05:58,  2.83s/it]Running Inference:  37%|███▋      | 74/200 [02:44<05:51,  2.79s/it]Running Inference:  38%|███▊      | 75/200 [02:44<04:21,  2.09s/it]Running Inference:  38%|███▊      | 76/200 [02:46<04:13,  2.05s/it]Running Inference:  38%|███▊      | 77/200 [02:49<04:35,  2.24s/it]Running Inference:  39%|███▉      | 78/200 [02:49<03:23,  1.67s/it]Running Inference:  40%|███▉      | 79/200 [02:51<03:34,  1.77s/it]Running Inference:  40%|████      | 80/200 [02:53<03:45,  1.88s/it]Running Inference:  40%|████      | 81/200 [02:56<04:17,  2.16s/it]Running Inference:  41%|████      | 82/200 [02:57<03:15,  1.65s/it]Running Inference:  42%|████▏     | 83/200 [03:00<04:20,  2.22s/it]Running Inference:  42%|████▏     | 84/200 [03:03<04:44,  2.45s/it]Running Inference:  42%|████▎     | 85/200 [03:05<04:08,  2.16s/it]Running Inference:  43%|████▎     | 86/200 [03:08<05:01,  2.65s/it]Running Inference:  44%|████▎     | 87/200 [03:12<05:12,  2.76s/it]Running Inference:  44%|████▍     | 88/200 [03:15<05:43,  3.07s/it]Running Inference:  44%|████▍     | 89/200 [03:19<05:46,  3.12s/it]Running Inference:  45%|████▌     | 90/200 [03:20<04:56,  2.70s/it]Running Inference:  46%|████▌     | 91/200 [03:24<05:37,  3.10s/it]Running Inference:  46%|████▌     | 92/200 [03:25<04:16,  2.37s/it]Running Inference:  46%|████▋     | 93/200 [03:28<04:31,  2.53s/it]Running Inference:  47%|████▋     | 94/200 [03:30<04:05,  2.32s/it]Running Inference:  48%|████▊     | 95/200 [03:34<05:14,  3.00s/it]Running Inference:  48%|████▊     | 96/200 [03:37<04:49,  2.78s/it]Running Inference:  48%|████▊     | 97/200 [03:40<05:06,  2.98s/it]Running Inference:  49%|████▉     | 98/200 [03:42<04:45,  2.80s/it]Running Inference:  50%|████▉     | 99/200 [03:46<05:01,  2.99s/it]Running Inference:  50%|█████     | 100/200 [03:49<05:07,  3.07s/it]Running Inference:  50%|█████     | 101/200 [03:51<04:41,  2.85s/it]Running Inference:  51%|█████     | 102/200 [03:54<04:23,  2.69s/it]Running Inference:  52%|█████▏    | 103/200 [03:56<04:01,  2.49s/it]Running Inference:  52%|█████▏    | 104/200 [03:59<04:14,  2.65s/it]Running Inference:  52%|█████▎    | 105/200 [04:01<04:03,  2.56s/it]Running Inference:  53%|█████▎    | 106/200 [04:04<04:17,  2.74s/it]Running Inference:  54%|█████▎    | 107/200 [04:09<04:58,  3.21s/it]Running Inference:  54%|█████▍    | 108/200 [04:10<03:57,  2.58s/it]Running Inference:  55%|█████▍    | 109/200 [04:11<03:09,  2.08s/it]Running Inference:  55%|█████▌    | 110/200 [04:11<02:25,  1.62s/it]Running Inference:  56%|█████▌    | 111/200 [04:13<02:42,  1.82s/it]Running Inference:  56%|█████▌    | 112/200 [04:17<03:21,  2.30s/it]Running Inference:  56%|█████▋    | 113/200 [04:20<03:43,  2.57s/it]Running Inference:  57%|█████▋    | 114/200 [04:25<04:37,  3.23s/it]Running Inference:  57%|█████▊    | 115/200 [04:27<03:56,  2.79s/it]Running Inference:  58%|█████▊    | 116/200 [04:27<02:53,  2.07s/it]Running Inference:  58%|█████▊    | 117/200 [04:31<03:30,  2.53s/it]Running Inference:  59%|█████▉    | 118/200 [04:32<03:04,  2.25s/it]Running Inference:  60%|█████▉    | 119/200 [04:34<02:43,  2.01s/it]Running Inference:  60%|██████    | 120/200 [04:37<03:17,  2.47s/it]Running Inference:  60%|██████    | 121/200 [04:38<02:25,  1.84s/it]Running Inference:  61%|██████    | 122/200 [04:39<02:18,  1.77s/it]Running Inference:  62%|██████▏   | 123/200 [04:40<01:46,  1.38s/it]Running Inference:  62%|██████▏   | 124/200 [04:40<01:30,  1.20s/it]Running Inference:  62%|██████▎   | 125/200 [04:42<01:47,  1.43s/it]Running Inference:  63%|██████▎   | 126/200 [04:45<02:14,  1.82s/it]Running Inference:  64%|██████▎   | 127/200 [04:48<02:29,  2.05s/it]Running Inference:  64%|██████▍   | 128/200 [04:48<01:59,  1.66s/it]Running Inference:  64%|██████▍   | 129/200 [04:49<01:44,  1.47s/it]Running Inference:  65%|██████▌   | 130/200 [04:50<01:27,  1.25s/it]Running Inference:  66%|██████▌   | 131/200 [04:54<02:10,  1.89s/it]Running Inference:  66%|██████▌   | 132/200 [04:56<02:11,  1.94s/it]Running Inference:  66%|██████▋   | 133/200 [04:59<02:32,  2.27s/it]Running Inference:  67%|██████▋   | 134/200 [05:01<02:22,  2.16s/it]Running Inference:  68%|██████▊   | 135/200 [05:04<02:41,  2.48s/it]Running Inference:  68%|██████▊   | 136/200 [05:07<02:53,  2.71s/it]Running Inference:  68%|██████▊   | 137/200 [05:09<02:29,  2.37s/it]Running Inference:  69%|██████▉   | 138/200 [05:11<02:36,  2.53s/it]Running Inference:  70%|██████▉   | 139/200 [05:14<02:35,  2.55s/it]Running Inference:  70%|███████   | 140/200 [05:16<02:23,  2.40s/it]Running Inference:  70%|███████   | 141/200 [05:19<02:22,  2.42s/it]Running Inference:  71%|███████   | 142/200 [05:21<02:26,  2.53s/it]Running Inference:  72%|███████▏  | 143/200 [05:24<02:27,  2.59s/it]Running Inference:  72%|███████▏  | 144/200 [05:26<02:07,  2.28s/it]Running Inference:  72%|███████▎  | 145/200 [05:27<01:56,  2.12s/it]Running Inference:  73%|███████▎  | 146/200 [05:30<02:01,  2.25s/it]Running Inference:  74%|███████▎  | 147/200 [05:31<01:33,  1.77s/it]Running Inference:  74%|███████▍  | 148/200 [05:35<02:06,  2.44s/it]Running Inference:  74%|███████▍  | 149/200 [05:40<02:50,  3.34s/it]Running Inference:  75%|███████▌  | 150/200 [05:41<02:10,  2.61s/it]Running Inference:  76%|███████▌  | 151/200 [05:45<02:25,  2.96s/it]Running Inference:  76%|███████▌  | 152/200 [05:46<01:57,  2.45s/it]Running Inference:  76%|███████▋  | 153/200 [05:49<01:56,  2.48s/it]Running Inference:  77%|███████▋  | 154/200 [05:52<02:02,  2.66s/it]Running Inference:  78%|███████▊  | 155/200 [05:56<02:15,  3.02s/it]Running Inference:  78%|███████▊  | 156/200 [05:58<02:04,  2.82s/it]Running Inference:  78%|███████▊  | 157/200 [06:01<01:59,  2.79s/it]Running Inference:  79%|███████▉  | 158/200 [06:02<01:44,  2.49s/it]Running Inference:  80%|███████▉  | 159/200 [06:04<01:28,  2.15s/it]Running Inference:  80%|████████  | 160/200 [06:05<01:12,  1.82s/it]Running Inference:  80%|████████  | 161/200 [06:07<01:10,  1.81s/it]Running Inference:  81%|████████  | 162/200 [06:08<01:08,  1.80s/it]Running Inference:  82%|████████▏ | 163/200 [06:10<01:10,  1.90s/it]Running Inference:  82%|████████▏ | 164/200 [06:11<00:55,  1.54s/it]Running Inference:  82%|████████▎ | 165/200 [06:15<01:18,  2.24s/it]Running Inference:  83%|████████▎ | 166/200 [06:17<01:14,  2.19s/it]Running Inference:  84%|████████▎ | 167/200 [06:18<01:03,  1.92s/it]Running Inference:  84%|████████▍ | 168/200 [06:21<01:12,  2.26s/it]Running Inference:  84%|████████▍ | 169/200 [06:23<01:00,  1.97s/it]Running Inference:  85%|████████▌ | 170/200 [06:24<00:48,  1.61s/it]Running Inference:  86%|████████▌ | 171/200 [06:24<00:40,  1.41s/it]Running Inference:  86%|████████▌ | 172/200 [06:26<00:42,  1.52s/it]Running Inference:  86%|████████▋ | 173/200 [06:28<00:42,  1.56s/it]Running Inference:  87%|████████▋ | 174/200 [06:30<00:43,  1.66s/it]Running Inference:  88%|████████▊ | 175/200 [06:31<00:41,  1.66s/it]Running Inference:  88%|████████▊ | 176/200 [06:33<00:35,  1.49s/it]Running Inference:  88%|████████▊ | 177/200 [06:35<00:38,  1.65s/it]Running Inference:  89%|████████▉ | 178/200 [06:37<00:42,  1.95s/it]Running Inference:  90%|████████▉ | 179/200 [06:40<00:45,  2.16s/it]Running Inference:  90%|█████████ | 180/200 [06:41<00:37,  1.89s/it]Running Inference:  90%|█████████ | 181/200 [06:42<00:32,  1.70s/it]Running Inference:  91%|█████████ | 182/200 [06:43<00:26,  1.45s/it]Running Inference:  92%|█████████▏| 183/200 [06:48<00:41,  2.42s/it]Running Inference:  92%|█████████▏| 184/200 [06:51<00:41,  2.62s/it]Running Inference:  92%|█████████▎| 185/200 [06:52<00:31,  2.10s/it]Running Inference:  93%|█████████▎| 186/200 [06:56<00:36,  2.57s/it]Running Inference:  94%|█████████▎| 187/200 [06:59<00:37,  2.88s/it]Running Inference:  94%|█████████▍| 188/200 [07:03<00:36,  3.08s/it]Running Inference:  94%|█████████▍| 189/200 [07:03<00:25,  2.29s/it]Running Inference:  95%|█████████▌| 190/200 [07:05<00:21,  2.13s/it]Running Inference:  96%|█████████▌| 191/200 [07:07<00:18,  2.05s/it]Running Inference:  96%|█████████▌| 192/200 [07:09<00:16,  2.09s/it]Running Inference:  96%|█████████▋| 193/200 [07:11<00:13,  1.93s/it]Running Inference:  97%|█████████▋| 194/200 [07:13<00:12,  2.12s/it]Running Inference:  98%|█████████▊| 195/200 [07:16<00:11,  2.26s/it]Running Inference:  98%|█████████▊| 196/200 [07:17<00:07,  1.88s/it]Running Inference:  98%|█████████▊| 197/200 [07:20<00:07,  2.39s/it]Running Inference:  99%|█████████▉| 198/200 [07:24<00:05,  2.78s/it]Running Inference: 100%|█████████▉| 199/200 [07:25<00:02,  2.35s/it]Running Inference: 100%|██████████| 200/200 [07:26<00:00,  1.96s/it]Running Inference: 100%|██████████| 200/200 [07:26<00:00,  2.23s/it]
2025-12-13 21:14:06,979 - INFO - Inference completed.
2025-12-13 21:14:07,003 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 21:14:07,003 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:14:07,039 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 21:14:07,039 - INFO - Metrics:
37.5
2025-12-13 21:14:07,040 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=triviaqa, ratio=0.2) on GPU 2

----------------------------------------
Task: triviaqa | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:14:13,416 - INFO - Set deterministic seeds to 42
2025-12-13 21:14:13,416 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:14:13,416 - INFO - Starting evaluation run...
2025-12-13 21:14:13,416 - INFO - Output directory set to: longbenchresult
2025-12-13 21:14:13,416 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 21:14:13,416 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 21:14:13,416 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:14:13,416 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.65it/s]
Device set to use cuda:0
2025-12-13 21:14:30,027 - INFO - Model pipeline loaded.
2025-12-13 21:14:30,027 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 21:14:40,869 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:14:40,869 - INFO - Dataset processed with 200 entries.
2025-12-13 21:14:40,899 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:52,  2.98s/it]Running Inference:   1%|          | 2/200 [00:04<07:20,  2.23s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:22,  2.55s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:11,  2.51s/it]Running Inference:   2%|▎         | 5/200 [00:10<05:47,  1.78s/it]Running Inference:   3%|▎         | 6/200 [00:11<04:40,  1.45s/it]Running Inference:   4%|▎         | 7/200 [00:11<03:31,  1.09s/it]Running Inference:   4%|▍         | 8/200 [00:14<04:58,  1.55s/it]Running Inference:   4%|▍         | 9/200 [00:15<04:43,  1.48s/it]Running Inference:   5%|▌         | 10/200 [00:17<04:50,  1.53s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:19<05:21,  1.70s/it]Running Inference:   6%|▌         | 12/200 [00:22<06:49,  2.18s/it]Running Inference:   6%|▋         | 13/200 [00:23<05:59,  1.92s/it]Running Inference:   7%|▋         | 14/200 [00:28<08:47,  2.84s/it]Running Inference:   8%|▊         | 15/200 [00:31<08:18,  2.70s/it]Running Inference:   8%|▊         | 16/200 [00:34<08:29,  2.77s/it]Running Inference:   8%|▊         | 17/200 [00:34<06:32,  2.14s/it]Running Inference:   9%|▉         | 18/200 [00:35<05:10,  1.71s/it]Running Inference:  10%|▉         | 19/200 [00:38<06:19,  2.10s/it]Running Inference:  10%|█         | 20/200 [00:40<05:46,  1.93s/it]Running Inference:  10%|█         | 21/200 [00:42<06:13,  2.08s/it]Running Inference:  11%|█         | 22/200 [00:43<05:34,  1.88s/it]Running Inference:  12%|█▏        | 23/200 [00:47<06:46,  2.29s/it]Running Inference:  12%|█▏        | 24/200 [00:49<06:39,  2.27s/it]Running Inference:  12%|█▎        | 25/200 [00:51<06:06,  2.09s/it]Running Inference:  13%|█▎        | 26/200 [00:52<05:24,  1.86s/it]Running Inference:  14%|█▎        | 27/200 [00:53<04:15,  1.48s/it]Running Inference:  14%|█▍        | 28/200 [00:53<03:33,  1.24s/it]Running Inference:  14%|█▍        | 29/200 [00:55<04:13,  1.48s/it]Running Inference:  15%|█▌        | 30/200 [00:58<05:02,  1.78s/it]Running Inference:  16%|█▌        | 31/200 [00:59<04:44,  1.68s/it]Running Inference:  16%|█▌        | 32/200 [01:03<06:31,  2.33s/it]Running Inference:  16%|█▋        | 33/200 [01:05<06:03,  2.17s/it]Running Inference:  17%|█▋        | 34/200 [01:06<05:22,  1.94s/it]Running Inference:  18%|█▊        | 35/200 [01:07<04:45,  1.73s/it]Running Inference:  18%|█▊        | 36/200 [01:12<06:45,  2.47s/it]Running Inference:  18%|█▊        | 37/200 [01:15<07:20,  2.70s/it]Running Inference:  19%|█▉        | 38/200 [01:20<08:56,  3.31s/it]Running Inference:  20%|█▉        | 39/200 [01:20<06:54,  2.57s/it]Running Inference:  20%|██        | 40/200 [01:24<07:20,  2.75s/it]Running Inference:  20%|██        | 41/200 [01:25<06:14,  2.35s/it]Running Inference:  21%|██        | 42/200 [01:28<06:26,  2.44s/it]Running Inference:  22%|██▏       | 43/200 [01:29<05:06,  1.95s/it]Running Inference:  22%|██▏       | 44/200 [01:30<04:54,  1.89s/it]Running Inference:  22%|██▎       | 45/200 [01:31<03:44,  1.45s/it]Running Inference:  23%|██▎       | 46/200 [01:34<04:51,  1.89s/it]Running Inference:  24%|██▎       | 47/200 [01:35<04:16,  1.67s/it]Running Inference:  24%|██▍       | 48/200 [01:38<05:22,  2.12s/it]Running Inference:  24%|██▍       | 49/200 [01:40<05:21,  2.13s/it]Running Inference:  25%|██▌       | 50/200 [01:43<05:47,  2.31s/it]Running Inference:  26%|██▌       | 51/200 [01:44<04:33,  1.84s/it]Running Inference:  26%|██▌       | 52/200 [01:46<04:47,  1.94s/it]Running Inference:  26%|██▋       | 53/200 [01:46<03:40,  1.50s/it]Running Inference:  27%|██▋       | 54/200 [01:47<02:56,  1.21s/it]Running Inference:  28%|██▊       | 55/200 [01:48<02:52,  1.19s/it]Running Inference:  28%|██▊       | 56/200 [01:50<03:43,  1.55s/it]Running Inference:  28%|██▊       | 57/200 [01:53<04:25,  1.86s/it]Running Inference:  29%|██▉       | 58/200 [01:55<04:18,  1.82s/it]Running Inference:  30%|██▉       | 59/200 [01:58<05:13,  2.22s/it]Running Inference:  30%|███       | 60/200 [01:59<04:50,  2.07s/it]Running Inference:  30%|███       | 61/200 [02:04<06:39,  2.87s/it]Running Inference:  31%|███       | 62/200 [02:08<07:10,  3.12s/it]Running Inference:  32%|███▏      | 63/200 [02:10<06:40,  2.93s/it]Running Inference:  32%|███▏      | 64/200 [02:11<05:14,  2.32s/it]Running Inference:  32%|███▎      | 65/200 [02:16<06:32,  2.91s/it]Running Inference:  33%|███▎      | 66/200 [02:20<07:20,  3.28s/it]Running Inference:  34%|███▎      | 67/200 [02:22<06:22,  2.88s/it]Running Inference:  34%|███▍      | 68/200 [02:25<06:23,  2.90s/it]Running Inference:  34%|███▍      | 69/200 [02:28<06:43,  3.08s/it]Running Inference:  35%|███▌      | 70/200 [02:32<07:24,  3.42s/it]Running Inference:  36%|███▌      | 71/200 [02:35<06:59,  3.25s/it]Running Inference:  36%|███▌      | 72/200 [02:38<06:37,  3.11s/it]Running Inference:  36%|███▋      | 73/200 [02:40<05:46,  2.73s/it]Running Inference:  37%|███▋      | 74/200 [02:43<06:04,  2.89s/it]Running Inference:  38%|███▊      | 75/200 [02:44<04:30,  2.16s/it]Running Inference:  38%|███▊      | 76/200 [02:45<04:19,  2.09s/it]Running Inference:  38%|███▊      | 77/200 [02:48<04:38,  2.27s/it]Running Inference:  39%|███▉      | 78/200 [02:48<03:25,  1.68s/it]Running Inference:  40%|███▉      | 79/200 [02:50<03:35,  1.78s/it]Running Inference:  40%|████      | 80/200 [02:53<03:45,  1.88s/it]Running Inference:  40%|████      | 81/200 [02:55<04:17,  2.16s/it]Running Inference:  41%|████      | 82/200 [02:56<03:14,  1.65s/it]Running Inference:  42%|████▏     | 83/200 [02:59<04:17,  2.20s/it]Running Inference:  42%|████▏     | 84/200 [03:02<04:42,  2.43s/it]Running Inference:  42%|████▎     | 85/200 [03:04<04:06,  2.15s/it]Running Inference:  43%|████▎     | 86/200 [03:08<04:59,  2.62s/it]Running Inference:  44%|████▎     | 87/200 [03:11<05:16,  2.80s/it]Running Inference:  44%|████▍     | 88/200 [03:14<05:44,  3.07s/it]Running Inference:  44%|████▍     | 89/200 [03:18<05:44,  3.10s/it]Running Inference:  45%|████▌     | 90/200 [03:19<04:54,  2.68s/it]Running Inference:  46%|████▌     | 91/200 [03:23<05:34,  3.07s/it]Running Inference:  46%|████▌     | 92/200 [03:24<04:13,  2.35s/it]Running Inference:  46%|████▋     | 93/200 [03:27<04:28,  2.51s/it]Running Inference:  47%|████▋     | 94/200 [03:28<03:56,  2.24s/it]Running Inference:  48%|████▊     | 95/200 [03:33<05:05,  2.91s/it]Running Inference:  48%|████▊     | 96/200 [03:35<04:43,  2.73s/it]Running Inference:  48%|████▊     | 97/200 [03:39<05:00,  2.92s/it]Running Inference:  49%|████▉     | 98/200 [03:41<04:40,  2.75s/it]Running Inference:  50%|████▉     | 99/200 [03:44<04:56,  2.93s/it]Running Inference:  50%|█████     | 100/200 [03:48<05:01,  3.01s/it]Running Inference:  50%|█████     | 101/200 [03:50<04:38,  2.81s/it]Running Inference:  51%|█████     | 102/200 [03:52<04:20,  2.66s/it]Running Inference:  52%|█████▏    | 103/200 [03:54<03:59,  2.47s/it]Running Inference:  52%|█████▏    | 104/200 [03:57<04:10,  2.61s/it]Running Inference:  52%|█████▎    | 105/200 [03:59<04:00,  2.53s/it]Running Inference:  53%|█████▎    | 106/200 [04:03<04:13,  2.70s/it]Running Inference:  54%|█████▎    | 107/200 [04:07<04:54,  3.16s/it]Running Inference:  54%|█████▍    | 108/200 [04:08<03:54,  2.54s/it]Running Inference:  55%|█████▍    | 109/200 [04:09<03:06,  2.05s/it]Running Inference:  55%|█████▌    | 110/200 [04:09<02:24,  1.60s/it]Running Inference:  56%|█████▌    | 111/200 [04:12<02:48,  1.90s/it]Running Inference:  56%|█████▌    | 112/200 [04:15<03:25,  2.33s/it]Running Inference:  56%|█████▋    | 113/200 [04:19<03:46,  2.60s/it]Running Inference:  57%|█████▋    | 114/200 [04:23<04:37,  3.22s/it]Running Inference:  57%|█████▊    | 115/200 [04:25<03:56,  2.78s/it]Running Inference:  58%|█████▊    | 116/200 [04:25<02:53,  2.07s/it]Running Inference:  58%|█████▊    | 117/200 [04:29<03:28,  2.51s/it]Running Inference:  59%|█████▉    | 118/200 [04:32<03:38,  2.66s/it]Running Inference:  60%|█████▉    | 119/200 [04:33<03:06,  2.30s/it]Running Inference:  60%|██████    | 120/200 [04:37<03:31,  2.65s/it]Running Inference:  60%|██████    | 121/200 [04:37<02:35,  1.96s/it]Running Inference:  61%|██████    | 122/200 [04:39<02:28,  1.91s/it]Running Inference:  62%|██████▏   | 123/200 [04:39<01:53,  1.47s/it]Running Inference:  62%|██████▏   | 124/200 [04:40<01:35,  1.26s/it]Running Inference:  62%|██████▎   | 125/200 [04:42<01:49,  1.47s/it]Running Inference:  63%|██████▎   | 126/200 [04:45<02:15,  1.83s/it]Running Inference:  64%|██████▎   | 127/200 [04:47<02:28,  2.04s/it]Running Inference:  64%|██████▍   | 128/200 [04:48<01:57,  1.64s/it]Running Inference:  64%|██████▍   | 129/200 [04:49<01:43,  1.45s/it]Running Inference:  65%|██████▌   | 130/200 [04:50<01:26,  1.23s/it]Running Inference:  66%|██████▌   | 131/200 [04:53<02:08,  1.86s/it]Running Inference:  66%|██████▌   | 132/200 [04:55<02:10,  1.91s/it]Running Inference:  66%|██████▋   | 133/200 [04:57<02:07,  1.91s/it]Running Inference:  67%|██████▋   | 134/200 [04:59<02:10,  1.97s/it]Running Inference:  68%|██████▊   | 135/200 [05:02<02:31,  2.34s/it]Running Inference:  68%|██████▊   | 136/200 [05:06<02:46,  2.60s/it]Running Inference:  68%|██████▊   | 137/200 [05:07<02:26,  2.32s/it]Running Inference:  69%|██████▉   | 138/200 [05:11<02:42,  2.62s/it]Running Inference:  70%|██████▉   | 139/200 [05:15<03:14,  3.18s/it]Running Inference:  70%|███████   | 140/200 [05:17<02:50,  2.84s/it]Running Inference:  70%|███████   | 141/200 [05:20<02:41,  2.73s/it]Running Inference:  71%|███████   | 142/200 [05:22<02:38,  2.74s/it]Running Inference:  72%|███████▏  | 143/200 [05:25<02:35,  2.73s/it]Running Inference:  72%|███████▏  | 144/200 [05:27<02:12,  2.37s/it]Running Inference:  72%|███████▎  | 145/200 [05:28<01:59,  2.18s/it]Running Inference:  73%|███████▎  | 146/200 [05:31<02:03,  2.28s/it]Running Inference:  74%|███████▎  | 147/200 [05:31<01:33,  1.76s/it]Running Inference:  74%|███████▍  | 148/200 [05:35<02:05,  2.41s/it]Running Inference:  74%|███████▍  | 149/200 [05:39<02:23,  2.81s/it]Running Inference:  75%|███████▌  | 150/200 [05:40<01:51,  2.24s/it]Running Inference:  76%|███████▌  | 151/200 [05:42<01:42,  2.09s/it]Running Inference:  76%|███████▌  | 152/200 [05:44<01:49,  2.29s/it]Running Inference:  76%|███████▋  | 153/200 [05:47<01:51,  2.36s/it]Running Inference:  77%|███████▋  | 154/200 [05:50<01:55,  2.52s/it]Running Inference:  78%|███████▊  | 155/200 [05:52<01:43,  2.31s/it]Running Inference:  78%|███████▊  | 156/200 [05:54<01:42,  2.32s/it]Running Inference:  78%|███████▊  | 157/200 [05:58<02:01,  2.82s/it]Running Inference:  79%|███████▉  | 158/200 [06:00<01:45,  2.51s/it]Running Inference:  80%|███████▉  | 159/200 [06:01<01:28,  2.17s/it]Running Inference:  80%|████████  | 160/200 [06:02<01:13,  1.83s/it]Running Inference:  80%|████████  | 161/200 [06:04<01:10,  1.82s/it]Running Inference:  81%|████████  | 162/200 [06:06<01:08,  1.80s/it]Running Inference:  82%|████████▏ | 163/200 [06:08<01:10,  1.90s/it]Running Inference:  82%|████████▏ | 164/200 [06:09<00:55,  1.53s/it]Running Inference:  82%|████████▎ | 165/200 [06:12<01:17,  2.22s/it]Running Inference:  83%|████████▎ | 166/200 [06:16<01:32,  2.71s/it]Running Inference:  84%|████████▎ | 167/200 [06:18<01:15,  2.28s/it]Running Inference:  84%|████████▍ | 168/200 [06:20<01:18,  2.45s/it]Running Inference:  84%|████████▍ | 169/200 [06:22<01:05,  2.10s/it]Running Inference:  85%|████████▌ | 170/200 [06:24<01:07,  2.24s/it]Running Inference:  86%|████████▌ | 171/200 [06:25<00:53,  1.85s/it]Running Inference:  86%|████████▌ | 172/200 [06:27<00:51,  1.83s/it]Running Inference:  86%|████████▋ | 173/200 [06:29<00:47,  1.77s/it]Running Inference:  87%|████████▋ | 174/200 [06:30<00:46,  1.81s/it]Running Inference:  88%|████████▊ | 175/200 [06:32<00:40,  1.62s/it]Running Inference:  88%|████████▊ | 176/200 [06:33<00:34,  1.46s/it]Running Inference:  88%|████████▊ | 177/200 [06:35<00:37,  1.62s/it]Running Inference:  89%|████████▉ | 178/200 [06:37<00:42,  1.93s/it]Running Inference:  90%|████████▉ | 179/200 [06:40<00:44,  2.13s/it]Running Inference:  90%|█████████ | 180/200 [06:41<00:37,  1.87s/it]Running Inference:  90%|█████████ | 181/200 [06:42<00:31,  1.68s/it]Running Inference:  91%|█████████ | 182/200 [06:43<00:25,  1.43s/it]Running Inference:  92%|█████████▏| 183/200 [06:48<00:40,  2.39s/it]Running Inference:  92%|█████████▏| 184/200 [06:51<00:41,  2.59s/it]Running Inference:  92%|█████████▎| 185/200 [06:52<00:31,  2.08s/it]Running Inference:  93%|█████████▎| 186/200 [06:54<00:28,  2.02s/it]Running Inference:  94%|█████████▎| 187/200 [06:57<00:32,  2.48s/it]Running Inference:  94%|█████████▍| 188/200 [07:02<00:36,  3.01s/it]Running Inference:  94%|█████████▍| 189/200 [07:02<00:24,  2.23s/it]Running Inference:  95%|█████████▌| 190/200 [07:04<00:20,  2.09s/it]Running Inference:  96%|█████████▌| 191/200 [07:05<00:17,  1.95s/it]Running Inference:  96%|█████████▌| 192/200 [07:08<00:16,  2.02s/it]Running Inference:  96%|█████████▋| 193/200 [07:09<00:13,  1.88s/it]Running Inference:  97%|█████████▋| 194/200 [07:12<00:12,  2.08s/it]Running Inference:  98%|█████████▊| 195/200 [07:14<00:11,  2.23s/it]Running Inference:  98%|█████████▊| 196/200 [07:16<00:08,  2.03s/it]Running Inference:  98%|█████████▊| 197/200 [07:19<00:07,  2.48s/it]Running Inference:  99%|█████████▉| 198/200 [07:23<00:05,  2.83s/it]Running Inference: 100%|█████████▉| 199/200 [07:23<00:02,  2.13s/it]Running Inference: 100%|██████████| 200/200 [07:25<00:00,  1.80s/it]Running Inference: 100%|██████████| 200/200 [07:25<00:00,  2.23s/it]
2025-12-13 21:22:05,925 - INFO - Inference completed.
2025-12-13 21:22:05,948 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-13 21:22:05,948 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:22:05,985 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-13 21:22:05,985 - INFO - Metrics:
36.48
2025-12-13 21:22:05,986 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=triviaqa, ratio=0.3) on GPU 2

----------------------------------------
Task: triviaqa | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:22:12,336 - INFO - Set deterministic seeds to 42
2025-12-13 21:22:12,337 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "triviaqa",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:22:12,337 - INFO - Starting evaluation run...
2025-12-13 21:22:12,337 - INFO - Output directory set to: longbenchresult
2025-12-13 21:22:12,337 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-13 21:22:12,337 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 21:22:12,337 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:22:12,337 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.82it/s]
Device set to use cuda:0
2025-12-13 21:23:06,435 - INFO - Model pipeline loaded.
2025-12-13 21:23:06,436 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: triviaqa)
2025-12-13 21:23:15,637 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:23:15,637 - INFO - Dataset processed with 200 entries.
2025-12-13 21:23:15,668 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<09:40,  2.92s/it]Running Inference:   1%|          | 2/200 [00:04<07:18,  2.21s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:22,  2.55s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:09,  2.50s/it]Running Inference:   2%|▎         | 5/200 [00:12<08:03,  2.48s/it]Running Inference:   3%|▎         | 6/200 [00:13<06:10,  1.91s/it]Running Inference:   4%|▎         | 7/200 [00:13<04:31,  1.40s/it]Running Inference:   4%|▍         | 8/200 [00:16<05:38,  1.76s/it]Running Inference:   4%|▍         | 9/200 [00:17<05:10,  1.63s/it]Running Inference:   5%|▌         | 10/200 [00:19<05:07,  1.62s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:21<06:18,  2.00s/it]Running Inference:   6%|▌         | 12/200 [00:25<07:27,  2.38s/it]Running Inference:   6%|▋         | 13/200 [00:26<06:24,  2.06s/it]Running Inference:   7%|▋         | 14/200 [00:30<08:28,  2.74s/it]Running Inference:   8%|▊         | 15/200 [00:33<08:05,  2.62s/it]Running Inference:   8%|▊         | 16/200 [00:36<08:19,  2.71s/it]Running Inference:   8%|▊         | 17/200 [00:36<06:24,  2.10s/it]Running Inference:   9%|▉         | 18/200 [00:37<05:05,  1.68s/it]Running Inference:  10%|▉         | 19/200 [00:40<06:13,  2.06s/it]Running Inference:  10%|█         | 20/200 [00:41<05:41,  1.90s/it]Running Inference:  10%|█         | 21/200 [00:44<06:09,  2.07s/it]Running Inference:  11%|█         | 22/200 [00:45<05:36,  1.89s/it]Running Inference:  12%|█▏        | 23/200 [00:49<06:46,  2.30s/it]Running Inference:  12%|█▏        | 24/200 [00:51<06:39,  2.27s/it]Running Inference:  12%|█▎        | 25/200 [00:53<06:50,  2.35s/it]Running Inference:  13%|█▎        | 26/200 [00:55<05:52,  2.03s/it]Running Inference:  14%|█▎        | 27/200 [00:57<06:16,  2.18s/it]Running Inference:  14%|█▍        | 28/200 [00:58<05:12,  1.82s/it]Running Inference:  14%|█▍        | 29/200 [01:00<05:22,  1.88s/it]Running Inference:  15%|█▌        | 30/200 [01:03<05:49,  2.06s/it]Running Inference:  16%|█▌        | 31/200 [01:04<05:16,  1.87s/it]Running Inference:  16%|█▌        | 32/200 [01:08<06:53,  2.46s/it]Running Inference:  16%|█▋        | 33/200 [01:10<06:36,  2.38s/it]Running Inference:  17%|█▋        | 34/200 [01:11<05:45,  2.08s/it]Running Inference:  18%|█▊        | 35/200 [01:13<05:00,  1.82s/it]Running Inference:  18%|█▊        | 36/200 [01:17<06:55,  2.53s/it]Running Inference:  18%|█▊        | 37/200 [01:20<07:26,  2.74s/it]Running Inference:  19%|█▉        | 38/200 [01:25<09:00,  3.34s/it]Running Inference:  20%|█▉        | 39/200 [01:26<06:57,  2.59s/it]Running Inference:  20%|██        | 40/200 [01:31<08:56,  3.35s/it]Running Inference:  20%|██        | 41/200 [01:32<07:20,  2.77s/it]Running Inference:  21%|██        | 42/200 [01:35<07:11,  2.73s/it]Running Inference:  22%|██▏       | 43/200 [01:36<05:37,  2.15s/it]Running Inference:  22%|██▏       | 44/200 [01:37<05:15,  2.02s/it]Running Inference:  22%|██▎       | 45/200 [01:38<03:59,  1.54s/it]Running Inference:  23%|██▎       | 46/200 [01:41<05:01,  1.96s/it]Running Inference:  24%|██▎       | 47/200 [01:42<04:22,  1.71s/it]Running Inference:  24%|██▍       | 48/200 [01:45<05:25,  2.14s/it]Running Inference:  24%|██▍       | 49/200 [01:47<05:22,  2.13s/it]Running Inference:  25%|██▌       | 50/200 [01:50<05:46,  2.31s/it]Running Inference:  26%|██▌       | 51/200 [01:51<04:33,  1.83s/it]Running Inference:  26%|██▌       | 52/200 [01:53<04:46,  1.94s/it]Running Inference:  26%|██▋       | 53/200 [01:53<03:40,  1.50s/it]Running Inference:  27%|██▋       | 54/200 [01:54<02:56,  1.21s/it]Running Inference:  28%|██▊       | 55/200 [01:55<02:51,  1.18s/it]Running Inference:  28%|██▊       | 56/200 [01:59<04:35,  1.91s/it]Running Inference:  28%|██▊       | 57/200 [02:01<05:01,  2.11s/it]Running Inference:  29%|██▉       | 58/200 [02:04<05:23,  2.28s/it]Running Inference:  30%|██▉       | 59/200 [02:07<05:58,  2.54s/it]Running Inference:  30%|███       | 60/200 [02:09<05:22,  2.30s/it]Running Inference:  30%|███       | 61/200 [02:13<07:00,  3.03s/it]Running Inference:  31%|███       | 62/200 [02:17<07:24,  3.22s/it]Running Inference:  32%|███▏      | 63/200 [02:20<06:50,  3.00s/it]Running Inference:  32%|███▏      | 64/200 [02:20<05:21,  2.36s/it]Running Inference:  32%|███▎      | 65/200 [02:24<05:48,  2.58s/it]Running Inference:  33%|███▎      | 66/200 [02:28<06:48,  3.05s/it]Running Inference:  34%|███▎      | 67/200 [02:30<06:00,  2.71s/it]Running Inference:  34%|███▍      | 68/200 [02:33<06:07,  2.78s/it]Running Inference:  34%|███▍      | 69/200 [02:36<06:31,  2.99s/it]Running Inference:  35%|███▌      | 70/200 [02:40<07:16,  3.36s/it]Running Inference:  36%|███▌      | 71/200 [02:43<06:57,  3.24s/it]Running Inference:  36%|███▌      | 72/200 [02:45<06:16,  2.94s/it]Running Inference:  36%|███▋      | 73/200 [02:47<05:31,  2.61s/it]Running Inference:  37%|███▋      | 74/200 [02:50<05:52,  2.80s/it]Running Inference:  38%|███▊      | 75/200 [02:51<04:30,  2.17s/it]Running Inference:  38%|███▊      | 76/200 [02:53<04:28,  2.17s/it]Running Inference:  38%|███▊      | 77/200 [02:56<04:44,  2.32s/it]Running Inference:  39%|███▉      | 78/200 [02:56<03:29,  1.72s/it]Running Inference:  40%|███▉      | 79/200 [02:58<03:37,  1.80s/it]Running Inference:  40%|████      | 80/200 [03:00<03:47,  1.90s/it]Running Inference:  40%|████      | 81/200 [03:03<04:17,  2.17s/it]Running Inference:  41%|████      | 82/200 [03:04<03:14,  1.65s/it]Running Inference:  42%|████▏     | 83/200 [03:07<04:17,  2.20s/it]Running Inference:  42%|████▏     | 84/200 [03:10<04:41,  2.42s/it]Running Inference:  42%|████▎     | 85/200 [03:12<04:06,  2.14s/it]Running Inference:  43%|████▎     | 86/200 [03:17<05:53,  3.10s/it]Running Inference:  44%|████▎     | 87/200 [03:20<05:53,  3.13s/it]Running Inference:  44%|████▍     | 88/200 [03:24<06:09,  3.30s/it]Running Inference:  44%|████▍     | 89/200 [03:27<06:01,  3.26s/it]Running Inference:  45%|████▌     | 90/200 [03:29<05:07,  2.80s/it]Running Inference:  46%|████▌     | 91/200 [03:33<05:42,  3.14s/it]Running Inference:  46%|████▌     | 92/200 [03:33<04:23,  2.44s/it]Running Inference:  46%|████▋     | 93/200 [03:36<04:34,  2.57s/it]Running Inference:  47%|████▋     | 94/200 [03:38<04:01,  2.27s/it]Running Inference:  48%|████▊     | 95/200 [03:42<05:10,  2.95s/it]Running Inference:  48%|████▊     | 96/200 [03:45<04:45,  2.75s/it]Running Inference:  48%|████▊     | 97/200 [03:48<05:01,  2.93s/it]Running Inference:  49%|████▉     | 98/200 [03:50<04:41,  2.76s/it]Running Inference:  50%|████▉     | 99/200 [03:54<04:55,  2.93s/it]Running Inference:  50%|█████     | 100/200 [03:55<04:01,  2.42s/it]Running Inference:  50%|█████     | 101/200 [03:57<03:56,  2.39s/it]Running Inference:  51%|█████     | 102/200 [04:00<03:53,  2.39s/it]Running Inference:  52%|█████▏    | 103/200 [04:02<03:40,  2.27s/it]Running Inference:  52%|█████▏    | 104/200 [04:03<03:14,  2.02s/it]Running Inference:  52%|█████▎    | 105/200 [04:06<03:29,  2.21s/it]Running Inference:  53%|█████▎    | 106/200 [04:09<03:52,  2.47s/it]Running Inference:  54%|█████▎    | 107/200 [04:13<04:38,  3.00s/it]Running Inference:  54%|█████▍    | 108/200 [04:15<04:04,  2.66s/it]Running Inference:  55%|█████▍    | 109/200 [04:16<03:13,  2.13s/it]Running Inference:  55%|█████▌    | 110/200 [04:16<02:28,  1.65s/it]Running Inference:  56%|█████▌    | 111/200 [04:20<03:12,  2.16s/it]Running Inference:  56%|█████▌    | 112/200 [04:23<03:40,  2.51s/it]Running Inference:  56%|█████▋    | 113/200 [04:26<03:54,  2.69s/it]Running Inference:  57%|█████▋    | 114/200 [04:31<04:42,  3.28s/it]Running Inference:  57%|█████▊    | 115/200 [04:33<03:59,  2.81s/it]Running Inference:  58%|█████▊    | 116/200 [04:33<02:55,  2.09s/it]Running Inference:  58%|█████▊    | 117/200 [04:36<03:28,  2.52s/it]Running Inference:  59%|█████▉    | 118/200 [04:37<02:44,  2.01s/it]Running Inference:  60%|█████▉    | 119/200 [04:39<02:28,  1.84s/it]Running Inference:  60%|██████    | 120/200 [04:42<03:05,  2.32s/it]Running Inference:  60%|██████    | 121/200 [04:43<02:19,  1.76s/it]Running Inference:  61%|██████    | 122/200 [04:44<02:15,  1.74s/it]Running Inference:  62%|██████▏   | 123/200 [04:45<01:44,  1.35s/it]Running Inference:  62%|██████▏   | 124/200 [04:47<02:13,  1.75s/it]Running Inference:  62%|██████▎   | 125/200 [04:49<02:15,  1.81s/it]Running Inference:  63%|██████▎   | 126/200 [04:52<02:33,  2.07s/it]Running Inference:  64%|██████▎   | 127/200 [04:56<03:18,  2.72s/it]Running Inference:  64%|██████▍   | 128/200 [04:57<02:37,  2.19s/it]Running Inference:  64%|██████▍   | 129/200 [04:58<02:10,  1.84s/it]Running Inference:  65%|██████▌   | 130/200 [04:59<01:45,  1.50s/it]Running Inference:  66%|██████▌   | 131/200 [05:02<02:12,  1.92s/it]Running Inference:  66%|██████▌   | 132/200 [05:04<02:14,  1.98s/it]Running Inference:  66%|██████▋   | 133/200 [05:05<01:58,  1.76s/it]Running Inference:  67%|██████▋   | 134/200 [05:08<02:06,  1.92s/it]Running Inference:  68%|██████▊   | 135/200 [05:11<02:29,  2.30s/it]Running Inference:  68%|██████▊   | 136/200 [05:16<03:19,  3.11s/it]Running Inference:  68%|██████▊   | 137/200 [05:17<02:48,  2.67s/it]Running Inference:  69%|██████▉   | 138/200 [05:21<03:08,  3.03s/it]Running Inference:  70%|██████▉   | 139/200 [05:26<03:31,  3.47s/it]Running Inference:  70%|███████   | 140/200 [05:28<03:01,  3.03s/it]Running Inference:  70%|███████   | 141/200 [05:30<02:48,  2.86s/it]Running Inference:  71%|███████   | 142/200 [05:33<02:43,  2.83s/it]Running Inference:  72%|███████▏  | 143/200 [05:36<02:38,  2.78s/it]Running Inference:  72%|███████▏  | 144/200 [05:37<02:13,  2.38s/it]Running Inference:  72%|███████▎  | 145/200 [05:39<01:57,  2.13s/it]Running Inference:  73%|███████▎  | 146/200 [05:41<02:02,  2.27s/it]Running Inference:  74%|███████▎  | 147/200 [05:42<01:32,  1.75s/it]Running Inference:  74%|███████▍  | 148/200 [05:46<02:04,  2.40s/it]Running Inference:  74%|███████▍  | 149/200 [05:50<02:24,  2.84s/it]Running Inference:  75%|███████▌  | 150/200 [05:50<01:52,  2.25s/it]Running Inference:  76%|███████▌  | 151/200 [05:54<02:11,  2.69s/it]Running Inference:  76%|███████▌  | 152/200 [05:55<01:45,  2.20s/it]Running Inference:  76%|███████▋  | 153/200 [05:58<01:48,  2.30s/it]Running Inference:  77%|███████▋  | 154/200 [06:02<02:13,  2.90s/it]Running Inference:  78%|███████▊  | 155/200 [06:04<01:55,  2.57s/it]Running Inference:  78%|███████▊  | 156/200 [06:06<01:50,  2.50s/it]Running Inference:  78%|███████▊  | 157/200 [06:11<02:13,  3.12s/it]Running Inference:  79%|███████▉  | 158/200 [06:13<01:54,  2.72s/it]Running Inference:  80%|███████▉  | 159/200 [06:14<01:34,  2.31s/it]Running Inference:  80%|████████  | 160/200 [06:15<01:17,  1.93s/it]Running Inference:  80%|████████  | 161/200 [06:17<01:13,  1.88s/it]Running Inference:  81%|████████  | 162/200 [06:18<01:10,  1.84s/it]Running Inference:  82%|████████▏ | 163/200 [06:21<01:11,  1.92s/it]Running Inference:  82%|████████▏ | 164/200 [06:21<00:56,  1.56s/it]Running Inference:  82%|████████▎ | 165/200 [06:25<01:18,  2.23s/it]Running Inference:  83%|████████▎ | 166/200 [06:27<01:12,  2.12s/it]Running Inference:  84%|████████▎ | 167/200 [06:28<01:01,  1.86s/it]Running Inference:  84%|████████▍ | 168/200 [06:31<01:08,  2.13s/it]Running Inference:  84%|████████▍ | 169/200 [06:32<00:57,  1.87s/it]Running Inference:  85%|████████▌ | 170/200 [06:33<00:48,  1.62s/it]Running Inference:  86%|████████▌ | 171/200 [06:34<00:40,  1.41s/it]Running Inference:  86%|████████▌ | 172/200 [06:36<00:42,  1.51s/it]Running Inference:  86%|████████▋ | 173/200 [06:38<00:41,  1.55s/it]Running Inference:  87%|████████▋ | 174/200 [06:39<00:42,  1.65s/it]Running Inference:  88%|████████▊ | 175/200 [06:41<00:40,  1.63s/it]Running Inference:  88%|████████▊ | 176/200 [06:42<00:35,  1.46s/it]Running Inference:  88%|████████▊ | 177/200 [06:46<00:50,  2.19s/it]Running Inference:  89%|████████▉ | 178/200 [06:49<00:51,  2.32s/it]Running Inference:  90%|████████▉ | 179/200 [06:51<00:50,  2.40s/it]Running Inference:  90%|█████████ | 180/200 [06:52<00:41,  2.06s/it]Running Inference:  90%|█████████ | 181/200 [06:54<00:34,  1.81s/it]Running Inference:  91%|█████████ | 182/200 [06:55<00:27,  1.52s/it]Running Inference:  92%|█████████▏| 183/200 [06:59<00:41,  2.44s/it]Running Inference:  92%|█████████▏| 184/200 [07:02<00:42,  2.65s/it]Running Inference:  92%|█████████▎| 185/200 [07:05<00:41,  2.75s/it]Running Inference:  93%|█████████▎| 186/200 [07:07<00:33,  2.38s/it]Running Inference:  94%|█████████▎| 187/200 [07:10<00:35,  2.72s/it]Running Inference:  94%|█████████▍| 188/200 [07:13<00:31,  2.64s/it]Running Inference:  94%|█████████▍| 189/200 [07:13<00:21,  1.98s/it]Running Inference:  95%|█████████▌| 190/200 [07:15<00:19,  1.91s/it]Running Inference:  96%|█████████▌| 191/200 [07:17<00:16,  1.82s/it]Running Inference:  96%|█████████▌| 192/200 [07:19<00:15,  1.95s/it]Running Inference:  96%|█████████▋| 193/200 [07:22<00:17,  2.44s/it]Running Inference:  97%|█████████▋| 194/200 [07:25<00:14,  2.47s/it]Running Inference:  98%|█████████▊| 195/200 [07:27<00:12,  2.50s/it]Running Inference:  98%|█████████▊| 196/200 [07:29<00:08,  2.21s/it]Running Inference:  98%|█████████▊| 197/200 [07:32<00:07,  2.60s/it]Running Inference:  99%|█████████▉| 198/200 [07:36<00:05,  2.90s/it]Running Inference: 100%|█████████▉| 199/200 [07:38<00:02,  2.49s/it]Running Inference: 100%|██████████| 200/200 [07:39<00:00,  2.05s/it]Running Inference: 100%|██████████| 200/200 [07:39<00:00,  2.30s/it]
2025-12-13 21:30:54,822 - INFO - Inference completed.
2025-12-13 21:30:54,846 - INFO - Results saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-13 21:30:54,846 - INFO - Calculating metrics for dataset: longbench
2025-12-13 21:30:54,882 - INFO - Metrics saved to longbenchresult/longbench__triviaqa__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-13 21:30:54,882 - INFO - Metrics:
36.11
2025-12-13 21:30:54,884 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=triviaqa, ratio=0.5) on GPU 2


========================================
LongBench Task: gov_report
========================================
----------------------------------------
Task: gov_report | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 21:31:01,220 - INFO - Set deterministic seeds to 42
2025-12-13 21:31:01,220 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 21:31:01,220 - INFO - Starting evaluation run...
2025-12-13 21:31:01,221 - INFO - Output directory set to: longbenchresult
2025-12-13 21:31:01,221 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-13 21:31:01,221 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 21:31:01,221 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 21:31:01,221 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.50it/s]
Device set to use cuda:0
2025-12-13 21:31:16,468 - INFO - Model pipeline loaded.
2025-12-13 21:31:16,468 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-13 21:31:24,802 - INFO - Dataset loaded with 200 entries.
2025-12-13 21:31:24,802 - INFO - Dataset processed with 200 entries.
2025-12-13 21:31:24,836 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:19:41, 24.03s/it]Running Inference:   1%|          | 2/200 [00:47<1:17:51, 23.59s/it]Running Inference:   2%|▏         | 3/200 [01:12<1:19:36, 24.25s/it]Running Inference:   2%|▏         | 4/200 [01:16<53:34, 16.40s/it]  Running Inference:   2%|▎         | 5/200 [01:33<53:40, 16.51s/it]Running Inference:   3%|▎         | 6/200 [01:58<1:03:01, 19.49s/it]Running Inference:   4%|▎         | 7/200 [02:15<59:48, 18.59s/it]  Running Inference:   4%|▍         | 8/200 [02:33<59:10, 18.49s/it]Running Inference:   4%|▍         | 9/200 [02:57<1:04:01, 20.11s/it]Running Inference:   5%|▌         | 10/200 [03:21<1:07:06, 21.19s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:44<1:08:52, 21.87s/it]Running Inference:   6%|▌         | 12/200 [04:07<1:09:58, 22.33s/it]Running Inference:   6%|▋         | 13/200 [04:32<1:11:23, 22.91s/it]Running Inference:   7%|▋         | 14/200 [04:55<1:11:49, 23.17s/it]Running Inference:   8%|▊         | 15/200 [05:08<1:01:26, 19.92s/it]Running Inference:   8%|▊         | 16/200 [05:32<1:04:51, 21.15s/it]Running Inference:   8%|▊         | 17/200 [05:56<1:07:39, 22.18s/it]Running Inference:   9%|▉         | 18/200 [06:14<1:03:25, 20.91s/it]Running Inference:  10%|▉         | 19/200 [06:41<1:08:08, 22.59s/it]Running Inference:  10%|█         | 20/200 [06:56<1:01:25, 20.47s/it]Running Inference:  10%|█         | 21/200 [07:20<1:04:07, 21.49s/it]Running Inference:  11%|█         | 22/200 [07:44<1:06:06, 22.29s/it]Running Inference:  12%|█▏        | 23/200 [08:09<1:07:27, 22.87s/it]Running Inference:  12%|█▏        | 24/200 [08:35<1:09:56, 23.84s/it]Running Inference:  12%|█▎        | 25/200 [08:58<1:08:59, 23.66s/it]Running Inference:  13%|█▎        | 26/200 [09:21<1:08:31, 23.63s/it]Running Inference:  14%|█▎        | 27/200 [09:45<1:08:00, 23.59s/it]Running Inference:  14%|█▍        | 28/200 [10:08<1:07:11, 23.44s/it]Running Inference:  14%|█▍        | 29/200 [10:35<1:09:39, 24.44s/it]Running Inference:  15%|█▌        | 30/200 [10:59<1:08:52, 24.31s/it]Running Inference:  16%|█▌        | 31/200 [11:22<1:07:55, 24.12s/it]Running Inference:  16%|█▌        | 32/200 [11:47<1:07:49, 24.22s/it]Running Inference:  16%|█▋        | 33/200 [12:10<1:06:21, 23.84s/it]Running Inference:  17%|█▋        | 34/200 [12:34<1:06:17, 23.96s/it]Running Inference:  18%|█▊        | 35/200 [12:36<47:18, 17.20s/it]  Running Inference:  18%|█▊        | 36/200 [13:00<53:04, 19.42s/it]Running Inference:  18%|█▊        | 37/200 [13:15<49:15, 18.13s/it]Running Inference:  19%|█▉        | 38/200 [13:39<53:48, 19.93s/it]Running Inference:  20%|█▉        | 39/200 [14:03<56:25, 21.03s/it]Running Inference:  20%|██        | 40/200 [14:06<41:41, 15.63s/it]Running Inference:  20%|██        | 41/200 [14:24<43:31, 16.42s/it]Running Inference:  21%|██        | 42/200 [14:27<32:12, 12.23s/it]Running Inference:  22%|██▏       | 43/200 [14:51<41:18, 15.79s/it]Running Inference:  22%|██▏       | 44/200 [15:14<46:48, 18.01s/it]Running Inference:  22%|██▎       | 45/200 [15:20<37:03, 14.35s/it]Running Inference:  23%|██▎       | 46/200 [15:43<43:46, 17.06s/it]Running Inference:  24%|██▎       | 47/200 [16:08<49:41, 19.49s/it]Running Inference:  24%|██▍       | 48/200 [16:32<52:27, 20.71s/it]Running Inference:  24%|██▍       | 49/200 [16:51<50:42, 20.15s/it]Running Inference:  25%|██▌       | 50/200 [17:14<52:52, 21.15s/it]Running Inference:  26%|██▌       | 51/200 [17:37<53:57, 21.73s/it]Running Inference:  26%|██▌       | 52/200 [18:01<55:06, 22.34s/it]Running Inference:  26%|██▋       | 53/200 [18:26<56:49, 23.19s/it]Running Inference:  27%|██▋       | 54/200 [18:49<56:20, 23.16s/it]Running Inference:  28%|██▊       | 55/200 [19:13<56:07, 23.23s/it]Running Inference:  28%|██▊       | 56/200 [19:37<56:07, 23.39s/it]Running Inference:  28%|██▊       | 57/200 [19:41<42:01, 17.63s/it]Running Inference:  29%|██▉       | 58/200 [20:04<45:58, 19.43s/it]Running Inference:  30%|██▉       | 59/200 [20:20<43:01, 18.31s/it]Running Inference:  30%|███       | 60/200 [20:28<35:23, 15.17s/it]Running Inference:  30%|███       | 61/200 [20:44<35:41, 15.41s/it]Running Inference:  31%|███       | 62/200 [21:07<40:56, 17.80s/it]Running Inference:  32%|███▏      | 63/200 [21:30<44:11, 19.35s/it]Running Inference:  32%|███▏      | 64/200 [21:53<46:31, 20.53s/it]Running Inference:  32%|███▎      | 65/200 [22:17<48:09, 21.41s/it]Running Inference:  33%|███▎      | 66/200 [22:41<49:24, 22.12s/it]Running Inference:  34%|███▎      | 67/200 [23:05<50:19, 22.70s/it]Running Inference:  34%|███▍      | 68/200 [23:28<50:27, 22.94s/it]Running Inference:  34%|███▍      | 69/200 [23:52<50:29, 23.12s/it]Running Inference:  35%|███▌      | 70/200 [24:10<46:45, 21.58s/it]Running Inference:  36%|███▌      | 71/200 [24:34<47:59, 22.32s/it]Running Inference:  36%|███▌      | 72/200 [24:49<42:44, 20.03s/it]Running Inference:  36%|███▋      | 73/200 [25:14<45:32, 21.52s/it]Running Inference:  37%|███▋      | 74/200 [25:37<46:24, 22.10s/it]Running Inference:  38%|███▊      | 75/200 [26:01<46:57, 22.54s/it]Running Inference:  38%|███▊      | 76/200 [26:25<47:41, 23.07s/it]Running Inference:  38%|███▊      | 77/200 [26:49<47:53, 23.36s/it]Running Inference:  39%|███▉      | 78/200 [27:12<47:23, 23.31s/it]Running Inference:  40%|███▉      | 79/200 [27:36<47:05, 23.35s/it]Running Inference:  40%|████      | 80/200 [28:00<47:16, 23.64s/it]Running Inference:  40%|████      | 81/200 [28:23<46:20, 23.37s/it]Running Inference:  41%|████      | 82/200 [28:47<46:22, 23.58s/it]Running Inference:  42%|████▏     | 83/200 [29:10<45:38, 23.41s/it]Running Inference:  42%|████▏     | 84/200 [29:34<45:31, 23.55s/it]Running Inference:  42%|████▎     | 85/200 [29:58<45:27, 23.72s/it]Running Inference:  43%|████▎     | 86/200 [30:18<43:11, 22.73s/it]Running Inference:  44%|████▎     | 87/200 [30:42<43:17, 22.99s/it]Running Inference:  44%|████▍     | 88/200 [31:08<44:47, 23.99s/it]Running Inference:  44%|████▍     | 89/200 [31:31<44:02, 23.81s/it]Running Inference:  45%|████▌     | 90/200 [31:49<40:21, 22.01s/it]Running Inference:  46%|████▌     | 91/200 [32:13<41:00, 22.58s/it]Running Inference:  46%|████▌     | 92/200 [32:36<40:57, 22.75s/it]Running Inference:  46%|████▋     | 93/200 [33:00<41:06, 23.05s/it]Running Inference:  47%|████▋     | 94/200 [33:23<40:40, 23.03s/it]Running Inference:  48%|████▊     | 95/200 [33:48<41:08, 23.51s/it]Running Inference:  48%|████▊     | 96/200 [34:12<40:59, 23.65s/it]Running Inference:  48%|████▊     | 97/200 [34:35<40:19, 23.49s/it]Running Inference:  49%|████▉     | 98/200 [34:58<39:50, 23.44s/it]Running Inference:  50%|████▉     | 99/200 [35:14<35:55, 21.34s/it]Running Inference:  50%|█████     | 100/200 [35:39<37:01, 22.21s/it]Running Inference:  50%|█████     | 101/200 [36:02<37:07, 22.50s/it]Running Inference:  51%|█████     | 102/200 [36:26<37:35, 23.01s/it]Running Inference:  52%|█████▏    | 103/200 [36:49<37:14, 23.04s/it]Running Inference:  52%|█████▏    | 104/200 [37:13<37:16, 23.30s/it]Running Inference:  52%|█████▎    | 105/200 [37:36<36:53, 23.30s/it]Running Inference:  53%|█████▎    | 106/200 [37:59<36:06, 23.05s/it]Running Inference:  54%|█████▎    | 107/200 [38:23<36:23, 23.48s/it]Running Inference:  54%|█████▍    | 108/200 [38:46<35:47, 23.34s/it]Running Inference:  55%|█████▍    | 109/200 [39:10<35:18, 23.28s/it]Running Inference:  55%|█████▌    | 110/200 [39:19<28:38, 19.09s/it]Running Inference:  56%|█████▌    | 111/200 [39:43<30:41, 20.69s/it]Running Inference:  56%|█████▌    | 112/200 [40:06<31:26, 21.44s/it]Running Inference:  56%|█████▋    | 113/200 [40:30<32:07, 22.16s/it]Running Inference:  57%|█████▋    | 114/200 [40:47<29:28, 20.57s/it]Running Inference:  57%|█████▊    | 115/200 [41:15<32:19, 22.82s/it]Running Inference:  58%|█████▊    | 116/200 [41:34<30:24, 21.72s/it]Running Inference:  58%|█████▊    | 117/200 [41:57<30:35, 22.11s/it]Running Inference:  59%|█████▉    | 118/200 [42:17<29:08, 21.32s/it]Running Inference:  60%|█████▉    | 119/200 [42:42<30:28, 22.58s/it]Running Inference:  60%|██████    | 120/200 [43:06<30:28, 22.86s/it]Running Inference:  60%|██████    | 121/200 [43:30<30:34, 23.22s/it]Running Inference:  61%|██████    | 122/200 [43:53<30:09, 23.20s/it]Running Inference:  62%|██████▏   | 123/200 [44:16<29:50, 23.25s/it]Running Inference:  62%|██████▏   | 124/200 [44:40<29:35, 23.36s/it]Running Inference:  62%|██████▎   | 125/200 [45:04<29:23, 23.51s/it]Running Inference:  63%|██████▎   | 126/200 [45:27<28:50, 23.38s/it]Running Inference:  64%|██████▎   | 127/200 [45:45<26:18, 21.62s/it]Running Inference:  64%|██████▍   | 128/200 [46:07<26:25, 22.02s/it]Running Inference:  64%|██████▍   | 129/200 [46:31<26:25, 22.33s/it]Running Inference:  65%|██████▌   | 130/200 [46:49<24:43, 21.19s/it]Running Inference:  66%|██████▌   | 131/200 [46:51<17:46, 15.45s/it]Running Inference:  66%|██████▌   | 132/200 [47:16<20:34, 18.16s/it]Running Inference:  66%|██████▋   | 133/200 [47:39<21:56, 19.65s/it]Running Inference:  67%|██████▋   | 134/200 [47:55<20:29, 18.64s/it]Running Inference:  68%|██████▊   | 135/200 [48:18<21:42, 20.04s/it]Running Inference:  68%|██████▊   | 136/200 [48:43<23:00, 21.57s/it]Running Inference:  68%|██████▊   | 137/200 [48:59<20:35, 19.61s/it]Running Inference:  69%|██████▉   | 138/200 [49:24<22:13, 21.50s/it]Running Inference:  70%|██████▉   | 139/200 [49:48<22:34, 22.20s/it]Running Inference:  70%|███████   | 140/200 [50:12<22:31, 22.52s/it]Running Inference:  70%|███████   | 141/200 [50:35<22:20, 22.72s/it]Running Inference:  71%|███████   | 142/200 [50:58<22:14, 23.02s/it]Running Inference:  72%|███████▏  | 143/200 [51:20<21:19, 22.45s/it]Running Inference:  72%|███████▏  | 144/200 [51:36<19:24, 20.80s/it]Running Inference:  72%|███████▎  | 145/200 [52:01<20:08, 21.98s/it]Running Inference:  73%|███████▎  | 146/200 [52:24<20:06, 22.34s/it]Running Inference:  74%|███████▎  | 147/200 [52:45<19:11, 21.74s/it]Running Inference:  74%|███████▍  | 148/200 [52:49<14:23, 16.61s/it]Running Inference:  74%|███████▍  | 149/200 [53:13<16:01, 18.85s/it]Running Inference:  75%|███████▌  | 150/200 [53:37<16:56, 20.33s/it]Running Inference:  76%|███████▌  | 151/200 [54:01<17:26, 21.36s/it]Running Inference:  76%|███████▌  | 152/200 [54:22<16:54, 21.13s/it]Running Inference:  76%|███████▋  | 153/200 [54:45<17:07, 21.86s/it]Running Inference:  77%|███████▋  | 154/200 [55:08<17:03, 22.24s/it]Running Inference:  78%|███████▊  | 155/200 [55:12<12:36, 16.81s/it]Running Inference:  78%|███████▊  | 156/200 [55:36<13:54, 18.96s/it]Running Inference:  78%|███████▊  | 157/200 [55:59<14:25, 20.14s/it]Running Inference:  79%|███████▉  | 158/200 [56:21<14:21, 20.51s/it]Running Inference:  80%|███████▉  | 159/200 [56:46<14:57, 21.89s/it]Running Inference:  80%|████████  | 160/200 [57:10<15:04, 22.60s/it]Running Inference:  80%|████████  | 161/200 [57:34<15:01, 23.12s/it]Running Inference:  81%|████████  | 162/200 [57:57<14:36, 23.06s/it]Running Inference:  82%|████████▏ | 163/200 [58:17<13:37, 22.11s/it]Running Inference:  82%|████████▏ | 164/200 [58:42<13:48, 23.01s/it]Running Inference:  82%|████████▎ | 165/200 [59:05<13:23, 22.95s/it]Running Inference:  83%|████████▎ | 166/200 [59:24<12:21, 21.81s/it]Running Inference:  84%|████████▎ | 167/200 [59:48<12:23, 22.52s/it]Running Inference:  84%|████████▍ | 168/200 [1:00:12<12:07, 22.73s/it]Running Inference:  84%|████████▍ | 169/200 [1:00:35<11:46, 22.81s/it]Running Inference:  85%|████████▌ | 170/200 [1:00:58<11:29, 22.98s/it]Running Inference:  86%|████████▌ | 171/200 [1:01:23<11:27, 23.70s/it]Running Inference:  86%|████████▌ | 172/200 [1:01:46<10:58, 23.51s/it]Running Inference:  86%|████████▋ | 173/200 [1:02:10<10:36, 23.58s/it]Running Inference:  87%|████████▋ | 174/200 [1:02:34<10:12, 23.55s/it]Running Inference:  88%|████████▊ | 175/200 [1:02:57<09:44, 23.39s/it]Running Inference:  88%|████████▊ | 176/200 [1:03:16<08:52, 22.19s/it]Running Inference:  88%|████████▊ | 177/200 [1:03:50<09:52, 25.78s/it]Running Inference:  89%|████████▉ | 178/200 [1:04:14<09:16, 25.28s/it]Running Inference:  90%|████████▉ | 179/200 [1:04:37<08:36, 24.60s/it]Running Inference:  90%|█████████ | 180/200 [1:04:56<07:36, 22.84s/it]Running Inference:  90%|█████████ | 181/200 [1:05:20<07:18, 23.07s/it]Running Inference:  91%|█████████ | 182/200 [1:05:43<06:53, 22.99s/it]Running Inference:  92%|█████████▏| 183/200 [1:06:15<07:19, 25.85s/it]Running Inference:  92%|█████████▏| 184/200 [1:06:34<06:21, 23.87s/it]Running Inference:  92%|█████████▎| 185/200 [1:06:58<05:58, 23.89s/it]Running Inference:  93%|█████████▎| 186/200 [1:07:01<04:05, 17.52s/it]Running Inference:  94%|█████████▎| 187/200 [1:07:24<04:09, 19.21s/it]Running Inference:  94%|█████████▍| 188/200 [1:07:37<03:26, 17.24s/it]Running Inference:  94%|█████████▍| 189/200 [1:08:00<03:29, 19.05s/it]Running Inference:  95%|█████████▌| 190/200 [1:08:23<03:22, 20.29s/it]Running Inference:  96%|█████████▌| 191/200 [1:08:47<03:11, 21.32s/it]Running Inference:  96%|█████████▌| 192/200 [1:09:15<03:06, 23.28s/it]Running Inference:  96%|█████████▋| 193/200 [1:09:37<02:40, 22.99s/it]Running Inference:  97%|█████████▋| 194/200 [1:09:58<02:14, 22.44s/it]Running Inference:  98%|█████████▊| 195/200 [1:10:21<01:52, 22.52s/it]Running Inference:  98%|█████████▊| 196/200 [1:10:44<01:31, 22.78s/it]Running Inference:  98%|█████████▊| 197/200 [1:11:09<01:09, 23.21s/it]Running Inference:  99%|█████████▉| 198/200 [1:11:35<00:48, 24.11s/it]Running Inference: 100%|█████████▉| 199/200 [1:11:58<00:23, 23.72s/it]Running Inference: 100%|██████████| 200/200 [1:12:21<00:00, 23.59s/it]Running Inference: 100%|██████████| 200/200 [1:12:21<00:00, 21.71s/it]
2025-12-13 22:43:46,165 - INFO - Inference completed.
2025-12-13 22:43:46,198 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-13 22:43:46,198 - INFO - Calculating metrics for dataset: longbench
2025-12-13 22:44:03,873 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-13 22:44:03,873 - INFO - Metrics:
14.77
2025-12-13 22:44:03,875 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=gov_report, ratio=0.1) on GPU 2

----------------------------------------
Task: gov_report | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 22:44:10,778 - INFO - Set deterministic seeds to 42
2025-12-13 22:44:10,778 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 22:44:10,778 - INFO - Starting evaluation run...
2025-12-13 22:44:10,778 - INFO - Output directory set to: longbenchresult
2025-12-13 22:44:10,778 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-13 22:44:10,778 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 22:44:10,778 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 22:44:10,778 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.71it/s]
Device set to use cuda:0
2025-12-13 22:44:25,732 - INFO - Model pipeline loaded.
2025-12-13 22:44:25,733 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-13 22:44:41,615 - INFO - Dataset loaded with 200 entries.
2025-12-13 22:44:41,615 - INFO - Dataset processed with 200 entries.
2025-12-13 22:44:41,648 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:45, 24.35s/it]Running Inference:   1%|          | 2/200 [00:43<1:10:27, 21.35s/it]Running Inference:   2%|▏         | 3/200 [01:08<1:15:40, 23.05s/it]Running Inference:   2%|▏         | 4/200 [01:31<1:15:12, 23.02s/it]Running Inference:   2%|▎         | 5/200 [01:55<1:15:26, 23.21s/it]Running Inference:   3%|▎         | 6/200 [02:20<1:17:00, 23.82s/it]Running Inference:   4%|▎         | 7/200 [02:44<1:16:41, 23.84s/it]Running Inference:   4%|▍         | 8/200 [03:02<1:10:31, 22.04s/it]Running Inference:   4%|▍         | 9/200 [03:25<1:11:48, 22.56s/it]Running Inference:   5%|▌         | 10/200 [03:49<1:12:30, 22.90s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:12<1:12:05, 22.89s/it]Running Inference:   6%|▌         | 12/200 [04:31<1:07:48, 21.64s/it]Running Inference:   6%|▋         | 13/200 [04:55<1:09:55, 22.44s/it]Running Inference:   7%|▋         | 14/200 [05:19<1:10:47, 22.84s/it]Running Inference:   8%|▊         | 15/200 [05:35<1:04:38, 20.96s/it]Running Inference:   8%|▊         | 16/200 [05:59<1:07:04, 21.87s/it]Running Inference:   8%|▊         | 17/200 [06:24<1:09:12, 22.69s/it]Running Inference:   9%|▉         | 18/200 [06:48<1:10:06, 23.11s/it]Running Inference:  10%|▉         | 19/200 [07:14<1:12:29, 24.03s/it]Running Inference:  10%|█         | 20/200 [07:38<1:11:38, 23.88s/it]Running Inference:  10%|█         | 21/200 [08:02<1:11:15, 23.89s/it]Running Inference:  11%|█         | 22/200 [08:26<1:11:13, 24.01s/it]Running Inference:  12%|█▏        | 23/200 [08:50<1:11:00, 24.07s/it]Running Inference:  12%|█▏        | 24/200 [09:14<1:10:14, 23.95s/it]Running Inference:  12%|█▎        | 25/200 [09:37<1:09:13, 23.73s/it]Running Inference:  13%|█▎        | 26/200 [10:01<1:08:41, 23.69s/it]Running Inference:  14%|█▎        | 27/200 [10:24<1:08:07, 23.63s/it]Running Inference:  14%|█▍        | 28/200 [10:44<1:04:29, 22.50s/it]Running Inference:  14%|█▍        | 29/200 [11:10<1:07:28, 23.68s/it]Running Inference:  15%|█▌        | 30/200 [11:34<1:07:22, 23.78s/it]Running Inference:  16%|█▌        | 31/200 [11:58<1:06:57, 23.77s/it]Running Inference:  16%|█▌        | 32/200 [12:23<1:07:09, 23.99s/it]Running Inference:  16%|█▋        | 33/200 [12:46<1:05:53, 23.68s/it]Running Inference:  17%|█▋        | 34/200 [13:10<1:06:00, 23.86s/it]Running Inference:  18%|█▊        | 35/200 [13:11<47:06, 17.13s/it]  Running Inference:  18%|█▊        | 36/200 [13:36<52:58, 19.38s/it]Running Inference:  18%|█▊        | 37/200 [13:59<55:57, 20.60s/it]Running Inference:  19%|█▉        | 38/200 [14:24<58:28, 21.66s/it]Running Inference:  20%|█▉        | 39/200 [14:47<59:38, 22.23s/it]Running Inference:  20%|██        | 40/200 [15:11<1:00:34, 22.72s/it]Running Inference:  20%|██        | 41/200 [15:35<1:01:19, 23.14s/it]Running Inference:  21%|██        | 42/200 [15:59<1:01:35, 23.39s/it]Running Inference:  22%|██▏       | 43/200 [16:23<1:01:47, 23.62s/it]Running Inference:  22%|██▏       | 44/200 [16:37<53:38, 20.63s/it]  Running Inference:  22%|██▎       | 45/200 [16:43<41:48, 16.19s/it]Running Inference:  23%|██▎       | 46/200 [16:47<32:21, 12.60s/it]Running Inference:  24%|██▎       | 47/200 [17:12<41:36, 16.31s/it]Running Inference:  24%|██▍       | 48/200 [17:36<46:51, 18.50s/it]Running Inference:  24%|██▍       | 49/200 [17:53<45:49, 18.21s/it]Running Inference:  25%|██▌       | 50/200 [18:17<49:27, 19.79s/it]Running Inference:  26%|██▌       | 51/200 [18:40<51:36, 20.78s/it]Running Inference:  26%|██▌       | 52/200 [19:03<53:27, 21.67s/it]Running Inference:  26%|██▋       | 53/200 [19:31<57:23, 23.43s/it]Running Inference:  27%|██▋       | 54/200 [19:54<56:42, 23.30s/it]Running Inference:  28%|██▊       | 55/200 [20:16<55:04, 22.79s/it]Running Inference:  28%|██▊       | 56/200 [20:39<55:27, 23.11s/it]Running Inference:  28%|██▊       | 57/200 [20:44<41:45, 17.52s/it]Running Inference:  29%|██▉       | 58/200 [21:07<45:46, 19.34s/it]Running Inference:  30%|██▉       | 59/200 [21:31<48:31, 20.65s/it]Running Inference:  30%|███       | 60/200 [21:56<51:23, 22.02s/it]Running Inference:  30%|███       | 61/200 [22:15<48:41, 21.02s/it]Running Inference:  31%|███       | 62/200 [22:39<50:02, 21.76s/it]Running Inference:  32%|███▏      | 63/200 [22:48<41:14, 18.06s/it]Running Inference:  32%|███▏      | 64/200 [23:11<44:31, 19.64s/it]Running Inference:  32%|███▎      | 65/200 [23:35<46:49, 20.81s/it]Running Inference:  33%|███▎      | 66/200 [23:59<48:28, 21.70s/it]Running Inference:  34%|███▎      | 67/200 [24:23<49:40, 22.41s/it]Running Inference:  34%|███▍      | 68/200 [24:28<37:50, 17.20s/it]Running Inference:  34%|███▍      | 69/200 [24:51<41:45, 19.13s/it]Running Inference:  35%|███▌      | 70/200 [25:16<44:54, 20.73s/it]Running Inference:  36%|███▌      | 71/200 [25:32<41:35, 19.35s/it]Running Inference:  36%|███▌      | 72/200 [25:54<42:44, 20.04s/it]Running Inference:  36%|███▋      | 73/200 [26:18<45:30, 21.50s/it]Running Inference:  37%|███▋      | 74/200 [26:42<46:25, 22.11s/it]Running Inference:  38%|███▊      | 75/200 [27:06<46:59, 22.55s/it]Running Inference:  38%|███▊      | 76/200 [27:30<47:42, 23.09s/it]Running Inference:  38%|███▊      | 77/200 [27:54<47:57, 23.39s/it]Running Inference:  39%|███▉      | 78/200 [28:17<47:29, 23.35s/it]Running Inference:  40%|███▉      | 79/200 [28:39<45:53, 22.75s/it]Running Inference:  40%|████      | 80/200 [29:03<46:26, 23.22s/it]Running Inference:  40%|████      | 81/200 [29:27<46:25, 23.41s/it]Running Inference:  41%|████      | 82/200 [29:51<46:26, 23.61s/it]Running Inference:  42%|████▏     | 83/200 [30:14<45:44, 23.45s/it]Running Inference:  42%|████▏     | 84/200 [30:38<45:39, 23.62s/it]Running Inference:  42%|████▎     | 85/200 [31:02<45:36, 23.79s/it]Running Inference:  43%|████▎     | 86/200 [31:14<38:13, 20.12s/it]Running Inference:  44%|████▎     | 87/200 [31:18<28:51, 15.33s/it]Running Inference:  44%|████▍     | 88/200 [31:44<34:39, 18.57s/it]Running Inference:  44%|████▍     | 89/200 [32:07<37:03, 20.03s/it]Running Inference:  45%|████▌     | 90/200 [32:25<35:15, 19.23s/it]Running Inference:  46%|████▌     | 91/200 [32:49<37:30, 20.64s/it]Running Inference:  46%|████▌     | 92/200 [33:12<38:33, 21.42s/it]Running Inference:  46%|████▋     | 93/200 [33:36<39:29, 22.14s/it]Running Inference:  47%|████▋     | 94/200 [33:59<39:33, 22.40s/it]Running Inference:  48%|████▊     | 95/200 [34:23<40:21, 23.07s/it]Running Inference:  48%|████▊     | 96/200 [34:48<40:29, 23.36s/it]Running Inference:  48%|████▊     | 97/200 [35:11<40:01, 23.32s/it]Running Inference:  49%|████▉     | 98/200 [35:34<39:38, 23.32s/it]Running Inference:  50%|████▉     | 99/200 [35:57<39:17, 23.35s/it]Running Inference:  50%|█████     | 100/200 [36:22<39:21, 23.62s/it]Running Inference:  50%|█████     | 101/200 [36:45<38:47, 23.51s/it]Running Inference:  51%|█████     | 102/200 [37:09<38:45, 23.73s/it]Running Inference:  52%|█████▏    | 103/200 [37:32<38:05, 23.56s/it]Running Inference:  52%|█████▏    | 104/200 [37:49<34:34, 21.61s/it]Running Inference:  52%|█████▎    | 105/200 [38:13<35:03, 22.14s/it]Running Inference:  53%|█████▎    | 106/200 [38:36<35:10, 22.46s/it]Running Inference:  54%|█████▎    | 107/200 [39:01<35:49, 23.11s/it]Running Inference:  54%|█████▍    | 108/200 [39:24<35:25, 23.10s/it]Running Inference:  55%|█████▍    | 109/200 [39:47<35:04, 23.13s/it]Running Inference:  55%|█████▌    | 110/200 [40:00<30:16, 20.19s/it]Running Inference:  56%|█████▌    | 111/200 [40:19<29:22, 19.80s/it]Running Inference:  56%|█████▌    | 112/200 [40:42<30:34, 20.85s/it]Running Inference:  56%|█████▋    | 113/200 [41:06<31:32, 21.75s/it]Running Inference:  57%|█████▋    | 114/200 [41:30<32:09, 22.43s/it]Running Inference:  57%|█████▊    | 115/200 [41:58<34:02, 24.03s/it]Running Inference:  58%|█████▊    | 116/200 [42:21<33:17, 23.78s/it]Running Inference:  58%|█████▊    | 117/200 [42:44<32:38, 23.59s/it]Running Inference:  59%|█████▉    | 118/200 [43:08<32:06, 23.50s/it]Running Inference:  60%|█████▉    | 119/200 [43:33<32:28, 24.05s/it]Running Inference:  60%|██████    | 120/200 [43:57<31:52, 23.91s/it]Running Inference:  60%|██████    | 121/200 [44:21<31:33, 23.97s/it]Running Inference:  61%|██████    | 122/200 [44:44<30:50, 23.73s/it]Running Inference:  62%|██████▏   | 123/200 [45:07<30:19, 23.63s/it]Running Inference:  62%|██████▏   | 124/200 [45:31<29:58, 23.67s/it]Running Inference:  62%|██████▎   | 125/200 [45:52<28:27, 22.76s/it]Running Inference:  63%|██████▎   | 126/200 [46:04<24:19, 19.72s/it]Running Inference:  64%|██████▎   | 127/200 [46:19<22:13, 18.27s/it]Running Inference:  64%|██████▍   | 128/200 [46:42<23:38, 19.71s/it]Running Inference:  64%|██████▍   | 129/200 [47:05<24:32, 20.74s/it]Running Inference:  65%|██████▌   | 130/200 [47:29<25:06, 21.52s/it]Running Inference:  66%|██████▌   | 131/200 [47:31<18:02, 15.68s/it]Running Inference:  66%|██████▌   | 132/200 [47:55<20:46, 18.34s/it]Running Inference:  66%|██████▋   | 133/200 [48:19<22:06, 19.80s/it]Running Inference:  67%|██████▋   | 134/200 [48:43<23:10, 21.07s/it]Running Inference:  68%|██████▊   | 135/200 [49:06<23:33, 21.75s/it]Running Inference:  68%|██████▊   | 136/200 [49:31<24:14, 22.73s/it]Running Inference:  68%|██████▊   | 137/200 [49:56<24:29, 23.33s/it]Running Inference:  69%|██████▉   | 138/200 [50:22<24:53, 24.09s/it]Running Inference:  70%|██████▉   | 139/200 [50:46<24:32, 24.13s/it]Running Inference:  70%|███████   | 140/200 [51:09<23:58, 23.98s/it]Running Inference:  70%|███████   | 141/200 [51:33<23:26, 23.84s/it]Running Inference:  71%|███████   | 142/200 [51:57<23:04, 23.86s/it]Running Inference:  72%|███████▏  | 143/200 [52:21<22:40, 23.86s/it]Running Inference:  72%|███████▏  | 144/200 [52:44<22:07, 23.71s/it]Running Inference:  72%|███████▎  | 145/200 [53:09<22:02, 24.04s/it]Running Inference:  73%|███████▎  | 146/200 [53:32<21:27, 23.84s/it]Running Inference:  74%|███████▎  | 147/200 [54:00<22:02, 24.96s/it]Running Inference:  74%|███████▍  | 148/200 [54:24<21:20, 24.62s/it]Running Inference:  74%|███████▍  | 149/200 [54:48<20:50, 24.52s/it]Running Inference:  75%|███████▌  | 150/200 [55:12<20:18, 24.36s/it]Running Inference:  76%|███████▌  | 151/200 [55:36<19:48, 24.26s/it]Running Inference:  76%|███████▌  | 152/200 [55:59<19:05, 23.87s/it]Running Inference:  76%|███████▋  | 153/200 [56:23<18:39, 23.83s/it]Running Inference:  77%|███████▋  | 154/200 [56:43<17:23, 22.67s/it]Running Inference:  78%|███████▊  | 155/200 [57:07<17:17, 23.05s/it]Running Inference:  78%|███████▊  | 156/200 [57:31<17:08, 23.38s/it]Running Inference:  78%|███████▊  | 157/200 [57:54<16:41, 23.29s/it]Running Inference:  79%|███████▉  | 158/200 [58:19<16:36, 23.72s/it]Running Inference:  80%|███████▉  | 159/200 [58:44<16:29, 24.14s/it]Running Inference:  80%|████████  | 160/200 [59:08<16:10, 24.26s/it]Running Inference:  80%|████████  | 161/200 [59:33<15:50, 24.36s/it]Running Inference:  81%|████████  | 162/200 [59:56<15:11, 23.99s/it]Running Inference:  82%|████████▏ | 163/200 [1:00:11<13:10, 21.37s/it]Running Inference:  82%|████████▏ | 164/200 [1:00:35<13:17, 22.17s/it]Running Inference:  82%|████████▎ | 165/200 [1:00:58<13:04, 22.41s/it]Running Inference:  83%|████████▎ | 166/200 [1:01:21<12:47, 22.57s/it]Running Inference:  84%|████████▎ | 167/200 [1:01:45<12:42, 23.12s/it]Running Inference:  84%|████████▍ | 168/200 [1:02:09<12:23, 23.23s/it]Running Inference:  84%|████████▍ | 169/200 [1:02:31<11:51, 22.94s/it]Running Inference:  85%|████████▌ | 170/200 [1:02:55<11:34, 23.16s/it]Running Inference:  86%|████████▌ | 171/200 [1:03:20<11:31, 23.83s/it]Running Inference:  86%|████████▌ | 172/200 [1:03:44<11:03, 23.70s/it]Running Inference:  86%|████████▋ | 173/200 [1:04:08<10:42, 23.79s/it]Running Inference:  87%|████████▋ | 174/200 [1:04:31<10:17, 23.77s/it]Running Inference:  88%|████████▊ | 175/200 [1:04:50<09:17, 22.31s/it]Running Inference:  88%|████████▊ | 176/200 [1:05:10<08:33, 21.38s/it]Running Inference:  88%|████████▊ | 177/200 [1:05:43<09:36, 25.08s/it]Running Inference:  89%|████████▉ | 178/200 [1:06:08<09:07, 24.87s/it]Running Inference:  90%|████████▉ | 179/200 [1:06:31<08:31, 24.38s/it]Running Inference:  90%|█████████ | 180/200 [1:06:49<07:31, 22.55s/it]Running Inference:  90%|█████████ | 181/200 [1:07:13<07:16, 22.96s/it]Running Inference:  91%|█████████ | 182/200 [1:07:33<06:35, 21.97s/it]Running Inference:  92%|█████████▏| 183/200 [1:08:05<07:05, 25.04s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:21<05:57, 22.34s/it]Running Inference:  92%|█████████▎| 185/200 [1:08:45<05:43, 22.90s/it]Running Inference:  93%|█████████▎| 186/200 [1:09:09<05:22, 23.04s/it]Running Inference:  94%|█████████▎| 187/200 [1:09:32<05:01, 23.16s/it]Running Inference:  94%|█████████▍| 188/200 [1:09:56<04:41, 23.49s/it]Running Inference:  94%|█████████▍| 189/200 [1:10:20<04:18, 23.51s/it]Running Inference:  95%|█████████▌| 190/200 [1:10:43<03:54, 23.50s/it]Running Inference:  96%|█████████▌| 191/200 [1:11:07<03:32, 23.63s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:35<03:18, 24.82s/it]Running Inference:  96%|█████████▋| 193/200 [1:11:58<02:51, 24.44s/it]Running Inference:  97%|█████████▋| 194/200 [1:12:23<02:26, 24.43s/it]Running Inference:  98%|█████████▊| 195/200 [1:12:46<01:59, 23.99s/it]Running Inference:  98%|█████████▊| 196/200 [1:13:09<01:35, 23.87s/it]Running Inference:  98%|█████████▊| 197/200 [1:13:34<01:12, 24.03s/it]Running Inference:  99%|█████████▉| 198/200 [1:14:00<00:49, 24.68s/it]Running Inference: 100%|█████████▉| 199/200 [1:14:23<00:24, 24.21s/it]Running Inference: 100%|██████████| 200/200 [1:14:47<00:00, 23.99s/it]Running Inference: 100%|██████████| 200/200 [1:14:47<00:00, 22.44s/it]
2025-12-13 23:59:28,669 - INFO - Inference completed.
2025-12-13 23:59:28,702 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-13 23:59:28,702 - INFO - Calculating metrics for dataset: longbench
2025-12-13 23:59:47,214 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-13 23:59:47,214 - INFO - Metrics:
14.29
2025-12-13 23:59:47,216 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=gov_report, ratio=0.2) on GPU 2

----------------------------------------
Task: gov_report | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-13 23:59:54,282 - INFO - Set deterministic seeds to 42
2025-12-13 23:59:54,282 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-13 23:59:54,282 - INFO - Starting evaluation run...
2025-12-13 23:59:54,282 - INFO - Output directory set to: longbenchresult
2025-12-13 23:59:54,283 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-13 23:59:54,283 - INFO - KV Press 'streaming_llm' setup.
2025-12-13 23:59:54,283 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-13 23:59:54,283 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.65it/s]
Device set to use cuda:0
2025-12-14 00:00:50,867 - INFO - Model pipeline loaded.
2025-12-14 00:00:50,868 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-14 00:01:03,588 - INFO - Dataset loaded with 200 entries.
2025-12-14 00:01:03,588 - INFO - Dataset processed with 200 entries.
2025-12-14 00:01:03,621 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:24, 24.24s/it]Running Inference:   1%|          | 2/200 [00:37<58:30, 17.73s/it]  Running Inference:   2%|▏         | 3/200 [00:59<1:04:41, 19.70s/it]Running Inference:   2%|▏         | 4/200 [01:00<39:58, 12.24s/it]  Running Inference:   2%|▎         | 5/200 [01:15<43:43, 13.45s/it]Running Inference:   3%|▎         | 6/200 [01:34<49:36, 15.34s/it]Running Inference:   4%|▎         | 7/200 [01:58<58:11, 18.09s/it]Running Inference:   4%|▍         | 8/200 [02:22<1:03:18, 19.78s/it]Running Inference:   4%|▍         | 9/200 [02:45<1:06:42, 20.95s/it]Running Inference:   5%|▌         | 10/200 [03:09<1:08:51, 21.75s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:32<1:09:57, 22.21s/it]Running Inference:   6%|▌         | 12/200 [03:50<1:05:26, 20.89s/it]Running Inference:   6%|▋         | 13/200 [04:14<1:08:12, 21.88s/it]Running Inference:   7%|▋         | 14/200 [04:31<1:03:50, 20.59s/it]Running Inference:   8%|▊         | 15/200 [04:55<1:06:13, 21.48s/it]Running Inference:   8%|▊         | 16/200 [05:15<1:04:23, 21.00s/it]Running Inference:   8%|▊         | 17/200 [05:39<1:07:13, 22.04s/it]Running Inference:   9%|▉         | 18/200 [06:03<1:08:33, 22.60s/it]Running Inference:  10%|▉         | 19/200 [06:29<1:11:01, 23.54s/it]Running Inference:  10%|█         | 20/200 [06:52<1:10:28, 23.49s/it]Running Inference:  10%|█         | 21/200 [07:16<1:10:17, 23.56s/it]Running Inference:  11%|█         | 22/200 [07:40<1:10:19, 23.70s/it]Running Inference:  12%|█▏        | 23/200 [08:04<1:10:16, 23.82s/it]Running Inference:  12%|█▏        | 24/200 [08:30<1:11:14, 24.29s/it]Running Inference:  12%|█▎        | 25/200 [08:53<1:09:45, 23.91s/it]Running Inference:  13%|█▎        | 26/200 [09:16<1:08:57, 23.78s/it]Running Inference:  14%|█▎        | 27/200 [09:39<1:08:11, 23.65s/it]Running Inference:  14%|█▍        | 28/200 [09:44<51:06, 17.83s/it]  Running Inference:  14%|█▍        | 29/200 [10:10<57:45, 20.26s/it]Running Inference:  15%|█▌        | 30/200 [10:34<1:00:33, 21.38s/it]Running Inference:  16%|█▌        | 31/200 [10:57<1:02:06, 22.05s/it]Running Inference:  16%|█▌        | 32/200 [11:22<1:03:42, 22.75s/it]Running Inference:  16%|█▋        | 33/200 [11:44<1:03:24, 22.78s/it]Running Inference:  17%|█▋        | 34/200 [12:09<1:04:09, 23.19s/it]Running Inference:  18%|█▊        | 35/200 [12:10<45:49, 16.66s/it]  Running Inference:  18%|█▊        | 36/200 [12:35<51:57, 19.01s/it]Running Inference:  18%|█▊        | 37/200 [12:58<55:08, 20.30s/it]Running Inference:  19%|█▉        | 38/200 [13:22<57:48, 21.41s/it]Running Inference:  20%|█▉        | 39/200 [13:40<54:57, 20.48s/it]Running Inference:  20%|██        | 40/200 [13:48<44:11, 16.57s/it]Running Inference:  20%|██        | 41/200 [14:12<49:45, 18.78s/it]Running Inference:  21%|██        | 42/200 [14:35<53:24, 20.28s/it]Running Inference:  22%|██▏       | 43/200 [14:59<55:59, 21.40s/it]Running Inference:  22%|██▏       | 44/200 [15:22<56:55, 21.90s/it]Running Inference:  22%|██▎       | 45/200 [15:46<57:30, 22.26s/it]Running Inference:  23%|██▎       | 46/200 [16:08<57:10, 22.27s/it]Running Inference:  24%|██▎       | 47/200 [16:33<58:42, 23.02s/it]Running Inference:  24%|██▍       | 48/200 [16:56<58:38, 23.15s/it]Running Inference:  24%|██▍       | 49/200 [17:15<54:46, 21.77s/it]Running Inference:  25%|██▌       | 50/200 [17:38<55:36, 22.24s/it]Running Inference:  26%|██▌       | 51/200 [18:01<55:46, 22.46s/it]Running Inference:  26%|██▌       | 52/200 [18:25<56:17, 22.82s/it]Running Inference:  26%|██▋       | 53/200 [18:51<58:54, 24.04s/it]Running Inference:  27%|██▋       | 54/200 [19:14<57:41, 23.71s/it]Running Inference:  28%|██▊       | 55/200 [19:38<56:59, 23.58s/it]Running Inference:  28%|██▊       | 56/200 [20:01<56:39, 23.61s/it]Running Inference:  28%|██▊       | 57/200 [20:18<51:03, 21.42s/it]Running Inference:  29%|██▉       | 58/200 [20:41<52:09, 22.04s/it]Running Inference:  30%|██▉       | 59/200 [20:58<47:57, 20.41s/it]Running Inference:  30%|███       | 60/200 [21:23<50:47, 21.77s/it]Running Inference:  30%|███       | 61/200 [21:46<51:39, 22.30s/it]Running Inference:  31%|███       | 62/200 [22:09<51:56, 22.59s/it]Running Inference:  32%|███▏      | 63/200 [22:32<51:46, 22.68s/it]Running Inference:  32%|███▏      | 64/200 [22:56<51:43, 22.82s/it]Running Inference:  32%|███▎      | 65/200 [23:19<51:41, 22.97s/it]Running Inference:  33%|███▎      | 66/200 [23:43<51:46, 23.18s/it]Running Inference:  34%|███▎      | 67/200 [23:58<46:34, 21.01s/it]Running Inference:  34%|███▍      | 68/200 [24:22<47:47, 21.72s/it]Running Inference:  34%|███▍      | 69/200 [24:45<48:32, 22.23s/it]Running Inference:  35%|███▌      | 70/200 [25:10<49:29, 22.84s/it]Running Inference:  36%|███▌      | 71/200 [25:33<49:48, 23.17s/it]Running Inference:  36%|███▌      | 72/200 [25:52<46:33, 21.82s/it]Running Inference:  36%|███▋      | 73/200 [26:17<47:59, 22.67s/it]Running Inference:  37%|███▋      | 74/200 [26:40<48:01, 22.87s/it]Running Inference:  38%|███▊      | 75/200 [27:04<48:00, 23.04s/it]Running Inference:  38%|███▊      | 76/200 [27:28<48:20, 23.39s/it]Running Inference:  38%|███▊      | 77/200 [27:52<48:15, 23.54s/it]Running Inference:  39%|███▉      | 78/200 [28:15<47:35, 23.40s/it]Running Inference:  40%|███▉      | 79/200 [28:38<47:09, 23.38s/it]Running Inference:  40%|████      | 80/200 [28:44<36:04, 18.04s/it]Running Inference:  40%|████      | 81/200 [29:07<39:07, 19.73s/it]Running Inference:  41%|████      | 82/200 [29:31<41:14, 20.97s/it]Running Inference:  42%|████▏     | 83/200 [29:51<40:27, 20.75s/it]Running Inference:  42%|████▏     | 84/200 [30:07<37:19, 19.31s/it]Running Inference:  42%|████▎     | 85/200 [30:31<39:40, 20.70s/it]Running Inference:  43%|████▎     | 86/200 [30:47<36:33, 19.24s/it]Running Inference:  44%|████▎     | 87/200 [31:03<34:24, 18.27s/it]Running Inference:  44%|████▍     | 88/200 [31:29<38:12, 20.47s/it]Running Inference:  44%|████▍     | 89/200 [31:52<39:23, 21.30s/it]Running Inference:  45%|████▌     | 90/200 [32:10<37:13, 20.31s/it]Running Inference:  46%|████▌     | 91/200 [32:34<38:47, 21.35s/it]Running Inference:  46%|████▌     | 92/200 [32:57<39:22, 21.88s/it]Running Inference:  46%|████▋     | 93/200 [33:21<40:00, 22.44s/it]Running Inference:  47%|████▋     | 94/200 [33:43<39:51, 22.56s/it]Running Inference:  48%|████▊     | 95/200 [34:08<40:35, 23.19s/it]Running Inference:  48%|████▊     | 96/200 [34:32<40:33, 23.40s/it]Running Inference:  48%|████▊     | 97/200 [34:55<39:58, 23.29s/it]Running Inference:  49%|████▉     | 98/200 [35:18<39:33, 23.27s/it]Running Inference:  50%|████▉     | 99/200 [35:42<39:10, 23.28s/it]Running Inference:  50%|█████     | 100/200 [36:06<39:08, 23.49s/it]Running Inference:  50%|█████     | 101/200 [36:29<38:33, 23.37s/it]Running Inference:  51%|█████     | 102/200 [36:53<38:33, 23.61s/it]Running Inference:  52%|█████▏    | 103/200 [37:16<37:54, 23.45s/it]Running Inference:  52%|█████▏    | 104/200 [37:40<37:43, 23.58s/it]Running Inference:  52%|█████▎    | 105/200 [38:03<37:11, 23.49s/it]Running Inference:  53%|█████▎    | 106/200 [38:26<36:35, 23.35s/it]Running Inference:  54%|█████▎    | 107/200 [38:39<31:12, 20.14s/it]Running Inference:  54%|█████▍    | 108/200 [39:02<32:11, 21.00s/it]Running Inference:  55%|█████▍    | 109/200 [39:25<32:48, 21.63s/it]Running Inference:  55%|█████▌    | 110/200 [39:41<30:06, 20.07s/it]Running Inference:  56%|█████▌    | 111/200 [40:06<31:41, 21.37s/it]Running Inference:  56%|█████▌    | 112/200 [40:29<32:06, 21.89s/it]Running Inference:  56%|█████▋    | 113/200 [40:53<32:35, 22.48s/it]Running Inference:  57%|█████▋    | 114/200 [41:05<27:57, 19.51s/it]Running Inference:  57%|█████▊    | 115/200 [41:33<30:59, 21.87s/it]Running Inference:  58%|█████▊    | 116/200 [41:56<31:08, 22.24s/it]Running Inference:  58%|█████▊    | 117/200 [42:19<31:04, 22.46s/it]Running Inference:  59%|█████▉    | 118/200 [42:42<30:57, 22.65s/it]Running Inference:  60%|█████▉    | 119/200 [43:07<31:32, 23.36s/it]Running Inference:  60%|██████    | 120/200 [43:30<31:10, 23.38s/it]Running Inference:  60%|██████    | 121/200 [43:54<31:01, 23.57s/it]Running Inference:  61%|██████    | 122/200 [44:17<30:27, 23.43s/it]Running Inference:  62%|██████▏   | 123/200 [44:41<30:01, 23.39s/it]Running Inference:  62%|██████▏   | 124/200 [45:04<29:42, 23.45s/it]Running Inference:  62%|██████▎   | 125/200 [45:28<29:26, 23.55s/it]Running Inference:  63%|██████▎   | 126/200 [45:48<27:47, 22.54s/it]Running Inference:  64%|██████▎   | 127/200 [46:05<25:15, 20.76s/it]Running Inference:  64%|██████▍   | 128/200 [46:28<25:42, 21.42s/it]Running Inference:  64%|██████▍   | 129/200 [46:51<25:55, 21.90s/it]Running Inference:  65%|██████▌   | 130/200 [47:08<23:56, 20.52s/it]Running Inference:  66%|██████▌   | 131/200 [47:10<17:13, 14.98s/it]Running Inference:  66%|██████▌   | 132/200 [47:35<20:11, 17.81s/it]Running Inference:  66%|██████▋   | 133/200 [47:58<21:39, 19.40s/it]Running Inference:  67%|██████▋   | 134/200 [48:17<21:23, 19.45s/it]Running Inference:  68%|██████▊   | 135/200 [48:40<22:16, 20.56s/it]Running Inference:  68%|██████▊   | 136/200 [49:05<23:19, 21.86s/it]Running Inference:  68%|██████▊   | 137/200 [49:30<23:50, 22.70s/it]Running Inference:  69%|██████▉   | 138/200 [49:55<24:18, 23.53s/it]Running Inference:  70%|██████▉   | 139/200 [50:19<24:03, 23.66s/it]Running Inference:  70%|███████   | 140/200 [50:43<23:35, 23.59s/it]Running Inference:  70%|███████   | 141/200 [51:06<23:07, 23.52s/it]Running Inference:  71%|███████   | 142/200 [51:30<22:55, 23.71s/it]Running Inference:  72%|███████▏  | 143/200 [51:55<22:40, 23.87s/it]Running Inference:  72%|███████▏  | 144/200 [52:18<22:12, 23.79s/it]Running Inference:  72%|███████▎  | 145/200 [52:43<22:07, 24.13s/it]Running Inference:  73%|███████▎  | 146/200 [52:59<19:31, 21.69s/it]Running Inference:  74%|███████▎  | 147/200 [53:26<20:37, 23.35s/it]Running Inference:  74%|███████▍  | 148/200 [53:50<20:24, 23.56s/it]Running Inference:  74%|███████▍  | 149/200 [54:15<20:15, 23.84s/it]Running Inference:  75%|███████▌  | 150/200 [54:39<19:56, 23.93s/it]Running Inference:  76%|███████▌  | 151/200 [54:45<15:11, 18.60s/it]Running Inference:  76%|███████▌  | 152/200 [55:08<16:00, 20.00s/it]Running Inference:  76%|███████▋  | 153/200 [55:30<16:00, 20.43s/it]Running Inference:  77%|███████▋  | 154/200 [55:53<16:24, 21.40s/it]Running Inference:  78%|███████▊  | 155/200 [56:18<16:39, 22.20s/it]Running Inference:  78%|███████▊  | 156/200 [56:40<16:19, 22.27s/it]Running Inference:  78%|███████▊  | 157/200 [56:51<13:38, 19.03s/it]Running Inference:  79%|███████▉  | 158/200 [57:16<14:32, 20.77s/it]Running Inference:  80%|███████▉  | 159/200 [57:41<15:04, 22.07s/it]Running Inference:  80%|████████  | 160/200 [57:58<13:42, 20.56s/it]Running Inference:  80%|████████  | 161/200 [58:23<14:10, 21.81s/it]Running Inference:  81%|████████  | 162/200 [58:46<14:06, 22.27s/it]Running Inference:  82%|████████▏ | 163/200 [59:10<13:57, 22.63s/it]Running Inference:  82%|████████▏ | 164/200 [59:33<13:40, 22.80s/it]Running Inference:  82%|████████▎ | 165/200 [59:56<13:21, 22.91s/it]Running Inference:  83%|████████▎ | 166/200 [1:00:19<13:01, 22.99s/it]Running Inference:  84%|████████▎ | 167/200 [1:00:42<12:38, 22.98s/it]Running Inference:  84%|████████▍ | 168/200 [1:01:06<12:21, 23.19s/it]Running Inference:  84%|████████▍ | 169/200 [1:01:30<12:01, 23.27s/it]Running Inference:  85%|████████▌ | 170/200 [1:01:53<11:42, 23.42s/it]Running Inference:  86%|████████▌ | 171/200 [1:02:19<11:35, 23.97s/it]Running Inference:  86%|████████▌ | 172/200 [1:02:42<11:07, 23.82s/it]Running Inference:  86%|████████▋ | 173/200 [1:03:06<10:45, 23.91s/it]Running Inference:  87%|████████▋ | 174/200 [1:03:30<10:21, 23.90s/it]Running Inference:  88%|████████▊ | 175/200 [1:03:50<09:26, 22.66s/it]Running Inference:  88%|████████▊ | 176/200 [1:04:25<10:36, 26.50s/it]Running Inference:  88%|████████▊ | 177/200 [1:04:58<10:55, 28.49s/it]Running Inference:  89%|████████▉ | 178/200 [1:05:23<10:00, 27.30s/it]Running Inference:  90%|████████▉ | 179/200 [1:05:46<09:08, 26.10s/it]Running Inference:  90%|█████████ | 180/200 [1:06:09<08:21, 25.10s/it]Running Inference:  90%|█████████ | 181/200 [1:06:33<07:50, 24.75s/it]Running Inference:  91%|█████████ | 182/200 [1:06:58<07:26, 24.83s/it]Running Inference:  92%|█████████▏| 183/200 [1:07:24<07:09, 25.29s/it]Running Inference:  92%|█████████▏| 184/200 [1:07:40<05:58, 22.40s/it]Running Inference:  92%|█████████▎| 185/200 [1:08:04<05:44, 22.96s/it]Running Inference:  93%|█████████▎| 186/200 [1:08:28<05:23, 23.11s/it]Running Inference:  94%|█████████▎| 187/200 [1:08:51<05:01, 23.23s/it]Running Inference:  94%|█████████▍| 188/200 [1:09:16<04:42, 23.58s/it]Running Inference:  94%|█████████▍| 189/200 [1:09:39<04:19, 23.59s/it]Running Inference:  95%|█████████▌| 190/200 [1:10:03<03:55, 23.57s/it]Running Inference:  96%|█████████▌| 191/200 [1:10:27<03:33, 23.71s/it]Running Inference:  96%|█████████▌| 192/200 [1:10:43<02:52, 21.57s/it]Running Inference:  96%|█████████▋| 193/200 [1:11:07<02:35, 22.21s/it]Running Inference:  97%|█████████▋| 194/200 [1:11:32<02:17, 22.89s/it]Running Inference:  98%|█████████▊| 195/200 [1:11:55<01:54, 22.92s/it]Running Inference:  98%|█████████▊| 196/200 [1:12:18<01:32, 23.15s/it]Running Inference:  98%|█████████▊| 197/200 [1:12:43<01:10, 23.56s/it]Running Inference:  99%|█████████▉| 198/200 [1:13:09<00:48, 24.25s/it]Running Inference: 100%|█████████▉| 199/200 [1:13:32<00:23, 23.92s/it]Running Inference: 100%|██████████| 200/200 [1:13:55<00:00, 23.83s/it]Running Inference: 100%|██████████| 200/200 [1:13:55<00:00, 22.18s/it]
2025-12-14 01:14:59,531 - INFO - Inference completed.
2025-12-14 01:14:59,565 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 01:14:59,565 - INFO - Calculating metrics for dataset: longbench
2025-12-14 01:15:18,012 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 01:15:18,013 - INFO - Metrics:
15.02
2025-12-14 01:15:18,014 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=gov_report, ratio=0.3) on GPU 2

----------------------------------------
Task: gov_report | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 01:15:25,072 - INFO - Set deterministic seeds to 42
2025-12-14 01:15:25,072 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 01:15:25,073 - INFO - Starting evaluation run...
2025-12-14 01:15:25,073 - INFO - Output directory set to: longbenchresult
2025-12-14 01:15:25,073 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 01:15:25,073 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 01:15:25,073 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 01:15:25,073 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.03it/s]
Device set to use cuda:0
2025-12-14 01:15:38,731 - INFO - Model pipeline loaded.
2025-12-14 01:15:38,731 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report)
2025-12-14 01:15:42,562 - INFO - Dataset loaded with 200 entries.
2025-12-14 01:15:42,563 - INFO - Dataset processed with 200 entries.
2025-12-14 01:15:42,594 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:14<49:28, 14.92s/it]Running Inference:   1%|          | 2/200 [00:28<45:47, 13.88s/it]Running Inference:   2%|▏         | 3/200 [00:46<52:56, 16.12s/it]Running Inference:   2%|▏         | 4/200 [01:09<1:01:21, 18.78s/it]Running Inference:   2%|▎         | 5/200 [01:18<48:55, 15.05s/it]  Running Inference:   3%|▎         | 6/200 [01:42<59:25, 18.38s/it]Running Inference:   4%|▎         | 7/200 [01:58<56:16, 17.49s/it]Running Inference:   4%|▍         | 8/200 [02:22<1:02:04, 19.40s/it]Running Inference:   4%|▍         | 9/200 [02:45<1:05:55, 20.71s/it]Running Inference:   5%|▌         | 10/200 [03:09<1:08:21, 21.59s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [03:32<1:09:40, 22.12s/it]Running Inference:   6%|▌         | 12/200 [03:50<1:05:22, 20.86s/it]Running Inference:   6%|▋         | 13/200 [04:14<1:08:10, 21.88s/it]Running Inference:   7%|▋         | 14/200 [04:38<1:09:32, 22.43s/it]Running Inference:   8%|▊         | 15/200 [04:54<1:03:18, 20.53s/it]Running Inference:   8%|▊         | 16/200 [05:08<56:24, 18.39s/it]  Running Inference:   8%|▊         | 17/200 [05:32<1:01:43, 20.24s/it]Running Inference:   9%|▉         | 18/200 [05:56<1:04:47, 21.36s/it]Running Inference:  10%|▉         | 19/200 [06:22<1:08:12, 22.61s/it]Running Inference:  10%|█         | 20/200 [06:45<1:08:33, 22.85s/it]Running Inference:  10%|█         | 21/200 [07:09<1:08:58, 23.12s/it]Running Inference:  11%|█         | 22/200 [07:33<1:09:25, 23.40s/it]Running Inference:  12%|█▏        | 23/200 [07:51<1:04:36, 21.90s/it]Running Inference:  12%|█▏        | 24/200 [08:14<1:05:10, 22.22s/it]Running Inference:  12%|█▎        | 25/200 [08:37<1:05:35, 22.49s/it]Running Inference:  13%|█▎        | 26/200 [09:01<1:06:03, 22.78s/it]Running Inference:  14%|█▎        | 27/200 [09:23<1:05:04, 22.57s/it]Running Inference:  14%|█▍        | 28/200 [09:36<56:26, 19.69s/it]  Running Inference:  14%|█▍        | 29/200 [10:01<1:01:12, 21.48s/it]Running Inference:  15%|█▌        | 30/200 [10:25<1:02:54, 22.20s/it]Running Inference:  16%|█▌        | 31/200 [10:49<1:03:44, 22.63s/it]Running Inference:  16%|█▌        | 32/200 [11:11<1:03:11, 22.57s/it]Running Inference:  16%|█▋        | 33/200 [11:34<1:03:02, 22.65s/it]Running Inference:  17%|█▋        | 34/200 [11:58<1:03:54, 23.10s/it]Running Inference:  18%|█▊        | 35/200 [12:14<57:33, 20.93s/it]  Running Inference:  18%|█▊        | 36/200 [12:39<1:00:11, 22.02s/it]Running Inference:  18%|█▊        | 37/200 [13:02<1:00:55, 22.43s/it]Running Inference:  19%|█▉        | 38/200 [13:26<1:01:58, 22.95s/it]Running Inference:  20%|█▉        | 39/200 [13:50<1:02:03, 23.12s/it]Running Inference:  20%|██        | 40/200 [14:14<1:02:14, 23.34s/it]Running Inference:  20%|██        | 41/200 [14:38<1:02:32, 23.60s/it]Running Inference:  21%|██        | 42/200 [14:40<45:17, 17.20s/it]  Running Inference:  22%|██▏       | 43/200 [15:04<50:20, 19.24s/it]Running Inference:  22%|██▏       | 44/200 [15:06<36:32, 14.05s/it]Running Inference:  22%|██▎       | 45/200 [15:30<43:29, 16.84s/it]Running Inference:  23%|██▎       | 46/200 [15:42<39:35, 15.42s/it]Running Inference:  24%|██▎       | 47/200 [16:07<46:34, 18.26s/it]Running Inference:  24%|██▍       | 48/200 [16:30<50:16, 19.84s/it]Running Inference:  24%|██▍       | 49/200 [16:48<48:50, 19.41s/it]Running Inference:  25%|██▌       | 50/200 [17:12<51:32, 20.62s/it]Running Inference:  26%|██▌       | 51/200 [17:35<53:01, 21.35s/it]Running Inference:  26%|██▌       | 52/200 [17:57<53:05, 21.52s/it]Running Inference:  26%|██▋       | 53/200 [18:23<56:14, 22.96s/it]Running Inference:  27%|██▋       | 54/200 [18:46<55:52, 22.97s/it]Running Inference:  28%|██▊       | 55/200 [19:09<55:46, 23.08s/it]Running Inference:  28%|██▊       | 56/200 [19:33<55:53, 23.29s/it]Running Inference:  28%|██▊       | 57/200 [19:50<51:06, 21.45s/it]Running Inference:  29%|██▉       | 58/200 [20:14<52:16, 22.09s/it]Running Inference:  30%|██▉       | 59/200 [20:38<53:01, 22.56s/it]Running Inference:  30%|███       | 60/200 [21:03<54:17, 23.27s/it]Running Inference:  30%|███       | 61/200 [21:26<54:03, 23.34s/it]Running Inference:  31%|███       | 62/200 [21:49<53:35, 23.30s/it]Running Inference:  32%|███▏      | 63/200 [22:12<52:51, 23.15s/it]Running Inference:  32%|███▏      | 64/200 [22:35<52:27, 23.14s/it]Running Inference:  32%|███▎      | 65/200 [22:59<52:10, 23.19s/it]Running Inference:  33%|███▎      | 66/200 [23:22<52:03, 23.31s/it]Running Inference:  34%|███▎      | 67/200 [23:46<52:05, 23.50s/it]Running Inference:  34%|███▍      | 68/200 [24:09<51:38, 23.47s/it]Running Inference:  34%|███▍      | 69/200 [24:33<51:12, 23.46s/it]Running Inference:  35%|███▌      | 70/200 [24:53<48:50, 22.54s/it]Running Inference:  36%|███▌      | 71/200 [25:17<49:20, 22.95s/it]Running Inference:  36%|███▌      | 72/200 [25:36<46:05, 21.60s/it]Running Inference:  36%|███▋      | 73/200 [26:00<47:40, 22.52s/it]Running Inference:  37%|███▋      | 74/200 [26:24<47:48, 22.76s/it]Running Inference:  38%|███▊      | 75/200 [26:44<45:56, 22.05s/it]Running Inference:  38%|███▊      | 76/200 [27:08<46:52, 22.68s/it]Running Inference:  38%|███▊      | 77/200 [27:32<47:12, 23.03s/it]Running Inference:  39%|███▉      | 78/200 [27:55<46:49, 23.03s/it]Running Inference:  40%|███▉      | 79/200 [28:18<46:36, 23.11s/it]Running Inference:  40%|████      | 80/200 [28:43<46:51, 23.43s/it]Running Inference:  40%|████      | 81/200 [29:06<46:39, 23.52s/it]Running Inference:  41%|████      | 82/200 [29:30<46:26, 23.62s/it]Running Inference:  42%|████▏     | 83/200 [29:53<45:34, 23.37s/it]Running Inference:  42%|████▏     | 84/200 [30:17<45:24, 23.48s/it]Running Inference:  42%|████▎     | 85/200 [30:41<45:18, 23.64s/it]Running Inference:  43%|████▎     | 86/200 [30:52<38:00, 20.01s/it]Running Inference:  44%|████▎     | 87/200 [31:16<39:35, 21.03s/it]Running Inference:  44%|████▍     | 88/200 [31:41<41:40, 22.33s/it]Running Inference:  44%|████▍     | 89/200 [32:04<41:48, 22.60s/it]Running Inference:  45%|████▌     | 90/200 [32:28<41:52, 22.85s/it]Running Inference:  46%|████▌     | 91/200 [32:51<42:04, 23.16s/it]Running Inference:  46%|████▌     | 92/200 [33:05<36:29, 20.27s/it]Running Inference:  46%|████▋     | 93/200 [33:29<37:55, 21.27s/it]Running Inference:  47%|████▋     | 94/200 [33:51<38:23, 21.73s/it]Running Inference:  48%|████▊     | 95/200 [34:16<39:25, 22.53s/it]Running Inference:  48%|████▊     | 96/200 [34:40<39:45, 22.94s/it]Running Inference:  48%|████▊     | 97/200 [35:03<39:23, 22.95s/it]Running Inference:  49%|████▉     | 98/200 [35:06<28:44, 16.91s/it]Running Inference:  50%|████▉     | 99/200 [35:29<31:37, 18.79s/it]Running Inference:  50%|█████     | 100/200 [35:49<32:11, 19.31s/it]Running Inference:  50%|█████     | 101/200 [36:12<33:41, 20.42s/it]Running Inference:  51%|█████     | 102/200 [36:36<35:07, 21.50s/it]Running Inference:  52%|█████▏    | 103/200 [36:59<35:29, 21.96s/it]Running Inference:  52%|█████▏    | 104/200 [37:23<36:03, 22.53s/it]Running Inference:  52%|█████▎    | 105/200 [37:46<36:00, 22.74s/it]Running Inference:  53%|█████▎    | 106/200 [38:09<35:44, 22.81s/it]Running Inference:  54%|█████▎    | 107/200 [38:34<36:05, 23.29s/it]Running Inference:  54%|█████▍    | 108/200 [38:57<35:35, 23.21s/it]Running Inference:  55%|█████▍    | 109/200 [39:20<35:07, 23.16s/it]Running Inference:  55%|█████▌    | 110/200 [39:43<34:56, 23.30s/it]Running Inference:  56%|█████▌    | 111/200 [39:57<30:02, 20.25s/it]Running Inference:  56%|█████▌    | 112/200 [40:20<30:56, 21.09s/it]Running Inference:  56%|█████▋    | 113/200 [40:43<31:43, 21.87s/it]Running Inference:  57%|█████▋    | 114/200 [41:04<30:51, 21.53s/it]Running Inference:  57%|█████▊    | 115/200 [41:30<32:34, 23.00s/it]Running Inference:  58%|█████▊    | 116/200 [41:54<32:13, 23.02s/it]Running Inference:  58%|█████▊    | 117/200 [42:16<31:47, 22.98s/it]Running Inference:  59%|█████▉    | 118/200 [42:20<23:24, 17.13s/it]Running Inference:  60%|█████▉    | 119/200 [42:45<26:17, 19.47s/it]Running Inference:  60%|██████    | 120/200 [43:08<27:31, 20.64s/it]Running Inference:  60%|██████    | 121/200 [43:32<28:28, 21.62s/it]Running Inference:  61%|██████    | 122/200 [43:55<28:39, 22.04s/it]Running Inference:  62%|██████▏   | 123/200 [44:18<28:44, 22.39s/it]Running Inference:  62%|██████▏   | 124/200 [44:42<28:47, 22.73s/it]Running Inference:  62%|██████▎   | 125/200 [45:06<28:46, 23.02s/it]Running Inference:  63%|██████▎   | 126/200 [45:29<28:22, 23.01s/it]Running Inference:  64%|██████▎   | 127/200 [45:46<26:03, 21.42s/it]Running Inference:  64%|██████▍   | 128/200 [46:09<26:12, 21.84s/it]Running Inference:  64%|██████▍   | 129/200 [46:32<26:15, 22.19s/it]Running Inference:  65%|██████▌   | 130/200 [46:51<24:42, 21.18s/it]Running Inference:  66%|██████▌   | 131/200 [47:14<24:59, 21.74s/it]Running Inference:  66%|██████▌   | 132/200 [47:38<25:31, 22.52s/it]Running Inference:  66%|██████▋   | 133/200 [48:01<25:19, 22.67s/it]Running Inference:  67%|██████▋   | 134/200 [48:25<25:19, 23.02s/it]Running Inference:  68%|██████▊   | 135/200 [48:48<24:57, 23.05s/it]Running Inference:  68%|██████▊   | 136/200 [49:13<25:08, 23.57s/it]Running Inference:  68%|██████▊   | 137/200 [49:38<25:03, 23.86s/it]Running Inference:  69%|██████▉   | 138/200 [50:03<25:05, 24.28s/it]Running Inference:  70%|██████▉   | 139/200 [50:27<24:33, 24.16s/it]Running Inference:  70%|███████   | 140/200 [50:50<23:54, 23.90s/it]Running Inference:  70%|███████   | 141/200 [51:13<23:17, 23.69s/it]Running Inference:  71%|███████   | 142/200 [51:37<22:54, 23.70s/it]Running Inference:  72%|███████▏  | 143/200 [52:01<22:34, 23.77s/it]Running Inference:  72%|███████▏  | 144/200 [52:24<22:01, 23.61s/it]Running Inference:  72%|███████▎  | 145/200 [52:49<21:54, 23.90s/it]Running Inference:  73%|███████▎  | 146/200 [53:12<21:20, 23.71s/it]Running Inference:  74%|███████▎  | 147/200 [53:38<21:36, 24.45s/it]Running Inference:  74%|███████▍  | 148/200 [54:02<21:00, 24.24s/it]Running Inference:  74%|███████▍  | 149/200 [54:26<20:34, 24.21s/it]Running Inference:  75%|███████▌  | 150/200 [54:50<20:05, 24.10s/it]Running Inference:  76%|███████▌  | 151/200 [55:14<19:35, 23.99s/it]Running Inference:  76%|███████▌  | 152/200 [55:36<18:55, 23.66s/it]Running Inference:  76%|███████▋  | 153/200 [55:57<17:53, 22.84s/it]Running Inference:  77%|███████▋  | 154/200 [56:21<17:35, 22.95s/it]Running Inference:  78%|███████▊  | 155/200 [56:44<17:23, 23.19s/it]Running Inference:  78%|███████▊  | 156/200 [57:08<17:11, 23.44s/it]Running Inference:  78%|███████▊  | 157/200 [57:23<14:54, 20.81s/it]Running Inference:  79%|███████▉  | 158/200 [57:48<15:20, 21.91s/it]Running Inference:  80%|███████▉  | 159/200 [58:13<15:35, 22.82s/it]Running Inference:  80%|████████  | 160/200 [58:31<14:26, 21.67s/it]Running Inference:  80%|████████  | 161/200 [58:56<14:36, 22.48s/it]Running Inference:  81%|████████  | 162/200 [59:17<13:56, 22.00s/it]Running Inference:  82%|████████▏ | 163/200 [59:40<13:47, 22.36s/it]Running Inference:  82%|████████▏ | 164/200 [1:00:05<13:51, 23.09s/it]Running Inference:  82%|████████▎ | 165/200 [1:00:28<13:25, 23.01s/it]Running Inference:  83%|████████▎ | 166/200 [1:00:50<13:00, 22.96s/it]Running Inference:  84%|████████▎ | 167/200 [1:01:12<12:19, 22.42s/it]Running Inference:  84%|████████▍ | 168/200 [1:01:35<12:05, 22.68s/it]Running Inference:  84%|████████▍ | 169/200 [1:01:58<11:46, 22.79s/it]Running Inference:  85%|████████▌ | 170/200 [1:02:21<11:29, 22.99s/it]Running Inference:  86%|████████▌ | 171/200 [1:02:46<11:22, 23.53s/it]Running Inference:  86%|████████▌ | 172/200 [1:03:09<10:55, 23.42s/it]Running Inference:  86%|████████▋ | 173/200 [1:03:33<10:35, 23.53s/it]Running Inference:  87%|████████▋ | 174/200 [1:03:57<10:11, 23.54s/it]Running Inference:  88%|████████▊ | 175/200 [1:04:20<09:44, 23.39s/it]Running Inference:  88%|████████▊ | 176/200 [1:04:53<10:34, 26.45s/it]Running Inference:  88%|████████▊ | 177/200 [1:05:25<10:42, 27.95s/it]Running Inference:  89%|████████▉ | 178/200 [1:05:49<09:49, 26.81s/it]Running Inference:  90%|████████▉ | 179/200 [1:06:12<08:58, 25.67s/it]Running Inference:  90%|█████████ | 180/200 [1:06:33<08:04, 24.23s/it]Running Inference:  90%|█████████ | 181/200 [1:06:56<07:37, 24.06s/it]Running Inference:  91%|█████████ | 182/200 [1:07:12<06:25, 21.43s/it]Running Inference:  92%|█████████▏| 183/200 [1:07:37<06:26, 22.72s/it]Running Inference:  92%|█████████▏| 184/200 [1:08:00<06:04, 22.80s/it]Running Inference:  92%|█████████▎| 185/200 [1:08:24<05:47, 23.14s/it]Running Inference:  93%|█████████▎| 186/200 [1:08:48<05:24, 23.15s/it]Running Inference:  94%|█████████▎| 187/200 [1:09:11<05:01, 23.16s/it]Running Inference:  94%|█████████▍| 188/200 [1:09:35<04:41, 23.44s/it]Running Inference:  94%|█████████▍| 189/200 [1:09:58<04:17, 23.39s/it]Running Inference:  95%|█████████▌| 190/200 [1:10:21<03:53, 23.34s/it]Running Inference:  96%|█████████▌| 191/200 [1:10:45<03:31, 23.46s/it]Running Inference:  96%|█████████▌| 192/200 [1:11:11<03:14, 24.30s/it]Running Inference:  96%|█████████▋| 193/200 [1:11:35<02:48, 24.02s/it]Running Inference:  97%|█████████▋| 194/200 [1:11:59<02:24, 24.06s/it]Running Inference:  98%|█████████▊| 195/200 [1:12:22<01:58, 23.67s/it]Running Inference:  98%|█████████▊| 196/200 [1:12:45<01:34, 23.60s/it]Running Inference:  98%|█████████▊| 197/200 [1:13:09<01:11, 23.80s/it]Running Inference:  99%|█████████▉| 198/200 [1:13:35<00:48, 24.27s/it]Running Inference: 100%|█████████▉| 199/200 [1:13:52<00:22, 22.29s/it]Running Inference: 100%|██████████| 200/200 [1:14:16<00:00, 22.60s/it]Running Inference: 100%|██████████| 200/200 [1:14:16<00:00, 22.28s/it]
2025-12-14 02:29:58,766 - INFO - Inference completed.
2025-12-14 02:29:58,800 - INFO - Results saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 02:29:58,800 - INFO - Calculating metrics for dataset: longbench
2025-12-14 02:30:18,059 - INFO - Metrics saved to longbenchresult/longbench__gov_report__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 02:30:18,059 - INFO - Metrics:
14.01
2025-12-14 02:30:18,061 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=gov_report, ratio=0.5) on GPU 2


========================================
LongBench Task: multi_news
========================================
----------------------------------------
Task: multi_news | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 02:30:25,323 - INFO - Set deterministic seeds to 42
2025-12-14 02:30:25,323 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 02:30:25,323 - INFO - Starting evaluation run...
2025-12-14 02:30:25,323 - INFO - Output directory set to: longbenchresult
2025-12-14 02:30:25,323 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 02:30:25,323 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 02:30:25,323 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 02:30:25,323 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.94it/s]
Device set to use cuda:0
2025-12-14 02:30:35,979 - INFO - Model pipeline loaded.
2025-12-14 02:30:35,979 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 02:30:39,045 - INFO - Dataset loaded with 200 entries.
2025-12-14 02:30:39,045 - INFO - Dataset processed with 200 entries.
2025-12-14 02:30:39,055 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:22:25, 24.85s/it]Running Inference:   1%|          | 2/200 [00:48<1:20:07, 24.28s/it]Running Inference:   2%|▏         | 3/200 [01:12<1:19:14, 24.14s/it]Running Inference:   2%|▏         | 4/200 [01:36<1:18:33, 24.05s/it]Running Inference:   2%|▎         | 5/200 [02:00<1:18:01, 24.01s/it]Running Inference:   3%|▎         | 6/200 [02:24<1:17:31, 23.98s/it]Running Inference:   4%|▎         | 7/200 [02:48<1:17:18, 24.04s/it]Running Inference:   4%|▍         | 8/200 [03:12<1:16:55, 24.04s/it]Running Inference:   4%|▍         | 9/200 [03:36<1:16:26, 24.01s/it]Running Inference:   5%|▌         | 10/200 [04:00<1:15:52, 23.96s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:24<1:15:24, 23.94s/it]Running Inference:   6%|▌         | 12/200 [04:48<1:15:05, 23.96s/it]Running Inference:   6%|▋         | 13/200 [05:12<1:14:41, 23.96s/it]Running Inference:   7%|▋         | 14/200 [05:36<1:14:11, 23.93s/it]Running Inference:   8%|▊         | 15/200 [06:00<1:13:58, 23.99s/it]Running Inference:   8%|▊         | 16/200 [06:24<1:13:37, 24.01s/it]Running Inference:   8%|▊         | 17/200 [06:45<1:10:57, 23.26s/it]Running Inference:   9%|▉         | 18/200 [07:10<1:11:43, 23.65s/it]Running Inference:  10%|▉         | 19/200 [07:34<1:11:36, 23.74s/it]Running Inference:  10%|█         | 20/200 [07:58<1:11:15, 23.75s/it]Running Inference:  10%|█         | 21/200 [08:21<1:10:53, 23.76s/it]Running Inference:  11%|█         | 22/200 [08:42<1:07:28, 22.74s/it]Running Inference:  12%|█▏        | 23/200 [09:06<1:08:02, 23.07s/it]Running Inference:  12%|█▏        | 24/200 [09:30<1:08:35, 23.38s/it]Running Inference:  12%|█▎        | 25/200 [09:54<1:08:35, 23.52s/it]Running Inference:  13%|█▎        | 26/200 [10:18<1:08:33, 23.64s/it]Running Inference:  14%|█▎        | 27/200 [10:43<1:09:32, 24.12s/it]Running Inference:  14%|█▍        | 28/200 [11:07<1:09:17, 24.17s/it]Running Inference:  14%|█▍        | 29/200 [11:25<1:03:32, 22.30s/it]Running Inference:  15%|█▌        | 30/200 [11:49<1:04:31, 22.77s/it]Running Inference:  16%|█▌        | 31/200 [12:13<1:05:15, 23.17s/it]Running Inference:  16%|█▌        | 32/200 [12:37<1:05:27, 23.38s/it]Running Inference:  16%|█▋        | 33/200 [13:01<1:05:28, 23.52s/it]Running Inference:  17%|█▋        | 34/200 [13:25<1:05:27, 23.66s/it]Running Inference:  18%|█▊        | 35/200 [13:49<1:05:14, 23.72s/it]Running Inference:  18%|█▊        | 36/200 [14:12<1:05:01, 23.79s/it]Running Inference:  18%|█▊        | 37/200 [14:37<1:05:00, 23.93s/it]Running Inference:  19%|█▉        | 38/200 [15:01<1:05:03, 24.09s/it]Running Inference:  20%|█▉        | 39/200 [15:25<1:04:28, 24.03s/it]Running Inference:  20%|██        | 40/200 [15:49<1:03:54, 23.96s/it]Running Inference:  20%|██        | 41/200 [16:13<1:03:30, 23.97s/it]Running Inference:  21%|██        | 42/200 [16:23<51:49, 19.68s/it]  Running Inference:  22%|██▏       | 43/200 [16:46<54:47, 20.94s/it]Running Inference:  22%|██▏       | 44/200 [17:10<56:45, 21.83s/it]Running Inference:  22%|██▎       | 45/200 [17:34<58:03, 22.47s/it]Running Inference:  23%|██▎       | 46/200 [17:53<54:52, 21.38s/it]Running Inference:  24%|██▎       | 47/200 [18:17<56:45, 22.26s/it]Running Inference:  24%|██▍       | 48/200 [18:42<57:48, 22.82s/it]Running Inference:  24%|██▍       | 49/200 [19:05<58:13, 23.14s/it]Running Inference:  25%|██▌       | 50/200 [19:30<58:34, 23.43s/it]Running Inference:  26%|██▌       | 51/200 [19:54<58:36, 23.60s/it]Running Inference:  26%|██▌       | 52/200 [20:18<58:29, 23.71s/it]Running Inference:  26%|██▋       | 53/200 [20:34<52:56, 21.61s/it]Running Inference:  27%|██▋       | 54/200 [20:58<54:16, 22.30s/it]Running Inference:  28%|██▊       | 55/200 [21:19<52:50, 21.87s/it]Running Inference:  28%|██▊       | 56/200 [21:43<54:01, 22.51s/it]Running Inference:  28%|██▊       | 57/200 [22:07<54:45, 22.97s/it]Running Inference:  29%|██▉       | 58/200 [22:31<54:59, 23.24s/it]Running Inference:  30%|██▉       | 59/200 [22:55<55:05, 23.44s/it]Running Inference:  30%|███       | 60/200 [23:19<54:57, 23.56s/it]Running Inference:  30%|███       | 61/200 [23:43<54:52, 23.69s/it]Running Inference:  31%|███       | 62/200 [23:59<49:11, 21.38s/it]Running Inference:  32%|███▏      | 63/200 [24:23<50:33, 22.14s/it]Running Inference:  32%|███▏      | 64/200 [24:47<51:23, 22.67s/it]Running Inference:  32%|███▎      | 65/200 [25:11<52:07, 23.16s/it]Running Inference:  33%|███▎      | 66/200 [25:35<52:12, 23.38s/it]Running Inference:  34%|███▎      | 67/200 [25:59<52:31, 23.69s/it]Running Inference:  34%|███▍      | 68/200 [26:23<52:21, 23.80s/it]Running Inference:  34%|███▍      | 69/200 [26:46<51:16, 23.48s/it]Running Inference:  35%|███▌      | 70/200 [27:10<51:08, 23.60s/it]Running Inference:  36%|███▌      | 71/200 [27:34<50:57, 23.70s/it]Running Inference:  36%|███▌      | 72/200 [27:58<50:49, 23.83s/it]Running Inference:  36%|███▋      | 73/200 [28:22<50:26, 23.83s/it]Running Inference:  37%|███▋      | 74/200 [28:46<50:05, 23.86s/it]Running Inference:  38%|███▊      | 75/200 [29:10<49:45, 23.88s/it]Running Inference:  38%|███▊      | 76/200 [29:33<49:19, 23.86s/it]Running Inference:  38%|███▊      | 77/200 [29:57<48:56, 23.87s/it]Running Inference:  39%|███▉      | 78/200 [30:21<48:35, 23.89s/it]Running Inference:  40%|███▉      | 79/200 [30:45<48:07, 23.86s/it]Running Inference:  40%|████      | 80/200 [31:09<47:47, 23.90s/it]Running Inference:  40%|████      | 81/200 [31:33<47:21, 23.88s/it]Running Inference:  41%|████      | 82/200 [31:54<45:21, 23.06s/it]Running Inference:  42%|████▏     | 83/200 [32:18<45:28, 23.32s/it]Running Inference:  42%|████▏     | 84/200 [32:42<45:32, 23.56s/it]Running Inference:  42%|████▎     | 85/200 [33:02<43:14, 22.56s/it]Running Inference:  43%|████▎     | 86/200 [33:22<41:12, 21.69s/it]Running Inference:  44%|████▎     | 87/200 [33:46<42:03, 22.33s/it]Running Inference:  44%|████▍     | 88/200 [34:10<42:39, 22.85s/it]Running Inference:  44%|████▍     | 89/200 [34:34<43:02, 23.26s/it]Running Inference:  45%|████▌     | 90/200 [34:58<43:07, 23.52s/it]Running Inference:  46%|████▌     | 91/200 [35:22<42:58, 23.66s/it]Running Inference:  46%|████▌     | 92/200 [35:46<42:44, 23.75s/it]Running Inference:  46%|████▋     | 93/200 [36:10<42:23, 23.77s/it]Running Inference:  47%|████▋     | 94/200 [36:34<42:08, 23.85s/it]Running Inference:  48%|████▊     | 95/200 [36:58<41:51, 23.92s/it]Running Inference:  48%|████▊     | 96/200 [37:22<41:29, 23.94s/it]Running Inference:  48%|████▊     | 97/200 [37:46<41:08, 23.96s/it]Running Inference:  49%|████▉     | 98/200 [38:10<40:45, 23.98s/it]Running Inference:  50%|████▉     | 99/200 [38:35<41:03, 24.39s/it]Running Inference:  50%|█████     | 100/200 [38:59<40:28, 24.28s/it]Running Inference:  50%|█████     | 101/200 [39:23<39:56, 24.20s/it]Running Inference:  51%|█████     | 102/200 [39:47<39:22, 24.11s/it]Running Inference:  52%|█████▏    | 103/200 [40:12<39:07, 24.20s/it]Running Inference:  52%|█████▏    | 104/200 [40:36<38:48, 24.26s/it]Running Inference:  52%|█████▎    | 105/200 [41:00<38:18, 24.19s/it]Running Inference:  53%|█████▎    | 106/200 [41:24<37:47, 24.12s/it]Running Inference:  54%|█████▎    | 107/200 [41:48<37:19, 24.08s/it]Running Inference:  54%|█████▍    | 108/200 [42:12<36:53, 24.06s/it]Running Inference:  55%|█████▍    | 109/200 [42:36<36:24, 24.00s/it]Running Inference:  55%|█████▌    | 110/200 [43:00<35:55, 23.95s/it]Running Inference:  56%|█████▌    | 111/200 [43:24<35:32, 23.96s/it]Running Inference:  56%|█████▌    | 112/200 [43:48<35:05, 23.92s/it]Running Inference:  56%|█████▋    | 113/200 [44:12<34:45, 23.97s/it]Running Inference:  57%|█████▋    | 114/200 [44:36<34:17, 23.93s/it]Running Inference:  57%|█████▊    | 115/200 [44:59<33:51, 23.90s/it]Running Inference:  58%|█████▊    | 116/200 [45:21<32:41, 23.35s/it]Running Inference:  58%|█████▊    | 117/200 [45:45<32:31, 23.51s/it]Running Inference:  59%|█████▉    | 118/200 [46:09<32:17, 23.62s/it]Running Inference:  60%|█████▉    | 119/200 [46:30<30:54, 22.89s/it]Running Inference:  60%|██████    | 120/200 [46:54<30:44, 23.05s/it]Running Inference:  60%|██████    | 121/200 [47:18<30:41, 23.31s/it]Running Inference:  61%|██████    | 122/200 [47:42<30:31, 23.48s/it]Running Inference:  62%|██████▏   | 123/200 [48:06<30:20, 23.64s/it]Running Inference:  62%|██████▏   | 124/200 [48:29<29:59, 23.67s/it]Running Inference:  62%|██████▎   | 125/200 [48:53<29:40, 23.74s/it]Running Inference:  63%|██████▎   | 126/200 [49:17<29:17, 23.75s/it]Running Inference:  64%|██████▎   | 127/200 [49:41<28:59, 23.83s/it]Running Inference:  64%|██████▍   | 128/200 [50:06<28:56, 24.11s/it]Running Inference:  64%|██████▍   | 129/200 [50:30<28:25, 24.02s/it]Running Inference:  65%|██████▌   | 130/200 [50:53<27:57, 23.96s/it]Running Inference:  66%|██████▌   | 131/200 [51:15<26:39, 23.18s/it]Running Inference:  66%|██████▌   | 132/200 [51:39<26:31, 23.41s/it]Running Inference:  66%|██████▋   | 133/200 [52:03<26:17, 23.54s/it]Running Inference:  67%|██████▋   | 134/200 [52:27<26:01, 23.66s/it]Running Inference:  68%|██████▊   | 135/200 [52:51<25:53, 23.90s/it]Running Inference:  68%|██████▊   | 136/200 [53:15<25:34, 23.97s/it]Running Inference:  68%|██████▊   | 137/200 [53:39<25:11, 24.00s/it]Running Inference:  69%|██████▉   | 138/200 [54:03<24:45, 23.96s/it]Running Inference:  70%|██████▉   | 139/200 [54:27<24:19, 23.93s/it]Running Inference:  70%|███████   | 140/200 [54:51<23:56, 23.94s/it]Running Inference:  70%|███████   | 141/200 [55:04<20:25, 20.77s/it]Running Inference:  71%|███████   | 142/200 [55:22<19:10, 19.84s/it]Running Inference:  72%|███████▏  | 143/200 [55:46<19:59, 21.05s/it]Running Inference:  72%|███████▏  | 144/200 [56:10<20:25, 21.89s/it]Running Inference:  72%|███████▎  | 145/200 [56:34<20:37, 22.51s/it]Running Inference:  73%|███████▎  | 146/200 [56:57<20:36, 22.91s/it]Running Inference:  74%|███████▎  | 147/200 [57:21<20:29, 23.20s/it]Running Inference:  74%|███████▍  | 148/200 [57:45<20:15, 23.38s/it]Running Inference:  74%|███████▍  | 149/200 [58:09<19:59, 23.52s/it]Running Inference:  75%|███████▌  | 150/200 [58:31<19:06, 22.93s/it]Running Inference:  76%|███████▌  | 151/200 [58:54<18:57, 23.22s/it]Running Inference:  76%|███████▌  | 152/200 [59:18<18:45, 23.45s/it]Running Inference:  76%|███████▋  | 153/200 [59:43<18:34, 23.71s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:07<18:15, 23.82s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:31<17:57, 23.94s/it]Running Inference:  78%|███████▊  | 156/200 [1:00:55<17:31, 23.89s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:19<17:11, 23.98s/it]Running Inference:  79%|███████▉  | 158/200 [1:01:43<16:49, 24.04s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:07<16:23, 23.99s/it]Running Inference:  80%|████████  | 160/200 [1:02:25<14:43, 22.10s/it]Running Inference:  80%|████████  | 161/200 [1:02:49<14:43, 22.65s/it]Running Inference:  81%|████████  | 162/200 [1:03:13<14:34, 23.02s/it]Running Inference:  82%|████████▏ | 163/200 [1:03:36<14:21, 23.29s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:01<14:07, 23.55s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:24<13:47, 23.63s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:48<13:26, 23.72s/it]Running Inference:  84%|████████▎ | 167/200 [1:05:12<13:04, 23.79s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:36<12:41, 23.81s/it]Running Inference:  84%|████████▍ | 169/200 [1:06:00<12:20, 23.88s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:24<11:58, 23.96s/it]Running Inference:  86%|████████▌ | 171/200 [1:06:48<11:34, 23.93s/it]Running Inference:  86%|████████▌ | 172/200 [1:07:12<11:10, 23.96s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:36<10:46, 23.93s/it]Running Inference:  87%|████████▋ | 174/200 [1:08:00<10:22, 23.93s/it]Running Inference:  88%|████████▊ | 175/200 [1:08:24<09:58, 23.93s/it]Running Inference:  88%|████████▊ | 176/200 [1:08:26<06:58, 17.46s/it]Running Inference:  88%|████████▊ | 177/200 [1:08:50<07:26, 19.42s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:15<07:39, 20.90s/it]Running Inference:  90%|████████▉ | 179/200 [1:09:39<07:37, 21.80s/it]Running Inference:  90%|█████████ | 180/200 [1:10:02<07:28, 22.42s/it]Running Inference:  90%|█████████ | 181/200 [1:10:26<07:14, 22.86s/it]Running Inference:  91%|█████████ | 182/200 [1:10:50<06:56, 23.17s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:14<06:37, 23.39s/it]Running Inference:  92%|█████████▏| 184/200 [1:11:37<06:10, 23.16s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:01<05:51, 23.41s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:25<05:29, 23.55s/it]Running Inference:  94%|█████████▎| 187/200 [1:12:49<05:07, 23.64s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:12<04:44, 23.73s/it]Running Inference:  94%|█████████▍| 189/200 [1:13:36<04:21, 23.75s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:00<03:57, 23.78s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:24<03:35, 23.91s/it]Running Inference:  96%|█████████▌| 192/200 [1:14:26<02:17, 17.22s/it]Running Inference:  96%|█████████▋| 193/200 [1:14:50<02:15, 19.31s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:14<02:03, 20.66s/it]Running Inference:  98%|█████████▊| 195/200 [1:15:38<01:48, 21.68s/it]Running Inference:  98%|█████████▊| 196/200 [1:15:45<01:09, 17.38s/it]Running Inference:  98%|█████████▊| 197/200 [1:16:09<00:58, 19.34s/it]Running Inference:  99%|█████████▉| 198/200 [1:16:33<00:41, 20.69s/it]Running Inference: 100%|█████████▉| 199/200 [1:16:57<00:21, 21.64s/it]Running Inference: 100%|██████████| 200/200 [1:17:21<00:00, 22.38s/it]Running Inference: 100%|██████████| 200/200 [1:17:21<00:00, 23.21s/it]
2025-12-14 03:48:00,600 - INFO - Inference completed.
2025-12-14 03:48:00,626 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 03:48:00,626 - INFO - Calculating metrics for dataset: longbench
2025-12-14 03:48:07,477 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 03:48:07,477 - INFO - Metrics:
12.77
2025-12-14 03:48:07,479 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multi_news, ratio=0.1) on GPU 2

----------------------------------------
Task: multi_news | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 03:48:14,336 - INFO - Set deterministic seeds to 42
2025-12-14 03:48:14,336 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 03:48:14,336 - INFO - Starting evaluation run...
2025-12-14 03:48:14,336 - INFO - Output directory set to: longbenchresult
2025-12-14 03:48:14,336 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 03:48:14,336 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 03:48:14,337 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 03:48:14,337 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.31it/s]
Device set to use cuda:0
2025-12-14 03:48:25,145 - INFO - Model pipeline loaded.
2025-12-14 03:48:25,145 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 03:48:28,008 - INFO - Dataset loaded with 200 entries.
2025-12-14 03:48:28,008 - INFO - Dataset processed with 200 entries.
2025-12-14 03:48:28,017 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:17:24, 23.34s/it]Running Inference:   1%|          | 2/200 [00:45<1:15:19, 22.82s/it]Running Inference:   2%|▏         | 3/200 [01:08<1:14:31, 22.70s/it]Running Inference:   2%|▏         | 4/200 [01:30<1:13:53, 22.62s/it]Running Inference:   2%|▎         | 5/200 [01:53<1:13:26, 22.60s/it]Running Inference:   3%|▎         | 6/200 [02:15<1:12:59, 22.58s/it]Running Inference:   4%|▎         | 7/200 [02:38<1:12:50, 22.65s/it]Running Inference:   4%|▍         | 8/200 [03:01<1:12:29, 22.65s/it]Running Inference:   4%|▍         | 9/200 [03:23<1:12:00, 22.62s/it]Running Inference:   5%|▌         | 10/200 [03:41<1:06:31, 21.01s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:03<1:07:33, 21.45s/it]Running Inference:   6%|▌         | 12/200 [04:26<1:08:17, 21.79s/it]Running Inference:   6%|▋         | 13/200 [04:42<1:02:24, 20.02s/it]Running Inference:   7%|▋         | 14/200 [05:04<1:04:23, 20.77s/it]Running Inference:   8%|▊         | 15/200 [05:27<1:05:56, 21.39s/it]Running Inference:   8%|▊         | 16/200 [05:50<1:06:45, 21.77s/it]Running Inference:   8%|▊         | 17/200 [06:10<1:05:08, 21.36s/it]Running Inference:   9%|▉         | 18/200 [06:33<1:06:28, 21.91s/it]Running Inference:  10%|▉         | 19/200 [06:56<1:06:41, 22.11s/it]Running Inference:  10%|█         | 20/200 [07:09<58:07, 19.37s/it]  Running Inference:  10%|█         | 21/200 [07:31<1:00:31, 20.29s/it]Running Inference:  11%|█         | 22/200 [07:54<1:02:11, 20.96s/it]Running Inference:  12%|█▏        | 23/200 [08:16<1:03:12, 21.43s/it]Running Inference:  12%|█▏        | 24/200 [08:39<1:04:00, 21.82s/it]Running Inference:  12%|█▎        | 25/200 [08:49<53:01, 18.18s/it]  Running Inference:  13%|█▎        | 26/200 [09:11<56:33, 19.50s/it]Running Inference:  14%|█▎        | 27/200 [09:35<59:58, 20.80s/it]Running Inference:  14%|█▍        | 28/200 [09:58<1:01:28, 21.45s/it]Running Inference:  14%|█▍        | 29/200 [10:21<1:02:00, 21.76s/it]Running Inference:  15%|█▌        | 30/200 [10:43<1:02:14, 21.96s/it]Running Inference:  16%|█▌        | 31/200 [11:06<1:02:30, 22.19s/it]Running Inference:  16%|█▌        | 32/200 [11:28<1:02:24, 22.29s/it]Running Inference:  16%|█▋        | 33/200 [11:48<1:00:02, 21.57s/it]Running Inference:  17%|█▋        | 34/200 [12:11<1:00:35, 21.90s/it]Running Inference:  18%|█▊        | 35/200 [12:33<1:00:44, 22.09s/it]Running Inference:  18%|█▊        | 36/200 [12:56<1:00:47, 22.24s/it]Running Inference:  18%|█▊        | 37/200 [13:19<1:00:55, 22.43s/it]Running Inference:  19%|█▉        | 38/200 [13:42<1:01:06, 22.64s/it]Running Inference:  20%|█▉        | 39/200 [14:05<1:00:39, 22.60s/it]Running Inference:  20%|██        | 40/200 [14:23<56:46, 21.29s/it]  Running Inference:  20%|██        | 41/200 [14:45<57:26, 21.68s/it]Running Inference:  21%|██        | 42/200 [15:08<57:41, 21.91s/it]Running Inference:  22%|██▏       | 43/200 [15:30<57:47, 22.08s/it]Running Inference:  22%|██▏       | 44/200 [15:53<57:46, 22.22s/it]Running Inference:  22%|██▎       | 45/200 [16:13<55:49, 21.61s/it]Running Inference:  23%|██▎       | 46/200 [16:28<50:36, 19.72s/it]Running Inference:  24%|██▎       | 47/200 [16:51<52:42, 20.67s/it]Running Inference:  24%|██▍       | 48/200 [16:57<41:21, 16.33s/it]Running Inference:  24%|██▍       | 49/200 [17:17<43:24, 17.25s/it]Running Inference:  25%|██▌       | 50/200 [17:40<47:11, 18.88s/it]Running Inference:  26%|██▌       | 51/200 [18:02<49:42, 20.02s/it]Running Inference:  26%|██▌       | 52/200 [18:25<51:22, 20.82s/it]Running Inference:  26%|██▋       | 53/200 [18:44<49:50, 20.34s/it]Running Inference:  27%|██▋       | 54/200 [19:07<51:03, 20.98s/it]Running Inference:  28%|██▊       | 55/200 [19:29<51:44, 21.41s/it]Running Inference:  28%|██▊       | 56/200 [19:52<52:15, 21.77s/it]Running Inference:  28%|██▊       | 57/200 [20:14<52:29, 22.03s/it]Running Inference:  29%|██▉       | 58/200 [20:37<52:27, 22.16s/it]Running Inference:  30%|██▉       | 59/200 [20:59<52:21, 22.28s/it]Running Inference:  30%|███       | 60/200 [21:22<52:05, 22.32s/it]Running Inference:  30%|███       | 61/200 [21:44<51:54, 22.41s/it]Running Inference:  31%|███       | 62/200 [22:07<51:33, 22.41s/it]Running Inference:  32%|███▏      | 63/200 [22:29<51:15, 22.45s/it]Running Inference:  32%|███▏      | 64/200 [22:52<50:55, 22.47s/it]Running Inference:  32%|███▎      | 65/200 [23:15<50:53, 22.62s/it]Running Inference:  33%|███▎      | 66/200 [23:37<50:28, 22.60s/it]Running Inference:  34%|███▎      | 67/200 [24:00<50:23, 22.73s/it]Running Inference:  34%|███▍      | 68/200 [24:23<49:58, 22.72s/it]Running Inference:  34%|███▍      | 69/200 [24:46<49:29, 22.67s/it]Running Inference:  35%|███▌      | 70/200 [25:08<49:00, 22.62s/it]Running Inference:  36%|███▌      | 71/200 [25:31<48:35, 22.60s/it]Running Inference:  36%|███▌      | 72/200 [25:53<48:19, 22.65s/it]Running Inference:  36%|███▋      | 73/200 [26:16<47:49, 22.59s/it]Running Inference:  37%|███▋      | 74/200 [26:38<47:25, 22.58s/it]Running Inference:  38%|███▊      | 75/200 [27:01<46:59, 22.56s/it]Running Inference:  38%|███▊      | 76/200 [27:23<46:32, 22.52s/it]Running Inference:  38%|███▊      | 77/200 [27:46<46:09, 22.51s/it]Running Inference:  39%|███▉      | 78/200 [28:06<44:14, 21.76s/it]Running Inference:  40%|███▉      | 79/200 [28:28<44:16, 21.96s/it]Running Inference:  40%|████      | 80/200 [28:51<44:18, 22.16s/it]Running Inference:  40%|████      | 81/200 [28:59<35:41, 17.99s/it]Running Inference:  41%|████      | 82/200 [29:18<35:41, 18.15s/it]Running Inference:  42%|████▏     | 83/200 [29:40<37:58, 19.47s/it]Running Inference:  42%|████▏     | 84/200 [30:03<39:31, 20.45s/it]Running Inference:  42%|████▎     | 85/200 [30:26<40:26, 21.10s/it]Running Inference:  43%|████▎     | 86/200 [30:48<41:04, 21.62s/it]Running Inference:  44%|████▎     | 87/200 [31:11<41:09, 21.85s/it]Running Inference:  44%|████▍     | 88/200 [31:34<41:17, 22.12s/it]Running Inference:  44%|████▍     | 89/200 [31:37<30:31, 16.50s/it]Running Inference:  45%|████▌     | 90/200 [32:00<33:40, 18.37s/it]Running Inference:  46%|████▌     | 91/200 [32:22<35:39, 19.63s/it]Running Inference:  46%|████▌     | 92/200 [32:45<36:54, 20.51s/it]Running Inference:  46%|████▋     | 93/200 [33:07<37:36, 21.09s/it]Running Inference:  47%|████▋     | 94/200 [33:30<38:04, 21.55s/it]Running Inference:  48%|████▊     | 95/200 [33:53<38:17, 21.88s/it]Running Inference:  48%|████▊     | 96/200 [34:15<38:16, 22.08s/it]Running Inference:  48%|████▊     | 97/200 [34:38<38:09, 22.23s/it]Running Inference:  49%|████▉     | 98/200 [35:00<37:59, 22.35s/it]Running Inference:  50%|████▉     | 99/200 [35:24<38:25, 22.83s/it]Running Inference:  50%|█████     | 100/200 [35:47<37:55, 22.76s/it]Running Inference:  50%|█████     | 101/200 [36:09<37:27, 22.71s/it]Running Inference:  51%|█████     | 102/200 [36:11<26:59, 16.53s/it]Running Inference:  52%|█████▏    | 103/200 [36:34<29:50, 18.46s/it]Running Inference:  52%|█████▏    | 104/200 [36:57<31:32, 19.71s/it]Running Inference:  52%|█████▎    | 105/200 [37:20<32:35, 20.58s/it]Running Inference:  53%|█████▎    | 106/200 [37:42<33:09, 21.17s/it]Running Inference:  54%|█████▎    | 107/200 [38:05<33:27, 21.59s/it]Running Inference:  54%|█████▍    | 108/200 [38:25<32:40, 21.31s/it]Running Inference:  55%|█████▍    | 109/200 [38:48<32:51, 21.66s/it]Running Inference:  55%|█████▌    | 110/200 [39:10<32:51, 21.90s/it]Running Inference:  56%|█████▌    | 111/200 [39:33<32:47, 22.10s/it]Running Inference:  56%|█████▌    | 112/200 [39:55<32:34, 22.21s/it]Running Inference:  56%|█████▋    | 113/200 [40:18<32:26, 22.37s/it]Running Inference:  57%|█████▋    | 114/200 [40:41<32:05, 22.39s/it]Running Inference:  57%|█████▊    | 115/200 [41:03<31:44, 22.41s/it]Running Inference:  58%|█████▊    | 116/200 [41:22<30:00, 21.43s/it]Running Inference:  58%|█████▊    | 117/200 [41:45<30:04, 21.75s/it]Running Inference:  59%|█████▉    | 118/200 [41:51<23:13, 16.99s/it]Running Inference:  60%|█████▉    | 119/200 [42:13<25:03, 18.56s/it]Running Inference:  60%|██████    | 120/200 [42:35<26:19, 19.75s/it]Running Inference:  60%|██████    | 121/200 [42:53<25:04, 19.05s/it]Running Inference:  61%|██████    | 122/200 [43:15<26:05, 20.06s/it]Running Inference:  62%|██████▏   | 123/200 [43:38<26:44, 20.84s/it]Running Inference:  62%|██████▏   | 124/200 [44:00<26:59, 21.31s/it]Running Inference:  62%|██████▎   | 125/200 [44:23<27:05, 21.67s/it]Running Inference:  63%|██████▎   | 126/200 [44:45<27:00, 21.90s/it]Running Inference:  64%|██████▎   | 127/200 [45:08<26:52, 22.09s/it]Running Inference:  64%|██████▍   | 128/200 [45:28<26:01, 21.68s/it]Running Inference:  64%|██████▍   | 129/200 [45:51<25:55, 21.90s/it]Running Inference:  65%|██████▌   | 130/200 [46:13<25:44, 22.06s/it]Running Inference:  66%|██████▌   | 131/200 [46:37<26:01, 22.64s/it]Running Inference:  66%|██████▌   | 132/200 [47:00<25:39, 22.64s/it]Running Inference:  66%|██████▋   | 133/200 [47:22<25:14, 22.60s/it]Running Inference:  67%|██████▋   | 134/200 [47:45<24:51, 22.60s/it]Running Inference:  68%|██████▊   | 135/200 [48:08<24:39, 22.75s/it]Running Inference:  68%|██████▊   | 136/200 [48:31<24:16, 22.76s/it]Running Inference:  68%|██████▊   | 137/200 [48:54<23:51, 22.73s/it]Running Inference:  69%|██████▉   | 138/200 [49:16<23:24, 22.66s/it]Running Inference:  70%|██████▉   | 139/200 [49:39<22:59, 22.62s/it]Running Inference:  70%|███████   | 140/200 [50:01<22:36, 22.61s/it]Running Inference:  70%|███████   | 141/200 [50:24<22:08, 22.52s/it]Running Inference:  71%|███████   | 142/200 [50:46<21:46, 22.52s/it]Running Inference:  72%|███████▏  | 143/200 [51:09<21:24, 22.53s/it]Running Inference:  72%|███████▏  | 144/200 [51:31<21:00, 22.51s/it]Running Inference:  72%|███████▎  | 145/200 [51:54<20:38, 22.53s/it]Running Inference:  73%|███████▎  | 146/200 [52:16<20:16, 22.53s/it]Running Inference:  74%|███████▎  | 147/200 [52:39<19:54, 22.53s/it]Running Inference:  74%|███████▍  | 148/200 [53:01<19:30, 22.50s/it]Running Inference:  74%|███████▍  | 149/200 [53:24<19:07, 22.49s/it]Running Inference:  75%|███████▌  | 150/200 [53:45<18:30, 22.20s/it]Running Inference:  76%|███████▌  | 151/200 [54:08<18:12, 22.29s/it]Running Inference:  76%|███████▌  | 152/200 [54:30<17:54, 22.39s/it]Running Inference:  76%|███████▋  | 153/200 [54:53<17:37, 22.51s/it]Running Inference:  77%|███████▋  | 154/200 [55:16<17:18, 22.58s/it]Running Inference:  78%|███████▊  | 155/200 [55:35<16:15, 21.67s/it]Running Inference:  78%|███████▊  | 156/200 [55:58<16:04, 21.93s/it]Running Inference:  78%|███████▊  | 157/200 [56:21<15:53, 22.18s/it]Running Inference:  79%|███████▉  | 158/200 [56:43<15:39, 22.36s/it]Running Inference:  80%|███████▉  | 159/200 [56:47<11:23, 16.66s/it]Running Inference:  80%|████████  | 160/200 [56:57<09:46, 14.66s/it]Running Inference:  80%|████████  | 161/200 [57:19<11:03, 17.02s/it]Running Inference:  81%|████████  | 162/200 [57:40<11:27, 18.08s/it]Running Inference:  82%|████████▏ | 163/200 [58:02<11:58, 19.42s/it]Running Inference:  82%|████████▏ | 164/200 [58:25<12:14, 20.41s/it]Running Inference:  82%|████████▎ | 165/200 [58:48<12:16, 21.03s/it]Running Inference:  83%|████████▎ | 166/200 [59:08<11:46, 20.79s/it]Running Inference:  84%|████████▎ | 167/200 [59:30<11:43, 21.32s/it]Running Inference:  84%|████████▍ | 168/200 [59:53<11:34, 21.69s/it]Running Inference:  84%|████████▍ | 169/200 [1:00:16<11:21, 21.98s/it]Running Inference:  85%|████████▌ | 170/200 [1:00:38<11:06, 22.22s/it]Running Inference:  86%|████████▌ | 171/200 [1:01:01<10:47, 22.34s/it]Running Inference:  86%|████████▌ | 172/200 [1:01:24<10:28, 22.44s/it]Running Inference:  86%|████████▋ | 173/200 [1:01:46<10:06, 22.45s/it]Running Inference:  87%|████████▋ | 174/200 [1:02:09<09:44, 22.50s/it]Running Inference:  88%|████████▊ | 175/200 [1:02:18<07:41, 18.45s/it]Running Inference:  88%|████████▊ | 176/200 [1:02:40<07:52, 19.70s/it]Running Inference:  88%|████████▊ | 177/200 [1:02:42<05:30, 14.38s/it]Running Inference:  89%|████████▉ | 178/200 [1:03:05<06:13, 16.96s/it]Running Inference:  90%|████████▉ | 179/200 [1:03:28<06:31, 18.63s/it]Running Inference:  90%|█████████ | 180/200 [1:03:50<06:36, 19.81s/it]Running Inference:  90%|█████████ | 181/200 [1:04:13<06:31, 20.62s/it]Running Inference:  91%|█████████ | 182/200 [1:04:35<06:21, 21.18s/it]Running Inference:  92%|█████████▏| 183/200 [1:04:58<06:06, 21.57s/it]Running Inference:  92%|█████████▏| 184/200 [1:05:20<05:46, 21.67s/it]Running Inference:  92%|█████████▎| 185/200 [1:05:42<05:29, 21.96s/it]Running Inference:  93%|█████████▎| 186/200 [1:06:05<05:09, 22.13s/it]Running Inference:  94%|█████████▎| 187/200 [1:06:27<04:49, 22.24s/it]Running Inference:  94%|█████████▍| 188/200 [1:06:50<04:28, 22.35s/it]Running Inference:  94%|█████████▍| 189/200 [1:07:13<04:06, 22.41s/it]Running Inference:  95%|█████████▌| 190/200 [1:07:35<03:44, 22.45s/it]Running Inference:  96%|█████████▌| 191/200 [1:07:58<03:23, 22.57s/it]Running Inference:  96%|█████████▌| 192/200 [1:08:00<02:10, 16.26s/it]Running Inference:  96%|█████████▋| 193/200 [1:08:22<02:07, 18.20s/it]Running Inference:  97%|█████████▋| 194/200 [1:08:45<01:57, 19.50s/it]Running Inference:  98%|█████████▊| 195/200 [1:09:07<01:42, 20.46s/it]Running Inference:  98%|█████████▊| 196/200 [1:09:30<01:24, 21.19s/it]Running Inference:  98%|█████████▊| 197/200 [1:09:53<01:04, 21.59s/it]Running Inference:  99%|█████████▉| 198/200 [1:10:15<00:43, 21.87s/it]Running Inference: 100%|█████████▉| 199/200 [1:10:38<00:22, 22.07s/it]Running Inference: 100%|██████████| 200/200 [1:11:01<00:00, 22.28s/it]Running Inference: 100%|██████████| 200/200 [1:11:01<00:00, 21.31s/it]
2025-12-14 04:59:29,272 - INFO - Inference completed.
2025-12-14 04:59:29,293 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 04:59:29,293 - INFO - Calculating metrics for dataset: longbench
2025-12-14 04:59:36,095 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 04:59:36,095 - INFO - Metrics:
12.4
2025-12-14 04:59:36,096 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multi_news, ratio=0.2) on GPU 2

----------------------------------------
Task: multi_news | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 04:59:42,765 - INFO - Set deterministic seeds to 42
2025-12-14 04:59:42,765 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 04:59:42,765 - INFO - Starting evaluation run...
2025-12-14 04:59:42,765 - INFO - Output directory set to: longbenchresult
2025-12-14 04:59:42,765 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 04:59:42,765 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 04:59:42,765 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 04:59:42,765 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.34it/s]
Device set to use cuda:0
2025-12-14 04:59:53,668 - INFO - Model pipeline loaded.
2025-12-14 04:59:53,669 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 04:59:56,865 - INFO - Dataset loaded with 200 entries.
2025-12-14 04:59:56,865 - INFO - Dataset processed with 200 entries.
2025-12-14 04:59:56,875 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:18:25, 23.65s/it]Running Inference:   1%|          | 2/200 [00:46<1:16:14, 23.10s/it]Running Inference:   2%|▏         | 3/200 [01:09<1:15:28, 22.99s/it]Running Inference:   2%|▏         | 4/200 [01:31<1:14:46, 22.89s/it]Running Inference:   2%|▎         | 5/200 [01:54<1:14:12, 22.83s/it]Running Inference:   3%|▎         | 6/200 [02:17<1:13:47, 22.82s/it]Running Inference:   4%|▎         | 7/200 [02:40<1:13:36, 22.88s/it]Running Inference:   4%|▍         | 8/200 [03:03<1:13:15, 22.89s/it]Running Inference:   4%|▍         | 9/200 [03:26<1:12:47, 22.87s/it]Running Inference:   5%|▌         | 10/200 [03:45<1:08:50, 21.74s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:08<1:09:19, 22.01s/it]Running Inference:   6%|▌         | 12/200 [04:30<1:09:45, 22.27s/it]Running Inference:   6%|▋         | 13/200 [04:53<1:09:52, 22.42s/it]Running Inference:   7%|▋         | 14/200 [05:16<1:09:47, 22.52s/it]Running Inference:   8%|▊         | 15/200 [05:39<1:09:53, 22.67s/it]Running Inference:   8%|▊         | 16/200 [06:02<1:09:44, 22.74s/it]Running Inference:   8%|▊         | 17/200 [06:25<1:09:26, 22.77s/it]Running Inference:   9%|▉         | 18/200 [06:48<1:09:39, 22.97s/it]Running Inference:  10%|▉         | 19/200 [07:11<1:09:06, 22.91s/it]Running Inference:  10%|█         | 20/200 [07:34<1:08:33, 22.85s/it]Running Inference:  10%|█         | 21/200 [07:56<1:08:00, 22.80s/it]Running Inference:  11%|█         | 22/200 [08:19<1:07:35, 22.78s/it]Running Inference:  12%|█▏        | 23/200 [08:35<1:01:12, 20.75s/it]Running Inference:  12%|█▏        | 24/200 [08:58<1:02:52, 21.43s/it]Running Inference:  12%|█▎        | 25/200 [09:12<55:43, 19.11s/it]  Running Inference:  13%|█▎        | 26/200 [09:34<58:33, 20.19s/it]Running Inference:  14%|█▎        | 27/200 [09:58<1:01:32, 21.34s/it]Running Inference:  14%|█▍        | 28/200 [10:22<1:02:47, 21.91s/it]Running Inference:  14%|█▍        | 29/200 [10:44<1:03:09, 22.16s/it]Running Inference:  15%|█▌        | 30/200 [11:07<1:03:18, 22.34s/it]Running Inference:  16%|█▌        | 31/200 [11:30<1:03:29, 22.54s/it]Running Inference:  16%|█▌        | 32/200 [11:53<1:03:16, 22.60s/it]Running Inference:  16%|█▋        | 33/200 [12:16<1:02:57, 22.62s/it]Running Inference:  17%|█▋        | 34/200 [12:39<1:02:50, 22.71s/it]Running Inference:  18%|█▊        | 35/200 [13:01<1:02:31, 22.74s/it]Running Inference:  18%|█▊        | 36/200 [13:24<1:02:13, 22.77s/it]Running Inference:  18%|█▊        | 37/200 [13:47<1:02:07, 22.87s/it]Running Inference:  19%|█▉        | 38/200 [14:11<1:02:04, 22.99s/it]Running Inference:  20%|█▉        | 39/200 [14:33<1:01:29, 22.92s/it]Running Inference:  20%|██        | 40/200 [14:56<1:00:58, 22.86s/it]Running Inference:  20%|██        | 41/200 [15:19<1:00:36, 22.87s/it]Running Inference:  21%|██        | 42/200 [15:20<42:53, 16.29s/it]  Running Inference:  22%|██▏       | 43/200 [15:36<42:37, 16.29s/it]Running Inference:  22%|██▏       | 44/200 [15:59<47:26, 18.25s/it]Running Inference:  22%|██▎       | 45/200 [16:22<50:40, 19.62s/it]Running Inference:  23%|██▎       | 46/200 [16:39<48:15, 18.80s/it]Running Inference:  24%|██▎       | 47/200 [17:02<51:17, 20.11s/it]Running Inference:  24%|██▍       | 48/200 [17:25<53:09, 20.98s/it]Running Inference:  24%|██▍       | 49/200 [17:47<53:45, 21.36s/it]Running Inference:  25%|██▌       | 50/200 [18:10<54:33, 21.82s/it]Running Inference:  26%|██▌       | 51/200 [18:33<54:55, 22.12s/it]Running Inference:  26%|██▌       | 52/200 [18:56<55:08, 22.36s/it]Running Inference:  26%|██▋       | 53/200 [19:18<55:02, 22.47s/it]Running Inference:  27%|██▋       | 54/200 [19:41<54:54, 22.57s/it]Running Inference:  28%|██▊       | 55/200 [20:04<54:34, 22.58s/it]Running Inference:  28%|██▊       | 56/200 [20:27<54:21, 22.65s/it]Running Inference:  28%|██▊       | 57/200 [20:50<54:09, 22.72s/it]Running Inference:  29%|██▉       | 58/200 [21:12<53:47, 22.73s/it]Running Inference:  30%|██▉       | 59/200 [21:35<53:26, 22.74s/it]Running Inference:  30%|███       | 60/200 [21:58<53:00, 22.72s/it]Running Inference:  30%|███       | 61/200 [22:21<52:40, 22.74s/it]Running Inference:  31%|███       | 62/200 [22:43<52:21, 22.76s/it]Running Inference:  32%|███▏      | 63/200 [23:06<51:55, 22.74s/it]Running Inference:  32%|███▏      | 64/200 [23:29<51:33, 22.75s/it]Running Inference:  32%|███▎      | 65/200 [23:52<51:28, 22.88s/it]Running Inference:  33%|███▎      | 66/200 [24:15<51:05, 22.87s/it]Running Inference:  34%|███▎      | 67/200 [24:38<51:04, 23.04s/it]Running Inference:  34%|███▍      | 68/200 [25:01<50:38, 23.02s/it]Running Inference:  34%|███▍      | 69/200 [25:24<50:08, 22.97s/it]Running Inference:  35%|███▌      | 70/200 [25:47<49:37, 22.91s/it]Running Inference:  36%|███▌      | 71/200 [26:10<49:10, 22.87s/it]Running Inference:  36%|███▌      | 72/200 [26:33<48:54, 22.93s/it]Running Inference:  36%|███▋      | 73/200 [26:55<48:21, 22.85s/it]Running Inference:  37%|███▋      | 74/200 [27:18<47:54, 22.81s/it]Running Inference:  38%|███▊      | 75/200 [27:41<47:28, 22.79s/it]Running Inference:  38%|███▊      | 76/200 [28:03<47:00, 22.74s/it]Running Inference:  38%|███▊      | 77/200 [28:26<46:36, 22.73s/it]Running Inference:  39%|███▉      | 78/200 [28:49<46:14, 22.75s/it]Running Inference:  40%|███▉      | 79/200 [29:12<45:48, 22.71s/it]Running Inference:  40%|████      | 80/200 [29:34<45:28, 22.74s/it]Running Inference:  40%|████      | 81/200 [29:48<39:44, 20.04s/it]Running Inference:  41%|████      | 82/200 [30:11<40:58, 20.84s/it]Running Inference:  42%|████▏     | 83/200 [30:34<41:47, 21.44s/it]Running Inference:  42%|████▏     | 84/200 [30:57<42:21, 21.91s/it]Running Inference:  42%|████▎     | 85/200 [31:19<42:29, 22.17s/it]Running Inference:  43%|████▎     | 86/200 [31:42<42:36, 22.43s/it]Running Inference:  44%|████▎     | 87/200 [32:05<42:23, 22.51s/it]Running Inference:  44%|████▍     | 88/200 [32:28<42:17, 22.65s/it]Running Inference:  44%|████▍     | 89/200 [32:31<30:52, 16.69s/it]Running Inference:  45%|████▌     | 90/200 [32:54<34:02, 18.57s/it]Running Inference:  46%|████▌     | 91/200 [33:17<36:01, 19.83s/it]Running Inference:  46%|████▌     | 92/200 [33:40<37:19, 20.73s/it]Running Inference:  46%|████▋     | 93/200 [34:02<38:01, 21.32s/it]Running Inference:  47%|████▋     | 94/200 [34:25<38:29, 21.79s/it]Running Inference:  48%|████▊     | 95/200 [34:48<38:41, 22.11s/it]Running Inference:  48%|████▊     | 96/200 [35:11<38:41, 22.32s/it]Running Inference:  48%|████▊     | 97/200 [35:34<38:37, 22.50s/it]Running Inference:  49%|████▉     | 98/200 [35:56<38:02, 22.38s/it]Running Inference:  50%|████▉     | 99/200 [36:20<38:34, 22.92s/it]Running Inference:  50%|█████     | 100/200 [36:43<38:11, 22.92s/it]Running Inference:  50%|█████     | 101/200 [37:06<37:47, 22.90s/it]Running Inference:  51%|█████     | 102/200 [37:28<37:18, 22.85s/it]Running Inference:  52%|█████▏    | 103/200 [37:52<37:07, 22.97s/it]Running Inference:  52%|█████▏    | 104/200 [38:15<36:52, 23.05s/it]Running Inference:  52%|█████▎    | 105/200 [38:38<36:23, 22.99s/it]Running Inference:  53%|█████▎    | 106/200 [39:01<35:53, 22.91s/it]Running Inference:  54%|█████▎    | 107/200 [39:23<35:27, 22.87s/it]Running Inference:  54%|█████▍    | 108/200 [39:46<35:04, 22.88s/it]Running Inference:  55%|█████▍    | 109/200 [40:09<34:38, 22.85s/it]Running Inference:  55%|█████▌    | 110/200 [40:32<34:11, 22.79s/it]Running Inference:  56%|█████▌    | 111/200 [40:54<33:48, 22.79s/it]Running Inference:  56%|█████▌    | 112/200 [40:55<23:46, 16.21s/it]Running Inference:  56%|█████▋    | 113/200 [41:18<26:28, 18.26s/it]Running Inference:  57%|█████▋    | 114/200 [41:41<28:05, 19.60s/it]Running Inference:  57%|█████▊    | 115/200 [42:04<29:07, 20.56s/it]Running Inference:  58%|█████▊    | 116/200 [42:24<28:34, 20.41s/it]Running Inference:  58%|█████▊    | 117/200 [42:47<29:09, 21.08s/it]Running Inference:  59%|█████▉    | 118/200 [43:09<29:31, 21.60s/it]Running Inference:  60%|█████▉    | 119/200 [43:32<29:44, 22.03s/it]Running Inference:  60%|██████    | 120/200 [43:55<29:39, 22.24s/it]Running Inference:  60%|██████    | 121/200 [44:18<29:30, 22.41s/it]Running Inference:  61%|██████    | 122/200 [44:41<29:15, 22.50s/it]Running Inference:  62%|██████▏   | 123/200 [45:03<29:00, 22.61s/it]Running Inference:  62%|██████▏   | 124/200 [45:23<27:33, 21.76s/it]Running Inference:  62%|██████▎   | 125/200 [45:28<20:42, 16.56s/it]Running Inference:  63%|██████▎   | 126/200 [45:50<22:39, 18.38s/it]Running Inference:  64%|██████▎   | 127/200 [46:13<23:57, 19.69s/it]Running Inference:  64%|██████▍   | 128/200 [46:37<25:03, 20.88s/it]Running Inference:  64%|██████▍   | 129/200 [46:59<25:21, 21.43s/it]Running Inference:  65%|██████▌   | 130/200 [47:22<25:27, 21.82s/it]Running Inference:  66%|██████▌   | 131/200 [47:46<25:55, 22.54s/it]Running Inference:  66%|██████▌   | 132/200 [48:09<25:38, 22.62s/it]Running Inference:  66%|██████▋   | 133/200 [48:32<25:19, 22.67s/it]Running Inference:  67%|██████▋   | 134/200 [48:55<24:58, 22.71s/it]Running Inference:  68%|██████▊   | 135/200 [49:18<24:48, 22.90s/it]Running Inference:  68%|██████▊   | 136/200 [49:41<24:26, 22.92s/it]Running Inference:  68%|██████▊   | 137/200 [50:04<24:02, 22.89s/it]Running Inference:  69%|██████▉   | 138/200 [50:27<23:38, 22.88s/it]Running Inference:  70%|██████▉   | 139/200 [50:50<23:13, 22.84s/it]Running Inference:  70%|███████   | 140/200 [51:12<22:50, 22.85s/it]Running Inference:  70%|███████   | 141/200 [51:35<22:29, 22.88s/it]Running Inference:  71%|███████   | 142/200 [51:58<22:05, 22.86s/it]Running Inference:  72%|███████▏  | 143/200 [52:21<21:42, 22.85s/it]Running Inference:  72%|███████▏  | 144/200 [52:44<21:15, 22.77s/it]Running Inference:  72%|███████▎  | 145/200 [53:06<20:52, 22.77s/it]Running Inference:  73%|███████▎  | 146/200 [53:29<20:30, 22.78s/it]Running Inference:  74%|███████▎  | 147/200 [53:52<20:07, 22.78s/it]Running Inference:  74%|███████▍  | 148/200 [54:15<19:43, 22.76s/it]Running Inference:  74%|███████▍  | 149/200 [54:31<17:48, 20.95s/it]Running Inference:  75%|███████▌  | 150/200 [54:55<18:01, 21.62s/it]Running Inference:  76%|███████▌  | 151/200 [55:17<17:57, 21.98s/it]Running Inference:  76%|███████▌  | 152/200 [55:40<17:48, 22.25s/it]Running Inference:  76%|███████▋  | 153/200 [56:03<17:36, 22.48s/it]Running Inference:  77%|███████▋  | 154/200 [56:26<17:20, 22.62s/it]Running Inference:  78%|███████▊  | 155/200 [56:49<17:04, 22.76s/it]Running Inference:  78%|███████▊  | 156/200 [57:12<16:41, 22.77s/it]Running Inference:  78%|███████▊  | 157/200 [57:35<16:21, 22.82s/it]Running Inference:  79%|███████▉  | 158/200 [57:58<15:58, 22.81s/it]Running Inference:  80%|███████▉  | 159/200 [58:20<15:31, 22.71s/it]Running Inference:  80%|████████  | 160/200 [58:43<15:09, 22.75s/it]Running Inference:  80%|████████  | 161/200 [59:06<14:45, 22.69s/it]Running Inference:  81%|████████  | 162/200 [59:27<14:06, 22.29s/it]Running Inference:  82%|████████▏ | 163/200 [59:50<13:47, 22.35s/it]Running Inference:  82%|████████▏ | 164/200 [1:00:12<13:29, 22.48s/it]Running Inference:  82%|████████▎ | 165/200 [1:00:35<13:06, 22.47s/it]Running Inference:  83%|████████▎ | 166/200 [1:00:57<12:44, 22.49s/it]Running Inference:  84%|████████▎ | 167/200 [1:01:20<12:22, 22.51s/it]Running Inference:  84%|████████▍ | 168/200 [1:01:42<12:00, 22.53s/it]Running Inference:  84%|████████▍ | 169/200 [1:02:05<11:39, 22.57s/it]Running Inference:  85%|████████▌ | 170/200 [1:02:28<11:19, 22.64s/it]Running Inference:  86%|████████▌ | 171/200 [1:02:51<10:56, 22.64s/it]Running Inference:  86%|████████▌ | 172/200 [1:03:13<10:34, 22.65s/it]Running Inference:  86%|████████▋ | 173/200 [1:03:36<10:09, 22.59s/it]Running Inference:  87%|████████▋ | 174/200 [1:03:58<09:47, 22.59s/it]Running Inference:  88%|████████▊ | 175/200 [1:04:06<07:35, 18.22s/it]Running Inference:  88%|████████▊ | 176/200 [1:04:29<07:48, 19.52s/it]Running Inference:  88%|████████▊ | 177/200 [1:04:51<07:50, 20.46s/it]Running Inference:  89%|████████▉ | 178/200 [1:05:14<07:46, 21.21s/it]Running Inference:  90%|████████▉ | 179/200 [1:05:37<07:33, 21.61s/it]Running Inference:  90%|█████████ | 180/200 [1:06:00<07:17, 21.88s/it]Running Inference:  90%|█████████ | 181/200 [1:06:22<06:59, 22.08s/it]Running Inference:  91%|█████████ | 182/200 [1:06:45<06:39, 22.21s/it]Running Inference:  92%|█████████▏| 183/200 [1:07:07<06:18, 22.28s/it]Running Inference:  92%|█████████▏| 184/200 [1:07:31<06:03, 22.69s/it]Running Inference:  92%|█████████▎| 185/200 [1:07:53<05:39, 22.67s/it]Running Inference:  93%|█████████▎| 186/200 [1:08:16<05:16, 22.63s/it]Running Inference:  94%|█████████▎| 187/200 [1:08:38<04:53, 22.59s/it]Running Inference:  94%|█████████▍| 188/200 [1:09:01<04:31, 22.59s/it]Running Inference:  94%|█████████▍| 189/200 [1:09:24<04:08, 22.61s/it]Running Inference:  95%|█████████▌| 190/200 [1:09:46<03:45, 22.60s/it]Running Inference:  96%|█████████▌| 191/200 [1:10:09<03:23, 22.66s/it]Running Inference:  96%|█████████▌| 192/200 [1:10:10<02:10, 16.33s/it]Running Inference:  96%|█████████▋| 193/200 [1:10:33<02:07, 18.25s/it]Running Inference:  97%|█████████▋| 194/200 [1:10:54<01:53, 18.86s/it]Running Inference:  98%|█████████▊| 195/200 [1:11:16<01:40, 20.01s/it]Running Inference:  98%|█████████▊| 196/200 [1:11:39<01:23, 20.86s/it]Running Inference:  98%|█████████▊| 197/200 [1:12:02<01:04, 21.37s/it]Running Inference:  99%|█████████▉| 198/200 [1:12:24<00:43, 21.70s/it]Running Inference: 100%|█████████▉| 199/200 [1:12:47<00:21, 21.94s/it]Running Inference: 100%|██████████| 200/200 [1:13:09<00:00, 22.19s/it]Running Inference: 100%|██████████| 200/200 [1:13:09<00:00, 21.95s/it]
2025-12-14 06:13:06,758 - INFO - Inference completed.
2025-12-14 06:13:06,779 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 06:13:06,779 - INFO - Calculating metrics for dataset: longbench
2025-12-14 06:13:13,648 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 06:13:13,648 - INFO - Metrics:
12.08
2025-12-14 06:13:13,649 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multi_news, ratio=0.3) on GPU 2

----------------------------------------
Task: multi_news | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 06:13:20,296 - INFO - Set deterministic seeds to 42
2025-12-14 06:13:20,296 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multi_news",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 06:13:20,296 - INFO - Starting evaluation run...
2025-12-14 06:13:20,296 - INFO - Output directory set to: longbenchresult
2025-12-14 06:13:20,296 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 06:13:20,296 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 06:13:20,296 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 06:13:20,296 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.78it/s]
Device set to use cuda:0
2025-12-14 06:13:30,718 - INFO - Model pipeline loaded.
2025-12-14 06:13:30,718 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multi_news)
2025-12-14 06:13:33,524 - INFO - Dataset loaded with 200 entries.
2025-12-14 06:13:33,524 - INFO - Dataset processed with 200 entries.
2025-12-14 06:13:33,534 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:18:07, 23.55s/it]Running Inference:   1%|          | 2/200 [00:46<1:16:05, 23.06s/it]Running Inference:   2%|▏         | 3/200 [01:09<1:15:16, 22.93s/it]Running Inference:   2%|▏         | 4/200 [01:31<1:14:41, 22.87s/it]Running Inference:   2%|▎         | 5/200 [01:54<1:14:12, 22.84s/it]Running Inference:   3%|▎         | 6/200 [02:17<1:13:44, 22.81s/it]Running Inference:   4%|▎         | 7/200 [02:40<1:13:36, 22.88s/it]Running Inference:   4%|▍         | 8/200 [03:03<1:13:13, 22.88s/it]Running Inference:   4%|▍         | 9/200 [03:26<1:12:45, 22.85s/it]Running Inference:   5%|▌         | 10/200 [03:48<1:12:14, 22.81s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:11<1:11:43, 22.77s/it]Running Inference:   6%|▌         | 12/200 [04:32<1:09:20, 22.13s/it]Running Inference:   6%|▋         | 13/200 [04:54<1:09:36, 22.33s/it]Running Inference:   7%|▋         | 14/200 [05:11<1:03:56, 20.63s/it]Running Inference:   8%|▊         | 15/200 [05:34<1:05:50, 21.35s/it]Running Inference:   8%|▊         | 16/200 [05:57<1:06:52, 21.81s/it]Running Inference:   8%|▊         | 17/200 [06:20<1:07:24, 22.10s/it]Running Inference:   9%|▉         | 18/200 [06:43<1:08:14, 22.50s/it]Running Inference:  10%|▉         | 19/200 [07:06<1:08:08, 22.59s/it]Running Inference:  10%|█         | 20/200 [07:29<1:07:50, 22.62s/it]Running Inference:  10%|█         | 21/200 [07:52<1:07:40, 22.68s/it]Running Inference:  11%|█         | 22/200 [08:14<1:07:24, 22.72s/it]Running Inference:  12%|█▏        | 23/200 [08:37<1:07:02, 22.73s/it]Running Inference:  12%|█▏        | 24/200 [09:00<1:06:53, 22.81s/it]Running Inference:  12%|█▎        | 25/200 [09:23<1:06:29, 22.80s/it]Running Inference:  13%|█▎        | 26/200 [09:40<1:00:53, 21.00s/it]Running Inference:  14%|█▎        | 27/200 [09:52<52:51, 18.33s/it]  Running Inference:  14%|█▍        | 28/200 [10:15<56:40, 19.77s/it]Running Inference:  14%|█▍        | 29/200 [10:38<58:52, 20.66s/it]Running Inference:  15%|█▌        | 30/200 [11:00<1:00:17, 21.28s/it]Running Inference:  16%|█▌        | 31/200 [11:23<1:01:21, 21.78s/it]Running Inference:  16%|█▌        | 32/200 [11:46<1:01:50, 22.09s/it]Running Inference:  16%|█▋        | 33/200 [12:09<1:02:00, 22.28s/it]Running Inference:  17%|█▋        | 34/200 [12:32<1:02:08, 22.46s/it]Running Inference:  18%|█▊        | 35/200 [12:55<1:02:03, 22.56s/it]Running Inference:  18%|█▊        | 36/200 [13:17<1:01:54, 22.65s/it]Running Inference:  18%|█▊        | 37/200 [13:41<1:01:56, 22.80s/it]Running Inference:  19%|█▉        | 38/200 [14:04<1:02:01, 22.97s/it]Running Inference:  20%|█▉        | 39/200 [14:27<1:01:29, 22.92s/it]Running Inference:  20%|██        | 40/200 [14:49<1:00:57, 22.86s/it]Running Inference:  20%|██        | 41/200 [15:12<1:00:33, 22.85s/it]Running Inference:  21%|██        | 42/200 [15:35<1:00:11, 22.86s/it]Running Inference:  22%|██▏       | 43/200 [15:58<59:42, 22.82s/it]  Running Inference:  22%|██▏       | 44/200 [16:21<59:20, 22.82s/it]Running Inference:  22%|██▎       | 45/200 [16:44<58:59, 22.83s/it]Running Inference:  23%|██▎       | 46/200 [16:56<50:36, 19.72s/it]Running Inference:  24%|██▎       | 47/200 [17:19<52:54, 20.75s/it]Running Inference:  24%|██▍       | 48/200 [17:42<54:17, 21.43s/it]Running Inference:  24%|██▍       | 49/200 [18:05<54:55, 21.83s/it]Running Inference:  25%|██▌       | 50/200 [18:28<55:23, 22.16s/it]Running Inference:  26%|██▌       | 51/200 [18:51<55:33, 22.37s/it]Running Inference:  26%|██▌       | 52/200 [19:14<55:32, 22.52s/it]Running Inference:  26%|██▋       | 53/200 [19:36<55:20, 22.59s/it]Running Inference:  27%|██▋       | 54/200 [19:59<55:06, 22.65s/it]Running Inference:  28%|██▊       | 55/200 [20:22<54:55, 22.73s/it]Running Inference:  28%|██▊       | 56/200 [20:45<54:39, 22.77s/it]Running Inference:  28%|██▊       | 57/200 [21:08<54:23, 22.82s/it]Running Inference:  29%|██▉       | 58/200 [21:31<53:56, 22.79s/it]Running Inference:  30%|██▉       | 59/200 [21:53<53:36, 22.81s/it]Running Inference:  30%|███       | 60/200 [22:16<53:12, 22.80s/it]Running Inference:  30%|███       | 61/200 [22:37<51:12, 22.10s/it]Running Inference:  31%|███       | 62/200 [22:59<51:20, 22.32s/it]Running Inference:  32%|███▏      | 63/200 [23:17<47:41, 20.89s/it]Running Inference:  32%|███▏      | 64/200 [23:40<48:36, 21.45s/it]Running Inference:  32%|███▎      | 65/200 [24:03<49:27, 21.98s/it]Running Inference:  33%|███▎      | 66/200 [24:26<49:41, 22.25s/it]Running Inference:  34%|███▎      | 67/200 [24:49<50:01, 22.56s/it]Running Inference:  34%|███▍      | 68/200 [24:53<37:31, 17.06s/it]Running Inference:  34%|███▍      | 69/200 [25:16<41:01, 18.79s/it]Running Inference:  35%|███▌      | 70/200 [25:39<43:19, 20.00s/it]Running Inference:  36%|███▌      | 71/200 [26:02<44:48, 20.84s/it]Running Inference:  36%|███▌      | 72/200 [26:25<45:52, 21.50s/it]Running Inference:  36%|███▋      | 73/200 [26:48<46:18, 21.88s/it]Running Inference:  37%|███▋      | 74/200 [27:10<46:30, 22.15s/it]Running Inference:  38%|███▊      | 75/200 [27:33<46:32, 22.34s/it]Running Inference:  38%|███▊      | 76/200 [27:56<46:30, 22.51s/it]Running Inference:  38%|███▊      | 77/200 [28:19<46:25, 22.65s/it]Running Inference:  39%|███▉      | 78/200 [28:42<46:10, 22.71s/it]Running Inference:  40%|███▉      | 79/200 [29:05<45:54, 22.76s/it]Running Inference:  40%|████      | 80/200 [29:28<45:35, 22.80s/it]Running Inference:  40%|████      | 81/200 [29:40<39:05, 19.71s/it]Running Inference:  41%|████      | 82/200 [29:59<38:23, 19.52s/it]Running Inference:  42%|████▏     | 83/200 [30:22<40:00, 20.51s/it]Running Inference:  42%|████▏     | 84/200 [30:45<41:05, 21.26s/it]Running Inference:  42%|████▎     | 85/200 [31:08<41:39, 21.73s/it]Running Inference:  43%|████▎     | 86/200 [31:31<42:04, 22.14s/it]Running Inference:  44%|████▎     | 87/200 [31:54<42:08, 22.37s/it]Running Inference:  44%|████▍     | 88/200 [32:17<42:07, 22.56s/it]Running Inference:  44%|████▍     | 89/200 [32:40<42:01, 22.72s/it]Running Inference:  45%|████▌     | 90/200 [33:03<41:48, 22.81s/it]Running Inference:  46%|████▌     | 91/200 [33:26<41:27, 22.82s/it]Running Inference:  46%|████▌     | 92/200 [33:49<41:04, 22.82s/it]Running Inference:  46%|████▋     | 93/200 [34:12<40:43, 22.83s/it]Running Inference:  47%|████▋     | 94/200 [34:34<40:21, 22.85s/it]Running Inference:  48%|████▊     | 95/200 [34:57<40:00, 22.86s/it]Running Inference:  48%|████▊     | 96/200 [35:20<39:37, 22.86s/it]Running Inference:  48%|████▊     | 97/200 [35:43<39:14, 22.86s/it]Running Inference:  49%|████▉     | 98/200 [36:06<38:51, 22.86s/it]Running Inference:  50%|████▉     | 99/200 [36:30<39:10, 23.27s/it]Running Inference:  50%|█████     | 100/200 [36:50<36:53, 22.14s/it]Running Inference:  50%|█████     | 101/200 [37:13<36:53, 22.36s/it]Running Inference:  51%|█████     | 102/200 [37:32<35:02, 21.45s/it]Running Inference:  52%|█████▏    | 103/200 [37:55<35:34, 22.00s/it]Running Inference:  52%|█████▏    | 104/200 [38:18<35:48, 22.38s/it]Running Inference:  52%|█████▎    | 105/200 [38:41<35:41, 22.54s/it]Running Inference:  53%|█████▎    | 106/200 [39:04<35:26, 22.63s/it]Running Inference:  54%|█████▎    | 107/200 [39:27<35:10, 22.70s/it]Running Inference:  54%|█████▍    | 108/200 [39:50<34:53, 22.75s/it]Running Inference:  55%|█████▍    | 109/200 [40:13<34:30, 22.75s/it]Running Inference:  55%|█████▌    | 110/200 [40:35<34:06, 22.74s/it]Running Inference:  56%|█████▌    | 111/200 [40:58<33:46, 22.77s/it]Running Inference:  56%|█████▌    | 112/200 [40:59<23:45, 16.20s/it]Running Inference:  56%|█████▋    | 113/200 [41:22<26:26, 18.23s/it]Running Inference:  57%|█████▋    | 114/200 [41:45<28:08, 19.64s/it]Running Inference:  57%|█████▊    | 115/200 [42:08<29:09, 20.58s/it]Running Inference:  58%|█████▊    | 116/200 [42:30<29:43, 21.23s/it]Running Inference:  58%|█████▊    | 117/200 [42:31<20:54, 15.12s/it]Running Inference:  59%|█████▉    | 118/200 [42:38<17:21, 12.70s/it]Running Inference:  60%|█████▉    | 119/200 [43:01<21:21, 15.82s/it]Running Inference:  60%|██████    | 120/200 [43:24<23:52, 17.90s/it]Running Inference:  60%|██████    | 121/200 [43:41<23:06, 17.55s/it]Running Inference:  61%|██████    | 122/200 [44:04<24:47, 19.07s/it]Running Inference:  62%|██████▏   | 123/200 [44:27<25:56, 20.22s/it]Running Inference:  62%|██████▏   | 124/200 [44:49<26:34, 20.99s/it]Running Inference:  62%|██████▎   | 125/200 [45:12<26:55, 21.54s/it]Running Inference:  63%|██████▎   | 126/200 [45:35<27:01, 21.91s/it]Running Inference:  64%|██████▎   | 127/200 [45:58<26:59, 22.18s/it]Running Inference:  64%|██████▍   | 128/200 [46:21<27:08, 22.61s/it]Running Inference:  64%|██████▍   | 129/200 [46:44<26:51, 22.70s/it]Running Inference:  65%|██████▌   | 130/200 [47:04<25:37, 21.97s/it]Running Inference:  66%|██████▌   | 131/200 [47:29<26:02, 22.65s/it]Running Inference:  66%|██████▌   | 132/200 [47:52<25:43, 22.70s/it]Running Inference:  66%|██████▋   | 133/200 [48:14<25:21, 22.71s/it]Running Inference:  67%|██████▋   | 134/200 [48:37<25:01, 22.75s/it]Running Inference:  68%|██████▊   | 135/200 [49:00<24:51, 22.94s/it]Running Inference:  68%|██████▊   | 136/200 [49:23<24:29, 22.96s/it]Running Inference:  68%|██████▊   | 137/200 [49:27<17:52, 17.03s/it]Running Inference:  69%|██████▉   | 138/200 [49:49<19:21, 18.73s/it]Running Inference:  70%|██████▉   | 139/200 [50:12<20:15, 19.93s/it]Running Inference:  70%|███████   | 140/200 [50:35<20:48, 20.80s/it]Running Inference:  70%|███████   | 141/200 [50:58<21:03, 21.41s/it]Running Inference:  71%|███████   | 142/200 [51:21<21:06, 21.83s/it]Running Inference:  72%|███████▏  | 143/200 [51:43<21:01, 22.12s/it]Running Inference:  72%|███████▏  | 144/200 [52:06<20:49, 22.31s/it]Running Inference:  72%|███████▎  | 145/200 [52:29<20:34, 22.45s/it]Running Inference:  73%|███████▎  | 146/200 [52:52<20:17, 22.55s/it]Running Inference:  74%|███████▎  | 147/200 [53:14<19:58, 22.62s/it]Running Inference:  74%|███████▍  | 148/200 [53:37<19:40, 22.70s/it]Running Inference:  74%|███████▍  | 149/200 [54:00<19:17, 22.70s/it]Running Inference:  75%|███████▌  | 150/200 [54:21<18:29, 22.19s/it]Running Inference:  76%|███████▌  | 151/200 [54:44<18:16, 22.37s/it]Running Inference:  76%|███████▌  | 152/200 [55:07<18:00, 22.52s/it]Running Inference:  76%|███████▋  | 153/200 [55:30<17:46, 22.69s/it]Running Inference:  77%|███████▋  | 154/200 [55:53<17:26, 22.75s/it]Running Inference:  78%|███████▊  | 155/200 [56:16<17:08, 22.85s/it]Running Inference:  78%|███████▊  | 156/200 [56:39<16:45, 22.85s/it]Running Inference:  78%|███████▊  | 157/200 [57:02<16:25, 22.92s/it]Running Inference:  79%|███████▉  | 158/200 [57:25<16:03, 22.95s/it]Running Inference:  80%|███████▉  | 159/200 [57:47<15:26, 22.60s/it]Running Inference:  80%|████████  | 160/200 [58:10<15:10, 22.76s/it]Running Inference:  80%|████████  | 161/200 [58:32<14:48, 22.78s/it]Running Inference:  81%|████████  | 162/200 [58:55<14:26, 22.79s/it]Running Inference:  82%|████████▏ | 163/200 [59:18<14:04, 22.84s/it]Running Inference:  82%|████████▏ | 164/200 [59:41<13:44, 22.91s/it]Running Inference:  82%|████████▎ | 165/200 [1:00:04<13:19, 22.85s/it]Running Inference:  83%|████████▎ | 166/200 [1:00:14<10:48, 19.08s/it]Running Inference:  84%|████████▎ | 167/200 [1:00:37<11:07, 20.22s/it]Running Inference:  84%|████████▍ | 168/200 [1:01:00<11:11, 20.98s/it]Running Inference:  84%|████████▍ | 169/200 [1:01:23<11:08, 21.57s/it]Running Inference:  85%|████████▌ | 170/200 [1:01:46<11:00, 22.02s/it]Running Inference:  86%|████████▌ | 171/200 [1:02:09<10:46, 22.29s/it]Running Inference:  86%|████████▌ | 172/200 [1:02:32<10:29, 22.48s/it]Running Inference:  86%|████████▋ | 173/200 [1:02:49<09:21, 20.80s/it]Running Inference:  87%|████████▋ | 174/200 [1:03:12<09:17, 21.43s/it]Running Inference:  88%|████████▊ | 175/200 [1:03:34<09:06, 21.85s/it]Running Inference:  88%|████████▊ | 176/200 [1:03:57<08:51, 22.16s/it]Running Inference:  88%|████████▊ | 177/200 [1:04:20<08:34, 22.39s/it]Running Inference:  89%|████████▉ | 178/200 [1:04:44<08:18, 22.66s/it]Running Inference:  90%|████████▉ | 179/200 [1:05:06<07:56, 22.71s/it]Running Inference:  90%|█████████ | 180/200 [1:05:29<07:34, 22.75s/it]Running Inference:  90%|█████████ | 181/200 [1:05:52<07:12, 22.77s/it]Running Inference:  91%|█████████ | 182/200 [1:06:12<06:33, 21.84s/it]Running Inference:  92%|█████████▏| 183/200 [1:06:34<06:16, 22.12s/it]Running Inference:  92%|█████████▏| 184/200 [1:06:58<06:02, 22.68s/it]Running Inference:  92%|█████████▎| 185/200 [1:07:21<05:41, 22.75s/it]Running Inference:  93%|█████████▎| 186/200 [1:07:44<05:18, 22.77s/it]Running Inference:  94%|█████████▎| 187/200 [1:08:07<04:56, 22.78s/it]Running Inference:  94%|█████████▍| 188/200 [1:08:30<04:33, 22.81s/it]Running Inference:  94%|█████████▍| 189/200 [1:08:47<03:52, 21.17s/it]Running Inference:  95%|█████████▌| 190/200 [1:09:10<03:36, 21.68s/it]Running Inference:  96%|█████████▌| 191/200 [1:09:29<03:07, 20.85s/it]Running Inference:  96%|█████████▌| 192/200 [1:09:52<02:52, 21.51s/it]Running Inference:  96%|█████████▋| 193/200 [1:10:15<02:33, 21.97s/it]Running Inference:  97%|█████████▋| 194/200 [1:10:28<01:55, 19.31s/it]Running Inference:  98%|█████████▊| 195/200 [1:10:51<01:42, 20.41s/it]Running Inference:  98%|█████████▊| 196/200 [1:11:14<01:24, 21.25s/it]Running Inference:  98%|█████████▊| 197/200 [1:11:37<01:05, 21.72s/it]Running Inference:  99%|█████████▉| 198/200 [1:12:00<00:44, 22.04s/it]Running Inference: 100%|█████████▉| 199/200 [1:12:23<00:22, 22.27s/it]Running Inference: 100%|██████████| 200/200 [1:12:46<00:00, 22.51s/it]Running Inference: 100%|██████████| 200/200 [1:12:46<00:00, 21.83s/it]
2025-12-14 07:26:19,885 - INFO - Inference completed.
2025-12-14 07:26:19,907 - INFO - Results saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 07:26:19,907 - INFO - Calculating metrics for dataset: longbench
2025-12-14 07:26:26,796 - INFO - Metrics saved to longbenchresult/longbench__multi_news__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 07:26:26,796 - INFO - Metrics:
11.18
2025-12-14 07:26:26,797 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multi_news, ratio=0.5) on GPU 2


========================================
LongBench Task: qmsum
========================================
----------------------------------------
Task: qmsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 07:26:33,463 - INFO - Set deterministic seeds to 42
2025-12-14 07:26:33,463 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 07:26:33,463 - INFO - Starting evaluation run...
2025-12-14 07:26:33,463 - INFO - Output directory set to: longbenchresult
2025-12-14 07:26:33,464 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 07:26:33,464 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 07:26:33,464 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 07:26:33,464 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.92it/s]
Device set to use cuda:0
2025-12-14 07:26:45,584 - INFO - Model pipeline loaded.
2025-12-14 07:26:45,584 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 07:26:48,846 - INFO - Dataset loaded with 200 entries.
2025-12-14 07:26:48,846 - INFO - Dataset processed with 200 entries.
2025-12-14 07:26:48,860 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [00:54<31:00, 54.73s/it]Running Inference:   6%|▌         | 2/35 [01:26<22:46, 41.42s/it]Running Inference:   9%|▊         | 3/35 [02:47<31:42, 59.44s/it]Running Inference:  11%|█▏        | 4/35 [03:48<30:55, 59.87s/it]Running Inference:  14%|█▍        | 5/35 [04:42<29:00, 58.00s/it]Running Inference:  17%|█▋        | 6/35 [05:21<24:45, 51.23s/it]Running Inference:  20%|██        | 7/35 [05:58<21:47, 46.69s/it]Running Inference:  23%|██▎       | 8/35 [07:53<30:46, 68.39s/it]Running Inference:  26%|██▌       | 9/35 [09:35<34:13, 78.99s/it]Running Inference:  29%|██▊       | 10/35 [10:50<32:21, 77.65s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [12:19<32:32, 81.35s/it]Running Inference:  34%|███▍      | 12/35 [13:04<26:57, 70.31s/it]Running Inference:  37%|███▋      | 13/35 [14:32<27:44, 75.66s/it]Running Inference:  40%|████      | 14/35 [15:31<24:41, 70.56s/it]Running Inference:  43%|████▎     | 15/35 [15:44<17:40, 53.04s/it]Running Inference:  46%|████▌     | 16/35 [16:24<15:37, 49.34s/it]Running Inference:  49%|████▊     | 17/35 [17:01<13:37, 45.44s/it]Running Inference:  51%|█████▏    | 18/35 [17:53<13:25, 47.38s/it]Running Inference:  54%|█████▍    | 19/35 [18:13<10:29, 39.32s/it]Running Inference:  57%|█████▋    | 20/35 [18:40<08:52, 35.50s/it]Running Inference:  60%|██████    | 21/35 [18:54<06:47, 29.08s/it]Running Inference:  63%|██████▎   | 22/35 [19:30<06:45, 31.19s/it]Running Inference:  66%|██████▌   | 23/35 [20:22<07:27, 37.32s/it]Running Inference:  69%|██████▊   | 24/35 [21:59<10:08, 55.30s/it]Running Inference:  71%|███████▏  | 25/35 [23:41<11:32, 69.23s/it]Running Inference:  74%|███████▍  | 26/35 [24:22<09:06, 60.77s/it]Running Inference:  77%|███████▋  | 27/35 [25:05<07:23, 55.43s/it]Running Inference:  80%|████████  | 28/35 [25:33<05:30, 47.18s/it]Running Inference:  83%|████████▎ | 29/35 [26:11<04:27, 44.59s/it]Running Inference:  86%|████████▌ | 30/35 [26:34<03:10, 38.10s/it]Running Inference:  89%|████████▊ | 31/35 [27:55<03:24, 51.04s/it]Running Inference:  91%|█████████▏| 32/35 [29:02<02:46, 55.65s/it]Running Inference:  94%|█████████▍| 33/35 [31:09<02:34, 77.09s/it]Running Inference:  97%|█████████▋| 34/35 [31:30<01:00, 60.38s/it]Running Inference: 100%|██████████| 35/35 [32:14<00:00, 55.36s/it]Running Inference: 100%|██████████| 35/35 [32:14<00:00, 55.27s/it]
2025-12-14 07:59:03,235 - INFO - Inference completed.
2025-12-14 07:59:03,248 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 07:59:03,248 - INFO - Calculating metrics for dataset: longbench
2025-12-14 07:59:04,449 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 07:59:04,449 - INFO - Metrics:
19.82
2025-12-14 07:59:04,451 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qmsum, ratio=0.1) on GPU 2

----------------------------------------
Task: qmsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 07:59:10,860 - INFO - Set deterministic seeds to 42
2025-12-14 07:59:10,860 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 07:59:10,860 - INFO - Starting evaluation run...
2025-12-14 07:59:10,860 - INFO - Output directory set to: longbenchresult
2025-12-14 07:59:10,861 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 07:59:10,861 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 07:59:10,861 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 07:59:10,861 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.54it/s]
Device set to use cuda:0
2025-12-14 07:59:22,193 - INFO - Model pipeline loaded.
2025-12-14 07:59:22,194 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 07:59:25,678 - INFO - Dataset loaded with 200 entries.
2025-12-14 07:59:25,679 - INFO - Dataset processed with 200 entries.
2025-12-14 07:59:25,693 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [01:00<34:25, 60.76s/it]Running Inference:   6%|▌         | 2/35 [01:13<17:47, 32.34s/it]Running Inference:   9%|▊         | 3/35 [02:32<28:47, 53.97s/it]Running Inference:  11%|█▏        | 4/35 [03:19<26:27, 51.20s/it]Running Inference:  14%|█▍        | 5/35 [04:15<26:20, 52.69s/it]Running Inference:  17%|█▋        | 6/35 [04:57<23:44, 49.12s/it]Running Inference:  20%|██        | 7/35 [05:13<17:49, 38.21s/it]Running Inference:  23%|██▎       | 8/35 [06:53<26:09, 58.13s/it]Running Inference:  26%|██▌       | 9/35 [08:45<32:21, 74.69s/it]Running Inference:  29%|██▊       | 10/35 [09:42<28:50, 69.24s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [10:50<27:33, 68.88s/it]Running Inference:  34%|███▍      | 12/35 [11:33<23:27, 61.18s/it]Running Inference:  37%|███▋      | 13/35 [13:01<25:21, 69.17s/it]Running Inference:  40%|████      | 14/35 [14:19<25:12, 72.00s/it]Running Inference:  43%|████▎     | 15/35 [14:33<18:07, 54.40s/it]Running Inference:  46%|████▌     | 16/35 [15:13<15:50, 50.03s/it]Running Inference:  49%|████▊     | 17/35 [16:09<15:36, 52.03s/it]Running Inference:  51%|█████▏    | 18/35 [16:57<14:21, 50.70s/it]Running Inference:  54%|█████▍    | 19/35 [17:19<11:12, 42.03s/it]Running Inference:  57%|█████▋    | 20/35 [17:43<09:09, 36.62s/it]Running Inference:  60%|██████    | 21/35 [17:54<06:44, 28.92s/it]Running Inference:  63%|██████▎   | 22/35 [18:28<06:35, 30.40s/it]Running Inference:  66%|██████▌   | 23/35 [19:43<08:46, 43.84s/it]Running Inference:  69%|██████▊   | 24/35 [21:51<12:40, 69.14s/it]Running Inference:  71%|███████▏  | 25/35 [22:34<10:11, 61.19s/it]Running Inference:  74%|███████▍  | 26/35 [22:57<07:28, 49.82s/it]Running Inference:  77%|███████▋  | 27/35 [23:41<06:23, 47.98s/it]Running Inference:  80%|████████  | 28/35 [24:51<06:23, 54.72s/it]Running Inference:  83%|████████▎ | 29/35 [25:45<05:25, 54.32s/it]Running Inference:  86%|████████▌ | 30/35 [25:50<03:17, 39.54s/it]Running Inference:  89%|████████▊ | 31/35 [26:59<03:13, 48.43s/it]Running Inference:  91%|█████████▏| 32/35 [28:24<02:58, 59.59s/it]Running Inference:  94%|█████████▍| 33/35 [30:46<02:48, 84.08s/it]Running Inference:  97%|█████████▋| 34/35 [31:04<01:04, 64.40s/it]Running Inference: 100%|██████████| 35/35 [32:19<00:00, 67.60s/it]Running Inference: 100%|██████████| 35/35 [32:19<00:00, 55.42s/it]
2025-12-14 08:31:45,376 - INFO - Inference completed.
2025-12-14 08:31:45,389 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 08:31:45,389 - INFO - Calculating metrics for dataset: longbench
2025-12-14 08:31:46,537 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 08:31:46,537 - INFO - Metrics:
20.43
2025-12-14 08:31:46,539 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qmsum, ratio=0.2) on GPU 2

----------------------------------------
Task: qmsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 08:31:52,996 - INFO - Set deterministic seeds to 42
2025-12-14 08:31:52,996 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 08:31:52,996 - INFO - Starting evaluation run...
2025-12-14 08:31:52,996 - INFO - Output directory set to: longbenchresult
2025-12-14 08:31:52,996 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 08:31:52,996 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 08:31:52,996 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 08:31:52,996 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.57it/s]
Device set to use cuda:0
2025-12-14 08:32:05,231 - INFO - Model pipeline loaded.
2025-12-14 08:32:05,231 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 08:32:08,855 - INFO - Dataset loaded with 200 entries.
2025-12-14 08:32:08,855 - INFO - Dataset processed with 200 entries.
2025-12-14 08:32:08,869 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [00:54<30:52, 54.48s/it]Running Inference:   6%|▌         | 2/35 [01:06<16:15, 29.58s/it]Running Inference:   9%|▊         | 3/35 [02:26<27:58, 52.46s/it]Running Inference:  11%|█▏        | 4/35 [03:26<28:45, 55.66s/it]Running Inference:  14%|█▍        | 5/35 [04:23<27:58, 55.96s/it]Running Inference:  17%|█▋        | 6/35 [04:45<21:26, 44.37s/it]Running Inference:  20%|██        | 7/35 [05:03<16:45, 35.92s/it]Running Inference:  23%|██▎       | 8/35 [06:22<22:17, 49.55s/it]Running Inference:  26%|██▌       | 9/35 [08:13<29:46, 68.71s/it]Running Inference:  29%|██▊       | 10/35 [09:30<29:42, 71.29s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [10:42<28:34, 71.44s/it]Running Inference:  34%|███▍      | 12/35 [11:48<26:46, 69.86s/it]Running Inference:  37%|███▋      | 13/35 [13:19<27:57, 76.27s/it]Running Inference:  40%|████      | 14/35 [14:24<25:29, 72.81s/it]Running Inference:  43%|████▎     | 15/35 [14:53<19:55, 59.79s/it]Running Inference:  46%|████▌     | 16/35 [15:36<17:19, 54.70s/it]Running Inference:  49%|████▊     | 17/35 [16:28<16:09, 53.89s/it]Running Inference:  51%|█████▏    | 18/35 [17:30<15:55, 56.23s/it]Running Inference:  54%|█████▍    | 19/35 [17:50<12:06, 45.38s/it]Running Inference:  57%|█████▋    | 20/35 [18:30<10:58, 43.88s/it]Running Inference:  60%|██████    | 21/35 [18:45<08:12, 35.21s/it]Running Inference:  63%|██████▎   | 22/35 [19:15<07:16, 33.58s/it]Running Inference:  66%|██████▌   | 23/35 [20:24<08:50, 44.24s/it]Running Inference:  69%|██████▊   | 24/35 [21:59<10:51, 59.26s/it]Running Inference:  71%|███████▏  | 25/35 [22:22<08:05, 48.58s/it]Running Inference:  74%|███████▍  | 26/35 [23:05<07:01, 46.88s/it]Running Inference:  77%|███████▋  | 27/35 [23:43<05:52, 44.05s/it]Running Inference:  80%|████████  | 28/35 [24:14<04:42, 40.33s/it]Running Inference:  83%|████████▎ | 29/35 [25:10<04:30, 45.01s/it]Running Inference:  86%|████████▌ | 30/35 [25:19<02:50, 34.09s/it]Running Inference:  89%|████████▊ | 31/35 [26:23<02:52, 43.13s/it]Running Inference:  91%|█████████▏| 32/35 [27:28<02:28, 49.65s/it]Running Inference:  94%|█████████▍| 33/35 [29:10<02:11, 65.52s/it]Running Inference:  97%|█████████▋| 34/35 [29:30<00:51, 51.86s/it]Running Inference: 100%|██████████| 35/35 [30:51<00:00, 60.40s/it]Running Inference: 100%|██████████| 35/35 [30:51<00:00, 52.89s/it]
2025-12-14 09:03:00,159 - INFO - Inference completed.
2025-12-14 09:03:00,172 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 09:03:00,172 - INFO - Calculating metrics for dataset: longbench
2025-12-14 09:03:01,352 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 09:03:01,352 - INFO - Metrics:
19.4
2025-12-14 09:03:01,353 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qmsum, ratio=0.3) on GPU 2

----------------------------------------
Task: qmsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 09:03:07,829 - INFO - Set deterministic seeds to 42
2025-12-14 09:03:07,830 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "qmsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 09:03:07,830 - INFO - Starting evaluation run...
2025-12-14 09:03:07,830 - INFO - Output directory set to: longbenchresult
2025-12-14 09:03:07,830 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 09:03:07,830 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 09:03:07,830 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 09:03:07,830 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.99it/s]
Device set to use cuda:0
2025-12-14 09:03:20,766 - INFO - Model pipeline loaded.
2025-12-14 09:03:20,766 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: qmsum)
2025-12-14 09:03:24,203 - INFO - Dataset loaded with 200 entries.
2025-12-14 09:03:24,203 - INFO - Dataset processed with 200 entries.
2025-12-14 09:03:24,218 - INFO - Starting inference...
Running Inference:   0%|          | 0/35 [00:00<?, ?it/s]Running Inference:   3%|▎         | 1/35 [00:54<30:59, 54.69s/it]Running Inference:   6%|▌         | 2/35 [01:27<22:55, 41.69s/it]Running Inference:   9%|▊         | 3/35 [02:44<30:48, 57.75s/it]Running Inference:  11%|█▏        | 4/35 [03:10<23:27, 45.40s/it]Running Inference:  14%|█▍        | 5/35 [04:04<24:19, 48.64s/it]Running Inference:  17%|█▋        | 6/35 [04:30<19:41, 40.74s/it]Running Inference:  20%|██        | 7/35 [05:08<18:34, 39.80s/it]Running Inference:  23%|██▎       | 8/35 [06:39<25:15, 56.13s/it]Running Inference:  26%|██▌       | 9/35 [08:29<31:42, 73.16s/it]Running Inference:  29%|██▊       | 10/35 [10:11<34:09, 81.96s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:  31%|███▏      | 11/35 [11:18<30:56, 77.35s/it]Running Inference:  34%|███▍      | 12/35 [12:04<25:56, 67.68s/it]Running Inference:  37%|███▋      | 13/35 [12:43<21:37, 58.97s/it]Running Inference:  40%|████      | 14/35 [14:20<24:40, 70.49s/it]Running Inference:  43%|████▎     | 15/35 [14:54<19:52, 59.64s/it]Running Inference:  46%|████▌     | 16/35 [15:35<17:04, 53.95s/it]Running Inference:  49%|████▊     | 17/35 [16:30<16:17, 54.30s/it]Running Inference:  51%|█████▏    | 18/35 [17:14<14:33, 51.35s/it]Running Inference:  54%|█████▍    | 19/35 [17:33<11:02, 41.40s/it]Running Inference:  57%|█████▋    | 20/35 [18:11<10:06, 40.44s/it]Running Inference:  60%|██████    | 21/35 [18:22<07:22, 31.59s/it]Running Inference:  63%|██████▎   | 22/35 [19:12<08:04, 37.27s/it]Running Inference:  66%|██████▌   | 23/35 [19:59<08:00, 40.08s/it]Running Inference:  69%|██████▊   | 24/35 [20:53<08:07, 44.31s/it]Running Inference:  71%|███████▏  | 25/35 [21:33<07:10, 43.10s/it]Running Inference:  74%|███████▍  | 26/35 [21:50<05:17, 35.27s/it]Running Inference:  77%|███████▋  | 27/35 [22:12<04:08, 31.03s/it]Running Inference:  80%|████████  | 28/35 [23:19<04:53, 41.92s/it]Running Inference:  83%|████████▎ | 29/35 [23:51<03:54, 39.04s/it]Running Inference:  86%|████████▌ | 30/35 [23:56<02:23, 28.66s/it]Running Inference:  89%|████████▊ | 31/35 [25:20<03:01, 45.36s/it]Running Inference:  91%|█████████▏| 32/35 [26:21<02:30, 50.00s/it]Running Inference:  94%|█████████▍| 33/35 [28:18<02:20, 70.28s/it]Running Inference:  97%|█████████▋| 34/35 [28:39<00:55, 55.25s/it]Running Inference: 100%|██████████| 35/35 [29:38<00:00, 56.39s/it]Running Inference: 100%|██████████| 35/35 [29:38<00:00, 50.80s/it]
2025-12-14 09:33:02,365 - INFO - Inference completed.
2025-12-14 09:33:02,378 - INFO - Results saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 09:33:02,378 - INFO - Calculating metrics for dataset: longbench
2025-12-14 09:33:03,425 - INFO - Metrics saved to longbenchresult/longbench__qmsum__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 09:33:03,425 - INFO - Metrics:
18.31
2025-12-14 09:33:03,426 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=qmsum, ratio=0.5) on GPU 2


========================================
LongBench Task: samsum
========================================
----------------------------------------
Task: samsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 09:33:09,952 - INFO - Set deterministic seeds to 42
2025-12-14 09:33:09,952 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 09:33:09,952 - INFO - Starting evaluation run...
2025-12-14 09:33:09,952 - INFO - Output directory set to: longbenchresult
2025-12-14 09:33:09,952 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 09:33:09,952 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 09:33:09,952 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 09:33:09,952 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.73it/s]
Device set to use cuda:0
2025-12-14 09:33:22,121 - INFO - Model pipeline loaded.
2025-12-14 09:33:22,121 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 09:33:25,352 - INFO - Dataset loaded with 200 entries.
2025-12-14 09:33:25,352 - INFO - Dataset processed with 200 entries.
2025-12-14 09:33:25,379 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:29,  3.46s/it]Running Inference:   1%|          | 2/200 [00:09<16:00,  4.85s/it]Running Inference:   2%|▏         | 3/200 [00:15<18:36,  5.67s/it]Running Inference:   2%|▏         | 4/200 [00:23<20:23,  6.24s/it]Running Inference:   2%|▎         | 5/200 [00:24<14:35,  4.49s/it]Running Inference:   3%|▎         | 6/200 [00:29<15:32,  4.81s/it]Running Inference:   4%|▎         | 7/200 [00:32<12:43,  3.96s/it]Running Inference:   4%|▍         | 8/200 [00:35<12:06,  3.78s/it]Running Inference:   4%|▍         | 9/200 [00:42<15:07,  4.75s/it]Running Inference:   5%|▌         | 10/200 [00:46<14:42,  4.64s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:54<17:44,  5.63s/it]Running Inference:   6%|▌         | 12/200 [00:58<15:48,  5.04s/it]Running Inference:   6%|▋         | 13/200 [01:00<12:58,  4.16s/it]Running Inference:   7%|▋         | 14/200 [01:04<12:57,  4.18s/it]Running Inference:   8%|▊         | 15/200 [01:07<11:37,  3.77s/it]Running Inference:   8%|▊         | 16/200 [01:14<14:28,  4.72s/it]Running Inference:   8%|▊         | 17/200 [01:22<17:30,  5.74s/it]Running Inference:   9%|▉         | 18/200 [01:29<18:40,  6.16s/it]Running Inference:  10%|▉         | 19/200 [01:37<20:11,  6.69s/it]Running Inference:  10%|█         | 20/200 [01:39<16:04,  5.36s/it]Running Inference:  10%|█         | 21/200 [01:42<13:24,  4.49s/it]Running Inference:  11%|█         | 22/200 [01:47<14:10,  4.78s/it]Running Inference:  12%|█▏        | 23/200 [01:51<13:27,  4.56s/it]Running Inference:  12%|█▏        | 24/200 [01:54<11:34,  3.95s/it]Running Inference:  12%|█▎        | 25/200 [02:01<13:55,  4.77s/it]Running Inference:  13%|█▎        | 26/200 [02:05<13:23,  4.62s/it]Running Inference:  14%|█▎        | 27/200 [02:11<15:00,  5.20s/it]Running Inference:  14%|█▍        | 28/200 [02:15<13:13,  4.61s/it]Running Inference:  14%|█▍        | 29/200 [02:16<10:09,  3.57s/it]Running Inference:  15%|█▌        | 30/200 [02:18<09:09,  3.23s/it]Running Inference:  16%|█▌        | 31/200 [02:20<08:09,  2.89s/it]Running Inference:  16%|█▌        | 32/200 [02:25<09:20,  3.33s/it]Running Inference:  16%|█▋        | 33/200 [02:31<11:59,  4.31s/it]Running Inference:  17%|█▋        | 34/200 [02:35<11:10,  4.04s/it]Running Inference:  18%|█▊        | 35/200 [02:38<10:17,  3.74s/it]Running Inference:  18%|█▊        | 36/200 [02:46<13:34,  4.97s/it]Running Inference:  18%|█▊        | 37/200 [02:48<11:37,  4.28s/it]Running Inference:  19%|█▉        | 38/200 [02:51<10:32,  3.91s/it]Running Inference:  20%|█▉        | 39/200 [03:00<14:04,  5.24s/it]Running Inference:  20%|██        | 40/200 [03:03<12:49,  4.81s/it]Running Inference:  20%|██        | 41/200 [03:06<11:05,  4.19s/it]Running Inference:  21%|██        | 42/200 [03:13<13:07,  4.98s/it]Running Inference:  22%|██▏       | 43/200 [03:15<10:48,  4.13s/it]Running Inference:  22%|██▏       | 44/200 [03:23<13:30,  5.20s/it]Running Inference:  22%|██▎       | 45/200 [03:31<15:35,  6.04s/it]Running Inference:  23%|██▎       | 46/200 [03:38<16:46,  6.54s/it]Running Inference:  24%|██▎       | 47/200 [03:44<15:32,  6.09s/it]Running Inference:  24%|██▍       | 48/200 [03:48<13:55,  5.50s/it]Running Inference:  24%|██▍       | 49/200 [03:55<15:34,  6.19s/it]Running Inference:  25%|██▌       | 50/200 [04:03<16:34,  6.63s/it]Running Inference:  26%|██▌       | 51/200 [04:10<16:26,  6.62s/it]Running Inference:  26%|██▌       | 52/200 [04:13<13:41,  5.55s/it]Running Inference:  26%|██▋       | 53/200 [04:16<11:33,  4.72s/it]Running Inference:  27%|██▋       | 54/200 [04:22<12:49,  5.27s/it]Running Inference:  28%|██▊       | 55/200 [04:25<11:13,  4.64s/it]Running Inference:  28%|██▊       | 56/200 [04:31<11:34,  4.82s/it]Running Inference:  28%|██▊       | 57/200 [04:34<10:23,  4.36s/it]Running Inference:  29%|██▉       | 58/200 [04:42<13:03,  5.52s/it]Running Inference:  30%|██▉       | 59/200 [04:45<11:00,  4.68s/it]Running Inference:  30%|███       | 60/200 [04:49<10:29,  4.50s/it]Running Inference:  30%|███       | 61/200 [04:53<09:58,  4.31s/it]Running Inference:  31%|███       | 62/200 [05:00<11:44,  5.10s/it]Running Inference:  32%|███▏      | 63/200 [05:07<12:54,  5.65s/it]Running Inference:  32%|███▏      | 64/200 [05:10<11:23,  5.02s/it]Running Inference:  32%|███▎      | 65/200 [05:16<11:42,  5.20s/it]Running Inference:  33%|███▎      | 66/200 [05:19<10:00,  4.48s/it]Running Inference:  34%|███▎      | 67/200 [05:19<07:29,  3.38s/it]Running Inference:  34%|███▍      | 68/200 [05:26<09:38,  4.38s/it]Running Inference:  34%|███▍      | 69/200 [05:28<08:11,  3.75s/it]Running Inference:  35%|███▌      | 70/200 [05:36<10:38,  4.91s/it]Running Inference:  36%|███▌      | 71/200 [05:38<08:57,  4.17s/it]Running Inference:  36%|███▌      | 72/200 [05:41<07:42,  3.61s/it]Running Inference:  36%|███▋      | 73/200 [05:43<06:47,  3.21s/it]Running Inference:  37%|███▋      | 74/200 [05:45<05:55,  2.82s/it]Running Inference:  38%|███▊      | 75/200 [05:46<04:53,  2.35s/it]Running Inference:  38%|███▊      | 76/200 [05:48<04:37,  2.24s/it]Running Inference:  38%|███▊      | 77/200 [05:51<05:00,  2.45s/it]Running Inference:  39%|███▉      | 78/200 [05:54<05:13,  2.57s/it]Running Inference:  40%|███▉      | 79/200 [05:56<04:59,  2.47s/it]Running Inference:  40%|████      | 80/200 [05:58<04:32,  2.27s/it]Running Inference:  40%|████      | 81/200 [05:59<03:30,  1.77s/it]Running Inference:  41%|████      | 82/200 [06:00<03:21,  1.71s/it]Running Inference:  42%|████▏     | 83/200 [06:08<07:10,  3.68s/it]Running Inference:  42%|████▏     | 84/200 [06:17<09:43,  5.03s/it]Running Inference:  42%|████▎     | 85/200 [06:24<10:59,  5.74s/it]Running Inference:  43%|████▎     | 86/200 [06:26<08:40,  4.57s/it]Running Inference:  44%|████▎     | 87/200 [06:29<07:57,  4.22s/it]Running Inference:  44%|████▍     | 88/200 [06:32<07:00,  3.75s/it]Running Inference:  44%|████▍     | 89/200 [06:36<07:16,  3.93s/it]Running Inference:  45%|████▌     | 90/200 [06:39<06:21,  3.47s/it]Running Inference:  46%|████▌     | 91/200 [06:46<08:36,  4.74s/it]Running Inference:  46%|████▌     | 92/200 [06:54<10:00,  5.56s/it]Running Inference:  46%|████▋     | 93/200 [06:58<09:01,  5.06s/it]Running Inference:  47%|████▋     | 94/200 [07:00<07:28,  4.23s/it]Running Inference:  48%|████▊     | 95/200 [07:02<06:20,  3.63s/it]Running Inference:  48%|████▊     | 96/200 [07:06<06:11,  3.57s/it]Running Inference:  48%|████▊     | 97/200 [07:07<05:01,  2.93s/it]Running Inference:  49%|████▉     | 98/200 [07:15<07:41,  4.53s/it]Running Inference:  50%|████▉     | 99/200 [07:22<08:48,  5.23s/it]Running Inference:  50%|█████     | 100/200 [07:30<09:51,  5.92s/it]Running Inference:  50%|█████     | 101/200 [07:33<08:13,  4.99s/it]Running Inference:  51%|█████     | 102/200 [07:36<07:28,  4.57s/it]Running Inference:  52%|█████▏    | 103/200 [07:43<08:33,  5.30s/it]Running Inference:  52%|█████▏    | 104/200 [07:50<09:10,  5.73s/it]Running Inference:  52%|█████▎    | 105/200 [07:52<07:34,  4.79s/it]Running Inference:  53%|█████▎    | 106/200 [08:00<08:58,  5.73s/it]Running Inference:  54%|█████▎    | 107/200 [08:07<09:27,  6.10s/it]Running Inference:  54%|█████▍    | 108/200 [08:15<10:00,  6.52s/it]Running Inference:  55%|█████▍    | 109/200 [08:17<07:39,  5.05s/it]Running Inference:  55%|█████▌    | 110/200 [08:22<07:48,  5.21s/it]Running Inference:  56%|█████▌    | 111/200 [08:24<06:28,  4.37s/it]Running Inference:  56%|█████▌    | 112/200 [08:31<07:30,  5.12s/it]Running Inference:  56%|█████▋    | 113/200 [08:34<06:26,  4.45s/it]Running Inference:  57%|█████▋    | 114/200 [08:41<07:20,  5.13s/it]Running Inference:  57%|█████▊    | 115/200 [08:49<08:29,  5.99s/it]Running Inference:  58%|█████▊    | 116/200 [08:52<07:06,  5.07s/it]Running Inference:  58%|█████▊    | 117/200 [08:53<05:32,  4.00s/it]Running Inference:  59%|█████▉    | 118/200 [08:59<06:17,  4.60s/it]Running Inference:  60%|█████▉    | 119/200 [09:02<05:20,  3.96s/it]Running Inference:  60%|██████    | 120/200 [09:10<06:51,  5.14s/it]Running Inference:  60%|██████    | 121/200 [09:13<05:49,  4.43s/it]Running Inference:  61%|██████    | 122/200 [09:16<05:21,  4.12s/it]Running Inference:  62%|██████▏   | 123/200 [09:23<06:20,  4.95s/it]Running Inference:  62%|██████▏   | 124/200 [09:29<06:54,  5.45s/it]Running Inference:  62%|██████▎   | 125/200 [09:35<06:44,  5.40s/it]Running Inference:  63%|██████▎   | 126/200 [09:38<05:49,  4.72s/it]Running Inference:  64%|██████▎   | 127/200 [09:40<04:56,  4.07s/it]Running Inference:  64%|██████▍   | 128/200 [09:45<05:05,  4.24s/it]Running Inference:  64%|██████▍   | 129/200 [09:48<04:24,  3.73s/it]Running Inference:  65%|██████▌   | 130/200 [09:55<05:44,  4.92s/it]Running Inference:  66%|██████▌   | 131/200 [10:00<05:43,  4.97s/it]Running Inference:  66%|██████▌   | 132/200 [10:03<04:52,  4.30s/it]Running Inference:  66%|██████▋   | 133/200 [10:06<04:22,  3.92s/it]Running Inference:  67%|██████▋   | 134/200 [10:09<03:59,  3.63s/it]Running Inference:  68%|██████▊   | 135/200 [10:16<05:00,  4.62s/it]Running Inference:  68%|██████▊   | 136/200 [10:23<05:45,  5.39s/it]Running Inference:  68%|██████▊   | 137/200 [10:26<04:42,  4.48s/it]Running Inference:  69%|██████▉   | 138/200 [10:31<05:04,  4.91s/it]Running Inference:  70%|██████▉   | 139/200 [10:38<05:32,  5.45s/it]Running Inference:  70%|███████   | 140/200 [10:43<05:22,  5.37s/it]Running Inference:  70%|███████   | 141/200 [10:51<05:49,  5.93s/it]Running Inference:  71%|███████   | 142/200 [10:53<04:35,  4.76s/it]Running Inference:  72%|███████▏  | 143/200 [11:00<05:14,  5.51s/it]Running Inference:  72%|███████▏  | 144/200 [11:07<05:41,  6.09s/it]Running Inference:  72%|███████▎  | 145/200 [11:09<04:28,  4.89s/it]Running Inference:  73%|███████▎  | 146/200 [11:12<03:54,  4.34s/it]Running Inference:  74%|███████▎  | 147/200 [11:15<03:27,  3.92s/it]Running Inference:  74%|███████▍  | 148/200 [11:17<02:47,  3.22s/it]Running Inference:  74%|███████▍  | 149/200 [11:24<03:37,  4.26s/it]Running Inference:  75%|███████▌  | 150/200 [11:26<03:02,  3.65s/it]Running Inference:  76%|███████▌  | 151/200 [11:33<03:47,  4.65s/it]Running Inference:  76%|███████▌  | 152/200 [11:34<02:57,  3.70s/it]Running Inference:  76%|███████▋  | 153/200 [11:38<02:47,  3.55s/it]Running Inference:  77%|███████▋  | 154/200 [11:39<02:15,  2.95s/it]Running Inference:  78%|███████▊  | 155/200 [11:41<01:54,  2.54s/it]Running Inference:  78%|███████▊  | 156/200 [11:43<01:52,  2.56s/it]Running Inference:  78%|███████▊  | 157/200 [11:45<01:32,  2.15s/it]Running Inference:  79%|███████▉  | 158/200 [11:47<01:33,  2.24s/it]Running Inference:  80%|███████▉  | 159/200 [11:52<02:05,  3.05s/it]Running Inference:  80%|████████  | 160/200 [11:59<02:47,  4.18s/it]Running Inference:  80%|████████  | 161/200 [12:06<03:14,  4.98s/it]Running Inference:  81%|████████  | 162/200 [12:13<03:40,  5.79s/it]Running Inference:  82%|████████▏ | 163/200 [12:22<04:02,  6.56s/it]Running Inference:  82%|████████▏ | 164/200 [12:27<03:38,  6.08s/it]Running Inference:  82%|████████▎ | 165/200 [12:33<03:40,  6.29s/it]Running Inference:  83%|████████▎ | 166/200 [12:40<03:40,  6.48s/it]Running Inference:  84%|████████▎ | 167/200 [12:43<02:52,  5.22s/it]Running Inference:  84%|████████▍ | 168/200 [12:46<02:32,  4.75s/it]Running Inference:  84%|████████▍ | 169/200 [12:48<01:59,  3.85s/it]Running Inference:  85%|████████▌ | 170/200 [12:50<01:37,  3.26s/it]Running Inference:  86%|████████▌ | 171/200 [12:53<01:32,  3.19s/it]Running Inference:  86%|████████▌ | 172/200 [12:56<01:30,  3.22s/it]Running Inference:  86%|████████▋ | 173/200 [12:58<01:18,  2.91s/it]Running Inference:  87%|████████▋ | 174/200 [13:05<01:48,  4.16s/it]Running Inference:  88%|████████▊ | 175/200 [13:09<01:35,  3.84s/it]Running Inference:  88%|████████▊ | 176/200 [13:15<01:53,  4.73s/it]Running Inference:  88%|████████▊ | 177/200 [13:22<02:02,  5.31s/it]Running Inference:  89%|████████▉ | 178/200 [13:30<02:13,  6.07s/it]Running Inference:  90%|████████▉ | 179/200 [13:33<01:46,  5.06s/it]Running Inference:  90%|█████████ | 180/200 [13:39<01:51,  5.59s/it]Running Inference:  90%|█████████ | 181/200 [13:42<01:28,  4.66s/it]Running Inference:  91%|█████████ | 182/200 [13:44<01:09,  3.84s/it]Running Inference:  92%|█████████▏| 183/200 [13:48<01:07,  3.95s/it]Running Inference:  92%|█████████▏| 184/200 [13:56<01:24,  5.25s/it]Running Inference:  92%|█████████▎| 185/200 [13:59<01:06,  4.46s/it]Running Inference:  93%|█████████▎| 186/200 [14:02<00:54,  3.91s/it]Running Inference:  94%|█████████▎| 187/200 [14:03<00:41,  3.18s/it]Running Inference:  94%|█████████▍| 188/200 [14:05<00:33,  2.81s/it]Running Inference:  94%|█████████▍| 189/200 [14:08<00:32,  2.95s/it]Running Inference:  95%|█████████▌| 190/200 [14:11<00:29,  2.95s/it]Running Inference:  96%|█████████▌| 191/200 [14:16<00:31,  3.48s/it]Running Inference:  96%|█████████▌| 192/200 [14:23<00:35,  4.44s/it]Running Inference:  96%|█████████▋| 193/200 [14:31<00:38,  5.54s/it]Running Inference:  97%|█████████▋| 194/200 [14:33<00:28,  4.70s/it]Running Inference:  98%|█████████▊| 195/200 [14:35<00:19,  3.83s/it]Running Inference:  98%|█████████▊| 196/200 [14:43<00:20,  5.08s/it]Running Inference:  98%|█████████▊| 197/200 [14:45<00:11,  3.97s/it]Running Inference:  99%|█████████▉| 198/200 [14:52<00:10,  5.08s/it]Running Inference: 100%|█████████▉| 199/200 [14:54<00:04,  4.12s/it]Running Inference: 100%|██████████| 200/200 [15:01<00:00,  5.02s/it]Running Inference: 100%|██████████| 200/200 [15:01<00:00,  4.51s/it]
2025-12-14 09:48:27,144 - INFO - Inference completed.
2025-12-14 09:48:27,155 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 09:48:27,155 - INFO - Calculating metrics for dataset: longbench
2025-12-14 09:48:27,277 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 09:48:27,277 - INFO - Metrics:
25.54
2025-12-14 09:48:27,278 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=samsum, ratio=0.1) on GPU 2

----------------------------------------
Task: samsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 09:48:33,668 - INFO - Set deterministic seeds to 42
2025-12-14 09:48:33,668 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 09:48:33,668 - INFO - Starting evaluation run...
2025-12-14 09:48:33,668 - INFO - Output directory set to: longbenchresult
2025-12-14 09:48:33,668 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 09:48:33,668 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 09:48:33,668 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 09:48:33,668 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.06it/s]
Device set to use cuda:0
2025-12-14 09:48:44,646 - INFO - Model pipeline loaded.
2025-12-14 09:48:44,646 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 09:48:47,628 - INFO - Dataset loaded with 200 entries.
2025-12-14 09:48:47,628 - INFO - Dataset processed with 200 entries.
2025-12-14 09:48:47,655 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:43,  3.54s/it]Running Inference:   1%|          | 2/200 [00:09<16:34,  5.02s/it]Running Inference:   2%|▏         | 3/200 [00:16<18:46,  5.72s/it]Running Inference:   2%|▏         | 4/200 [00:23<20:23,  6.24s/it]Running Inference:   2%|▎         | 5/200 [00:25<15:13,  4.68s/it]Running Inference:   3%|▎         | 6/200 [00:32<17:51,  5.52s/it]Running Inference:   4%|▎         | 7/200 [00:34<13:55,  4.33s/it]Running Inference:   4%|▍         | 8/200 [00:38<13:51,  4.33s/it]Running Inference:   4%|▍         | 9/200 [00:45<16:14,  5.10s/it]Running Inference:   5%|▌         | 10/200 [00:49<15:45,  4.98s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:57<18:24,  5.84s/it]Running Inference:   6%|▌         | 12/200 [01:02<16:46,  5.35s/it]Running Inference:   6%|▋         | 13/200 [01:04<13:37,  4.37s/it]Running Inference:   7%|▋         | 14/200 [01:08<13:39,  4.41s/it]Running Inference:   8%|▊         | 15/200 [01:11<12:04,  3.92s/it]Running Inference:   8%|▊         | 16/200 [01:18<14:43,  4.80s/it]Running Inference:   8%|▊         | 17/200 [01:26<17:35,  5.77s/it]Running Inference:   9%|▉         | 18/200 [01:33<18:40,  6.15s/it]Running Inference:  10%|▉         | 19/200 [01:41<20:06,  6.66s/it]Running Inference:  10%|█         | 20/200 [01:43<16:02,  5.34s/it]Running Inference:  10%|█         | 21/200 [01:45<13:21,  4.48s/it]Running Inference:  11%|█         | 22/200 [01:51<14:11,  4.78s/it]Running Inference:  12%|█▏        | 23/200 [01:58<16:26,  5.57s/it]Running Inference:  12%|█▏        | 24/200 [02:01<13:37,  4.64s/it]Running Inference:  12%|█▎        | 25/200 [02:07<15:15,  5.23s/it]Running Inference:  13%|█▎        | 26/200 [02:12<14:20,  4.95s/it]Running Inference:  14%|█▎        | 27/200 [02:17<14:37,  5.07s/it]Running Inference:  14%|█▍        | 28/200 [02:20<12:56,  4.51s/it]Running Inference:  14%|█▍        | 29/200 [02:22<10:16,  3.61s/it]Running Inference:  15%|█▌        | 30/200 [02:24<09:13,  3.26s/it]Running Inference:  16%|█▌        | 31/200 [02:26<08:10,  2.90s/it]Running Inference:  16%|█▌        | 32/200 [02:34<11:56,  4.27s/it]Running Inference:  16%|█▋        | 33/200 [02:40<13:44,  4.93s/it]Running Inference:  17%|█▋        | 34/200 [02:44<12:21,  4.46s/it]Running Inference:  18%|█▊        | 35/200 [02:47<11:03,  4.02s/it]Running Inference:  18%|█▊        | 36/200 [02:54<14:01,  5.13s/it]Running Inference:  18%|█▊        | 37/200 [02:57<12:09,  4.48s/it]Running Inference:  19%|█▉        | 38/200 [03:00<10:54,  4.04s/it]Running Inference:  20%|█▉        | 39/200 [03:08<14:14,  5.31s/it]Running Inference:  20%|██        | 40/200 [03:12<12:57,  4.86s/it]Running Inference:  20%|██        | 41/200 [03:15<11:05,  4.19s/it]Running Inference:  21%|██        | 42/200 [03:22<13:00,  4.94s/it]Running Inference:  22%|██▏       | 43/200 [03:24<10:42,  4.09s/it]Running Inference:  22%|██▏       | 44/200 [03:31<13:20,  5.13s/it]Running Inference:  22%|██▎       | 45/200 [03:39<15:21,  5.95s/it]Running Inference:  23%|██▎       | 46/200 [03:47<16:29,  6.43s/it]Running Inference:  24%|██▎       | 47/200 [03:54<16:54,  6.63s/it]Running Inference:  24%|██▍       | 48/200 [04:02<17:42,  6.99s/it]Running Inference:  24%|██▍       | 49/200 [04:09<18:05,  7.19s/it]Running Inference:  25%|██▌       | 50/200 [04:11<13:49,  5.53s/it]Running Inference:  26%|██▌       | 51/200 [04:17<14:25,  5.81s/it]Running Inference:  26%|██▌       | 52/200 [04:21<12:28,  5.06s/it]Running Inference:  26%|██▋       | 53/200 [04:23<10:40,  4.36s/it]Running Inference:  27%|██▋       | 54/200 [04:30<12:06,  4.97s/it]Running Inference:  28%|██▊       | 55/200 [04:33<10:55,  4.52s/it]Running Inference:  28%|██▊       | 56/200 [04:38<11:17,  4.71s/it]Running Inference:  28%|██▊       | 57/200 [04:41<10:02,  4.21s/it]Running Inference:  29%|██▉       | 58/200 [04:50<12:42,  5.37s/it]Running Inference:  30%|██▉       | 59/200 [04:52<10:42,  4.56s/it]Running Inference:  30%|███       | 60/200 [04:59<12:05,  5.18s/it]Running Inference:  30%|███       | 61/200 [05:06<13:13,  5.71s/it]Running Inference:  31%|███       | 62/200 [05:13<13:53,  6.04s/it]Running Inference:  32%|███▏      | 63/200 [05:19<14:17,  6.26s/it]Running Inference:  32%|███▏      | 64/200 [05:23<12:19,  5.44s/it]Running Inference:  32%|███▎      | 65/200 [05:29<12:42,  5.65s/it]Running Inference:  33%|███▎      | 66/200 [05:32<10:41,  4.79s/it]Running Inference:  34%|███▎      | 67/200 [05:33<07:57,  3.59s/it]Running Inference:  34%|███▍      | 68/200 [05:39<09:51,  4.48s/it]Running Inference:  34%|███▍      | 69/200 [05:41<08:20,  3.82s/it]Running Inference:  35%|███▌      | 70/200 [05:49<10:35,  4.89s/it]Running Inference:  36%|███▌      | 71/200 [05:51<08:52,  4.13s/it]Running Inference:  36%|███▌      | 72/200 [05:54<07:42,  3.61s/it]Running Inference:  36%|███▋      | 73/200 [05:56<06:45,  3.19s/it]Running Inference:  37%|███▋      | 74/200 [05:58<05:51,  2.79s/it]Running Inference:  38%|███▊      | 75/200 [05:59<04:49,  2.31s/it]Running Inference:  38%|███▊      | 76/200 [06:01<04:43,  2.29s/it]Running Inference:  38%|███▊      | 77/200 [06:04<05:03,  2.47s/it]Running Inference:  39%|███▉      | 78/200 [06:07<05:14,  2.58s/it]Running Inference:  40%|███▉      | 79/200 [06:09<05:12,  2.59s/it]Running Inference:  40%|████      | 80/200 [06:11<04:36,  2.30s/it]Running Inference:  40%|████      | 81/200 [06:11<03:26,  1.74s/it]Running Inference:  41%|████      | 82/200 [06:13<03:17,  1.67s/it]Running Inference:  42%|████▏     | 83/200 [06:21<07:02,  3.61s/it]Running Inference:  42%|████▏     | 84/200 [06:29<09:32,  4.94s/it]Running Inference:  42%|████▎     | 85/200 [06:36<10:47,  5.63s/it]Running Inference:  43%|████▎     | 86/200 [06:43<11:13,  5.91s/it]Running Inference:  44%|████▎     | 87/200 [06:46<09:44,  5.17s/it]Running Inference:  44%|████▍     | 88/200 [06:49<08:05,  4.34s/it]Running Inference:  44%|████▍     | 89/200 [06:56<09:34,  5.17s/it]Running Inference:  45%|████▌     | 90/200 [06:58<07:56,  4.33s/it]Running Inference:  46%|████▌     | 91/200 [07:06<09:36,  5.29s/it]Running Inference:  46%|████▌     | 92/200 [07:13<10:36,  5.90s/it]Running Inference:  46%|████▋     | 93/200 [07:17<09:14,  5.18s/it]Running Inference:  47%|████▋     | 94/200 [07:19<07:25,  4.21s/it]Running Inference:  48%|████▊     | 95/200 [07:21<06:17,  3.60s/it]Running Inference:  48%|████▊     | 96/200 [07:24<05:48,  3.35s/it]Running Inference:  48%|████▊     | 97/200 [07:30<07:24,  4.32s/it]Running Inference:  49%|████▉     | 98/200 [07:38<09:16,  5.45s/it]Running Inference:  50%|████▉     | 99/200 [07:40<07:11,  4.27s/it]Running Inference:  50%|█████     | 100/200 [07:47<08:40,  5.20s/it]Running Inference:  50%|█████     | 101/200 [07:49<06:55,  4.19s/it]Running Inference:  51%|█████     | 102/200 [07:53<06:41,  4.10s/it]Running Inference:  52%|█████▏    | 103/200 [08:00<07:56,  4.92s/it]Running Inference:  52%|█████▏    | 104/200 [08:03<06:56,  4.34s/it]Running Inference:  52%|█████▎    | 105/200 [08:05<06:00,  3.80s/it]Running Inference:  53%|█████▎    | 106/200 [08:13<07:49,  5.00s/it]Running Inference:  54%|█████▎    | 107/200 [08:20<08:35,  5.54s/it]Running Inference:  54%|█████▍    | 108/200 [08:27<09:19,  6.08s/it]Running Inference:  55%|█████▍    | 109/200 [08:29<07:08,  4.71s/it]Running Inference:  55%|█████▌    | 110/200 [08:33<07:02,  4.69s/it]Running Inference:  56%|█████▌    | 111/200 [08:36<06:12,  4.19s/it]Running Inference:  56%|█████▌    | 112/200 [08:43<07:15,  4.95s/it]Running Inference:  56%|█████▋    | 113/200 [08:45<05:48,  4.00s/it]Running Inference:  57%|█████▋    | 114/200 [08:51<06:50,  4.77s/it]Running Inference:  57%|█████▊    | 115/200 [08:59<08:04,  5.69s/it]Running Inference:  58%|█████▊    | 116/200 [09:02<06:48,  4.86s/it]Running Inference:  58%|█████▊    | 117/200 [09:04<05:18,  3.83s/it]Running Inference:  59%|█████▉    | 118/200 [09:11<06:39,  4.87s/it]Running Inference:  60%|█████▉    | 119/200 [09:13<05:35,  4.14s/it]Running Inference:  60%|██████    | 120/200 [09:21<06:56,  5.21s/it]Running Inference:  60%|██████    | 121/200 [09:24<05:53,  4.47s/it]Running Inference:  61%|██████    | 122/200 [09:27<05:21,  4.13s/it]Running Inference:  62%|██████▏   | 123/200 [09:34<06:17,  4.91s/it]Running Inference:  62%|██████▏   | 124/200 [09:40<06:48,  5.37s/it]Running Inference:  62%|██████▎   | 125/200 [09:45<06:38,  5.32s/it]Running Inference:  63%|██████▎   | 126/200 [09:48<05:42,  4.63s/it]Running Inference:  64%|██████▎   | 127/200 [09:51<04:57,  4.08s/it]Running Inference:  64%|██████▍   | 128/200 [09:56<05:04,  4.22s/it]Running Inference:  64%|██████▍   | 129/200 [09:58<04:26,  3.76s/it]Running Inference:  65%|██████▌   | 130/200 [10:06<05:42,  4.89s/it]Running Inference:  66%|██████▌   | 131/200 [10:11<05:40,  4.93s/it]Running Inference:  66%|██████▌   | 132/200 [10:14<04:49,  4.26s/it]Running Inference:  66%|██████▋   | 133/200 [10:18<04:42,  4.22s/it]Running Inference:  67%|██████▋   | 134/200 [10:21<04:12,  3.83s/it]Running Inference:  68%|██████▊   | 135/200 [10:28<05:06,  4.71s/it]Running Inference:  68%|██████▊   | 136/200 [10:35<05:46,  5.41s/it]Running Inference:  68%|██████▊   | 137/200 [10:37<04:46,  4.55s/it]Running Inference:  69%|██████▉   | 138/200 [10:44<05:32,  5.36s/it]Running Inference:  70%|██████▉   | 139/200 [10:51<05:48,  5.71s/it]Running Inference:  70%|███████   | 140/200 [10:56<05:32,  5.54s/it]Running Inference:  70%|███████   | 141/200 [11:03<05:54,  6.01s/it]Running Inference:  71%|███████   | 142/200 [11:10<06:03,  6.27s/it]Running Inference:  72%|███████▏  | 143/200 [11:12<04:47,  5.04s/it]Running Inference:  72%|███████▏  | 144/200 [11:19<05:20,  5.72s/it]Running Inference:  72%|███████▎  | 145/200 [11:22<04:25,  4.83s/it]Running Inference:  73%|███████▎  | 146/200 [11:25<03:51,  4.28s/it]Running Inference:  74%|███████▎  | 147/200 [11:33<04:43,  5.35s/it]Running Inference:  74%|███████▍  | 148/200 [11:35<03:39,  4.22s/it]Running Inference:  74%|███████▍  | 149/200 [11:40<03:56,  4.64s/it]Running Inference:  75%|███████▌  | 150/200 [11:42<03:15,  3.91s/it]Running Inference:  76%|███████▌  | 151/200 [11:49<03:54,  4.78s/it]Running Inference:  76%|███████▌  | 152/200 [11:51<02:58,  3.72s/it]Running Inference:  76%|███████▋  | 153/200 [11:54<02:47,  3.56s/it]Running Inference:  77%|███████▋  | 154/200 [11:55<02:17,  2.99s/it]Running Inference:  78%|███████▊  | 155/200 [11:57<01:55,  2.57s/it]Running Inference:  78%|███████▊  | 156/200 [12:00<01:52,  2.57s/it]Running Inference:  78%|███████▊  | 157/200 [12:01<01:35,  2.22s/it]Running Inference:  79%|███████▉  | 158/200 [12:04<01:38,  2.35s/it]Running Inference:  80%|███████▉  | 159/200 [12:08<02:06,  3.08s/it]Running Inference:  80%|████████  | 160/200 [12:15<02:46,  4.16s/it]Running Inference:  80%|████████  | 161/200 [12:22<03:11,  4.92s/it]Running Inference:  81%|████████  | 162/200 [12:29<03:36,  5.70s/it]Running Inference:  82%|████████▏ | 163/200 [12:37<03:58,  6.45s/it]Running Inference:  82%|████████▏ | 164/200 [12:43<03:41,  6.15s/it]Running Inference:  82%|████████▎ | 165/200 [12:50<03:40,  6.30s/it]Running Inference:  83%|████████▎ | 166/200 [12:56<03:38,  6.44s/it]Running Inference:  84%|████████▎ | 167/200 [12:59<02:50,  5.18s/it]Running Inference:  84%|████████▍ | 168/200 [13:02<02:30,  4.71s/it]Running Inference:  84%|████████▍ | 169/200 [13:05<02:10,  4.22s/it]Running Inference:  85%|████████▌ | 170/200 [13:07<01:43,  3.46s/it]Running Inference:  86%|████████▌ | 171/200 [13:10<01:36,  3.33s/it]Running Inference:  86%|████████▌ | 172/200 [13:12<01:26,  3.08s/it]Running Inference:  86%|████████▋ | 173/200 [13:16<01:23,  3.09s/it]Running Inference:  87%|████████▋ | 174/200 [13:23<01:50,  4.25s/it]Running Inference:  88%|████████▊ | 175/200 [13:25<01:35,  3.81s/it]Running Inference:  88%|████████▊ | 176/200 [13:32<01:52,  4.67s/it]Running Inference:  88%|████████▊ | 177/200 [13:39<02:00,  5.22s/it]Running Inference:  89%|████████▉ | 178/200 [13:46<02:11,  5.96s/it]Running Inference:  90%|████████▉ | 179/200 [13:50<01:52,  5.34s/it]Running Inference:  90%|█████████ | 180/200 [13:57<01:54,  5.74s/it]Running Inference:  90%|█████████ | 181/200 [13:59<01:30,  4.76s/it]Running Inference:  91%|█████████ | 182/200 [14:06<01:36,  5.35s/it]Running Inference:  92%|█████████▏| 183/200 [14:10<01:26,  5.09s/it]Running Inference:  92%|█████████▏| 184/200 [14:19<01:35,  6.00s/it]Running Inference:  92%|█████████▎| 185/200 [14:21<01:13,  4.92s/it]Running Inference:  93%|█████████▎| 186/200 [14:23<00:55,  3.95s/it]Running Inference:  94%|█████████▎| 187/200 [14:24<00:41,  3.19s/it]Running Inference:  94%|█████████▍| 188/200 [14:27<00:35,  2.99s/it]Running Inference:  94%|█████████▍| 189/200 [14:30<00:32,  2.96s/it]Running Inference:  95%|█████████▌| 190/200 [14:33<00:31,  3.13s/it]Running Inference:  96%|█████████▌| 191/200 [14:37<00:31,  3.48s/it]Running Inference:  96%|█████████▌| 192/200 [14:44<00:35,  4.40s/it]Running Inference:  96%|█████████▋| 193/200 [14:52<00:38,  5.46s/it]Running Inference:  97%|█████████▋| 194/200 [14:58<00:34,  5.74s/it]Running Inference:  98%|█████████▊| 195/200 [15:00<00:22,  4.55s/it]Running Inference:  98%|█████████▊| 196/200 [15:08<00:22,  5.54s/it]Running Inference:  98%|█████████▊| 197/200 [15:09<00:12,  4.29s/it]Running Inference:  99%|█████████▉| 198/200 [15:17<00:10,  5.25s/it]Running Inference: 100%|█████████▉| 199/200 [15:19<00:04,  4.24s/it]Running Inference: 100%|██████████| 200/200 [15:26<00:00,  5.06s/it]Running Inference: 100%|██████████| 200/200 [15:26<00:00,  4.63s/it]
2025-12-14 10:04:13,703 - INFO - Inference completed.
2025-12-14 10:04:13,714 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 10:04:13,714 - INFO - Calculating metrics for dataset: longbench
2025-12-14 10:04:13,836 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 10:04:13,836 - INFO - Metrics:
24.55
2025-12-14 10:04:13,837 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=samsum, ratio=0.2) on GPU 2

----------------------------------------
Task: samsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 10:04:20,198 - INFO - Set deterministic seeds to 42
2025-12-14 10:04:20,198 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 10:04:20,198 - INFO - Starting evaluation run...
2025-12-14 10:04:20,199 - INFO - Output directory set to: longbenchresult
2025-12-14 10:04:20,199 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 10:04:20,199 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 10:04:20,199 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 10:04:20,199 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.96it/s]
Device set to use cuda:0
2025-12-14 10:04:31,526 - INFO - Model pipeline loaded.
2025-12-14 10:04:31,527 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 10:04:34,838 - INFO - Dataset loaded with 200 entries.
2025-12-14 10:04:34,838 - INFO - Dataset processed with 200 entries.
2025-12-14 10:04:34,866 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<12:38,  3.81s/it]Running Inference:   1%|          | 2/200 [00:09<17:07,  5.19s/it]Running Inference:   2%|▏         | 3/200 [00:16<19:01,  5.80s/it]Running Inference:   2%|▏         | 4/200 [00:23<20:34,  6.30s/it]Running Inference:   2%|▎         | 5/200 [00:30<20:45,  6.39s/it]Running Inference:   3%|▎         | 6/200 [00:34<18:32,  5.73s/it]Running Inference:   4%|▎         | 7/200 [00:36<14:23,  4.47s/it]Running Inference:   4%|▍         | 8/200 [00:40<14:04,  4.40s/it]Running Inference:   4%|▍         | 9/200 [00:47<16:21,  5.14s/it]Running Inference:   5%|▌         | 10/200 [00:52<15:46,  4.98s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:55<13:48,  4.38s/it]Running Inference:   6%|▌         | 12/200 [01:00<14:14,  4.55s/it]Running Inference:   6%|▋         | 13/200 [01:06<16:04,  5.16s/it]Running Inference:   7%|▋         | 14/200 [01:11<15:19,  4.95s/it]Running Inference:   8%|▊         | 15/200 [01:13<13:13,  4.29s/it]Running Inference:   8%|▊         | 16/200 [01:20<15:27,  5.04s/it]Running Inference:   8%|▊         | 17/200 [01:28<18:03,  5.92s/it]Running Inference:   9%|▉         | 18/200 [01:31<15:30,  5.11s/it]Running Inference:  10%|▉         | 19/200 [01:39<17:52,  5.92s/it]Running Inference:  10%|█         | 20/200 [01:41<14:27,  4.82s/it]Running Inference:  10%|█         | 21/200 [01:44<12:15,  4.11s/it]Running Inference:  11%|█         | 22/200 [01:49<13:24,  4.52s/it]Running Inference:  12%|█▏        | 23/200 [01:53<12:47,  4.34s/it]Running Inference:  12%|█▏        | 24/200 [01:56<11:03,  3.77s/it]Running Inference:  12%|█▎        | 25/200 [02:02<13:26,  4.61s/it]Running Inference:  13%|█▎        | 26/200 [02:09<15:34,  5.37s/it]Running Inference:  14%|█▎        | 27/200 [02:18<17:56,  6.22s/it]Running Inference:  14%|█▍        | 28/200 [02:21<15:14,  5.32s/it]Running Inference:  14%|█▍        | 29/200 [02:22<11:51,  4.16s/it]Running Inference:  15%|█▌        | 30/200 [02:25<10:19,  3.64s/it]Running Inference:  16%|█▌        | 31/200 [02:27<09:08,  3.25s/it]Running Inference:  16%|█▌        | 32/200 [02:34<12:34,  4.49s/it]Running Inference:  16%|█▋        | 33/200 [02:41<14:08,  5.08s/it]Running Inference:  17%|█▋        | 34/200 [02:44<12:41,  4.59s/it]Running Inference:  18%|█▊        | 35/200 [02:48<11:33,  4.20s/it]Running Inference:  18%|█▊        | 36/200 [02:52<11:40,  4.27s/it]Running Inference:  18%|█▊        | 37/200 [02:55<10:30,  3.87s/it]Running Inference:  19%|█▉        | 38/200 [02:58<09:44,  3.61s/it]Running Inference:  20%|█▉        | 39/200 [03:06<12:59,  4.84s/it]Running Inference:  20%|██        | 40/200 [03:14<15:20,  5.75s/it]Running Inference:  20%|██        | 41/200 [03:16<12:52,  4.86s/it]Running Inference:  21%|██        | 42/200 [03:23<14:15,  5.41s/it]Running Inference:  22%|██▏       | 43/200 [03:30<15:04,  5.76s/it]Running Inference:  22%|██▏       | 44/200 [03:37<16:21,  6.29s/it]Running Inference:  22%|██▎       | 45/200 [03:45<17:28,  6.76s/it]Running Inference:  23%|██▎       | 46/200 [03:53<17:58,  7.01s/it]Running Inference:  24%|██▎       | 47/200 [04:00<17:57,  7.04s/it]Running Inference:  24%|██▍       | 48/200 [04:08<18:26,  7.28s/it]Running Inference:  24%|██▍       | 49/200 [04:15<18:36,  7.39s/it]Running Inference:  25%|██▌       | 50/200 [04:17<14:15,  5.70s/it]Running Inference:  26%|██▌       | 51/200 [04:23<14:43,  5.93s/it]Running Inference:  26%|██▌       | 52/200 [04:27<12:45,  5.17s/it]Running Inference:  26%|██▋       | 53/200 [04:30<10:52,  4.44s/it]Running Inference:  27%|██▋       | 54/200 [04:36<12:15,  5.04s/it]Running Inference:  28%|██▊       | 55/200 [04:39<11:01,  4.56s/it]Running Inference:  28%|██▊       | 56/200 [04:45<11:22,  4.74s/it]Running Inference:  28%|██▊       | 57/200 [04:48<10:11,  4.27s/it]Running Inference:  29%|██▉       | 58/200 [04:56<12:48,  5.42s/it]Running Inference:  30%|██▉       | 59/200 [05:02<13:31,  5.76s/it]Running Inference:  30%|███       | 60/200 [05:09<14:03,  6.03s/it]Running Inference:  30%|███       | 61/200 [05:13<12:10,  5.26s/it]Running Inference:  31%|███       | 62/200 [05:19<12:50,  5.58s/it]Running Inference:  32%|███▏      | 63/200 [05:23<12:03,  5.28s/it]Running Inference:  32%|███▏      | 64/200 [05:27<10:42,  4.72s/it]Running Inference:  32%|███▎      | 65/200 [05:33<11:27,  5.09s/it]Running Inference:  33%|███▎      | 66/200 [05:36<09:49,  4.40s/it]Running Inference:  34%|███▎      | 67/200 [05:40<09:40,  4.37s/it]Running Inference:  34%|███▍      | 68/200 [05:46<11:03,  5.03s/it]Running Inference:  34%|███▍      | 69/200 [05:49<09:10,  4.20s/it]Running Inference:  35%|███▌      | 70/200 [05:56<11:10,  5.16s/it]Running Inference:  36%|███▌      | 71/200 [05:59<09:32,  4.44s/it]Running Inference:  36%|███▌      | 72/200 [06:01<08:09,  3.83s/it]Running Inference:  36%|███▋      | 73/200 [06:08<09:49,  4.65s/it]Running Inference:  37%|███▋      | 74/200 [06:10<08:27,  4.03s/it]Running Inference:  38%|███▊      | 75/200 [06:12<06:37,  3.18s/it]Running Inference:  38%|███▊      | 76/200 [06:14<05:58,  2.89s/it]Running Inference:  38%|███▊      | 77/200 [06:17<05:55,  2.89s/it]Running Inference:  39%|███▉      | 78/200 [06:20<05:50,  2.88s/it]Running Inference:  40%|███▉      | 79/200 [06:22<05:38,  2.80s/it]Running Inference:  40%|████      | 80/200 [06:24<04:49,  2.42s/it]Running Inference:  40%|████      | 81/200 [06:25<04:10,  2.11s/it]Running Inference:  41%|████      | 82/200 [06:27<03:55,  2.00s/it]Running Inference:  42%|████▏     | 83/200 [06:35<07:29,  3.84s/it]Running Inference:  42%|████▏     | 84/200 [06:43<09:51,  5.10s/it]Running Inference:  42%|████▎     | 85/200 [06:50<11:00,  5.74s/it]Running Inference:  43%|████▎     | 86/200 [06:57<11:22,  5.99s/it]Running Inference:  44%|████▎     | 87/200 [07:04<12:09,  6.46s/it]Running Inference:  44%|████▍     | 88/200 [07:07<10:08,  5.43s/it]Running Inference:  44%|████▍     | 89/200 [07:11<09:11,  4.97s/it]Running Inference:  45%|████▌     | 90/200 [07:14<07:40,  4.19s/it]Running Inference:  46%|████▌     | 91/200 [07:20<08:39,  4.77s/it]Running Inference:  46%|████▌     | 92/200 [07:27<09:57,  5.53s/it]Running Inference:  46%|████▋     | 93/200 [07:31<08:47,  4.93s/it]Running Inference:  47%|████▋     | 94/200 [07:33<07:12,  4.08s/it]Running Inference:  48%|████▊     | 95/200 [07:35<06:08,  3.51s/it]Running Inference:  48%|████▊     | 96/200 [07:42<08:07,  4.69s/it]Running Inference:  48%|████▊     | 97/200 [07:44<06:14,  3.64s/it]Running Inference:  49%|████▉     | 98/200 [07:47<06:14,  3.67s/it]Running Inference:  50%|████▉     | 99/200 [07:49<05:27,  3.24s/it]Running Inference:  50%|█████     | 100/200 [07:53<05:43,  3.43s/it]Running Inference:  50%|█████     | 101/200 [07:55<04:52,  2.96s/it]Running Inference:  51%|█████     | 102/200 [07:58<04:50,  2.97s/it]Running Inference:  52%|█████▏    | 103/200 [08:05<06:40,  4.13s/it]Running Inference:  52%|█████▏    | 104/200 [08:09<06:18,  3.94s/it]Running Inference:  52%|█████▎    | 105/200 [08:11<05:30,  3.48s/it]Running Inference:  53%|█████▎    | 106/200 [08:19<07:28,  4.78s/it]Running Inference:  54%|█████▎    | 107/200 [08:26<08:21,  5.39s/it]Running Inference:  54%|█████▍    | 108/200 [08:30<08:01,  5.24s/it]Running Inference:  55%|█████▍    | 109/200 [08:32<06:22,  4.21s/it]Running Inference:  55%|█████▌    | 110/200 [08:38<06:53,  4.59s/it]Running Inference:  56%|█████▌    | 111/200 [08:41<06:06,  4.12s/it]Running Inference:  56%|█████▌    | 112/200 [08:48<07:11,  4.90s/it]Running Inference:  56%|█████▋    | 113/200 [08:49<05:45,  3.97s/it]Running Inference:  57%|█████▋    | 114/200 [08:56<06:48,  4.75s/it]Running Inference:  57%|█████▊    | 115/200 [09:04<08:03,  5.69s/it]Running Inference:  58%|█████▊    | 116/200 [09:07<06:47,  4.86s/it]Running Inference:  58%|█████▊    | 117/200 [09:08<05:17,  3.83s/it]Running Inference:  59%|█████▉    | 118/200 [09:14<06:03,  4.44s/it]Running Inference:  60%|█████▉    | 119/200 [09:16<05:10,  3.83s/it]Running Inference:  60%|██████    | 120/200 [09:24<06:40,  5.00s/it]Running Inference:  60%|██████    | 121/200 [09:27<05:41,  4.33s/it]Running Inference:  61%|██████    | 122/200 [09:34<06:50,  5.27s/it]Running Inference:  62%|██████▏   | 123/200 [09:41<07:19,  5.71s/it]Running Inference:  62%|██████▏   | 124/200 [09:48<07:31,  5.94s/it]Running Inference:  62%|██████▎   | 125/200 [09:51<06:38,  5.31s/it]Running Inference:  63%|██████▎   | 126/200 [09:54<05:43,  4.65s/it]Running Inference:  64%|██████▎   | 127/200 [09:57<04:58,  4.09s/it]Running Inference:  64%|██████▍   | 128/200 [10:02<05:05,  4.24s/it]Running Inference:  64%|██████▍   | 129/200 [10:05<04:37,  3.90s/it]Running Inference:  65%|██████▌   | 130/200 [10:13<05:49,  4.99s/it]Running Inference:  66%|██████▌   | 131/200 [10:21<06:48,  5.92s/it]Running Inference:  66%|██████▌   | 132/200 [10:23<05:36,  4.95s/it]Running Inference:  66%|██████▋   | 133/200 [10:28<05:20,  4.79s/it]Running Inference:  67%|██████▋   | 134/200 [10:31<04:42,  4.28s/it]Running Inference:  68%|██████▊   | 135/200 [10:38<05:27,  5.04s/it]Running Inference:  68%|██████▊   | 136/200 [10:45<06:01,  5.65s/it]Running Inference:  68%|██████▊   | 137/200 [10:47<04:54,  4.67s/it]Running Inference:  69%|██████▉   | 138/200 [10:53<05:08,  4.97s/it]Running Inference:  70%|██████▉   | 139/200 [10:55<04:19,  4.26s/it]Running Inference:  70%|███████   | 140/200 [11:00<04:30,  4.52s/it]Running Inference:  70%|███████   | 141/200 [11:08<05:12,  5.29s/it]Running Inference:  71%|███████   | 142/200 [11:14<05:34,  5.77s/it]Running Inference:  72%|███████▏  | 143/200 [11:17<04:27,  4.69s/it]Running Inference:  72%|███████▏  | 144/200 [11:21<04:16,  4.57s/it]Running Inference:  72%|███████▎  | 145/200 [11:24<03:41,  4.03s/it]Running Inference:  73%|███████▎  | 146/200 [11:27<03:20,  3.71s/it]Running Inference:  74%|███████▎  | 147/200 [11:35<04:22,  4.96s/it]Running Inference:  74%|███████▍  | 148/200 [11:37<03:33,  4.11s/it]Running Inference:  74%|███████▍  | 149/200 [11:42<03:52,  4.56s/it]Running Inference:  75%|███████▌  | 150/200 [11:49<04:26,  5.32s/it]Running Inference:  76%|███████▌  | 151/200 [11:56<04:43,  5.78s/it]Running Inference:  76%|███████▌  | 152/200 [11:57<03:32,  4.42s/it]Running Inference:  76%|███████▋  | 153/200 [12:01<03:14,  4.13s/it]Running Inference:  77%|███████▋  | 154/200 [12:03<02:36,  3.40s/it]Running Inference:  78%|███████▊  | 155/200 [12:04<02:08,  2.85s/it]Running Inference:  78%|███████▊  | 156/200 [12:07<02:01,  2.76s/it]Running Inference:  78%|███████▊  | 157/200 [12:08<01:38,  2.29s/it]Running Inference:  79%|███████▉  | 158/200 [12:11<01:41,  2.41s/it]Running Inference:  80%|███████▉  | 159/200 [12:19<02:46,  4.06s/it]Running Inference:  80%|████████  | 160/200 [12:25<03:13,  4.85s/it]Running Inference:  80%|████████  | 161/200 [12:32<03:30,  5.41s/it]Running Inference:  81%|████████  | 162/200 [12:39<03:49,  6.04s/it]Running Inference:  82%|████████▏ | 163/200 [12:48<04:07,  6.69s/it]Running Inference:  82%|████████▏ | 164/200 [12:53<03:41,  6.15s/it]Running Inference:  82%|████████▎ | 165/200 [12:59<03:40,  6.30s/it]Running Inference:  83%|████████▎ | 166/200 [13:06<03:39,  6.44s/it]Running Inference:  84%|████████▎ | 167/200 [13:08<02:49,  5.14s/it]Running Inference:  84%|████████▍ | 168/200 [13:16<03:09,  5.91s/it]Running Inference:  84%|████████▍ | 169/200 [13:19<02:36,  5.06s/it]Running Inference:  85%|████████▌ | 170/200 [13:21<02:01,  4.05s/it]Running Inference:  86%|████████▌ | 171/200 [13:24<01:48,  3.74s/it]Running Inference:  86%|████████▌ | 172/200 [13:26<01:34,  3.37s/it]Running Inference:  86%|████████▋ | 173/200 [13:28<01:21,  3.00s/it]Running Inference:  87%|████████▋ | 174/200 [13:35<01:49,  4.20s/it]Running Inference:  88%|████████▊ | 175/200 [13:38<01:36,  3.86s/it]Running Inference:  88%|████████▊ | 176/200 [13:45<01:53,  4.71s/it]Running Inference:  88%|████████▊ | 177/200 [13:52<02:01,  5.28s/it]Running Inference:  89%|████████▉ | 178/200 [13:59<02:12,  6.00s/it]Running Inference:  90%|████████▉ | 179/200 [14:03<01:51,  5.33s/it]Running Inference:  90%|█████████ | 180/200 [14:10<01:54,  5.74s/it]Running Inference:  90%|█████████ | 181/200 [14:13<01:32,  4.89s/it]Running Inference:  91%|█████████ | 182/200 [14:15<01:13,  4.09s/it]Running Inference:  92%|█████████▏| 183/200 [14:20<01:13,  4.30s/it]Running Inference:  92%|█████████▏| 184/200 [14:28<01:27,  5.45s/it]Running Inference:  92%|█████████▎| 185/200 [14:30<01:08,  4.53s/it]Running Inference:  93%|█████████▎| 186/200 [14:33<00:55,  3.95s/it]Running Inference:  94%|█████████▎| 187/200 [14:34<00:40,  3.15s/it]Running Inference:  94%|█████████▍| 188/200 [14:36<00:34,  2.91s/it]Running Inference:  94%|█████████▍| 189/200 [14:39<00:31,  2.91s/it]Running Inference:  95%|█████████▌| 190/200 [14:43<00:31,  3.16s/it]Running Inference:  96%|█████████▌| 191/200 [14:49<00:36,  4.02s/it]Running Inference:  96%|█████████▌| 192/200 [14:56<00:38,  4.78s/it]Running Inference:  96%|█████████▋| 193/200 [15:04<00:40,  5.74s/it]Running Inference:  97%|█████████▋| 194/200 [15:08<00:31,  5.31s/it]Running Inference:  98%|█████████▊| 195/200 [15:10<00:21,  4.32s/it]Running Inference:  98%|█████████▊| 196/200 [15:18<00:21,  5.38s/it]Running Inference:  98%|█████████▊| 197/200 [15:19<00:12,  4.18s/it]Running Inference:  99%|█████████▉| 198/200 [15:27<00:10,  5.18s/it]Running Inference: 100%|█████████▉| 199/200 [15:30<00:04,  4.50s/it]Running Inference: 100%|██████████| 200/200 [15:37<00:00,  5.24s/it]Running Inference: 100%|██████████| 200/200 [15:37<00:00,  4.69s/it]
2025-12-14 10:20:11,957 - INFO - Inference completed.
2025-12-14 10:20:11,968 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 10:20:11,968 - INFO - Calculating metrics for dataset: longbench
2025-12-14 10:20:12,094 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 10:20:12,094 - INFO - Metrics:
23.72
2025-12-14 10:20:12,096 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=samsum, ratio=0.3) on GPU 2

----------------------------------------
Task: samsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 10:20:18,558 - INFO - Set deterministic seeds to 42
2025-12-14 10:20:18,558 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "samsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 10:20:18,558 - INFO - Starting evaluation run...
2025-12-14 10:20:18,558 - INFO - Output directory set to: longbenchresult
2025-12-14 10:20:18,558 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 10:20:18,558 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 10:20:18,558 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 10:20:18,558 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.46it/s]
Device set to use cuda:0
2025-12-14 10:20:30,532 - INFO - Model pipeline loaded.
2025-12-14 10:20:30,533 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: samsum)
2025-12-14 10:20:35,347 - INFO - Dataset loaded with 200 entries.
2025-12-14 10:20:35,347 - INFO - Dataset processed with 200 entries.
2025-12-14 10:20:35,372 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<14:00,  4.22s/it]Running Inference:   1%|          | 2/200 [00:10<18:00,  5.46s/it]Running Inference:   2%|▏         | 3/200 [00:17<19:25,  5.92s/it]Running Inference:   2%|▏         | 4/200 [00:24<20:43,  6.34s/it]Running Inference:   2%|▎         | 5/200 [00:25<15:06,  4.65s/it]Running Inference:   3%|▎         | 6/200 [00:30<15:01,  4.65s/it]Running Inference:   4%|▎         | 7/200 [00:32<12:00,  3.74s/it]Running Inference:   4%|▍         | 8/200 [00:38<14:09,  4.42s/it]Running Inference:   4%|▍         | 9/200 [00:44<16:23,  5.15s/it]Running Inference:   5%|▌         | 10/200 [00:50<16:46,  5.30s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:53<14:54,  4.73s/it]Running Inference:   6%|▌         | 12/200 [00:57<14:09,  4.52s/it]Running Inference:   6%|▋         | 13/200 [01:04<15:59,  5.13s/it]Running Inference:   7%|▋         | 14/200 [01:09<15:25,  4.98s/it]Running Inference:   8%|▊         | 15/200 [01:12<14:10,  4.60s/it]Running Inference:   8%|▊         | 16/200 [01:19<16:06,  5.25s/it]Running Inference:   8%|▊         | 17/200 [01:27<18:29,  6.07s/it]Running Inference:   9%|▉         | 18/200 [01:30<15:49,  5.22s/it]Running Inference:  10%|▉         | 19/200 [01:38<18:04,  5.99s/it]Running Inference:  10%|█         | 20/200 [01:40<14:36,  4.87s/it]Running Inference:  10%|█         | 21/200 [01:43<12:50,  4.31s/it]Running Inference:  11%|█         | 22/200 [01:50<14:30,  4.89s/it]Running Inference:  12%|█▏        | 23/200 [01:57<16:36,  5.63s/it]Running Inference:  12%|█▏        | 24/200 [01:59<13:38,  4.65s/it]Running Inference:  12%|█▎        | 25/200 [02:02<11:47,  4.05s/it]Running Inference:  13%|█▎        | 26/200 [02:09<14:26,  4.98s/it]Running Inference:  14%|█▎        | 27/200 [02:16<15:48,  5.48s/it]Running Inference:  14%|█▍        | 28/200 [02:19<13:48,  4.82s/it]Running Inference:  14%|█▍        | 29/200 [02:21<11:18,  3.97s/it]Running Inference:  15%|█▌        | 30/200 [02:23<09:56,  3.51s/it]Running Inference:  16%|█▌        | 31/200 [02:26<08:52,  3.15s/it]Running Inference:  16%|█▌        | 32/200 [02:31<10:24,  3.72s/it]Running Inference:  16%|█▋        | 33/200 [02:32<07:59,  2.87s/it]Running Inference:  17%|█▋        | 34/200 [02:39<12:01,  4.35s/it]Running Inference:  18%|█▊        | 35/200 [02:43<11:15,  4.09s/it]Running Inference:  18%|█▊        | 36/200 [02:51<14:06,  5.16s/it]Running Inference:  18%|█▊        | 37/200 [02:54<12:18,  4.53s/it]Running Inference:  19%|█▉        | 38/200 [02:57<10:59,  4.07s/it]Running Inference:  20%|█▉        | 39/200 [03:04<13:46,  5.13s/it]Running Inference:  20%|██        | 40/200 [03:08<12:28,  4.68s/it]Running Inference:  20%|██        | 41/200 [03:11<10:56,  4.13s/it]Running Inference:  21%|██        | 42/200 [03:17<12:54,  4.90s/it]Running Inference:  22%|██▏       | 43/200 [03:24<14:07,  5.40s/it]Running Inference:  22%|██▏       | 44/200 [03:32<15:41,  6.03s/it]Running Inference:  22%|██▎       | 45/200 [03:39<16:59,  6.58s/it]Running Inference:  23%|██▎       | 46/200 [03:47<17:37,  6.87s/it]Running Inference:  24%|██▎       | 47/200 [03:51<15:10,  5.95s/it]Running Inference:  24%|██▍       | 48/200 [03:59<16:30,  6.52s/it]Running Inference:  24%|██▍       | 49/200 [04:06<17:15,  6.86s/it]Running Inference:  25%|██▌       | 50/200 [04:08<13:18,  5.32s/it]Running Inference:  26%|██▌       | 51/200 [04:14<14:04,  5.67s/it]Running Inference:  26%|██▌       | 52/200 [04:18<12:24,  5.03s/it]Running Inference:  26%|██▋       | 53/200 [04:25<14:07,  5.77s/it]Running Inference:  27%|██▋       | 54/200 [04:27<11:03,  4.55s/it]Running Inference:  28%|██▊       | 55/200 [04:31<10:11,  4.22s/it]Running Inference:  28%|██▊       | 56/200 [04:36<10:43,  4.47s/it]Running Inference:  28%|██▊       | 57/200 [04:40<10:52,  4.56s/it]Running Inference:  29%|██▉       | 58/200 [04:49<13:17,  5.61s/it]Running Inference:  30%|██▉       | 59/200 [04:53<12:09,  5.17s/it]Running Inference:  30%|███       | 60/200 [04:59<13:05,  5.61s/it]Running Inference:  30%|███       | 61/200 [05:06<13:55,  6.01s/it]Running Inference:  31%|███       | 62/200 [05:11<13:16,  5.77s/it]Running Inference:  32%|███▏      | 63/200 [05:16<12:32,  5.50s/it]Running Inference:  32%|███▏      | 64/200 [05:20<11:02,  4.87s/it]Running Inference:  32%|███▎      | 65/200 [05:25<11:23,  5.06s/it]Running Inference:  33%|███▎      | 66/200 [05:28<09:47,  4.39s/it]Running Inference:  34%|███▎      | 67/200 [05:35<11:13,  5.07s/it]Running Inference:  34%|███▍      | 68/200 [05:41<12:07,  5.51s/it]Running Inference:  34%|███▍      | 69/200 [05:45<11:01,  5.05s/it]Running Inference:  35%|███▌      | 70/200 [05:53<12:26,  5.75s/it]Running Inference:  36%|███▌      | 71/200 [05:55<10:29,  4.88s/it]Running Inference:  36%|███▌      | 72/200 [05:58<09:01,  4.23s/it]Running Inference:  36%|███▋      | 73/200 [06:05<10:24,  4.92s/it]Running Inference:  37%|███▋      | 74/200 [06:08<09:19,  4.44s/it]Running Inference:  38%|███▊      | 75/200 [06:09<07:13,  3.47s/it]Running Inference:  38%|███▊      | 76/200 [06:11<06:23,  3.09s/it]Running Inference:  38%|███▊      | 77/200 [06:20<09:28,  4.62s/it]Running Inference:  39%|███▉      | 78/200 [06:22<08:18,  4.09s/it]Running Inference:  40%|███▉      | 79/200 [06:30<10:27,  5.18s/it]Running Inference:  40%|████      | 80/200 [06:31<07:59,  4.00s/it]Running Inference:  40%|████      | 81/200 [06:32<05:53,  2.97s/it]Running Inference:  41%|████      | 82/200 [06:33<04:53,  2.49s/it]Running Inference:  42%|████▏     | 83/200 [06:40<07:24,  3.80s/it]Running Inference:  42%|████▏     | 84/200 [06:44<07:36,  3.93s/it]Running Inference:  42%|████▎     | 85/200 [06:52<09:25,  4.92s/it]Running Inference:  43%|████▎     | 86/200 [06:58<10:16,  5.41s/it]Running Inference:  44%|████▎     | 87/200 [07:01<08:37,  4.58s/it]Running Inference:  44%|████▍     | 88/200 [07:03<07:25,  3.97s/it]Running Inference:  44%|████▍     | 89/200 [07:07<07:12,  3.90s/it]Running Inference:  45%|████▌     | 90/200 [07:09<06:13,  3.40s/it]Running Inference:  46%|████▌     | 91/200 [07:15<07:24,  4.08s/it]Running Inference:  46%|████▌     | 92/200 [07:22<09:05,  5.05s/it]Running Inference:  46%|████▋     | 93/200 [07:26<08:12,  4.60s/it]Running Inference:  47%|████▋     | 94/200 [07:28<06:42,  3.80s/it]Running Inference:  48%|████▊     | 95/200 [07:33<07:31,  4.30s/it]Running Inference:  48%|████▊     | 96/200 [07:41<09:04,  5.24s/it]Running Inference:  48%|████▊     | 97/200 [07:42<06:56,  4.05s/it]Running Inference:  49%|████▉     | 98/200 [07:50<08:56,  5.26s/it]Running Inference:  50%|████▉     | 99/200 [07:52<07:19,  4.35s/it]Running Inference:  50%|█████     | 100/200 [08:00<08:45,  5.25s/it]Running Inference:  50%|█████     | 101/200 [08:06<09:23,  5.69s/it]Running Inference:  51%|█████     | 102/200 [08:14<10:03,  6.15s/it]Running Inference:  52%|█████▏    | 103/200 [08:20<10:16,  6.35s/it]Running Inference:  52%|█████▏    | 104/200 [08:25<09:12,  5.76s/it]Running Inference:  52%|█████▎    | 105/200 [08:27<07:31,  4.75s/it]Running Inference:  53%|█████▎    | 106/200 [08:35<08:52,  5.66s/it]Running Inference:  54%|█████▎    | 107/200 [08:42<09:19,  6.01s/it]Running Inference:  54%|█████▍    | 108/200 [08:46<08:29,  5.54s/it]Running Inference:  55%|█████▍    | 109/200 [08:48<06:37,  4.37s/it]Running Inference:  55%|█████▌    | 110/200 [08:52<06:25,  4.28s/it]Running Inference:  56%|█████▌    | 111/200 [08:55<05:47,  3.90s/it]Running Inference:  56%|█████▌    | 112/200 [08:58<05:16,  3.59s/it]Running Inference:  56%|█████▋    | 113/200 [09:00<04:41,  3.23s/it]Running Inference:  57%|█████▋    | 114/200 [09:07<06:03,  4.23s/it]Running Inference:  57%|█████▊    | 115/200 [09:15<07:31,  5.31s/it]Running Inference:  58%|█████▊    | 116/200 [09:18<06:42,  4.79s/it]Running Inference:  58%|█████▊    | 117/200 [09:20<05:14,  3.79s/it]Running Inference:  59%|█████▉    | 118/200 [09:27<06:37,  4.84s/it]Running Inference:  60%|█████▉    | 119/200 [09:29<05:25,  4.02s/it]Running Inference:  60%|██████    | 120/200 [09:37<06:50,  5.13s/it]Running Inference:  60%|██████    | 121/200 [09:40<05:54,  4.49s/it]Running Inference:  61%|██████    | 122/200 [09:47<06:59,  5.38s/it]Running Inference:  62%|██████▏   | 123/200 [09:54<07:25,  5.78s/it]Running Inference:  62%|██████▏   | 124/200 [09:56<05:58,  4.72s/it]Running Inference:  62%|██████▎   | 125/200 [10:00<05:33,  4.44s/it]Running Inference:  63%|██████▎   | 126/200 [10:03<05:00,  4.06s/it]Running Inference:  64%|██████▎   | 127/200 [10:06<04:33,  3.75s/it]Running Inference:  64%|██████▍   | 128/200 [10:13<05:40,  4.72s/it]Running Inference:  64%|██████▍   | 129/200 [10:17<05:10,  4.37s/it]Running Inference:  65%|██████▌   | 130/200 [10:24<06:12,  5.32s/it]Running Inference:  66%|██████▌   | 131/200 [10:32<07:04,  6.15s/it]Running Inference:  66%|██████▌   | 132/200 [10:35<05:50,  5.16s/it]Running Inference:  66%|██████▋   | 133/200 [10:39<05:15,  4.71s/it]Running Inference:  67%|██████▋   | 134/200 [10:43<04:50,  4.41s/it]Running Inference:  68%|██████▊   | 135/200 [10:49<05:33,  5.13s/it]Running Inference:  68%|██████▊   | 136/200 [10:56<06:04,  5.70s/it]Running Inference:  68%|██████▊   | 137/200 [10:59<04:55,  4.70s/it]Running Inference:  69%|██████▉   | 138/200 [11:06<05:30,  5.33s/it]Running Inference:  70%|██████▉   | 139/200 [11:12<05:47,  5.69s/it]Running Inference:  70%|███████   | 140/200 [11:20<06:23,  6.39s/it]Running Inference:  70%|███████   | 141/200 [11:27<06:29,  6.60s/it]Running Inference:  71%|███████   | 142/200 [11:34<06:27,  6.68s/it]Running Inference:  72%|███████▏  | 143/200 [11:41<06:28,  6.82s/it]Running Inference:  72%|███████▏  | 144/200 [11:49<06:29,  6.96s/it]Running Inference:  72%|███████▎  | 145/200 [11:51<05:05,  5.55s/it]Running Inference:  73%|███████▎  | 146/200 [11:54<04:18,  4.78s/it]Running Inference:  74%|███████▎  | 147/200 [12:02<05:02,  5.70s/it]Running Inference:  74%|███████▍  | 148/200 [12:03<03:56,  4.55s/it]Running Inference:  74%|███████▍  | 149/200 [12:09<04:08,  4.88s/it]Running Inference:  75%|███████▌  | 150/200 [12:16<04:36,  5.54s/it]Running Inference:  76%|███████▌  | 151/200 [12:23<04:50,  5.92s/it]Running Inference:  76%|███████▌  | 152/200 [12:24<03:37,  4.52s/it]Running Inference:  76%|███████▋  | 153/200 [12:28<03:16,  4.18s/it]Running Inference:  77%|███████▋  | 154/200 [12:29<02:37,  3.41s/it]Running Inference:  78%|███████▊  | 155/200 [12:31<02:16,  3.02s/it]Running Inference:  78%|███████▊  | 156/200 [12:37<02:48,  3.82s/it]Running Inference:  78%|███████▊  | 157/200 [12:38<02:10,  3.03s/it]Running Inference:  79%|███████▉  | 158/200 [12:42<02:10,  3.10s/it]Running Inference:  80%|███████▉  | 159/200 [12:46<02:24,  3.51s/it]Running Inference:  80%|████████  | 160/200 [12:53<02:58,  4.46s/it]Running Inference:  80%|████████  | 161/200 [12:59<03:20,  5.13s/it]Running Inference:  81%|████████  | 162/200 [13:07<03:42,  5.85s/it]Running Inference:  82%|████████▏ | 163/200 [13:15<04:02,  6.55s/it]Running Inference:  82%|████████▏ | 164/200 [13:22<04:02,  6.75s/it]Running Inference:  82%|████████▎ | 165/200 [13:27<03:33,  6.10s/it]Running Inference:  83%|████████▎ | 166/200 [13:34<03:34,  6.30s/it]Running Inference:  84%|████████▎ | 167/200 [13:41<03:38,  6.63s/it]Running Inference:  84%|████████▍ | 168/200 [13:45<03:04,  5.77s/it]Running Inference:  84%|████████▍ | 169/200 [13:48<02:34,  4.99s/it]Running Inference:  85%|████████▌ | 170/200 [13:50<02:00,  4.00s/it]Running Inference:  86%|████████▌ | 171/200 [13:54<02:03,  4.24s/it]Running Inference:  86%|████████▌ | 172/200 [14:01<02:19,  4.99s/it]Running Inference:  86%|████████▋ | 173/200 [14:08<02:30,  5.57s/it]Running Inference:  87%|████████▋ | 174/200 [14:15<02:35,  5.98s/it]Running Inference:  88%|████████▊ | 175/200 [14:19<02:16,  5.45s/it]Running Inference:  88%|████████▊ | 176/200 [14:22<01:48,  4.50s/it]Running Inference:  88%|████████▊ | 177/200 [14:26<01:44,  4.55s/it]Running Inference:  89%|████████▉ | 178/200 [14:31<01:38,  4.48s/it]Running Inference:  90%|████████▉ | 179/200 [14:34<01:30,  4.31s/it]Running Inference:  90%|█████████ | 180/200 [14:41<01:40,  5.02s/it]Running Inference:  90%|█████████ | 181/200 [14:48<01:47,  5.66s/it]Running Inference:  91%|█████████ | 182/200 [14:55<01:47,  5.98s/it]Running Inference:  92%|█████████▏| 183/200 [15:00<01:35,  5.63s/it]Running Inference:  92%|█████████▏| 184/200 [15:08<01:42,  6.38s/it]Running Inference:  92%|█████████▎| 185/200 [15:11<01:18,  5.24s/it]Running Inference:  93%|█████████▎| 186/200 [15:13<01:02,  4.49s/it]Running Inference:  94%|█████████▎| 187/200 [15:15<00:45,  3.52s/it]Running Inference:  94%|█████████▍| 188/200 [15:17<00:38,  3.17s/it]Running Inference:  94%|█████████▍| 189/200 [15:20<00:34,  3.12s/it]Running Inference:  95%|█████████▌| 190/200 [15:24<00:33,  3.38s/it]Running Inference:  96%|█████████▌| 191/200 [15:31<00:40,  4.47s/it]Running Inference:  96%|█████████▌| 192/200 [15:37<00:40,  5.09s/it]Running Inference:  96%|█████████▋| 193/200 [15:45<00:41,  5.94s/it]Running Inference:  97%|█████████▋| 194/200 [15:49<00:31,  5.31s/it]Running Inference:  98%|█████████▊| 195/200 [15:51<00:21,  4.32s/it]Running Inference:  98%|█████████▊| 196/200 [15:59<00:21,  5.38s/it]Running Inference:  98%|█████████▊| 197/200 [16:01<00:12,  4.25s/it]Running Inference:  99%|█████████▉| 198/200 [16:08<00:10,  5.22s/it]Running Inference: 100%|█████████▉| 199/200 [16:10<00:04,  4.31s/it]Running Inference: 100%|██████████| 200/200 [16:17<00:00,  5.10s/it]Running Inference: 100%|██████████| 200/200 [16:17<00:00,  4.89s/it]
2025-12-14 10:36:53,206 - INFO - Inference completed.
2025-12-14 10:36:53,217 - INFO - Results saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 10:36:53,217 - INFO - Calculating metrics for dataset: longbench
2025-12-14 10:36:53,337 - INFO - Metrics saved to longbenchresult/longbench__samsum__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 10:36:53,337 - INFO - Metrics:
22.17
2025-12-14 10:36:53,339 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=samsum, ratio=0.5) on GPU 2


========================================
LongBench Task: vcsum
========================================
----------------------------------------
Task: vcsum | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 10:36:59,703 - INFO - Set deterministic seeds to 42
2025-12-14 10:36:59,703 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 10:36:59,703 - INFO - Starting evaluation run...
2025-12-14 10:36:59,703 - INFO - Output directory set to: longbenchresult
2025-12-14 10:36:59,703 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 10:36:59,703 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 10:36:59,703 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 10:36:59,703 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.05it/s]
Device set to use cuda:0
2025-12-14 10:37:11,335 - INFO - Model pipeline loaded.
2025-12-14 10:37:11,336 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 10:37:14,559 - INFO - Dataset loaded with 200 entries.
2025-12-14 10:37:14,559 - INFO - Dataset processed with 200 entries.
2025-12-14 10:37:14,586 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:20:09, 24.17s/it]Running Inference:   1%|          | 2/200 [00:48<1:19:40, 24.14s/it]Running Inference:   2%|▏         | 3/200 [01:12<1:18:56, 24.04s/it]Running Inference:   2%|▏         | 4/200 [01:35<1:17:56, 23.86s/it]Running Inference:   2%|▎         | 5/200 [01:59<1:17:56, 23.98s/it]Running Inference:   3%|▎         | 6/200 [02:23<1:16:38, 23.70s/it]Running Inference:   4%|▎         | 7/200 [02:46<1:16:21, 23.74s/it]Running Inference:   4%|▍         | 8/200 [03:10<1:15:20, 23.54s/it]Running Inference:   4%|▍         | 9/200 [03:33<1:14:41, 23.46s/it]Running Inference:   5%|▌         | 10/200 [03:58<1:15:47, 23.94s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:22<1:15:16, 23.89s/it]Running Inference:   6%|▌         | 12/200 [04:45<1:14:08, 23.66s/it]Running Inference:   6%|▋         | 13/200 [05:09<1:14:19, 23.85s/it]Running Inference:   7%|▋         | 14/200 [05:32<1:13:06, 23.58s/it]Running Inference:   8%|▊         | 15/200 [05:55<1:12:20, 23.46s/it]Running Inference:   8%|▊         | 16/200 [06:20<1:12:58, 23.79s/it]Running Inference:   8%|▊         | 17/200 [06:43<1:12:01, 23.61s/it]Running Inference:   9%|▉         | 18/200 [07:09<1:13:39, 24.28s/it]Running Inference:  10%|▉         | 19/200 [07:33<1:12:42, 24.10s/it]Running Inference:  10%|█         | 20/200 [07:57<1:12:46, 24.26s/it]Running Inference:  10%|█         | 21/200 [08:21<1:12:15, 24.22s/it]Running Inference:  11%|█         | 22/200 [08:45<1:11:36, 24.14s/it]Running Inference:  12%|█▏        | 23/200 [09:09<1:10:33, 23.92s/it]Running Inference:  12%|█▏        | 24/200 [09:35<1:12:09, 24.60s/it]Running Inference:  12%|█▎        | 25/200 [09:58<1:10:52, 24.30s/it]Running Inference:  13%|█▎        | 26/200 [10:24<1:11:12, 24.55s/it]Running Inference:  14%|█▎        | 27/200 [10:47<1:10:09, 24.33s/it]Running Inference:  14%|█▍        | 28/200 [11:11<1:09:10, 24.13s/it]Running Inference:  14%|█▍        | 29/200 [11:35<1:08:40, 24.10s/it]Running Inference:  15%|█▌        | 30/200 [11:58<1:07:38, 23.87s/it]Running Inference:  16%|█▌        | 31/200 [12:22<1:06:53, 23.75s/it]Running Inference:  16%|█▌        | 32/200 [12:45<1:05:49, 23.51s/it]Running Inference:  16%|█▋        | 33/200 [13:08<1:04:54, 23.32s/it]Running Inference:  17%|█▋        | 34/200 [13:31<1:04:06, 23.17s/it]Running Inference:  18%|█▊        | 35/200 [13:54<1:04:12, 23.35s/it]Running Inference:  18%|█▊        | 36/200 [14:18<1:03:46, 23.33s/it]Running Inference:  18%|█▊        | 37/200 [14:41<1:03:13, 23.27s/it]Running Inference:  19%|█▉        | 38/200 [15:06<1:04:09, 23.76s/it]Running Inference:  20%|█▉        | 39/200 [15:29<1:03:43, 23.75s/it]Running Inference:  20%|██        | 40/200 [15:54<1:04:26, 24.17s/it]Running Inference:  20%|██        | 41/200 [16:22<1:06:42, 25.17s/it]Running Inference:  21%|██        | 42/200 [16:45<1:04:56, 24.66s/it]Running Inference:  22%|██▏       | 43/200 [17:09<1:03:48, 24.39s/it]Running Inference:  22%|██▏       | 44/200 [17:34<1:03:59, 24.61s/it]Running Inference:  22%|██▎       | 45/200 [17:58<1:02:37, 24.24s/it]Running Inference:  23%|██▎       | 46/200 [18:21<1:01:20, 23.90s/it]Running Inference:  24%|██▎       | 47/200 [18:46<1:01:35, 24.16s/it]Running Inference:  24%|██▍       | 48/200 [19:09<1:00:29, 23.88s/it]Running Inference:  24%|██▍       | 49/200 [19:32<59:29, 23.64s/it]  Running Inference:  25%|██▌       | 50/200 [19:55<58:39, 23.46s/it]Running Inference:  26%|██▌       | 51/200 [20:19<58:42, 23.64s/it]Running Inference:  26%|██▌       | 52/200 [20:43<58:32, 23.74s/it]Running Inference:  26%|██▋       | 53/200 [21:06<57:53, 23.63s/it]Running Inference:  27%|██▋       | 54/200 [21:30<57:15, 23.53s/it]Running Inference:  28%|██▊       | 55/200 [21:53<56:39, 23.44s/it]Running Inference:  28%|██▊       | 56/200 [22:16<56:05, 23.37s/it]Running Inference:  28%|██▊       | 57/200 [22:39<55:18, 23.20s/it]Running Inference:  29%|██▉       | 58/200 [23:03<55:22, 23.40s/it]Running Inference:  30%|██▉       | 59/200 [23:27<55:18, 23.53s/it]Running Inference:  30%|███       | 60/200 [23:51<55:21, 23.72s/it]Running Inference:  30%|███       | 61/200 [24:14<54:31, 23.53s/it]Running Inference:  31%|███       | 62/200 [24:37<53:46, 23.38s/it]Running Inference:  32%|███▏      | 63/200 [25:00<53:27, 23.41s/it]Running Inference:  32%|███▏      | 64/200 [25:24<53:18, 23.52s/it]Running Inference:  32%|███▎      | 65/200 [25:48<53:08, 23.62s/it]Running Inference:  33%|███▎      | 66/200 [26:11<52:20, 23.44s/it]Running Inference:  34%|███▎      | 67/200 [26:35<52:02, 23.48s/it]Running Inference:  34%|███▍      | 68/200 [26:58<51:40, 23.49s/it]Running Inference:  34%|███▍      | 69/200 [27:23<52:05, 23.86s/it]Running Inference:  35%|███▌      | 70/200 [27:46<51:16, 23.66s/it]Running Inference:  36%|███▌      | 71/200 [28:09<50:31, 23.50s/it]Running Inference:  36%|███▌      | 72/200 [28:33<50:32, 23.69s/it]Running Inference:  36%|███▋      | 73/200 [28:57<50:11, 23.71s/it]Running Inference:  37%|███▋      | 74/200 [29:20<49:30, 23.58s/it]Running Inference:  38%|███▊      | 75/200 [29:44<48:59, 23.51s/it]Running Inference:  38%|███▊      | 76/200 [30:07<48:21, 23.40s/it]Running Inference:  38%|███▊      | 77/200 [30:32<48:51, 23.83s/it]Running Inference:  39%|███▉      | 78/200 [30:55<47:57, 23.59s/it]Running Inference:  40%|███▉      | 79/200 [31:18<47:28, 23.54s/it]Running Inference:  40%|████      | 80/200 [31:41<46:51, 23.43s/it]Running Inference:  40%|████      | 81/200 [32:05<46:31, 23.46s/it]Running Inference:  41%|████      | 82/200 [32:28<46:05, 23.43s/it]Running Inference:  42%|████▏     | 83/200 [32:51<45:36, 23.39s/it]Running Inference:  42%|████▏     | 84/200 [33:16<45:39, 23.62s/it]Running Inference:  42%|████▎     | 85/200 [33:39<45:18, 23.64s/it]Running Inference:  43%|████▎     | 86/200 [34:03<45:09, 23.77s/it]Running Inference:  44%|████▎     | 87/200 [34:27<44:33, 23.66s/it]Running Inference:  44%|████▍     | 88/200 [34:50<44:01, 23.58s/it]Running Inference:  44%|████▍     | 89/200 [35:13<43:07, 23.31s/it]Running Inference:  45%|████▌     | 90/200 [35:36<42:39, 23.27s/it]Running Inference:  46%|████▌     | 91/200 [36:00<42:45, 23.54s/it]Running Inference:  46%|████▌     | 92/200 [36:23<41:58, 23.32s/it]Running Inference:  46%|████▋     | 93/200 [36:47<42:08, 23.63s/it]Running Inference:  47%|████▋     | 94/200 [37:12<42:20, 23.97s/it]Running Inference:  48%|████▊     | 95/200 [37:37<42:32, 24.30s/it]Running Inference:  48%|████▊     | 96/200 [38:00<41:29, 23.94s/it]Running Inference:  48%|████▊     | 97/200 [38:23<40:37, 23.66s/it]Running Inference:  49%|████▉     | 98/200 [38:47<40:17, 23.70s/it]Running Inference:  50%|████▉     | 99/200 [39:10<39:41, 23.58s/it]Running Inference:  50%|█████     | 100/200 [39:34<39:24, 23.65s/it]Running Inference:  50%|█████     | 101/200 [39:59<39:47, 24.12s/it]Running Inference:  51%|█████     | 102/200 [40:27<41:02, 25.13s/it]Running Inference:  52%|█████▏    | 103/200 [40:52<40:26, 25.02s/it]Running Inference:  52%|█████▏    | 104/200 [41:15<39:17, 24.56s/it]Running Inference:  52%|█████▎    | 105/200 [41:42<39:53, 25.19s/it]Running Inference:  53%|█████▎    | 106/200 [42:05<38:38, 24.67s/it]Running Inference:  54%|█████▎    | 107/200 [42:28<37:25, 24.15s/it]Running Inference:  54%|█████▍    | 108/200 [42:52<36:46, 23.99s/it]Running Inference:  55%|█████▍    | 109/200 [43:15<35:57, 23.71s/it]Running Inference:  55%|█████▌    | 110/200 [43:39<35:32, 23.70s/it]Running Inference:  56%|█████▌    | 111/200 [44:01<34:43, 23.42s/it]Running Inference:  56%|█████▌    | 112/200 [44:25<34:23, 23.45s/it]Running Inference:  56%|█████▋    | 113/200 [44:48<33:57, 23.42s/it]Running Inference:  57%|█████▋    | 114/200 [45:11<33:29, 23.37s/it]Running Inference:  57%|█████▊    | 115/200 [45:35<33:15, 23.47s/it]Running Inference:  58%|█████▊    | 116/200 [45:59<32:49, 23.44s/it]Running Inference:  58%|█████▊    | 117/200 [46:22<32:25, 23.44s/it]Running Inference:  59%|█████▉    | 118/200 [46:45<31:44, 23.22s/it]Running Inference:  60%|█████▉    | 119/200 [47:08<31:36, 23.41s/it]Running Inference:  60%|██████    | 120/200 [47:33<31:35, 23.70s/it]Running Inference:  60%|██████    | 121/200 [47:57<31:24, 23.85s/it]Running Inference:  61%|██████    | 122/200 [48:20<30:32, 23.50s/it]Running Inference:  62%|██████▏   | 123/200 [48:45<30:40, 23.91s/it]Running Inference:  62%|██████▏   | 124/200 [49:09<30:30, 24.09s/it]Running Inference:  62%|██████▎   | 125/200 [49:32<29:47, 23.84s/it]Running Inference:  63%|██████▎   | 126/200 [49:56<29:16, 23.74s/it]Running Inference:  64%|██████▎   | 127/200 [50:20<29:08, 23.96s/it]Running Inference:  64%|██████▍   | 128/200 [50:44<28:38, 23.87s/it]Running Inference:  64%|██████▍   | 129/200 [51:08<28:09, 23.79s/it]Running Inference:  65%|██████▌   | 130/200 [51:30<27:22, 23.47s/it]Running Inference:  66%|██████▌   | 131/200 [51:54<26:59, 23.47s/it]Running Inference:  66%|██████▌   | 132/200 [52:17<26:29, 23.38s/it]Running Inference:  66%|██████▋   | 133/200 [52:41<26:16, 23.53s/it]Running Inference:  67%|██████▋   | 134/200 [53:04<25:41, 23.36s/it]Running Inference:  68%|██████▊   | 135/200 [53:27<25:17, 23.35s/it]Running Inference:  68%|██████▊   | 136/200 [53:50<24:45, 23.21s/it]Running Inference:  68%|██████▊   | 137/200 [54:13<24:23, 23.23s/it]Running Inference:  69%|██████▉   | 138/200 [54:36<23:52, 23.11s/it]Running Inference:  70%|██████▉   | 139/200 [55:00<23:51, 23.46s/it]Running Inference:  70%|███████   | 140/200 [55:25<23:41, 23.69s/it]Running Inference:  70%|███████   | 141/200 [55:48<23:11, 23.58s/it]Running Inference:  71%|███████   | 142/200 [56:11<22:45, 23.54s/it]Running Inference:  72%|███████▏  | 143/200 [56:35<22:27, 23.63s/it]Running Inference:  72%|███████▏  | 144/200 [56:59<21:57, 23.53s/it]Running Inference:  72%|███████▎  | 145/200 [57:24<22:07, 24.14s/it]Running Inference:  73%|███████▎  | 146/200 [57:48<21:35, 23.99s/it]Running Inference:  74%|███████▎  | 147/200 [58:11<21:00, 23.79s/it]Running Inference:  74%|███████▍  | 148/200 [58:35<20:31, 23.68s/it]Running Inference:  74%|███████▍  | 149/200 [58:59<20:12, 23.77s/it]Running Inference:  75%|███████▌  | 150/200 [59:23<20:05, 24.11s/it]Running Inference:  76%|███████▌  | 151/200 [59:48<19:49, 24.27s/it]Running Inference:  76%|███████▌  | 152/200 [1:00:13<19:31, 24.40s/it]Running Inference:  76%|███████▋  | 153/200 [1:00:36<18:55, 24.16s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:59<18:16, 23.84s/it]Running Inference:  78%|███████▊  | 155/200 [1:01:23<17:46, 23.70s/it]Running Inference:  78%|███████▊  | 156/200 [1:01:46<17:15, 23.54s/it]Running Inference:  78%|███████▊  | 157/200 [1:02:09<16:51, 23.52s/it]Running Inference:  79%|███████▉  | 158/200 [1:02:33<16:31, 23.60s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:56<15:58, 23.37s/it]Running Inference:  80%|████████  | 160/200 [1:03:21<15:58, 23.95s/it]Running Inference:  80%|████████  | 161/200 [1:03:45<15:27, 23.79s/it]Running Inference:  81%|████████  | 162/200 [1:04:09<15:13, 24.03s/it]Running Inference:  82%|████████▏ | 163/200 [1:04:36<15:13, 24.70s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:59<14:37, 24.38s/it]Running Inference:  82%|████████▎ | 165/200 [1:05:24<14:20, 24.59s/it]Running Inference:  83%|████████▎ | 166/200 [1:05:48<13:41, 24.16s/it]Running Inference:  84%|████████▎ | 167/200 [1:06:11<13:09, 23.93s/it]Running Inference:  84%|████████▍ | 168/200 [1:06:35<12:42, 23.84s/it]Running Inference:  84%|████████▍ | 169/200 [1:06:58<12:15, 23.73s/it]Running Inference:  85%|████████▌ | 170/200 [1:07:22<11:51, 23.72s/it]Running Inference:  86%|████████▌ | 171/200 [1:07:45<11:22, 23.52s/it]Running Inference:  86%|████████▌ | 172/200 [1:08:09<11:00, 23.60s/it]Running Inference:  86%|████████▋ | 173/200 [1:08:34<10:49, 24.06s/it]Running Inference:  87%|████████▋ | 174/200 [1:08:57<10:21, 23.90s/it]Running Inference:  88%|████████▊ | 175/200 [1:09:21<09:55, 23.83s/it]Running Inference:  88%|████████▊ | 176/200 [1:09:44<09:27, 23.64s/it]Running Inference:  88%|████████▊ | 177/200 [1:10:07<09:00, 23.49s/it]Running Inference:  89%|████████▉ | 178/200 [1:10:30<08:33, 23.36s/it]Running Inference:  90%|████████▉ | 179/200 [1:10:54<08:15, 23.61s/it]Running Inference:  90%|█████████ | 180/200 [1:11:18<07:49, 23.46s/it]Running Inference:  90%|█████████ | 181/200 [1:11:41<07:24, 23.41s/it]Running Inference:  91%|█████████ | 182/200 [1:12:05<07:04, 23.58s/it]Running Inference:  92%|█████████▏| 183/200 [1:12:28<06:39, 23.52s/it]Running Inference:  92%|█████████▏| 184/200 [1:12:52<06:15, 23.47s/it]Running Inference:  92%|█████████▎| 185/200 [1:13:15<05:50, 23.35s/it]Running Inference:  93%|█████████▎| 186/200 [1:13:38<05:27, 23.37s/it]Running Inference:  94%|█████████▎| 187/200 [1:14:01<05:03, 23.33s/it]Running Inference:  94%|█████████▍| 188/200 [1:14:25<04:41, 23.48s/it]Running Inference:  94%|█████████▍| 189/200 [1:14:48<04:16, 23.36s/it]Running Inference:  95%|█████████▌| 190/200 [1:15:12<03:53, 23.38s/it]Running Inference:  96%|█████████▌| 191/200 [1:15:35<03:29, 23.29s/it]Running Inference:  96%|█████████▌| 192/200 [1:15:58<03:06, 23.30s/it]Running Inference:  96%|█████████▋| 193/200 [1:16:22<02:43, 23.37s/it]Running Inference:  97%|█████████▋| 194/200 [1:16:46<02:21, 23.54s/it]Running Inference:  98%|█████████▊| 195/200 [1:17:11<02:01, 24.24s/it]Running Inference:  98%|█████████▊| 196/200 [1:17:35<01:36, 24.12s/it]Running Inference:  98%|█████████▊| 197/200 [1:17:59<01:12, 24.06s/it]Running Inference:  99%|█████████▉| 198/200 [1:18:23<00:47, 23.86s/it]Running Inference: 100%|█████████▉| 199/200 [1:18:46<00:23, 23.73s/it]Running Inference: 100%|██████████| 200/200 [1:19:10<00:00, 23.71s/it]Running Inference: 100%|██████████| 200/200 [1:19:10<00:00, 23.75s/it]
2025-12-14 11:56:24,729 - INFO - Inference completed.
2025-12-14 11:56:24,742 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 11:56:24,742 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.655 seconds.
Prefix dict has been built successfully.
2025-12-14 11:56:32,502 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 11:56:32,502 - INFO - Metrics:
11.03
2025-12-14 11:56:32,503 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: streaming_llm (task=vcsum, ratio=0.1) on GPU 2

----------------------------------------
Task: vcsum | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 11:56:39,554 - INFO - Set deterministic seeds to 42
2025-12-14 11:56:39,554 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 11:56:39,554 - INFO - Starting evaluation run...
2025-12-14 11:56:39,554 - INFO - Output directory set to: longbenchresult
2025-12-14 11:56:39,555 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 11:56:39,555 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 11:56:39,555 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 11:56:39,555 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.46it/s]
Device set to use cuda:0
2025-12-14 11:56:51,367 - INFO - Model pipeline loaded.
2025-12-14 11:56:51,367 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 11:56:56,979 - INFO - Dataset loaded with 200 entries.
2025-12-14 11:56:56,979 - INFO - Dataset processed with 200 entries.
2025-12-14 11:56:57,007 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:18:19, 23.61s/it]Running Inference:   1%|          | 2/200 [00:47<1:17:49, 23.58s/it]Running Inference:   2%|▏         | 3/200 [01:10<1:17:07, 23.49s/it]Running Inference:   2%|▏         | 4/200 [01:33<1:16:06, 23.30s/it]Running Inference:   2%|▎         | 5/200 [01:57<1:16:07, 23.42s/it]Running Inference:   3%|▎         | 6/200 [02:19<1:14:51, 23.15s/it]Running Inference:   4%|▎         | 7/200 [02:43<1:14:35, 23.19s/it]Running Inference:   4%|▍         | 8/200 [03:05<1:13:41, 23.03s/it]Running Inference:   4%|▍         | 9/200 [03:28<1:13:17, 23.03s/it]Running Inference:   5%|▌         | 10/200 [03:53<1:14:20, 23.47s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:16<1:13:54, 23.46s/it]Running Inference:   6%|▌         | 12/200 [04:39<1:12:51, 23.25s/it]Running Inference:   6%|▋         | 13/200 [05:03<1:13:01, 23.43s/it]Running Inference:   7%|▋         | 14/200 [05:25<1:11:48, 23.17s/it]Running Inference:   8%|▊         | 15/200 [05:48<1:11:06, 23.06s/it]Running Inference:   8%|▊         | 16/200 [06:12<1:11:44, 23.40s/it]Running Inference:   8%|▊         | 17/200 [06:35<1:10:44, 23.20s/it]Running Inference:   9%|▉         | 18/200 [07:00<1:12:06, 23.77s/it]Running Inference:  10%|▉         | 19/200 [07:24<1:11:16, 23.63s/it]Running Inference:  10%|█         | 20/200 [07:48<1:11:23, 23.80s/it]Running Inference:  10%|█         | 21/200 [08:11<1:10:52, 23.76s/it]Running Inference:  11%|█         | 22/200 [08:35<1:10:14, 23.68s/it]Running Inference:  12%|█▏        | 23/200 [08:58<1:09:13, 23.47s/it]Running Inference:  12%|█▏        | 24/200 [09:23<1:10:30, 24.04s/it]Running Inference:  12%|█▎        | 25/200 [09:46<1:09:22, 23.78s/it]Running Inference:  13%|█▎        | 26/200 [10:11<1:09:38, 24.01s/it]Running Inference:  14%|█▎        | 27/200 [10:34<1:08:45, 23.85s/it]Running Inference:  14%|█▍        | 28/200 [10:58<1:07:49, 23.66s/it]Running Inference:  14%|█▍        | 29/200 [11:21<1:07:23, 23.64s/it]Running Inference:  15%|█▌        | 30/200 [11:44<1:06:20, 23.41s/it]Running Inference:  16%|█▌        | 31/200 [12:07<1:05:38, 23.30s/it]Running Inference:  16%|█▌        | 32/200 [12:30<1:04:35, 23.07s/it]Running Inference:  16%|█▋        | 33/200 [12:52<1:03:51, 22.94s/it]Running Inference:  17%|█▋        | 34/200 [13:15<1:03:16, 22.87s/it]Running Inference:  18%|█▊        | 35/200 [13:39<1:03:26, 23.07s/it]Running Inference:  18%|█▊        | 36/200 [14:02<1:03:03, 23.07s/it]Running Inference:  18%|█▊        | 37/200 [14:25<1:02:34, 23.04s/it]Running Inference:  19%|█▉        | 38/200 [14:49<1:03:24, 23.48s/it]Running Inference:  20%|█▉        | 39/200 [15:13<1:03:03, 23.50s/it]Running Inference:  20%|██        | 40/200 [15:38<1:03:44, 23.90s/it]Running Inference:  20%|██        | 41/200 [16:04<1:05:46, 24.82s/it]Running Inference:  21%|██        | 42/200 [16:28<1:04:11, 24.37s/it]Running Inference:  22%|██▏       | 43/200 [16:51<1:03:11, 24.15s/it]Running Inference:  22%|██▏       | 44/200 [17:16<1:03:22, 24.37s/it]Running Inference:  22%|██▎       | 45/200 [17:40<1:02:06, 24.04s/it]Running Inference:  23%|██▎       | 46/200 [18:03<1:00:51, 23.71s/it]Running Inference:  24%|██▎       | 47/200 [18:27<1:01:03, 23.94s/it]Running Inference:  24%|██▍       | 48/200 [18:50<1:00:00, 23.69s/it]Running Inference:  24%|██▍       | 49/200 [19:13<59:03, 23.47s/it]  Running Inference:  25%|██▌       | 50/200 [19:36<58:13, 23.29s/it]Running Inference:  26%|██▌       | 51/200 [20:00<58:17, 23.47s/it]Running Inference:  26%|██▌       | 52/200 [20:24<58:09, 23.58s/it]Running Inference:  26%|██▋       | 53/200 [20:47<57:29, 23.46s/it]Running Inference:  27%|██▋       | 54/200 [21:10<56:55, 23.39s/it]Running Inference:  28%|██▊       | 55/200 [21:33<56:19, 23.31s/it]Running Inference:  28%|██▊       | 56/200 [21:56<55:46, 23.24s/it]Running Inference:  28%|██▊       | 57/200 [22:19<55:01, 23.08s/it]Running Inference:  29%|██▉       | 58/200 [22:43<55:03, 23.27s/it]Running Inference:  30%|██▉       | 59/200 [23:06<54:56, 23.38s/it]Running Inference:  30%|███       | 60/200 [23:30<55:01, 23.58s/it]Running Inference:  30%|███       | 61/200 [23:53<54:13, 23.40s/it]Running Inference:  31%|███       | 62/200 [24:16<53:29, 23.26s/it]Running Inference:  32%|███▏      | 63/200 [24:40<53:09, 23.28s/it]Running Inference:  32%|███▏      | 64/200 [25:03<52:59, 23.38s/it]Running Inference:  32%|███▎      | 65/200 [25:27<52:47, 23.47s/it]Running Inference:  33%|███▎      | 66/200 [25:50<52:01, 23.30s/it]Running Inference:  34%|███▎      | 67/200 [26:13<51:42, 23.33s/it]Running Inference:  34%|███▍      | 68/200 [26:37<51:20, 23.34s/it]Running Inference:  34%|███▍      | 69/200 [27:01<51:37, 23.65s/it]Running Inference:  35%|███▌      | 70/200 [27:24<50:49, 23.46s/it]Running Inference:  36%|███▌      | 71/200 [27:47<50:08, 23.32s/it]Running Inference:  36%|███▌      | 72/200 [28:11<50:10, 23.52s/it]Running Inference:  36%|███▋      | 73/200 [28:35<49:50, 23.54s/it]Running Inference:  37%|███▋      | 74/200 [28:58<49:09, 23.41s/it]Running Inference:  38%|███▊      | 75/200 [29:21<48:39, 23.36s/it]Running Inference:  38%|███▊      | 76/200 [29:44<48:03, 23.25s/it]Running Inference:  38%|███▊      | 77/200 [30:08<48:27, 23.64s/it]Running Inference:  39%|███▉      | 78/200 [30:31<47:35, 23.40s/it]Running Inference:  40%|███▉      | 79/200 [30:54<47:05, 23.35s/it]Running Inference:  40%|████      | 80/200 [31:18<46:33, 23.28s/it]Running Inference:  40%|████      | 81/200 [31:41<46:13, 23.31s/it]Running Inference:  41%|████      | 82/200 [32:04<45:48, 23.29s/it]Running Inference:  42%|████▏     | 83/200 [32:27<45:18, 23.23s/it]Running Inference:  42%|████▏     | 84/200 [32:51<45:21, 23.46s/it]Running Inference:  42%|████▎     | 85/200 [33:15<44:59, 23.48s/it]Running Inference:  43%|████▎     | 86/200 [33:39<44:50, 23.60s/it]Running Inference:  44%|████▎     | 87/200 [34:02<44:15, 23.50s/it]Running Inference:  44%|████▍     | 88/200 [34:25<43:44, 23.43s/it]Running Inference:  44%|████▍     | 89/200 [34:48<42:50, 23.15s/it]Running Inference:  45%|████▌     | 90/200 [35:11<42:24, 23.13s/it]Running Inference:  46%|████▌     | 91/200 [35:35<42:30, 23.40s/it]Running Inference:  46%|████▌     | 92/200 [35:58<41:44, 23.19s/it]Running Inference:  46%|████▋     | 93/200 [36:22<41:56, 23.52s/it]Running Inference:  47%|████▋     | 94/200 [36:46<42:02, 23.79s/it]Running Inference:  48%|████▊     | 95/200 [37:11<42:04, 24.05s/it]Running Inference:  48%|████▊     | 96/200 [37:34<41:05, 23.71s/it]Running Inference:  48%|████▊     | 97/200 [37:57<40:15, 23.46s/it]Running Inference:  49%|████▉     | 98/200 [38:20<39:56, 23.50s/it]Running Inference:  50%|████▉     | 99/200 [38:43<39:21, 23.39s/it]Running Inference:  50%|█████     | 100/200 [39:07<39:05, 23.46s/it]Running Inference:  50%|█████     | 101/200 [39:32<39:20, 23.84s/it]Running Inference:  51%|█████     | 102/200 [39:59<40:26, 24.76s/it]Running Inference:  52%|█████▏    | 103/200 [40:23<39:52, 24.66s/it]Running Inference:  52%|█████▏    | 104/200 [40:47<38:50, 24.28s/it]Running Inference:  52%|█████▎    | 105/200 [41:13<39:20, 24.85s/it]Running Inference:  53%|█████▎    | 106/200 [41:36<38:13, 24.40s/it]Running Inference:  54%|█████▎    | 107/200 [41:59<37:05, 23.93s/it]Running Inference:  54%|█████▍    | 108/200 [42:22<36:30, 23.81s/it]Running Inference:  55%|█████▍    | 109/200 [42:45<35:42, 23.54s/it]Running Inference:  55%|█████▌    | 110/200 [43:09<35:18, 23.54s/it]Running Inference:  56%|█████▌    | 111/200 [43:31<34:31, 23.27s/it]Running Inference:  56%|█████▌    | 112/200 [43:55<34:13, 23.34s/it]Running Inference:  56%|█████▋    | 113/200 [44:18<33:48, 23.31s/it]Running Inference:  57%|█████▋    | 114/200 [44:41<33:20, 23.26s/it]Running Inference:  57%|█████▊    | 115/200 [45:05<33:05, 23.35s/it]Running Inference:  58%|█████▊    | 116/200 [45:28<32:40, 23.34s/it]Running Inference:  58%|█████▊    | 117/200 [45:52<32:15, 23.32s/it]Running Inference:  59%|█████▉    | 118/200 [46:14<31:34, 23.10s/it]Running Inference:  60%|█████▉    | 119/200 [46:38<31:27, 23.30s/it]Running Inference:  60%|██████    | 120/200 [47:02<31:26, 23.58s/it]Running Inference:  60%|██████    | 121/200 [47:26<31:15, 23.74s/it]Running Inference:  61%|██████    | 122/200 [47:49<30:24, 23.39s/it]Running Inference:  62%|██████▏   | 123/200 [48:13<30:28, 23.74s/it]Running Inference:  62%|██████▏   | 124/200 [48:38<30:18, 23.92s/it]Running Inference:  62%|██████▎   | 125/200 [49:01<29:36, 23.69s/it]Running Inference:  63%|██████▎   | 126/200 [49:24<29:06, 23.60s/it]Running Inference:  64%|██████▎   | 127/200 [49:49<28:57, 23.80s/it]Running Inference:  64%|██████▍   | 128/200 [50:12<28:27, 23.72s/it]Running Inference:  64%|██████▍   | 129/200 [50:36<27:59, 23.65s/it]Running Inference:  65%|██████▌   | 130/200 [50:58<27:13, 23.33s/it]Running Inference:  66%|██████▌   | 131/200 [51:22<26:51, 23.35s/it]Running Inference:  66%|██████▌   | 132/200 [51:45<26:22, 23.27s/it]Running Inference:  66%|██████▋   | 133/200 [52:08<26:10, 23.44s/it]Running Inference:  67%|██████▋   | 134/200 [52:31<25:35, 23.27s/it]Running Inference:  68%|██████▊   | 135/200 [52:55<25:13, 23.28s/it]Running Inference:  68%|██████▊   | 136/200 [53:17<24:39, 23.12s/it]Running Inference:  68%|██████▊   | 137/200 [53:41<24:18, 23.15s/it]Running Inference:  69%|██████▉   | 138/200 [53:56<21:34, 20.88s/it]Running Inference:  70%|██████▉   | 139/200 [54:20<22:13, 21.86s/it]Running Inference:  70%|███████   | 140/200 [54:44<22:32, 22.55s/it]Running Inference:  70%|███████   | 141/200 [55:08<22:22, 22.76s/it]Running Inference:  71%|███████   | 142/200 [55:31<22:10, 22.94s/it]Running Inference:  72%|███████▏  | 143/200 [55:55<22:03, 23.22s/it]Running Inference:  72%|███████▏  | 144/200 [56:18<21:40, 23.23s/it]Running Inference:  72%|███████▎  | 145/200 [56:43<21:43, 23.71s/it]Running Inference:  73%|███████▎  | 146/200 [57:06<21:11, 23.54s/it]Running Inference:  74%|███████▎  | 147/200 [57:29<20:37, 23.36s/it]Running Inference:  74%|███████▍  | 148/200 [57:52<20:10, 23.29s/it]Running Inference:  74%|███████▍  | 149/200 [58:16<19:51, 23.37s/it]Running Inference:  75%|███████▌  | 150/200 [58:40<19:43, 23.67s/it]Running Inference:  76%|███████▌  | 151/200 [59:04<19:29, 23.86s/it]Running Inference:  76%|███████▌  | 152/200 [59:29<19:11, 24.00s/it]Running Inference:  76%|███████▋  | 153/200 [59:52<18:37, 23.77s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:15<17:58, 23.44s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:38<17:30, 23.33s/it]Running Inference:  78%|███████▊  | 156/200 [1:01:01<17:00, 23.19s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:24<16:36, 23.18s/it]Running Inference:  79%|███████▉  | 158/200 [1:01:47<16:16, 23.25s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:10<15:44, 23.04s/it]Running Inference:  80%|████████  | 160/200 [1:02:34<15:41, 23.53s/it]Running Inference:  80%|████████  | 161/200 [1:02:57<15:11, 23.38s/it]Running Inference:  81%|████████  | 162/200 [1:03:22<14:58, 23.64s/it]Running Inference:  82%|████████▏ | 163/200 [1:03:47<14:57, 24.24s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:11<14:22, 23.95s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:35<14:04, 24.14s/it]Running Inference:  83%|████████▎ | 166/200 [1:04:58<13:27, 23.74s/it]Running Inference:  84%|████████▎ | 167/200 [1:05:21<12:56, 23.55s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:44<12:30, 23.46s/it]Running Inference:  84%|████████▍ | 169/200 [1:06:07<12:02, 23.32s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:31<11:38, 23.30s/it]Running Inference:  86%|████████▌ | 171/200 [1:06:53<11:10, 23.13s/it]Running Inference:  86%|████████▌ | 172/200 [1:07:17<10:50, 23.23s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:41<10:38, 23.64s/it]Running Inference:  87%|████████▋ | 174/200 [1:08:05<10:11, 23.50s/it]Running Inference:  88%|████████▊ | 175/200 [1:08:28<09:46, 23.46s/it]Running Inference:  88%|████████▊ | 176/200 [1:08:51<09:18, 23.27s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:14<08:51, 23.13s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:36<08:26, 23.02s/it]Running Inference:  90%|████████▉ | 179/200 [1:10:00<08:08, 23.26s/it]Running Inference:  90%|█████████ | 180/200 [1:10:23<07:41, 23.09s/it]Running Inference:  90%|█████████ | 181/200 [1:10:46<07:18, 23.07s/it]Running Inference:  91%|█████████ | 182/200 [1:11:09<06:58, 23.24s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:33<06:34, 23.18s/it]Running Inference:  92%|█████████▏| 184/200 [1:11:55<06:09, 23.11s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:18<05:44, 22.97s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:41<05:21, 22.99s/it]Running Inference:  94%|█████████▎| 187/200 [1:13:04<04:58, 22.95s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:27<04:37, 23.11s/it]Running Inference:  94%|█████████▍| 189/200 [1:13:50<04:12, 22.98s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:13<03:50, 23.00s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:36<03:26, 22.92s/it]Running Inference:  96%|█████████▌| 192/200 [1:14:59<03:03, 22.93s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:22<02:41, 23.01s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:46<02:19, 23.19s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:11<01:59, 23.81s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:34<01:34, 23.69s/it]Running Inference:  98%|█████████▊| 197/200 [1:16:58<01:10, 23.64s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:21<00:46, 23.44s/it]Running Inference: 100%|█████████▉| 199/200 [1:17:44<00:23, 23.32s/it]Running Inference: 100%|██████████| 200/200 [1:18:07<00:00, 23.30s/it]Running Inference: 100%|██████████| 200/200 [1:18:07<00:00, 23.44s/it]
2025-12-14 13:15:04,676 - INFO - Inference completed.
2025-12-14 13:15:04,689 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 13:15:04,689 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.671 seconds.
Prefix dict has been built successfully.
2025-12-14 13:15:12,280 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 13:15:12,280 - INFO - Metrics:
11.05
2025-12-14 13:15:12,281 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: streaming_llm (task=vcsum, ratio=0.2) on GPU 2

----------------------------------------
Task: vcsum | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 13:15:19,424 - INFO - Set deterministic seeds to 42
2025-12-14 13:15:19,424 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 13:15:19,424 - INFO - Starting evaluation run...
2025-12-14 13:15:19,424 - INFO - Output directory set to: longbenchresult
2025-12-14 13:15:19,424 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 13:15:19,424 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 13:15:19,424 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 13:15:19,424 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.53it/s]
Device set to use cuda:0
2025-12-14 13:15:33,661 - INFO - Model pipeline loaded.
2025-12-14 13:15:33,661 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 13:15:38,200 - INFO - Dataset loaded with 200 entries.
2025-12-14 13:15:38,200 - INFO - Dataset processed with 200 entries.
2025-12-14 13:15:38,228 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:24<1:19:37, 24.01s/it]Running Inference:   1%|          | 2/200 [00:47<1:19:00, 23.94s/it]Running Inference:   2%|▏         | 3/200 [01:11<1:18:18, 23.85s/it]Running Inference:   2%|▏         | 4/200 [01:34<1:17:15, 23.65s/it]Running Inference:   2%|▎         | 5/200 [01:58<1:17:15, 23.77s/it]Running Inference:   3%|▎         | 6/200 [02:21<1:15:56, 23.49s/it]Running Inference:   4%|▎         | 7/200 [02:45<1:15:39, 23.52s/it]Running Inference:   4%|▍         | 8/200 [03:08<1:14:37, 23.32s/it]Running Inference:   4%|▍         | 9/200 [03:31<1:14:00, 23.25s/it]Running Inference:   5%|▌         | 10/200 [03:56<1:14:56, 23.67s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:19<1:14:26, 23.63s/it]Running Inference:   6%|▌         | 12/200 [04:42<1:13:20, 23.41s/it]Running Inference:   6%|▋         | 13/200 [05:06<1:13:33, 23.60s/it]Running Inference:   7%|▋         | 14/200 [05:29<1:12:21, 23.34s/it]Running Inference:   8%|▊         | 15/200 [05:52<1:11:35, 23.22s/it]Running Inference:   8%|▊         | 16/200 [06:16<1:12:13, 23.55s/it]Running Inference:   8%|▊         | 17/200 [06:39<1:11:17, 23.37s/it]Running Inference:   9%|▉         | 18/200 [07:04<1:12:26, 23.88s/it]Running Inference:  10%|▉         | 19/200 [07:28<1:11:41, 23.77s/it]Running Inference:  10%|█         | 20/200 [07:52<1:11:50, 23.95s/it]Running Inference:  10%|█         | 21/200 [08:16<1:11:22, 23.92s/it]Running Inference:  11%|█         | 22/200 [08:40<1:10:46, 23.86s/it]Running Inference:  12%|█▏        | 23/200 [09:03<1:09:47, 23.66s/it]Running Inference:  12%|█▏        | 24/200 [09:28<1:10:43, 24.11s/it]Running Inference:  12%|█▎        | 25/200 [09:51<1:09:41, 23.89s/it]Running Inference:  13%|█▎        | 26/200 [10:16<1:09:59, 24.13s/it]Running Inference:  14%|█▎        | 27/200 [10:40<1:09:08, 23.98s/it]Running Inference:  14%|█▍        | 28/200 [11:03<1:08:16, 23.82s/it]Running Inference:  14%|█▍        | 29/200 [11:27<1:07:49, 23.80s/it]Running Inference:  15%|█▌        | 30/200 [11:50<1:06:51, 23.59s/it]Running Inference:  16%|█▌        | 31/200 [12:13<1:06:07, 23.48s/it]Running Inference:  16%|█▌        | 32/200 [12:36<1:05:04, 23.24s/it]Running Inference:  16%|█▋        | 33/200 [12:59<1:04:20, 23.11s/it]Running Inference:  17%|█▋        | 34/200 [13:21<1:03:42, 23.03s/it]Running Inference:  18%|█▊        | 35/200 [13:45<1:03:57, 23.26s/it]Running Inference:  18%|█▊        | 36/200 [14:09<1:03:35, 23.26s/it]Running Inference:  18%|█▊        | 37/200 [14:32<1:03:06, 23.23s/it]Running Inference:  19%|█▉        | 38/200 [14:56<1:03:56, 23.68s/it]Running Inference:  20%|█▉        | 39/200 [15:20<1:03:35, 23.70s/it]Running Inference:  20%|██        | 40/200 [15:45<1:04:08, 24.05s/it]Running Inference:  20%|██        | 41/200 [16:12<1:05:52, 24.86s/it]Running Inference:  21%|██        | 42/200 [16:35<1:04:24, 24.46s/it]Running Inference:  22%|██▏       | 43/200 [16:59<1:03:28, 24.26s/it]Running Inference:  22%|██▏       | 44/200 [17:24<1:03:31, 24.43s/it]Running Inference:  22%|██▎       | 45/200 [17:47<1:02:20, 24.13s/it]Running Inference:  23%|██▎       | 46/200 [18:10<1:01:10, 23.83s/it]Running Inference:  24%|██▎       | 47/200 [18:35<1:01:23, 24.08s/it]Running Inference:  24%|██▍       | 48/200 [18:58<1:00:26, 23.86s/it]Running Inference:  24%|██▍       | 49/200 [19:22<59:39, 23.70s/it]  Running Inference:  25%|██▌       | 50/200 [19:45<58:48, 23.53s/it]Running Inference:  26%|██▌       | 51/200 [20:09<58:51, 23.70s/it]Running Inference:  26%|██▌       | 52/200 [20:33<58:41, 23.79s/it]Running Inference:  26%|██▋       | 53/200 [20:56<58:02, 23.69s/it]Running Inference:  27%|██▋       | 54/200 [21:20<57:25, 23.60s/it]Running Inference:  28%|██▊       | 55/200 [21:43<56:49, 23.51s/it]Running Inference:  28%|██▊       | 56/200 [22:06<56:14, 23.43s/it]Running Inference:  28%|██▊       | 57/200 [22:29<55:26, 23.26s/it]Running Inference:  29%|██▉       | 58/200 [22:53<55:28, 23.44s/it]Running Inference:  30%|██▉       | 59/200 [23:17<55:24, 23.57s/it]Running Inference:  30%|███       | 60/200 [23:41<55:29, 23.78s/it]Running Inference:  30%|███       | 61/200 [24:04<54:39, 23.60s/it]Running Inference:  31%|███       | 62/200 [24:28<53:55, 23.45s/it]Running Inference:  32%|███▏      | 63/200 [24:51<53:36, 23.48s/it]Running Inference:  32%|███▏      | 64/200 [25:15<53:26, 23.58s/it]Running Inference:  32%|███▎      | 65/200 [25:39<53:15, 23.67s/it]Running Inference:  33%|███▎      | 66/200 [26:02<52:27, 23.49s/it]Running Inference:  34%|███▎      | 67/200 [26:26<52:10, 23.53s/it]Running Inference:  34%|███▍      | 68/200 [26:49<51:47, 23.54s/it]Running Inference:  34%|███▍      | 69/200 [27:14<52:06, 23.86s/it]Running Inference:  35%|███▌      | 70/200 [27:37<51:18, 23.68s/it]Running Inference:  36%|███▌      | 71/200 [28:00<50:35, 23.53s/it]Running Inference:  36%|███▌      | 72/200 [28:24<50:35, 23.72s/it]Running Inference:  36%|███▋      | 73/200 [28:48<50:15, 23.75s/it]Running Inference:  37%|███▋      | 74/200 [29:11<49:37, 23.63s/it]Running Inference:  38%|███▊      | 75/200 [29:35<49:07, 23.58s/it]Running Inference:  38%|███▊      | 76/200 [29:58<48:28, 23.46s/it]Running Inference:  38%|███▊      | 77/200 [30:23<48:52, 23.84s/it]Running Inference:  39%|███▉      | 78/200 [30:46<48:00, 23.61s/it]Running Inference:  40%|███▉      | 79/200 [31:09<47:32, 23.57s/it]Running Inference:  40%|████      | 80/200 [31:33<46:57, 23.48s/it]Running Inference:  40%|████      | 81/200 [31:56<46:37, 23.51s/it]Running Inference:  41%|████      | 82/200 [32:20<46:11, 23.49s/it]Running Inference:  42%|████▏     | 83/200 [32:43<45:43, 23.45s/it]Running Inference:  42%|████▏     | 84/200 [33:07<45:46, 23.68s/it]Running Inference:  42%|████▎     | 85/200 [33:31<45:26, 23.71s/it]Running Inference:  43%|████▎     | 86/200 [33:55<45:16, 23.83s/it]Running Inference:  44%|████▎     | 87/200 [34:19<44:40, 23.72s/it]Running Inference:  44%|████▍     | 88/200 [34:42<44:08, 23.65s/it]Running Inference:  44%|████▍     | 89/200 [35:05<43:13, 23.36s/it]Running Inference:  45%|████▌     | 90/200 [35:28<42:46, 23.33s/it]Running Inference:  46%|████▌     | 91/200 [35:52<42:51, 23.59s/it]Running Inference:  46%|████▌     | 92/200 [36:15<42:04, 23.37s/it]Running Inference:  46%|████▋     | 93/200 [36:40<42:15, 23.70s/it]Running Inference:  47%|████▋     | 94/200 [37:04<42:20, 23.97s/it]Running Inference:  48%|████▊     | 95/200 [37:29<42:24, 24.23s/it]Running Inference:  48%|████▊     | 96/200 [37:52<41:26, 23.91s/it]Running Inference:  48%|████▊     | 97/200 [38:15<40:37, 23.66s/it]Running Inference:  49%|████▉     | 98/200 [38:39<40:18, 23.71s/it]Running Inference:  50%|████▉     | 99/200 [39:02<39:44, 23.61s/it]Running Inference:  50%|█████     | 100/200 [39:26<39:28, 23.69s/it]Running Inference:  50%|█████     | 101/200 [39:51<39:41, 24.05s/it]Running Inference:  51%|█████     | 102/200 [40:18<40:33, 24.83s/it]Running Inference:  52%|█████▏    | 103/200 [40:43<40:03, 24.78s/it]Running Inference:  52%|█████▏    | 104/200 [41:06<39:05, 24.43s/it]Running Inference:  52%|█████▎    | 105/200 [41:32<39:25, 24.90s/it]Running Inference:  53%|█████▎    | 106/200 [41:56<38:20, 24.48s/it]Running Inference:  54%|█████▎    | 107/200 [42:19<37:15, 24.04s/it]Running Inference:  54%|█████▍    | 108/200 [42:42<36:41, 23.93s/it]Running Inference:  55%|█████▍    | 109/200 [43:05<35:55, 23.69s/it]Running Inference:  55%|█████▌    | 110/200 [43:29<35:34, 23.71s/it]Running Inference:  56%|█████▌    | 111/200 [43:52<34:45, 23.44s/it]Running Inference:  56%|█████▌    | 112/200 [44:16<34:28, 23.50s/it]Running Inference:  56%|█████▋    | 113/200 [44:39<34:02, 23.48s/it]Running Inference:  57%|█████▋    | 114/200 [45:02<33:35, 23.43s/it]Running Inference:  57%|█████▊    | 115/200 [45:26<33:20, 23.53s/it]Running Inference:  58%|█████▊    | 116/200 [45:50<32:54, 23.51s/it]Running Inference:  58%|█████▊    | 117/200 [46:13<32:30, 23.50s/it]Running Inference:  59%|█████▉    | 118/200 [46:36<31:50, 23.29s/it]Running Inference:  60%|█████▉    | 119/200 [47:00<31:41, 23.47s/it]Running Inference:  60%|██████    | 120/200 [47:24<31:40, 23.75s/it]Running Inference:  60%|██████    | 121/200 [47:49<31:29, 23.91s/it]Running Inference:  61%|██████    | 122/200 [48:11<30:37, 23.56s/it]Running Inference:  62%|██████▏   | 123/200 [48:36<30:41, 23.91s/it]Running Inference:  62%|██████▏   | 124/200 [49:00<30:30, 24.09s/it]Running Inference:  62%|██████▎   | 125/200 [49:24<29:52, 23.91s/it]Running Inference:  63%|██████▎   | 126/200 [49:48<29:24, 23.84s/it]Running Inference:  64%|██████▎   | 127/200 [50:12<29:14, 24.04s/it]Running Inference:  64%|██████▍   | 128/200 [50:36<28:44, 23.95s/it]Running Inference:  64%|██████▍   | 129/200 [51:00<28:15, 23.88s/it]Running Inference:  65%|██████▌   | 130/200 [51:22<27:28, 23.55s/it]Running Inference:  66%|██████▌   | 131/200 [51:46<27:04, 23.55s/it]Running Inference:  66%|██████▌   | 132/200 [52:09<26:35, 23.47s/it]Running Inference:  66%|██████▋   | 133/200 [52:33<26:21, 23.60s/it]Running Inference:  67%|██████▋   | 134/200 [52:56<25:46, 23.43s/it]Running Inference:  68%|██████▊   | 135/200 [53:20<25:21, 23.40s/it]Running Inference:  68%|██████▊   | 136/200 [53:42<24:47, 23.24s/it]Running Inference:  68%|██████▊   | 137/200 [54:06<24:26, 23.28s/it]Running Inference:  69%|██████▉   | 138/200 [54:29<23:56, 23.17s/it]Running Inference:  70%|██████▉   | 139/200 [54:53<23:54, 23.52s/it]Running Inference:  70%|███████   | 140/200 [55:17<23:45, 23.76s/it]Running Inference:  70%|███████   | 141/200 [55:41<23:15, 23.66s/it]Running Inference:  71%|███████   | 142/200 [56:04<22:49, 23.61s/it]Running Inference:  72%|███████▏  | 143/200 [56:28<22:32, 23.72s/it]Running Inference:  72%|███████▏  | 144/200 [56:52<22:02, 23.61s/it]Running Inference:  72%|███████▎  | 145/200 [57:16<21:59, 23.98s/it]Running Inference:  73%|███████▎  | 146/200 [57:40<21:24, 23.79s/it]Running Inference:  74%|███████▎  | 147/200 [58:03<20:49, 23.58s/it]Running Inference:  74%|███████▍  | 148/200 [58:26<20:20, 23.47s/it]Running Inference:  74%|███████▍  | 149/200 [58:50<20:01, 23.56s/it]Running Inference:  75%|███████▌  | 150/200 [59:14<19:53, 23.87s/it]Running Inference:  76%|███████▌  | 151/200 [59:39<19:37, 24.04s/it]Running Inference:  76%|███████▌  | 152/200 [1:00:03<19:20, 24.18s/it]Running Inference:  76%|███████▋  | 153/200 [1:00:27<18:46, 23.96s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:50<18:07, 23.64s/it]Running Inference:  78%|███████▊  | 155/200 [1:01:13<17:38, 23.51s/it]Running Inference:  78%|███████▊  | 156/200 [1:01:36<17:07, 23.36s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:59<16:43, 23.33s/it]Running Inference:  79%|███████▉  | 158/200 [1:02:23<16:23, 23.41s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:46<15:52, 23.24s/it]Running Inference:  80%|████████  | 160/200 [1:03:10<15:48, 23.71s/it]Running Inference:  80%|████████  | 161/200 [1:03:34<15:18, 23.56s/it]Running Inference:  81%|████████  | 162/200 [1:03:58<15:05, 23.82s/it]Running Inference:  82%|████████▏ | 163/200 [1:04:23<14:59, 24.30s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:47<14:26, 24.06s/it]Running Inference:  82%|████████▎ | 165/200 [1:05:12<14:08, 24.25s/it]Running Inference:  83%|████████▎ | 166/200 [1:05:35<13:31, 23.87s/it]Running Inference:  84%|████████▎ | 167/200 [1:05:58<13:01, 23.68s/it]Running Inference:  84%|████████▍ | 168/200 [1:06:21<12:35, 23.60s/it]Running Inference:  84%|████████▍ | 169/200 [1:06:44<12:07, 23.48s/it]Running Inference:  85%|████████▌ | 170/200 [1:07:08<11:44, 23.47s/it]Running Inference:  86%|████████▌ | 171/200 [1:07:31<11:15, 23.29s/it]Running Inference:  86%|████████▌ | 172/200 [1:07:54<10:54, 23.39s/it]Running Inference:  86%|████████▋ | 173/200 [1:08:19<10:42, 23.79s/it]Running Inference:  87%|████████▋ | 174/200 [1:08:43<10:15, 23.66s/it]Running Inference:  88%|████████▊ | 175/200 [1:09:06<09:50, 23.61s/it]Running Inference:  88%|████████▊ | 176/200 [1:09:29<09:22, 23.44s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:52<08:56, 23.31s/it]Running Inference:  89%|████████▉ | 178/200 [1:10:15<08:30, 23.19s/it]Running Inference:  90%|████████▉ | 179/200 [1:10:39<08:12, 23.44s/it]Running Inference:  90%|█████████ | 180/200 [1:11:02<07:45, 23.28s/it]Running Inference:  90%|█████████ | 181/200 [1:11:25<07:21, 23.24s/it]Running Inference:  91%|█████████ | 182/200 [1:11:49<07:01, 23.41s/it]Running Inference:  92%|█████████▏| 183/200 [1:12:12<06:36, 23.35s/it]Running Inference:  92%|█████████▏| 184/200 [1:12:35<06:12, 23.30s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:58<05:47, 23.15s/it]Running Inference:  93%|█████████▎| 186/200 [1:13:21<05:24, 23.18s/it]Running Inference:  94%|█████████▎| 187/200 [1:13:44<05:00, 23.15s/it]Running Inference:  94%|█████████▍| 188/200 [1:14:08<04:39, 23.31s/it]Running Inference:  94%|█████████▍| 189/200 [1:14:31<04:15, 23.19s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:54<03:52, 23.21s/it]Running Inference:  96%|█████████▌| 191/200 [1:15:17<03:28, 23.12s/it]Running Inference:  96%|█████████▌| 192/200 [1:15:40<03:05, 23.13s/it]Running Inference:  96%|█████████▋| 193/200 [1:16:04<02:42, 23.20s/it]Running Inference:  97%|█████████▋| 194/200 [1:16:27<02:20, 23.37s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:53<01:59, 23.90s/it]Running Inference:  98%|█████████▊| 196/200 [1:17:16<01:35, 23.81s/it]Running Inference:  98%|█████████▊| 197/200 [1:17:40<01:11, 23.79s/it]Running Inference:  99%|█████████▉| 198/200 [1:18:03<00:47, 23.62s/it]Running Inference: 100%|█████████▉| 199/200 [1:18:26<00:23, 23.51s/it]Running Inference: 100%|██████████| 200/200 [1:18:50<00:00, 23.50s/it]Running Inference: 100%|██████████| 200/200 [1:18:50<00:00, 23.65s/it]
2025-12-14 14:34:28,604 - INFO - Inference completed.
2025-12-14 14:34:28,617 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 14:34:28,617 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.666 seconds.
Prefix dict has been built successfully.
2025-12-14 14:34:36,492 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 14:34:36,493 - INFO - Metrics:
10.61
2025-12-14 14:34:36,494 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: streaming_llm (task=vcsum, ratio=0.3) on GPU 2

----------------------------------------
Task: vcsum | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 14:34:43,571 - INFO - Set deterministic seeds to 42
2025-12-14 14:34:43,571 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "vcsum",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 14:34:43,571 - INFO - Starting evaluation run...
2025-12-14 14:34:43,571 - INFO - Output directory set to: longbenchresult
2025-12-14 14:34:43,571 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 14:34:43,571 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 14:34:43,571 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 14:34:43,571 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.05it/s]
Device set to use cuda:0
2025-12-14 14:34:57,667 - INFO - Model pipeline loaded.
2025-12-14 14:34:57,667 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: vcsum)
2025-12-14 14:35:01,846 - INFO - Dataset loaded with 200 entries.
2025-12-14 14:35:01,846 - INFO - Dataset processed with 200 entries.
2025-12-14 14:35:01,873 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:23<1:19:10, 23.87s/it]Running Inference:   1%|          | 2/200 [00:47<1:18:31, 23.80s/it]Running Inference:   2%|▏         | 3/200 [01:11<1:17:44, 23.68s/it]Running Inference:   2%|▏         | 4/200 [01:34<1:16:43, 23.48s/it]Running Inference:   2%|▎         | 5/200 [01:58<1:16:42, 23.60s/it]Running Inference:   3%|▎         | 6/200 [02:20<1:15:22, 23.31s/it]Running Inference:   4%|▎         | 7/200 [02:44<1:15:06, 23.35s/it]Running Inference:   4%|▍         | 8/200 [03:07<1:14:06, 23.16s/it]Running Inference:   4%|▍         | 9/200 [03:30<1:13:29, 23.09s/it]Running Inference:   5%|▌         | 10/200 [03:54<1:14:27, 23.52s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [04:17<1:13:59, 23.49s/it]Running Inference:   6%|▌         | 12/200 [04:40<1:12:52, 23.26s/it]Running Inference:   6%|▋         | 13/200 [05:04<1:13:06, 23.46s/it]Running Inference:   7%|▋         | 14/200 [05:27<1:11:56, 23.21s/it]Running Inference:   8%|▊         | 15/200 [05:49<1:11:10, 23.08s/it]Running Inference:   8%|▊         | 16/200 [06:14<1:11:48, 23.42s/it]Running Inference:   8%|▊         | 17/200 [06:36<1:10:50, 23.23s/it]Running Inference:   9%|▉         | 18/200 [07:01<1:12:01, 23.75s/it]Running Inference:  10%|▉         | 19/200 [07:25<1:11:15, 23.62s/it]Running Inference:  10%|█         | 20/200 [07:49<1:11:24, 23.80s/it]Running Inference:  10%|█         | 21/200 [08:13<1:10:56, 23.78s/it]Running Inference:  11%|█         | 22/200 [08:36<1:10:19, 23.71s/it]Running Inference:  12%|█▏        | 23/200 [08:59<1:09:20, 23.50s/it]Running Inference:  12%|█▏        | 24/200 [09:24<1:10:18, 23.97s/it]Running Inference:  12%|█▎        | 25/200 [09:48<1:09:13, 23.73s/it]Running Inference:  13%|█▎        | 26/200 [10:12<1:09:34, 23.99s/it]Running Inference:  14%|█▎        | 27/200 [10:36<1:08:42, 23.83s/it]Running Inference:  14%|█▍        | 28/200 [10:59<1:07:50, 23.67s/it]Running Inference:  14%|█▍        | 29/200 [11:22<1:07:23, 23.65s/it]Running Inference:  15%|█▌        | 30/200 [11:45<1:06:25, 23.44s/it]Running Inference:  16%|█▌        | 31/200 [12:08<1:05:41, 23.32s/it]Running Inference:  16%|█▌        | 32/200 [12:31<1:04:49, 23.15s/it]Running Inference:  16%|█▋        | 33/200 [12:54<1:04:02, 23.01s/it]Running Inference:  17%|█▋        | 34/200 [13:17<1:03:23, 22.91s/it]Running Inference:  18%|█▊        | 35/200 [13:40<1:03:35, 23.12s/it]Running Inference:  18%|█▊        | 36/200 [14:03<1:03:12, 23.12s/it]Running Inference:  18%|█▊        | 37/200 [14:26<1:02:43, 23.09s/it]Running Inference:  19%|█▉        | 38/200 [14:51<1:03:31, 23.53s/it]Running Inference:  20%|█▉        | 39/200 [15:14<1:03:12, 23.56s/it]Running Inference:  20%|██        | 40/200 [15:39<1:03:45, 23.91s/it]Running Inference:  20%|██        | 41/200 [16:05<1:04:59, 24.53s/it]Running Inference:  21%|██        | 42/200 [16:29<1:03:40, 24.18s/it]Running Inference:  22%|██▏       | 43/200 [16:52<1:02:50, 24.02s/it]Running Inference:  22%|██▏       | 44/200 [17:17<1:03:08, 24.28s/it]Running Inference:  22%|██▎       | 45/200 [17:40<1:01:56, 23.98s/it]Running Inference:  23%|██▎       | 46/200 [18:03<1:00:49, 23.70s/it]Running Inference:  24%|██▎       | 47/200 [18:28<1:01:09, 23.98s/it]Running Inference:  24%|██▍       | 48/200 [18:51<1:00:08, 23.74s/it]Running Inference:  24%|██▍       | 49/200 [19:14<59:12, 23.53s/it]  Running Inference:  25%|██▌       | 50/200 [19:37<58:25, 23.37s/it]Running Inference:  26%|██▌       | 51/200 [20:01<58:28, 23.55s/it]Running Inference:  26%|██▌       | 52/200 [20:25<58:23, 23.67s/it]Running Inference:  26%|██▋       | 53/200 [20:49<57:46, 23.58s/it]Running Inference:  27%|██▋       | 54/200 [21:12<57:11, 23.50s/it]Running Inference:  28%|██▊       | 55/200 [21:35<56:34, 23.41s/it]Running Inference:  28%|██▊       | 56/200 [21:58<56:01, 23.34s/it]Running Inference:  28%|██▊       | 57/200 [22:21<55:14, 23.17s/it]Running Inference:  29%|██▉       | 58/200 [22:45<55:15, 23.35s/it]Running Inference:  30%|██▉       | 59/200 [23:09<55:10, 23.48s/it]Running Inference:  30%|███       | 60/200 [23:33<55:14, 23.68s/it]Running Inference:  30%|███       | 61/200 [23:56<54:25, 23.49s/it]Running Inference:  31%|███       | 62/200 [24:19<53:41, 23.34s/it]Running Inference:  32%|███▏      | 63/200 [24:42<53:21, 23.37s/it]Running Inference:  32%|███▏      | 64/200 [25:06<53:12, 23.48s/it]Running Inference:  32%|███▎      | 65/200 [25:30<53:01, 23.56s/it]Running Inference:  33%|███▎      | 66/200 [25:53<52:13, 23.39s/it]Running Inference:  34%|███▎      | 67/200 [26:16<51:55, 23.43s/it]Running Inference:  34%|███▍      | 68/200 [26:40<51:33, 23.44s/it]Running Inference:  34%|███▍      | 69/200 [27:04<51:52, 23.76s/it]Running Inference:  35%|███▌      | 70/200 [27:27<51:04, 23.57s/it]Running Inference:  36%|███▌      | 71/200 [27:50<50:22, 23.43s/it]Running Inference:  36%|███▌      | 72/200 [28:14<50:22, 23.62s/it]Running Inference:  36%|███▋      | 73/200 [28:38<50:01, 23.63s/it]Running Inference:  37%|███▋      | 74/200 [29:01<49:22, 23.51s/it]Running Inference:  38%|███▊      | 75/200 [29:25<48:50, 23.45s/it]Running Inference:  38%|███▊      | 76/200 [29:48<48:13, 23.34s/it]Running Inference:  38%|███▊      | 77/200 [30:12<48:36, 23.71s/it]Running Inference:  39%|███▉      | 78/200 [30:35<47:45, 23.49s/it]Running Inference:  40%|███▉      | 79/200 [30:59<47:17, 23.45s/it]Running Inference:  40%|████      | 80/200 [31:22<46:42, 23.35s/it]Running Inference:  40%|████      | 81/200 [31:45<46:22, 23.38s/it]Running Inference:  41%|████      | 82/200 [32:09<45:57, 23.37s/it]Running Inference:  42%|████▏     | 83/200 [32:32<45:28, 23.32s/it]Running Inference:  42%|████▏     | 84/200 [32:56<45:32, 23.55s/it]Running Inference:  42%|████▎     | 85/200 [33:20<45:12, 23.58s/it]Running Inference:  43%|████▎     | 86/200 [33:44<45:03, 23.71s/it]Running Inference:  44%|████▎     | 87/200 [34:07<44:26, 23.60s/it]Running Inference:  44%|████▍     | 88/200 [34:30<43:53, 23.51s/it]Running Inference:  44%|████▍     | 89/200 [34:53<42:59, 23.24s/it]Running Inference:  45%|████▌     | 90/200 [35:16<42:33, 23.21s/it]Running Inference:  46%|████▌     | 91/200 [35:40<42:38, 23.47s/it]Running Inference:  46%|████▌     | 92/200 [36:03<41:51, 23.26s/it]Running Inference:  46%|████▋     | 93/200 [36:27<42:02, 23.58s/it]Running Inference:  47%|████▋     | 94/200 [36:52<42:08, 23.85s/it]Running Inference:  48%|████▊     | 95/200 [37:16<42:11, 24.11s/it]Running Inference:  48%|████▊     | 96/200 [37:39<41:13, 23.78s/it]Running Inference:  48%|████▊     | 97/200 [38:02<40:24, 23.54s/it]Running Inference:  49%|████▉     | 98/200 [38:26<40:05, 23.58s/it]Running Inference:  50%|████▉     | 99/200 [38:49<39:30, 23.47s/it]Running Inference:  50%|█████     | 100/200 [39:13<39:15, 23.56s/it]Running Inference:  50%|█████     | 101/200 [39:38<39:27, 23.91s/it]Running Inference:  51%|█████     | 102/200 [40:04<40:03, 24.53s/it]Running Inference:  52%|█████▏    | 103/200 [40:28<39:40, 24.54s/it]Running Inference:  52%|█████▏    | 104/200 [40:52<38:45, 24.22s/it]Running Inference:  52%|█████▎    | 105/200 [41:17<38:59, 24.62s/it]Running Inference:  53%|█████▎    | 106/200 [41:41<37:59, 24.25s/it]Running Inference:  54%|█████▎    | 107/200 [42:03<36:57, 23.84s/it]Running Inference:  54%|█████▍    | 108/200 [42:27<36:25, 23.76s/it]Running Inference:  55%|█████▍    | 109/200 [42:50<35:41, 23.53s/it]Running Inference:  55%|█████▌    | 110/200 [43:14<35:20, 23.57s/it]Running Inference:  56%|█████▌    | 111/200 [43:36<34:34, 23.31s/it]Running Inference:  56%|█████▌    | 112/200 [44:00<34:17, 23.38s/it]Running Inference:  56%|█████▋    | 113/200 [44:23<33:51, 23.35s/it]Running Inference:  57%|█████▋    | 114/200 [44:46<33:23, 23.30s/it]Running Inference:  57%|█████▊    | 115/200 [45:10<33:10, 23.41s/it]Running Inference:  58%|█████▊    | 116/200 [45:33<32:44, 23.39s/it]Running Inference:  58%|█████▊    | 117/200 [45:57<32:24, 23.43s/it]Running Inference:  59%|█████▉    | 118/200 [46:20<31:45, 23.23s/it]Running Inference:  60%|█████▉    | 119/200 [46:44<31:35, 23.40s/it]Running Inference:  60%|██████    | 120/200 [47:08<31:32, 23.66s/it]Running Inference:  60%|██████    | 121/200 [47:32<31:20, 23.81s/it]Running Inference:  61%|██████    | 122/200 [47:55<30:29, 23.45s/it]Running Inference:  62%|██████▏   | 123/200 [48:19<30:31, 23.79s/it]Running Inference:  62%|██████▏   | 124/200 [48:44<30:21, 23.97s/it]Running Inference:  62%|██████▎   | 125/200 [49:07<29:40, 23.74s/it]Running Inference:  63%|██████▎   | 126/200 [49:30<29:10, 23.66s/it]Running Inference:  64%|██████▎   | 127/200 [49:55<29:02, 23.86s/it]Running Inference:  64%|██████▍   | 128/200 [50:18<28:32, 23.79s/it]Running Inference:  64%|██████▍   | 129/200 [50:42<28:04, 23.72s/it]Running Inference:  65%|██████▌   | 130/200 [51:04<27:18, 23.41s/it]Running Inference:  66%|██████▌   | 131/200 [51:28<26:55, 23.42s/it]Running Inference:  66%|██████▌   | 132/200 [51:51<26:27, 23.34s/it]Running Inference:  66%|██████▋   | 133/200 [52:15<26:13, 23.49s/it]Running Inference:  67%|██████▋   | 134/200 [52:38<25:38, 23.32s/it]Running Inference:  68%|██████▊   | 135/200 [53:01<25:14, 23.29s/it]Running Inference:  68%|██████▊   | 136/200 [53:24<24:40, 23.13s/it]Running Inference:  68%|██████▊   | 137/200 [53:47<24:22, 23.22s/it]Running Inference:  69%|██████▉   | 138/200 [54:10<23:50, 23.08s/it]Running Inference:  70%|██████▉   | 139/200 [54:34<23:48, 23.41s/it]Running Inference:  70%|███████   | 140/200 [54:58<23:38, 23.64s/it]Running Inference:  70%|███████   | 141/200 [55:22<23:08, 23.54s/it]Running Inference:  71%|███████   | 142/200 [55:45<22:41, 23.48s/it]Running Inference:  72%|███████▏  | 143/200 [56:09<22:24, 23.59s/it]Running Inference:  72%|███████▏  | 144/200 [56:32<21:54, 23.47s/it]Running Inference:  72%|███████▎  | 145/200 [56:57<21:51, 23.85s/it]Running Inference:  73%|███████▎  | 146/200 [57:20<21:17, 23.67s/it]Running Inference:  74%|███████▎  | 147/200 [57:43<20:42, 23.45s/it]Running Inference:  74%|███████▍  | 148/200 [58:06<20:13, 23.35s/it]Running Inference:  74%|███████▍  | 149/200 [58:30<19:54, 23.43s/it]Running Inference:  75%|███████▌  | 150/200 [58:54<19:45, 23.72s/it]Running Inference:  76%|███████▌  | 151/200 [59:18<19:29, 23.87s/it]Running Inference:  76%|███████▌  | 152/200 [59:42<19:11, 23.99s/it]Running Inference:  76%|███████▋  | 153/200 [1:00:06<18:36, 23.76s/it]Running Inference:  77%|███████▋  | 154/200 [1:00:28<17:58, 23.44s/it]Running Inference:  78%|███████▊  | 155/200 [1:00:51<17:29, 23.32s/it]Running Inference:  78%|███████▊  | 156/200 [1:01:14<17:00, 23.18s/it]Running Inference:  78%|███████▊  | 157/200 [1:01:37<16:36, 23.18s/it]Running Inference:  79%|███████▉  | 158/200 [1:02:01<16:16, 23.25s/it]Running Inference:  80%|███████▉  | 159/200 [1:02:24<15:45, 23.07s/it]Running Inference:  80%|████████  | 160/200 [1:02:48<15:41, 23.53s/it]Running Inference:  80%|████████  | 161/200 [1:03:11<15:11, 23.38s/it]Running Inference:  81%|████████  | 162/200 [1:03:35<14:58, 23.64s/it]Running Inference:  82%|████████▏ | 163/200 [1:04:01<14:51, 24.09s/it]Running Inference:  82%|████████▏ | 164/200 [1:04:24<14:18, 23.84s/it]Running Inference:  82%|████████▎ | 165/200 [1:04:48<14:01, 24.05s/it]Running Inference:  83%|████████▎ | 166/200 [1:05:11<13:24, 23.67s/it]Running Inference:  84%|████████▎ | 167/200 [1:05:34<12:55, 23.49s/it]Running Inference:  84%|████████▍ | 168/200 [1:05:57<12:29, 23.42s/it]Running Inference:  84%|████████▍ | 169/200 [1:06:20<12:01, 23.29s/it]Running Inference:  85%|████████▌ | 170/200 [1:06:44<11:38, 23.28s/it]Running Inference:  86%|████████▌ | 171/200 [1:07:06<11:09, 23.10s/it]Running Inference:  86%|████████▌ | 172/200 [1:07:30<10:49, 23.20s/it]Running Inference:  86%|████████▋ | 173/200 [1:07:54<10:37, 23.60s/it]Running Inference:  87%|████████▋ | 174/200 [1:08:17<10:10, 23.46s/it]Running Inference:  88%|████████▊ | 175/200 [1:08:41<09:45, 23.41s/it]Running Inference:  88%|████████▊ | 176/200 [1:09:04<09:18, 23.29s/it]Running Inference:  88%|████████▊ | 177/200 [1:09:27<08:52, 23.14s/it]Running Inference:  89%|████████▉ | 178/200 [1:09:49<08:26, 23.00s/it]Running Inference:  90%|████████▉ | 179/200 [1:10:13<08:08, 23.24s/it]Running Inference:  90%|█████████ | 180/200 [1:10:36<07:41, 23.08s/it]Running Inference:  90%|█████████ | 181/200 [1:10:59<07:17, 23.05s/it]Running Inference:  91%|█████████ | 182/200 [1:11:22<06:57, 23.20s/it]Running Inference:  92%|█████████▏| 183/200 [1:11:45<06:33, 23.15s/it]Running Inference:  92%|█████████▏| 184/200 [1:12:08<06:09, 23.10s/it]Running Inference:  92%|█████████▎| 185/200 [1:12:31<05:44, 22.96s/it]Running Inference:  93%|█████████▎| 186/200 [1:12:54<05:21, 22.99s/it]Running Inference:  94%|█████████▎| 187/200 [1:13:17<04:58, 22.96s/it]Running Inference:  94%|█████████▍| 188/200 [1:13:40<04:37, 23.12s/it]Running Inference:  94%|█████████▍| 189/200 [1:14:03<04:12, 22.98s/it]Running Inference:  95%|█████████▌| 190/200 [1:14:26<03:50, 23.01s/it]Running Inference:  96%|█████████▌| 191/200 [1:14:49<03:26, 22.91s/it]Running Inference:  96%|█████████▌| 192/200 [1:15:12<03:03, 22.92s/it]Running Inference:  96%|█████████▋| 193/200 [1:15:35<02:40, 23.00s/it]Running Inference:  97%|█████████▋| 194/200 [1:15:58<02:18, 23.16s/it]Running Inference:  98%|█████████▊| 195/200 [1:16:23<01:58, 23.69s/it]Running Inference:  98%|█████████▊| 196/200 [1:16:47<01:34, 23.66s/it]Running Inference:  98%|█████████▊| 197/200 [1:17:10<01:10, 23.63s/it]Running Inference:  99%|█████████▉| 198/200 [1:17:33<00:46, 23.44s/it]Running Inference: 100%|█████████▉| 199/200 [1:17:57<00:23, 23.32s/it]Running Inference: 100%|██████████| 200/200 [1:18:20<00:00, 23.29s/it]Running Inference: 100%|██████████| 200/200 [1:18:20<00:00, 23.50s/it]
2025-12-14 15:53:22,143 - INFO - Inference completed.
2025-12-14 15:53:22,156 - INFO - Results saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 15:53:22,157 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.657 seconds.
Prefix dict has been built successfully.
2025-12-14 15:53:30,029 - INFO - Metrics saved to longbenchresult/longbench__vcsum__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 15:53:30,029 - INFO - Metrics:
10.12
2025-12-14 15:53:30,030 - INFO - Evaluation run completed successfully.
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
An error occurred: maximum recursion depth exceeded
✓ Completed: streaming_llm (task=vcsum, ratio=0.5) on GPU 2


========================================
LongBench Task: passage_count
========================================
----------------------------------------
Task: passage_count | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 15:53:37,196 - INFO - Set deterministic seeds to 42
2025-12-14 15:53:37,196 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 15:53:37,196 - INFO - Starting evaluation run...
2025-12-14 15:53:37,196 - INFO - Output directory set to: longbenchresult
2025-12-14 15:53:37,196 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 15:53:37,196 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 15:53:37,196 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 15:53:37,196 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.83it/s]
Device set to use cuda:0
2025-12-14 15:53:51,638 - INFO - Model pipeline loaded.
2025-12-14 15:53:51,638 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 15:54:00,649 - INFO - Dataset loaded with 200 entries.
2025-12-14 15:54:00,649 - INFO - Dataset processed with 200 entries.
2025-12-14 15:54:00,686 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:07,  1.85s/it]Running Inference:   1%|          | 2/200 [00:04<06:55,  2.10s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:06,  1.86s/it]Running Inference:   2%|▏         | 4/200 [00:08<07:32,  2.31s/it]Running Inference:   2%|▎         | 5/200 [00:11<07:31,  2.32s/it]Running Inference:   3%|▎         | 6/200 [00:12<06:56,  2.15s/it]Running Inference:   4%|▎         | 7/200 [00:14<05:57,  1.85s/it]Running Inference:   4%|▍         | 8/200 [00:14<04:57,  1.55s/it]Running Inference:   4%|▍         | 9/200 [00:17<06:13,  1.96s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:24,  2.02s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:33,  2.40s/it]Running Inference:   6%|▌         | 12/200 [00:25<07:33,  2.41s/it]Running Inference:   6%|▋         | 13/200 [00:27<06:39,  2.14s/it]Running Inference:   7%|▋         | 14/200 [00:28<05:32,  1.79s/it]Running Inference:   8%|▊         | 15/200 [00:31<06:55,  2.25s/it]Running Inference:   8%|▊         | 16/200 [00:33<06:33,  2.14s/it]Running Inference:   8%|▊         | 17/200 [00:34<05:56,  1.95s/it]Running Inference:   9%|▉         | 18/200 [00:36<05:43,  1.89s/it]Running Inference:  10%|▉         | 19/200 [00:38<05:30,  1.83s/it]Running Inference:  10%|█         | 20/200 [00:40<05:43,  1.91s/it]Running Inference:  10%|█         | 21/200 [00:43<06:43,  2.26s/it]Running Inference:  11%|█         | 22/200 [00:45<06:33,  2.21s/it]Running Inference:  12%|█▏        | 23/200 [00:48<06:52,  2.33s/it]Running Inference:  12%|█▏        | 24/200 [00:50<06:55,  2.36s/it]Running Inference:  12%|█▎        | 25/200 [00:52<06:47,  2.33s/it]Running Inference:  13%|█▎        | 26/200 [00:55<06:39,  2.29s/it]Running Inference:  14%|█▎        | 27/200 [00:56<06:12,  2.15s/it]Running Inference:  14%|█▍        | 28/200 [00:59<06:24,  2.24s/it]Running Inference:  14%|█▍        | 29/200 [01:02<07:12,  2.53s/it]Running Inference:  15%|█▌        | 30/200 [01:03<06:06,  2.16s/it]Running Inference:  16%|█▌        | 31/200 [01:05<05:29,  1.95s/it]Running Inference:  16%|█▌        | 32/200 [01:08<06:46,  2.42s/it]Running Inference:  16%|█▋        | 33/200 [01:09<05:27,  1.96s/it]Running Inference:  17%|█▋        | 34/200 [01:12<05:48,  2.10s/it]Running Inference:  18%|█▊        | 35/200 [01:14<05:54,  2.15s/it]Running Inference:  18%|█▊        | 36/200 [01:16<06:05,  2.23s/it]Running Inference:  18%|█▊        | 37/200 [01:18<05:45,  2.12s/it]Running Inference:  19%|█▉        | 38/200 [01:20<05:19,  1.97s/it]Running Inference:  20%|█▉        | 39/200 [01:22<05:44,  2.14s/it]Running Inference:  20%|██        | 40/200 [01:24<05:22,  2.02s/it]Running Inference:  20%|██        | 41/200 [01:26<05:17,  2.00s/it]Running Inference:  21%|██        | 42/200 [01:27<04:34,  1.73s/it]Running Inference:  22%|██▏       | 43/200 [01:29<04:52,  1.86s/it]Running Inference:  22%|██▏       | 44/200 [01:31<04:53,  1.88s/it]Running Inference:  22%|██▎       | 45/200 [01:34<05:35,  2.16s/it]Running Inference:  23%|██▎       | 46/200 [01:36<05:40,  2.21s/it]Running Inference:  24%|██▎       | 47/200 [01:38<04:48,  1.88s/it]Running Inference:  24%|██▍       | 48/200 [01:38<04:00,  1.58s/it]Running Inference:  24%|██▍       | 49/200 [01:40<03:58,  1.58s/it]Running Inference:  25%|██▌       | 50/200 [01:41<03:36,  1.44s/it]Running Inference:  26%|██▌       | 51/200 [01:43<04:15,  1.71s/it]Running Inference:  26%|██▌       | 52/200 [01:45<03:59,  1.62s/it]Running Inference:  26%|██▋       | 53/200 [01:48<04:47,  1.96s/it]Running Inference:  27%|██▋       | 54/200 [01:50<04:57,  2.04s/it]Running Inference:  28%|██▊       | 55/200 [01:51<04:11,  1.73s/it]Running Inference:  28%|██▊       | 56/200 [01:53<04:15,  1.78s/it]Running Inference:  28%|██▊       | 57/200 [01:55<04:27,  1.87s/it]Running Inference:  29%|██▉       | 58/200 [01:57<04:36,  1.94s/it]Running Inference:  30%|██▉       | 59/200 [02:00<05:39,  2.40s/it]Running Inference:  30%|███       | 60/200 [02:03<05:55,  2.54s/it]Running Inference:  30%|███       | 61/200 [02:05<05:09,  2.22s/it]Running Inference:  31%|███       | 62/200 [02:06<04:16,  1.86s/it]Running Inference:  32%|███▏      | 63/200 [02:08<04:16,  1.87s/it]Running Inference:  32%|███▏      | 64/200 [02:10<04:46,  2.11s/it]Running Inference:  32%|███▎      | 65/200 [02:13<04:54,  2.18s/it]Running Inference:  33%|███▎      | 66/200 [02:15<05:00,  2.24s/it]Running Inference:  34%|███▎      | 67/200 [02:17<04:36,  2.08s/it]Running Inference:  34%|███▍      | 68/200 [02:19<04:35,  2.09s/it]Running Inference:  34%|███▍      | 69/200 [02:22<05:20,  2.45s/it]Running Inference:  35%|███▌      | 70/200 [02:24<05:02,  2.33s/it]Running Inference:  36%|███▌      | 71/200 [02:27<05:10,  2.41s/it]Running Inference:  36%|███▌      | 72/200 [02:29<04:52,  2.28s/it]Running Inference:  36%|███▋      | 73/200 [02:32<05:21,  2.53s/it]Running Inference:  37%|███▋      | 74/200 [02:34<05:13,  2.49s/it]Running Inference:  38%|███▊      | 75/200 [02:36<04:38,  2.23s/it]Running Inference:  38%|███▊      | 76/200 [02:37<03:58,  1.92s/it]Running Inference:  38%|███▊      | 77/200 [02:41<05:21,  2.61s/it]Running Inference:  39%|███▉      | 78/200 [02:43<04:57,  2.44s/it]Running Inference:  40%|███▉      | 79/200 [02:46<04:50,  2.40s/it]Running Inference:  40%|████      | 80/200 [02:48<04:48,  2.40s/it]Running Inference:  40%|████      | 81/200 [02:49<04:07,  2.08s/it]Running Inference:  41%|████      | 82/200 [02:51<03:32,  1.80s/it]Running Inference:  42%|████▏     | 83/200 [02:53<03:49,  1.97s/it]Running Inference:  42%|████▏     | 84/200 [02:55<03:47,  1.96s/it]Running Inference:  42%|████▎     | 85/200 [02:58<04:16,  2.23s/it]Running Inference:  43%|████▎     | 86/200 [03:00<04:15,  2.24s/it]Running Inference:  44%|████▎     | 87/200 [03:02<04:02,  2.15s/it]Running Inference:  44%|████▍     | 88/200 [03:04<04:02,  2.16s/it]Running Inference:  44%|████▍     | 89/200 [03:07<04:21,  2.36s/it]Running Inference:  45%|████▌     | 90/200 [03:10<04:31,  2.47s/it]Running Inference:  46%|████▌     | 91/200 [03:11<03:46,  2.08s/it]Running Inference:  46%|████▌     | 92/200 [03:12<03:07,  1.73s/it]Running Inference:  46%|████▋     | 93/200 [03:13<03:00,  1.69s/it]Running Inference:  47%|████▋     | 94/200 [03:15<03:03,  1.73s/it]Running Inference:  48%|████▊     | 95/200 [03:17<03:15,  1.87s/it]Running Inference:  48%|████▊     | 96/200 [03:19<03:15,  1.88s/it]Running Inference:  48%|████▊     | 97/200 [03:22<03:30,  2.04s/it]Running Inference:  49%|████▉     | 98/200 [03:25<04:14,  2.50s/it]Running Inference:  50%|████▉     | 99/200 [03:28<04:24,  2.62s/it]Running Inference:  50%|█████     | 100/200 [03:30<03:53,  2.34s/it]Running Inference:  50%|█████     | 101/200 [03:31<03:21,  2.03s/it]Running Inference:  51%|█████     | 102/200 [03:33<03:09,  1.93s/it]Running Inference:  52%|█████▏    | 103/200 [03:36<03:42,  2.30s/it]Running Inference:  52%|█████▏    | 104/200 [03:38<03:43,  2.32s/it]Running Inference:  52%|█████▎    | 105/200 [03:39<02:53,  1.83s/it]Running Inference:  53%|█████▎    | 106/200 [03:41<03:10,  2.02s/it]Running Inference:  54%|█████▎    | 107/200 [03:45<03:38,  2.35s/it]Running Inference:  54%|█████▍    | 108/200 [03:47<03:27,  2.25s/it]Running Inference:  55%|█████▍    | 109/200 [03:49<03:41,  2.44s/it]Running Inference:  55%|█████▌    | 110/200 [03:51<03:13,  2.15s/it]Running Inference:  56%|█████▌    | 111/200 [03:52<02:45,  1.86s/it]Running Inference:  56%|█████▌    | 112/200 [03:53<02:20,  1.60s/it]Running Inference:  56%|█████▋    | 113/200 [03:55<02:16,  1.57s/it]Running Inference:  57%|█████▋    | 114/200 [03:56<02:10,  1.52s/it]Running Inference:  57%|█████▊    | 115/200 [03:59<02:36,  1.84s/it]Running Inference:  58%|█████▊    | 116/200 [04:01<02:49,  2.02s/it]Running Inference:  58%|█████▊    | 117/200 [04:03<02:34,  1.86s/it]Running Inference:  59%|█████▉    | 118/200 [04:04<02:30,  1.83s/it]Running Inference:  60%|█████▉    | 119/200 [04:06<02:27,  1.82s/it]Running Inference:  60%|██████    | 120/200 [04:08<02:37,  1.97s/it]Running Inference:  60%|██████    | 121/200 [04:12<03:07,  2.37s/it]Running Inference:  61%|██████    | 122/200 [04:15<03:24,  2.62s/it]Running Inference:  62%|██████▏   | 123/200 [04:16<02:42,  2.11s/it]Running Inference:  62%|██████▏   | 124/200 [04:18<02:49,  2.23s/it]Running Inference:  62%|██████▎   | 125/200 [04:20<02:38,  2.11s/it]Running Inference:  63%|██████▎   | 126/200 [04:22<02:28,  2.00s/it]Running Inference:  64%|██████▎   | 127/200 [04:24<02:34,  2.11s/it]Running Inference:  64%|██████▍   | 128/200 [04:26<02:32,  2.12s/it]Running Inference:  64%|██████▍   | 129/200 [04:29<02:45,  2.33s/it]Running Inference:  65%|██████▌   | 130/200 [04:30<02:16,  1.94s/it]Running Inference:  66%|██████▌   | 131/200 [04:34<02:45,  2.39s/it]Running Inference:  66%|██████▌   | 132/200 [04:37<02:53,  2.55s/it]Running Inference:  66%|██████▋   | 133/200 [04:38<02:24,  2.16s/it]Running Inference:  67%|██████▋   | 134/200 [04:41<02:34,  2.34s/it]Running Inference:  68%|██████▊   | 135/200 [04:42<02:19,  2.15s/it]Running Inference:  68%|██████▊   | 136/200 [04:44<02:10,  2.04s/it]Running Inference:  68%|██████▊   | 137/200 [04:46<02:13,  2.12s/it]Running Inference:  69%|██████▉   | 138/200 [04:48<02:07,  2.06s/it]Running Inference:  70%|██████▉   | 139/200 [04:52<02:26,  2.41s/it]Running Inference:  70%|███████   | 140/200 [04:54<02:30,  2.51s/it]Running Inference:  70%|███████   | 141/200 [04:56<02:13,  2.26s/it]Running Inference:  71%|███████   | 142/200 [04:59<02:14,  2.32s/it]Running Inference:  72%|███████▏  | 143/200 [05:02<02:30,  2.63s/it]Running Inference:  72%|███████▏  | 144/200 [05:04<02:15,  2.42s/it]Running Inference:  72%|███████▎  | 145/200 [05:06<02:11,  2.39s/it]Running Inference:  73%|███████▎  | 146/200 [05:08<02:07,  2.35s/it]Running Inference:  74%|███████▎  | 147/200 [05:10<01:58,  2.24s/it]Running Inference:  74%|███████▍  | 148/200 [05:13<01:57,  2.26s/it]Running Inference:  74%|███████▍  | 149/200 [05:16<02:10,  2.56s/it]Running Inference:  75%|███████▌  | 150/200 [05:17<01:50,  2.21s/it]Running Inference:  76%|███████▌  | 151/200 [05:18<01:28,  1.81s/it]Running Inference:  76%|███████▌  | 152/200 [05:20<01:34,  1.96s/it]Running Inference:  76%|███████▋  | 153/200 [05:21<01:13,  1.57s/it]Running Inference:  77%|███████▋  | 154/200 [05:24<01:26,  1.89s/it]Running Inference:  78%|███████▊  | 155/200 [05:25<01:20,  1.78s/it]Running Inference:  78%|███████▊  | 156/200 [05:27<01:20,  1.82s/it]Running Inference:  78%|███████▊  | 157/200 [05:28<01:09,  1.63s/it]Running Inference:  79%|███████▉  | 158/200 [05:30<01:13,  1.76s/it]Running Inference:  80%|███████▉  | 159/200 [05:32<01:13,  1.79s/it]Running Inference:  80%|████████  | 160/200 [05:36<01:31,  2.28s/it]Running Inference:  80%|████████  | 161/200 [05:38<01:31,  2.35s/it]Running Inference:  81%|████████  | 162/200 [05:40<01:26,  2.27s/it]Running Inference:  82%|████████▏ | 163/200 [05:43<01:26,  2.34s/it]Running Inference:  82%|████████▏ | 164/200 [05:45<01:17,  2.14s/it]Running Inference:  82%|████████▎ | 165/200 [05:47<01:16,  2.20s/it]Running Inference:  83%|████████▎ | 166/200 [05:50<01:24,  2.48s/it]Running Inference:  84%|████████▎ | 167/200 [05:52<01:13,  2.22s/it]Running Inference:  84%|████████▍ | 168/200 [05:54<01:16,  2.38s/it]Running Inference:  84%|████████▍ | 169/200 [05:55<00:58,  1.89s/it]Running Inference:  85%|████████▌ | 170/200 [05:57<01:00,  2.01s/it]Running Inference:  86%|████████▌ | 171/200 [06:00<01:00,  2.10s/it]Running Inference:  86%|████████▌ | 172/200 [06:01<00:50,  1.81s/it]Running Inference:  86%|████████▋ | 173/200 [06:03<00:51,  1.92s/it]Running Inference:  87%|████████▋ | 174/200 [06:05<00:48,  1.85s/it]Running Inference:  88%|████████▊ | 175/200 [06:07<00:49,  1.97s/it]Running Inference:  88%|████████▊ | 176/200 [06:09<00:45,  1.90s/it]Running Inference:  88%|████████▊ | 177/200 [06:10<00:41,  1.81s/it]Running Inference:  89%|████████▉ | 178/200 [06:13<00:43,  2.00s/it]Running Inference:  90%|████████▉ | 179/200 [06:15<00:45,  2.16s/it]Running Inference:  90%|█████████ | 180/200 [06:17<00:43,  2.16s/it]Running Inference:  90%|█████████ | 181/200 [06:19<00:37,  1.95s/it]Running Inference:  91%|█████████ | 182/200 [06:21<00:35,  1.99s/it]Running Inference:  92%|█████████▏| 183/200 [06:22<00:29,  1.74s/it]Running Inference:  92%|█████████▏| 184/200 [06:26<00:35,  2.23s/it]Running Inference:  92%|█████████▎| 185/200 [06:27<00:31,  2.11s/it]Running Inference:  93%|█████████▎| 186/200 [06:30<00:33,  2.37s/it]Running Inference:  94%|█████████▎| 187/200 [06:33<00:32,  2.51s/it]Running Inference:  94%|█████████▍| 188/200 [06:36<00:30,  2.52s/it]Running Inference:  94%|█████████▍| 189/200 [06:37<00:22,  2.08s/it]Running Inference:  95%|█████████▌| 190/200 [06:40<00:23,  2.36s/it]Running Inference:  96%|█████████▌| 191/200 [06:43<00:23,  2.56s/it]Running Inference:  96%|█████████▌| 192/200 [06:44<00:16,  2.12s/it]Running Inference:  96%|█████████▋| 193/200 [06:45<00:13,  1.87s/it]Running Inference:  97%|█████████▋| 194/200 [06:48<00:12,  2.10s/it]Running Inference:  98%|█████████▊| 195/200 [06:50<00:10,  2.15s/it]Running Inference:  98%|█████████▊| 196/200 [06:51<00:07,  1.92s/it]Running Inference:  98%|█████████▊| 197/200 [06:53<00:05,  1.76s/it]Running Inference:  99%|█████████▉| 198/200 [06:54<00:03,  1.60s/it]Running Inference: 100%|█████████▉| 199/200 [06:56<00:01,  1.55s/it]Running Inference: 100%|██████████| 200/200 [06:57<00:00,  1.59s/it]Running Inference: 100%|██████████| 200/200 [06:57<00:00,  2.09s/it]
2025-12-14 16:00:58,426 - INFO - Inference completed.
2025-12-14 16:00:58,434 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 16:00:58,434 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:00:58,435 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 16:00:58,435 - INFO - Metrics:
9.0
2025-12-14 16:00:58,436 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_count, ratio=0.1) on GPU 2

----------------------------------------
Task: passage_count | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:01:04,915 - INFO - Set deterministic seeds to 42
2025-12-14 16:01:04,915 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:01:04,915 - INFO - Starting evaluation run...
2025-12-14 16:01:04,915 - INFO - Output directory set to: longbenchresult
2025-12-14 16:01:04,915 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 16:01:04,915 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:01:04,915 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:01:04,915 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.17it/s]
Device set to use cuda:0
2025-12-14 16:01:18,385 - INFO - Model pipeline loaded.
2025-12-14 16:01:18,385 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 16:01:27,051 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:01:27,051 - INFO - Dataset processed with 200 entries.
2025-12-14 16:01:27,088 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:05,  1.84s/it]Running Inference:   1%|          | 2/200 [00:04<06:54,  2.09s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:06,  1.86s/it]Running Inference:   2%|▏         | 4/200 [00:08<07:29,  2.29s/it]Running Inference:   2%|▎         | 5/200 [00:13<09:57,  3.06s/it]Running Inference:   3%|▎         | 6/200 [00:14<08:32,  2.64s/it]Running Inference:   4%|▎         | 7/200 [00:16<07:01,  2.19s/it]Running Inference:   4%|▍         | 8/200 [00:17<05:40,  1.77s/it]Running Inference:   4%|▍         | 9/200 [00:19<06:46,  2.13s/it]Running Inference:   5%|▌         | 10/200 [00:22<06:46,  2.14s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:25<07:48,  2.48s/it]Running Inference:   6%|▌         | 12/200 [00:29<09:43,  3.10s/it]Running Inference:   6%|▋         | 13/200 [00:31<08:10,  2.62s/it]Running Inference:   7%|▋         | 14/200 [00:32<06:35,  2.13s/it]Running Inference:   8%|▊         | 15/200 [00:35<07:38,  2.48s/it]Running Inference:   8%|▊         | 16/200 [00:37<07:05,  2.31s/it]Running Inference:   8%|▊         | 17/200 [00:39<06:19,  2.07s/it]Running Inference:   9%|▉         | 18/200 [00:40<05:58,  1.97s/it]Running Inference:  10%|▉         | 19/200 [00:42<05:43,  1.90s/it]Running Inference:  10%|█         | 20/200 [00:44<05:52,  1.96s/it]Running Inference:  10%|█         | 21/200 [00:47<06:50,  2.29s/it]Running Inference:  11%|█         | 22/200 [00:49<06:38,  2.24s/it]Running Inference:  12%|█▏        | 23/200 [00:52<06:55,  2.35s/it]Running Inference:  12%|█▏        | 24/200 [00:54<06:56,  2.37s/it]Running Inference:  12%|█▎        | 25/200 [00:57<06:50,  2.35s/it]Running Inference:  13%|█▎        | 26/200 [00:59<06:41,  2.31s/it]Running Inference:  14%|█▎        | 27/200 [01:01<06:13,  2.16s/it]Running Inference:  14%|█▍        | 28/200 [01:03<06:25,  2.24s/it]Running Inference:  14%|█▍        | 29/200 [01:06<07:12,  2.53s/it]Running Inference:  15%|█▌        | 30/200 [01:08<06:06,  2.16s/it]Running Inference:  16%|█▌        | 31/200 [01:09<05:28,  1.94s/it]Running Inference:  16%|█▌        | 32/200 [01:13<06:43,  2.40s/it]Running Inference:  16%|█▋        | 33/200 [01:13<05:24,  1.95s/it]Running Inference:  17%|█▋        | 34/200 [01:16<05:46,  2.09s/it]Running Inference:  18%|█▊        | 35/200 [01:18<05:52,  2.14s/it]Running Inference:  18%|█▊        | 36/200 [01:21<06:04,  2.22s/it]Running Inference:  18%|█▊        | 37/200 [01:22<05:44,  2.11s/it]Running Inference:  19%|█▉        | 38/200 [01:24<05:18,  1.97s/it]Running Inference:  20%|█▉        | 39/200 [01:27<05:43,  2.14s/it]Running Inference:  20%|██        | 40/200 [01:28<05:22,  2.01s/it]Running Inference:  20%|██        | 41/200 [01:30<05:17,  2.00s/it]Running Inference:  21%|██        | 42/200 [01:31<04:33,  1.73s/it]Running Inference:  22%|██▏       | 43/200 [01:34<04:52,  1.86s/it]Running Inference:  22%|██▏       | 44/200 [01:35<04:52,  1.88s/it]Running Inference:  22%|██▎       | 45/200 [01:38<05:34,  2.16s/it]Running Inference:  23%|██▎       | 46/200 [01:41<05:39,  2.21s/it]Running Inference:  24%|██▎       | 47/200 [01:42<04:49,  1.89s/it]Running Inference:  24%|██▍       | 48/200 [01:43<04:02,  1.59s/it]Running Inference:  24%|██▍       | 49/200 [01:44<03:58,  1.58s/it]Running Inference:  25%|██▌       | 50/200 [01:45<03:36,  1.44s/it]Running Inference:  26%|██▌       | 51/200 [01:48<04:14,  1.71s/it]Running Inference:  26%|██▌       | 52/200 [01:49<03:58,  1.61s/it]Running Inference:  26%|██▋       | 53/200 [01:52<04:49,  1.97s/it]Running Inference:  27%|██▋       | 54/200 [01:54<04:56,  2.03s/it]Running Inference:  28%|██▊       | 55/200 [01:55<04:10,  1.73s/it]Running Inference:  28%|██▊       | 56/200 [01:57<04:14,  1.77s/it]Running Inference:  28%|██▊       | 57/200 [01:59<04:25,  1.86s/it]Running Inference:  29%|██▉       | 58/200 [02:01<04:35,  1.94s/it]Running Inference:  30%|██▉       | 59/200 [02:05<05:38,  2.40s/it]Running Inference:  30%|███       | 60/200 [02:07<05:54,  2.53s/it]Running Inference:  30%|███       | 61/200 [02:09<05:08,  2.22s/it]Running Inference:  31%|███       | 62/200 [02:12<05:42,  2.48s/it]Running Inference:  32%|███▏      | 63/200 [02:14<05:16,  2.31s/it]Running Inference:  32%|███▏      | 64/200 [02:17<05:27,  2.41s/it]Running Inference:  32%|███▎      | 65/200 [02:19<05:22,  2.39s/it]Running Inference:  33%|███▎      | 66/200 [02:21<05:19,  2.39s/it]Running Inference:  34%|███▎      | 67/200 [02:23<04:49,  2.17s/it]Running Inference:  34%|███▍      | 68/200 [02:25<04:44,  2.15s/it]Running Inference:  34%|███▍      | 69/200 [02:28<05:26,  2.49s/it]Running Inference:  35%|███▌      | 70/200 [02:30<05:06,  2.35s/it]Running Inference:  36%|███▌      | 71/200 [02:33<05:13,  2.43s/it]Running Inference:  36%|███▌      | 72/200 [02:35<04:53,  2.29s/it]Running Inference:  36%|███▋      | 73/200 [02:38<05:24,  2.55s/it]Running Inference:  37%|███▋      | 74/200 [02:40<05:15,  2.50s/it]Running Inference:  38%|███▊      | 75/200 [02:42<04:38,  2.23s/it]Running Inference:  38%|███▊      | 76/200 [02:43<03:58,  1.92s/it]Running Inference:  38%|███▊      | 77/200 [02:47<05:21,  2.61s/it]Running Inference:  39%|███▉      | 78/200 [02:50<04:57,  2.44s/it]Running Inference:  40%|███▉      | 79/200 [02:52<04:49,  2.40s/it]Running Inference:  40%|████      | 80/200 [02:54<04:47,  2.40s/it]Running Inference:  40%|████      | 81/200 [02:56<04:07,  2.08s/it]Running Inference:  41%|████      | 82/200 [02:57<03:32,  1.80s/it]Running Inference:  42%|████▏     | 83/200 [02:59<03:49,  1.96s/it]Running Inference:  42%|████▏     | 84/200 [03:01<03:46,  1.95s/it]Running Inference:  42%|████▎     | 85/200 [03:04<04:15,  2.22s/it]Running Inference:  43%|████▎     | 86/200 [03:06<04:14,  2.24s/it]Running Inference:  44%|████▎     | 87/200 [03:08<04:01,  2.14s/it]Running Inference:  44%|████▍     | 88/200 [03:10<04:01,  2.16s/it]Running Inference:  44%|████▍     | 89/200 [03:13<04:20,  2.35s/it]Running Inference:  45%|████▌     | 90/200 [03:16<04:30,  2.46s/it]Running Inference:  46%|████▌     | 91/200 [03:17<03:45,  2.07s/it]Running Inference:  46%|████▌     | 92/200 [03:18<03:06,  1.73s/it]Running Inference:  46%|████▋     | 93/200 [03:19<03:00,  1.68s/it]Running Inference:  47%|████▋     | 94/200 [03:21<03:02,  1.72s/it]Running Inference:  48%|████▊     | 95/200 [03:23<03:15,  1.86s/it]Running Inference:  48%|████▊     | 96/200 [03:25<03:15,  1.88s/it]Running Inference:  48%|████▊     | 97/200 [03:28<03:30,  2.05s/it]Running Inference:  49%|████▉     | 98/200 [03:31<04:15,  2.50s/it]Running Inference:  50%|████▉     | 99/200 [03:34<04:24,  2.62s/it]Running Inference:  50%|█████     | 100/200 [03:36<03:53,  2.33s/it]Running Inference:  50%|█████     | 101/200 [03:37<03:20,  2.03s/it]Running Inference:  51%|█████     | 102/200 [03:39<03:07,  1.91s/it]Running Inference:  52%|█████▏    | 103/200 [03:42<03:40,  2.27s/it]Running Inference:  52%|█████▏    | 104/200 [03:44<03:41,  2.31s/it]Running Inference:  52%|█████▎    | 105/200 [03:45<02:52,  1.82s/it]Running Inference:  53%|█████▎    | 106/200 [03:47<03:09,  2.01s/it]Running Inference:  54%|█████▎    | 107/200 [03:51<03:37,  2.34s/it]Running Inference:  54%|█████▍    | 108/200 [03:53<03:27,  2.26s/it]Running Inference:  55%|█████▍    | 109/200 [03:56<03:41,  2.44s/it]Running Inference:  55%|█████▌    | 110/200 [03:57<03:13,  2.15s/it]Running Inference:  56%|█████▌    | 111/200 [03:58<02:44,  1.85s/it]Running Inference:  56%|█████▌    | 112/200 [03:59<02:19,  1.59s/it]Running Inference:  56%|█████▋    | 113/200 [04:01<02:15,  1.56s/it]Running Inference:  57%|█████▋    | 114/200 [04:02<02:10,  1.51s/it]Running Inference:  57%|█████▊    | 115/200 [04:05<02:35,  1.83s/it]Running Inference:  58%|█████▊    | 116/200 [04:07<02:49,  2.02s/it]Running Inference:  58%|█████▊    | 117/200 [04:09<02:34,  1.86s/it]Running Inference:  59%|█████▉    | 118/200 [04:09<02:06,  1.55s/it]Running Inference:  60%|█████▉    | 119/200 [04:11<02:11,  1.62s/it]Running Inference:  60%|██████    | 120/200 [04:13<02:25,  1.82s/it]Running Inference:  60%|██████    | 121/200 [04:17<03:00,  2.28s/it]Running Inference:  61%|██████    | 122/200 [04:20<03:18,  2.55s/it]Running Inference:  62%|██████▏   | 123/200 [04:21<02:38,  2.06s/it]Running Inference:  62%|██████▏   | 124/200 [04:23<02:45,  2.18s/it]Running Inference:  62%|██████▎   | 125/200 [04:25<02:35,  2.07s/it]Running Inference:  63%|██████▎   | 126/200 [04:28<02:51,  2.32s/it]Running Inference:  64%|██████▎   | 127/200 [04:30<02:51,  2.35s/it]Running Inference:  64%|██████▍   | 128/200 [04:33<02:44,  2.28s/it]Running Inference:  64%|██████▍   | 129/200 [04:35<02:53,  2.44s/it]Running Inference:  65%|██████▌   | 130/200 [04:36<02:21,  2.02s/it]Running Inference:  66%|██████▌   | 131/200 [04:40<02:48,  2.45s/it]Running Inference:  66%|██████▌   | 132/200 [04:43<02:56,  2.60s/it]Running Inference:  66%|██████▋   | 133/200 [04:44<02:25,  2.18s/it]Running Inference:  67%|██████▋   | 134/200 [04:47<02:36,  2.37s/it]Running Inference:  68%|██████▊   | 135/200 [04:49<02:20,  2.16s/it]Running Inference:  68%|██████▊   | 136/200 [04:50<02:10,  2.04s/it]Running Inference:  68%|██████▊   | 137/200 [04:53<02:13,  2.13s/it]Running Inference:  69%|██████▉   | 138/200 [04:55<02:07,  2.06s/it]Running Inference:  70%|██████▉   | 139/200 [04:58<02:26,  2.41s/it]Running Inference:  70%|███████   | 140/200 [05:00<02:30,  2.51s/it]Running Inference:  70%|███████   | 141/200 [05:02<02:13,  2.27s/it]Running Inference:  71%|███████   | 142/200 [05:05<02:14,  2.32s/it]Running Inference:  72%|███████▏  | 143/200 [05:06<01:53,  1.99s/it]Running Inference:  72%|███████▏  | 144/200 [05:08<01:50,  1.98s/it]Running Inference:  72%|███████▎  | 145/200 [05:10<01:54,  2.08s/it]Running Inference:  73%|███████▎  | 146/200 [05:12<01:55,  2.13s/it]Running Inference:  74%|███████▎  | 147/200 [05:14<01:51,  2.11s/it]Running Inference:  74%|███████▍  | 148/200 [05:17<01:52,  2.17s/it]Running Inference:  74%|███████▍  | 149/200 [05:20<02:06,  2.48s/it]Running Inference:  75%|███████▌  | 150/200 [05:21<01:47,  2.15s/it]Running Inference:  76%|███████▌  | 151/200 [05:22<01:26,  1.77s/it]Running Inference:  76%|███████▌  | 152/200 [05:25<01:32,  1.93s/it]Running Inference:  76%|███████▋  | 153/200 [05:25<01:12,  1.55s/it]Running Inference:  77%|███████▋  | 154/200 [05:28<01:26,  1.88s/it]Running Inference:  78%|███████▊  | 155/200 [05:29<01:19,  1.77s/it]Running Inference:  78%|███████▊  | 156/200 [05:31<01:19,  1.81s/it]Running Inference:  78%|███████▊  | 157/200 [05:32<01:09,  1.62s/it]Running Inference:  79%|███████▉  | 158/200 [05:34<01:13,  1.75s/it]Running Inference:  80%|███████▉  | 159/200 [05:36<01:13,  1.78s/it]Running Inference:  80%|████████  | 160/200 [05:40<01:30,  2.27s/it]Running Inference:  80%|████████  | 161/200 [05:42<01:31,  2.35s/it]Running Inference:  81%|████████  | 162/200 [05:44<01:25,  2.26s/it]Running Inference:  82%|████████▏ | 163/200 [05:47<01:26,  2.33s/it]Running Inference:  82%|████████▏ | 164/200 [05:48<01:16,  2.13s/it]Running Inference:  82%|████████▎ | 165/200 [05:51<01:16,  2.18s/it]Running Inference:  83%|████████▎ | 166/200 [05:54<01:23,  2.47s/it]Running Inference:  84%|████████▎ | 167/200 [05:56<01:13,  2.22s/it]Running Inference:  84%|████████▍ | 168/200 [05:58<01:16,  2.40s/it]Running Inference:  84%|████████▍ | 169/200 [05:59<00:58,  1.90s/it]Running Inference:  85%|████████▌ | 170/200 [06:01<01:00,  2.02s/it]Running Inference:  86%|████████▌ | 171/200 [06:04<01:00,  2.09s/it]Running Inference:  86%|████████▌ | 172/200 [06:05<00:50,  1.81s/it]Running Inference:  86%|████████▋ | 173/200 [06:07<00:51,  1.92s/it]Running Inference:  87%|████████▋ | 174/200 [06:09<00:48,  1.85s/it]Running Inference:  88%|████████▊ | 175/200 [06:13<01:04,  2.59s/it]Running Inference:  88%|████████▊ | 176/200 [06:15<00:55,  2.32s/it]Running Inference:  88%|████████▊ | 177/200 [06:16<00:48,  2.11s/it]Running Inference:  89%|████████▉ | 178/200 [06:19<00:48,  2.20s/it]Running Inference:  90%|████████▉ | 179/200 [06:21<00:48,  2.30s/it]Running Inference:  90%|█████████ | 180/200 [06:23<00:45,  2.26s/it]Running Inference:  90%|█████████ | 181/200 [06:25<00:38,  2.02s/it]Running Inference:  91%|█████████ | 182/200 [06:27<00:36,  2.04s/it]Running Inference:  92%|█████████▏| 183/200 [06:28<00:29,  1.76s/it]Running Inference:  92%|█████████▏| 184/200 [06:31<00:36,  2.25s/it]Running Inference:  92%|█████████▎| 185/200 [06:33<00:31,  2.12s/it]Running Inference:  93%|█████████▎| 186/200 [06:36<00:33,  2.38s/it]Running Inference:  94%|█████████▎| 187/200 [06:39<00:32,  2.52s/it]Running Inference:  94%|█████████▍| 188/200 [06:42<00:30,  2.53s/it]Running Inference:  94%|█████████▍| 189/200 [06:43<00:22,  2.08s/it]Running Inference:  95%|█████████▌| 190/200 [06:46<00:23,  2.36s/it]Running Inference:  96%|█████████▌| 191/200 [06:49<00:23,  2.57s/it]Running Inference:  96%|█████████▌| 192/200 [06:50<00:17,  2.13s/it]Running Inference:  96%|█████████▋| 193/200 [06:51<00:13,  1.88s/it]Running Inference:  97%|█████████▋| 194/200 [06:54<00:12,  2.10s/it]Running Inference:  98%|█████████▊| 195/200 [06:56<00:10,  2.15s/it]Running Inference:  98%|█████████▊| 196/200 [06:57<00:07,  1.92s/it]Running Inference:  98%|█████████▊| 197/200 [06:59<00:05,  1.75s/it]Running Inference:  99%|█████████▉| 198/200 [07:00<00:03,  1.59s/it]Running Inference: 100%|█████████▉| 199/200 [07:01<00:01,  1.54s/it]Running Inference: 100%|██████████| 200/200 [07:03<00:00,  1.59s/it]Running Inference: 100%|██████████| 200/200 [07:03<00:00,  2.12s/it]
2025-12-14 16:08:30,730 - INFO - Inference completed.
2025-12-14 16:08:30,738 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 16:08:30,738 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:08:30,739 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 16:08:30,739 - INFO - Metrics:
7.97
2025-12-14 16:08:30,740 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_count, ratio=0.2) on GPU 2

----------------------------------------
Task: passage_count | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:08:37,157 - INFO - Set deterministic seeds to 42
2025-12-14 16:08:37,157 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:08:37,157 - INFO - Starting evaluation run...
2025-12-14 16:08:37,157 - INFO - Output directory set to: longbenchresult
2025-12-14 16:08:37,157 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 16:08:37,157 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:08:37,157 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:08:37,157 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.51it/s]
Device set to use cuda:0
2025-12-14 16:08:50,722 - INFO - Model pipeline loaded.
2025-12-14 16:08:50,723 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 16:08:56,782 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:08:56,782 - INFO - Dataset processed with 200 entries.
2025-12-14 16:08:56,823 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:10,  1.86s/it]Running Inference:   1%|          | 2/200 [00:04<06:56,  2.10s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:09,  1.87s/it]Running Inference:   2%|▏         | 4/200 [00:08<07:34,  2.32s/it]Running Inference:   2%|▎         | 5/200 [00:13<10:03,  3.10s/it]Running Inference:   3%|▎         | 6/200 [00:15<08:37,  2.67s/it]Running Inference:   4%|▎         | 7/200 [00:16<07:07,  2.22s/it]Running Inference:   4%|▍         | 8/200 [00:17<05:44,  1.80s/it]Running Inference:   4%|▍         | 9/200 [00:20<06:49,  2.15s/it]Running Inference:   5%|▌         | 10/200 [00:22<06:51,  2.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:25<07:54,  2.51s/it]Running Inference:   6%|▌         | 12/200 [00:28<07:45,  2.48s/it]Running Inference:   6%|▋         | 13/200 [00:29<06:48,  2.18s/it]Running Inference:   7%|▋         | 14/200 [00:30<05:38,  1.82s/it]Running Inference:   8%|▊         | 15/200 [00:33<06:59,  2.27s/it]Running Inference:   8%|▊         | 16/200 [00:35<06:38,  2.17s/it]Running Inference:   8%|▊         | 17/200 [00:37<05:58,  1.96s/it]Running Inference:   9%|▉         | 18/200 [00:38<05:44,  1.89s/it]Running Inference:  10%|▉         | 19/200 [00:40<05:33,  1.84s/it]Running Inference:  10%|█         | 20/200 [00:42<05:45,  1.92s/it]Running Inference:  10%|█         | 21/200 [00:45<06:45,  2.26s/it]Running Inference:  11%|█         | 22/200 [00:48<06:36,  2.23s/it]Running Inference:  12%|█▏        | 23/200 [00:50<06:57,  2.36s/it]Running Inference:  12%|█▏        | 24/200 [00:53<06:57,  2.37s/it]Running Inference:  12%|█▎        | 25/200 [00:55<06:51,  2.35s/it]Running Inference:  13%|█▎        | 26/200 [00:57<06:42,  2.31s/it]Running Inference:  14%|█▎        | 27/200 [00:59<06:14,  2.16s/it]Running Inference:  14%|█▍        | 28/200 [01:01<06:25,  2.24s/it]Running Inference:  14%|█▍        | 29/200 [01:05<07:13,  2.53s/it]Running Inference:  15%|█▌        | 30/200 [01:06<06:07,  2.16s/it]Running Inference:  16%|█▌        | 31/200 [01:07<05:29,  1.95s/it]Running Inference:  16%|█▌        | 32/200 [01:11<06:46,  2.42s/it]Running Inference:  16%|█▋        | 33/200 [01:12<05:26,  1.96s/it]Running Inference:  17%|█▋        | 34/200 [01:14<05:47,  2.09s/it]Running Inference:  18%|█▊        | 35/200 [01:16<05:54,  2.15s/it]Running Inference:  18%|█▊        | 36/200 [01:19<06:05,  2.23s/it]Running Inference:  18%|█▊        | 37/200 [01:21<05:44,  2.12s/it]Running Inference:  19%|█▉        | 38/200 [01:22<05:18,  1.97s/it]Running Inference:  20%|█▉        | 39/200 [01:25<05:44,  2.14s/it]Running Inference:  20%|██        | 40/200 [01:27<05:22,  2.01s/it]Running Inference:  20%|██        | 41/200 [01:29<05:17,  2.00s/it]Running Inference:  21%|██        | 42/200 [01:30<04:34,  1.74s/it]Running Inference:  22%|██▏       | 43/200 [01:32<04:52,  1.86s/it]Running Inference:  22%|██▏       | 44/200 [01:34<04:53,  1.88s/it]Running Inference:  22%|██▎       | 45/200 [01:37<05:34,  2.16s/it]Running Inference:  23%|██▎       | 46/200 [01:39<05:39,  2.21s/it]Running Inference:  24%|██▎       | 47/200 [01:40<04:49,  1.89s/it]Running Inference:  24%|██▍       | 48/200 [01:41<04:01,  1.59s/it]Running Inference:  24%|██▍       | 49/200 [01:42<03:56,  1.57s/it]Running Inference:  25%|██▌       | 50/200 [01:44<03:35,  1.43s/it]Running Inference:  26%|██▌       | 51/200 [01:46<04:15,  1.72s/it]Running Inference:  26%|██▌       | 52/200 [01:47<03:59,  1.62s/it]Running Inference:  26%|██▋       | 53/200 [01:50<04:49,  1.97s/it]Running Inference:  27%|██▋       | 54/200 [01:52<04:56,  2.03s/it]Running Inference:  28%|██▊       | 55/200 [01:53<04:11,  1.73s/it]Running Inference:  28%|██▊       | 56/200 [01:55<04:15,  1.77s/it]Running Inference:  28%|██▊       | 57/200 [01:57<04:26,  1.86s/it]Running Inference:  29%|██▉       | 58/200 [01:59<04:35,  1.94s/it]Running Inference:  30%|██▉       | 59/200 [02:03<05:38,  2.40s/it]Running Inference:  30%|███       | 60/200 [02:06<05:56,  2.54s/it]Running Inference:  30%|███       | 61/200 [02:07<05:09,  2.23s/it]Running Inference:  31%|███       | 62/200 [02:08<04:15,  1.85s/it]Running Inference:  32%|███▏      | 63/200 [02:10<04:16,  1.87s/it]Running Inference:  32%|███▏      | 64/200 [02:13<04:47,  2.12s/it]Running Inference:  32%|███▎      | 65/200 [02:15<04:55,  2.19s/it]Running Inference:  33%|███▎      | 66/200 [02:18<05:02,  2.26s/it]Running Inference:  34%|███▎      | 67/200 [02:19<04:37,  2.09s/it]Running Inference:  34%|███▍      | 68/200 [02:21<04:36,  2.09s/it]Running Inference:  34%|███▍      | 69/200 [02:25<05:22,  2.46s/it]Running Inference:  35%|███▌      | 70/200 [02:27<05:03,  2.33s/it]Running Inference:  36%|███▌      | 71/200 [02:29<05:11,  2.41s/it]Running Inference:  36%|███▌      | 72/200 [02:31<04:52,  2.28s/it]Running Inference:  36%|███▋      | 73/200 [02:34<05:22,  2.54s/it]Running Inference:  37%|███▋      | 74/200 [02:37<05:14,  2.49s/it]Running Inference:  38%|███▊      | 75/200 [02:38<04:38,  2.23s/it]Running Inference:  38%|███▊      | 76/200 [02:40<03:58,  1.92s/it]Running Inference:  38%|███▊      | 77/200 [02:44<05:21,  2.61s/it]Running Inference:  39%|███▉      | 78/200 [02:46<04:56,  2.43s/it]Running Inference:  40%|███▉      | 79/200 [02:48<04:49,  2.39s/it]Running Inference:  40%|████      | 80/200 [02:51<04:47,  2.40s/it]Running Inference:  40%|████      | 81/200 [02:52<04:07,  2.08s/it]Running Inference:  41%|████      | 82/200 [02:53<03:32,  1.80s/it]Running Inference:  42%|████▏     | 83/200 [02:57<05:01,  2.58s/it]Running Inference:  42%|████▏     | 84/200 [02:59<04:37,  2.39s/it]Running Inference:  42%|████▎     | 85/200 [03:02<04:52,  2.54s/it]Running Inference:  43%|████▎     | 86/200 [03:05<04:40,  2.46s/it]Running Inference:  44%|████▎     | 87/200 [03:06<04:19,  2.30s/it]Running Inference:  44%|████▍     | 88/200 [03:09<04:13,  2.27s/it]Running Inference:  44%|████▍     | 89/200 [03:11<04:29,  2.43s/it]Running Inference:  45%|████▌     | 90/200 [03:14<04:37,  2.52s/it]Running Inference:  46%|████▌     | 91/200 [03:15<03:49,  2.11s/it]Running Inference:  46%|████▌     | 92/200 [03:16<03:10,  1.76s/it]Running Inference:  46%|████▋     | 93/200 [03:18<03:02,  1.71s/it]Running Inference:  47%|████▋     | 94/200 [03:20<03:05,  1.75s/it]Running Inference:  48%|████▊     | 95/200 [03:22<03:17,  1.88s/it]Running Inference:  48%|████▊     | 96/200 [03:24<03:17,  1.90s/it]Running Inference:  48%|████▊     | 97/200 [03:26<03:34,  2.08s/it]Running Inference:  49%|████▉     | 98/200 [03:30<04:17,  2.52s/it]Running Inference:  50%|████▉     | 99/200 [03:33<04:26,  2.64s/it]Running Inference:  50%|█████     | 100/200 [03:35<03:54,  2.35s/it]Running Inference:  50%|█████     | 101/200 [03:36<03:22,  2.04s/it]Running Inference:  51%|█████     | 102/200 [03:38<03:08,  1.93s/it]Running Inference:  52%|█████▏    | 103/200 [03:41<03:42,  2.29s/it]Running Inference:  52%|█████▏    | 104/200 [03:43<03:43,  2.33s/it]Running Inference:  52%|█████▎    | 105/200 [03:44<02:56,  1.85s/it]Running Inference:  53%|█████▎    | 106/200 [03:46<03:12,  2.05s/it]Running Inference:  54%|█████▎    | 107/200 [03:49<03:40,  2.37s/it]Running Inference:  54%|█████▍    | 108/200 [03:52<03:30,  2.28s/it]Running Inference:  55%|█████▍    | 109/200 [03:54<03:43,  2.46s/it]Running Inference:  55%|█████▌    | 110/200 [03:56<03:14,  2.16s/it]Running Inference:  56%|█████▌    | 111/200 [03:57<02:45,  1.86s/it]Running Inference:  56%|█████▌    | 112/200 [03:58<02:20,  1.59s/it]Running Inference:  56%|█████▋    | 113/200 [03:59<02:16,  1.57s/it]Running Inference:  57%|█████▋    | 114/200 [04:01<02:10,  1.52s/it]Running Inference:  57%|█████▊    | 115/200 [04:03<02:35,  1.83s/it]Running Inference:  58%|█████▊    | 116/200 [04:06<02:50,  2.02s/it]Running Inference:  58%|█████▊    | 117/200 [04:07<02:34,  1.86s/it]Running Inference:  59%|█████▉    | 118/200 [04:08<02:06,  1.55s/it]Running Inference:  60%|█████▉    | 119/200 [04:10<02:11,  1.62s/it]Running Inference:  60%|██████    | 120/200 [04:12<02:25,  1.82s/it]Running Inference:  60%|██████    | 121/200 [04:16<03:00,  2.28s/it]Running Inference:  61%|██████    | 122/200 [04:19<03:19,  2.56s/it]Running Inference:  62%|██████▏   | 123/200 [04:20<02:54,  2.26s/it]Running Inference:  62%|██████▏   | 124/200 [04:23<02:56,  2.32s/it]Running Inference:  62%|██████▎   | 125/200 [04:25<02:42,  2.17s/it]Running Inference:  63%|██████▎   | 126/200 [04:28<02:58,  2.41s/it]Running Inference:  64%|██████▎   | 127/200 [04:30<02:56,  2.41s/it]Running Inference:  64%|██████▍   | 128/200 [04:32<02:47,  2.32s/it]Running Inference:  64%|██████▍   | 129/200 [04:35<02:55,  2.48s/it]Running Inference:  65%|██████▌   | 130/200 [04:36<02:23,  2.05s/it]Running Inference:  66%|██████▌   | 131/200 [04:40<02:50,  2.46s/it]Running Inference:  66%|██████▌   | 132/200 [04:42<02:57,  2.61s/it]Running Inference:  66%|██████▋   | 133/200 [04:44<02:26,  2.19s/it]Running Inference:  67%|██████▋   | 134/200 [04:46<02:36,  2.37s/it]Running Inference:  68%|██████▊   | 135/200 [04:48<02:21,  2.17s/it]Running Inference:  68%|██████▊   | 136/200 [04:50<02:11,  2.05s/it]Running Inference:  68%|██████▊   | 137/200 [04:52<02:13,  2.13s/it]Running Inference:  69%|██████▉   | 138/200 [04:54<02:07,  2.06s/it]Running Inference:  70%|██████▉   | 139/200 [04:57<02:27,  2.42s/it]Running Inference:  70%|███████   | 140/200 [05:00<02:31,  2.52s/it]Running Inference:  70%|███████   | 141/200 [05:02<02:14,  2.27s/it]Running Inference:  71%|███████   | 142/200 [05:04<02:15,  2.33s/it]Running Inference:  72%|███████▏  | 143/200 [05:06<01:53,  1.99s/it]Running Inference:  72%|███████▏  | 144/200 [05:08<01:51,  1.98s/it]Running Inference:  72%|███████▎  | 145/200 [05:10<01:54,  2.09s/it]Running Inference:  73%|███████▎  | 146/200 [05:12<01:55,  2.14s/it]Running Inference:  74%|███████▎  | 147/200 [05:14<01:51,  2.10s/it]Running Inference:  74%|███████▍  | 148/200 [05:16<01:53,  2.18s/it]Running Inference:  74%|███████▍  | 149/200 [05:20<02:07,  2.50s/it]Running Inference:  75%|███████▌  | 150/200 [05:21<01:48,  2.17s/it]Running Inference:  76%|███████▌  | 151/200 [05:22<01:27,  1.78s/it]Running Inference:  76%|███████▌  | 152/200 [05:24<01:32,  1.94s/it]Running Inference:  76%|███████▋  | 153/200 [05:25<01:12,  1.55s/it]Running Inference:  77%|███████▋  | 154/200 [05:28<01:26,  1.89s/it]Running Inference:  78%|███████▊  | 155/200 [05:29<01:19,  1.78s/it]Running Inference:  78%|███████▊  | 156/200 [05:31<01:19,  1.82s/it]Running Inference:  78%|███████▊  | 157/200 [05:32<01:09,  1.62s/it]Running Inference:  79%|███████▉  | 158/200 [05:34<01:13,  1.76s/it]Running Inference:  80%|███████▉  | 159/200 [05:36<01:13,  1.79s/it]Running Inference:  80%|████████  | 160/200 [05:40<01:30,  2.27s/it]Running Inference:  80%|████████  | 161/200 [05:42<01:31,  2.35s/it]Running Inference:  81%|████████  | 162/200 [05:44<01:26,  2.27s/it]Running Inference:  82%|████████▏ | 163/200 [05:47<01:26,  2.34s/it]Running Inference:  82%|████████▏ | 164/200 [05:48<01:16,  2.13s/it]Running Inference:  82%|████████▎ | 165/200 [05:51<01:17,  2.20s/it]Running Inference:  83%|████████▎ | 166/200 [05:54<01:24,  2.49s/it]Running Inference:  84%|████████▎ | 167/200 [05:55<01:13,  2.23s/it]Running Inference:  84%|████████▍ | 168/200 [05:58<01:16,  2.40s/it]Running Inference:  84%|████████▍ | 169/200 [05:59<00:58,  1.90s/it]Running Inference:  85%|████████▌ | 170/200 [06:01<01:00,  2.02s/it]Running Inference:  86%|████████▌ | 171/200 [06:04<01:00,  2.10s/it]Running Inference:  86%|████████▌ | 172/200 [06:05<00:50,  1.81s/it]Running Inference:  86%|████████▋ | 173/200 [06:07<00:52,  1.93s/it]Running Inference:  87%|████████▋ | 174/200 [06:09<00:48,  1.86s/it]Running Inference:  88%|████████▊ | 175/200 [06:13<01:05,  2.61s/it]Running Inference:  88%|████████▊ | 176/200 [06:15<00:56,  2.34s/it]Running Inference:  88%|████████▊ | 177/200 [06:16<00:48,  2.12s/it]Running Inference:  89%|████████▉ | 178/200 [06:19<00:48,  2.23s/it]Running Inference:  90%|████████▉ | 179/200 [06:21<00:48,  2.32s/it]Running Inference:  90%|█████████ | 180/200 [06:23<00:45,  2.28s/it]Running Inference:  90%|█████████ | 181/200 [06:25<00:38,  2.03s/it]Running Inference:  91%|█████████ | 182/200 [06:27<00:37,  2.06s/it]Running Inference:  92%|█████████▏| 183/200 [06:28<00:30,  1.78s/it]Running Inference:  92%|█████████▏| 184/200 [06:32<00:36,  2.26s/it]Running Inference:  92%|█████████▎| 185/200 [06:33<00:31,  2.13s/it]Running Inference:  93%|█████████▎| 186/200 [06:36<00:33,  2.38s/it]Running Inference:  94%|█████████▎| 187/200 [06:39<00:32,  2.53s/it]Running Inference:  94%|█████████▍| 188/200 [06:42<00:30,  2.53s/it]Running Inference:  94%|█████████▍| 189/200 [06:43<00:22,  2.07s/it]Running Inference:  95%|█████████▌| 190/200 [06:46<00:23,  2.37s/it]Running Inference:  96%|█████████▌| 191/200 [06:49<00:23,  2.57s/it]Running Inference:  96%|█████████▌| 192/200 [06:50<00:17,  2.13s/it]Running Inference:  96%|█████████▋| 193/200 [06:51<00:13,  1.88s/it]Running Inference:  97%|█████████▋| 194/200 [06:54<00:12,  2.10s/it]Running Inference:  98%|█████████▊| 195/200 [06:57<00:11,  2.29s/it]Running Inference:  98%|█████████▊| 196/200 [06:58<00:08,  2.02s/it]Running Inference:  98%|█████████▊| 197/200 [06:59<00:05,  1.82s/it]Running Inference:  99%|█████████▉| 198/200 [07:01<00:03,  1.64s/it]Running Inference: 100%|█████████▉| 199/200 [07:02<00:01,  1.58s/it]Running Inference: 100%|██████████| 200/200 [07:04<00:00,  1.62s/it]Running Inference: 100%|██████████| 200/200 [07:04<00:00,  2.12s/it]
2025-12-14 16:16:01,077 - INFO - Inference completed.
2025-12-14 16:16:01,085 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 16:16:01,085 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:16:01,086 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 16:16:01,086 - INFO - Metrics:
8.12
2025-12-14 16:16:01,087 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_count, ratio=0.3) on GPU 2

----------------------------------------
Task: passage_count | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:16:07,520 - INFO - Set deterministic seeds to 42
2025-12-14 16:16:07,520 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_count",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:16:07,520 - INFO - Starting evaluation run...
2025-12-14 16:16:07,520 - INFO - Output directory set to: longbenchresult
2025-12-14 16:16:07,520 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 16:16:07,520 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:16:07,520 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:16:07,521 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.81it/s]
Device set to use cuda:0
2025-12-14 16:16:21,980 - INFO - Model pipeline loaded.
2025-12-14 16:16:21,980 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_count)
2025-12-14 16:16:28,162 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:16:28,162 - INFO - Dataset processed with 200 entries.
2025-12-14 16:16:28,200 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<06:13,  1.88s/it]Running Inference:   1%|          | 2/200 [00:04<06:57,  2.11s/it]Running Inference:   2%|▏         | 3/200 [00:05<06:07,  1.87s/it]Running Inference:   2%|▏         | 4/200 [00:06<04:47,  1.47s/it]Running Inference:   2%|▎         | 5/200 [00:11<08:17,  2.55s/it]Running Inference:   3%|▎         | 6/200 [00:12<07:26,  2.30s/it]Running Inference:   4%|▎         | 7/200 [00:14<06:17,  1.96s/it]Running Inference:   4%|▍         | 8/200 [00:15<05:12,  1.63s/it]Running Inference:   4%|▍         | 9/200 [00:17<06:26,  2.02s/it]Running Inference:   5%|▌         | 10/200 [00:20<06:34,  2.08s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<07:42,  2.45s/it]Running Inference:   6%|▌         | 12/200 [00:25<07:40,  2.45s/it]Running Inference:   6%|▋         | 13/200 [00:27<06:44,  2.16s/it]Running Inference:   7%|▋         | 14/200 [00:28<05:35,  1.80s/it]Running Inference:   8%|▊         | 15/200 [00:31<06:56,  2.25s/it]Running Inference:   8%|▊         | 16/200 [00:33<06:36,  2.15s/it]Running Inference:   8%|▊         | 17/200 [00:35<05:56,  1.95s/it]Running Inference:   9%|▉         | 18/200 [00:36<05:40,  1.87s/it]Running Inference:  10%|▉         | 19/200 [00:38<05:28,  1.81s/it]Running Inference:  10%|█         | 20/200 [00:40<05:41,  1.90s/it]Running Inference:  10%|█         | 21/200 [00:43<06:42,  2.25s/it]Running Inference:  11%|█         | 22/200 [00:45<06:34,  2.21s/it]Running Inference:  12%|█▏        | 23/200 [00:48<06:52,  2.33s/it]Running Inference:  12%|█▏        | 24/200 [00:50<06:54,  2.36s/it]Running Inference:  12%|█▎        | 25/200 [00:52<06:46,  2.32s/it]Running Inference:  13%|█▎        | 26/200 [00:55<06:38,  2.29s/it]Running Inference:  14%|█▎        | 27/200 [00:56<06:09,  2.14s/it]Running Inference:  14%|█▍        | 28/200 [00:59<06:22,  2.22s/it]Running Inference:  14%|█▍        | 29/200 [01:02<07:12,  2.53s/it]Running Inference:  15%|█▌        | 30/200 [01:03<06:03,  2.14s/it]Running Inference:  16%|█▌        | 31/200 [01:05<05:26,  1.93s/it]Running Inference:  16%|█▌        | 32/200 [01:08<06:43,  2.40s/it]Running Inference:  16%|█▋        | 33/200 [01:09<05:24,  1.95s/it]Running Inference:  17%|█▋        | 34/200 [01:12<05:43,  2.07s/it]Running Inference:  18%|█▊        | 35/200 [01:14<05:50,  2.12s/it]Running Inference:  18%|█▊        | 36/200 [01:16<06:02,  2.21s/it]Running Inference:  18%|█▊        | 37/200 [01:18<05:42,  2.10s/it]Running Inference:  19%|█▉        | 38/200 [01:20<05:17,  1.96s/it]Running Inference:  20%|█▉        | 39/200 [01:22<05:42,  2.13s/it]Running Inference:  20%|██        | 40/200 [01:24<05:21,  2.01s/it]Running Inference:  20%|██        | 41/200 [01:26<05:16,  1.99s/it]Running Inference:  21%|██        | 42/200 [01:29<06:14,  2.37s/it]Running Inference:  22%|██▏       | 43/200 [01:31<06:02,  2.31s/it]Running Inference:  22%|██▏       | 44/200 [01:33<05:41,  2.19s/it]Running Inference:  22%|██▎       | 45/200 [01:36<06:09,  2.39s/it]Running Inference:  23%|██▎       | 46/200 [01:38<06:03,  2.36s/it]Running Inference:  24%|██▎       | 47/200 [01:39<05:04,  1.99s/it]Running Inference:  24%|██▍       | 48/200 [01:40<04:11,  1.66s/it]Running Inference:  24%|██▍       | 49/200 [01:42<04:03,  1.61s/it]Running Inference:  25%|██▌       | 50/200 [01:43<03:37,  1.45s/it]Running Inference:  26%|██▌       | 51/200 [01:45<04:17,  1.73s/it]Running Inference:  26%|██▌       | 52/200 [01:47<04:00,  1.62s/it]Running Inference:  26%|██▋       | 53/200 [01:49<04:49,  1.97s/it]Running Inference:  27%|██▋       | 54/200 [01:52<04:56,  2.03s/it]Running Inference:  28%|██▊       | 55/200 [01:53<04:11,  1.73s/it]Running Inference:  28%|██▊       | 56/200 [01:55<04:13,  1.76s/it]Running Inference:  28%|██▊       | 57/200 [01:57<04:22,  1.84s/it]Running Inference:  29%|██▉       | 58/200 [01:59<04:32,  1.92s/it]Running Inference:  30%|██▉       | 59/200 [02:02<05:36,  2.38s/it]Running Inference:  30%|███       | 60/200 [02:05<05:54,  2.53s/it]Running Inference:  30%|███       | 61/200 [02:06<05:06,  2.20s/it]Running Inference:  31%|███       | 62/200 [02:07<04:14,  1.84s/it]Running Inference:  32%|███▏      | 63/200 [02:09<04:11,  1.83s/it]Running Inference:  32%|███▏      | 64/200 [02:12<04:43,  2.09s/it]Running Inference:  32%|███▎      | 65/200 [02:14<04:52,  2.17s/it]Running Inference:  33%|███▎      | 66/200 [02:17<05:00,  2.24s/it]Running Inference:  34%|███▎      | 67/200 [02:18<04:33,  2.06s/it]Running Inference:  34%|███▍      | 68/200 [02:20<04:31,  2.06s/it]Running Inference:  34%|███▍      | 69/200 [02:24<05:18,  2.43s/it]Running Inference:  35%|███▌      | 70/200 [02:26<05:00,  2.32s/it]Running Inference:  36%|███▌      | 71/200 [02:28<05:09,  2.40s/it]Running Inference:  36%|███▌      | 72/200 [02:30<04:50,  2.27s/it]Running Inference:  36%|███▋      | 73/200 [02:33<05:21,  2.53s/it]Running Inference:  37%|███▋      | 74/200 [02:36<05:13,  2.49s/it]Running Inference:  38%|███▊      | 75/200 [02:37<04:37,  2.22s/it]Running Inference:  38%|███▊      | 76/200 [02:39<03:57,  1.92s/it]Running Inference:  38%|███▊      | 77/200 [02:43<05:20,  2.60s/it]Running Inference:  39%|███▉      | 78/200 [02:45<04:55,  2.42s/it]Running Inference:  40%|███▉      | 79/200 [02:47<04:48,  2.39s/it]Running Inference:  40%|████      | 80/200 [02:49<04:45,  2.38s/it]Running Inference:  40%|████      | 81/200 [02:51<04:05,  2.06s/it]Running Inference:  41%|████      | 82/200 [02:53<03:56,  2.01s/it]Running Inference:  42%|████▏     | 83/200 [02:56<04:31,  2.32s/it]Running Inference:  42%|████▏     | 84/200 [02:58<04:15,  2.21s/it]Running Inference:  42%|████▎     | 85/200 [03:01<04:35,  2.40s/it]Running Inference:  43%|████▎     | 86/200 [03:03<04:28,  2.36s/it]Running Inference:  44%|████▎     | 87/200 [03:05<04:11,  2.22s/it]Running Inference:  44%|████▍     | 88/200 [03:07<04:07,  2.21s/it]Running Inference:  44%|████▍     | 89/200 [03:10<04:25,  2.39s/it]Running Inference:  45%|████▌     | 90/200 [03:12<04:35,  2.50s/it]Running Inference:  46%|████▌     | 91/200 [03:14<03:48,  2.10s/it]Running Inference:  46%|████▌     | 92/200 [03:15<03:08,  1.75s/it]Running Inference:  46%|████▋     | 93/200 [03:16<03:01,  1.70s/it]Running Inference:  47%|████▋     | 94/200 [03:18<03:04,  1.74s/it]Running Inference:  48%|████▊     | 95/200 [03:20<03:16,  1.87s/it]Running Inference:  48%|████▊     | 96/200 [03:22<03:16,  1.89s/it]Running Inference:  48%|████▊     | 97/200 [03:24<03:31,  2.05s/it]Running Inference:  49%|████▉     | 98/200 [03:28<04:16,  2.51s/it]Running Inference:  50%|████▉     | 99/200 [03:31<04:26,  2.64s/it]Running Inference:  50%|█████     | 100/200 [03:33<03:54,  2.35s/it]Running Inference:  50%|█████     | 101/200 [03:34<03:20,  2.03s/it]Running Inference:  51%|█████     | 102/200 [03:36<03:08,  1.92s/it]Running Inference:  52%|█████▏    | 103/200 [03:39<03:41,  2.29s/it]Running Inference:  52%|█████▏    | 104/200 [03:41<03:42,  2.32s/it]Running Inference:  52%|█████▎    | 105/200 [03:42<02:54,  1.84s/it]Running Inference:  53%|█████▎    | 106/200 [03:44<03:11,  2.04s/it]Running Inference:  54%|█████▎    | 107/200 [03:47<03:38,  2.35s/it]Running Inference:  54%|█████▍    | 108/200 [03:50<03:28,  2.27s/it]Running Inference:  55%|█████▍    | 109/200 [03:52<03:42,  2.44s/it]Running Inference:  55%|█████▌    | 110/200 [03:54<03:13,  2.15s/it]Running Inference:  56%|█████▌    | 111/200 [03:55<02:44,  1.85s/it]Running Inference:  56%|█████▌    | 112/200 [03:56<02:19,  1.59s/it]Running Inference:  56%|█████▋    | 113/200 [03:57<02:15,  1.56s/it]Running Inference:  57%|█████▋    | 114/200 [03:59<02:10,  1.51s/it]Running Inference:  57%|█████▊    | 115/200 [04:01<02:36,  1.84s/it]Running Inference:  58%|█████▊    | 116/200 [04:04<02:49,  2.02s/it]Running Inference:  58%|█████▊    | 117/200 [04:05<02:34,  1.86s/it]Running Inference:  59%|█████▉    | 118/200 [04:06<02:07,  1.55s/it]Running Inference:  60%|█████▉    | 119/200 [04:08<02:10,  1.61s/it]Running Inference:  60%|██████    | 120/200 [04:10<02:25,  1.82s/it]Running Inference:  60%|██████    | 121/200 [04:14<02:59,  2.28s/it]Running Inference:  61%|██████    | 122/200 [04:15<02:29,  1.92s/it]Running Inference:  62%|██████▏   | 123/200 [04:16<02:04,  1.62s/it]Running Inference:  62%|██████▏   | 124/200 [04:18<02:22,  1.87s/it]Running Inference:  62%|██████▎   | 125/200 [04:20<02:18,  1.85s/it]Running Inference:  63%|██████▎   | 126/200 [04:21<01:54,  1.55s/it]Running Inference:  64%|██████▎   | 127/200 [04:23<02:11,  1.81s/it]Running Inference:  64%|██████▍   | 128/200 [04:25<02:16,  1.90s/it]Running Inference:  64%|██████▍   | 129/200 [04:28<02:34,  2.17s/it]Running Inference:  65%|██████▌   | 130/200 [04:29<02:08,  1.83s/it]Running Inference:  66%|██████▌   | 131/200 [04:33<02:39,  2.31s/it]Running Inference:  66%|██████▌   | 132/200 [04:35<02:49,  2.50s/it]Running Inference:  66%|██████▋   | 133/200 [04:37<02:21,  2.11s/it]Running Inference:  67%|██████▋   | 134/200 [04:39<02:32,  2.32s/it]Running Inference:  68%|██████▊   | 135/200 [04:41<02:18,  2.13s/it]Running Inference:  68%|██████▊   | 136/200 [04:43<02:09,  2.02s/it]Running Inference:  68%|██████▊   | 137/200 [04:45<02:13,  2.12s/it]Running Inference:  69%|██████▉   | 138/200 [04:47<02:08,  2.07s/it]Running Inference:  70%|██████▉   | 139/200 [04:50<02:27,  2.41s/it]Running Inference:  70%|███████   | 140/200 [04:53<02:31,  2.52s/it]Running Inference:  70%|███████   | 141/200 [04:55<02:14,  2.27s/it]Running Inference:  71%|███████   | 142/200 [04:57<02:15,  2.34s/it]Running Inference:  72%|███████▏  | 143/200 [04:59<01:53,  2.00s/it]Running Inference:  72%|███████▏  | 144/200 [05:01<01:50,  1.98s/it]Running Inference:  72%|███████▎  | 145/200 [05:03<01:54,  2.08s/it]Running Inference:  73%|███████▎  | 146/200 [05:05<01:55,  2.13s/it]Running Inference:  74%|███████▎  | 147/200 [05:07<01:51,  2.10s/it]Running Inference:  74%|███████▍  | 148/200 [05:10<01:53,  2.18s/it]Running Inference:  74%|███████▍  | 149/200 [05:13<02:07,  2.49s/it]Running Inference:  75%|███████▌  | 150/200 [05:14<01:48,  2.16s/it]Running Inference:  76%|███████▌  | 151/200 [05:15<01:27,  1.78s/it]Running Inference:  76%|███████▌  | 152/200 [05:17<01:32,  1.93s/it]Running Inference:  76%|███████▋  | 153/200 [05:18<01:12,  1.55s/it]Running Inference:  77%|███████▋  | 154/200 [05:21<01:26,  1.89s/it]Running Inference:  78%|███████▊  | 155/200 [05:22<01:20,  1.78s/it]Running Inference:  78%|███████▊  | 156/200 [05:24<01:20,  1.82s/it]Running Inference:  78%|███████▊  | 157/200 [05:25<01:09,  1.63s/it]Running Inference:  79%|███████▉  | 158/200 [05:27<01:13,  1.74s/it]Running Inference:  80%|███████▉  | 159/200 [05:29<01:12,  1.78s/it]Running Inference:  80%|████████  | 160/200 [05:33<01:30,  2.27s/it]Running Inference:  80%|████████  | 161/200 [05:35<01:31,  2.35s/it]Running Inference:  81%|████████  | 162/200 [05:37<01:25,  2.25s/it]Running Inference:  82%|████████▏ | 163/200 [05:40<01:25,  2.32s/it]Running Inference:  82%|████████▏ | 164/200 [05:41<01:16,  2.11s/it]Running Inference:  82%|████████▎ | 165/200 [05:44<01:16,  2.17s/it]Running Inference:  83%|████████▎ | 166/200 [05:47<01:23,  2.47s/it]Running Inference:  84%|████████▎ | 167/200 [05:48<01:12,  2.21s/it]Running Inference:  84%|████████▍ | 168/200 [05:51<01:16,  2.39s/it]Running Inference:  84%|████████▍ | 169/200 [05:52<00:58,  1.89s/it]Running Inference:  85%|████████▌ | 170/200 [05:54<01:00,  2.01s/it]Running Inference:  86%|████████▌ | 171/200 [05:56<01:00,  2.09s/it]Running Inference:  86%|████████▌ | 172/200 [05:58<00:50,  1.79s/it]Running Inference:  86%|████████▋ | 173/200 [06:00<00:51,  1.91s/it]Running Inference:  87%|████████▋ | 174/200 [06:01<00:47,  1.83s/it]Running Inference:  88%|████████▊ | 175/200 [06:04<00:48,  1.95s/it]Running Inference:  88%|████████▊ | 176/200 [06:05<00:45,  1.88s/it]Running Inference:  88%|████████▊ | 177/200 [06:07<00:41,  1.79s/it]Running Inference:  89%|████████▉ | 178/200 [06:09<00:43,  1.99s/it]Running Inference:  90%|████████▉ | 179/200 [06:12<00:45,  2.17s/it]Running Inference:  90%|█████████ | 180/200 [06:14<00:43,  2.17s/it]Running Inference:  90%|█████████ | 181/200 [06:16<00:37,  1.96s/it]Running Inference:  91%|█████████ | 182/200 [06:18<00:36,  2.01s/it]Running Inference:  92%|█████████▏| 183/200 [06:19<00:29,  1.75s/it]Running Inference:  92%|█████████▏| 184/200 [06:22<00:35,  2.25s/it]Running Inference:  92%|█████████▎| 185/200 [06:24<00:31,  2.11s/it]Running Inference:  93%|█████████▎| 186/200 [06:27<00:33,  2.37s/it]Running Inference:  94%|█████████▎| 187/200 [06:30<00:32,  2.51s/it]Running Inference:  94%|█████████▍| 188/200 [06:32<00:30,  2.53s/it]Running Inference:  94%|█████████▍| 189/200 [06:33<00:22,  2.07s/it]Running Inference:  95%|█████████▌| 190/200 [06:36<00:23,  2.37s/it]Running Inference:  96%|█████████▌| 191/200 [06:40<00:23,  2.57s/it]Running Inference:  96%|█████████▌| 192/200 [06:41<00:17,  2.13s/it]Running Inference:  96%|█████████▋| 193/200 [06:42<00:13,  1.88s/it]Running Inference:  97%|█████████▋| 194/200 [06:45<00:12,  2.10s/it]Running Inference:  98%|█████████▊| 195/200 [06:47<00:11,  2.29s/it]Running Inference:  98%|█████████▊| 196/200 [06:49<00:08,  2.02s/it]Running Inference:  98%|█████████▊| 197/200 [06:50<00:05,  1.82s/it]Running Inference:  99%|█████████▉| 198/200 [06:51<00:03,  1.64s/it]Running Inference: 100%|█████████▉| 199/200 [06:53<00:01,  1.56s/it]Running Inference: 100%|██████████| 200/200 [06:54<00:00,  1.60s/it]Running Inference: 100%|██████████| 200/200 [06:54<00:00,  2.07s/it]
2025-12-14 16:23:23,014 - INFO - Inference completed.
2025-12-14 16:23:23,022 - INFO - Results saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 16:23:23,022 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:23:23,023 - INFO - Metrics saved to longbenchresult/longbench__passage_count__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 16:23:23,023 - INFO - Metrics:
8.6
2025-12-14 16:23:23,025 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_count, ratio=0.5) on GPU 2


========================================
LongBench Task: passage_retrieval_en
========================================
----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:23:29,518 - INFO - Set deterministic seeds to 42
2025-12-14 16:23:29,518 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:23:29,519 - INFO - Starting evaluation run...
2025-12-14 16:23:29,519 - INFO - Output directory set to: longbenchresult
2025-12-14 16:23:29,519 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 16:23:29,519 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:23:29,519 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:23:29,519 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.14it/s]
Device set to use cuda:0
2025-12-14 16:23:43,586 - INFO - Model pipeline loaded.
2025-12-14 16:23:43,587 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 16:23:49,301 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:23:49,301 - INFO - Dataset processed with 200 entries.
2025-12-14 16:23:49,333 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:29,  3.47s/it]Running Inference:   1%|          | 2/200 [00:04<07:34,  2.30s/it]Running Inference:   2%|▏         | 3/200 [00:06<06:58,  2.12s/it]Running Inference:   2%|▏         | 4/200 [00:08<06:17,  1.93s/it]Running Inference:   2%|▎         | 5/200 [00:10<06:01,  1.86s/it]Running Inference:   3%|▎         | 6/200 [00:13<07:24,  2.29s/it]Running Inference:   4%|▎         | 7/200 [00:17<09:05,  2.83s/it]Running Inference:   4%|▍         | 8/200 [00:20<09:20,  2.92s/it]Running Inference:   4%|▍         | 9/200 [00:21<07:58,  2.50s/it]Running Inference:   5%|▌         | 10/200 [00:23<06:50,  2.16s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:26<07:36,  2.41s/it]Running Inference:   6%|▌         | 12/200 [00:28<07:02,  2.25s/it]Running Inference:   6%|▋         | 13/200 [00:31<08:08,  2.61s/it]Running Inference:   7%|▋         | 14/200 [00:34<08:40,  2.80s/it]Running Inference:   8%|▊         | 15/200 [00:37<08:46,  2.85s/it]Running Inference:   8%|▊         | 16/200 [00:41<09:31,  3.11s/it]Running Inference:   8%|▊         | 17/200 [00:44<09:43,  3.19s/it]Running Inference:   9%|▉         | 18/200 [00:48<10:04,  3.32s/it]Running Inference:  10%|▉         | 19/200 [00:52<10:23,  3.45s/it]Running Inference:  10%|█         | 20/200 [00:54<08:45,  2.92s/it]Running Inference:  10%|█         | 21/200 [00:57<09:37,  3.22s/it]Running Inference:  11%|█         | 22/200 [00:59<08:05,  2.73s/it]Running Inference:  12%|█▏        | 23/200 [01:01<07:08,  2.42s/it]Running Inference:  12%|█▏        | 24/200 [01:02<06:21,  2.17s/it]Running Inference:  12%|█▎        | 25/200 [01:04<05:37,  1.93s/it]Running Inference:  13%|█▎        | 26/200 [01:05<05:23,  1.86s/it]Running Inference:  14%|█▎        | 27/200 [01:09<07:04,  2.45s/it]Running Inference:  14%|█▍        | 28/200 [01:13<07:52,  2.75s/it]Running Inference:  14%|█▍        | 29/200 [01:16<07:54,  2.78s/it]Running Inference:  15%|█▌        | 30/200 [01:19<08:37,  3.04s/it]Running Inference:  16%|█▌        | 31/200 [01:21<07:33,  2.68s/it]Running Inference:  16%|█▌        | 32/200 [01:23<06:46,  2.42s/it]Running Inference:  16%|█▋        | 33/200 [01:27<07:52,  2.83s/it]Running Inference:  17%|█▋        | 34/200 [01:30<07:59,  2.89s/it]Running Inference:  18%|█▊        | 35/200 [01:34<08:45,  3.19s/it]Running Inference:  18%|█▊        | 36/200 [01:35<07:13,  2.64s/it]Running Inference:  18%|█▊        | 37/200 [01:39<08:09,  3.00s/it]Running Inference:  19%|█▉        | 38/200 [01:43<08:47,  3.26s/it]Running Inference:  20%|█▉        | 39/200 [01:45<08:24,  3.14s/it]Running Inference:  20%|██        | 40/200 [01:49<08:50,  3.32s/it]Running Inference:  20%|██        | 41/200 [01:53<08:57,  3.38s/it]Running Inference:  21%|██        | 42/200 [01:56<08:54,  3.39s/it]Running Inference:  22%|██▏       | 43/200 [01:58<07:45,  2.97s/it]Running Inference:  22%|██▏       | 44/200 [02:00<06:48,  2.62s/it]Running Inference:  22%|██▎       | 45/200 [02:02<06:25,  2.48s/it]Running Inference:  23%|██▎       | 46/200 [02:06<07:29,  2.92s/it]Running Inference:  24%|██▎       | 47/200 [02:08<06:21,  2.49s/it]Running Inference:  24%|██▍       | 48/200 [02:11<07:06,  2.81s/it]Running Inference:  24%|██▍       | 49/200 [02:15<07:55,  3.15s/it]Running Inference:  25%|██▌       | 50/200 [02:19<08:15,  3.31s/it]Running Inference:  26%|██▌       | 51/200 [02:22<08:31,  3.44s/it]Running Inference:  26%|██▌       | 52/200 [02:26<08:17,  3.36s/it]Running Inference:  26%|██▋       | 53/200 [02:30<08:39,  3.53s/it]Running Inference:  27%|██▋       | 54/200 [02:33<08:24,  3.46s/it]Running Inference:  28%|██▊       | 55/200 [02:34<06:53,  2.85s/it]Running Inference:  28%|██▊       | 56/200 [02:37<07:06,  2.96s/it]Running Inference:  28%|██▊       | 57/200 [02:41<07:25,  3.11s/it]Running Inference:  29%|██▉       | 58/200 [02:43<06:26,  2.72s/it]Running Inference:  30%|██▉       | 59/200 [02:46<06:54,  2.94s/it]Running Inference:  30%|███       | 60/200 [02:50<07:23,  3.17s/it]Running Inference:  30%|███       | 61/200 [02:53<07:38,  3.30s/it]Running Inference:  31%|███       | 62/200 [02:57<08:00,  3.48s/it]Running Inference:  32%|███▏      | 63/200 [03:00<07:23,  3.23s/it]Running Inference:  32%|███▏      | 64/200 [03:02<06:45,  2.98s/it]Running Inference:  32%|███▎      | 65/200 [03:06<07:06,  3.16s/it]Running Inference:  33%|███▎      | 66/200 [03:10<07:37,  3.41s/it]Running Inference:  34%|███▎      | 67/200 [03:11<06:12,  2.80s/it]Running Inference:  34%|███▍      | 68/200 [03:13<05:32,  2.52s/it]Running Inference:  34%|███▍      | 69/200 [03:17<06:24,  2.94s/it]Running Inference:  35%|███▌      | 70/200 [03:19<05:39,  2.61s/it]Running Inference:  36%|███▌      | 71/200 [03:23<06:28,  3.01s/it]Running Inference:  36%|███▌      | 72/200 [03:25<05:40,  2.66s/it]Running Inference:  36%|███▋      | 73/200 [03:28<06:04,  2.87s/it]Running Inference:  37%|███▋      | 74/200 [03:31<06:00,  2.86s/it]Running Inference:  38%|███▊      | 75/200 [03:32<05:06,  2.45s/it]Running Inference:  38%|███▊      | 76/200 [03:36<05:27,  2.64s/it]Running Inference:  38%|███▊      | 77/200 [03:37<04:47,  2.34s/it]Running Inference:  39%|███▉      | 78/200 [03:40<05:14,  2.58s/it]Running Inference:  40%|███▉      | 79/200 [03:43<05:19,  2.64s/it]Running Inference:  40%|████      | 80/200 [03:47<05:48,  2.91s/it]Running Inference:  40%|████      | 81/200 [03:48<04:54,  2.48s/it]Running Inference:  41%|████      | 82/200 [03:52<05:27,  2.78s/it]Running Inference:  42%|████▏     | 83/200 [03:55<05:57,  3.06s/it]Running Inference:  42%|████▏     | 84/200 [03:59<06:13,  3.22s/it]Running Inference:  42%|████▎     | 85/200 [04:00<05:12,  2.72s/it]Running Inference:  43%|████▎     | 86/200 [04:02<04:35,  2.42s/it]Running Inference:  44%|████▎     | 87/200 [04:06<05:05,  2.70s/it]Running Inference:  44%|████▍     | 88/200 [04:09<05:31,  2.96s/it]Running Inference:  44%|████▍     | 89/200 [04:12<05:40,  3.07s/it]Running Inference:  45%|████▌     | 90/200 [04:16<05:54,  3.22s/it]Running Inference:  46%|████▌     | 91/200 [04:18<04:58,  2.74s/it]Running Inference:  46%|████▌     | 92/200 [04:21<05:31,  3.07s/it]Running Inference:  46%|████▋     | 93/200 [04:25<05:43,  3.21s/it]Running Inference:  47%|████▋     | 94/200 [04:29<06:02,  3.42s/it]Running Inference:  48%|████▊     | 95/200 [04:33<06:04,  3.47s/it]Running Inference:  48%|████▊     | 96/200 [04:36<06:05,  3.51s/it]Running Inference:  48%|████▊     | 97/200 [04:38<05:03,  2.94s/it]Running Inference:  49%|████▉     | 98/200 [04:42<05:29,  3.23s/it]Running Inference:  50%|████▉     | 99/200 [04:45<05:37,  3.34s/it]Running Inference:  50%|█████     | 100/200 [04:49<05:45,  3.45s/it]Running Inference:  50%|█████     | 101/200 [04:50<04:42,  2.86s/it]Running Inference:  51%|█████     | 102/200 [04:53<04:30,  2.76s/it]Running Inference:  52%|█████▏    | 103/200 [04:57<04:50,  3.00s/it]Running Inference:  52%|█████▏    | 104/200 [04:58<04:05,  2.56s/it]Running Inference:  52%|█████▎    | 105/200 [05:00<03:41,  2.33s/it]Running Inference:  53%|█████▎    | 106/200 [05:03<03:50,  2.45s/it]Running Inference:  54%|█████▎    | 107/200 [05:06<04:16,  2.76s/it]Running Inference:  54%|█████▍    | 108/200 [05:08<03:51,  2.52s/it]Running Inference:  55%|█████▍    | 109/200 [05:10<03:31,  2.33s/it]Running Inference:  55%|█████▌    | 110/200 [05:12<03:16,  2.18s/it]Running Inference:  56%|█████▌    | 111/200 [05:15<03:47,  2.56s/it]Running Inference:  56%|█████▌    | 112/200 [05:19<04:14,  2.89s/it]Running Inference:  56%|█████▋    | 113/200 [05:23<04:34,  3.15s/it]Running Inference:  57%|█████▋    | 114/200 [05:26<04:41,  3.27s/it]Running Inference:  57%|█████▊    | 115/200 [05:28<03:54,  2.76s/it]Running Inference:  58%|█████▊    | 116/200 [05:31<04:09,  2.98s/it]Running Inference:  58%|█████▊    | 117/200 [05:35<04:20,  3.14s/it]Running Inference:  59%|█████▉    | 118/200 [05:38<04:25,  3.23s/it]Running Inference:  60%|█████▉    | 119/200 [05:40<03:47,  2.81s/it]Running Inference:  60%|██████    | 120/200 [05:44<04:11,  3.14s/it]Running Inference:  60%|██████    | 121/200 [05:48<04:21,  3.31s/it]Running Inference:  61%|██████    | 122/200 [05:51<04:28,  3.44s/it]Running Inference:  62%|██████▏   | 123/200 [05:55<04:27,  3.48s/it]Running Inference:  62%|██████▏   | 124/200 [05:58<04:15,  3.36s/it]Running Inference:  62%|██████▎   | 125/200 [06:01<04:12,  3.37s/it]Running Inference:  63%|██████▎   | 126/200 [06:05<04:06,  3.33s/it]Running Inference:  64%|██████▎   | 127/200 [06:08<04:09,  3.42s/it]Running Inference:  64%|██████▍   | 128/200 [06:10<03:24,  2.84s/it]Running Inference:  64%|██████▍   | 129/200 [06:11<02:48,  2.38s/it]Running Inference:  65%|██████▌   | 130/200 [06:14<02:53,  2.47s/it]Running Inference:  66%|██████▌   | 131/200 [06:17<02:59,  2.60s/it]Running Inference:  66%|██████▌   | 132/200 [06:21<03:23,  2.99s/it]Running Inference:  66%|██████▋   | 133/200 [06:24<03:37,  3.25s/it]Running Inference:  67%|██████▋   | 134/200 [06:28<03:44,  3.40s/it]Running Inference:  68%|██████▊   | 135/200 [06:30<03:04,  2.85s/it]Running Inference:  68%|██████▊   | 136/200 [06:31<02:34,  2.41s/it]Running Inference:  68%|██████▊   | 137/200 [06:35<02:57,  2.82s/it]Running Inference:  69%|██████▉   | 138/200 [06:36<02:32,  2.46s/it]Running Inference:  70%|██████▉   | 139/200 [06:40<02:53,  2.84s/it]Running Inference:  70%|███████   | 140/200 [06:44<03:02,  3.05s/it]Running Inference:  70%|███████   | 141/200 [06:47<03:10,  3.23s/it]Running Inference:  71%|███████   | 142/200 [06:49<02:37,  2.72s/it]Running Inference:  72%|███████▏  | 143/200 [06:51<02:18,  2.42s/it]Running Inference:  72%|███████▏  | 144/200 [06:53<02:08,  2.30s/it]Running Inference:  72%|███████▎  | 145/200 [06:54<01:51,  2.02s/it]Running Inference:  73%|███████▎  | 146/200 [06:56<01:44,  1.94s/it]Running Inference:  74%|███████▎  | 147/200 [06:59<02:02,  2.31s/it]Running Inference:  74%|███████▍  | 148/200 [07:01<01:53,  2.18s/it]Running Inference:  74%|███████▍  | 149/200 [07:02<01:42,  2.01s/it]Running Inference:  75%|███████▌  | 150/200 [07:05<01:56,  2.32s/it]Running Inference:  76%|███████▌  | 151/200 [07:07<01:45,  2.15s/it]Running Inference:  76%|███████▌  | 152/200 [07:10<01:51,  2.32s/it]Running Inference:  76%|███████▋  | 153/200 [07:13<01:57,  2.50s/it]Running Inference:  77%|███████▋  | 154/200 [07:15<01:48,  2.35s/it]Running Inference:  78%|███████▊  | 155/200 [07:17<01:36,  2.14s/it]Running Inference:  78%|███████▊  | 156/200 [07:18<01:30,  2.06s/it]Running Inference:  78%|███████▊  | 157/200 [07:20<01:22,  1.93s/it]Running Inference:  79%|███████▉  | 158/200 [07:22<01:15,  1.81s/it]Running Inference:  80%|███████▉  | 159/200 [07:24<01:24,  2.07s/it]Running Inference:  80%|████████  | 160/200 [07:27<01:35,  2.39s/it]Running Inference:  80%|████████  | 161/200 [07:29<01:23,  2.13s/it]Running Inference:  81%|████████  | 162/200 [07:31<01:15,  1.98s/it]Running Inference:  82%|████████▏ | 163/200 [07:33<01:22,  2.23s/it]Running Inference:  82%|████████▏ | 164/200 [07:35<01:14,  2.07s/it]Running Inference:  82%|████████▎ | 165/200 [07:37<01:08,  1.95s/it]Running Inference:  83%|████████▎ | 166/200 [07:40<01:18,  2.30s/it]Running Inference:  84%|████████▎ | 167/200 [07:43<01:20,  2.45s/it]Running Inference:  84%|████████▍ | 168/200 [07:46<01:31,  2.84s/it]Running Inference:  84%|████████▍ | 169/200 [07:50<01:37,  3.14s/it]Running Inference:  85%|████████▌ | 170/200 [07:52<01:22,  2.76s/it]Running Inference:  86%|████████▌ | 171/200 [07:56<01:29,  3.09s/it]Running Inference:  86%|████████▌ | 172/200 [08:00<01:30,  3.23s/it]Running Inference:  86%|████████▋ | 173/200 [08:03<01:31,  3.38s/it]Running Inference:  87%|████████▋ | 174/200 [08:05<01:15,  2.91s/it]Running Inference:  88%|████████▊ | 175/200 [08:07<01:03,  2.52s/it]Running Inference:  88%|████████▊ | 176/200 [08:08<00:52,  2.19s/it]Running Inference:  88%|████████▊ | 177/200 [08:10<00:47,  2.05s/it]Running Inference:  89%|████████▉ | 178/200 [08:13<00:50,  2.32s/it]Running Inference:  90%|████████▉ | 179/200 [08:14<00:43,  2.06s/it]Running Inference:  90%|█████████ | 180/200 [08:18<00:50,  2.54s/it]Running Inference:  90%|█████████ | 181/200 [08:19<00:42,  2.23s/it]Running Inference:  91%|█████████ | 182/200 [08:23<00:47,  2.66s/it]Running Inference:  92%|█████████▏| 183/200 [08:26<00:48,  2.86s/it]Running Inference:  92%|█████████▏| 184/200 [08:28<00:40,  2.53s/it]Running Inference:  92%|█████████▎| 185/200 [08:30<00:37,  2.47s/it]Running Inference:  93%|█████████▎| 186/200 [08:34<00:39,  2.85s/it]Running Inference:  94%|█████████▎| 187/200 [08:37<00:35,  2.74s/it]Running Inference:  94%|█████████▍| 188/200 [08:40<00:33,  2.79s/it]Running Inference:  94%|█████████▍| 189/200 [08:42<00:29,  2.71s/it]Running Inference:  95%|█████████▌| 190/200 [08:44<00:23,  2.32s/it]Running Inference:  96%|█████████▌| 191/200 [08:47<00:24,  2.73s/it]Running Inference:  96%|█████████▌| 192/200 [08:51<00:24,  3.04s/it]Running Inference:  96%|█████████▋| 193/200 [08:55<00:22,  3.26s/it]Running Inference:  97%|█████████▋| 194/200 [08:59<00:20,  3.44s/it]Running Inference:  98%|█████████▊| 195/200 [09:02<00:17,  3.56s/it]Running Inference:  98%|█████████▊| 196/200 [09:06<00:14,  3.63s/it]Running Inference:  98%|█████████▊| 197/200 [09:10<00:10,  3.57s/it]Running Inference:  99%|█████████▉| 198/200 [09:11<00:05,  2.99s/it]Running Inference: 100%|█████████▉| 199/200 [09:15<00:03,  3.31s/it]Running Inference: 100%|██████████| 200/200 [09:19<00:00,  3.39s/it]Running Inference: 100%|██████████| 200/200 [09:19<00:00,  2.80s/it]
2025-12-14 16:33:08,795 - INFO - Inference completed.
2025-12-14 16:33:08,806 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 16:33:08,806 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:33:08,808 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 16:33:08,808 - INFO - Metrics:
60.08
2025-12-14 16:33:08,809 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_en, ratio=0.1) on GPU 2

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:33:15,241 - INFO - Set deterministic seeds to 42
2025-12-14 16:33:15,241 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:33:15,241 - INFO - Starting evaluation run...
2025-12-14 16:33:15,241 - INFO - Output directory set to: longbenchresult
2025-12-14 16:33:15,241 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 16:33:15,242 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:33:15,242 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:33:15,242 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.11it/s]
Device set to use cuda:0
2025-12-14 16:33:28,383 - INFO - Model pipeline loaded.
2025-12-14 16:33:28,383 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 16:33:33,544 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:33:33,544 - INFO - Dataset processed with 200 entries.
2025-12-14 16:33:33,576 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:12,  3.38s/it]Running Inference:   1%|          | 2/200 [00:06<10:19,  3.13s/it]Running Inference:   2%|▏         | 3/200 [00:10<11:22,  3.47s/it]Running Inference:   2%|▏         | 4/200 [00:13<11:35,  3.55s/it]Running Inference:   2%|▎         | 5/200 [00:15<09:23,  2.89s/it]Running Inference:   3%|▎         | 6/200 [00:18<09:32,  2.95s/it]Running Inference:   4%|▎         | 7/200 [00:22<10:27,  3.25s/it]Running Inference:   4%|▍         | 8/200 [00:25<10:12,  3.19s/it]Running Inference:   4%|▍         | 9/200 [00:27<08:33,  2.69s/it]Running Inference:   5%|▌         | 10/200 [00:28<07:14,  2.29s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:31<07:49,  2.49s/it]Running Inference:   6%|▌         | 12/200 [00:33<07:11,  2.30s/it]Running Inference:   6%|▋         | 13/200 [00:36<08:15,  2.65s/it]Running Inference:   7%|▋         | 14/200 [00:39<08:41,  2.80s/it]Running Inference:   8%|▊         | 15/200 [00:42<08:46,  2.84s/it]Running Inference:   8%|▊         | 16/200 [00:46<09:24,  3.07s/it]Running Inference:   8%|▊         | 17/200 [00:49<09:34,  3.14s/it]Running Inference:   9%|▉         | 18/200 [00:53<09:54,  3.27s/it]Running Inference:  10%|▉         | 19/200 [00:57<10:13,  3.39s/it]Running Inference:  10%|█         | 20/200 [00:58<08:38,  2.88s/it]Running Inference:  10%|█         | 21/200 [01:02<09:28,  3.17s/it]Running Inference:  11%|█         | 22/200 [01:04<07:58,  2.69s/it]Running Inference:  12%|█▏        | 23/200 [01:05<07:04,  2.40s/it]Running Inference:  12%|█▏        | 24/200 [01:07<06:18,  2.15s/it]Running Inference:  12%|█▎        | 25/200 [01:08<05:34,  1.91s/it]Running Inference:  13%|█▎        | 26/200 [01:10<05:20,  1.84s/it]Running Inference:  14%|█▎        | 27/200 [01:14<07:13,  2.51s/it]Running Inference:  14%|█▍        | 28/200 [01:16<06:59,  2.44s/it]Running Inference:  14%|█▍        | 29/200 [01:19<07:14,  2.54s/it]Running Inference:  15%|█▌        | 30/200 [01:23<08:01,  2.83s/it]Running Inference:  16%|█▌        | 31/200 [01:24<07:07,  2.53s/it]Running Inference:  16%|█▌        | 32/200 [01:28<08:11,  2.93s/it]Running Inference:  16%|█▋        | 33/200 [01:32<08:47,  3.16s/it]Running Inference:  17%|█▋        | 34/200 [01:34<07:44,  2.80s/it]Running Inference:  18%|█▊        | 35/200 [01:36<06:49,  2.48s/it]Running Inference:  18%|█▊        | 36/200 [01:37<05:52,  2.15s/it]Running Inference:  18%|█▊        | 37/200 [01:41<07:07,  2.62s/it]Running Inference:  19%|█▉        | 38/200 [01:45<07:59,  2.96s/it]Running Inference:  20%|█▉        | 39/200 [01:47<07:49,  2.92s/it]Running Inference:  20%|██        | 40/200 [01:51<08:22,  3.14s/it]Running Inference:  20%|██        | 41/200 [01:54<08:31,  3.22s/it]Running Inference:  21%|██        | 42/200 [01:56<07:01,  2.67s/it]Running Inference:  22%|██▏       | 43/200 [01:58<06:26,  2.46s/it]Running Inference:  22%|██▏       | 44/200 [02:00<05:53,  2.26s/it]Running Inference:  22%|██▎       | 45/200 [02:02<05:45,  2.23s/it]Running Inference:  23%|██▎       | 46/200 [02:05<06:35,  2.57s/it]Running Inference:  24%|██▎       | 47/200 [02:09<07:16,  2.86s/it]Running Inference:  24%|██▍       | 48/200 [02:12<07:41,  3.04s/it]Running Inference:  24%|██▍       | 49/200 [02:14<06:45,  2.68s/it]Running Inference:  25%|██▌       | 50/200 [02:18<07:25,  2.97s/it]Running Inference:  26%|██▌       | 51/200 [02:21<07:31,  3.03s/it]Running Inference:  26%|██▌       | 52/200 [02:24<07:34,  3.07s/it]Running Inference:  26%|██▋       | 53/200 [02:28<08:07,  3.32s/it]Running Inference:  27%|██▋       | 54/200 [02:32<08:25,  3.46s/it]Running Inference:  28%|██▊       | 55/200 [02:33<06:54,  2.86s/it]Running Inference:  28%|██▊       | 56/200 [02:37<07:28,  3.12s/it]Running Inference:  28%|██▊       | 57/200 [02:40<07:39,  3.21s/it]Running Inference:  29%|██▉       | 58/200 [02:42<06:35,  2.79s/it]Running Inference:  30%|██▉       | 59/200 [02:44<05:51,  2.49s/it]Running Inference:  30%|███       | 60/200 [02:47<06:23,  2.74s/it]Running Inference:  30%|███       | 61/200 [02:51<06:55,  2.99s/it]Running Inference:  31%|███       | 62/200 [02:55<07:28,  3.25s/it]Running Inference:  32%|███▏      | 63/200 [02:57<07:06,  3.11s/it]Running Inference:  32%|███▏      | 64/200 [03:01<07:23,  3.26s/it]Running Inference:  32%|███▎      | 65/200 [03:04<07:31,  3.34s/it]Running Inference:  33%|███▎      | 66/200 [03:09<07:56,  3.55s/it]Running Inference:  34%|███▎      | 67/200 [03:10<06:25,  2.90s/it]Running Inference:  34%|███▍      | 68/200 [03:12<05:41,  2.58s/it]Running Inference:  34%|███▍      | 69/200 [03:16<06:30,  2.98s/it]Running Inference:  35%|███▌      | 70/200 [03:17<05:42,  2.63s/it]Running Inference:  36%|███▌      | 71/200 [03:21<06:30,  3.02s/it]Running Inference:  36%|███▌      | 72/200 [03:23<05:42,  2.67s/it]Running Inference:  36%|███▋      | 73/200 [03:27<06:03,  2.86s/it]Running Inference:  37%|███▋      | 74/200 [03:28<05:14,  2.50s/it]Running Inference:  38%|███▊      | 75/200 [03:30<04:34,  2.20s/it]Running Inference:  38%|███▊      | 76/200 [03:33<05:05,  2.46s/it]Running Inference:  38%|███▊      | 77/200 [03:34<04:32,  2.22s/it]Running Inference:  39%|███▉      | 78/200 [03:38<05:03,  2.49s/it]Running Inference:  40%|███▉      | 79/200 [03:40<05:11,  2.57s/it]Running Inference:  40%|████      | 80/200 [03:44<05:46,  2.88s/it]Running Inference:  40%|████      | 81/200 [03:45<04:54,  2.47s/it]Running Inference:  41%|████      | 82/200 [03:49<05:22,  2.74s/it]Running Inference:  42%|████▏     | 83/200 [03:50<04:41,  2.41s/it]Running Inference:  42%|████▏     | 84/200 [03:54<05:19,  2.75s/it]Running Inference:  42%|████▎     | 85/200 [03:56<04:35,  2.39s/it]Running Inference:  43%|████▎     | 86/200 [03:59<05:18,  2.80s/it]Running Inference:  44%|████▎     | 87/200 [04:03<05:30,  2.92s/it]Running Inference:  44%|████▍     | 88/200 [04:04<04:38,  2.49s/it]Running Inference:  44%|████▍     | 89/200 [04:07<05:05,  2.75s/it]Running Inference:  45%|████▌     | 90/200 [04:11<05:18,  2.90s/it]Running Inference:  46%|████▌     | 91/200 [04:12<04:33,  2.51s/it]Running Inference:  46%|████▌     | 92/200 [04:16<05:12,  2.89s/it]Running Inference:  46%|████▋     | 93/200 [04:17<04:24,  2.47s/it]Running Inference:  47%|████▋     | 94/200 [04:21<04:41,  2.66s/it]Running Inference:  48%|████▊     | 95/200 [04:24<05:07,  2.93s/it]Running Inference:  48%|████▊     | 96/200 [04:28<05:24,  3.12s/it]Running Inference:  48%|████▊     | 97/200 [04:29<04:34,  2.67s/it]Running Inference:  49%|████▉     | 98/200 [04:33<05:08,  3.02s/it]Running Inference:  50%|████▉     | 99/200 [04:37<05:21,  3.19s/it]Running Inference:  50%|█████     | 100/200 [04:39<04:51,  2.92s/it]Running Inference:  50%|█████     | 101/200 [04:40<04:05,  2.48s/it]Running Inference:  51%|█████     | 102/200 [04:44<04:36,  2.82s/it]Running Inference:  52%|█████▏    | 103/200 [04:48<04:55,  3.04s/it]Running Inference:  52%|█████▏    | 104/200 [04:49<04:08,  2.59s/it]Running Inference:  52%|█████▎    | 105/200 [04:51<03:44,  2.36s/it]Running Inference:  53%|█████▎    | 106/200 [04:54<03:51,  2.47s/it]Running Inference:  54%|█████▎    | 107/200 [04:57<04:12,  2.72s/it]Running Inference:  54%|█████▍    | 108/200 [04:59<03:49,  2.49s/it]Running Inference:  55%|█████▍    | 109/200 [05:01<03:29,  2.30s/it]Running Inference:  55%|█████▌    | 110/200 [05:05<04:09,  2.77s/it]Running Inference:  56%|█████▌    | 111/200 [05:08<04:26,  2.99s/it]Running Inference:  56%|█████▌    | 112/200 [05:10<03:45,  2.57s/it]Running Inference:  56%|█████▋    | 113/200 [05:14<04:13,  2.91s/it]Running Inference:  57%|█████▋    | 114/200 [05:17<04:25,  3.09s/it]Running Inference:  57%|█████▊    | 115/200 [05:21<04:35,  3.24s/it]Running Inference:  58%|█████▊    | 116/200 [05:24<04:37,  3.30s/it]Running Inference:  58%|█████▊    | 117/200 [05:28<04:38,  3.36s/it]Running Inference:  59%|█████▉    | 118/200 [05:30<04:24,  3.23s/it]Running Inference:  60%|█████▉    | 119/200 [05:32<03:47,  2.81s/it]Running Inference:  60%|██████    | 120/200 [05:36<04:10,  3.13s/it]Running Inference:  60%|██████    | 121/200 [05:38<03:31,  2.67s/it]Running Inference:  61%|██████    | 122/200 [05:41<03:48,  2.93s/it]Running Inference:  62%|██████▏   | 123/200 [05:45<03:59,  3.11s/it]Running Inference:  62%|██████▏   | 124/200 [05:48<03:55,  3.09s/it]Running Inference:  62%|██████▎   | 125/200 [05:52<04:03,  3.25s/it]Running Inference:  63%|██████▎   | 126/200 [05:54<03:42,  3.00s/it]Running Inference:  64%|██████▎   | 127/200 [05:57<03:45,  3.09s/it]Running Inference:  64%|██████▍   | 128/200 [05:59<03:07,  2.60s/it]Running Inference:  64%|██████▍   | 129/200 [06:00<02:37,  2.21s/it]Running Inference:  65%|██████▌   | 130/200 [06:03<02:56,  2.52s/it]Running Inference:  66%|██████▌   | 131/200 [06:07<03:16,  2.85s/it]Running Inference:  66%|██████▌   | 132/200 [06:11<03:34,  3.16s/it]Running Inference:  66%|██████▋   | 133/200 [06:14<03:38,  3.27s/it]Running Inference:  67%|██████▋   | 134/200 [06:18<03:44,  3.40s/it]Running Inference:  68%|██████▊   | 135/200 [06:21<03:43,  3.43s/it]Running Inference:  68%|██████▊   | 136/200 [06:23<03:00,  2.82s/it]Running Inference:  68%|██████▊   | 137/200 [06:27<03:14,  3.08s/it]Running Inference:  69%|██████▉   | 138/200 [06:28<02:44,  2.65s/it]Running Inference:  70%|██████▉   | 139/200 [06:30<02:23,  2.35s/it]Running Inference:  70%|███████   | 140/200 [06:33<02:41,  2.69s/it]Running Inference:  70%|███████   | 141/200 [06:35<02:18,  2.35s/it]Running Inference:  71%|███████   | 142/200 [06:36<02:02,  2.10s/it]Running Inference:  72%|███████▏  | 143/200 [06:38<01:53,  1.99s/it]Running Inference:  72%|███████▏  | 144/200 [06:40<01:51,  1.99s/it]Running Inference:  72%|███████▎  | 145/200 [06:42<01:39,  1.81s/it]Running Inference:  73%|███████▎  | 146/200 [06:45<02:07,  2.36s/it]Running Inference:  74%|███████▎  | 147/200 [06:49<02:21,  2.67s/it]Running Inference:  74%|███████▍  | 148/200 [06:50<02:05,  2.42s/it]Running Inference:  74%|███████▍  | 149/200 [06:52<01:51,  2.18s/it]Running Inference:  75%|███████▌  | 150/200 [06:54<01:38,  1.98s/it]Running Inference:  76%|███████▌  | 151/200 [06:55<01:33,  1.91s/it]Running Inference:  76%|███████▌  | 152/200 [06:57<01:27,  1.81s/it]Running Inference:  76%|███████▋  | 153/200 [07:00<01:40,  2.13s/it]Running Inference:  77%|███████▋  | 154/200 [07:01<01:27,  1.90s/it]Running Inference:  78%|███████▊  | 155/200 [07:03<01:22,  1.82s/it]Running Inference:  78%|███████▊  | 156/200 [07:05<01:20,  1.83s/it]Running Inference:  78%|███████▊  | 157/200 [07:06<01:16,  1.77s/it]Running Inference:  79%|███████▉  | 158/200 [07:08<01:11,  1.70s/it]Running Inference:  80%|███████▉  | 159/200 [07:10<01:21,  2.00s/it]Running Inference:  80%|████████  | 160/200 [07:14<01:33,  2.33s/it]Running Inference:  80%|████████  | 161/200 [07:15<01:21,  2.09s/it]Running Inference:  81%|████████  | 162/200 [07:17<01:14,  1.95s/it]Running Inference:  82%|████████▏ | 163/200 [07:19<01:21,  2.20s/it]Running Inference:  82%|████████▏ | 164/200 [07:21<01:13,  2.06s/it]Running Inference:  82%|████████▎ | 165/200 [07:23<01:07,  1.94s/it]Running Inference:  83%|████████▎ | 166/200 [07:25<01:05,  1.91s/it]Running Inference:  84%|████████▎ | 167/200 [07:28<01:11,  2.17s/it]Running Inference:  84%|████████▍ | 168/200 [07:31<01:20,  2.51s/it]Running Inference:  84%|████████▍ | 169/200 [07:35<01:29,  2.89s/it]Running Inference:  85%|████████▌ | 170/200 [07:36<01:17,  2.58s/it]Running Inference:  86%|████████▌ | 171/200 [07:38<01:08,  2.35s/it]Running Inference:  86%|████████▌ | 172/200 [07:41<01:06,  2.37s/it]Running Inference:  86%|████████▋ | 173/200 [07:44<01:14,  2.77s/it]Running Inference:  87%|████████▋ | 174/200 [07:46<01:04,  2.48s/it]Running Inference:  88%|████████▊ | 175/200 [07:48<00:55,  2.22s/it]Running Inference:  88%|████████▊ | 176/200 [07:49<00:47,  1.98s/it]Running Inference:  88%|████████▊ | 177/200 [07:52<00:48,  2.09s/it]Running Inference:  89%|████████▉ | 178/200 [07:54<00:51,  2.34s/it]Running Inference:  90%|████████▉ | 179/200 [07:56<00:43,  2.07s/it]Running Inference:  90%|█████████ | 180/200 [07:58<00:38,  1.93s/it]Running Inference:  90%|█████████ | 181/200 [07:59<00:34,  1.80s/it]Running Inference:  91%|█████████ | 182/200 [08:03<00:42,  2.35s/it]Running Inference:  92%|█████████▏| 183/200 [08:06<00:44,  2.63s/it]Running Inference:  92%|█████████▏| 184/200 [08:08<00:37,  2.37s/it]Running Inference:  92%|█████████▎| 185/200 [08:10<00:35,  2.36s/it]Running Inference:  93%|█████████▎| 186/200 [08:14<00:38,  2.75s/it]Running Inference:  94%|█████████▎| 187/200 [08:16<00:34,  2.63s/it]Running Inference:  94%|█████████▍| 188/200 [08:20<00:36,  3.00s/it]Running Inference:  94%|█████████▍| 189/200 [08:22<00:31,  2.85s/it]Running Inference:  95%|█████████▌| 190/200 [08:24<00:24,  2.42s/it]Running Inference:  96%|█████████▌| 191/200 [08:27<00:25,  2.79s/it]Running Inference:  96%|█████████▌| 192/200 [08:31<00:24,  3.07s/it]Running Inference:  96%|█████████▋| 193/200 [08:35<00:22,  3.27s/it]Running Inference:  97%|█████████▋| 194/200 [08:39<00:20,  3.44s/it]Running Inference:  98%|█████████▊| 195/200 [08:43<00:17,  3.55s/it]Running Inference:  98%|█████████▊| 196/200 [08:46<00:14,  3.61s/it]Running Inference:  98%|█████████▊| 197/200 [08:48<00:09,  3.05s/it]Running Inference:  99%|█████████▉| 198/200 [08:50<00:05,  2.62s/it]Running Inference: 100%|█████████▉| 199/200 [08:54<00:03,  3.06s/it]Running Inference: 100%|██████████| 200/200 [08:57<00:00,  3.03s/it]Running Inference: 100%|██████████| 200/200 [08:57<00:00,  2.69s/it]
2025-12-14 16:42:30,847 - INFO - Inference completed.
2025-12-14 16:42:30,859 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 16:42:30,859 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:42:30,860 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 16:42:30,860 - INFO - Metrics:
53.42
2025-12-14 16:42:30,862 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_en, ratio=0.2) on GPU 2

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:42:37,295 - INFO - Set deterministic seeds to 42
2025-12-14 16:42:37,295 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:42:37,296 - INFO - Starting evaluation run...
2025-12-14 16:42:37,296 - INFO - Output directory set to: longbenchresult
2025-12-14 16:42:37,296 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 16:42:37,296 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:42:37,296 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:42:37,296 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.11it/s]
Device set to use cuda:0
2025-12-14 16:42:52,565 - INFO - Model pipeline loaded.
2025-12-14 16:42:52,565 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 16:43:01,496 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:43:01,496 - INFO - Dataset processed with 200 entries.
2025-12-14 16:43:01,529 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:38,  2.31s/it]Running Inference:   1%|          | 2/200 [00:05<08:50,  2.68s/it]Running Inference:   2%|▏         | 3/200 [00:07<07:38,  2.33s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:38,  2.65s/it]Running Inference:   2%|▎         | 5/200 [00:12<07:31,  2.31s/it]Running Inference:   3%|▎         | 6/200 [00:13<06:29,  2.01s/it]Running Inference:   4%|▎         | 7/200 [00:17<08:22,  2.60s/it]Running Inference:   4%|▍         | 8/200 [00:20<09:25,  2.95s/it]Running Inference:   4%|▍         | 9/200 [00:22<08:00,  2.52s/it]Running Inference:   5%|▌         | 10/200 [00:23<06:52,  2.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:26<07:33,  2.40s/it]Running Inference:   6%|▌         | 12/200 [00:30<08:56,  2.85s/it]Running Inference:   6%|▋         | 13/200 [00:34<09:22,  3.01s/it]Running Inference:   7%|▋         | 14/200 [00:37<09:52,  3.19s/it]Running Inference:   8%|▊         | 15/200 [00:40<09:34,  3.11s/it]Running Inference:   8%|▊         | 16/200 [00:44<10:00,  3.26s/it]Running Inference:   8%|▊         | 17/200 [00:47<09:58,  3.27s/it]Running Inference:   9%|▉         | 18/200 [00:49<08:19,  2.74s/it]Running Inference:  10%|▉         | 19/200 [00:50<07:15,  2.41s/it]Running Inference:  10%|█         | 20/200 [00:52<06:34,  2.19s/it]Running Inference:  10%|█         | 21/200 [00:56<08:00,  2.68s/it]Running Inference:  11%|█         | 22/200 [00:57<06:57,  2.35s/it]Running Inference:  12%|█▏        | 23/200 [00:59<06:21,  2.15s/it]Running Inference:  12%|█▏        | 24/200 [01:01<05:47,  1.98s/it]Running Inference:  12%|█▎        | 25/200 [01:02<05:13,  1.79s/it]Running Inference:  13%|█▎        | 26/200 [01:04<05:06,  1.76s/it]Running Inference:  14%|█▎        | 27/200 [01:07<06:48,  2.36s/it]Running Inference:  14%|█▍        | 28/200 [01:11<07:37,  2.66s/it]Running Inference:  14%|█▍        | 29/200 [01:14<08:13,  2.88s/it]Running Inference:  15%|█▌        | 30/200 [01:18<08:41,  3.07s/it]Running Inference:  16%|█▌        | 31/200 [01:19<07:35,  2.70s/it]Running Inference:  16%|█▌        | 32/200 [01:21<06:48,  2.43s/it]Running Inference:  16%|█▋        | 33/200 [01:23<06:36,  2.38s/it]Running Inference:  17%|█▋        | 34/200 [01:25<06:13,  2.25s/it]Running Inference:  18%|█▊        | 35/200 [01:27<05:46,  2.10s/it]Running Inference:  18%|█▊        | 36/200 [01:29<05:08,  1.88s/it]Running Inference:  18%|█▊        | 37/200 [01:32<06:35,  2.43s/it]Running Inference:  19%|█▉        | 38/200 [01:34<05:57,  2.21s/it]Running Inference:  20%|█▉        | 39/200 [01:37<06:24,  2.39s/it]Running Inference:  20%|██        | 40/200 [01:40<07:22,  2.77s/it]Running Inference:  20%|██        | 41/200 [01:44<07:49,  2.95s/it]Running Inference:  21%|██        | 42/200 [01:45<06:31,  2.48s/it]Running Inference:  22%|██▏       | 43/200 [01:47<06:05,  2.33s/it]Running Inference:  22%|██▏       | 44/200 [01:49<05:38,  2.17s/it]Running Inference:  22%|██▎       | 45/200 [01:51<05:35,  2.16s/it]Running Inference:  23%|██▎       | 46/200 [01:55<06:39,  2.60s/it]Running Inference:  24%|██▎       | 47/200 [01:58<07:19,  2.87s/it]Running Inference:  24%|██▍       | 48/200 [02:02<07:42,  3.04s/it]Running Inference:  24%|██▍       | 49/200 [02:03<06:45,  2.69s/it]Running Inference:  25%|██▌       | 50/200 [02:07<07:24,  2.97s/it]Running Inference:  26%|██▌       | 51/200 [02:11<07:53,  3.18s/it]Running Inference:  26%|██▌       | 52/200 [02:14<07:49,  3.17s/it]Running Inference:  26%|██▋       | 53/200 [02:18<08:17,  3.38s/it]Running Inference:  27%|██▋       | 54/200 [02:22<08:31,  3.50s/it]Running Inference:  28%|██▊       | 55/200 [02:23<06:57,  2.88s/it]Running Inference:  28%|██▊       | 56/200 [02:25<06:03,  2.52s/it]Running Inference:  28%|██▊       | 57/200 [02:26<05:12,  2.18s/it]Running Inference:  29%|██▉       | 58/200 [02:28<04:53,  2.07s/it]Running Inference:  30%|██▉       | 59/200 [02:30<04:39,  1.99s/it]Running Inference:  30%|███       | 60/200 [02:31<04:22,  1.87s/it]Running Inference:  30%|███       | 61/200 [02:33<04:06,  1.77s/it]Running Inference:  31%|███       | 62/200 [02:35<04:06,  1.79s/it]Running Inference:  32%|███▏      | 63/200 [02:38<04:52,  2.14s/it]Running Inference:  32%|███▏      | 64/200 [02:39<04:26,  1.96s/it]Running Inference:  32%|███▎      | 65/200 [02:43<05:27,  2.43s/it]Running Inference:  33%|███▎      | 66/200 [02:47<06:26,  2.89s/it]Running Inference:  34%|███▎      | 67/200 [02:48<05:23,  2.43s/it]Running Inference:  34%|███▍      | 68/200 [02:50<04:57,  2.25s/it]Running Inference:  34%|███▍      | 69/200 [02:54<05:58,  2.74s/it]Running Inference:  35%|███▌      | 70/200 [02:56<05:20,  2.47s/it]Running Inference:  36%|███▌      | 71/200 [02:59<06:12,  2.89s/it]Running Inference:  36%|███▌      | 72/200 [03:01<05:29,  2.57s/it]Running Inference:  36%|███▋      | 73/200 [03:03<04:45,  2.25s/it]Running Inference:  37%|███▋      | 74/200 [03:06<05:18,  2.53s/it]Running Inference:  38%|███▊      | 75/200 [03:07<04:37,  2.22s/it]Running Inference:  38%|███▊      | 76/200 [03:11<05:06,  2.47s/it]Running Inference:  38%|███▊      | 77/200 [03:13<04:53,  2.39s/it]Running Inference:  39%|███▉      | 78/200 [03:16<05:38,  2.78s/it]Running Inference:  40%|███▉      | 79/200 [03:18<04:53,  2.43s/it]Running Inference:  40%|████      | 80/200 [03:21<05:21,  2.68s/it]Running Inference:  40%|████      | 81/200 [03:23<04:36,  2.33s/it]Running Inference:  41%|████      | 82/200 [03:26<05:17,  2.69s/it]Running Inference:  42%|████▏     | 83/200 [03:28<04:37,  2.38s/it]Running Inference:  42%|████▏     | 84/200 [03:31<05:15,  2.72s/it]Running Inference:  42%|████▎     | 85/200 [03:33<04:32,  2.37s/it]Running Inference:  43%|████▎     | 86/200 [03:35<04:07,  2.17s/it]Running Inference:  44%|████▎     | 87/200 [03:38<04:57,  2.63s/it]Running Inference:  44%|████▍     | 88/200 [03:42<05:23,  2.89s/it]Running Inference:  44%|████▍     | 89/200 [03:43<04:32,  2.46s/it]Running Inference:  45%|████▌     | 90/200 [03:47<05:05,  2.78s/it]Running Inference:  46%|████▌     | 91/200 [03:48<04:23,  2.42s/it]Running Inference:  46%|████▌     | 92/200 [03:50<03:59,  2.22s/it]Running Inference:  46%|████▋     | 93/200 [03:52<03:33,  1.99s/it]Running Inference:  47%|████▋     | 94/200 [03:55<04:06,  2.32s/it]Running Inference:  48%|████▊     | 95/200 [03:58<04:43,  2.70s/it]Running Inference:  48%|████▊     | 96/200 [04:00<04:04,  2.35s/it]Running Inference:  48%|████▊     | 97/200 [04:01<03:38,  2.12s/it]Running Inference:  49%|████▉     | 98/200 [04:05<04:28,  2.63s/it]Running Inference:  50%|████▉     | 99/200 [04:09<04:53,  2.91s/it]Running Inference:  50%|█████     | 100/200 [04:10<04:11,  2.52s/it]Running Inference:  50%|█████     | 101/200 [04:12<03:38,  2.20s/it]Running Inference:  51%|█████     | 102/200 [04:14<03:17,  2.01s/it]Running Inference:  52%|█████▏    | 103/200 [04:17<04:00,  2.48s/it]Running Inference:  52%|█████▏    | 104/200 [04:19<03:30,  2.19s/it]Running Inference:  52%|█████▎    | 105/200 [04:20<03:18,  2.08s/it]Running Inference:  53%|█████▎    | 106/200 [04:23<03:33,  2.27s/it]Running Inference:  54%|█████▎    | 107/200 [04:27<04:03,  2.61s/it]Running Inference:  54%|█████▍    | 108/200 [04:28<03:42,  2.42s/it]Running Inference:  55%|█████▍    | 109/200 [04:30<03:24,  2.25s/it]Running Inference:  55%|█████▌    | 110/200 [04:33<03:29,  2.33s/it]Running Inference:  56%|█████▌    | 111/200 [04:36<03:52,  2.62s/it]Running Inference:  56%|█████▌    | 112/200 [04:38<03:22,  2.30s/it]Running Inference:  56%|█████▋    | 113/200 [04:41<03:57,  2.73s/it]Running Inference:  57%|█████▋    | 114/200 [04:45<04:15,  2.97s/it]Running Inference:  57%|█████▊    | 115/200 [04:49<04:28,  3.16s/it]Running Inference:  58%|█████▊    | 116/200 [04:50<03:41,  2.63s/it]Running Inference:  58%|█████▊    | 117/200 [04:53<03:59,  2.89s/it]Running Inference:  59%|█████▉    | 118/200 [04:57<04:11,  3.07s/it]Running Inference:  60%|█████▉    | 119/200 [04:59<03:38,  2.69s/it]Running Inference:  60%|██████    | 120/200 [05:03<04:03,  3.05s/it]Running Inference:  60%|██████    | 121/200 [05:04<03:26,  2.62s/it]Running Inference:  61%|██████    | 122/200 [05:08<03:45,  2.89s/it]Running Inference:  62%|██████▏   | 123/200 [05:11<03:57,  3.08s/it]Running Inference:  62%|██████▏   | 124/200 [05:13<03:23,  2.68s/it]Running Inference:  62%|██████▎   | 125/200 [05:17<03:43,  2.99s/it]Running Inference:  63%|██████▎   | 126/200 [05:18<03:08,  2.55s/it]Running Inference:  64%|██████▎   | 127/200 [05:22<03:22,  2.78s/it]Running Inference:  64%|██████▍   | 128/200 [05:23<02:51,  2.38s/it]Running Inference:  64%|██████▍   | 129/200 [05:24<02:26,  2.06s/it]Running Inference:  65%|██████▌   | 130/200 [05:26<02:14,  1.92s/it]Running Inference:  66%|██████▌   | 131/200 [05:30<02:48,  2.44s/it]Running Inference:  66%|██████▌   | 132/200 [05:34<03:15,  2.87s/it]Running Inference:  66%|██████▋   | 133/200 [05:35<02:49,  2.53s/it]Running Inference:  67%|██████▋   | 134/200 [05:39<03:10,  2.88s/it]Running Inference:  68%|██████▊   | 135/200 [05:40<02:41,  2.48s/it]Running Inference:  68%|██████▊   | 136/200 [05:42<02:17,  2.15s/it]Running Inference:  68%|██████▊   | 137/200 [05:45<02:34,  2.45s/it]Running Inference:  69%|██████▉   | 138/200 [05:47<02:16,  2.20s/it]Running Inference:  70%|██████▉   | 139/200 [05:48<02:04,  2.04s/it]Running Inference:  70%|███████   | 140/200 [05:52<02:28,  2.47s/it]Running Inference:  70%|███████   | 141/200 [05:53<02:09,  2.20s/it]Running Inference:  71%|███████   | 142/200 [05:55<01:55,  2.00s/it]Running Inference:  72%|███████▏  | 143/200 [05:57<01:49,  1.91s/it]Running Inference:  72%|███████▏  | 144/200 [06:01<02:22,  2.55s/it]Running Inference:  72%|███████▎  | 145/200 [06:02<02:00,  2.19s/it]Running Inference:  73%|███████▎  | 146/200 [06:05<02:18,  2.57s/it]Running Inference:  74%|███████▎  | 147/200 [06:08<02:11,  2.49s/it]Running Inference:  74%|███████▍  | 148/200 [06:10<01:59,  2.29s/it]Running Inference:  74%|███████▍  | 149/200 [06:11<01:46,  2.09s/it]Running Inference:  75%|███████▌  | 150/200 [06:13<01:35,  1.91s/it]Running Inference:  76%|███████▌  | 151/200 [06:14<01:31,  1.87s/it]Running Inference:  76%|███████▌  | 152/200 [06:16<01:25,  1.78s/it]Running Inference:  76%|███████▋  | 153/200 [06:18<01:19,  1.70s/it]Running Inference:  77%|███████▋  | 154/200 [06:19<01:13,  1.59s/it]Running Inference:  78%|███████▊  | 155/200 [06:21<01:12,  1.61s/it]Running Inference:  78%|███████▊  | 156/200 [06:22<01:14,  1.69s/it]Running Inference:  78%|███████▊  | 157/200 [06:26<01:37,  2.27s/it]Running Inference:  79%|███████▉  | 158/200 [06:28<01:26,  2.05s/it]Running Inference:  80%|███████▉  | 159/200 [06:31<01:37,  2.37s/it]Running Inference:  80%|████████  | 160/200 [06:32<01:23,  2.08s/it]Running Inference:  80%|████████  | 161/200 [06:34<01:14,  1.91s/it]Running Inference:  81%|████████  | 162/200 [06:35<01:09,  1.82s/it]Running Inference:  82%|████████▏ | 163/200 [06:38<01:18,  2.11s/it]Running Inference:  82%|████████▏ | 164/200 [06:40<01:11,  1.99s/it]Running Inference:  82%|████████▎ | 165/200 [06:43<01:27,  2.50s/it]Running Inference:  83%|████████▎ | 166/200 [06:46<01:25,  2.50s/it]Running Inference:  84%|████████▎ | 167/200 [06:49<01:25,  2.60s/it]Running Inference:  84%|████████▍ | 168/200 [06:52<01:29,  2.79s/it]Running Inference:  84%|████████▍ | 169/200 [06:55<01:28,  2.87s/it]Running Inference:  85%|████████▌ | 170/200 [06:57<01:16,  2.56s/it]Running Inference:  86%|████████▌ | 171/200 [07:01<01:25,  2.94s/it]Running Inference:  86%|████████▌ | 172/200 [07:04<01:26,  3.11s/it]Running Inference:  86%|████████▋ | 173/200 [07:06<01:11,  2.66s/it]Running Inference:  87%|████████▋ | 174/200 [07:08<01:02,  2.41s/it]Running Inference:  88%|████████▊ | 175/200 [07:09<00:54,  2.17s/it]Running Inference:  88%|████████▊ | 176/200 [07:11<00:46,  1.94s/it]Running Inference:  88%|████████▊ | 177/200 [07:12<00:43,  1.88s/it]Running Inference:  89%|████████▉ | 178/200 [07:15<00:48,  2.19s/it]Running Inference:  90%|████████▉ | 179/200 [07:17<00:41,  1.97s/it]Running Inference:  90%|█████████ | 180/200 [07:20<00:49,  2.46s/it]Running Inference:  90%|█████████ | 181/200 [07:22<00:41,  2.17s/it]Running Inference:  91%|█████████ | 182/200 [07:24<00:39,  2.21s/it]Running Inference:  92%|█████████▏| 183/200 [07:27<00:43,  2.53s/it]Running Inference:  92%|█████████▏| 184/200 [07:31<00:46,  2.91s/it]Running Inference:  92%|█████████▎| 185/200 [07:34<00:40,  2.73s/it]Running Inference:  93%|█████████▎| 186/200 [07:37<00:42,  3.01s/it]Running Inference:  94%|█████████▎| 187/200 [07:40<00:36,  2.84s/it]Running Inference:  94%|█████████▍| 188/200 [07:44<00:37,  3.15s/it]Running Inference:  94%|█████████▍| 189/200 [07:45<00:29,  2.69s/it]Running Inference:  95%|█████████▌| 190/200 [07:47<00:23,  2.31s/it]Running Inference:  96%|█████████▌| 191/200 [07:50<00:24,  2.70s/it]Running Inference:  96%|█████████▌| 192/200 [07:54<00:23,  3.00s/it]Running Inference:  96%|█████████▋| 193/200 [07:56<00:19,  2.76s/it]Running Inference:  97%|█████████▋| 194/200 [08:00<00:18,  3.07s/it]Running Inference:  98%|█████████▊| 195/200 [08:03<00:15,  3.16s/it]Running Inference:  98%|█████████▊| 196/200 [08:07<00:13,  3.33s/it]Running Inference:  98%|█████████▊| 197/200 [08:09<00:08,  2.85s/it]Running Inference:  99%|█████████▉| 198/200 [08:10<00:04,  2.48s/it]Running Inference: 100%|█████████▉| 199/200 [08:14<00:02,  2.85s/it]Running Inference: 100%|██████████| 200/200 [08:17<00:00,  2.88s/it]Running Inference: 100%|██████████| 200/200 [08:17<00:00,  2.49s/it]
2025-12-14 16:51:19,006 - INFO - Inference completed.
2025-12-14 16:51:19,017 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 16:51:19,018 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:51:19,019 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 16:51:19,019 - INFO - Metrics:
40.83
2025-12-14 16:51:19,020 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_en, ratio=0.3) on GPU 2

----------------------------------------
Task: passage_retrieval_en | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:51:25,462 - INFO - Set deterministic seeds to 42
2025-12-14 16:51:25,462 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_en",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:51:25,462 - INFO - Starting evaluation run...
2025-12-14 16:51:25,462 - INFO - Output directory set to: longbenchresult
2025-12-14 16:51:25,462 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 16:51:25,462 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:51:25,462 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:51:25,462 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.55it/s]
Device set to use cuda:0
2025-12-14 16:51:40,496 - INFO - Model pipeline loaded.
2025-12-14 16:51:40,496 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_en)
2025-12-14 16:51:46,126 - INFO - Dataset loaded with 200 entries.
2025-12-14 16:51:46,126 - INFO - Dataset processed with 200 entries.
2025-12-14 16:51:46,159 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:02<07:32,  2.28s/it]Running Inference:   1%|          | 2/200 [00:05<09:57,  3.02s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:13,  2.51s/it]Running Inference:   2%|▏         | 4/200 [00:11<09:28,  2.90s/it]Running Inference:   2%|▎         | 5/200 [00:12<08:01,  2.47s/it]Running Inference:   3%|▎         | 6/200 [00:14<06:49,  2.11s/it]Running Inference:   4%|▎         | 7/200 [00:17<07:57,  2.48s/it]Running Inference:   4%|▍         | 8/200 [00:21<09:11,  2.87s/it]Running Inference:   4%|▍         | 9/200 [00:22<07:50,  2.47s/it]Running Inference:   5%|▌         | 10/200 [00:24<06:45,  2.13s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:27<08:02,  2.55s/it]Running Inference:   6%|▌         | 12/200 [00:29<07:20,  2.34s/it]Running Inference:   6%|▋         | 13/200 [00:33<08:17,  2.66s/it]Running Inference:   7%|▋         | 14/200 [00:35<08:22,  2.70s/it]Running Inference:   8%|▊         | 15/200 [00:38<08:32,  2.77s/it]Running Inference:   8%|▊         | 16/200 [00:40<07:23,  2.41s/it]Running Inference:   8%|▊         | 17/200 [00:43<08:11,  2.69s/it]Running Inference:   9%|▉         | 18/200 [00:45<07:04,  2.33s/it]Running Inference:  10%|▉         | 19/200 [00:46<06:23,  2.12s/it]Running Inference:  10%|█         | 20/200 [00:48<05:58,  1.99s/it]Running Inference:  10%|█         | 21/200 [00:52<07:37,  2.55s/it]Running Inference:  11%|█         | 22/200 [00:53<06:40,  2.25s/it]Running Inference:  12%|█▏        | 23/200 [00:55<06:10,  2.09s/it]Running Inference:  12%|█▏        | 24/200 [00:59<07:28,  2.55s/it]Running Inference:  12%|█▎        | 25/200 [01:00<06:23,  2.19s/it]Running Inference:  13%|█▎        | 26/200 [01:02<05:54,  2.04s/it]Running Inference:  14%|█▎        | 27/200 [01:06<07:38,  2.65s/it]Running Inference:  14%|█▍        | 28/200 [01:07<06:26,  2.25s/it]Running Inference:  14%|█▍        | 29/200 [01:11<07:26,  2.61s/it]Running Inference:  15%|█▌        | 30/200 [01:14<08:14,  2.91s/it]Running Inference:  16%|█▌        | 31/200 [01:16<07:16,  2.58s/it]Running Inference:  16%|█▌        | 32/200 [01:18<06:34,  2.35s/it]Running Inference:  16%|█▋        | 33/200 [01:20<06:27,  2.32s/it]Running Inference:  17%|█▋        | 34/200 [01:22<06:06,  2.21s/it]Running Inference:  18%|█▊        | 35/200 [01:24<05:40,  2.07s/it]Running Inference:  18%|█▊        | 36/200 [01:25<05:04,  1.86s/it]Running Inference:  18%|█▊        | 37/200 [01:27<04:53,  1.80s/it]Running Inference:  19%|█▉        | 38/200 [01:28<04:46,  1.77s/it]Running Inference:  20%|█▉        | 39/200 [01:32<06:08,  2.29s/it]Running Inference:  20%|██        | 40/200 [01:36<07:13,  2.71s/it]Running Inference:  20%|██        | 41/200 [01:40<08:06,  3.06s/it]Running Inference:  21%|██        | 42/200 [01:41<06:43,  2.55s/it]Running Inference:  22%|██▏       | 43/200 [01:43<06:14,  2.38s/it]Running Inference:  22%|██▏       | 44/200 [01:45<05:43,  2.20s/it]Running Inference:  22%|██▎       | 45/200 [01:46<05:07,  1.98s/it]Running Inference:  23%|██▎       | 46/200 [01:50<06:21,  2.48s/it]Running Inference:  24%|██▎       | 47/200 [01:53<06:56,  2.72s/it]Running Inference:  24%|██▍       | 48/200 [01:57<07:28,  2.95s/it]Running Inference:  24%|██▍       | 49/200 [01:58<06:35,  2.62s/it]Running Inference:  25%|██▌       | 50/200 [02:01<06:52,  2.75s/it]Running Inference:  26%|██▌       | 51/200 [02:05<07:13,  2.91s/it]Running Inference:  26%|██▌       | 52/200 [02:09<07:54,  3.20s/it]Running Inference:  26%|██▋       | 53/200 [02:13<08:21,  3.41s/it]Running Inference:  27%|██▋       | 54/200 [02:16<08:39,  3.56s/it]Running Inference:  28%|██▊       | 55/200 [02:18<07:04,  2.93s/it]Running Inference:  28%|██▊       | 56/200 [02:22<07:32,  3.14s/it]Running Inference:  28%|██▊       | 57/200 [02:23<06:14,  2.62s/it]Running Inference:  29%|██▉       | 58/200 [02:25<05:36,  2.37s/it]Running Inference:  30%|██▉       | 59/200 [02:27<05:10,  2.21s/it]Running Inference:  30%|███       | 60/200 [02:28<04:43,  2.03s/it]Running Inference:  30%|███       | 61/200 [02:30<04:20,  1.87s/it]Running Inference:  31%|███       | 62/200 [02:31<04:16,  1.86s/it]Running Inference:  32%|███▏      | 63/200 [02:35<05:08,  2.25s/it]Running Inference:  32%|███▏      | 64/200 [02:36<04:37,  2.04s/it]Running Inference:  32%|███▎      | 65/200 [02:40<05:37,  2.50s/it]Running Inference:  33%|███▎      | 66/200 [02:44<06:34,  2.95s/it]Running Inference:  34%|███▎      | 67/200 [02:47<06:37,  2.99s/it]Running Inference:  34%|███▍      | 68/200 [02:49<05:49,  2.64s/it]Running Inference:  34%|███▍      | 69/200 [02:53<06:36,  3.02s/it]Running Inference:  35%|███▌      | 70/200 [02:54<05:46,  2.66s/it]Running Inference:  36%|███▌      | 71/200 [02:58<06:32,  3.04s/it]Running Inference:  36%|███▌      | 72/200 [03:00<05:42,  2.68s/it]Running Inference:  36%|███▋      | 73/200 [03:02<04:54,  2.32s/it]Running Inference:  37%|███▋      | 74/200 [03:05<05:45,  2.74s/it]Running Inference:  38%|███▊      | 75/200 [03:07<04:55,  2.37s/it]Running Inference:  38%|███▊      | 76/200 [03:10<05:19,  2.58s/it]Running Inference:  38%|███▊      | 77/200 [03:12<05:02,  2.46s/it]Running Inference:  39%|███▉      | 78/200 [03:16<05:46,  2.84s/it]Running Inference:  40%|███▉      | 79/200 [03:17<04:59,  2.47s/it]Running Inference:  40%|████      | 80/200 [03:21<05:38,  2.82s/it]Running Inference:  40%|████      | 81/200 [03:23<04:48,  2.43s/it]Running Inference:  41%|████      | 82/200 [03:26<05:19,  2.71s/it]Running Inference:  42%|████▏     | 83/200 [03:28<04:39,  2.38s/it]Running Inference:  42%|████▏     | 84/200 [03:31<05:18,  2.74s/it]Running Inference:  42%|████▎     | 85/200 [03:35<05:35,  2.92s/it]Running Inference:  43%|████▎     | 86/200 [03:36<04:50,  2.55s/it]Running Inference:  44%|████▎     | 87/200 [03:38<04:18,  2.29s/it]Running Inference:  44%|████▍     | 88/200 [03:39<03:48,  2.04s/it]Running Inference:  44%|████▍     | 89/200 [03:41<03:27,  1.87s/it]Running Inference:  45%|████▌     | 90/200 [03:42<03:12,  1.75s/it]Running Inference:  46%|████▌     | 91/200 [03:44<03:05,  1.70s/it]Running Inference:  46%|████▌     | 92/200 [03:46<03:05,  1.72s/it]Running Inference:  46%|████▋     | 93/200 [03:47<02:55,  1.64s/it]Running Inference:  47%|████▋     | 94/200 [03:49<02:59,  1.69s/it]Running Inference:  48%|████▊     | 95/200 [03:53<03:58,  2.27s/it]Running Inference:  48%|████▊     | 96/200 [03:54<03:33,  2.05s/it]Running Inference:  48%|████▊     | 97/200 [03:56<03:17,  1.92s/it]Running Inference:  49%|████▉     | 98/200 [04:00<04:15,  2.50s/it]Running Inference:  50%|████▉     | 99/200 [04:01<03:42,  2.21s/it]Running Inference:  50%|█████     | 100/200 [04:03<03:22,  2.03s/it]Running Inference:  50%|█████     | 101/200 [04:04<03:03,  1.86s/it]Running Inference:  51%|█████     | 102/200 [04:08<03:54,  2.39s/it]Running Inference:  52%|█████▏    | 103/200 [04:11<04:26,  2.75s/it]Running Inference:  52%|█████▏    | 104/200 [04:13<03:48,  2.38s/it]Running Inference:  52%|█████▎    | 105/200 [04:15<03:30,  2.22s/it]Running Inference:  53%|█████▎    | 106/200 [04:16<03:10,  2.03s/it]Running Inference:  54%|█████▎    | 107/200 [04:20<03:48,  2.46s/it]Running Inference:  54%|█████▍    | 108/200 [04:22<03:32,  2.31s/it]Running Inference:  55%|█████▍    | 109/200 [04:24<03:17,  2.17s/it]Running Inference:  55%|█████▌    | 110/200 [04:25<03:06,  2.07s/it]Running Inference:  56%|█████▌    | 111/200 [04:29<03:34,  2.41s/it]Running Inference:  56%|█████▌    | 112/200 [04:30<03:09,  2.16s/it]Running Inference:  56%|█████▋    | 113/200 [04:34<03:49,  2.63s/it]Running Inference:  57%|█████▋    | 114/200 [04:37<04:09,  2.90s/it]Running Inference:  57%|█████▊    | 115/200 [04:39<03:32,  2.50s/it]Running Inference:  58%|█████▊    | 116/200 [04:40<03:02,  2.17s/it]Running Inference:  58%|█████▊    | 117/200 [04:42<02:41,  1.95s/it]Running Inference:  59%|█████▉    | 118/200 [04:44<02:33,  1.88s/it]Running Inference:  60%|█████▉    | 119/200 [04:47<03:20,  2.48s/it]Running Inference:  60%|██████    | 120/200 [04:51<03:52,  2.91s/it]Running Inference:  60%|██████    | 121/200 [04:53<03:18,  2.52s/it]Running Inference:  61%|██████    | 122/200 [04:57<03:45,  2.89s/it]Running Inference:  62%|██████▏   | 123/200 [05:00<04:00,  3.13s/it]Running Inference:  62%|██████▏   | 124/200 [05:02<03:25,  2.71s/it]Running Inference:  62%|██████▎   | 125/200 [05:04<02:57,  2.37s/it]Running Inference:  63%|██████▎   | 126/200 [05:05<02:36,  2.11s/it]Running Inference:  64%|██████▎   | 127/200 [05:09<03:00,  2.48s/it]Running Inference:  64%|██████▍   | 128/200 [05:10<02:36,  2.17s/it]Running Inference:  64%|██████▍   | 129/200 [05:11<02:15,  1.91s/it]Running Inference:  65%|██████▌   | 130/200 [05:13<02:07,  1.82s/it]Running Inference:  66%|██████▌   | 131/200 [05:17<02:43,  2.37s/it]Running Inference:  66%|██████▌   | 132/200 [05:18<02:29,  2.20s/it]Running Inference:  66%|██████▋   | 133/200 [05:20<02:18,  2.06s/it]Running Inference:  67%|██████▋   | 134/200 [05:24<02:49,  2.56s/it]Running Inference:  68%|██████▊   | 135/200 [05:25<02:26,  2.26s/it]Running Inference:  68%|██████▊   | 136/200 [05:27<02:07,  2.00s/it]Running Inference:  68%|██████▊   | 137/200 [05:31<02:39,  2.53s/it]Running Inference:  69%|██████▉   | 138/200 [05:34<02:54,  2.82s/it]Running Inference:  70%|██████▉   | 139/200 [05:36<02:30,  2.47s/it]Running Inference:  70%|███████   | 140/200 [05:39<02:46,  2.78s/it]Running Inference:  70%|███████   | 141/200 [05:41<02:22,  2.41s/it]Running Inference:  71%|███████   | 142/200 [05:42<02:04,  2.14s/it]Running Inference:  72%|███████▏  | 143/200 [05:44<01:54,  2.02s/it]Running Inference:  72%|███████▏  | 144/200 [05:48<02:27,  2.63s/it]Running Inference:  72%|███████▎  | 145/200 [05:49<02:03,  2.25s/it]Running Inference:  73%|███████▎  | 146/200 [05:51<01:53,  2.10s/it]Running Inference:  74%|███████▎  | 147/200 [05:53<01:54,  2.16s/it]Running Inference:  74%|███████▍  | 148/200 [05:55<01:47,  2.07s/it]Running Inference:  74%|███████▍  | 149/200 [05:57<01:38,  1.93s/it]Running Inference:  75%|███████▌  | 150/200 [05:58<01:29,  1.80s/it]Running Inference:  76%|███████▌  | 151/200 [06:02<01:57,  2.41s/it]Running Inference:  76%|███████▌  | 152/200 [06:04<01:43,  2.16s/it]Running Inference:  76%|███████▋  | 153/200 [06:05<01:32,  1.96s/it]Running Inference:  77%|███████▋  | 154/200 [06:07<01:21,  1.78s/it]Running Inference:  78%|███████▊  | 155/200 [06:08<01:18,  1.74s/it]Running Inference:  78%|███████▊  | 156/200 [06:10<01:17,  1.77s/it]Running Inference:  78%|███████▊  | 157/200 [06:14<01:40,  2.35s/it]Running Inference:  79%|███████▉  | 158/200 [06:15<01:28,  2.10s/it]Running Inference:  80%|███████▉  | 159/200 [06:17<01:17,  1.89s/it]Running Inference:  80%|████████  | 160/200 [06:18<01:09,  1.74s/it]Running Inference:  80%|████████  | 161/200 [06:20<01:05,  1.68s/it]Running Inference:  81%|████████  | 162/200 [06:21<01:03,  1.66s/it]Running Inference:  82%|████████▏ | 163/200 [06:24<01:14,  2.00s/it]Running Inference:  82%|████████▏ | 164/200 [06:26<01:08,  1.91s/it]Running Inference:  82%|████████▎ | 165/200 [06:27<01:04,  1.83s/it]Running Inference:  83%|████████▎ | 166/200 [06:30<01:09,  2.06s/it]Running Inference:  84%|████████▎ | 167/200 [06:34<01:24,  2.55s/it]Running Inference:  84%|████████▍ | 168/200 [06:35<01:13,  2.28s/it]Running Inference:  84%|████████▍ | 169/200 [06:39<01:24,  2.74s/it]Running Inference:  85%|████████▌ | 170/200 [06:41<01:14,  2.47s/it]Running Inference:  86%|████████▌ | 171/200 [06:43<01:05,  2.27s/it]Running Inference:  86%|████████▌ | 172/200 [06:46<01:14,  2.65s/it]Running Inference:  86%|████████▋ | 173/200 [06:48<01:03,  2.34s/it]Running Inference:  87%|████████▋ | 174/200 [06:50<00:56,  2.18s/it]Running Inference:  88%|████████▊ | 175/200 [06:51<00:50,  2.01s/it]Running Inference:  88%|████████▊ | 176/200 [06:53<00:43,  1.82s/it]Running Inference:  88%|████████▊ | 177/200 [06:55<00:41,  1.80s/it]Running Inference:  89%|████████▉ | 178/200 [06:56<00:40,  1.84s/it]Running Inference:  90%|████████▉ | 179/200 [06:58<00:36,  1.73s/it]Running Inference:  90%|█████████ | 180/200 [07:00<00:33,  1.68s/it]Running Inference:  90%|█████████ | 181/200 [07:01<00:30,  1.63s/it]Running Inference:  91%|█████████ | 182/200 [07:03<00:29,  1.61s/it]Running Inference:  92%|█████████▏| 183/200 [07:06<00:36,  2.12s/it]Running Inference:  92%|█████████▏| 184/200 [07:08<00:32,  2.01s/it]Running Inference:  92%|█████████▎| 185/200 [07:11<00:38,  2.54s/it]Running Inference:  93%|█████████▎| 186/200 [07:15<00:40,  2.89s/it]Running Inference:  94%|█████████▎| 187/200 [07:17<00:32,  2.51s/it]Running Inference:  94%|█████████▍| 188/200 [07:21<00:35,  2.92s/it]Running Inference:  94%|█████████▍| 189/200 [07:22<00:27,  2.53s/it]Running Inference:  95%|█████████▌| 190/200 [07:26<00:28,  2.82s/it]Running Inference:  96%|█████████▌| 191/200 [07:29<00:27,  3.06s/it]Running Inference:  96%|█████████▌| 192/200 [07:33<00:26,  3.27s/it]Running Inference:  96%|█████████▋| 193/200 [07:37<00:23,  3.40s/it]Running Inference:  97%|█████████▋| 194/200 [07:39<00:17,  2.91s/it]Running Inference:  98%|█████████▊| 195/200 [07:40<00:12,  2.56s/it]Running Inference:  98%|█████████▊| 196/200 [07:42<00:09,  2.31s/it]Running Inference:  98%|█████████▊| 197/200 [07:44<00:06,  2.13s/it]Running Inference:  99%|█████████▉| 198/200 [07:45<00:03,  1.98s/it]Running Inference: 100%|█████████▉| 199/200 [07:50<00:02,  2.61s/it]Running Inference: 100%|██████████| 200/200 [07:51<00:00,  2.27s/it]Running Inference: 100%|██████████| 200/200 [07:51<00:00,  2.36s/it]
2025-12-14 16:59:37,653 - INFO - Inference completed.
2025-12-14 16:59:37,664 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 16:59:37,664 - INFO - Calculating metrics for dataset: longbench
2025-12-14 16:59:37,665 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_en__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 16:59:37,665 - INFO - Metrics:
32.0
2025-12-14 16:59:37,667 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_en, ratio=0.5) on GPU 2


========================================
LongBench Task: trec
========================================
----------------------------------------
Task: trec | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 16:59:44,238 - INFO - Set deterministic seeds to 42
2025-12-14 16:59:44,239 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 16:59:44,239 - INFO - Starting evaluation run...
2025-12-14 16:59:44,239 - INFO - Output directory set to: longbenchresult
2025-12-14 16:59:44,239 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 16:59:44,239 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 16:59:44,239 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 16:59:44,239 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.78it/s]
Device set to use cuda:0
2025-12-14 16:59:56,818 - INFO - Model pipeline loaded.
2025-12-14 16:59:56,818 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 17:00:02,861 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:00:02,861 - INFO - Dataset processed with 200 entries.
2025-12-14 17:00:02,875 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:48,  1.45s/it]Running Inference:   1%|          | 2/200 [00:05<09:51,  2.99s/it]Running Inference:   2%|▏         | 3/200 [00:06<06:54,  2.10s/it]Running Inference:   2%|▏         | 4/200 [00:07<05:05,  1.56s/it]Running Inference:   2%|▎         | 5/200 [00:08<04:50,  1.49s/it]Running Inference:   3%|▎         | 6/200 [00:09<04:08,  1.28s/it]Running Inference:   4%|▎         | 7/200 [00:10<04:00,  1.24s/it]Running Inference:   4%|▍         | 8/200 [00:11<03:16,  1.02s/it]Running Inference:   4%|▍         | 9/200 [00:13<04:25,  1.39s/it]Running Inference:   5%|▌         | 10/200 [00:15<04:45,  1.50s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:19<07:10,  2.28s/it]Running Inference:   6%|▌         | 12/200 [00:20<06:31,  2.08s/it]Running Inference:   6%|▋         | 13/200 [00:22<05:59,  1.92s/it]Running Inference:   7%|▋         | 14/200 [00:27<08:29,  2.74s/it]Running Inference:   8%|▊         | 15/200 [00:31<09:37,  3.12s/it]Running Inference:   8%|▊         | 16/200 [00:35<10:31,  3.43s/it]Running Inference:   8%|▊         | 17/200 [00:36<08:32,  2.80s/it]Running Inference:   9%|▉         | 18/200 [00:40<09:30,  3.14s/it]Running Inference:  10%|▉         | 19/200 [00:44<10:37,  3.52s/it]Running Inference:  10%|█         | 20/200 [00:46<09:00,  3.00s/it]Running Inference:  10%|█         | 21/200 [00:47<06:59,  2.34s/it]Running Inference:  11%|█         | 22/200 [00:51<08:23,  2.83s/it]Running Inference:  12%|█▏        | 23/200 [00:55<09:15,  3.14s/it]Running Inference:  12%|█▏        | 24/200 [00:59<10:31,  3.59s/it]Running Inference:  12%|█▎        | 25/200 [01:01<08:39,  2.97s/it]Running Inference:  13%|█▎        | 26/200 [01:05<09:51,  3.40s/it]Running Inference:  14%|█▎        | 27/200 [01:07<07:54,  2.74s/it]Running Inference:  14%|█▍        | 28/200 [01:08<06:41,  2.34s/it]Running Inference:  14%|█▍        | 29/200 [01:09<05:08,  1.81s/it]Running Inference:  15%|█▌        | 30/200 [01:13<07:31,  2.66s/it]Running Inference:  16%|█▌        | 31/200 [01:14<06:19,  2.25s/it]Running Inference:  16%|█▌        | 32/200 [01:15<05:11,  1.85s/it]Running Inference:  16%|█▋        | 33/200 [01:18<05:46,  2.07s/it]Running Inference:  17%|█▋        | 34/200 [01:19<04:30,  1.63s/it]Running Inference:  18%|█▊        | 35/200 [01:20<04:01,  1.47s/it]Running Inference:  18%|█▊        | 36/200 [01:22<04:36,  1.68s/it]Running Inference:  18%|█▊        | 37/200 [01:23<04:16,  1.58s/it]Running Inference:  19%|█▉        | 38/200 [01:24<03:59,  1.48s/it]Running Inference:  20%|█▉        | 39/200 [01:25<03:21,  1.25s/it]Running Inference:  20%|██        | 40/200 [01:26<02:54,  1.09s/it]Running Inference:  20%|██        | 41/200 [01:27<03:13,  1.22s/it]Running Inference:  21%|██        | 42/200 [01:28<02:47,  1.06s/it]Running Inference:  22%|██▏       | 43/200 [01:29<02:28,  1.06it/s]Running Inference:  22%|██▏       | 44/200 [01:33<05:03,  1.95s/it]Running Inference:  22%|██▎       | 45/200 [01:35<04:43,  1.83s/it]Running Inference:  23%|██▎       | 46/200 [01:35<03:38,  1.42s/it]Running Inference:  24%|██▎       | 47/200 [01:36<03:18,  1.30s/it]Running Inference:  24%|██▍       | 48/200 [01:37<02:37,  1.04s/it]Running Inference:  24%|██▍       | 49/200 [01:37<02:31,  1.00s/it]Running Inference:  25%|██▌       | 50/200 [01:41<04:33,  1.82s/it]Running Inference:  26%|██▌       | 51/200 [01:42<03:49,  1.54s/it]Running Inference:  26%|██▌       | 52/200 [01:43<03:20,  1.35s/it]Running Inference:  26%|██▋       | 53/200 [01:44<02:51,  1.17s/it]Running Inference:  27%|██▋       | 54/200 [01:44<02:17,  1.06it/s]Running Inference:  28%|██▊       | 55/200 [01:45<01:52,  1.29it/s]Running Inference:  28%|██▊       | 56/200 [01:46<02:03,  1.16it/s]Running Inference:  28%|██▊       | 57/200 [01:46<01:50,  1.29it/s]Running Inference:  29%|██▉       | 58/200 [01:47<02:10,  1.09it/s]Running Inference:  30%|██▉       | 59/200 [01:48<02:10,  1.08it/s]Running Inference:  30%|███       | 60/200 [01:49<01:50,  1.27it/s]Running Inference:  30%|███       | 61/200 [01:50<02:12,  1.05it/s]Running Inference:  31%|███       | 62/200 [01:51<01:56,  1.18it/s]Running Inference:  32%|███▏      | 63/200 [01:52<01:59,  1.14it/s]Running Inference:  32%|███▏      | 64/200 [01:53<02:00,  1.13it/s]Running Inference:  32%|███▎      | 65/200 [01:53<01:53,  1.19it/s]Running Inference:  33%|███▎      | 66/200 [01:54<01:52,  1.19it/s]Running Inference:  34%|███▎      | 67/200 [01:58<03:54,  1.76s/it]Running Inference:  34%|███▍      | 68/200 [01:59<03:12,  1.46s/it]Running Inference:  34%|███▍      | 69/200 [02:00<02:44,  1.26s/it]Running Inference:  35%|███▌      | 70/200 [02:01<02:37,  1.21s/it]Running Inference:  36%|███▌      | 71/200 [02:02<02:38,  1.23s/it]Running Inference:  36%|███▌      | 72/200 [02:03<02:42,  1.27s/it]Running Inference:  36%|███▋      | 73/200 [02:04<02:34,  1.22s/it]Running Inference:  37%|███▋      | 74/200 [02:08<04:08,  1.98s/it]Running Inference:  38%|███▊      | 75/200 [02:09<03:24,  1.64s/it]Running Inference:  38%|███▊      | 76/200 [02:10<03:00,  1.45s/it]Running Inference:  38%|███▊      | 77/200 [02:11<02:36,  1.27s/it]Running Inference:  39%|███▉      | 78/200 [02:12<02:36,  1.28s/it]Running Inference:  40%|███▉      | 79/200 [02:16<04:11,  2.08s/it]Running Inference:  40%|████      | 80/200 [02:17<03:30,  1.75s/it]Running Inference:  40%|████      | 81/200 [02:18<03:04,  1.55s/it]Running Inference:  41%|████      | 82/200 [02:19<02:47,  1.42s/it]Running Inference:  42%|████▏     | 83/200 [02:24<04:26,  2.28s/it]Running Inference:  42%|████▏     | 84/200 [02:25<03:56,  2.04s/it]Running Inference:  42%|████▎     | 85/200 [02:26<03:23,  1.77s/it]Running Inference:  43%|████▎     | 86/200 [02:30<04:44,  2.49s/it]Running Inference:  44%|████▎     | 87/200 [02:35<05:47,  3.07s/it]Running Inference:  44%|████▍     | 88/200 [02:39<06:13,  3.34s/it]Running Inference:  44%|████▍     | 89/200 [02:44<06:56,  3.76s/it]Running Inference:  45%|████▌     | 90/200 [02:44<05:18,  2.89s/it]Running Inference:  46%|████▌     | 91/200 [02:45<04:06,  2.26s/it]Running Inference:  46%|████▌     | 92/200 [02:47<03:33,  1.98s/it]Running Inference:  46%|████▋     | 93/200 [02:48<03:23,  1.90s/it]Running Inference:  47%|████▋     | 94/200 [02:50<03:07,  1.77s/it]Running Inference:  48%|████▊     | 95/200 [02:50<02:24,  1.38s/it]Running Inference:  48%|████▊     | 96/200 [02:54<03:54,  2.25s/it]Running Inference:  48%|████▊     | 97/200 [02:56<03:29,  2.03s/it]Running Inference:  49%|████▉     | 98/200 [02:57<02:50,  1.67s/it]Running Inference:  50%|████▉     | 99/200 [02:58<02:35,  1.54s/it]Running Inference:  50%|█████     | 100/200 [03:00<02:32,  1.52s/it]Running Inference:  50%|█████     | 101/200 [03:00<02:13,  1.34s/it]Running Inference:  51%|█████     | 102/200 [03:01<01:53,  1.15s/it]Running Inference:  52%|█████▏    | 103/200 [03:02<01:56,  1.20s/it]Running Inference:  52%|█████▏    | 104/200 [03:07<03:24,  2.13s/it]Running Inference:  52%|█████▎    | 105/200 [03:08<03:03,  1.93s/it]Running Inference:  53%|█████▎    | 106/200 [03:09<02:39,  1.70s/it]Running Inference:  54%|█████▎    | 107/200 [03:10<02:17,  1.48s/it]Running Inference:  54%|█████▍    | 108/200 [03:15<03:42,  2.42s/it]Running Inference:  55%|█████▍    | 109/200 [03:19<04:31,  2.98s/it]Running Inference:  55%|█████▌    | 110/200 [03:23<04:57,  3.31s/it]Running Inference:  56%|█████▌    | 111/200 [03:27<05:08,  3.46s/it]Running Inference:  56%|█████▌    | 112/200 [03:32<05:32,  3.78s/it]Running Inference:  56%|█████▋    | 113/200 [03:33<04:13,  2.91s/it]Running Inference:  57%|█████▋    | 114/200 [03:37<04:43,  3.30s/it]Running Inference:  57%|█████▊    | 115/200 [03:41<05:00,  3.53s/it]Running Inference:  58%|█████▊    | 116/200 [03:45<05:09,  3.68s/it]Running Inference:  58%|█████▊    | 117/200 [03:50<05:33,  4.02s/it]Running Inference:  59%|█████▉    | 118/200 [03:52<04:36,  3.37s/it]Running Inference:  60%|█████▉    | 119/200 [03:53<03:38,  2.70s/it]Running Inference:  60%|██████    | 120/200 [03:54<03:09,  2.37s/it]Running Inference:  60%|██████    | 121/200 [03:55<02:24,  1.83s/it]Running Inference:  61%|██████    | 122/200 [03:56<02:15,  1.73s/it]Running Inference:  62%|██████▏   | 123/200 [03:57<01:53,  1.47s/it]Running Inference:  62%|██████▏   | 124/200 [04:02<03:04,  2.43s/it]Running Inference:  62%|██████▎   | 125/200 [04:03<02:37,  2.11s/it]Running Inference:  63%|██████▎   | 126/200 [04:07<03:14,  2.63s/it]Running Inference:  64%|██████▎   | 127/200 [04:11<03:48,  3.13s/it]Running Inference:  64%|██████▍   | 128/200 [04:15<04:02,  3.37s/it]Running Inference:  64%|██████▍   | 129/200 [04:17<03:14,  2.74s/it]Running Inference:  65%|██████▌   | 130/200 [04:18<02:41,  2.31s/it]Running Inference:  66%|██████▌   | 131/200 [04:19<02:12,  1.92s/it]Running Inference:  66%|██████▌   | 132/200 [04:20<01:59,  1.76s/it]Running Inference:  66%|██████▋   | 133/200 [04:22<01:50,  1.64s/it]Running Inference:  67%|██████▋   | 134/200 [04:22<01:22,  1.26s/it]Running Inference:  68%|██████▊   | 135/200 [04:23<01:06,  1.03s/it]Running Inference:  68%|██████▊   | 136/200 [04:23<01:00,  1.05it/s]Running Inference:  68%|██████▊   | 137/200 [04:25<01:09,  1.11s/it]Running Inference:  69%|██████▉   | 138/200 [04:29<01:58,  1.91s/it]Running Inference:  70%|██████▉   | 139/200 [04:29<01:39,  1.62s/it]Running Inference:  70%|███████   | 140/200 [04:31<01:32,  1.55s/it]Running Inference:  70%|███████   | 141/200 [04:32<01:20,  1.37s/it]Running Inference:  71%|███████   | 142/200 [04:36<02:13,  2.29s/it]Running Inference:  72%|███████▏  | 143/200 [04:38<01:56,  2.04s/it]Running Inference:  72%|███████▏  | 144/200 [04:42<02:29,  2.66s/it]Running Inference:  72%|███████▎  | 145/200 [04:46<02:44,  3.00s/it]Running Inference:  73%|███████▎  | 146/200 [04:47<02:16,  2.53s/it]Running Inference:  74%|███████▎  | 147/200 [04:49<02:02,  2.31s/it]Running Inference:  74%|███████▍  | 148/200 [04:49<01:33,  1.81s/it]Running Inference:  74%|███████▍  | 149/200 [04:53<02:05,  2.46s/it]Running Inference:  75%|███████▌  | 150/200 [04:54<01:39,  1.98s/it]Running Inference:  76%|███████▌  | 151/200 [04:56<01:30,  1.85s/it]Running Inference:  76%|███████▌  | 152/200 [05:00<02:07,  2.65s/it]Running Inference:  76%|███████▋  | 153/200 [05:01<01:39,  2.11s/it]Running Inference:  77%|███████▋  | 154/200 [05:03<01:27,  1.90s/it]Running Inference:  78%|███████▊  | 155/200 [05:03<01:08,  1.52s/it]Running Inference:  78%|███████▊  | 156/200 [05:04<01:01,  1.40s/it]Running Inference:  78%|███████▊  | 157/200 [05:06<01:01,  1.44s/it]Running Inference:  79%|███████▉  | 158/200 [05:08<01:08,  1.63s/it]Running Inference:  80%|███████▉  | 159/200 [05:09<01:01,  1.50s/it]Running Inference:  80%|████████  | 160/200 [05:11<00:59,  1.50s/it]Running Inference:  80%|████████  | 161/200 [05:12<00:50,  1.30s/it]Running Inference:  81%|████████  | 162/200 [05:12<00:43,  1.14s/it]Running Inference:  82%|████████▏ | 163/200 [05:13<00:41,  1.13s/it]Running Inference:  82%|████████▏ | 164/200 [05:15<00:45,  1.27s/it]Running Inference:  82%|████████▎ | 165/200 [05:16<00:44,  1.26s/it]Running Inference:  83%|████████▎ | 166/200 [05:17<00:35,  1.03s/it]Running Inference:  84%|████████▎ | 167/200 [05:18<00:36,  1.12s/it]Running Inference:  84%|████████▍ | 168/200 [05:19<00:36,  1.15s/it]Running Inference:  84%|████████▍ | 169/200 [05:20<00:28,  1.07it/s]Running Inference:  85%|████████▌ | 170/200 [05:20<00:25,  1.17it/s]Running Inference:  86%|████████▌ | 171/200 [05:24<00:49,  1.72s/it]Running Inference:  86%|████████▌ | 172/200 [05:25<00:43,  1.55s/it]Running Inference:  86%|████████▋ | 173/200 [05:26<00:33,  1.24s/it]Running Inference:  87%|████████▋ | 174/200 [05:30<00:51,  2.00s/it]Running Inference:  88%|████████▊ | 175/200 [05:31<00:44,  1.79s/it]Running Inference:  88%|████████▊ | 176/200 [05:32<00:40,  1.67s/it]Running Inference:  88%|████████▊ | 177/200 [05:33<00:33,  1.46s/it]Running Inference:  89%|████████▉ | 178/200 [05:34<00:30,  1.39s/it]Running Inference:  90%|████████▉ | 179/200 [05:35<00:24,  1.16s/it]Running Inference:  90%|█████████ | 180/200 [05:39<00:38,  1.95s/it]Running Inference:  90%|█████████ | 181/200 [05:40<00:31,  1.66s/it]Running Inference:  91%|█████████ | 182/200 [05:41<00:27,  1.55s/it]Running Inference:  92%|█████████▏| 183/200 [05:42<00:25,  1.49s/it]Running Inference:  92%|█████████▏| 184/200 [05:47<00:36,  2.29s/it]Running Inference:  92%|█████████▎| 185/200 [05:48<00:28,  1.87s/it]Running Inference:  93%|█████████▎| 186/200 [05:49<00:22,  1.61s/it]Running Inference:  94%|█████████▎| 187/200 [05:50<00:18,  1.43s/it]Running Inference:  94%|█████████▍| 188/200 [05:53<00:26,  2.18s/it]Running Inference:  94%|█████████▍| 189/200 [05:54<00:19,  1.73s/it]Running Inference:  95%|█████████▌| 190/200 [05:55<00:14,  1.41s/it]Running Inference:  96%|█████████▌| 191/200 [05:55<00:10,  1.19s/it]Running Inference:  96%|█████████▌| 192/200 [05:57<00:09,  1.23s/it]Running Inference:  96%|█████████▋| 193/200 [05:58<00:08,  1.26s/it]Running Inference:  97%|█████████▋| 194/200 [05:59<00:07,  1.21s/it]Running Inference:  98%|█████████▊| 195/200 [06:03<00:10,  2.00s/it]Running Inference:  98%|█████████▊| 196/200 [06:07<00:10,  2.55s/it]Running Inference:  98%|█████████▊| 197/200 [06:08<00:06,  2.17s/it]Running Inference:  99%|█████████▉| 198/200 [06:09<00:03,  1.82s/it]Running Inference: 100%|█████████▉| 199/200 [06:11<00:01,  1.72s/it]Running Inference: 100%|██████████| 200/200 [06:12<00:00,  1.50s/it]Running Inference: 100%|██████████| 200/200 [06:12<00:00,  1.86s/it]
2025-12-14 17:06:15,056 - INFO - Inference completed.
2025-12-14 17:06:15,078 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 17:06:15,078 - INFO - Calculating metrics for dataset: longbench
2025-12-14 17:06:15,080 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 17:06:15,080 - INFO - Metrics:
29.0
2025-12-14 17:06:15,081 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=trec, ratio=0.1) on GPU 2

----------------------------------------
Task: trec | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:06:21,511 - INFO - Set deterministic seeds to 42
2025-12-14 17:06:21,511 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:06:21,511 - INFO - Starting evaluation run...
2025-12-14 17:06:21,511 - INFO - Output directory set to: longbenchresult
2025-12-14 17:06:21,511 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 17:06:21,511 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 17:06:21,511 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:06:21,511 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.26it/s]
Device set to use cuda:0
2025-12-14 17:06:34,216 - INFO - Model pipeline loaded.
2025-12-14 17:06:34,217 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 17:06:40,346 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:06:40,346 - INFO - Dataset processed with 200 entries.
2025-12-14 17:06:40,359 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<05:01,  1.52s/it]Running Inference:   1%|          | 2/200 [00:05<09:59,  3.03s/it]Running Inference:   2%|▏         | 3/200 [00:06<07:00,  2.14s/it]Running Inference:   2%|▏         | 4/200 [00:10<09:36,  2.94s/it]Running Inference:   2%|▎         | 5/200 [00:12<07:43,  2.38s/it]Running Inference:   3%|▎         | 6/200 [00:13<05:58,  1.85s/it]Running Inference:   4%|▎         | 7/200 [00:17<08:27,  2.63s/it]Running Inference:   4%|▍         | 8/200 [00:17<06:18,  1.97s/it]Running Inference:   4%|▍         | 9/200 [00:19<06:23,  2.01s/it]Running Inference:   5%|▌         | 10/200 [00:20<05:24,  1.71s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:25<07:40,  2.44s/it]Running Inference:   6%|▌         | 12/200 [00:26<06:53,  2.20s/it]Running Inference:   6%|▋         | 13/200 [00:30<08:24,  2.70s/it]Running Inference:   7%|▋         | 14/200 [00:35<10:12,  3.30s/it]Running Inference:   8%|▊         | 15/200 [00:39<10:54,  3.54s/it]Running Inference:   8%|▊         | 16/200 [00:40<08:23,  2.74s/it]Running Inference:   8%|▊         | 17/200 [00:41<07:03,  2.32s/it]Running Inference:   9%|▉         | 18/200 [00:45<08:35,  2.83s/it]Running Inference:  10%|▉         | 19/200 [00:50<10:02,  3.33s/it]Running Inference:  10%|█         | 20/200 [00:51<08:33,  2.85s/it]Running Inference:  10%|█         | 21/200 [00:56<09:42,  3.26s/it]Running Inference:  11%|█         | 22/200 [01:00<10:20,  3.49s/it]Running Inference:  12%|█▏        | 23/200 [01:03<10:39,  3.61s/it]Running Inference:  12%|█▏        | 24/200 [01:05<08:36,  2.94s/it]Running Inference:  12%|█▎        | 25/200 [01:06<07:19,  2.51s/it]Running Inference:  13%|█▎        | 26/200 [01:11<08:52,  3.06s/it]Running Inference:  14%|█▎        | 27/200 [01:12<07:13,  2.51s/it]Running Inference:  14%|█▍        | 28/200 [01:13<06:14,  2.17s/it]Running Inference:  14%|█▍        | 29/200 [01:14<04:49,  1.69s/it]Running Inference:  15%|█▌        | 30/200 [01:19<07:30,  2.65s/it]Running Inference:  16%|█▌        | 31/200 [01:20<06:18,  2.24s/it]Running Inference:  16%|█▌        | 32/200 [01:21<05:10,  1.85s/it]Running Inference:  16%|█▋        | 33/200 [01:25<06:50,  2.46s/it]Running Inference:  17%|█▋        | 34/200 [01:25<05:15,  1.90s/it]Running Inference:  18%|█▊        | 35/200 [01:27<04:33,  1.65s/it]Running Inference:  18%|█▊        | 36/200 [01:28<04:13,  1.54s/it]Running Inference:  18%|█▊        | 37/200 [01:29<04:00,  1.48s/it]Running Inference:  19%|█▉        | 38/200 [01:30<03:48,  1.41s/it]Running Inference:  20%|█▉        | 39/200 [01:31<03:14,  1.21s/it]Running Inference:  20%|██        | 40/200 [01:35<05:29,  2.06s/it]Running Inference:  20%|██        | 41/200 [01:37<05:01,  1.89s/it]Running Inference:  21%|██        | 42/200 [01:37<04:06,  1.56s/it]Running Inference:  22%|██▏       | 43/200 [01:38<03:24,  1.30s/it]Running Inference:  22%|██▏       | 44/200 [01:42<05:46,  2.22s/it]Running Inference:  22%|██▎       | 45/200 [01:44<05:12,  2.02s/it]Running Inference:  23%|██▎       | 46/200 [01:45<04:03,  1.58s/it]Running Inference:  24%|██▎       | 47/200 [01:49<06:13,  2.44s/it]Running Inference:  24%|██▍       | 48/200 [01:49<04:39,  1.84s/it]Running Inference:  24%|██▍       | 49/200 [01:51<04:02,  1.60s/it]Running Inference:  25%|██▌       | 50/200 [01:54<05:40,  2.27s/it]Running Inference:  26%|██▌       | 51/200 [01:55<04:35,  1.85s/it]Running Inference:  26%|██▌       | 52/200 [01:56<03:48,  1.55s/it]Running Inference:  26%|██▋       | 53/200 [01:57<03:11,  1.31s/it]Running Inference:  27%|██▋       | 54/200 [01:57<02:35,  1.07s/it]Running Inference:  28%|██▊       | 55/200 [01:58<02:16,  1.06it/s]Running Inference:  28%|██▊       | 56/200 [01:59<02:20,  1.02it/s]Running Inference:  28%|██▊       | 57/200 [02:00<02:03,  1.16it/s]Running Inference:  29%|██▉       | 58/200 [02:01<02:19,  1.02it/s]Running Inference:  30%|██▉       | 59/200 [02:02<02:16,  1.03it/s]Running Inference:  30%|███       | 60/200 [02:02<01:54,  1.22it/s]Running Inference:  30%|███       | 61/200 [02:04<02:18,  1.00it/s]Running Inference:  31%|███       | 62/200 [02:04<02:01,  1.14it/s]Running Inference:  32%|███▏      | 63/200 [02:05<02:03,  1.11it/s]Running Inference:  32%|███▏      | 64/200 [02:06<02:03,  1.10it/s]Running Inference:  32%|███▎      | 65/200 [02:07<01:54,  1.17it/s]Running Inference:  33%|███▎      | 66/200 [02:08<01:54,  1.17it/s]Running Inference:  34%|███▎      | 67/200 [02:12<03:58,  1.79s/it]Running Inference:  34%|███▍      | 68/200 [02:13<03:16,  1.48s/it]Running Inference:  34%|███▍      | 69/200 [02:17<05:04,  2.33s/it]Running Inference:  35%|███▌      | 70/200 [02:18<04:15,  1.96s/it]Running Inference:  36%|███▌      | 71/200 [02:19<03:46,  1.76s/it]Running Inference:  36%|███▌      | 72/200 [02:21<03:29,  1.64s/it]Running Inference:  36%|███▋      | 73/200 [02:22<03:07,  1.47s/it]Running Inference:  37%|███▋      | 74/200 [02:22<02:25,  1.15s/it]Running Inference:  38%|███▊      | 75/200 [02:23<02:12,  1.06s/it]Running Inference:  38%|███▊      | 76/200 [02:24<02:10,  1.05s/it]Running Inference:  38%|███▊      | 77/200 [02:25<02:01,  1.01it/s]Running Inference:  39%|███▉      | 78/200 [02:26<02:10,  1.07s/it]Running Inference:  40%|███▉      | 79/200 [02:30<03:57,  1.96s/it]Running Inference:  40%|████      | 80/200 [02:31<03:20,  1.67s/it]Running Inference:  40%|████      | 81/200 [02:32<02:57,  1.49s/it]Running Inference:  41%|████      | 82/200 [02:33<02:43,  1.38s/it]Running Inference:  42%|████▏     | 83/200 [02:38<04:25,  2.27s/it]Running Inference:  42%|████▏     | 84/200 [02:39<03:56,  2.04s/it]Running Inference:  42%|████▎     | 85/200 [02:43<05:09,  2.69s/it]Running Inference:  43%|████▎     | 86/200 [02:48<06:00,  3.16s/it]Running Inference:  44%|████▎     | 87/200 [02:51<06:11,  3.28s/it]Running Inference:  44%|████▍     | 88/200 [02:53<05:29,  2.94s/it]Running Inference:  44%|████▍     | 89/200 [02:58<06:28,  3.50s/it]Running Inference:  45%|████▌     | 90/200 [02:59<04:58,  2.71s/it]Running Inference:  46%|████▌     | 91/200 [03:00<03:53,  2.14s/it]Running Inference:  46%|████▌     | 92/200 [03:01<03:24,  1.89s/it]Running Inference:  46%|████▋     | 93/200 [03:06<04:43,  2.65s/it]Running Inference:  47%|████▋     | 94/200 [03:07<04:03,  2.30s/it]Running Inference:  48%|████▊     | 95/200 [03:07<03:03,  1.75s/it]Running Inference:  48%|████▊     | 96/200 [03:08<02:39,  1.53s/it]Running Inference:  48%|████▊     | 97/200 [03:10<02:37,  1.53s/it]Running Inference:  49%|████▉     | 98/200 [03:11<02:17,  1.35s/it]Running Inference:  50%|████▉     | 99/200 [03:12<02:12,  1.31s/it]Running Inference:  50%|█████     | 100/200 [03:14<02:16,  1.37s/it]Running Inference:  50%|█████     | 101/200 [03:15<02:02,  1.23s/it]Running Inference:  51%|█████     | 102/200 [03:15<01:45,  1.08s/it]Running Inference:  52%|█████▏    | 103/200 [03:17<01:51,  1.15s/it]Running Inference:  52%|█████▏    | 104/200 [03:21<03:23,  2.12s/it]Running Inference:  52%|█████▎    | 105/200 [03:22<02:55,  1.85s/it]Running Inference:  53%|█████▎    | 106/200 [03:27<04:11,  2.67s/it]Running Inference:  54%|█████▎    | 107/200 [03:28<03:21,  2.16s/it]Running Inference:  54%|█████▍    | 108/200 [03:30<03:26,  2.25s/it]Running Inference:  55%|█████▍    | 109/200 [03:31<02:53,  1.91s/it]Running Inference:  55%|█████▌    | 110/200 [03:36<03:52,  2.59s/it]Running Inference:  56%|█████▌    | 111/200 [03:39<04:25,  2.98s/it]Running Inference:  56%|█████▌    | 112/200 [03:43<04:35,  3.13s/it]Running Inference:  56%|█████▋    | 113/200 [03:44<03:34,  2.46s/it]Running Inference:  57%|█████▋    | 114/200 [03:45<02:52,  2.01s/it]Running Inference:  57%|█████▊    | 115/200 [03:45<02:18,  1.62s/it]Running Inference:  58%|█████▊    | 116/200 [03:48<02:29,  1.78s/it]Running Inference:  58%|█████▊    | 117/200 [03:49<02:21,  1.70s/it]Running Inference:  59%|█████▉    | 118/200 [03:53<03:12,  2.35s/it]Running Inference:  60%|█████▉    | 119/200 [03:54<02:43,  2.01s/it]Running Inference:  60%|██████    | 120/200 [03:56<02:33,  1.92s/it]Running Inference:  60%|██████    | 121/200 [03:57<01:59,  1.52s/it]Running Inference:  61%|██████    | 122/200 [03:58<02:00,  1.54s/it]Running Inference:  62%|██████▏   | 123/200 [03:59<01:43,  1.34s/it]Running Inference:  62%|██████▏   | 124/200 [04:04<02:59,  2.37s/it]Running Inference:  62%|██████▎   | 125/200 [04:05<02:35,  2.08s/it]Running Inference:  63%|██████▎   | 126/200 [04:06<02:00,  1.63s/it]Running Inference:  64%|██████▎   | 127/200 [04:07<01:57,  1.61s/it]Running Inference:  64%|██████▍   | 128/200 [04:08<01:35,  1.32s/it]Running Inference:  64%|██████▍   | 129/200 [04:09<01:34,  1.33s/it]Running Inference:  65%|██████▌   | 130/200 [04:11<01:33,  1.33s/it]Running Inference:  66%|██████▌   | 131/200 [04:12<01:25,  1.23s/it]Running Inference:  66%|██████▌   | 132/200 [04:13<01:27,  1.28s/it]Running Inference:  66%|██████▋   | 133/200 [04:14<01:27,  1.31s/it]Running Inference:  67%|██████▋   | 134/200 [04:15<01:07,  1.02s/it]Running Inference:  68%|██████▊   | 135/200 [04:19<02:02,  1.89s/it]Running Inference:  68%|██████▊   | 136/200 [04:19<01:39,  1.55s/it]Running Inference:  68%|██████▊   | 137/200 [04:21<01:36,  1.53s/it]Running Inference:  69%|██████▉   | 138/200 [04:25<02:18,  2.23s/it]Running Inference:  70%|██████▉   | 139/200 [04:26<01:52,  1.85s/it]Running Inference:  70%|███████   | 140/200 [04:27<01:43,  1.73s/it]Running Inference:  70%|███████   | 141/200 [04:28<01:28,  1.50s/it]Running Inference:  71%|███████   | 142/200 [04:29<01:23,  1.43s/it]Running Inference:  72%|███████▏  | 143/200 [04:31<01:24,  1.48s/it]Running Inference:  72%|███████▏  | 144/200 [04:33<01:28,  1.59s/it]Running Inference:  72%|███████▎  | 145/200 [04:37<02:04,  2.27s/it]Running Inference:  73%|███████▎  | 146/200 [04:38<01:48,  2.02s/it]Running Inference:  74%|███████▎  | 147/200 [04:40<01:43,  1.96s/it]Running Inference:  74%|███████▍  | 148/200 [04:41<01:20,  1.55s/it]Running Inference:  74%|███████▍  | 149/200 [04:41<01:05,  1.28s/it]Running Inference:  75%|███████▌  | 150/200 [04:44<01:23,  1.66s/it]Running Inference:  76%|███████▌  | 151/200 [04:45<01:19,  1.63s/it]Running Inference:  76%|███████▌  | 152/200 [04:47<01:14,  1.55s/it]Running Inference:  76%|███████▋  | 153/200 [04:48<01:02,  1.34s/it]Running Inference:  77%|███████▋  | 154/200 [04:49<01:02,  1.36s/it]Running Inference:  78%|███████▊  | 155/200 [04:50<00:51,  1.15s/it]Running Inference:  78%|███████▊  | 156/200 [04:51<00:50,  1.14s/it]Running Inference:  78%|███████▊  | 157/200 [04:52<00:52,  1.23s/it]Running Inference:  79%|███████▉  | 158/200 [04:57<01:36,  2.29s/it]Running Inference:  80%|███████▉  | 159/200 [04:58<01:21,  1.99s/it]Running Inference:  80%|████████  | 160/200 [05:00<01:14,  1.87s/it]Running Inference:  80%|████████  | 161/200 [05:01<00:59,  1.54s/it]Running Inference:  81%|████████  | 162/200 [05:01<00:49,  1.31s/it]Running Inference:  82%|████████▏ | 163/200 [05:02<00:46,  1.25s/it]Running Inference:  82%|████████▏ | 164/200 [05:04<00:49,  1.38s/it]Running Inference:  82%|████████▎ | 165/200 [05:05<00:46,  1.34s/it]Running Inference:  83%|████████▎ | 166/200 [05:09<01:12,  2.13s/it]Running Inference:  84%|████████▎ | 167/200 [05:10<01:00,  1.82s/it]Running Inference:  84%|████████▍ | 168/200 [05:12<00:52,  1.65s/it]Running Inference:  84%|████████▍ | 169/200 [05:12<00:39,  1.28s/it]Running Inference:  85%|████████▌ | 170/200 [05:13<00:33,  1.10s/it]Running Inference:  86%|████████▌ | 171/200 [05:14<00:36,  1.26s/it]Running Inference:  86%|████████▌ | 172/200 [05:16<00:34,  1.24s/it]Running Inference:  86%|████████▋ | 173/200 [05:16<00:27,  1.01s/it]Running Inference:  87%|████████▋ | 174/200 [05:17<00:27,  1.07s/it]Running Inference:  88%|████████▊ | 175/200 [05:19<00:29,  1.17s/it]Running Inference:  88%|████████▊ | 176/200 [05:20<00:29,  1.24s/it]Running Inference:  88%|████████▊ | 177/200 [05:21<00:23,  1.04s/it]Running Inference:  89%|████████▉ | 178/200 [05:22<00:24,  1.10s/it]Running Inference:  90%|████████▉ | 179/200 [05:22<00:19,  1.09it/s]Running Inference:  90%|█████████ | 180/200 [05:26<00:35,  1.80s/it]Running Inference:  90%|█████████ | 181/200 [05:27<00:29,  1.53s/it]Running Inference:  91%|█████████ | 182/200 [05:28<00:26,  1.46s/it]Running Inference:  92%|█████████▏| 183/200 [05:30<00:24,  1.43s/it]Running Inference:  92%|█████████▏| 184/200 [05:32<00:24,  1.52s/it]Running Inference:  92%|█████████▎| 185/200 [05:32<00:20,  1.34s/it]Running Inference:  93%|█████████▎| 186/200 [05:33<00:17,  1.24s/it]Running Inference:  94%|█████████▎| 187/200 [05:35<00:15,  1.18s/it]Running Inference:  94%|█████████▍| 188/200 [05:35<00:12,  1.02s/it]Running Inference:  94%|█████████▍| 189/200 [05:36<00:10,  1.05it/s]Running Inference:  95%|█████████▌| 190/200 [05:37<00:08,  1.15it/s]Running Inference:  96%|█████████▌| 191/200 [05:37<00:07,  1.19it/s]Running Inference:  96%|█████████▌| 192/200 [05:39<00:07,  1.03it/s]Running Inference:  96%|█████████▋| 193/200 [05:40<00:07,  1.08s/it]Running Inference:  97%|█████████▋| 194/200 [05:41<00:06,  1.08s/it]Running Inference:  98%|█████████▊| 195/200 [05:45<00:09,  1.94s/it]Running Inference:  98%|█████████▊| 196/200 [05:49<00:10,  2.53s/it]Running Inference:  98%|█████████▊| 197/200 [05:50<00:06,  2.15s/it]Running Inference:  99%|█████████▉| 198/200 [05:51<00:03,  1.81s/it]Running Inference: 100%|█████████▉| 199/200 [05:53<00:01,  1.72s/it]Running Inference: 100%|██████████| 200/200 [05:54<00:00,  1.50s/it]Running Inference: 100%|██████████| 200/200 [05:54<00:00,  1.77s/it]
2025-12-14 17:12:34,615 - INFO - Inference completed.
2025-12-14 17:12:34,638 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 17:12:34,638 - INFO - Calculating metrics for dataset: longbench
2025-12-14 17:12:34,639 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 17:12:34,639 - INFO - Metrics:
30.5
2025-12-14 17:12:34,641 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=trec, ratio=0.2) on GPU 2

----------------------------------------
Task: trec | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:12:41,170 - INFO - Set deterministic seeds to 42
2025-12-14 17:12:41,170 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:12:41,170 - INFO - Starting evaluation run...
2025-12-14 17:12:41,170 - INFO - Output directory set to: longbenchresult
2025-12-14 17:12:41,170 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 17:12:41,170 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 17:12:41,170 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:12:41,170 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.71it/s]
Device set to use cuda:0
2025-12-14 17:12:53,517 - INFO - Model pipeline loaded.
2025-12-14 17:12:53,517 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 17:12:58,921 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:12:58,921 - INFO - Dataset processed with 200 entries.
2025-12-14 17:12:58,934 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:04<15:50,  4.78s/it]Running Inference:   1%|          | 2/200 [00:08<14:26,  4.38s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:24,  2.87s/it]Running Inference:   2%|▏         | 4/200 [00:14<11:02,  3.38s/it]Running Inference:   2%|▎         | 5/200 [00:15<08:37,  2.65s/it]Running Inference:   3%|▎         | 6/200 [00:16<06:34,  2.03s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:19,  2.59s/it]Running Inference:   4%|▍         | 8/200 [00:20<06:12,  1.94s/it]Running Inference:   4%|▍         | 9/200 [00:24<08:09,  2.57s/it]Running Inference:   5%|▌         | 10/200 [00:25<06:37,  2.09s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:26<05:20,  1.70s/it]Running Inference:   6%|▌         | 12/200 [00:28<05:16,  1.68s/it]Running Inference:   6%|▋         | 13/200 [00:31<07:14,  2.33s/it]Running Inference:   7%|▋         | 14/200 [00:36<09:22,  3.02s/it]Running Inference:   8%|▊         | 15/200 [00:40<10:16,  3.33s/it]Running Inference:   8%|▊         | 16/200 [00:41<07:57,  2.60s/it]Running Inference:   8%|▊         | 17/200 [00:46<09:52,  3.24s/it]Running Inference:   9%|▉         | 18/200 [00:50<10:29,  3.46s/it]Running Inference:  10%|▉         | 19/200 [00:54<11:20,  3.76s/it]Running Inference:  10%|█         | 20/200 [00:59<12:13,  4.07s/it]Running Inference:  10%|█         | 21/200 [01:03<12:14,  4.10s/it]Running Inference:  11%|█         | 22/200 [01:07<12:05,  4.07s/it]Running Inference:  12%|█▏        | 23/200 [01:11<11:50,  4.01s/it]Running Inference:  12%|█▏        | 24/200 [01:12<09:26,  3.22s/it]Running Inference:  12%|█▎        | 25/200 [01:14<07:54,  2.71s/it]Running Inference:  13%|█▎        | 26/200 [01:18<09:21,  3.23s/it]Running Inference:  14%|█▎        | 27/200 [01:19<07:32,  2.62s/it]Running Inference:  14%|█▍        | 28/200 [01:21<06:27,  2.25s/it]Running Inference:  14%|█▍        | 29/200 [01:25<07:54,  2.78s/it]Running Inference:  15%|█▌        | 30/200 [01:27<07:46,  2.75s/it]Running Inference:  16%|█▌        | 31/200 [01:29<06:30,  2.31s/it]Running Inference:  16%|█▌        | 32/200 [01:30<05:18,  1.90s/it]Running Inference:  16%|█▋        | 33/200 [01:31<04:27,  1.60s/it]Running Inference:  17%|█▋        | 34/200 [01:31<03:35,  1.30s/it]Running Inference:  18%|█▊        | 35/200 [01:32<03:23,  1.23s/it]Running Inference:  18%|█▊        | 36/200 [01:33<02:53,  1.06s/it]Running Inference:  18%|█▊        | 37/200 [01:34<03:04,  1.13s/it]Running Inference:  19%|█▉        | 38/200 [01:36<03:09,  1.17s/it]Running Inference:  20%|█▉        | 39/200 [01:36<02:51,  1.06s/it]Running Inference:  20%|██        | 40/200 [01:40<05:12,  1.95s/it]Running Inference:  20%|██        | 41/200 [01:42<04:49,  1.82s/it]Running Inference:  21%|██        | 42/200 [01:43<04:04,  1.55s/it]Running Inference:  22%|██▏       | 43/200 [01:43<03:22,  1.29s/it]Running Inference:  22%|██▏       | 44/200 [01:48<05:43,  2.20s/it]Running Inference:  22%|██▎       | 45/200 [01:49<05:10,  2.01s/it]Running Inference:  23%|██▎       | 46/200 [01:53<06:38,  2.59s/it]Running Inference:  24%|██▎       | 47/200 [01:54<05:29,  2.15s/it]Running Inference:  24%|██▍       | 48/200 [01:55<04:08,  1.64s/it]Running Inference:  24%|██▍       | 49/200 [01:56<03:40,  1.46s/it]Running Inference:  25%|██▌       | 50/200 [01:57<03:34,  1.43s/it]Running Inference:  26%|██▌       | 51/200 [01:58<03:08,  1.27s/it]Running Inference:  26%|██▌       | 52/200 [01:59<02:46,  1.12s/it]Running Inference:  26%|██▋       | 53/200 [02:00<02:28,  1.01s/it]Running Inference:  27%|██▋       | 54/200 [02:04<04:32,  1.86s/it]Running Inference:  28%|██▊       | 55/200 [02:04<03:29,  1.45s/it]Running Inference:  28%|██▊       | 56/200 [02:05<03:11,  1.33s/it]Running Inference:  28%|██▊       | 57/200 [02:06<02:39,  1.12s/it]Running Inference:  29%|██▉       | 58/200 [02:07<02:45,  1.16s/it]Running Inference:  30%|██▉       | 59/200 [02:08<02:34,  1.10s/it]Running Inference:  30%|███       | 60/200 [02:08<02:06,  1.10it/s]Running Inference:  30%|███       | 61/200 [02:10<02:26,  1.06s/it]Running Inference:  31%|███       | 62/200 [02:10<02:06,  1.09it/s]Running Inference:  32%|███▏      | 63/200 [02:15<04:30,  1.98s/it]Running Inference:  32%|███▏      | 64/200 [02:16<03:45,  1.66s/it]Running Inference:  32%|███▎      | 65/200 [02:16<03:05,  1.38s/it]Running Inference:  33%|███▎      | 66/200 [02:17<02:43,  1.22s/it]Running Inference:  34%|███▎      | 67/200 [02:21<04:30,  2.04s/it]Running Inference:  34%|███▍      | 68/200 [02:24<05:09,  2.34s/it]Running Inference:  34%|███▍      | 69/200 [02:25<04:08,  1.90s/it]Running Inference:  35%|███▌      | 70/200 [02:26<03:36,  1.67s/it]Running Inference:  36%|███▌      | 71/200 [02:28<03:22,  1.57s/it]Running Inference:  36%|███▌      | 72/200 [02:29<03:12,  1.51s/it]Running Inference:  36%|███▋      | 73/200 [02:30<02:55,  1.38s/it]Running Inference:  37%|███▋      | 74/200 [02:30<02:16,  1.09s/it]Running Inference:  38%|███▊      | 75/200 [02:31<02:00,  1.04it/s]Running Inference:  38%|███▊      | 76/200 [02:32<02:01,  1.02it/s]Running Inference:  38%|███▊      | 77/200 [02:33<01:55,  1.06it/s]Running Inference:  39%|███▉      | 78/200 [02:34<02:03,  1.01s/it]Running Inference:  40%|███▉      | 79/200 [02:35<01:49,  1.10it/s]Running Inference:  40%|████      | 80/200 [02:36<01:48,  1.11it/s]Running Inference:  40%|████      | 81/200 [02:40<03:57,  2.00s/it]Running Inference:  41%|████      | 82/200 [02:41<03:24,  1.74s/it]Running Inference:  42%|████▏     | 83/200 [02:46<04:53,  2.51s/it]Running Inference:  42%|████▏     | 84/200 [02:47<04:15,  2.20s/it]Running Inference:  42%|████▎     | 85/200 [02:51<05:21,  2.80s/it]Running Inference:  43%|████▎     | 86/200 [02:52<04:11,  2.21s/it]Running Inference:  44%|████▎     | 87/200 [02:57<05:26,  2.89s/it]Running Inference:  44%|████▍     | 88/200 [02:58<04:30,  2.42s/it]Running Inference:  44%|████▍     | 89/200 [03:01<04:54,  2.65s/it]Running Inference:  45%|████▌     | 90/200 [03:02<03:54,  2.13s/it]Running Inference:  46%|████▌     | 91/200 [03:03<03:08,  1.73s/it]Running Inference:  46%|████▌     | 92/200 [03:04<02:53,  1.60s/it]Running Inference:  46%|████▋     | 93/200 [03:09<04:21,  2.44s/it]Running Inference:  47%|████▋     | 94/200 [03:10<03:39,  2.07s/it]Running Inference:  48%|████▊     | 95/200 [03:10<02:47,  1.59s/it]Running Inference:  48%|████▊     | 96/200 [03:11<02:27,  1.42s/it]Running Inference:  48%|████▊     | 97/200 [03:13<02:29,  1.45s/it]Running Inference:  49%|████▉     | 98/200 [03:17<03:53,  2.29s/it]Running Inference:  50%|████▉     | 99/200 [03:22<05:00,  2.97s/it]Running Inference:  50%|█████     | 100/200 [03:23<04:09,  2.49s/it]Running Inference:  50%|█████     | 101/200 [03:24<03:17,  2.00s/it]Running Inference:  51%|█████     | 102/200 [03:25<02:38,  1.61s/it]Running Inference:  52%|█████▏    | 103/200 [03:26<02:27,  1.52s/it]Running Inference:  52%|█████▏    | 104/200 [03:30<03:47,  2.37s/it]Running Inference:  52%|█████▎    | 105/200 [03:32<03:12,  2.02s/it]Running Inference:  53%|█████▎    | 106/200 [03:36<04:21,  2.78s/it]Running Inference:  54%|█████▎    | 107/200 [03:37<03:28,  2.24s/it]Running Inference:  54%|█████▍    | 108/200 [03:40<03:47,  2.47s/it]Running Inference:  55%|█████▍    | 109/200 [03:41<03:07,  2.06s/it]Running Inference:  55%|█████▌    | 110/200 [03:42<02:34,  1.71s/it]Running Inference:  56%|█████▌    | 111/200 [03:46<03:30,  2.36s/it]Running Inference:  56%|█████▌    | 112/200 [03:50<04:25,  3.02s/it]Running Inference:  56%|█████▋    | 113/200 [03:51<03:27,  2.39s/it]Running Inference:  57%|█████▋    | 114/200 [03:52<02:47,  1.95s/it]Running Inference:  57%|█████▊    | 115/200 [03:53<02:14,  1.59s/it]Running Inference:  58%|█████▊    | 116/200 [03:57<03:16,  2.34s/it]Running Inference:  58%|█████▊    | 117/200 [03:59<02:53,  2.09s/it]Running Inference:  59%|█████▉    | 118/200 [04:00<02:30,  1.83s/it]Running Inference:  60%|█████▉    | 119/200 [04:01<02:13,  1.65s/it]Running Inference:  60%|██████    | 120/200 [04:03<02:12,  1.66s/it]Running Inference:  60%|██████    | 121/200 [04:03<01:45,  1.34s/it]Running Inference:  61%|██████    | 122/200 [04:05<01:48,  1.39s/it]Running Inference:  62%|██████▏   | 123/200 [04:06<01:34,  1.23s/it]Running Inference:  62%|██████▏   | 124/200 [04:10<02:52,  2.27s/it]Running Inference:  62%|██████▎   | 125/200 [04:12<02:44,  2.19s/it]Running Inference:  63%|██████▎   | 126/200 [04:13<02:06,  1.71s/it]Running Inference:  64%|██████▎   | 127/200 [04:14<01:48,  1.48s/it]Running Inference:  64%|██████▍   | 128/200 [04:15<01:28,  1.24s/it]Running Inference:  64%|██████▍   | 129/200 [04:16<01:28,  1.24s/it]Running Inference:  65%|██████▌   | 130/200 [04:17<01:28,  1.26s/it]Running Inference:  66%|██████▌   | 131/200 [04:18<01:21,  1.19s/it]Running Inference:  66%|██████▌   | 132/200 [04:20<01:24,  1.25s/it]Running Inference:  66%|██████▋   | 133/200 [04:21<01:26,  1.29s/it]Running Inference:  67%|██████▋   | 134/200 [04:21<01:06,  1.01s/it]Running Inference:  68%|██████▊   | 135/200 [04:25<02:01,  1.86s/it]Running Inference:  68%|██████▊   | 136/200 [04:26<01:38,  1.53s/it]Running Inference:  68%|██████▊   | 137/200 [04:27<01:35,  1.51s/it]Running Inference:  69%|██████▉   | 138/200 [04:31<02:17,  2.21s/it]Running Inference:  70%|██████▉   | 139/200 [04:32<01:51,  1.83s/it]Running Inference:  70%|███████   | 140/200 [04:34<01:43,  1.72s/it]Running Inference:  70%|███████   | 141/200 [04:35<01:27,  1.48s/it]Running Inference:  71%|███████   | 142/200 [04:36<01:22,  1.42s/it]Running Inference:  72%|███████▏  | 143/200 [04:37<01:21,  1.43s/it]Running Inference:  72%|███████▏  | 144/200 [04:42<02:06,  2.26s/it]Running Inference:  72%|███████▎  | 145/200 [04:45<02:30,  2.73s/it]Running Inference:  73%|███████▎  | 146/200 [04:47<02:06,  2.34s/it]Running Inference:  74%|███████▎  | 147/200 [04:51<02:30,  2.84s/it]Running Inference:  74%|███████▍  | 148/200 [04:51<01:52,  2.17s/it]Running Inference:  74%|███████▍  | 149/200 [04:52<01:27,  1.72s/it]Running Inference:  75%|███████▌  | 150/200 [04:53<01:14,  1.50s/it]Running Inference:  76%|███████▌  | 151/200 [04:55<01:14,  1.51s/it]Running Inference:  76%|███████▌  | 152/200 [04:56<01:10,  1.46s/it]Running Inference:  76%|███████▋  | 153/200 [04:57<01:00,  1.28s/it]Running Inference:  77%|███████▋  | 154/200 [04:58<01:00,  1.32s/it]Running Inference:  78%|███████▊  | 155/200 [04:59<00:51,  1.14s/it]Running Inference:  78%|███████▊  | 156/200 [05:00<00:49,  1.13s/it]Running Inference:  78%|███████▊  | 157/200 [05:05<01:35,  2.22s/it]Running Inference:  79%|███████▉  | 158/200 [05:06<01:23,  1.99s/it]Running Inference:  80%|███████▉  | 159/200 [05:08<01:12,  1.78s/it]Running Inference:  80%|████████  | 160/200 [05:12<01:37,  2.43s/it]Running Inference:  80%|████████  | 161/200 [05:12<01:15,  1.93s/it]Running Inference:  81%|████████  | 162/200 [05:16<01:38,  2.60s/it]Running Inference:  82%|████████▏ | 163/200 [05:18<01:19,  2.16s/it]Running Inference:  82%|████████▏ | 164/200 [05:19<01:09,  1.94s/it]Running Inference:  82%|████████▎ | 165/200 [05:20<01:02,  1.80s/it]Running Inference:  83%|████████▎ | 166/200 [05:24<01:23,  2.45s/it]Running Inference:  84%|████████▎ | 167/200 [05:29<01:40,  3.05s/it]Running Inference:  84%|████████▍ | 168/200 [05:30<01:20,  2.51s/it]Running Inference:  84%|████████▍ | 169/200 [05:31<00:59,  1.91s/it]Running Inference:  85%|████████▌ | 170/200 [05:31<00:46,  1.54s/it]Running Inference:  86%|████████▌ | 171/200 [05:33<00:45,  1.56s/it]Running Inference:  86%|████████▌ | 172/200 [05:34<00:40,  1.45s/it]Running Inference:  86%|████████▋ | 173/200 [05:38<00:58,  2.18s/it]Running Inference:  87%|████████▋ | 174/200 [05:39<00:48,  1.88s/it]Running Inference:  88%|████████▊ | 175/200 [05:41<00:43,  1.74s/it]Running Inference:  88%|████████▊ | 176/200 [05:42<00:39,  1.64s/it]Running Inference:  88%|████████▊ | 177/200 [05:43<00:30,  1.34s/it]Running Inference:  89%|████████▉ | 178/200 [05:44<00:28,  1.31s/it]Running Inference:  90%|████████▉ | 179/200 [05:44<00:22,  1.05s/it]Running Inference:  90%|█████████ | 180/200 [05:48<00:37,  1.88s/it]Running Inference:  90%|█████████ | 181/200 [05:49<00:30,  1.59s/it]Running Inference:  91%|█████████ | 182/200 [05:50<00:26,  1.46s/it]Running Inference:  92%|█████████▏| 183/200 [05:52<00:24,  1.43s/it]Running Inference:  92%|█████████▏| 184/200 [05:56<00:36,  2.26s/it]Running Inference:  92%|█████████▎| 185/200 [06:00<00:43,  2.90s/it]Running Inference:  93%|█████████▎| 186/200 [06:01<00:32,  2.33s/it]Running Inference:  94%|█████████▎| 187/200 [06:02<00:25,  1.93s/it]Running Inference:  94%|█████████▍| 188/200 [06:06<00:30,  2.54s/it]Running Inference:  94%|█████████▍| 189/200 [06:07<00:22,  2.01s/it]Running Inference:  95%|█████████▌| 190/200 [06:08<00:16,  1.61s/it]Running Inference:  96%|█████████▌| 191/200 [06:08<00:12,  1.36s/it]Running Inference:  96%|█████████▌| 192/200 [06:13<00:18,  2.33s/it]Running Inference:  96%|█████████▋| 193/200 [06:14<00:14,  2.02s/it]Running Inference:  97%|█████████▋| 194/200 [06:15<00:10,  1.75s/it]Running Inference:  98%|█████████▊| 195/200 [06:19<00:11,  2.39s/it]Running Inference:  98%|█████████▊| 196/200 [06:23<00:11,  2.84s/it]Running Inference:  98%|█████████▊| 197/200 [06:24<00:07,  2.37s/it]Running Inference:  99%|█████████▉| 198/200 [06:25<00:03,  1.96s/it]Running Inference: 100%|█████████▉| 199/200 [06:27<00:01,  1.82s/it]Running Inference: 100%|██████████| 200/200 [06:28<00:00,  1.60s/it]Running Inference: 100%|██████████| 200/200 [06:28<00:00,  1.94s/it]
2025-12-14 17:19:27,418 - INFO - Inference completed.
2025-12-14 17:19:27,440 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 17:19:27,441 - INFO - Calculating metrics for dataset: longbench
2025-12-14 17:19:27,442 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 17:19:27,442 - INFO - Metrics:
32.0
2025-12-14 17:19:27,443 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=trec, ratio=0.3) on GPU 2

----------------------------------------
Task: trec | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:19:33,988 - INFO - Set deterministic seeds to 42
2025-12-14 17:19:33,988 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "trec",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:19:33,988 - INFO - Starting evaluation run...
2025-12-14 17:19:33,988 - INFO - Output directory set to: longbenchresult
2025-12-14 17:19:33,989 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 17:19:33,989 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 17:19:33,989 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:19:33,989 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.47it/s]
Device set to use cuda:0
2025-12-14 17:19:46,825 - INFO - Model pipeline loaded.
2025-12-14 17:19:46,825 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: trec)
2025-12-14 17:20:01,379 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:20:01,379 - INFO - Dataset processed with 200 entries.
2025-12-14 17:20:01,392 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<04:29,  1.36s/it]Running Inference:   1%|          | 2/200 [00:02<03:09,  1.04it/s]Running Inference:   2%|▏         | 3/200 [00:03<03:16,  1.00it/s]Running Inference:   2%|▏         | 4/200 [00:07<07:17,  2.23s/it]Running Inference:   2%|▎         | 5/200 [00:08<06:18,  1.94s/it]Running Inference:   3%|▎         | 6/200 [00:09<05:08,  1.59s/it]Running Inference:   4%|▎         | 7/200 [00:10<04:21,  1.36s/it]Running Inference:   4%|▍         | 8/200 [00:10<03:31,  1.10s/it]Running Inference:   4%|▍         | 9/200 [00:14<06:18,  1.98s/it]Running Inference:   5%|▌         | 10/200 [00:19<08:33,  2.70s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:23<09:47,  3.11s/it]Running Inference:   6%|▌         | 12/200 [00:27<11:04,  3.53s/it]Running Inference:   6%|▋         | 13/200 [00:28<08:14,  2.64s/it]Running Inference:   7%|▋         | 14/200 [00:32<10:02,  3.24s/it]Running Inference:   8%|▊         | 15/200 [00:36<10:42,  3.47s/it]Running Inference:   8%|▊         | 16/200 [00:37<08:14,  2.69s/it]Running Inference:   8%|▊         | 17/200 [00:42<10:02,  3.29s/it]Running Inference:   9%|▉         | 18/200 [00:46<10:34,  3.48s/it]Running Inference:  10%|▉         | 19/200 [00:50<11:21,  3.77s/it]Running Inference:  10%|█         | 20/200 [00:52<09:25,  3.14s/it]Running Inference:  10%|█         | 21/200 [00:56<10:15,  3.44s/it]Running Inference:  11%|█         | 22/200 [01:00<10:41,  3.60s/it]Running Inference:  12%|█▏        | 23/200 [01:04<10:50,  3.68s/it]Running Inference:  12%|█▏        | 24/200 [01:05<08:44,  2.98s/it]Running Inference:  12%|█▎        | 25/200 [01:10<09:52,  3.39s/it]Running Inference:  13%|█▎        | 26/200 [01:11<07:50,  2.71s/it]Running Inference:  14%|█▎        | 27/200 [01:15<09:27,  3.28s/it]Running Inference:  14%|█▍        | 28/200 [01:17<07:46,  2.71s/it]Running Inference:  14%|█▍        | 29/200 [01:17<05:58,  2.10s/it]Running Inference:  15%|█▌        | 30/200 [01:22<08:15,  2.91s/it]Running Inference:  16%|█▌        | 31/200 [01:24<06:49,  2.42s/it]Running Inference:  16%|█▌        | 32/200 [01:25<05:32,  1.98s/it]Running Inference:  16%|█▋        | 33/200 [01:25<04:36,  1.66s/it]Running Inference:  17%|█▋        | 34/200 [01:26<03:43,  1.35s/it]Running Inference:  18%|█▊        | 35/200 [01:28<03:47,  1.38s/it]Running Inference:  18%|█▊        | 36/200 [01:28<03:10,  1.16s/it]Running Inference:  18%|█▊        | 37/200 [01:29<03:16,  1.21s/it]Running Inference:  19%|█▉        | 38/200 [01:31<03:17,  1.22s/it]Running Inference:  20%|█▉        | 39/200 [01:35<05:32,  2.07s/it]Running Inference:  20%|██        | 40/200 [01:39<07:03,  2.64s/it]Running Inference:  20%|██        | 41/200 [01:44<08:42,  3.29s/it]Running Inference:  21%|██        | 42/200 [01:48<09:11,  3.49s/it]Running Inference:  22%|██▏       | 43/200 [01:51<09:26,  3.61s/it]Running Inference:  22%|██▏       | 44/200 [01:56<09:54,  3.81s/it]Running Inference:  22%|██▎       | 45/200 [02:00<10:33,  4.09s/it]Running Inference:  23%|██▎       | 46/200 [02:01<07:46,  3.03s/it]Running Inference:  24%|██▎       | 47/200 [02:05<08:45,  3.43s/it]Running Inference:  24%|██▍       | 48/200 [02:06<06:47,  2.68s/it]Running Inference:  24%|██▍       | 49/200 [02:11<07:56,  3.15s/it]Running Inference:  25%|██▌       | 50/200 [02:14<08:08,  3.26s/it]Running Inference:  26%|██▌       | 51/200 [02:15<06:19,  2.55s/it]Running Inference:  26%|██▌       | 52/200 [02:19<07:25,  3.01s/it]Running Inference:  26%|██▋       | 53/200 [02:20<05:59,  2.45s/it]Running Inference:  27%|██▋       | 54/200 [02:21<04:33,  1.88s/it]Running Inference:  28%|██▊       | 55/200 [02:25<05:57,  2.47s/it]Running Inference:  28%|██▊       | 56/200 [02:26<04:54,  2.04s/it]Running Inference:  28%|██▊       | 57/200 [02:26<03:51,  1.62s/it]Running Inference:  29%|██▉       | 58/200 [02:27<03:34,  1.51s/it]Running Inference:  30%|██▉       | 59/200 [02:28<03:08,  1.34s/it]Running Inference:  30%|███       | 60/200 [02:29<02:30,  1.07s/it]Running Inference:  30%|███       | 61/200 [02:34<05:01,  2.17s/it]Running Inference:  31%|███       | 62/200 [02:34<03:54,  1.70s/it]Running Inference:  32%|███▏      | 63/200 [02:37<04:53,  2.14s/it]Running Inference:  32%|███▏      | 64/200 [02:38<04:01,  1.77s/it]Running Inference:  32%|███▎      | 65/200 [02:42<05:29,  2.44s/it]Running Inference:  33%|███▎      | 66/200 [02:43<04:23,  1.96s/it]Running Inference:  34%|███▎      | 67/200 [02:47<05:38,  2.55s/it]Running Inference:  34%|███▍      | 68/200 [02:51<06:35,  3.00s/it]Running Inference:  34%|███▍      | 69/200 [02:55<07:20,  3.36s/it]Running Inference:  35%|███▌      | 70/200 [02:56<05:49,  2.69s/it]Running Inference:  36%|███▌      | 71/200 [03:01<06:58,  3.25s/it]Running Inference:  36%|███▌      | 72/200 [03:02<05:42,  2.68s/it]Running Inference:  36%|███▋      | 73/200 [03:03<04:39,  2.20s/it]Running Inference:  37%|███▋      | 74/200 [03:04<03:51,  1.84s/it]Running Inference:  38%|███▊      | 75/200 [03:08<05:12,  2.50s/it]Running Inference:  38%|███▊      | 76/200 [03:09<04:13,  2.04s/it]Running Inference:  38%|███▊      | 77/200 [03:10<03:26,  1.68s/it]Running Inference:  39%|███▉      | 78/200 [03:12<03:09,  1.55s/it]Running Inference:  40%|███▉      | 79/200 [03:15<04:35,  2.28s/it]Running Inference:  40%|████      | 80/200 [03:20<05:41,  2.85s/it]Running Inference:  40%|████      | 81/200 [03:21<04:38,  2.34s/it]Running Inference:  41%|████      | 82/200 [03:22<03:52,  1.97s/it]Running Inference:  42%|████▏     | 83/200 [03:26<05:11,  2.66s/it]Running Inference:  42%|████▏     | 84/200 [03:28<04:27,  2.31s/it]Running Inference:  42%|████▎     | 85/200 [03:32<05:29,  2.86s/it]Running Inference:  43%|████▎     | 86/200 [03:34<04:49,  2.54s/it]Running Inference:  44%|████▎     | 87/200 [03:38<05:51,  3.11s/it]Running Inference:  44%|████▍     | 88/200 [03:39<04:28,  2.40s/it]Running Inference:  44%|████▍     | 89/200 [03:44<05:46,  3.12s/it]Running Inference:  45%|████▌     | 90/200 [03:45<04:30,  2.46s/it]Running Inference:  46%|████▌     | 91/200 [03:45<03:36,  1.98s/it]Running Inference:  46%|████▌     | 92/200 [03:47<03:12,  1.78s/it]Running Inference:  46%|████▋     | 93/200 [03:51<04:33,  2.56s/it]Running Inference:  47%|████▋     | 94/200 [03:52<03:48,  2.15s/it]Running Inference:  48%|████▊     | 95/200 [03:55<04:15,  2.44s/it]Running Inference:  48%|████▊     | 96/200 [03:56<03:29,  2.01s/it]Running Inference:  48%|████▊     | 97/200 [03:58<03:12,  1.87s/it]Running Inference:  49%|████▉     | 98/200 [04:02<04:21,  2.57s/it]Running Inference:  50%|████▉     | 99/200 [04:06<04:45,  2.83s/it]Running Inference:  50%|█████     | 100/200 [04:07<03:57,  2.38s/it]Running Inference:  50%|█████     | 101/200 [04:08<03:09,  1.91s/it]Running Inference:  51%|█████     | 102/200 [04:08<02:32,  1.55s/it]Running Inference:  52%|█████▏    | 103/200 [04:12<03:28,  2.15s/it]Running Inference:  52%|█████▏    | 104/200 [04:16<04:28,  2.79s/it]Running Inference:  52%|█████▎    | 105/200 [04:17<03:40,  2.32s/it]Running Inference:  53%|█████▎    | 106/200 [04:22<04:39,  2.98s/it]Running Inference:  54%|█████▎    | 107/200 [04:23<03:40,  2.37s/it]Running Inference:  54%|█████▍    | 108/200 [04:28<04:40,  3.05s/it]Running Inference:  55%|█████▍    | 109/200 [04:32<05:11,  3.43s/it]Running Inference:  55%|█████▌    | 110/200 [04:34<04:27,  2.97s/it]Running Inference:  56%|█████▌    | 111/200 [04:34<03:19,  2.25s/it]Running Inference:  56%|█████▌    | 112/200 [04:38<03:54,  2.66s/it]Running Inference:  56%|█████▋    | 113/200 [04:39<03:05,  2.13s/it]Running Inference:  57%|█████▋    | 114/200 [04:40<02:32,  1.77s/it]Running Inference:  57%|█████▊    | 115/200 [04:41<02:03,  1.46s/it]Running Inference:  58%|█████▊    | 116/200 [04:43<02:36,  1.86s/it]Running Inference:  58%|█████▊    | 117/200 [04:45<02:25,  1.75s/it]Running Inference:  59%|█████▉    | 118/200 [04:46<02:10,  1.59s/it]Running Inference:  60%|█████▉    | 119/200 [04:50<03:11,  2.37s/it]Running Inference:  60%|██████    | 120/200 [04:54<03:47,  2.84s/it]Running Inference:  60%|██████    | 121/200 [04:55<02:46,  2.11s/it]Running Inference:  61%|██████    | 122/200 [04:59<03:45,  2.89s/it]Running Inference:  62%|██████▏   | 123/200 [05:03<04:11,  3.26s/it]Running Inference:  62%|██████▏   | 124/200 [05:05<03:23,  2.67s/it]Running Inference:  62%|██████▎   | 125/200 [05:09<04:06,  3.29s/it]Running Inference:  63%|██████▎   | 126/200 [05:10<03:03,  2.48s/it]Running Inference:  64%|██████▎   | 127/200 [05:12<02:43,  2.24s/it]Running Inference:  64%|██████▍   | 128/200 [05:12<02:07,  1.77s/it]Running Inference:  64%|██████▍   | 129/200 [05:14<01:56,  1.64s/it]Running Inference:  65%|██████▌   | 130/200 [05:18<02:56,  2.52s/it]Running Inference:  66%|██████▌   | 131/200 [05:19<02:22,  2.07s/it]Running Inference:  66%|██████▌   | 132/200 [05:21<02:06,  1.86s/it]Running Inference:  66%|██████▋   | 133/200 [05:25<03:01,  2.71s/it]Running Inference:  67%|██████▋   | 134/200 [05:26<02:12,  2.00s/it]Running Inference:  68%|██████▊   | 135/200 [05:30<02:45,  2.55s/it]Running Inference:  68%|██████▊   | 136/200 [05:30<02:10,  2.04s/it]Running Inference:  68%|██████▊   | 137/200 [05:32<01:57,  1.86s/it]Running Inference:  69%|██████▉   | 138/200 [05:36<02:31,  2.45s/it]Running Inference:  70%|██████▉   | 139/200 [05:37<02:03,  2.02s/it]Running Inference:  70%|███████   | 140/200 [05:38<01:51,  1.85s/it]Running Inference:  70%|███████   | 141/200 [05:39<01:33,  1.58s/it]Running Inference:  71%|███████   | 142/200 [05:41<01:41,  1.75s/it]Running Inference:  72%|███████▏  | 143/200 [05:44<01:57,  2.06s/it]Running Inference:  72%|███████▏  | 144/200 [05:48<02:26,  2.61s/it]Running Inference:  72%|███████▎  | 145/200 [05:52<02:43,  2.97s/it]Running Inference:  73%|███████▎  | 146/200 [05:53<02:15,  2.51s/it]Running Inference:  74%|███████▎  | 147/200 [05:57<02:36,  2.95s/it]Running Inference:  74%|███████▍  | 148/200 [05:58<01:57,  2.26s/it]Running Inference:  74%|███████▍  | 149/200 [05:58<01:30,  1.78s/it]Running Inference:  75%|███████▌  | 150/200 [05:59<01:16,  1.53s/it]Running Inference:  76%|███████▌  | 151/200 [06:04<02:02,  2.49s/it]Running Inference:  76%|███████▌  | 152/200 [06:06<01:43,  2.15s/it]Running Inference:  76%|███████▋  | 153/200 [06:10<02:07,  2.72s/it]Running Inference:  77%|███████▋  | 154/200 [06:14<02:32,  3.31s/it]Running Inference:  78%|███████▊  | 155/200 [06:15<01:52,  2.51s/it]Running Inference:  78%|███████▊  | 156/200 [06:16<01:31,  2.09s/it]Running Inference:  78%|███████▊  | 157/200 [06:21<02:03,  2.88s/it]Running Inference:  79%|███████▉  | 158/200 [06:22<01:42,  2.44s/it]Running Inference:  80%|███████▉  | 159/200 [06:27<02:05,  3.05s/it]Running Inference:  80%|████████  | 160/200 [06:27<01:34,  2.36s/it]Running Inference:  80%|████████  | 161/200 [06:28<01:13,  1.87s/it]Running Inference:  81%|████████  | 162/200 [06:32<01:36,  2.55s/it]Running Inference:  82%|████████▏ | 163/200 [06:33<01:18,  2.13s/it]Running Inference:  82%|████████▏ | 164/200 [06:35<01:09,  1.93s/it]Running Inference:  82%|████████▎ | 165/200 [06:36<01:00,  1.72s/it]Running Inference:  83%|████████▎ | 166/200 [06:40<01:21,  2.38s/it]Running Inference:  84%|████████▎ | 167/200 [06:44<01:38,  2.98s/it]Running Inference:  84%|████████▍ | 168/200 [06:46<01:18,  2.46s/it]Running Inference:  84%|████████▍ | 169/200 [06:49<01:28,  2.87s/it]Running Inference:  85%|████████▌ | 170/200 [06:53<01:35,  3.17s/it]Running Inference:  86%|████████▌ | 171/200 [06:54<01:07,  2.33s/it]Running Inference:  86%|████████▌ | 172/200 [06:57<01:10,  2.50s/it]Running Inference:  86%|████████▋ | 173/200 [07:00<01:18,  2.91s/it]Running Inference:  87%|████████▋ | 174/200 [07:04<01:22,  3.18s/it]Running Inference:  88%|████████▊ | 175/200 [07:09<01:31,  3.64s/it]Running Inference:  88%|████████▊ | 176/200 [07:14<01:34,  3.95s/it]Running Inference:  88%|████████▊ | 177/200 [07:17<01:30,  3.92s/it]Running Inference:  89%|████████▉ | 178/200 [07:19<01:08,  3.11s/it]Running Inference:  90%|████████▉ | 179/200 [07:19<00:48,  2.33s/it]Running Inference:  90%|█████████ | 180/200 [07:23<00:55,  2.77s/it]Running Inference:  90%|█████████ | 181/200 [07:27<01:00,  3.19s/it]Running Inference:  91%|█████████ | 182/200 [07:32<01:04,  3.58s/it]Running Inference:  92%|█████████▏| 183/200 [07:36<01:05,  3.88s/it]Running Inference:  92%|█████████▏| 184/200 [07:37<00:47,  2.98s/it]Running Inference:  92%|█████████▎| 185/200 [07:41<00:50,  3.39s/it]Running Inference:  93%|█████████▎| 186/200 [07:42<00:37,  2.67s/it]Running Inference:  94%|█████████▎| 187/200 [07:45<00:32,  2.53s/it]Running Inference:  94%|█████████▍| 188/200 [07:49<00:35,  2.95s/it]Running Inference:  94%|█████████▍| 189/200 [07:53<00:36,  3.31s/it]Running Inference:  95%|█████████▌| 190/200 [07:57<00:34,  3.49s/it]Running Inference:  96%|█████████▌| 191/200 [07:57<00:23,  2.66s/it]Running Inference:  96%|█████████▌| 192/200 [08:02<00:25,  3.22s/it]Running Inference:  96%|█████████▋| 193/200 [08:03<00:18,  2.65s/it]Running Inference:  97%|█████████▋| 194/200 [08:04<00:13,  2.18s/it]Running Inference:  98%|█████████▊| 195/200 [08:08<00:13,  2.69s/it]Running Inference:  98%|█████████▊| 196/200 [08:09<00:08,  2.05s/it]Running Inference:  98%|█████████▊| 197/200 [08:10<00:05,  1.82s/it]Running Inference:  99%|█████████▉| 198/200 [08:11<00:03,  1.61s/it]Running Inference: 100%|█████████▉| 199/200 [08:16<00:02,  2.56s/it]Running Inference: 100%|██████████| 200/200 [08:17<00:00,  2.11s/it]Running Inference: 100%|██████████| 200/200 [08:17<00:00,  2.49s/it]
2025-12-14 17:28:18,907 - INFO - Inference completed.
2025-12-14 17:28:18,930 - INFO - Results saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 17:28:18,930 - INFO - Calculating metrics for dataset: longbench
2025-12-14 17:28:18,931 - INFO - Metrics saved to longbenchresult/longbench__trec__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 17:28:18,931 - INFO - Metrics:
28.5
2025-12-14 17:28:18,933 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=trec, ratio=0.5) on GPU 2


========================================
LongBench Task: dureader
========================================
----------------------------------------
Task: dureader | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:28:25,310 - INFO - Set deterministic seeds to 42
2025-12-14 17:28:25,311 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:28:25,311 - INFO - Starting evaluation run...
2025-12-14 17:28:25,311 - INFO - Output directory set to: longbenchresult
2025-12-14 17:28:25,311 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 17:28:25,311 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 17:28:25,311 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:28:25,311 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.74it/s]
Device set to use cuda:0
2025-12-14 17:28:38,076 - INFO - Model pipeline loaded.
2025-12-14 17:28:38,077 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 17:28:46,892 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:28:46,892 - INFO - Dataset processed with 200 entries.
2025-12-14 17:28:46,920 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:01,  8.75s/it]Running Inference:   1%|          | 2/200 [00:10<15:20,  4.65s/it]Running Inference:   2%|▏         | 3/200 [00:17<19:17,  5.88s/it]Running Inference:   2%|▏         | 4/200 [00:19<13:13,  4.05s/it]Running Inference:   2%|▎         | 5/200 [00:22<12:49,  3.95s/it]Running Inference:   3%|▎         | 6/200 [00:30<16:42,  5.17s/it]Running Inference:   4%|▎         | 7/200 [00:38<19:12,  5.97s/it]Running Inference:   4%|▍         | 8/200 [00:45<20:58,  6.56s/it]Running Inference:   4%|▍         | 9/200 [00:53<22:15,  6.99s/it]Running Inference:   5%|▌         | 10/200 [01:01<22:24,  7.07s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:08<22:17,  7.08s/it]Running Inference:   6%|▌         | 12/200 [01:15<22:18,  7.12s/it]Running Inference:   6%|▋         | 13/200 [01:22<22:15,  7.14s/it]Running Inference:   7%|▋         | 14/200 [01:29<22:24,  7.23s/it]Running Inference:   8%|▊         | 15/200 [01:37<22:18,  7.24s/it]Running Inference:   8%|▊         | 16/200 [01:44<22:00,  7.18s/it]Running Inference:   8%|▊         | 17/200 [01:51<21:54,  7.18s/it]Running Inference:   9%|▉         | 18/200 [01:59<22:13,  7.33s/it]Running Inference:  10%|▉         | 19/200 [02:06<22:04,  7.32s/it]Running Inference:  10%|█         | 20/200 [02:14<22:15,  7.42s/it]Running Inference:  10%|█         | 21/200 [02:21<22:25,  7.52s/it]Running Inference:  11%|█         | 22/200 [02:29<22:03,  7.43s/it]Running Inference:  12%|█▏        | 23/200 [02:30<16:48,  5.70s/it]Running Inference:  12%|█▏        | 24/200 [02:37<18:05,  6.17s/it]Running Inference:  12%|█▎        | 25/200 [02:44<18:40,  6.40s/it]Running Inference:  13%|█▎        | 26/200 [02:50<17:33,  6.05s/it]Running Inference:  14%|█▎        | 27/200 [02:57<18:32,  6.43s/it]Running Inference:  14%|█▍        | 28/200 [03:04<19:05,  6.66s/it]Running Inference:  14%|█▍        | 29/200 [03:12<19:45,  6.93s/it]Running Inference:  15%|█▌        | 30/200 [03:19<19:37,  6.93s/it]Running Inference:  16%|█▌        | 31/200 [03:26<19:56,  7.08s/it]Running Inference:  16%|█▌        | 32/200 [03:33<19:40,  7.03s/it]Running Inference:  16%|█▋        | 33/200 [03:40<19:48,  7.12s/it]Running Inference:  17%|█▋        | 34/200 [03:48<19:56,  7.20s/it]Running Inference:  18%|█▊        | 35/200 [03:55<19:45,  7.19s/it]Running Inference:  18%|█▊        | 36/200 [04:03<20:06,  7.36s/it]Running Inference:  18%|█▊        | 37/200 [04:10<20:21,  7.49s/it]Running Inference:  19%|█▉        | 38/200 [04:19<21:00,  7.78s/it]Running Inference:  20%|█▉        | 39/200 [04:21<16:01,  5.97s/it]Running Inference:  20%|██        | 40/200 [04:29<17:26,  6.54s/it]Running Inference:  20%|██        | 41/200 [04:36<17:46,  6.71s/it]Running Inference:  21%|██        | 42/200 [04:43<18:00,  6.84s/it]Running Inference:  22%|██▏       | 43/200 [04:50<18:34,  7.10s/it]Running Inference:  22%|██▏       | 44/200 [04:57<18:22,  7.07s/it]Running Inference:  22%|██▎       | 45/200 [05:05<18:52,  7.30s/it]Running Inference:  23%|██▎       | 46/200 [05:13<18:43,  7.29s/it]Running Inference:  24%|██▎       | 47/200 [05:20<18:37,  7.30s/it]Running Inference:  24%|██▍       | 48/200 [05:27<18:38,  7.36s/it]Running Inference:  24%|██▍       | 49/200 [05:35<18:29,  7.35s/it]Running Inference:  25%|██▌       | 50/200 [05:42<18:24,  7.36s/it]Running Inference:  26%|██▌       | 51/200 [05:50<18:20,  7.39s/it]Running Inference:  26%|██▌       | 52/200 [05:52<14:32,  5.90s/it]Running Inference:  26%|██▋       | 53/200 [06:00<15:41,  6.41s/it]Running Inference:  27%|██▋       | 54/200 [06:07<16:34,  6.81s/it]Running Inference:  28%|██▊       | 55/200 [06:14<16:35,  6.86s/it]Running Inference:  28%|██▊       | 56/200 [06:21<16:26,  6.85s/it]Running Inference:  28%|██▊       | 57/200 [06:28<16:26,  6.90s/it]Running Inference:  29%|██▉       | 58/200 [06:35<16:11,  6.84s/it]Running Inference:  30%|██▉       | 59/200 [06:40<15:03,  6.41s/it]Running Inference:  30%|███       | 60/200 [06:48<15:48,  6.77s/it]Running Inference:  30%|███       | 61/200 [06:56<16:30,  7.12s/it]Running Inference:  31%|███       | 62/200 [07:03<16:38,  7.23s/it]Running Inference:  32%|███▏      | 63/200 [07:11<16:52,  7.39s/it]Running Inference:  32%|███▏      | 64/200 [07:18<16:46,  7.40s/it]Running Inference:  32%|███▎      | 65/200 [07:22<14:09,  6.29s/it]Running Inference:  33%|███▎      | 66/200 [07:30<14:47,  6.63s/it]Running Inference:  34%|███▎      | 67/200 [07:37<15:25,  6.96s/it]Running Inference:  34%|███▍      | 68/200 [07:44<15:25,  7.01s/it]Running Inference:  34%|███▍      | 69/200 [07:49<13:49,  6.33s/it]Running Inference:  35%|███▌      | 70/200 [07:57<14:44,  6.80s/it]Running Inference:  36%|███▌      | 71/200 [08:05<15:06,  7.03s/it]Running Inference:  36%|███▌      | 72/200 [08:12<14:52,  6.97s/it]Running Inference:  36%|███▋      | 73/200 [08:19<15:21,  7.25s/it]Running Inference:  37%|███▋      | 74/200 [08:27<15:19,  7.30s/it]Running Inference:  38%|███▊      | 75/200 [08:34<15:08,  7.27s/it]Running Inference:  38%|███▊      | 76/200 [08:42<15:13,  7.37s/it]Running Inference:  38%|███▊      | 77/200 [08:47<13:38,  6.65s/it]Running Inference:  39%|███▉      | 78/200 [08:54<14:09,  6.97s/it]Running Inference:  40%|███▉      | 79/200 [09:02<14:16,  7.08s/it]Running Inference:  40%|████      | 80/200 [09:09<14:23,  7.20s/it]Running Inference:  40%|████      | 81/200 [09:16<14:12,  7.17s/it]Running Inference:  41%|████      | 82/200 [09:24<14:11,  7.22s/it]Running Inference:  42%|████▏     | 83/200 [09:31<13:59,  7.18s/it]Running Inference:  42%|████▏     | 84/200 [09:36<13:01,  6.74s/it]Running Inference:  42%|████▎     | 85/200 [09:44<13:14,  6.91s/it]Running Inference:  43%|████▎     | 86/200 [09:47<11:10,  5.88s/it]Running Inference:  44%|████▎     | 87/200 [09:54<11:42,  6.22s/it]Running Inference:  44%|████▍     | 88/200 [10:01<12:05,  6.48s/it]Running Inference:  44%|████▍     | 89/200 [10:09<12:29,  6.75s/it]Running Inference:  45%|████▌     | 90/200 [10:16<12:30,  6.82s/it]Running Inference:  46%|████▌     | 91/200 [10:23<12:43,  7.00s/it]Running Inference:  46%|████▌     | 92/200 [10:27<10:46,  5.98s/it]Running Inference:  46%|████▋     | 93/200 [10:34<11:34,  6.49s/it]Running Inference:  47%|████▋     | 94/200 [10:42<12:07,  6.86s/it]Running Inference:  48%|████▊     | 95/200 [10:49<12:16,  7.01s/it]Running Inference:  48%|████▊     | 96/200 [10:57<12:16,  7.08s/it]Running Inference:  48%|████▊     | 97/200 [11:03<12:01,  7.00s/it]Running Inference:  49%|████▉     | 98/200 [11:10<11:52,  6.98s/it]Running Inference:  50%|████▉     | 99/200 [11:18<12:01,  7.14s/it]Running Inference:  50%|█████     | 100/200 [11:25<11:52,  7.12s/it]Running Inference:  50%|█████     | 101/200 [11:32<11:46,  7.13s/it]Running Inference:  51%|█████     | 102/200 [11:39<11:41,  7.16s/it]Running Inference:  52%|█████▏    | 103/200 [11:47<11:41,  7.23s/it]Running Inference:  52%|█████▏    | 104/200 [11:54<11:33,  7.23s/it]Running Inference:  52%|█████▎    | 105/200 [12:01<11:25,  7.22s/it]Running Inference:  53%|█████▎    | 106/200 [12:09<11:21,  7.25s/it]Running Inference:  54%|█████▎    | 107/200 [12:16<11:21,  7.33s/it]Running Inference:  54%|█████▍    | 108/200 [12:20<09:36,  6.27s/it]Running Inference:  55%|█████▍    | 109/200 [12:27<10:04,  6.64s/it]Running Inference:  55%|█████▌    | 110/200 [12:36<10:40,  7.12s/it]Running Inference:  56%|█████▌    | 111/200 [12:43<10:34,  7.13s/it]Running Inference:  56%|█████▌    | 112/200 [12:50<10:22,  7.07s/it]Running Inference:  56%|█████▋    | 113/200 [12:57<10:25,  7.19s/it]Running Inference:  57%|█████▋    | 114/200 [13:05<10:23,  7.25s/it]Running Inference:  57%|█████▊    | 115/200 [13:12<10:20,  7.30s/it]Running Inference:  58%|█████▊    | 116/200 [13:19<10:17,  7.36s/it]Running Inference:  58%|█████▊    | 117/200 [13:27<10:09,  7.34s/it]Running Inference:  59%|█████▉    | 118/200 [13:35<10:22,  7.60s/it]Running Inference:  60%|█████▉    | 119/200 [13:43<10:28,  7.76s/it]Running Inference:  60%|██████    | 120/200 [13:52<10:37,  7.97s/it]Running Inference:  60%|██████    | 121/200 [13:59<10:18,  7.83s/it]Running Inference:  61%|██████    | 122/200 [14:06<09:49,  7.56s/it]Running Inference:  62%|██████▏   | 123/200 [14:14<09:46,  7.62s/it]Running Inference:  62%|██████▏   | 124/200 [14:21<09:31,  7.52s/it]Running Inference:  62%|██████▎   | 125/200 [14:28<09:08,  7.32s/it]Running Inference:  63%|██████▎   | 126/200 [14:35<08:55,  7.23s/it]Running Inference:  64%|██████▎   | 127/200 [14:42<08:50,  7.27s/it]Running Inference:  64%|██████▍   | 128/200 [14:49<08:42,  7.25s/it]Running Inference:  64%|██████▍   | 129/200 [14:55<08:07,  6.87s/it]Running Inference:  65%|██████▌   | 130/200 [15:03<08:08,  6.98s/it]Running Inference:  66%|██████▌   | 131/200 [15:10<08:04,  7.02s/it]Running Inference:  66%|██████▌   | 132/200 [15:17<08:02,  7.10s/it]Running Inference:  66%|██████▋   | 133/200 [15:24<07:58,  7.15s/it]Running Inference:  67%|██████▋   | 134/200 [15:32<08:06,  7.38s/it]Running Inference:  68%|██████▊   | 135/200 [15:39<07:55,  7.31s/it]Running Inference:  68%|██████▊   | 136/200 [15:45<07:19,  6.87s/it]Running Inference:  68%|██████▊   | 137/200 [15:51<06:53,  6.57s/it]Running Inference:  69%|██████▉   | 138/200 [15:59<07:17,  7.05s/it]Running Inference:  70%|██████▉   | 139/200 [16:07<07:18,  7.20s/it]Running Inference:  70%|███████   | 140/200 [16:14<07:05,  7.09s/it]Running Inference:  70%|███████   | 141/200 [16:19<06:29,  6.59s/it]Running Inference:  71%|███████   | 142/200 [16:23<05:37,  5.82s/it]Running Inference:  72%|███████▏  | 143/200 [16:30<05:58,  6.29s/it]Running Inference:  72%|███████▏  | 144/200 [16:37<06:03,  6.50s/it]Running Inference:  72%|███████▎  | 145/200 [16:45<06:12,  6.78s/it]Running Inference:  73%|███████▎  | 146/200 [16:52<06:14,  6.93s/it]Running Inference:  74%|███████▎  | 147/200 [16:59<06:08,  6.94s/it]Running Inference:  74%|███████▍  | 148/200 [17:07<06:17,  7.25s/it]Running Inference:  74%|███████▍  | 149/200 [17:15<06:19,  7.45s/it]Running Inference:  75%|███████▌  | 150/200 [17:22<06:09,  7.39s/it]Running Inference:  76%|███████▌  | 151/200 [17:30<06:03,  7.42s/it]Running Inference:  76%|███████▌  | 152/200 [17:37<05:50,  7.30s/it]Running Inference:  76%|███████▋  | 153/200 [17:45<05:49,  7.43s/it]Running Inference:  77%|███████▋  | 154/200 [17:52<05:36,  7.31s/it]Running Inference:  78%|███████▊  | 155/200 [17:59<05:31,  7.36s/it]Running Inference:  78%|███████▊  | 156/200 [18:05<05:02,  6.87s/it]Running Inference:  78%|███████▊  | 157/200 [18:13<05:07,  7.15s/it]Running Inference:  79%|███████▉  | 158/200 [18:20<04:59,  7.13s/it]Running Inference:  80%|███████▉  | 159/200 [18:27<05:00,  7.33s/it]Running Inference:  80%|████████  | 160/200 [18:34<04:46,  7.17s/it]Running Inference:  80%|████████  | 161/200 [18:42<04:41,  7.22s/it]Running Inference:  81%|████████  | 162/200 [18:49<04:31,  7.15s/it]Running Inference:  82%|████████▏ | 163/200 [18:57<04:37,  7.49s/it]Running Inference:  82%|████████▏ | 164/200 [19:05<04:39,  7.75s/it]Running Inference:  82%|████████▎ | 165/200 [19:13<04:27,  7.65s/it]Running Inference:  83%|████████▎ | 166/200 [19:20<04:17,  7.59s/it]Running Inference:  84%|████████▎ | 167/200 [19:28<04:08,  7.54s/it]Running Inference:  84%|████████▍ | 168/200 [19:35<04:02,  7.57s/it]Running Inference:  84%|████████▍ | 169/200 [19:43<03:55,  7.59s/it]Running Inference:  85%|████████▌ | 170/200 [19:47<03:18,  6.62s/it]Running Inference:  86%|████████▌ | 171/200 [19:55<03:22,  6.98s/it]Running Inference:  86%|████████▌ | 172/200 [20:03<03:22,  7.22s/it]Running Inference:  86%|████████▋ | 173/200 [20:10<03:15,  7.26s/it]Running Inference:  87%|████████▋ | 174/200 [20:17<03:05,  7.13s/it]Running Inference:  88%|████████▊ | 175/200 [20:24<02:56,  7.05s/it]Running Inference:  88%|████████▊ | 176/200 [20:31<02:53,  7.25s/it]Running Inference:  88%|████████▊ | 177/200 [20:39<02:50,  7.39s/it]Running Inference:  89%|████████▉ | 178/200 [20:47<02:45,  7.53s/it]Running Inference:  90%|████████▉ | 179/200 [20:54<02:33,  7.30s/it]Running Inference:  90%|█████████ | 180/200 [21:03<02:34,  7.72s/it]Running Inference:  90%|█████████ | 181/200 [21:10<02:26,  7.69s/it]Running Inference:  91%|█████████ | 182/200 [21:18<02:18,  7.68s/it]Running Inference:  92%|█████████▏| 183/200 [21:25<02:07,  7.53s/it]Running Inference:  92%|█████████▏| 184/200 [21:33<02:02,  7.63s/it]Running Inference:  92%|█████████▎| 185/200 [21:40<01:52,  7.49s/it]Running Inference:  93%|█████████▎| 186/200 [21:48<01:47,  7.65s/it]Running Inference:  94%|█████████▎| 187/200 [21:56<01:39,  7.64s/it]Running Inference:  94%|█████████▍| 188/200 [22:03<01:30,  7.51s/it]Running Inference:  94%|█████████▍| 189/200 [22:07<01:12,  6.61s/it]Running Inference:  95%|█████████▌| 190/200 [22:15<01:07,  6.77s/it]Running Inference:  96%|█████████▌| 191/200 [22:22<01:02,  6.94s/it]Running Inference:  96%|█████████▌| 192/200 [22:29<00:56,  7.05s/it]Running Inference:  96%|█████████▋| 193/200 [22:37<00:51,  7.30s/it]Running Inference:  97%|█████████▋| 194/200 [22:45<00:44,  7.43s/it]Running Inference:  98%|█████████▊| 195/200 [22:52<00:36,  7.34s/it]Running Inference:  98%|█████████▊| 196/200 [23:00<00:29,  7.42s/it]Running Inference:  98%|█████████▊| 197/200 [23:07<00:22,  7.51s/it]Running Inference:  99%|█████████▉| 198/200 [23:14<00:14,  7.34s/it]Running Inference: 100%|█████████▉| 199/200 [23:22<00:07,  7.52s/it]Running Inference: 100%|██████████| 200/200 [23:29<00:00,  7.47s/it]Running Inference: 100%|██████████| 200/200 [23:29<00:00,  7.05s/it]
2025-12-14 17:52:16,932 - INFO - Inference completed.
2025-12-14 17:52:16,943 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 17:52:16,943 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.681 seconds.
Prefix dict has been built successfully.
2025-12-14 17:52:19,437 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 17:52:19,437 - INFO - Metrics:
13.4
2025-12-14 17:52:19,438 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=dureader, ratio=0.1) on GPU 2

----------------------------------------
Task: dureader | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 17:52:25,994 - INFO - Set deterministic seeds to 42
2025-12-14 17:52:25,995 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 17:52:25,995 - INFO - Starting evaluation run...
2025-12-14 17:52:25,995 - INFO - Output directory set to: longbenchresult
2025-12-14 17:52:25,995 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 17:52:25,995 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 17:52:25,995 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 17:52:25,995 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.71it/s]
Device set to use cuda:0
2025-12-14 17:52:39,708 - INFO - Model pipeline loaded.
2025-12-14 17:52:39,708 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 17:52:43,004 - INFO - Dataset loaded with 200 entries.
2025-12-14 17:52:43,004 - INFO - Dataset processed with 200 entries.
2025-12-14 17:52:43,030 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:18,  8.84s/it]Running Inference:   1%|          | 2/200 [00:12<18:58,  5.75s/it]Running Inference:   2%|▏         | 3/200 [00:19<21:16,  6.48s/it]Running Inference:   2%|▏         | 4/200 [00:21<14:30,  4.44s/it]Running Inference:   2%|▎         | 5/200 [00:24<13:17,  4.09s/it]Running Inference:   3%|▎         | 6/200 [00:32<17:00,  5.26s/it]Running Inference:   4%|▎         | 7/200 [00:39<19:25,  6.04s/it]Running Inference:   4%|▍         | 8/200 [00:47<21:06,  6.60s/it]Running Inference:   4%|▍         | 9/200 [00:55<22:21,  7.02s/it]Running Inference:   5%|▌         | 10/200 [01:02<22:27,  7.09s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:09<22:19,  7.09s/it]Running Inference:   6%|▌         | 12/200 [01:16<21:46,  6.95s/it]Running Inference:   6%|▋         | 13/200 [01:20<19:21,  6.21s/it]Running Inference:   7%|▋         | 14/200 [01:28<20:21,  6.57s/it]Running Inference:   8%|▊         | 15/200 [01:35<20:52,  6.77s/it]Running Inference:   8%|▊         | 16/200 [01:42<20:59,  6.85s/it]Running Inference:   8%|▊         | 17/200 [01:49<21:11,  6.95s/it]Running Inference:   9%|▉         | 18/200 [01:57<21:44,  7.17s/it]Running Inference:  10%|▉         | 19/200 [02:04<21:44,  7.21s/it]Running Inference:  10%|█         | 20/200 [02:12<21:59,  7.33s/it]Running Inference:  10%|█         | 21/200 [02:20<22:15,  7.46s/it]Running Inference:  11%|█         | 22/200 [02:27<21:55,  7.39s/it]Running Inference:  12%|█▏        | 23/200 [02:29<16:43,  5.67s/it]Running Inference:  12%|█▏        | 24/200 [02:36<18:01,  6.15s/it]Running Inference:  12%|█▎        | 25/200 [02:43<18:37,  6.38s/it]Running Inference:  13%|█▎        | 26/200 [02:47<16:31,  5.70s/it]Running Inference:  14%|█▎        | 27/200 [02:54<17:49,  6.18s/it]Running Inference:  14%|█▍        | 28/200 [03:01<18:35,  6.48s/it]Running Inference:  14%|█▍        | 29/200 [03:09<19:27,  6.83s/it]Running Inference:  15%|█▌        | 30/200 [03:16<19:22,  6.84s/it]Running Inference:  16%|█▌        | 31/200 [03:23<19:43,  7.00s/it]Running Inference:  16%|█▌        | 32/200 [03:27<16:41,  5.96s/it]Running Inference:  16%|█▋        | 33/200 [03:34<17:39,  6.35s/it]Running Inference:  17%|█▋        | 34/200 [03:41<18:20,  6.63s/it]Running Inference:  18%|█▊        | 35/200 [03:48<18:34,  6.75s/it]Running Inference:  18%|█▊        | 36/200 [03:56<19:10,  7.02s/it]Running Inference:  18%|█▊        | 37/200 [04:04<19:34,  7.20s/it]Running Inference:  19%|█▉        | 38/200 [04:12<20:16,  7.51s/it]Running Inference:  20%|█▉        | 39/200 [04:14<15:30,  5.78s/it]Running Inference:  20%|██        | 40/200 [04:19<15:08,  5.68s/it]Running Inference:  20%|██        | 41/200 [04:26<16:05,  6.07s/it]Running Inference:  21%|██        | 42/200 [04:33<16:44,  6.36s/it]Running Inference:  22%|██▏       | 43/200 [04:41<17:37,  6.73s/it]Running Inference:  22%|██▏       | 44/200 [04:48<17:38,  6.79s/it]Running Inference:  22%|██▎       | 45/200 [04:55<18:16,  7.07s/it]Running Inference:  23%|██▎       | 46/200 [05:02<18:13,  7.10s/it]Running Inference:  24%|██▎       | 47/200 [05:10<18:12,  7.14s/it]Running Inference:  24%|██▍       | 48/200 [05:17<18:16,  7.21s/it]Running Inference:  24%|██▍       | 49/200 [05:24<18:09,  7.22s/it]Running Inference:  25%|██▌       | 50/200 [05:32<18:07,  7.25s/it]Running Inference:  26%|██▌       | 51/200 [05:39<18:03,  7.28s/it]Running Inference:  26%|██▌       | 52/200 [05:42<14:27,  5.86s/it]Running Inference:  26%|██▋       | 53/200 [05:48<14:35,  5.96s/it]Running Inference:  27%|██▋       | 54/200 [05:55<15:44,  6.47s/it]Running Inference:  28%|██▊       | 55/200 [06:02<15:56,  6.59s/it]Running Inference:  28%|██▊       | 56/200 [06:09<15:53,  6.62s/it]Running Inference:  28%|██▊       | 57/200 [06:16<15:59,  6.71s/it]Running Inference:  29%|██▉       | 58/200 [06:20<14:08,  5.97s/it]Running Inference:  30%|██▉       | 59/200 [06:22<11:23,  4.85s/it]Running Inference:  30%|███       | 60/200 [06:30<13:10,  5.64s/it]Running Inference:  30%|███       | 61/200 [06:38<14:35,  6.30s/it]Running Inference:  31%|███       | 62/200 [06:45<15:13,  6.62s/it]Running Inference:  32%|███▏      | 63/200 [06:53<15:49,  6.93s/it]Running Inference:  32%|███▏      | 64/200 [07:00<15:57,  7.04s/it]Running Inference:  32%|███▎      | 65/200 [07:08<16:13,  7.21s/it]Running Inference:  33%|███▎      | 66/200 [07:15<16:09,  7.24s/it]Running Inference:  34%|███▎      | 67/200 [07:23<16:18,  7.36s/it]Running Inference:  34%|███▍      | 68/200 [07:30<15:58,  7.26s/it]Running Inference:  34%|███▍      | 69/200 [07:34<14:09,  6.49s/it]Running Inference:  35%|███▌      | 70/200 [07:42<14:54,  6.88s/it]Running Inference:  36%|███▌      | 71/200 [07:49<15:09,  7.05s/it]Running Inference:  36%|███▌      | 72/200 [07:55<13:48,  6.47s/it]Running Inference:  36%|███▋      | 73/200 [08:02<14:33,  6.87s/it]Running Inference:  37%|███▋      | 74/200 [08:10<14:42,  7.00s/it]Running Inference:  38%|███▊      | 75/200 [08:17<14:38,  7.03s/it]Running Inference:  38%|███▊      | 76/200 [08:22<13:18,  6.44s/it]Running Inference:  38%|███▊      | 77/200 [08:27<12:15,  5.98s/it]Running Inference:  39%|███▉      | 78/200 [08:34<13:08,  6.46s/it]Running Inference:  40%|███▉      | 79/200 [08:42<13:30,  6.70s/it]Running Inference:  40%|████      | 80/200 [08:49<13:47,  6.90s/it]Running Inference:  40%|████      | 81/200 [08:56<13:43,  6.92s/it]Running Inference:  41%|████      | 82/200 [09:03<13:47,  7.01s/it]Running Inference:  42%|████▏     | 83/200 [09:10<13:39,  7.00s/it]Running Inference:  42%|████▏     | 84/200 [09:17<13:35,  7.03s/it]Running Inference:  42%|████▎     | 85/200 [09:19<10:37,  5.54s/it]Running Inference:  43%|████▎     | 86/200 [09:26<11:22,  5.98s/it]Running Inference:  44%|████▎     | 87/200 [09:33<11:47,  6.26s/it]Running Inference:  44%|████▍     | 88/200 [09:40<12:05,  6.48s/it]Running Inference:  44%|████▍     | 89/200 [09:48<12:26,  6.73s/it]Running Inference:  45%|████▌     | 90/200 [09:54<12:24,  6.77s/it]Running Inference:  46%|████▌     | 91/200 [09:57<09:58,  5.49s/it]Running Inference:  46%|████▌     | 92/200 [10:00<08:40,  4.82s/it]Running Inference:  46%|████▋     | 93/200 [10:02<07:15,  4.07s/it]Running Inference:  47%|████▋     | 94/200 [10:10<09:04,  5.14s/it]Running Inference:  48%|████▊     | 95/200 [10:17<10:06,  5.78s/it]Running Inference:  48%|████▊     | 96/200 [10:25<10:43,  6.19s/it]Running Inference:  48%|████▊     | 97/200 [10:31<10:53,  6.35s/it]Running Inference:  49%|████▉     | 98/200 [10:38<11:02,  6.50s/it]Running Inference:  50%|████▉     | 99/200 [10:46<11:26,  6.80s/it]Running Inference:  50%|█████     | 100/200 [10:53<11:24,  6.85s/it]Running Inference:  50%|█████     | 101/200 [10:59<11:10,  6.77s/it]Running Inference:  51%|█████     | 102/200 [11:06<11:13,  6.88s/it]Running Inference:  52%|█████▏    | 103/200 [11:10<09:29,  5.88s/it]Running Inference:  52%|█████▏    | 104/200 [11:17<09:59,  6.25s/it]Running Inference:  52%|█████▎    | 105/200 [11:24<10:17,  6.50s/it]Running Inference:  53%|█████▎    | 106/200 [11:31<10:30,  6.71s/it]Running Inference:  54%|█████▎    | 107/200 [11:39<10:41,  6.90s/it]Running Inference:  54%|█████▍    | 108/200 [11:46<10:37,  6.93s/it]Running Inference:  55%|█████▍    | 109/200 [11:53<10:43,  7.07s/it]Running Inference:  55%|█████▌    | 110/200 [12:01<11:04,  7.38s/it]Running Inference:  56%|█████▌    | 111/200 [12:06<09:50,  6.64s/it]Running Inference:  56%|█████▌    | 112/200 [12:13<09:49,  6.70s/it]Running Inference:  56%|█████▋    | 113/200 [12:20<10:00,  6.91s/it]Running Inference:  57%|█████▋    | 114/200 [12:28<10:04,  7.02s/it]Running Inference:  57%|█████▊    | 115/200 [12:35<10:04,  7.12s/it]Running Inference:  58%|█████▊    | 116/200 [12:38<08:10,  5.84s/it]Running Inference:  58%|█████▊    | 117/200 [12:42<07:14,  5.23s/it]Running Inference:  59%|█████▉    | 118/200 [12:50<08:18,  6.09s/it]Running Inference:  60%|█████▉    | 119/200 [12:58<09:00,  6.67s/it]Running Inference:  60%|██████    | 120/200 [13:06<09:32,  7.16s/it]Running Inference:  60%|██████    | 121/200 [13:13<09:31,  7.23s/it]Running Inference:  61%|██████    | 122/200 [13:20<09:14,  7.11s/it]Running Inference:  62%|██████▏   | 123/200 [13:28<09:20,  7.28s/it]Running Inference:  62%|██████▏   | 124/200 [13:35<09:11,  7.25s/it]Running Inference:  62%|██████▎   | 125/200 [13:42<08:56,  7.15s/it]Running Inference:  63%|██████▎   | 126/200 [13:49<08:43,  7.08s/it]Running Inference:  64%|██████▎   | 127/200 [13:56<08:40,  7.13s/it]Running Inference:  64%|██████▍   | 128/200 [14:03<08:32,  7.12s/it]Running Inference:  64%|██████▍   | 129/200 [14:05<06:40,  5.65s/it]Running Inference:  65%|██████▌   | 130/200 [14:09<05:59,  5.14s/it]Running Inference:  66%|██████▌   | 131/200 [14:16<06:33,  5.70s/it]Running Inference:  66%|██████▌   | 132/200 [14:24<06:57,  6.14s/it]Running Inference:  66%|██████▋   | 133/200 [14:31<07:12,  6.45s/it]Running Inference:  67%|██████▋   | 134/200 [14:38<07:32,  6.85s/it]Running Inference:  68%|██████▊   | 135/200 [14:46<07:29,  6.91s/it]Running Inference:  68%|██████▊   | 136/200 [14:53<07:35,  7.11s/it]Running Inference:  68%|██████▊   | 137/200 [15:00<07:26,  7.09s/it]Running Inference:  69%|██████▉   | 138/200 [15:08<07:37,  7.38s/it]Running Inference:  70%|██████▉   | 139/200 [15:16<07:30,  7.39s/it]Running Inference:  70%|███████   | 140/200 [15:19<06:03,  6.06s/it]Running Inference:  70%|███████   | 141/200 [15:25<06:03,  6.16s/it]Running Inference:  71%|███████   | 142/200 [15:31<05:51,  6.06s/it]Running Inference:  72%|███████▏  | 143/200 [15:38<06:06,  6.42s/it]Running Inference:  72%|███████▏  | 144/200 [15:45<06:07,  6.56s/it]Running Inference:  72%|███████▎  | 145/200 [15:52<06:13,  6.79s/it]Running Inference:  73%|███████▎  | 146/200 [15:59<06:13,  6.91s/it]Running Inference:  74%|███████▎  | 147/200 [16:06<06:05,  6.90s/it]Running Inference:  74%|███████▍  | 148/200 [16:14<06:13,  7.19s/it]Running Inference:  74%|███████▍  | 149/200 [16:22<06:15,  7.37s/it]Running Inference:  75%|███████▌  | 150/200 [16:29<06:05,  7.31s/it]Running Inference:  76%|███████▌  | 151/200 [16:32<04:59,  6.12s/it]Running Inference:  76%|███████▌  | 152/200 [16:37<04:30,  5.63s/it]Running Inference:  76%|███████▋  | 153/200 [16:45<04:52,  6.23s/it]Running Inference:  77%|███████▋  | 154/200 [16:48<04:01,  5.24s/it]Running Inference:  78%|███████▊  | 155/200 [16:55<04:24,  5.88s/it]Running Inference:  78%|███████▊  | 156/200 [17:01<04:18,  5.88s/it]Running Inference:  78%|███████▊  | 157/200 [17:08<04:36,  6.42s/it]Running Inference:  79%|███████▉  | 158/200 [17:15<04:37,  6.60s/it]Running Inference:  80%|███████▉  | 159/200 [17:23<04:43,  6.92s/it]Running Inference:  80%|████████  | 160/200 [17:29<04:26,  6.67s/it]Running Inference:  80%|████████  | 161/200 [17:36<04:26,  6.84s/it]Running Inference:  81%|████████  | 162/200 [17:44<04:23,  6.95s/it]Running Inference:  82%|████████▏ | 163/200 [17:52<04:30,  7.31s/it]Running Inference:  82%|████████▏ | 164/200 [17:55<03:38,  6.06s/it]Running Inference:  82%|████████▎ | 165/200 [18:02<03:45,  6.44s/it]Running Inference:  83%|████████▎ | 166/200 [18:10<03:47,  6.70s/it]Running Inference:  84%|████████▎ | 167/200 [18:17<03:46,  6.87s/it]Running Inference:  84%|████████▍ | 168/200 [18:24<03:46,  7.07s/it]Running Inference:  84%|████████▍ | 169/200 [18:32<03:41,  7.16s/it]Running Inference:  85%|████████▌ | 170/200 [18:39<03:32,  7.08s/it]Running Inference:  86%|████████▌ | 171/200 [18:46<03:30,  7.26s/it]Running Inference:  86%|████████▌ | 172/200 [18:54<03:26,  7.38s/it]Running Inference:  86%|████████▋ | 173/200 [19:01<03:18,  7.33s/it]Running Inference:  87%|████████▋ | 174/200 [19:08<03:05,  7.14s/it]Running Inference:  88%|████████▊ | 175/200 [19:15<02:55,  7.03s/it]Running Inference:  88%|████████▊ | 176/200 [19:22<02:52,  7.20s/it]Running Inference:  88%|████████▊ | 177/200 [19:30<02:48,  7.32s/it]Running Inference:  89%|████████▉ | 178/200 [19:33<02:14,  6.12s/it]Running Inference:  90%|████████▉ | 179/200 [19:40<02:12,  6.30s/it]Running Inference:  90%|█████████ | 180/200 [19:49<02:19,  7.00s/it]Running Inference:  90%|█████████ | 181/200 [19:51<01:49,  5.74s/it]Running Inference:  91%|█████████ | 182/200 [19:59<01:53,  6.31s/it]Running Inference:  92%|█████████▏| 183/200 [20:02<01:31,  5.36s/it]Running Inference:  92%|█████████▏| 184/200 [20:10<01:37,  6.08s/it]Running Inference:  92%|█████████▎| 185/200 [20:17<01:35,  6.38s/it]Running Inference:  93%|█████████▎| 186/200 [20:20<01:16,  5.44s/it]Running Inference:  94%|█████████▎| 187/200 [20:28<01:18,  6.07s/it]Running Inference:  94%|█████████▍| 188/200 [20:35<01:16,  6.37s/it]Running Inference:  94%|█████████▍| 189/200 [20:37<00:56,  5.15s/it]Running Inference:  95%|█████████▌| 190/200 [20:44<00:57,  5.72s/it]Running Inference:  96%|█████████▌| 191/200 [20:51<00:55,  6.17s/it]Running Inference:  96%|█████████▌| 192/200 [20:59<00:51,  6.48s/it]Running Inference:  96%|█████████▋| 193/200 [21:06<00:48,  6.87s/it]Running Inference:  97%|█████████▋| 194/200 [21:14<00:42,  7.09s/it]Running Inference:  98%|█████████▊| 195/200 [21:21<00:35,  7.08s/it]Running Inference:  98%|█████████▊| 196/200 [21:29<00:28,  7.21s/it]Running Inference:  98%|█████████▊| 197/200 [21:36<00:21,  7.33s/it]Running Inference:  99%|█████████▉| 198/200 [21:43<00:14,  7.17s/it]Running Inference: 100%|█████████▉| 199/200 [21:51<00:07,  7.37s/it]Running Inference: 100%|██████████| 200/200 [21:58<00:00,  7.33s/it]Running Inference: 100%|██████████| 200/200 [21:58<00:00,  6.59s/it]
2025-12-14 18:14:41,596 - INFO - Inference completed.
2025-12-14 18:14:41,607 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 18:14:41,607 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.660 seconds.
Prefix dict has been built successfully.
2025-12-14 18:14:44,016 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 18:14:44,017 - INFO - Metrics:
12.58
2025-12-14 18:14:44,018 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=dureader, ratio=0.2) on GPU 2

----------------------------------------
Task: dureader | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:14:51,052 - INFO - Set deterministic seeds to 42
2025-12-14 18:14:51,052 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:14:51,052 - INFO - Starting evaluation run...
2025-12-14 18:14:51,052 - INFO - Output directory set to: longbenchresult
2025-12-14 18:14:51,052 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 18:14:51,052 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 18:14:51,052 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:14:51,052 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.54it/s]
Device set to use cuda:0
2025-12-14 18:15:05,752 - INFO - Model pipeline loaded.
2025-12-14 18:15:05,753 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 18:15:12,339 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:15:12,339 - INFO - Dataset processed with 200 entries.
2025-12-14 18:15:12,367 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:42,  8.96s/it]Running Inference:   1%|          | 2/200 [00:12<19:47,  6.00s/it]Running Inference:   2%|▏         | 3/200 [00:20<21:58,  6.69s/it]Running Inference:   2%|▏         | 4/200 [00:21<14:56,  4.58s/it]Running Inference:   2%|▎         | 5/200 [00:24<12:28,  3.84s/it]Running Inference:   3%|▎         | 6/200 [00:31<16:40,  5.16s/it]Running Inference:   4%|▎         | 7/200 [00:39<19:22,  6.02s/it]Running Inference:   4%|▍         | 8/200 [00:43<17:10,  5.37s/it]Running Inference:   4%|▍         | 9/200 [00:51<19:51,  6.24s/it]Running Inference:   5%|▌         | 10/200 [00:59<20:56,  6.61s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:06<21:28,  6.82s/it]Running Inference:   6%|▌         | 12/200 [01:12<20:43,  6.61s/it]Running Inference:   6%|▋         | 13/200 [01:20<21:18,  6.84s/it]Running Inference:   7%|▋         | 14/200 [01:27<21:54,  7.07s/it]Running Inference:   8%|▊         | 15/200 [01:35<22:09,  7.18s/it]Running Inference:   8%|▊         | 16/200 [01:42<22:04,  7.20s/it]Running Inference:   8%|▊         | 17/200 [01:49<22:14,  7.29s/it]Running Inference:   9%|▉         | 18/200 [01:57<22:40,  7.47s/it]Running Inference:  10%|▉         | 19/200 [02:05<22:37,  7.50s/it]Running Inference:  10%|█         | 20/200 [02:13<22:47,  7.60s/it]Running Inference:  10%|█         | 21/200 [02:21<22:58,  7.70s/it]Running Inference:  11%|█         | 22/200 [02:28<22:37,  7.63s/it]Running Inference:  12%|█▏        | 23/200 [02:30<17:13,  5.84s/it]Running Inference:  12%|█▏        | 24/200 [02:37<18:32,  6.32s/it]Running Inference:  12%|█▎        | 25/200 [02:44<19:09,  6.57s/it]Running Inference:  13%|█▎        | 26/200 [02:48<16:34,  5.72s/it]Running Inference:  14%|█▎        | 27/200 [02:56<18:01,  6.25s/it]Running Inference:  14%|█▍        | 28/200 [03:03<18:54,  6.59s/it]Running Inference:  14%|█▍        | 29/200 [03:11<19:46,  6.94s/it]Running Inference:  15%|█▌        | 30/200 [03:18<19:48,  6.99s/it]Running Inference:  16%|█▌        | 31/200 [03:22<17:38,  6.26s/it]Running Inference:  16%|█▌        | 32/200 [03:29<18:13,  6.51s/it]Running Inference:  16%|█▋        | 33/200 [03:37<18:57,  6.81s/it]Running Inference:  17%|█▋        | 34/200 [03:43<18:00,  6.51s/it]Running Inference:  18%|█▊        | 35/200 [03:50<18:34,  6.76s/it]Running Inference:  18%|█▊        | 36/200 [03:58<19:25,  7.11s/it]Running Inference:  18%|█▊        | 37/200 [04:06<19:59,  7.36s/it]Running Inference:  19%|█▉        | 38/200 [04:15<20:47,  7.70s/it]Running Inference:  20%|█▉        | 39/200 [04:16<15:53,  5.92s/it]Running Inference:  20%|██        | 40/200 [04:24<17:29,  6.56s/it]Running Inference:  20%|██        | 41/200 [04:32<17:58,  6.78s/it]Running Inference:  21%|██        | 42/200 [04:39<18:16,  6.94s/it]Running Inference:  22%|██▏       | 43/200 [04:47<18:54,  7.23s/it]Running Inference:  22%|██▏       | 44/200 [04:49<14:52,  5.72s/it]Running Inference:  22%|██▎       | 45/200 [04:57<16:34,  6.42s/it]Running Inference:  23%|██▎       | 46/200 [05:05<17:17,  6.74s/it]Running Inference:  24%|██▎       | 47/200 [05:12<17:47,  6.98s/it]Running Inference:  24%|██▍       | 48/200 [05:20<18:12,  7.19s/it]Running Inference:  24%|██▍       | 49/200 [05:27<18:20,  7.29s/it]Running Inference:  25%|██▌       | 50/200 [05:35<18:27,  7.39s/it]Running Inference:  26%|██▌       | 51/200 [05:43<18:31,  7.46s/it]Running Inference:  26%|██▌       | 52/200 [05:51<18:44,  7.60s/it]Running Inference:  26%|██▋       | 53/200 [05:55<16:31,  6.74s/it]Running Inference:  27%|██▋       | 54/200 [06:03<17:18,  7.11s/it]Running Inference:  28%|██▊       | 55/200 [06:10<17:14,  7.14s/it]Running Inference:  28%|██▊       | 56/200 [06:17<17:03,  7.11s/it]Running Inference:  28%|██▊       | 57/200 [06:25<17:00,  7.13s/it]Running Inference:  29%|██▉       | 58/200 [06:29<14:52,  6.29s/it]Running Inference:  30%|██▉       | 59/200 [06:31<11:58,  5.09s/it]Running Inference:  30%|███       | 60/200 [06:39<13:46,  5.90s/it]Running Inference:  30%|███       | 61/200 [06:47<15:14,  6.58s/it]Running Inference:  31%|███       | 62/200 [06:51<13:04,  5.68s/it]Running Inference:  32%|███▏      | 63/200 [06:59<14:31,  6.36s/it]Running Inference:  32%|███▏      | 64/200 [07:06<15:16,  6.74s/it]Running Inference:  32%|███▎      | 65/200 [07:14<15:56,  7.09s/it]Running Inference:  33%|███▎      | 66/200 [07:22<16:09,  7.24s/it]Running Inference:  34%|███▎      | 67/200 [07:30<16:29,  7.44s/it]Running Inference:  34%|███▍      | 68/200 [07:37<16:16,  7.40s/it]Running Inference:  34%|███▍      | 69/200 [07:41<13:49,  6.33s/it]Running Inference:  35%|███▌      | 70/200 [07:49<14:51,  6.86s/it]Running Inference:  36%|███▌      | 71/200 [07:57<15:18,  7.12s/it]Running Inference:  36%|███▌      | 72/200 [08:04<15:08,  7.10s/it]Running Inference:  36%|███▋      | 73/200 [08:12<15:39,  7.40s/it]Running Inference:  37%|███▋      | 74/200 [08:20<15:42,  7.48s/it]Running Inference:  38%|███▊      | 75/200 [08:27<15:31,  7.45s/it]Running Inference:  38%|███▊      | 76/200 [08:35<15:38,  7.57s/it]Running Inference:  38%|███▊      | 77/200 [08:39<13:17,  6.48s/it]Running Inference:  39%|███▉      | 78/200 [08:47<14:02,  6.90s/it]Running Inference:  40%|███▉      | 79/200 [08:54<14:18,  7.09s/it]Running Inference:  40%|████      | 80/200 [09:02<14:31,  7.26s/it]Running Inference:  40%|████      | 81/200 [09:09<14:25,  7.27s/it]Running Inference:  41%|████      | 82/200 [09:17<14:26,  7.34s/it]Running Inference:  42%|████▏     | 83/200 [09:24<14:16,  7.32s/it]Running Inference:  42%|████▏     | 84/200 [09:29<13:09,  6.81s/it]Running Inference:  42%|████▎     | 85/200 [09:32<10:21,  5.40s/it]Running Inference:  43%|████▎     | 86/200 [09:35<09:20,  4.92s/it]Running Inference:  44%|████▎     | 87/200 [09:43<10:32,  5.60s/it]Running Inference:  44%|████▍     | 88/200 [09:50<11:23,  6.11s/it]Running Inference:  44%|████▍     | 89/200 [09:57<12:06,  6.55s/it]Running Inference:  45%|████▌     | 90/200 [10:05<12:21,  6.74s/it]Running Inference:  46%|████▌     | 91/200 [10:07<09:58,  5.49s/it]Running Inference:  46%|████▌     | 92/200 [10:10<08:36,  4.78s/it]Running Inference:  46%|████▋     | 93/200 [10:13<07:14,  4.06s/it]Running Inference:  47%|████▋     | 94/200 [10:21<09:12,  5.21s/it]Running Inference:  48%|████▊     | 95/200 [10:24<07:54,  4.52s/it]Running Inference:  48%|████▊     | 96/200 [10:31<09:21,  5.39s/it]Running Inference:  48%|████▊     | 97/200 [10:38<10:05,  5.88s/it]Running Inference:  49%|████▉     | 98/200 [10:41<08:30,  5.00s/it]Running Inference:  50%|████▉     | 99/200 [10:47<08:55,  5.30s/it]Running Inference:  50%|█████     | 100/200 [10:54<09:48,  5.89s/it]Running Inference:  50%|█████     | 101/200 [11:02<10:26,  6.33s/it]Running Inference:  51%|█████     | 102/200 [11:09<10:51,  6.65s/it]Running Inference:  52%|█████▏    | 103/200 [11:13<09:36,  5.94s/it]Running Inference:  52%|█████▏    | 104/200 [11:21<10:12,  6.38s/it]Running Inference:  52%|█████▎    | 105/200 [11:25<08:58,  5.67s/it]Running Inference:  53%|█████▎    | 106/200 [11:32<09:43,  6.21s/it]Running Inference:  54%|█████▎    | 107/200 [11:40<10:17,  6.64s/it]Running Inference:  54%|█████▍    | 108/200 [11:47<10:29,  6.84s/it]Running Inference:  55%|█████▍    | 109/200 [11:51<09:09,  6.04s/it]Running Inference:  55%|█████▌    | 110/200 [11:58<09:12,  6.14s/it]Running Inference:  56%|█████▌    | 111/200 [12:05<09:38,  6.50s/it]Running Inference:  56%|█████▌    | 112/200 [12:12<09:48,  6.69s/it]Running Inference:  56%|█████▋    | 113/200 [12:20<10:06,  6.97s/it]Running Inference:  57%|█████▋    | 114/200 [12:27<10:14,  7.15s/it]Running Inference:  57%|█████▊    | 115/200 [12:35<10:19,  7.29s/it]Running Inference:  58%|█████▊    | 116/200 [12:38<08:32,  6.10s/it]Running Inference:  58%|█████▊    | 117/200 [12:46<09:01,  6.52s/it]Running Inference:  59%|█████▉    | 118/200 [12:54<09:39,  7.07s/it]Running Inference:  60%|█████▉    | 119/200 [13:02<10:03,  7.45s/it]Running Inference:  60%|██████    | 120/200 [13:11<10:22,  7.79s/it]Running Inference:  60%|██████    | 121/200 [13:19<10:12,  7.76s/it]Running Inference:  61%|██████    | 122/200 [13:26<09:50,  7.57s/it]Running Inference:  62%|██████▏   | 123/200 [13:34<09:51,  7.68s/it]Running Inference:  62%|██████▏   | 124/200 [13:38<08:23,  6.63s/it]Running Inference:  62%|██████▎   | 125/200 [13:45<08:30,  6.81s/it]Running Inference:  63%|██████▎   | 126/200 [13:52<08:33,  6.93s/it]Running Inference:  64%|██████▎   | 127/200 [14:00<08:39,  7.12s/it]Running Inference:  64%|██████▍   | 128/200 [14:04<07:16,  6.06s/it]Running Inference:  64%|██████▍   | 129/200 [14:06<05:57,  5.04s/it]Running Inference:  65%|██████▌   | 130/200 [14:10<05:17,  4.53s/it]Running Inference:  66%|██████▌   | 131/200 [14:17<06:10,  5.37s/it]Running Inference:  66%|██████▌   | 132/200 [14:24<06:47,  6.00s/it]Running Inference:  66%|██████▋   | 133/200 [14:32<07:11,  6.44s/it]Running Inference:  67%|██████▋   | 134/200 [14:40<07:37,  6.93s/it]Running Inference:  68%|██████▊   | 135/200 [14:47<07:39,  7.07s/it]Running Inference:  68%|██████▊   | 136/200 [14:55<07:47,  7.31s/it]Running Inference:  68%|██████▊   | 137/200 [15:01<07:13,  6.88s/it]Running Inference:  69%|██████▉   | 138/200 [15:09<07:34,  7.33s/it]Running Inference:  70%|██████▉   | 139/200 [15:17<07:35,  7.47s/it]Running Inference:  70%|███████   | 140/200 [15:24<07:20,  7.34s/it]Running Inference:  70%|███████   | 141/200 [15:32<07:20,  7.46s/it]Running Inference:  71%|███████   | 142/200 [15:36<06:16,  6.49s/it]Running Inference:  72%|███████▏  | 143/200 [15:44<06:28,  6.82s/it]Running Inference:  72%|███████▏  | 144/200 [15:51<06:27,  6.93s/it]Running Inference:  72%|███████▎  | 145/200 [15:59<06:32,  7.14s/it]Running Inference:  73%|███████▎  | 146/200 [16:06<06:31,  7.25s/it]Running Inference:  74%|███████▎  | 147/200 [16:13<06:13,  7.05s/it]Running Inference:  74%|███████▍  | 148/200 [16:21<06:24,  7.39s/it]Running Inference:  74%|███████▍  | 149/200 [16:29<06:27,  7.60s/it]Running Inference:  75%|███████▌  | 150/200 [16:36<06:18,  7.56s/it]Running Inference:  76%|███████▌  | 151/200 [16:44<06:12,  7.59s/it]Running Inference:  76%|███████▌  | 152/200 [16:49<05:29,  6.86s/it]Running Inference:  76%|███████▋  | 153/200 [16:57<05:37,  7.17s/it]Running Inference:  77%|███████▋  | 154/200 [17:02<04:55,  6.43s/it]Running Inference:  78%|███████▊  | 155/200 [17:09<05:05,  6.80s/it]Running Inference:  78%|███████▊  | 156/200 [17:17<05:04,  6.92s/it]Running Inference:  78%|███████▊  | 157/200 [17:25<05:11,  7.24s/it]Running Inference:  79%|███████▉  | 158/200 [17:32<05:05,  7.26s/it]Running Inference:  80%|███████▉  | 159/200 [17:40<05:06,  7.47s/it]Running Inference:  80%|████████  | 160/200 [17:46<04:40,  7.01s/it]Running Inference:  80%|████████  | 161/200 [17:53<04:39,  7.16s/it]Running Inference:  81%|████████  | 162/200 [17:58<03:58,  6.29s/it]Running Inference:  82%|████████▏ | 163/200 [18:06<04:16,  6.93s/it]Running Inference:  82%|████████▏ | 164/200 [18:15<04:26,  7.40s/it]Running Inference:  82%|████████▎ | 165/200 [18:22<04:21,  7.46s/it]Running Inference:  83%|████████▎ | 166/200 [18:30<04:15,  7.51s/it]Running Inference:  84%|████████▎ | 167/200 [18:37<04:08,  7.52s/it]Running Inference:  84%|████████▍ | 168/200 [18:45<04:03,  7.61s/it]Running Inference:  84%|████████▍ | 169/200 [18:53<03:56,  7.62s/it]Running Inference:  85%|████████▌ | 170/200 [18:54<02:52,  5.76s/it]Running Inference:  86%|████████▌ | 171/200 [19:02<03:06,  6.42s/it]Running Inference:  86%|████████▌ | 172/200 [19:10<03:12,  6.88s/it]Running Inference:  86%|████████▋ | 173/200 [19:18<03:11,  7.08s/it]Running Inference:  87%|████████▋ | 174/200 [19:25<03:03,  7.06s/it]Running Inference:  88%|████████▊ | 175/200 [19:28<02:29,  5.99s/it]Running Inference:  88%|████████▊ | 176/200 [19:36<02:37,  6.56s/it]Running Inference:  88%|████████▊ | 177/200 [19:44<02:40,  6.97s/it]Running Inference:  89%|████████▉ | 178/200 [19:47<02:06,  5.77s/it]Running Inference:  90%|████████▉ | 179/200 [19:54<02:08,  6.13s/it]Running Inference:  90%|█████████ | 180/200 [20:03<02:18,  6.92s/it]Running Inference:  90%|█████████ | 181/200 [20:05<01:47,  5.67s/it]Running Inference:  91%|█████████ | 182/200 [20:13<01:53,  6.33s/it]Running Inference:  92%|█████████▏| 183/200 [20:17<01:31,  5.40s/it]Running Inference:  92%|█████████▏| 184/200 [20:25<01:39,  6.21s/it]Running Inference:  92%|█████████▎| 185/200 [20:32<01:38,  6.56s/it]Running Inference:  93%|█████████▎| 186/200 [20:35<01:18,  5.59s/it]Running Inference:  94%|█████████▎| 187/200 [20:43<01:21,  6.28s/it]Running Inference:  94%|█████████▍| 188/200 [20:51<01:19,  6.62s/it]Running Inference:  94%|█████████▍| 189/200 [20:53<00:58,  5.34s/it]Running Inference:  95%|█████████▌| 190/200 [21:00<00:59,  5.94s/it]Running Inference:  96%|█████████▌| 191/200 [21:08<00:57,  6.41s/it]Running Inference:  96%|█████████▌| 192/200 [21:15<00:54,  6.76s/it]Running Inference:  96%|█████████▋| 193/200 [21:24<00:50,  7.15s/it]Running Inference:  97%|█████████▋| 194/200 [21:31<00:44,  7.40s/it]Running Inference:  98%|█████████▊| 195/200 [21:39<00:36,  7.39s/it]Running Inference:  98%|█████████▊| 196/200 [21:47<00:30,  7.51s/it]Running Inference:  98%|█████████▊| 197/200 [21:55<00:22,  7.64s/it]Running Inference:  99%|█████████▉| 198/200 [22:02<00:14,  7.48s/it]Running Inference: 100%|█████████▉| 199/200 [22:10<00:07,  7.68s/it]Running Inference: 100%|██████████| 200/200 [22:17<00:00,  7.64s/it]Running Inference: 100%|██████████| 200/200 [22:17<00:00,  6.69s/it]
2025-12-14 18:37:30,280 - INFO - Inference completed.
2025-12-14 18:37:30,290 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 18:37:30,290 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.665 seconds.
Prefix dict has been built successfully.
2025-12-14 18:37:32,671 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 18:37:32,671 - INFO - Metrics:
12.16
2025-12-14 18:37:32,672 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=dureader, ratio=0.3) on GPU 2

----------------------------------------
Task: dureader | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:37:39,306 - INFO - Set deterministic seeds to 42
2025-12-14 18:37:39,306 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "dureader",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:37:39,306 - INFO - Starting evaluation run...
2025-12-14 18:37:39,306 - INFO - Output directory set to: longbenchresult
2025-12-14 18:37:39,306 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 18:37:39,306 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 18:37:39,307 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:37:39,307 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.59it/s]
Device set to use cuda:0
2025-12-14 18:37:50,945 - INFO - Model pipeline loaded.
2025-12-14 18:37:50,945 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: dureader)
2025-12-14 18:38:00,480 - INFO - Dataset loaded with 200 entries.
2025-12-14 18:38:00,480 - INFO - Dataset processed with 200 entries.
2025-12-14 18:38:00,507 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:08<29:40,  8.95s/it]Running Inference:   1%|          | 2/200 [00:11<17:37,  5.34s/it]Running Inference:   2%|▏         | 3/200 [00:19<20:43,  6.31s/it]Running Inference:   2%|▏         | 4/200 [00:20<14:04,  4.31s/it]Running Inference:   2%|▎         | 5/200 [00:23<12:36,  3.88s/it]Running Inference:   3%|▎         | 6/200 [00:28<14:02,  4.34s/it]Running Inference:   4%|▎         | 7/200 [00:36<17:33,  5.46s/it]Running Inference:   4%|▍         | 8/200 [00:44<19:59,  6.25s/it]Running Inference:   4%|▍         | 9/200 [00:52<21:43,  6.82s/it]Running Inference:   5%|▌         | 10/200 [01:00<22:09,  7.00s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [01:07<22:16,  7.07s/it]Running Inference:   6%|▌         | 12/200 [01:10<18:26,  5.89s/it]Running Inference:   6%|▋         | 13/200 [01:17<19:41,  6.32s/it]Running Inference:   7%|▋         | 14/200 [01:25<20:44,  6.69s/it]Running Inference:   8%|▊         | 15/200 [01:32<21:16,  6.90s/it]Running Inference:   8%|▊         | 16/200 [01:39<21:24,  6.98s/it]Running Inference:   8%|▊         | 17/200 [01:47<21:36,  7.08s/it]Running Inference:   9%|▉         | 18/200 [01:54<22:08,  7.30s/it]Running Inference:  10%|▉         | 19/200 [02:02<22:08,  7.34s/it]Running Inference:  10%|█         | 20/200 [02:10<22:24,  7.47s/it]Running Inference:  10%|█         | 21/200 [02:18<22:49,  7.65s/it]Running Inference:  11%|█         | 22/200 [02:25<22:29,  7.58s/it]Running Inference:  12%|█▏        | 23/200 [02:27<17:08,  5.81s/it]Running Inference:  12%|█▏        | 24/200 [02:34<18:30,  6.31s/it]Running Inference:  12%|█▎        | 25/200 [02:41<19:06,  6.55s/it]Running Inference:  13%|█▎        | 26/200 [02:47<18:01,  6.22s/it]Running Inference:  14%|█▎        | 27/200 [02:54<19:00,  6.59s/it]Running Inference:  14%|█▍        | 28/200 [03:02<19:32,  6.82s/it]Running Inference:  14%|█▍        | 29/200 [03:09<20:11,  7.09s/it]Running Inference:  15%|█▌        | 30/200 [03:16<20:03,  7.08s/it]Running Inference:  16%|█▌        | 31/200 [03:24<20:21,  7.23s/it]Running Inference:  16%|█▌        | 32/200 [03:31<20:04,  7.17s/it]Running Inference:  16%|█▋        | 33/200 [03:39<20:12,  7.26s/it]Running Inference:  17%|█▋        | 34/200 [03:45<19:14,  6.95s/it]Running Inference:  18%|█▊        | 35/200 [03:52<19:23,  7.05s/it]Running Inference:  18%|█▊        | 36/200 [03:57<17:15,  6.31s/it]Running Inference:  18%|█▊        | 37/200 [04:05<18:26,  6.79s/it]Running Inference:  19%|█▉        | 38/200 [04:13<19:40,  7.29s/it]Running Inference:  20%|█▉        | 39/200 [04:15<15:06,  5.63s/it]Running Inference:  20%|██        | 40/200 [04:18<12:52,  4.83s/it]Running Inference:  20%|██        | 41/200 [04:19<10:07,  3.82s/it]Running Inference:  21%|██        | 42/200 [04:26<12:46,  4.85s/it]Running Inference:  22%|██▏       | 43/200 [04:34<15:01,  5.74s/it]Running Inference:  22%|██▏       | 44/200 [04:36<11:54,  4.58s/it]Running Inference:  22%|██▎       | 45/200 [04:38<09:43,  3.76s/it]Running Inference:  23%|██▎       | 46/200 [04:45<12:28,  4.86s/it]Running Inference:  24%|██▎       | 47/200 [04:49<11:33,  4.53s/it]Running Inference:  24%|██▍       | 48/200 [04:57<13:50,  5.46s/it]Running Inference:  24%|██▍       | 49/200 [05:04<15:15,  6.07s/it]Running Inference:  25%|██▌       | 50/200 [05:12<16:16,  6.51s/it]Running Inference:  26%|██▌       | 51/200 [05:15<13:37,  5.49s/it]Running Inference:  26%|██▌       | 52/200 [05:18<11:26,  4.64s/it]Running Inference:  26%|██▋       | 53/200 [05:23<12:06,  4.94s/it]Running Inference:  27%|██▋       | 54/200 [05:31<14:11,  5.83s/it]Running Inference:  28%|██▊       | 55/200 [05:38<15:01,  6.22s/it]Running Inference:  28%|██▊       | 56/200 [05:45<15:26,  6.44s/it]Running Inference:  28%|██▊       | 57/200 [05:52<15:50,  6.65s/it]Running Inference:  29%|██▉       | 58/200 [05:56<13:31,  5.71s/it]Running Inference:  30%|██▉       | 59/200 [05:58<11:01,  4.69s/it]Running Inference:  30%|███       | 60/200 [06:06<13:05,  5.61s/it]Running Inference:  30%|███       | 61/200 [06:14<14:42,  6.35s/it]Running Inference:  31%|███       | 62/200 [06:22<15:29,  6.73s/it]Running Inference:  32%|███▏      | 63/200 [06:29<16:09,  7.08s/it]Running Inference:  32%|███▏      | 64/200 [06:37<16:21,  7.22s/it]Running Inference:  32%|███▎      | 65/200 [06:45<16:39,  7.41s/it]Running Inference:  33%|███▎      | 66/200 [06:52<16:37,  7.45s/it]Running Inference:  34%|███▎      | 67/200 [07:00<16:47,  7.57s/it]Running Inference:  34%|███▍      | 68/200 [07:08<16:28,  7.49s/it]Running Inference:  34%|███▍      | 69/200 [07:16<16:40,  7.64s/it]Running Inference:  35%|███▌      | 70/200 [07:24<16:50,  7.77s/it]Running Inference:  36%|███▌      | 71/200 [07:31<16:41,  7.77s/it]Running Inference:  36%|███▌      | 72/200 [07:36<14:36,  6.85s/it]Running Inference:  36%|███▋      | 73/200 [07:44<15:16,  7.22s/it]Running Inference:  37%|███▋      | 74/200 [07:46<12:01,  5.73s/it]Running Inference:  38%|███▊      | 75/200 [07:54<12:55,  6.20s/it]Running Inference:  38%|███▊      | 76/200 [08:02<13:46,  6.67s/it]Running Inference:  38%|███▊      | 77/200 [08:09<13:53,  6.78s/it]Running Inference:  39%|███▉      | 78/200 [08:16<14:25,  7.09s/it]Running Inference:  40%|███▉      | 79/200 [08:24<14:32,  7.21s/it]Running Inference:  40%|████      | 80/200 [08:31<14:39,  7.33s/it]Running Inference:  40%|████      | 81/200 [08:39<14:28,  7.30s/it]Running Inference:  41%|████      | 82/200 [08:42<12:17,  6.25s/it]Running Inference:  42%|████▏     | 83/200 [08:50<12:46,  6.55s/it]Running Inference:  42%|████▏     | 84/200 [08:57<13:06,  6.78s/it]Running Inference:  42%|████▎     | 85/200 [08:59<10:18,  5.38s/it]Running Inference:  43%|████▎     | 86/200 [09:06<11:17,  5.94s/it]Running Inference:  44%|████▎     | 87/200 [09:14<11:51,  6.30s/it]Running Inference:  44%|████▍     | 88/200 [09:21<12:16,  6.58s/it]Running Inference:  44%|████▍     | 89/200 [09:28<12:42,  6.87s/it]Running Inference:  45%|████▌     | 90/200 [09:35<12:43,  6.94s/it]Running Inference:  46%|████▌     | 91/200 [09:38<10:08,  5.58s/it]Running Inference:  46%|████▌     | 92/200 [09:42<09:24,  5.23s/it]Running Inference:  46%|████▋     | 93/200 [09:45<07:48,  4.38s/it]Running Inference:  47%|████▋     | 94/200 [09:53<09:35,  5.43s/it]Running Inference:  48%|████▊     | 95/200 [09:55<08:06,  4.63s/it]Running Inference:  48%|████▊     | 96/200 [10:03<09:27,  5.45s/it]Running Inference:  48%|████▊     | 97/200 [10:10<10:07,  5.90s/it]Running Inference:  49%|████▉     | 98/200 [10:13<08:42,  5.12s/it]Running Inference:  50%|████▉     | 99/200 [10:21<09:53,  5.88s/it]Running Inference:  50%|█████     | 100/200 [10:28<10:27,  6.28s/it]Running Inference:  50%|█████     | 101/200 [10:35<10:52,  6.59s/it]Running Inference:  51%|█████     | 102/200 [10:42<11:08,  6.82s/it]Running Inference:  52%|█████▏    | 103/200 [10:46<09:29,  5.87s/it]Running Inference:  52%|█████▏    | 104/200 [10:53<10:06,  6.32s/it]Running Inference:  52%|█████▎    | 105/200 [11:01<10:29,  6.63s/it]Running Inference:  53%|█████▎    | 106/200 [11:08<10:45,  6.86s/it]Running Inference:  54%|█████▎    | 107/200 [11:16<10:58,  7.08s/it]Running Inference:  54%|█████▍    | 108/200 [11:20<09:43,  6.35s/it]Running Inference:  55%|█████▍    | 109/200 [11:28<10:12,  6.73s/it]Running Inference:  55%|█████▌    | 110/200 [11:35<10:02,  6.69s/it]Running Inference:  56%|█████▌    | 111/200 [11:42<10:11,  6.87s/it]Running Inference:  56%|█████▌    | 112/200 [11:49<10:09,  6.93s/it]Running Inference:  56%|█████▋    | 113/200 [11:57<10:22,  7.15s/it]Running Inference:  57%|█████▋    | 114/200 [12:04<10:25,  7.28s/it]Running Inference:  57%|█████▊    | 115/200 [12:12<10:26,  7.37s/it]Running Inference:  58%|█████▊    | 116/200 [12:14<08:11,  5.85s/it]Running Inference:  58%|█████▊    | 117/200 [12:22<08:45,  6.33s/it]Running Inference:  59%|█████▉    | 118/200 [12:30<09:28,  6.93s/it]Running Inference:  60%|█████▉    | 119/200 [12:38<09:54,  7.33s/it]Running Inference:  60%|██████    | 120/200 [12:47<10:15,  7.69s/it]Running Inference:  60%|██████    | 121/200 [12:54<10:06,  7.68s/it]Running Inference:  61%|██████    | 122/200 [13:01<09:45,  7.50s/it]Running Inference:  62%|██████▏   | 123/200 [13:09<09:47,  7.62s/it]Running Inference:  62%|██████▏   | 124/200 [13:16<09:17,  7.33s/it]Running Inference:  62%|██████▎   | 125/200 [13:23<09:06,  7.28s/it]Running Inference:  63%|██████▎   | 126/200 [13:30<08:56,  7.25s/it]Running Inference:  64%|██████▎   | 127/200 [13:38<08:55,  7.33s/it]Running Inference:  64%|██████▍   | 128/200 [13:41<07:24,  6.17s/it]Running Inference:  64%|██████▍   | 129/200 [13:45<06:26,  5.44s/it]Running Inference:  65%|██████▌   | 130/200 [13:48<05:35,  4.80s/it]Running Inference:  66%|██████▌   | 131/200 [13:56<06:22,  5.54s/it]Running Inference:  66%|██████▌   | 132/200 [14:01<06:11,  5.46s/it]Running Inference:  66%|██████▋   | 133/200 [14:09<06:48,  6.09s/it]Running Inference:  67%|██████▋   | 134/200 [14:14<06:21,  5.78s/it]Running Inference:  68%|██████▊   | 135/200 [14:21<06:45,  6.25s/it]Running Inference:  68%|██████▊   | 136/200 [14:27<06:30,  6.09s/it]Running Inference:  68%|██████▊   | 137/200 [14:34<06:47,  6.46s/it]Running Inference:  69%|██████▉   | 138/200 [14:42<07:15,  7.03s/it]Running Inference:  70%|██████▉   | 139/200 [14:50<07:20,  7.23s/it]Running Inference:  70%|███████   | 140/200 [14:57<07:09,  7.17s/it]Running Inference:  70%|███████   | 141/200 [15:02<06:18,  6.42s/it]Running Inference:  71%|███████   | 142/200 [15:06<05:33,  5.74s/it]Running Inference:  72%|███████▏  | 143/200 [15:13<05:58,  6.29s/it]Running Inference:  72%|███████▏  | 144/200 [15:21<06:06,  6.55s/it]Running Inference:  72%|███████▎  | 145/200 [15:28<06:17,  6.86s/it]Running Inference:  73%|███████▎  | 146/200 [15:36<06:20,  7.05s/it]Running Inference:  74%|███████▎  | 147/200 [15:43<06:15,  7.08s/it]Running Inference:  74%|███████▍  | 148/200 [15:51<06:24,  7.40s/it]Running Inference:  74%|███████▍  | 149/200 [15:59<06:27,  7.60s/it]Running Inference:  75%|███████▌  | 150/200 [16:06<06:17,  7.56s/it]Running Inference:  76%|███████▌  | 151/200 [16:14<06:11,  7.59s/it]Running Inference:  76%|███████▌  | 152/200 [16:18<05:11,  6.50s/it]Running Inference:  76%|███████▋  | 153/200 [16:26<05:24,  6.91s/it]Running Inference:  77%|███████▋  | 154/200 [16:33<05:21,  6.99s/it]Running Inference:  78%|███████▊  | 155/200 [16:41<05:23,  7.19s/it]Running Inference:  78%|███████▊  | 156/200 [16:48<05:16,  7.19s/it]Running Inference:  78%|███████▊  | 157/200 [16:56<05:19,  7.42s/it]Running Inference:  79%|███████▉  | 158/200 [17:03<05:10,  7.38s/it]Running Inference:  80%|███████▉  | 159/200 [17:11<05:09,  7.56s/it]Running Inference:  80%|████████  | 160/200 [17:14<04:06,  6.17s/it]Running Inference:  80%|████████  | 161/200 [17:22<04:16,  6.57s/it]Running Inference:  81%|████████  | 162/200 [17:28<04:06,  6.48s/it]Running Inference:  82%|████████▏ | 163/200 [17:32<03:32,  5.75s/it]Running Inference:  82%|████████▏ | 164/200 [17:40<03:56,  6.57s/it]Running Inference:  82%|████████▎ | 165/200 [17:48<04:00,  6.87s/it]Running Inference:  83%|████████▎ | 166/200 [17:56<04:00,  7.09s/it]Running Inference:  84%|████████▎ | 167/200 [18:03<03:58,  7.22s/it]Running Inference:  84%|████████▍ | 168/200 [18:11<03:56,  7.40s/it]Running Inference:  84%|████████▍ | 169/200 [18:19<03:51,  7.47s/it]Running Inference:  85%|████████▌ | 170/200 [18:26<03:41,  7.39s/it]Running Inference:  86%|████████▌ | 171/200 [18:34<03:38,  7.55s/it]Running Inference:  86%|████████▌ | 172/200 [18:42<03:34,  7.66s/it]Running Inference:  86%|████████▋ | 173/200 [18:49<03:25,  7.61s/it]Running Inference:  87%|████████▋ | 174/200 [18:56<03:12,  7.42s/it]Running Inference:  88%|████████▊ | 175/200 [19:03<03:02,  7.31s/it]Running Inference:  88%|████████▊ | 176/200 [19:09<02:45,  6.90s/it]Running Inference:  88%|████████▊ | 177/200 [19:17<02:45,  7.20s/it]Running Inference:  89%|████████▉ | 178/200 [19:20<02:10,  5.92s/it]Running Inference:  90%|████████▉ | 179/200 [19:27<02:10,  6.23s/it]Running Inference:  90%|█████████ | 180/200 [19:36<02:19,  6.98s/it]Running Inference:  90%|█████████ | 181/200 [19:40<01:55,  6.06s/it]Running Inference:  91%|█████████ | 182/200 [19:47<01:58,  6.59s/it]Running Inference:  92%|█████████▏| 183/200 [19:55<01:55,  6.81s/it]Running Inference:  92%|█████████▏| 184/200 [19:58<01:33,  5.85s/it]Running Inference:  92%|█████████▎| 185/200 [20:06<01:34,  6.30s/it]Running Inference:  93%|█████████▎| 186/200 [20:10<01:18,  5.60s/it]Running Inference:  94%|█████████▎| 187/200 [20:17<01:21,  6.26s/it]Running Inference:  94%|█████████▍| 188/200 [20:23<01:14,  6.17s/it]Running Inference:  94%|█████████▍| 189/200 [20:26<00:55,  5.02s/it]Running Inference:  95%|█████████▌| 190/200 [20:33<00:57,  5.71s/it]Running Inference:  96%|█████████▌| 191/200 [20:41<00:56,  6.25s/it]Running Inference:  96%|█████████▌| 192/200 [20:48<00:52,  6.62s/it]Running Inference:  96%|█████████▋| 193/200 [20:56<00:49,  7.05s/it]Running Inference:  97%|█████████▋| 194/200 [21:04<00:43,  7.30s/it]Running Inference:  98%|█████████▊| 195/200 [21:11<00:36,  7.31s/it]Running Inference:  98%|█████████▊| 196/200 [21:19<00:29,  7.45s/it]Running Inference:  98%|█████████▊| 197/200 [21:27<00:22,  7.58s/it]Running Inference:  99%|█████████▉| 198/200 [21:34<00:14,  7.44s/it]Running Inference: 100%|█████████▉| 199/200 [21:42<00:07,  7.64s/it]Running Inference: 100%|██████████| 200/200 [21:50<00:00,  7.60s/it]Running Inference: 100%|██████████| 200/200 [21:50<00:00,  6.55s/it]
2025-12-14 18:59:50,734 - INFO - Inference completed.
2025-12-14 18:59:50,745 - INFO - Results saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 18:59:50,745 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.674 seconds.
Prefix dict has been built successfully.
2025-12-14 18:59:53,134 - INFO - Metrics saved to longbenchresult/longbench__dureader__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 18:59:53,134 - INFO - Metrics:
12.52
2025-12-14 18:59:53,135 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=dureader, ratio=0.5) on GPU 2


========================================
LongBench Task: lcc
========================================
----------------------------------------
Task: lcc | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 18:59:59,698 - INFO - Set deterministic seeds to 42
2025-12-14 18:59:59,699 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 18:59:59,699 - INFO - Starting evaluation run...
2025-12-14 18:59:59,699 - INFO - Output directory set to: longbenchresult
2025-12-14 18:59:59,699 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 18:59:59,699 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 18:59:59,699 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 18:59:59,699 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.76it/s]
Device set to use cuda:0
2025-12-14 19:00:11,927 - INFO - Model pipeline loaded.
2025-12-14 19:00:11,927 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 19:00:19,964 - INFO - Dataset loaded with 500 entries.
2025-12-14 19:00:19,964 - INFO - Dataset processed with 500 entries.
2025-12-14 19:00:19,979 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<39:44,  4.78s/it]Running Inference:   0%|          | 2/500 [00:08<34:55,  4.21s/it]Running Inference:   1%|          | 3/500 [00:12<34:58,  4.22s/it]Running Inference:   1%|          | 4/500 [00:16<33:26,  4.05s/it]Running Inference:   1%|          | 5/500 [00:20<32:37,  3.95s/it]Running Inference:   1%|          | 6/500 [00:24<32:39,  3.97s/it]Running Inference:   1%|▏         | 7/500 [00:28<32:17,  3.93s/it]Running Inference:   2%|▏         | 8/500 [00:31<31:43,  3.87s/it]Running Inference:   2%|▏         | 9/500 [00:35<31:19,  3.83s/it]Running Inference:   2%|▏         | 10/500 [00:39<31:02,  3.80s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:43<31:59,  3.93s/it]Running Inference:   2%|▏         | 12/500 [00:47<31:37,  3.89s/it]Running Inference:   3%|▎         | 13/500 [00:51<31:15,  3.85s/it]Running Inference:   3%|▎         | 14/500 [00:55<31:01,  3.83s/it]Running Inference:   3%|▎         | 15/500 [00:58<30:47,  3.81s/it]Running Inference:   3%|▎         | 16/500 [01:02<30:33,  3.79s/it]Running Inference:   3%|▎         | 17/500 [01:06<32:06,  3.99s/it]Running Inference:   4%|▎         | 18/500 [01:10<31:44,  3.95s/it]Running Inference:   4%|▍         | 19/500 [01:14<31:18,  3.90s/it]Running Inference:   4%|▍         | 20/500 [01:18<30:53,  3.86s/it]Running Inference:   4%|▍         | 21/500 [01:22<30:35,  3.83s/it]Running Inference:   4%|▍         | 22/500 [01:25<30:20,  3.81s/it]Running Inference:   5%|▍         | 23/500 [01:29<30:06,  3.79s/it]Running Inference:   5%|▍         | 24/500 [01:33<29:55,  3.77s/it]Running Inference:   5%|▌         | 25/500 [01:37<29:55,  3.78s/it]Running Inference:   5%|▌         | 26/500 [01:41<29:59,  3.80s/it]Running Inference:   5%|▌         | 27/500 [01:44<29:53,  3.79s/it]Running Inference:   6%|▌         | 28/500 [01:49<32:52,  4.18s/it]Running Inference:   6%|▌         | 29/500 [01:53<32:24,  4.13s/it]Running Inference:   6%|▌         | 30/500 [01:58<32:57,  4.21s/it]Running Inference:   6%|▌         | 31/500 [02:01<31:13,  3.99s/it]Running Inference:   6%|▋         | 32/500 [02:05<30:47,  3.95s/it]Running Inference:   7%|▋         | 33/500 [02:09<30:16,  3.89s/it]Running Inference:   7%|▋         | 34/500 [02:13<29:53,  3.85s/it]Running Inference:   7%|▋         | 35/500 [02:16<29:33,  3.81s/it]Running Inference:   7%|▋         | 36/500 [02:21<30:22,  3.93s/it]Running Inference:   7%|▋         | 37/500 [02:25<30:54,  4.01s/it]Running Inference:   8%|▊         | 38/500 [02:29<30:21,  3.94s/it]Running Inference:   8%|▊         | 39/500 [02:33<30:51,  4.02s/it]Running Inference:   8%|▊         | 40/500 [02:34<23:59,  3.13s/it]Running Inference:   8%|▊         | 41/500 [02:38<25:31,  3.34s/it]Running Inference:   8%|▊         | 42/500 [02:42<28:24,  3.72s/it]Running Inference:   9%|▊         | 43/500 [02:47<30:08,  3.96s/it]Running Inference:   9%|▉         | 44/500 [02:51<29:50,  3.93s/it]Running Inference:   9%|▉         | 45/500 [02:54<29:37,  3.91s/it]Running Inference:   9%|▉         | 46/500 [02:58<29:07,  3.85s/it]Running Inference:   9%|▉         | 47/500 [03:02<29:07,  3.86s/it]Running Inference:  10%|▉         | 48/500 [03:06<28:44,  3.81s/it]Running Inference:  10%|▉         | 49/500 [03:10<28:55,  3.85s/it]Running Inference:  10%|█         | 50/500 [03:14<30:20,  4.04s/it]Running Inference:  10%|█         | 51/500 [03:18<29:36,  3.96s/it]Running Inference:  10%|█         | 52/500 [03:22<29:10,  3.91s/it]Running Inference:  11%|█         | 53/500 [03:23<22:58,  3.08s/it]Running Inference:  11%|█         | 54/500 [03:27<24:30,  3.30s/it]Running Inference:  11%|█         | 55/500 [03:30<25:27,  3.43s/it]Running Inference:  11%|█         | 56/500 [03:34<26:09,  3.54s/it]Running Inference:  11%|█▏        | 57/500 [03:43<37:09,  5.03s/it]Running Inference:  12%|█▏        | 58/500 [03:46<34:10,  4.64s/it]Running Inference:  12%|█▏        | 59/500 [03:50<32:02,  4.36s/it]Running Inference:  12%|█▏        | 60/500 [03:54<30:38,  4.18s/it]Running Inference:  12%|█▏        | 61/500 [03:58<29:44,  4.06s/it]Running Inference:  12%|█▏        | 62/500 [04:01<28:57,  3.97s/it]Running Inference:  13%|█▎        | 63/500 [04:05<28:27,  3.91s/it]Running Inference:  13%|█▎        | 64/500 [04:09<28:01,  3.86s/it]Running Inference:  13%|█▎        | 65/500 [04:13<27:45,  3.83s/it]Running Inference:  13%|█▎        | 66/500 [04:16<26:16,  3.63s/it]Running Inference:  13%|█▎        | 67/500 [04:20<26:38,  3.69s/it]Running Inference:  14%|█▎        | 68/500 [04:20<19:47,  2.75s/it]Running Inference:  14%|█▍        | 69/500 [04:24<21:55,  3.05s/it]Running Inference:  14%|█▍        | 70/500 [04:28<23:22,  3.26s/it]Running Inference:  14%|█▍        | 71/500 [04:32<24:21,  3.41s/it]Running Inference:  14%|█▍        | 72/500 [04:36<25:34,  3.59s/it]Running Inference:  15%|█▍        | 73/500 [04:39<25:51,  3.63s/it]Running Inference:  15%|█▍        | 74/500 [04:43<26:12,  3.69s/it]Running Inference:  15%|█▌        | 75/500 [04:47<26:53,  3.80s/it]Running Inference:  15%|█▌        | 76/500 [04:51<27:05,  3.83s/it]Running Inference:  15%|█▌        | 77/500 [04:55<27:01,  3.83s/it]Running Inference:  16%|█▌        | 78/500 [04:58<26:20,  3.74s/it]Running Inference:  16%|█▌        | 79/500 [05:02<26:13,  3.74s/it]Running Inference:  16%|█▌        | 80/500 [05:06<26:17,  3.76s/it]Running Inference:  16%|█▌        | 81/500 [05:10<27:09,  3.89s/it]Running Inference:  16%|█▋        | 82/500 [05:14<27:57,  4.01s/it]Running Inference:  17%|█▋        | 83/500 [05:18<27:33,  3.96s/it]Running Inference:  17%|█▋        | 84/500 [05:22<27:12,  3.92s/it]Running Inference:  17%|█▋        | 85/500 [05:24<22:47,  3.30s/it]Running Inference:  17%|█▋        | 86/500 [05:28<23:40,  3.43s/it]Running Inference:  17%|█▋        | 87/500 [05:31<24:15,  3.53s/it]Running Inference:  18%|█▊        | 88/500 [05:35<24:43,  3.60s/it]Running Inference:  18%|█▊        | 89/500 [05:39<25:07,  3.67s/it]Running Inference:  18%|█▊        | 90/500 [05:42<22:59,  3.37s/it]Running Inference:  18%|█▊        | 91/500 [05:43<17:47,  2.61s/it]Running Inference:  18%|█▊        | 92/500 [05:46<20:18,  2.99s/it]Running Inference:  19%|█▊        | 93/500 [05:50<21:55,  3.23s/it]Running Inference:  19%|█▉        | 94/500 [05:54<22:56,  3.39s/it]Running Inference:  19%|█▉        | 95/500 [05:58<23:42,  3.51s/it]Running Inference:  19%|█▉        | 96/500 [06:02<24:20,  3.62s/it]Running Inference:  19%|█▉        | 97/500 [06:05<24:35,  3.66s/it]Running Inference:  20%|█▉        | 98/500 [06:09<24:20,  3.63s/it]Running Inference:  20%|█▉        | 99/500 [06:13<24:28,  3.66s/it]Running Inference:  20%|██        | 100/500 [06:17<25:19,  3.80s/it]Running Inference:  20%|██        | 101/500 [06:21<25:33,  3.84s/it]Running Inference:  20%|██        | 102/500 [06:22<21:05,  3.18s/it]Running Inference:  21%|██        | 103/500 [06:26<22:09,  3.35s/it]Running Inference:  21%|██        | 104/500 [06:32<26:53,  4.08s/it]Running Inference:  21%|██        | 105/500 [06:36<27:47,  4.22s/it]Running Inference:  21%|██        | 106/500 [06:42<29:51,  4.55s/it]Running Inference:  21%|██▏       | 107/500 [06:46<28:38,  4.37s/it]Running Inference:  22%|██▏       | 108/500 [06:50<27:24,  4.19s/it]Running Inference:  22%|██▏       | 109/500 [06:53<26:37,  4.09s/it]Running Inference:  22%|██▏       | 110/500 [06:57<26:08,  4.02s/it]Running Inference:  22%|██▏       | 111/500 [07:01<25:31,  3.94s/it]Running Inference:  22%|██▏       | 112/500 [07:05<25:09,  3.89s/it]Running Inference:  23%|██▎       | 113/500 [07:09<25:31,  3.96s/it]Running Inference:  23%|██▎       | 114/500 [07:14<27:41,  4.30s/it]Running Inference:  23%|██▎       | 115/500 [07:18<27:07,  4.23s/it]Running Inference:  23%|██▎       | 116/500 [07:22<26:49,  4.19s/it]Running Inference:  23%|██▎       | 117/500 [07:26<25:51,  4.05s/it]Running Inference:  24%|██▎       | 118/500 [07:30<26:25,  4.15s/it]Running Inference:  24%|██▍       | 119/500 [07:34<25:32,  4.02s/it]Running Inference:  24%|██▍       | 120/500 [07:38<25:40,  4.05s/it]Running Inference:  24%|██▍       | 121/500 [07:39<20:26,  3.23s/it]Running Inference:  24%|██▍       | 122/500 [07:43<21:15,  3.38s/it]Running Inference:  25%|██▍       | 123/500 [07:47<21:54,  3.49s/it]Running Inference:  25%|██▍       | 124/500 [07:50<21:40,  3.46s/it]Running Inference:  25%|██▌       | 125/500 [07:54<22:44,  3.64s/it]Running Inference:  25%|██▌       | 126/500 [07:58<22:52,  3.67s/it]Running Inference:  25%|██▌       | 127/500 [07:59<18:03,  2.90s/it]Running Inference:  26%|██▌       | 128/500 [08:03<20:03,  3.24s/it]Running Inference:  26%|██▌       | 129/500 [08:07<20:58,  3.39s/it]Running Inference:  26%|██▌       | 130/500 [08:11<21:49,  3.54s/it]Running Inference:  26%|██▌       | 131/500 [08:15<22:09,  3.60s/it]Running Inference:  26%|██▋       | 132/500 [08:18<22:27,  3.66s/it]Running Inference:  27%|██▋       | 133/500 [08:22<22:43,  3.72s/it]Running Inference:  27%|██▋       | 134/500 [08:26<22:48,  3.74s/it]Running Inference:  27%|██▋       | 135/500 [08:30<23:00,  3.78s/it]Running Inference:  27%|██▋       | 136/500 [08:34<23:04,  3.80s/it]Running Inference:  27%|██▋       | 137/500 [08:37<22:51,  3.78s/it]Running Inference:  28%|██▊       | 138/500 [08:41<23:00,  3.81s/it]Running Inference:  28%|██▊       | 139/500 [08:45<22:46,  3.78s/it]Running Inference:  28%|██▊       | 140/500 [08:49<22:52,  3.81s/it]Running Inference:  28%|██▊       | 141/500 [08:53<23:31,  3.93s/it]Running Inference:  28%|██▊       | 142/500 [08:57<23:36,  3.96s/it]Running Inference:  29%|██▊       | 143/500 [09:01<23:13,  3.90s/it]Running Inference:  29%|██▉       | 144/500 [09:05<22:51,  3.85s/it]Running Inference:  29%|██▉       | 145/500 [09:09<22:54,  3.87s/it]Running Inference:  29%|██▉       | 146/500 [09:13<23:02,  3.90s/it]Running Inference:  29%|██▉       | 147/500 [09:17<23:12,  3.95s/it]Running Inference:  30%|██▉       | 148/500 [09:20<22:45,  3.88s/it]Running Inference:  30%|██▉       | 149/500 [09:24<22:26,  3.84s/it]Running Inference:  30%|███       | 150/500 [09:28<22:10,  3.80s/it]Running Inference:  30%|███       | 151/500 [09:32<22:10,  3.81s/it]Running Inference:  30%|███       | 152/500 [09:35<21:58,  3.79s/it]Running Inference:  31%|███       | 153/500 [09:39<21:51,  3.78s/it]Running Inference:  31%|███       | 154/500 [09:43<21:43,  3.77s/it]Running Inference:  31%|███       | 155/500 [09:47<21:34,  3.75s/it]Running Inference:  31%|███       | 156/500 [09:50<21:32,  3.76s/it]Running Inference:  31%|███▏      | 157/500 [09:54<21:49,  3.82s/it]Running Inference:  32%|███▏      | 158/500 [09:58<21:44,  3.81s/it]Running Inference:  32%|███▏      | 159/500 [10:02<21:32,  3.79s/it]Running Inference:  32%|███▏      | 160/500 [10:06<21:22,  3.77s/it]Running Inference:  32%|███▏      | 161/500 [10:09<21:19,  3.78s/it]Running Inference:  32%|███▏      | 162/500 [10:13<21:26,  3.81s/it]Running Inference:  33%|███▎      | 163/500 [10:17<21:13,  3.78s/it]Running Inference:  33%|███▎      | 164/500 [10:21<21:08,  3.78s/it]Running Inference:  33%|███▎      | 165/500 [10:25<21:10,  3.79s/it]Running Inference:  33%|███▎      | 166/500 [10:28<20:57,  3.77s/it]Running Inference:  33%|███▎      | 167/500 [10:32<21:29,  3.87s/it]Running Inference:  34%|███▎      | 168/500 [10:36<21:15,  3.84s/it]Running Inference:  34%|███▍      | 169/500 [10:40<21:11,  3.84s/it]Running Inference:  34%|███▍      | 170/500 [10:44<20:58,  3.81s/it]Running Inference:  34%|███▍      | 171/500 [10:48<21:00,  3.83s/it]Running Inference:  34%|███▍      | 172/500 [10:51<20:47,  3.80s/it]Running Inference:  35%|███▍      | 173/500 [10:55<20:40,  3.79s/it]Running Inference:  35%|███▍      | 174/500 [10:59<21:01,  3.87s/it]Running Inference:  35%|███▌      | 175/500 [11:04<21:45,  4.02s/it]Running Inference:  35%|███▌      | 176/500 [11:07<21:17,  3.94s/it]Running Inference:  35%|███▌      | 177/500 [11:12<22:01,  4.09s/it]Running Inference:  36%|███▌      | 178/500 [11:15<21:18,  3.97s/it]Running Inference:  36%|███▌      | 179/500 [11:19<20:52,  3.90s/it]Running Inference:  36%|███▌      | 180/500 [11:23<20:42,  3.88s/it]Running Inference:  36%|███▌      | 181/500 [11:27<20:22,  3.83s/it]Running Inference:  36%|███▋      | 182/500 [11:30<20:08,  3.80s/it]Running Inference:  37%|███▋      | 183/500 [11:34<20:18,  3.84s/it]Running Inference:  37%|███▋      | 184/500 [11:38<20:16,  3.85s/it]Running Inference:  37%|███▋      | 185/500 [11:42<20:16,  3.86s/it]Running Inference:  37%|███▋      | 186/500 [11:46<20:06,  3.84s/it]Running Inference:  37%|███▋      | 187/500 [11:50<20:06,  3.86s/it]Running Inference:  38%|███▊      | 188/500 [11:54<19:54,  3.83s/it]Running Inference:  38%|███▊      | 189/500 [11:57<19:51,  3.83s/it]Running Inference:  38%|███▊      | 190/500 [12:02<20:15,  3.92s/it]Running Inference:  38%|███▊      | 191/500 [12:05<19:53,  3.86s/it]Running Inference:  38%|███▊      | 192/500 [12:09<19:48,  3.86s/it]Running Inference:  39%|███▊      | 193/500 [12:13<19:38,  3.84s/it]Running Inference:  39%|███▉      | 194/500 [12:17<19:30,  3.82s/it]Running Inference:  39%|███▉      | 195/500 [12:21<19:21,  3.81s/it]Running Inference:  39%|███▉      | 196/500 [12:24<19:14,  3.80s/it]Running Inference:  39%|███▉      | 197/500 [12:28<19:20,  3.83s/it]Running Inference:  40%|███▉      | 198/500 [12:32<19:06,  3.80s/it]Running Inference:  40%|███▉      | 199/500 [12:36<18:55,  3.77s/it]Running Inference:  40%|████      | 200/500 [12:40<19:00,  3.80s/it]Running Inference:  40%|████      | 201/500 [12:43<19:08,  3.84s/it]Running Inference:  40%|████      | 202/500 [12:47<18:55,  3.81s/it]Running Inference:  41%|████      | 203/500 [12:52<20:54,  4.22s/it]Running Inference:  41%|████      | 204/500 [12:56<20:13,  4.10s/it]Running Inference:  41%|████      | 205/500 [13:00<19:38,  3.99s/it]Running Inference:  41%|████      | 206/500 [13:04<19:10,  3.91s/it]Running Inference:  41%|████▏     | 207/500 [13:07<18:53,  3.87s/it]Running Inference:  42%|████▏     | 208/500 [13:12<19:12,  3.95s/it]Running Inference:  42%|████▏     | 209/500 [13:16<19:29,  4.02s/it]Running Inference:  42%|████▏     | 210/500 [13:19<19:01,  3.94s/it]Running Inference:  42%|████▏     | 211/500 [13:23<19:00,  3.95s/it]Running Inference:  42%|████▏     | 212/500 [13:27<18:36,  3.88s/it]Running Inference:  43%|████▎     | 213/500 [13:31<18:14,  3.82s/it]Running Inference:  43%|████▎     | 214/500 [13:35<18:07,  3.80s/it]Running Inference:  43%|████▎     | 215/500 [13:38<18:05,  3.81s/it]Running Inference:  43%|████▎     | 216/500 [13:42<17:56,  3.79s/it]Running Inference:  43%|████▎     | 217/500 [13:46<18:13,  3.86s/it]Running Inference:  44%|████▎     | 218/500 [13:50<18:18,  3.89s/it]Running Inference:  44%|████▍     | 219/500 [13:54<17:58,  3.84s/it]Running Inference:  44%|████▍     | 220/500 [13:58<17:49,  3.82s/it]Running Inference:  44%|████▍     | 221/500 [14:01<17:35,  3.78s/it]Running Inference:  44%|████▍     | 222/500 [14:05<17:25,  3.76s/it]Running Inference:  45%|████▍     | 223/500 [14:09<17:21,  3.76s/it]Running Inference:  45%|████▍     | 224/500 [14:13<17:18,  3.76s/it]Running Inference:  45%|████▌     | 225/500 [14:16<17:10,  3.75s/it]Running Inference:  45%|████▌     | 226/500 [14:20<17:03,  3.74s/it]Running Inference:  45%|████▌     | 227/500 [14:24<17:00,  3.74s/it]Running Inference:  46%|████▌     | 228/500 [14:27<16:49,  3.71s/it]Running Inference:  46%|████▌     | 229/500 [14:31<16:44,  3.70s/it]Running Inference:  46%|████▌     | 230/500 [14:35<16:42,  3.71s/it]Running Inference:  46%|████▌     | 231/500 [14:39<16:41,  3.72s/it]Running Inference:  46%|████▋     | 232/500 [14:42<16:40,  3.73s/it]Running Inference:  47%|████▋     | 233/500 [14:46<16:46,  3.77s/it]Running Inference:  47%|████▋     | 234/500 [14:50<16:45,  3.78s/it]Running Inference:  47%|████▋     | 235/500 [14:54<17:07,  3.88s/it]Running Inference:  47%|████▋     | 236/500 [14:58<16:49,  3.82s/it]Running Inference:  47%|████▋     | 237/500 [15:02<16:37,  3.79s/it]Running Inference:  48%|████▊     | 238/500 [15:05<16:29,  3.78s/it]Running Inference:  48%|████▊     | 239/500 [15:09<16:31,  3.80s/it]Running Inference:  48%|████▊     | 240/500 [15:13<16:25,  3.79s/it]Running Inference:  48%|████▊     | 241/500 [15:17<16:21,  3.79s/it]Running Inference:  48%|████▊     | 242/500 [15:20<16:10,  3.76s/it]Running Inference:  49%|████▊     | 243/500 [15:25<16:58,  3.96s/it]Running Inference:  49%|████▉     | 244/500 [15:29<17:13,  4.04s/it]Running Inference:  49%|████▉     | 245/500 [15:33<16:56,  3.98s/it]Running Inference:  49%|████▉     | 246/500 [15:37<16:29,  3.90s/it]Running Inference:  49%|████▉     | 247/500 [15:40<16:22,  3.88s/it]Running Inference:  50%|████▉     | 248/500 [15:44<16:09,  3.85s/it]Running Inference:  50%|████▉     | 249/500 [15:48<16:02,  3.83s/it]Running Inference:  50%|█████     | 250/500 [15:52<15:58,  3.83s/it]Running Inference:  50%|█████     | 251/500 [15:56<16:12,  3.90s/it]Running Inference:  50%|█████     | 252/500 [16:00<16:51,  4.08s/it]Running Inference:  51%|█████     | 253/500 [16:04<16:22,  3.98s/it]Running Inference:  51%|█████     | 254/500 [16:08<16:03,  3.92s/it]Running Inference:  51%|█████     | 255/500 [16:12<15:59,  3.92s/it]Running Inference:  51%|█████     | 256/500 [16:16<15:43,  3.87s/it]Running Inference:  51%|█████▏    | 257/500 [16:19<15:46,  3.89s/it]Running Inference:  52%|█████▏    | 258/500 [16:23<15:32,  3.85s/it]Running Inference:  52%|█████▏    | 259/500 [16:27<15:24,  3.84s/it]Running Inference:  52%|█████▏    | 260/500 [16:31<15:12,  3.80s/it]Running Inference:  52%|█████▏    | 261/500 [16:34<15:02,  3.78s/it]Running Inference:  52%|█████▏    | 262/500 [16:38<15:07,  3.81s/it]Running Inference:  53%|█████▎    | 263/500 [16:42<15:01,  3.80s/it]Running Inference:  53%|█████▎    | 264/500 [16:46<14:51,  3.78s/it]Running Inference:  53%|█████▎    | 265/500 [16:50<14:57,  3.82s/it]Running Inference:  53%|█████▎    | 266/500 [16:54<14:50,  3.81s/it]Running Inference:  53%|█████▎    | 267/500 [16:58<15:17,  3.94s/it]Running Inference:  54%|█████▎    | 268/500 [17:02<15:04,  3.90s/it]Running Inference:  54%|█████▍    | 269/500 [17:05<14:57,  3.88s/it]Running Inference:  54%|█████▍    | 270/500 [17:09<14:46,  3.86s/it]Running Inference:  54%|█████▍    | 271/500 [17:13<14:31,  3.81s/it]Running Inference:  54%|█████▍    | 272/500 [17:17<14:25,  3.80s/it]Running Inference:  55%|█████▍    | 273/500 [17:21<14:25,  3.81s/it]Running Inference:  55%|█████▍    | 274/500 [17:24<14:15,  3.78s/it]Running Inference:  55%|█████▌    | 275/500 [17:28<14:08,  3.77s/it]Running Inference:  55%|█████▌    | 276/500 [17:32<14:06,  3.78s/it]Running Inference:  55%|█████▌    | 277/500 [17:36<14:01,  3.77s/it]Running Inference:  56%|█████▌    | 278/500 [17:39<14:01,  3.79s/it]Running Inference:  56%|█████▌    | 279/500 [17:43<14:08,  3.84s/it]Running Inference:  56%|█████▌    | 280/500 [17:47<14:16,  3.89s/it]Running Inference:  56%|█████▌    | 281/500 [17:51<14:09,  3.88s/it]Running Inference:  56%|█████▋    | 282/500 [17:55<14:00,  3.86s/it]Running Inference:  57%|█████▋    | 283/500 [17:59<13:50,  3.83s/it]Running Inference:  57%|█████▋    | 284/500 [18:03<13:49,  3.84s/it]Running Inference:  57%|█████▋    | 285/500 [18:07<13:46,  3.84s/it]Running Inference:  57%|█████▋    | 286/500 [18:10<13:34,  3.80s/it]Running Inference:  57%|█████▋    | 287/500 [18:15<14:18,  4.03s/it]Running Inference:  58%|█████▊    | 288/500 [18:19<14:11,  4.02s/it]Running Inference:  58%|█████▊    | 289/500 [18:23<13:51,  3.94s/it]Running Inference:  58%|█████▊    | 290/500 [18:26<13:44,  3.93s/it]Running Inference:  58%|█████▊    | 291/500 [18:30<13:41,  3.93s/it]Running Inference:  58%|█████▊    | 292/500 [18:34<13:23,  3.86s/it]Running Inference:  59%|█████▊    | 293/500 [18:38<13:20,  3.87s/it]Running Inference:  59%|█████▉    | 294/500 [18:42<13:07,  3.82s/it]Running Inference:  59%|█████▉    | 295/500 [18:45<12:59,  3.80s/it]Running Inference:  59%|█████▉    | 296/500 [18:49<12:54,  3.80s/it]Running Inference:  59%|█████▉    | 297/500 [18:53<12:53,  3.81s/it]Running Inference:  60%|█████▉    | 298/500 [18:57<12:44,  3.79s/it]Running Inference:  60%|█████▉    | 299/500 [19:01<12:36,  3.76s/it]Running Inference:  60%|██████    | 300/500 [19:04<12:27,  3.74s/it]Running Inference:  60%|██████    | 301/500 [19:08<12:40,  3.82s/it]Running Inference:  60%|██████    | 302/500 [19:12<12:33,  3.81s/it]Running Inference:  61%|██████    | 303/500 [19:16<12:27,  3.79s/it]Running Inference:  61%|██████    | 304/500 [19:20<12:53,  3.95s/it]Running Inference:  61%|██████    | 305/500 [19:24<12:35,  3.87s/it]Running Inference:  61%|██████    | 306/500 [19:28<12:50,  3.97s/it]Running Inference:  61%|██████▏   | 307/500 [19:32<12:36,  3.92s/it]Running Inference:  62%|██████▏   | 308/500 [19:35<12:20,  3.86s/it]Running Inference:  62%|██████▏   | 309/500 [19:39<12:08,  3.81s/it]Running Inference:  62%|██████▏   | 310/500 [19:43<12:01,  3.80s/it]Running Inference:  62%|██████▏   | 311/500 [19:47<12:02,  3.83s/it]Running Inference:  62%|██████▏   | 312/500 [19:51<11:55,  3.81s/it]Running Inference:  63%|██████▎   | 313/500 [19:54<11:56,  3.83s/it]Running Inference:  63%|██████▎   | 314/500 [19:58<11:46,  3.80s/it]Running Inference:  63%|██████▎   | 315/500 [20:02<11:36,  3.76s/it]Running Inference:  63%|██████▎   | 316/500 [20:06<11:28,  3.74s/it]Running Inference:  63%|██████▎   | 317/500 [20:09<11:23,  3.74s/it]Running Inference:  64%|██████▎   | 318/500 [20:13<11:19,  3.74s/it]Running Inference:  64%|██████▍   | 319/500 [20:17<11:19,  3.75s/it]Running Inference:  64%|██████▍   | 320/500 [20:21<11:44,  3.92s/it]Running Inference:  64%|██████▍   | 321/500 [20:25<11:48,  3.96s/it]Running Inference:  64%|██████▍   | 322/500 [20:28<10:47,  3.64s/it]Running Inference:  65%|██████▍   | 323/500 [20:32<10:49,  3.67s/it]Running Inference:  65%|██████▍   | 324/500 [20:36<10:51,  3.70s/it]Running Inference:  65%|██████▌   | 325/500 [20:39<10:51,  3.72s/it]Running Inference:  65%|██████▌   | 326/500 [20:44<11:11,  3.86s/it]Running Inference:  65%|██████▌   | 327/500 [20:49<12:20,  4.28s/it]Running Inference:  66%|██████▌   | 328/500 [20:53<12:13,  4.27s/it]Running Inference:  66%|██████▌   | 329/500 [20:57<11:40,  4.10s/it]Running Inference:  66%|██████▌   | 330/500 [21:00<11:16,  3.98s/it]Running Inference:  66%|██████▌   | 331/500 [21:05<11:36,  4.12s/it]Running Inference:  66%|██████▋   | 332/500 [21:09<11:25,  4.08s/it]Running Inference:  67%|██████▋   | 333/500 [21:13<11:04,  3.98s/it]Running Inference:  67%|██████▋   | 334/500 [21:16<10:51,  3.92s/it]Running Inference:  67%|██████▋   | 335/500 [21:20<10:41,  3.89s/it]Running Inference:  67%|██████▋   | 336/500 [21:24<10:31,  3.85s/it]Running Inference:  67%|██████▋   | 337/500 [21:28<10:24,  3.83s/it]Running Inference:  68%|██████▊   | 338/500 [21:32<10:19,  3.82s/it]Running Inference:  68%|██████▊   | 339/500 [21:36<10:32,  3.93s/it]Running Inference:  68%|██████▊   | 340/500 [21:42<12:00,  4.50s/it]Running Inference:  68%|██████▊   | 341/500 [21:45<11:21,  4.29s/it]Running Inference:  68%|██████▊   | 342/500 [21:49<10:55,  4.15s/it]Running Inference:  69%|██████▊   | 343/500 [21:53<10:49,  4.14s/it]Running Inference:  69%|██████▉   | 344/500 [21:56<09:26,  3.63s/it]Running Inference:  69%|██████▉   | 345/500 [22:00<09:33,  3.70s/it]Running Inference:  69%|██████▉   | 346/500 [22:04<09:42,  3.78s/it]Running Inference:  69%|██████▉   | 347/500 [22:07<09:41,  3.80s/it]Running Inference:  70%|██████▉   | 348/500 [22:11<09:40,  3.82s/it]Running Inference:  70%|██████▉   | 349/500 [22:15<09:35,  3.81s/it]Running Inference:  70%|███████   | 350/500 [22:21<11:09,  4.47s/it]Running Inference:  70%|███████   | 351/500 [22:25<10:55,  4.40s/it]Running Inference:  70%|███████   | 352/500 [22:29<10:20,  4.19s/it]Running Inference:  71%|███████   | 353/500 [22:33<09:55,  4.05s/it]Running Inference:  71%|███████   | 354/500 [22:37<09:44,  4.01s/it]Running Inference:  71%|███████   | 355/500 [22:43<11:35,  4.79s/it]Running Inference:  71%|███████   | 356/500 [22:47<10:46,  4.49s/it]Running Inference:  71%|███████▏  | 357/500 [22:51<10:27,  4.39s/it]Running Inference:  72%|███████▏  | 358/500 [22:55<09:56,  4.20s/it]Running Inference:  72%|███████▏  | 359/500 [22:59<09:31,  4.05s/it]Running Inference:  72%|███████▏  | 360/500 [23:02<09:14,  3.96s/it]Running Inference:  72%|███████▏  | 361/500 [23:06<09:05,  3.93s/it]Running Inference:  72%|███████▏  | 362/500 [23:10<09:03,  3.94s/it]Running Inference:  73%|███████▎  | 363/500 [23:13<08:31,  3.73s/it]Running Inference:  73%|███████▎  | 364/500 [23:17<08:35,  3.79s/it]Running Inference:  73%|███████▎  | 365/500 [23:19<07:07,  3.17s/it]Running Inference:  73%|███████▎  | 366/500 [23:23<07:45,  3.48s/it]Running Inference:  73%|███████▎  | 367/500 [23:27<07:56,  3.59s/it]Running Inference:  74%|███████▎  | 368/500 [23:31<08:07,  3.69s/it]Running Inference:  74%|███████▍  | 369/500 [23:35<08:20,  3.82s/it]Running Inference:  74%|███████▍  | 370/500 [23:40<08:36,  3.97s/it]Running Inference:  74%|███████▍  | 371/500 [23:43<08:24,  3.91s/it]Running Inference:  74%|███████▍  | 372/500 [23:47<08:21,  3.92s/it]Running Inference:  75%|███████▍  | 373/500 [23:51<08:12,  3.88s/it]Running Inference:  75%|███████▍  | 374/500 [23:55<08:08,  3.88s/it]Running Inference:  75%|███████▌  | 375/500 [23:59<07:58,  3.83s/it]Running Inference:  75%|███████▌  | 376/500 [24:02<07:55,  3.83s/it]Running Inference:  75%|███████▌  | 377/500 [24:06<07:49,  3.81s/it]Running Inference:  76%|███████▌  | 378/500 [24:10<07:46,  3.82s/it]Running Inference:  76%|███████▌  | 379/500 [24:14<07:48,  3.87s/it]Running Inference:  76%|███████▌  | 380/500 [24:18<07:40,  3.84s/it]Running Inference:  76%|███████▌  | 381/500 [24:22<07:34,  3.82s/it]Running Inference:  76%|███████▋  | 382/500 [24:26<07:42,  3.92s/it]Running Inference:  77%|███████▋  | 383/500 [24:30<07:39,  3.92s/it]Running Inference:  77%|███████▋  | 384/500 [24:33<07:29,  3.87s/it]Running Inference:  77%|███████▋  | 385/500 [24:34<05:43,  2.99s/it]Running Inference:  77%|███████▋  | 386/500 [24:38<06:13,  3.28s/it]Running Inference:  77%|███████▋  | 387/500 [24:42<06:30,  3.46s/it]Running Inference:  78%|███████▊  | 388/500 [24:46<06:37,  3.55s/it]Running Inference:  78%|███████▊  | 389/500 [24:50<06:44,  3.64s/it]Running Inference:  78%|███████▊  | 390/500 [24:54<06:45,  3.68s/it]Running Inference:  78%|███████▊  | 391/500 [24:57<06:44,  3.71s/it]Running Inference:  78%|███████▊  | 392/500 [25:01<06:42,  3.72s/it]Running Inference:  79%|███████▊  | 393/500 [25:05<06:37,  3.71s/it]Running Inference:  79%|███████▉  | 394/500 [25:09<06:37,  3.75s/it]Running Inference:  79%|███████▉  | 395/500 [25:12<06:36,  3.77s/it]Running Inference:  79%|███████▉  | 396/500 [25:16<06:29,  3.75s/it]Running Inference:  79%|███████▉  | 397/500 [25:20<06:35,  3.84s/it]Running Inference:  80%|███████▉  | 398/500 [25:24<06:27,  3.80s/it]Running Inference:  80%|███████▉  | 399/500 [25:28<06:21,  3.77s/it]Running Inference:  80%|████████  | 400/500 [25:31<06:15,  3.75s/it]Running Inference:  80%|████████  | 401/500 [25:35<06:10,  3.74s/it]Running Inference:  80%|████████  | 402/500 [25:39<06:05,  3.73s/it]Running Inference:  81%|████████  | 403/500 [25:42<06:01,  3.73s/it]Running Inference:  81%|████████  | 404/500 [25:46<05:57,  3.72s/it]Running Inference:  81%|████████  | 405/500 [25:50<05:55,  3.75s/it]Running Inference:  81%|████████  | 406/500 [25:54<05:52,  3.75s/it]Running Inference:  81%|████████▏ | 407/500 [25:57<05:48,  3.74s/it]Running Inference:  82%|████████▏ | 408/500 [26:01<05:44,  3.75s/it]Running Inference:  82%|████████▏ | 409/500 [26:05<05:41,  3.75s/it]Running Inference:  82%|████████▏ | 410/500 [26:09<05:38,  3.77s/it]Running Inference:  82%|████████▏ | 411/500 [26:13<05:37,  3.79s/it]Running Inference:  82%|████████▏ | 412/500 [26:16<05:32,  3.78s/it]Running Inference:  83%|████████▎ | 413/500 [26:20<05:27,  3.76s/it]Running Inference:  83%|████████▎ | 414/500 [26:24<05:23,  3.77s/it]Running Inference:  83%|████████▎ | 415/500 [26:28<05:19,  3.76s/it]Running Inference:  83%|████████▎ | 416/500 [26:31<05:13,  3.73s/it]Running Inference:  83%|████████▎ | 417/500 [26:35<05:09,  3.72s/it]Running Inference:  84%|████████▎ | 418/500 [26:39<05:07,  3.75s/it]Running Inference:  84%|████████▍ | 419/500 [26:43<05:02,  3.74s/it]Running Inference:  84%|████████▍ | 420/500 [26:46<04:58,  3.73s/it]Running Inference:  84%|████████▍ | 421/500 [26:50<04:54,  3.73s/it]Running Inference:  84%|████████▍ | 422/500 [26:54<04:50,  3.72s/it]Running Inference:  85%|████████▍ | 423/500 [26:57<04:45,  3.71s/it]Running Inference:  85%|████████▍ | 424/500 [27:01<04:42,  3.72s/it]Running Inference:  85%|████████▌ | 425/500 [27:05<04:46,  3.82s/it]Running Inference:  85%|████████▌ | 426/500 [27:09<04:40,  3.79s/it]Running Inference:  85%|████████▌ | 427/500 [27:13<04:33,  3.75s/it]Running Inference:  86%|████████▌ | 428/500 [27:16<04:29,  3.74s/it]Running Inference:  86%|████████▌ | 429/500 [27:20<04:24,  3.73s/it]Running Inference:  86%|████████▌ | 430/500 [27:24<04:29,  3.85s/it]Running Inference:  86%|████████▌ | 431/500 [27:28<04:24,  3.84s/it]Running Inference:  86%|████████▋ | 432/500 [27:32<04:24,  3.90s/it]Running Inference:  87%|████████▋ | 433/500 [27:36<04:23,  3.93s/it]Running Inference:  87%|████████▋ | 434/500 [27:40<04:19,  3.93s/it]Running Inference:  87%|████████▋ | 435/500 [27:44<04:12,  3.88s/it]Running Inference:  87%|████████▋ | 436/500 [27:47<04:07,  3.86s/it]Running Inference:  87%|████████▋ | 437/500 [27:51<04:01,  3.84s/it]Running Inference:  88%|████████▊ | 438/500 [27:55<03:55,  3.80s/it]Running Inference:  88%|████████▊ | 439/500 [27:59<03:52,  3.80s/it]Running Inference:  88%|████████▊ | 440/500 [28:03<03:47,  3.80s/it]Running Inference:  88%|████████▊ | 441/500 [28:06<03:42,  3.76s/it]Running Inference:  88%|████████▊ | 442/500 [28:10<03:36,  3.74s/it]Running Inference:  89%|████████▊ | 443/500 [28:14<03:32,  3.74s/it]Running Inference:  89%|████████▉ | 444/500 [28:17<03:31,  3.77s/it]Running Inference:  89%|████████▉ | 445/500 [28:21<03:29,  3.80s/it]Running Inference:  89%|████████▉ | 446/500 [28:25<03:26,  3.83s/it]Running Inference:  89%|████████▉ | 447/500 [28:29<03:21,  3.80s/it]Running Inference:  90%|████████▉ | 448/500 [28:33<03:17,  3.81s/it]Running Inference:  90%|████████▉ | 449/500 [28:37<03:13,  3.79s/it]Running Inference:  90%|█████████ | 450/500 [28:39<02:53,  3.48s/it]Running Inference:  90%|█████████ | 451/500 [28:43<02:54,  3.55s/it]Running Inference:  90%|█████████ | 452/500 [28:47<02:53,  3.62s/it]Running Inference:  91%|█████████ | 453/500 [28:51<02:53,  3.70s/it]Running Inference:  91%|█████████ | 454/500 [28:54<02:50,  3.71s/it]Running Inference:  91%|█████████ | 455/500 [28:58<02:46,  3.70s/it]Running Inference:  91%|█████████ | 456/500 [29:02<02:43,  3.71s/it]Running Inference:  91%|█████████▏| 457/500 [29:06<02:40,  3.73s/it]Running Inference:  92%|█████████▏| 458/500 [29:09<02:36,  3.74s/it]Running Inference:  92%|█████████▏| 459/500 [29:13<02:37,  3.83s/it]Running Inference:  92%|█████████▏| 460/500 [29:17<02:36,  3.90s/it]Running Inference:  92%|█████████▏| 461/500 [29:21<02:31,  3.89s/it]Running Inference:  92%|█████████▏| 462/500 [29:26<02:30,  3.97s/it]Running Inference:  93%|█████████▎| 463/500 [29:29<02:24,  3.89s/it]Running Inference:  93%|█████████▎| 464/500 [29:33<02:17,  3.83s/it]Running Inference:  93%|█████████▎| 465/500 [29:37<02:12,  3.79s/it]Running Inference:  93%|█████████▎| 466/500 [29:40<02:08,  3.79s/it]Running Inference:  93%|█████████▎| 467/500 [29:44<02:04,  3.76s/it]Running Inference:  94%|█████████▎| 468/500 [29:48<02:02,  3.82s/it]Running Inference:  94%|█████████▍| 469/500 [29:52<01:57,  3.79s/it]Running Inference:  94%|█████████▍| 470/500 [29:55<01:53,  3.77s/it]Running Inference:  94%|█████████▍| 471/500 [29:59<01:50,  3.80s/it]Running Inference:  94%|█████████▍| 472/500 [30:03<01:45,  3.78s/it]Running Inference:  95%|█████████▍| 473/500 [30:07<01:44,  3.86s/it]Running Inference:  95%|█████████▍| 474/500 [30:11<01:39,  3.83s/it]Running Inference:  95%|█████████▌| 475/500 [30:15<01:35,  3.81s/it]Running Inference:  95%|█████████▌| 476/500 [30:18<01:31,  3.81s/it]Running Inference:  95%|█████████▌| 477/500 [30:22<01:27,  3.80s/it]Running Inference:  96%|█████████▌| 478/500 [30:26<01:24,  3.85s/it]Running Inference:  96%|█████████▌| 479/500 [30:31<01:26,  4.14s/it]Running Inference:  96%|█████████▌| 480/500 [30:35<01:21,  4.06s/it]Running Inference:  96%|█████████▌| 481/500 [30:39<01:19,  4.20s/it]Running Inference:  96%|█████████▋| 482/500 [30:43<01:12,  4.05s/it]Running Inference:  97%|█████████▋| 483/500 [30:47<01:07,  3.96s/it]Running Inference:  97%|█████████▋| 484/500 [30:51<01:03,  3.97s/it]Running Inference:  97%|█████████▋| 485/500 [30:56<01:04,  4.33s/it]Running Inference:  97%|█████████▋| 486/500 [31:00<00:58,  4.15s/it]Running Inference:  97%|█████████▋| 487/500 [31:04<00:52,  4.03s/it]Running Inference:  98%|█████████▊| 488/500 [31:07<00:47,  3.92s/it]Running Inference:  98%|█████████▊| 489/500 [31:11<00:42,  3.90s/it]Running Inference:  98%|█████████▊| 490/500 [31:15<00:38,  3.89s/it]Running Inference:  98%|█████████▊| 491/500 [31:19<00:34,  3.85s/it]Running Inference:  98%|█████████▊| 492/500 [31:23<00:30,  3.86s/it]Running Inference:  99%|█████████▊| 493/500 [31:26<00:26,  3.81s/it]Running Inference:  99%|█████████▉| 494/500 [31:30<00:22,  3.83s/it]Running Inference:  99%|█████████▉| 495/500 [31:34<00:19,  3.82s/it]Running Inference:  99%|█████████▉| 496/500 [31:38<00:15,  3.83s/it]Running Inference:  99%|█████████▉| 497/500 [31:41<00:11,  3.80s/it]Running Inference: 100%|█████████▉| 498/500 [31:45<00:07,  3.81s/it]Running Inference: 100%|█████████▉| 499/500 [31:49<00:03,  3.79s/it]Running Inference: 100%|██████████| 500/500 [31:53<00:00,  3.87s/it]Running Inference: 100%|██████████| 500/500 [31:53<00:00,  3.83s/it]
2025-12-14 19:32:13,603 - INFO - Inference completed.
2025-12-14 19:32:13,618 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 19:32:13,618 - INFO - Calculating metrics for dataset: longbench
2025-12-14 19:32:13,620 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 19:32:13,620 - INFO - Metrics:
42.16
2025-12-14 19:32:13,622 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lcc, ratio=0.1) on GPU 2

----------------------------------------
Task: lcc | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 19:32:20,033 - INFO - Set deterministic seeds to 42
2025-12-14 19:32:20,033 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 19:32:20,033 - INFO - Starting evaluation run...
2025-12-14 19:32:20,033 - INFO - Output directory set to: longbenchresult
2025-12-14 19:32:20,033 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 19:32:20,033 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 19:32:20,033 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 19:32:20,033 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.33it/s]
Device set to use cuda:0
2025-12-14 19:32:37,274 - INFO - Model pipeline loaded.
2025-12-14 19:32:37,274 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 19:32:45,518 - INFO - Dataset loaded with 500 entries.
2025-12-14 19:32:45,519 - INFO - Dataset processed with 500 entries.
2025-12-14 19:32:45,533 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<41:13,  4.96s/it]Running Inference:   0%|          | 2/500 [00:08<36:03,  4.34s/it]Running Inference:   1%|          | 3/500 [00:13<36:00,  4.35s/it]Running Inference:   1%|          | 4/500 [00:17<34:25,  4.16s/it]Running Inference:   1%|          | 5/500 [00:21<33:33,  4.07s/it]Running Inference:   1%|          | 6/500 [00:25<33:32,  4.07s/it]Running Inference:   1%|▏         | 7/500 [00:29<33:09,  4.03s/it]Running Inference:   2%|▏         | 8/500 [00:32<32:36,  3.98s/it]Running Inference:   2%|▏         | 9/500 [00:36<32:12,  3.94s/it]Running Inference:   2%|▏         | 10/500 [00:40<31:54,  3.91s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:44<32:50,  4.03s/it]Running Inference:   2%|▏         | 12/500 [00:48<32:27,  3.99s/it]Running Inference:   3%|▎         | 13/500 [00:52<32:05,  3.95s/it]Running Inference:   3%|▎         | 14/500 [00:56<31:50,  3.93s/it]Running Inference:   3%|▎         | 15/500 [01:00<31:36,  3.91s/it]Running Inference:   3%|▎         | 16/500 [01:04<31:23,  3.89s/it]Running Inference:   3%|▎         | 17/500 [01:08<32:57,  4.09s/it]Running Inference:   4%|▎         | 18/500 [01:12<32:34,  4.06s/it]Running Inference:   4%|▍         | 19/500 [01:16<32:07,  4.01s/it]Running Inference:   4%|▍         | 20/500 [01:20<31:41,  3.96s/it]Running Inference:   4%|▍         | 21/500 [01:24<31:25,  3.94s/it]Running Inference:   4%|▍         | 22/500 [01:28<31:10,  3.91s/it]Running Inference:   5%|▍         | 23/500 [01:32<30:57,  3.89s/it]Running Inference:   5%|▍         | 24/500 [01:35<30:47,  3.88s/it]Running Inference:   5%|▌         | 25/500 [01:39<30:44,  3.88s/it]Running Inference:   5%|▌         | 26/500 [01:43<30:48,  3.90s/it]Running Inference:   5%|▌         | 27/500 [01:47<30:42,  3.90s/it]Running Inference:   6%|▌         | 28/500 [01:52<33:42,  4.28s/it]Running Inference:   6%|▌         | 29/500 [01:56<33:14,  4.23s/it]Running Inference:   6%|▌         | 30/500 [02:01<33:47,  4.31s/it]Running Inference:   6%|▌         | 31/500 [02:05<32:00,  4.10s/it]Running Inference:   6%|▋         | 32/500 [02:09<31:35,  4.05s/it]Running Inference:   7%|▋         | 33/500 [02:12<31:04,  3.99s/it]Running Inference:   7%|▋         | 34/500 [02:16<30:42,  3.95s/it]Running Inference:   7%|▋         | 35/500 [02:20<30:22,  3.92s/it]Running Inference:   7%|▋         | 36/500 [02:24<31:10,  4.03s/it]Running Inference:   7%|▋         | 37/500 [02:26<25:47,  3.34s/it]Running Inference:   8%|▊         | 38/500 [02:30<27:02,  3.51s/it]Running Inference:   8%|▊         | 39/500 [02:34<28:46,  3.75s/it]Running Inference:   8%|▊         | 40/500 [02:35<22:35,  2.95s/it]Running Inference:   8%|▊         | 41/500 [02:39<24:47,  3.24s/it]Running Inference:   8%|▊         | 42/500 [02:44<28:07,  3.68s/it]Running Inference:   9%|▊         | 43/500 [02:49<30:11,  3.96s/it]Running Inference:   9%|▉         | 44/500 [02:53<30:07,  3.96s/it]Running Inference:   9%|▉         | 45/500 [02:57<30:06,  3.97s/it]Running Inference:   9%|▉         | 46/500 [03:00<29:44,  3.93s/it]Running Inference:   9%|▉         | 47/500 [03:04<29:49,  3.95s/it]Running Inference:  10%|▉         | 48/500 [03:08<29:30,  3.92s/it]Running Inference:  10%|▉         | 49/500 [03:12<29:43,  3.95s/it]Running Inference:  10%|█         | 50/500 [03:17<31:08,  4.15s/it]Running Inference:  10%|█         | 51/500 [03:21<30:25,  4.07s/it]Running Inference:  10%|█         | 52/500 [03:25<29:59,  4.02s/it]Running Inference:  11%|█         | 53/500 [03:26<23:35,  3.17s/it]Running Inference:  11%|█         | 54/500 [03:30<25:10,  3.39s/it]Running Inference:  11%|█         | 55/500 [03:34<26:19,  3.55s/it]Running Inference:  11%|█         | 56/500 [03:38<27:00,  3.65s/it]Running Inference:  11%|█▏        | 57/500 [03:46<37:44,  5.11s/it]Running Inference:  12%|█▏        | 58/500 [03:50<34:48,  4.72s/it]Running Inference:  12%|█▏        | 59/500 [03:54<32:41,  4.45s/it]Running Inference:  12%|█▏        | 60/500 [03:57<29:35,  4.04s/it]Running Inference:  12%|█▏        | 61/500 [04:01<29:14,  4.00s/it]Running Inference:  12%|█▏        | 62/500 [04:04<26:56,  3.69s/it]Running Inference:  13%|█▎        | 63/500 [04:08<27:15,  3.74s/it]Running Inference:  13%|█▎        | 64/500 [04:11<27:24,  3.77s/it]Running Inference:  13%|█▎        | 65/500 [04:15<27:35,  3.81s/it]Running Inference:  13%|█▎        | 66/500 [04:18<26:13,  3.63s/it]Running Inference:  13%|█▎        | 67/500 [04:22<26:48,  3.71s/it]Running Inference:  14%|█▎        | 68/500 [04:23<20:51,  2.90s/it]Running Inference:  14%|█▍        | 69/500 [04:27<22:53,  3.19s/it]Running Inference:  14%|█▍        | 70/500 [04:31<24:15,  3.39s/it]Running Inference:  14%|█▍        | 71/500 [04:35<25:12,  3.52s/it]Running Inference:  14%|█▍        | 72/500 [04:39<26:23,  3.70s/it]Running Inference:  15%|█▍        | 73/500 [04:43<26:38,  3.74s/it]Running Inference:  15%|█▍        | 74/500 [04:47<26:57,  3.80s/it]Running Inference:  15%|█▌        | 75/500 [04:51<27:36,  3.90s/it]Running Inference:  15%|█▌        | 76/500 [04:55<27:48,  3.94s/it]Running Inference:  15%|█▌        | 77/500 [04:58<26:48,  3.80s/it]Running Inference:  16%|█▌        | 78/500 [05:02<26:20,  3.75s/it]Running Inference:  16%|█▌        | 79/500 [05:06<26:26,  3.77s/it]Running Inference:  16%|█▌        | 80/500 [05:10<26:39,  3.81s/it]Running Inference:  16%|█▌        | 81/500 [05:14<27:36,  3.95s/it]Running Inference:  16%|█▋        | 82/500 [05:19<28:27,  4.09s/it]Running Inference:  17%|█▋        | 83/500 [05:22<28:07,  4.05s/it]Running Inference:  17%|█▋        | 84/500 [05:26<27:49,  4.01s/it]Running Inference:  17%|█▋        | 85/500 [05:28<22:48,  3.30s/it]Running Inference:  17%|█▋        | 86/500 [05:32<23:53,  3.46s/it]Running Inference:  17%|█▋        | 87/500 [05:36<24:38,  3.58s/it]Running Inference:  18%|█▊        | 88/500 [05:40<25:11,  3.67s/it]Running Inference:  18%|█▊        | 89/500 [05:43<25:07,  3.67s/it]Running Inference:  18%|█▊        | 90/500 [05:46<23:17,  3.41s/it]Running Inference:  18%|█▊        | 91/500 [05:47<17:56,  2.63s/it]Running Inference:  18%|█▊        | 92/500 [05:51<20:36,  3.03s/it]Running Inference:  19%|█▊        | 93/500 [05:55<22:19,  3.29s/it]Running Inference:  19%|█▉        | 94/500 [05:59<23:25,  3.46s/it]Running Inference:  19%|█▉        | 95/500 [06:02<24:13,  3.59s/it]Running Inference:  19%|█▉        | 96/500 [06:06<24:53,  3.70s/it]Running Inference:  19%|█▉        | 97/500 [06:10<25:11,  3.75s/it]Running Inference:  20%|█▉        | 98/500 [06:14<25:18,  3.78s/it]Running Inference:  20%|█▉        | 99/500 [06:18<25:22,  3.80s/it]Running Inference:  20%|██        | 100/500 [06:22<26:08,  3.92s/it]Running Inference:  20%|██        | 101/500 [06:23<20:01,  3.01s/it]Running Inference:  20%|██        | 102/500 [06:25<17:29,  2.64s/it]Running Inference:  21%|██        | 103/500 [06:29<19:50,  3.00s/it]Running Inference:  21%|██        | 104/500 [06:35<25:23,  3.85s/it]Running Inference:  21%|██        | 105/500 [06:39<26:55,  4.09s/it]Running Inference:  21%|██        | 106/500 [06:45<29:26,  4.48s/it]Running Inference:  21%|██▏       | 107/500 [06:49<28:32,  4.36s/it]Running Inference:  22%|██▏       | 108/500 [06:53<27:32,  4.22s/it]Running Inference:  22%|██▏       | 109/500 [06:56<26:55,  4.13s/it]Running Inference:  22%|██▏       | 110/500 [07:00<26:32,  4.08s/it]Running Inference:  22%|██▏       | 111/500 [07:04<25:59,  4.01s/it]Running Inference:  22%|██▏       | 112/500 [07:08<25:39,  3.97s/it]Running Inference:  23%|██▎       | 113/500 [07:12<26:05,  4.04s/it]Running Inference:  23%|██▎       | 114/500 [07:18<28:16,  4.40s/it]Running Inference:  23%|██▎       | 115/500 [07:22<27:43,  4.32s/it]Running Inference:  23%|██▎       | 116/500 [07:26<27:26,  4.29s/it]Running Inference:  23%|██▎       | 117/500 [07:30<26:28,  4.15s/it]Running Inference:  24%|██▎       | 118/500 [07:34<27:03,  4.25s/it]Running Inference:  24%|██▍       | 119/500 [07:38<26:09,  4.12s/it]Running Inference:  24%|██▍       | 120/500 [07:42<26:17,  4.15s/it]Running Inference:  24%|██▍       | 121/500 [07:46<25:51,  4.09s/it]Running Inference:  24%|██▍       | 122/500 [07:50<25:14,  4.01s/it]Running Inference:  25%|██▍       | 123/500 [07:54<24:51,  3.96s/it]Running Inference:  25%|██▍       | 124/500 [07:58<24:34,  3.92s/it]Running Inference:  25%|██▌       | 125/500 [08:02<24:57,  3.99s/it]Running Inference:  25%|██▌       | 126/500 [08:06<24:37,  3.95s/it]Running Inference:  25%|██▌       | 127/500 [08:07<19:19,  3.11s/it]Running Inference:  26%|██▌       | 128/500 [08:11<21:01,  3.39s/it]Running Inference:  26%|██▌       | 129/500 [08:15<21:49,  3.53s/it]Running Inference:  26%|██▌       | 130/500 [08:19<22:34,  3.66s/it]Running Inference:  26%|██▌       | 131/500 [08:23<22:53,  3.72s/it]Running Inference:  26%|██▋       | 132/500 [08:26<23:05,  3.76s/it]Running Inference:  27%|██▋       | 133/500 [08:31<23:34,  3.85s/it]Running Inference:  27%|██▋       | 134/500 [08:34<23:35,  3.87s/it]Running Inference:  27%|██▋       | 135/500 [08:38<23:45,  3.90s/it]Running Inference:  27%|██▋       | 136/500 [08:42<23:47,  3.92s/it]Running Inference:  27%|██▋       | 137/500 [08:46<23:36,  3.90s/it]Running Inference:  28%|██▊       | 138/500 [08:50<23:45,  3.94s/it]Running Inference:  28%|██▊       | 139/500 [08:54<23:28,  3.90s/it]Running Inference:  28%|██▊       | 140/500 [08:58<23:32,  3.92s/it]Running Inference:  28%|██▊       | 141/500 [09:02<24:16,  4.06s/it]Running Inference:  28%|██▊       | 142/500 [09:07<24:20,  4.08s/it]Running Inference:  29%|██▊       | 143/500 [09:10<23:55,  4.02s/it]Running Inference:  29%|██▉       | 144/500 [09:14<23:43,  4.00s/it]Running Inference:  29%|██▉       | 145/500 [09:18<23:42,  4.01s/it]Running Inference:  29%|██▉       | 146/500 [09:23<23:46,  4.03s/it]Running Inference:  29%|██▉       | 147/500 [09:27<23:55,  4.07s/it]Running Inference:  30%|██▉       | 148/500 [09:30<23:24,  3.99s/it]Running Inference:  30%|██▉       | 149/500 [09:34<23:05,  3.95s/it]Running Inference:  30%|███       | 150/500 [09:38<22:47,  3.91s/it]Running Inference:  30%|███       | 151/500 [09:42<22:46,  3.92s/it]Running Inference:  30%|███       | 152/500 [09:46<22:35,  3.89s/it]Running Inference:  31%|███       | 153/500 [09:50<22:26,  3.88s/it]Running Inference:  31%|███       | 154/500 [09:54<22:19,  3.87s/it]Running Inference:  31%|███       | 155/500 [09:57<22:09,  3.85s/it]Running Inference:  31%|███       | 156/500 [10:01<22:07,  3.86s/it]Running Inference:  31%|███▏      | 157/500 [10:05<22:22,  3.92s/it]Running Inference:  32%|███▏      | 158/500 [10:09<22:16,  3.91s/it]Running Inference:  32%|███▏      | 159/500 [10:13<22:06,  3.89s/it]Running Inference:  32%|███▏      | 160/500 [10:17<21:58,  3.88s/it]Running Inference:  32%|███▏      | 161/500 [10:21<21:56,  3.88s/it]Running Inference:  32%|███▏      | 162/500 [10:25<22:04,  3.92s/it]Running Inference:  33%|███▎      | 163/500 [10:29<21:51,  3.89s/it]Running Inference:  33%|███▎      | 164/500 [10:33<21:47,  3.89s/it]Running Inference:  33%|███▎      | 165/500 [10:36<21:47,  3.90s/it]Running Inference:  33%|███▎      | 166/500 [10:40<21:35,  3.88s/it]Running Inference:  33%|███▎      | 167/500 [10:45<22:06,  3.98s/it]Running Inference:  34%|███▎      | 168/500 [10:48<21:52,  3.95s/it]Running Inference:  34%|███▍      | 169/500 [10:52<21:47,  3.95s/it]Running Inference:  34%|███▍      | 170/500 [10:56<21:34,  3.92s/it]Running Inference:  34%|███▍      | 171/500 [11:00<21:36,  3.94s/it]Running Inference:  34%|███▍      | 172/500 [11:04<21:23,  3.91s/it]Running Inference:  35%|███▍      | 173/500 [11:08<21:16,  3.90s/it]Running Inference:  35%|███▍      | 174/500 [11:12<21:36,  3.98s/it]Running Inference:  35%|███▌      | 175/500 [11:17<22:19,  4.12s/it]Running Inference:  35%|███▌      | 176/500 [11:20<21:51,  4.05s/it]Running Inference:  35%|███▌      | 177/500 [11:25<22:35,  4.20s/it]Running Inference:  36%|███▌      | 178/500 [11:29<21:52,  4.08s/it]Running Inference:  36%|███▌      | 179/500 [11:33<21:26,  4.01s/it]Running Inference:  36%|███▌      | 180/500 [11:37<21:16,  3.99s/it]Running Inference:  36%|███▌      | 181/500 [11:40<20:56,  3.94s/it]Running Inference:  36%|███▋      | 182/500 [11:44<20:41,  3.91s/it]Running Inference:  37%|███▋      | 183/500 [11:48<20:51,  3.95s/it]Running Inference:  37%|███▋      | 184/500 [11:52<20:48,  3.95s/it]Running Inference:  37%|███▋      | 185/500 [11:56<20:48,  3.96s/it]Running Inference:  37%|███▋      | 186/500 [12:00<20:37,  3.94s/it]Running Inference:  37%|███▋      | 187/500 [12:04<20:37,  3.95s/it]Running Inference:  38%|███▊      | 188/500 [12:08<20:24,  3.93s/it]Running Inference:  38%|███▊      | 189/500 [12:11<18:36,  3.59s/it]Running Inference:  38%|███▊      | 190/500 [12:15<19:32,  3.78s/it]Running Inference:  38%|███▊      | 191/500 [12:19<19:32,  3.79s/it]Running Inference:  38%|███▊      | 192/500 [12:23<19:41,  3.84s/it]Running Inference:  39%|███▊      | 193/500 [12:27<19:43,  3.86s/it]Running Inference:  39%|███▉      | 194/500 [12:31<19:42,  3.86s/it]Running Inference:  39%|███▉      | 195/500 [12:34<19:40,  3.87s/it]Running Inference:  39%|███▉      | 196/500 [12:38<19:36,  3.87s/it]Running Inference:  39%|███▉      | 197/500 [12:42<19:43,  3.91s/it]Running Inference:  40%|███▉      | 198/500 [12:46<19:31,  3.88s/it]Running Inference:  40%|███▉      | 199/500 [12:50<19:21,  3.86s/it]Running Inference:  40%|████      | 200/500 [12:54<19:27,  3.89s/it]Running Inference:  40%|████      | 201/500 [12:58<19:35,  3.93s/it]Running Inference:  40%|████      | 202/500 [13:02<19:24,  3.91s/it]Running Inference:  41%|████      | 203/500 [13:07<21:23,  4.32s/it]Running Inference:  41%|████      | 204/500 [13:11<20:41,  4.20s/it]Running Inference:  41%|████      | 205/500 [13:15<20:07,  4.09s/it]Running Inference:  41%|████      | 206/500 [13:16<16:20,  3.33s/it]Running Inference:  41%|████▏     | 207/500 [13:20<17:04,  3.50s/it]Running Inference:  42%|████▏     | 208/500 [13:24<18:04,  3.72s/it]Running Inference:  42%|████▏     | 209/500 [13:29<18:50,  3.88s/it]Running Inference:  42%|████▏     | 210/500 [13:33<18:43,  3.87s/it]Running Inference:  42%|████▏     | 211/500 [13:37<18:56,  3.93s/it]Running Inference:  42%|████▏     | 212/500 [13:40<18:42,  3.90s/it]Running Inference:  43%|████▎     | 213/500 [13:42<14:51,  3.11s/it]Running Inference:  43%|████▎     | 214/500 [13:46<15:54,  3.34s/it]Running Inference:  43%|████▎     | 215/500 [13:49<16:41,  3.51s/it]Running Inference:  43%|████▎     | 216/500 [13:53<17:06,  3.61s/it]Running Inference:  43%|████▎     | 217/500 [13:57<17:46,  3.77s/it]Running Inference:  44%|████▎     | 218/500 [14:02<18:07,  3.86s/it]Running Inference:  44%|████▍     | 219/500 [14:05<18:00,  3.84s/it]Running Inference:  44%|████▍     | 220/500 [14:09<17:58,  3.85s/it]Running Inference:  44%|████▍     | 221/500 [14:13<17:50,  3.84s/it]Running Inference:  44%|████▍     | 222/500 [14:17<17:45,  3.83s/it]Running Inference:  45%|████▍     | 223/500 [14:21<17:43,  3.84s/it]Running Inference:  45%|████▍     | 224/500 [14:25<17:42,  3.85s/it]Running Inference:  45%|████▌     | 225/500 [14:28<17:35,  3.84s/it]Running Inference:  45%|████▌     | 226/500 [14:32<17:29,  3.83s/it]Running Inference:  45%|████▌     | 227/500 [14:36<17:26,  3.83s/it]Running Inference:  46%|████▌     | 228/500 [14:40<17:16,  3.81s/it]Running Inference:  46%|████▌     | 229/500 [14:44<17:10,  3.80s/it]Running Inference:  46%|████▌     | 230/500 [14:47<17:09,  3.81s/it]Running Inference:  46%|████▌     | 231/500 [14:51<17:07,  3.82s/it]Running Inference:  46%|████▋     | 232/500 [14:55<17:06,  3.83s/it]Running Inference:  47%|████▋     | 233/500 [14:59<17:12,  3.87s/it]Running Inference:  47%|████▋     | 234/500 [15:03<17:11,  3.88s/it]Running Inference:  47%|████▋     | 235/500 [15:07<17:32,  3.97s/it]Running Inference:  47%|████▋     | 236/500 [15:11<17:14,  3.92s/it]Running Inference:  47%|████▋     | 237/500 [15:15<17:02,  3.89s/it]Running Inference:  48%|████▊     | 238/500 [15:19<16:55,  3.88s/it]Running Inference:  48%|████▊     | 239/500 [15:23<16:57,  3.90s/it]Running Inference:  48%|████▊     | 240/500 [15:26<16:51,  3.89s/it]Running Inference:  48%|████▊     | 241/500 [15:30<16:46,  3.89s/it]Running Inference:  48%|████▊     | 242/500 [15:33<15:18,  3.56s/it]Running Inference:  49%|████▊     | 243/500 [15:38<16:28,  3.85s/it]Running Inference:  49%|████▉     | 244/500 [15:42<17:00,  3.99s/it]Running Inference:  49%|████▉     | 245/500 [15:43<13:45,  3.24s/it]Running Inference:  49%|████▉     | 246/500 [15:47<14:23,  3.40s/it]Running Inference:  49%|████▉     | 247/500 [15:51<15:02,  3.57s/it]Running Inference:  50%|████▉     | 248/500 [15:55<15:21,  3.66s/it]Running Inference:  50%|████▉     | 249/500 [15:59<15:35,  3.73s/it]Running Inference:  50%|█████     | 250/500 [16:03<15:47,  3.79s/it]Running Inference:  50%|█████     | 251/500 [16:07<16:11,  3.90s/it]Running Inference:  50%|█████     | 252/500 [16:12<16:57,  4.10s/it]Running Inference:  51%|█████     | 253/500 [16:15<16:34,  4.03s/it]Running Inference:  51%|█████     | 254/500 [16:19<16:19,  3.98s/it]Running Inference:  51%|█████     | 255/500 [16:23<16:18,  4.00s/it]Running Inference:  51%|█████     | 256/500 [16:27<16:04,  3.95s/it]Running Inference:  51%|█████▏    | 257/500 [16:31<16:08,  3.98s/it]Running Inference:  52%|█████▏    | 258/500 [16:35<15:55,  3.95s/it]Running Inference:  52%|█████▏    | 259/500 [16:39<15:47,  3.93s/it]Running Inference:  52%|█████▏    | 260/500 [16:43<15:36,  3.90s/it]Running Inference:  52%|█████▏    | 261/500 [16:47<15:25,  3.87s/it]Running Inference:  52%|█████▏    | 262/500 [16:51<15:26,  3.89s/it]Running Inference:  53%|█████▎    | 263/500 [16:54<15:21,  3.89s/it]Running Inference:  53%|█████▎    | 264/500 [16:58<15:12,  3.87s/it]Running Inference:  53%|█████▎    | 265/500 [17:02<15:19,  3.91s/it]Running Inference:  53%|█████▎    | 266/500 [17:06<15:11,  3.90s/it]Running Inference:  53%|█████▎    | 267/500 [17:11<15:38,  4.03s/it]Running Inference:  54%|█████▎    | 268/500 [17:14<15:23,  3.98s/it]Running Inference:  54%|█████▍    | 269/500 [17:18<15:13,  3.96s/it]Running Inference:  54%|█████▍    | 270/500 [17:22<15:02,  3.92s/it]Running Inference:  54%|█████▍    | 271/500 [17:26<14:49,  3.88s/it]Running Inference:  54%|█████▍    | 272/500 [17:30<14:45,  3.88s/it]Running Inference:  55%|█████▍    | 273/500 [17:34<14:46,  3.91s/it]Running Inference:  55%|█████▍    | 274/500 [17:38<14:37,  3.88s/it]Running Inference:  55%|█████▌    | 275/500 [17:41<14:31,  3.87s/it]Running Inference:  55%|█████▌    | 276/500 [17:45<14:29,  3.88s/it]Running Inference:  55%|█████▌    | 277/500 [17:49<14:24,  3.88s/it]Running Inference:  56%|█████▌    | 278/500 [17:53<14:24,  3.89s/it]Running Inference:  56%|█████▌    | 279/500 [17:57<14:30,  3.94s/it]Running Inference:  56%|█████▌    | 280/500 [18:01<14:39,  4.00s/it]Running Inference:  56%|█████▌    | 281/500 [18:05<14:31,  3.98s/it]Running Inference:  56%|█████▋    | 282/500 [18:09<14:22,  3.96s/it]Running Inference:  57%|█████▋    | 283/500 [18:13<14:12,  3.93s/it]Running Inference:  57%|█████▋    | 284/500 [18:17<14:12,  3.95s/it]Running Inference:  57%|█████▋    | 285/500 [18:21<14:08,  3.95s/it]Running Inference:  57%|█████▋    | 286/500 [18:25<13:56,  3.91s/it]Running Inference:  57%|█████▋    | 287/500 [18:29<14:40,  4.14s/it]Running Inference:  58%|█████▊    | 288/500 [18:34<14:34,  4.12s/it]Running Inference:  58%|█████▊    | 289/500 [18:37<14:13,  4.04s/it]Running Inference:  58%|█████▊    | 290/500 [18:41<14:06,  4.03s/it]Running Inference:  58%|█████▊    | 291/500 [18:45<14:03,  4.04s/it]Running Inference:  58%|█████▊    | 292/500 [18:49<13:44,  3.97s/it]Running Inference:  59%|█████▊    | 293/500 [18:53<13:41,  3.97s/it]Running Inference:  59%|█████▉    | 294/500 [18:55<10:55,  3.18s/it]Running Inference:  59%|█████▉    | 295/500 [18:58<11:33,  3.38s/it]Running Inference:  59%|█████▉    | 296/500 [19:02<12:00,  3.53s/it]Running Inference:  59%|█████▉    | 297/500 [19:06<12:22,  3.66s/it]Running Inference:  60%|█████▉    | 298/500 [19:10<12:28,  3.71s/it]Running Inference:  60%|█████▉    | 299/500 [19:14<12:31,  3.74s/it]Running Inference:  60%|██████    | 300/500 [19:18<12:29,  3.75s/it]Running Inference:  60%|██████    | 301/500 [19:22<12:48,  3.86s/it]Running Inference:  60%|██████    | 302/500 [19:26<12:45,  3.86s/it]Running Inference:  61%|██████    | 303/500 [19:30<12:41,  3.87s/it]Running Inference:  61%|██████    | 304/500 [19:34<13:08,  4.02s/it]Running Inference:  61%|██████    | 305/500 [19:38<12:51,  3.96s/it]Running Inference:  61%|██████    | 306/500 [19:42<13:07,  4.06s/it]Running Inference:  61%|██████▏   | 307/500 [19:46<12:54,  4.01s/it]Running Inference:  62%|██████▏   | 308/500 [19:50<12:38,  3.95s/it]Running Inference:  62%|██████▏   | 309/500 [19:54<12:26,  3.91s/it]Running Inference:  62%|██████▏   | 310/500 [19:57<12:20,  3.90s/it]Running Inference:  62%|██████▏   | 311/500 [20:01<12:20,  3.92s/it]Running Inference:  62%|██████▏   | 312/500 [20:05<12:13,  3.90s/it]Running Inference:  63%|██████▎   | 313/500 [20:09<12:13,  3.92s/it]Running Inference:  63%|██████▎   | 314/500 [20:13<12:03,  3.89s/it]Running Inference:  63%|██████▎   | 315/500 [20:17<11:53,  3.86s/it]Running Inference:  63%|██████▎   | 316/500 [20:21<11:45,  3.84s/it]Running Inference:  63%|██████▎   | 317/500 [20:24<11:40,  3.83s/it]Running Inference:  64%|██████▎   | 318/500 [20:28<11:37,  3.83s/it]Running Inference:  64%|██████▍   | 319/500 [20:32<11:37,  3.85s/it]Running Inference:  64%|██████▍   | 320/500 [20:37<12:03,  4.02s/it]Running Inference:  64%|██████▍   | 321/500 [20:41<12:07,  4.06s/it]Running Inference:  64%|██████▍   | 322/500 [20:43<10:53,  3.67s/it]Running Inference:  65%|██████▍   | 323/500 [20:47<10:58,  3.72s/it]Running Inference:  65%|██████▍   | 324/500 [20:51<11:03,  3.77s/it]Running Inference:  65%|██████▌   | 325/500 [20:55<11:05,  3.81s/it]Running Inference:  65%|██████▌   | 326/500 [20:59<11:26,  3.95s/it]Running Inference:  65%|██████▌   | 327/500 [21:05<12:36,  4.38s/it]Running Inference:  66%|██████▌   | 328/500 [21:09<12:30,  4.36s/it]Running Inference:  66%|██████▌   | 329/500 [21:13<11:48,  4.15s/it]Running Inference:  66%|██████▌   | 330/500 [21:17<11:28,  4.05s/it]Running Inference:  66%|██████▌   | 331/500 [21:21<11:49,  4.20s/it]Running Inference:  66%|██████▋   | 332/500 [21:25<11:40,  4.17s/it]Running Inference:  67%|██████▋   | 333/500 [21:29<11:21,  4.08s/it]Running Inference:  67%|██████▋   | 334/500 [21:33<11:07,  4.02s/it]Running Inference:  67%|██████▋   | 335/500 [21:37<10:55,  3.97s/it]Running Inference:  67%|██████▋   | 336/500 [21:41<10:44,  3.93s/it]Running Inference:  67%|██████▋   | 337/500 [21:44<10:36,  3.91s/it]Running Inference:  68%|██████▊   | 338/500 [21:48<10:31,  3.90s/it]Running Inference:  68%|██████▊   | 339/500 [21:53<10:45,  4.01s/it]Running Inference:  68%|██████▊   | 340/500 [21:58<12:11,  4.57s/it]Running Inference:  68%|██████▊   | 341/500 [22:02<11:33,  4.36s/it]Running Inference:  68%|██████▊   | 342/500 [22:06<11:07,  4.22s/it]Running Inference:  69%|██████▊   | 343/500 [22:10<11:01,  4.22s/it]Running Inference:  69%|██████▉   | 344/500 [22:15<10:54,  4.20s/it]Running Inference:  69%|██████▉   | 345/500 [22:19<10:39,  4.13s/it]Running Inference:  69%|██████▉   | 346/500 [22:23<10:32,  4.11s/it]Running Inference:  69%|██████▉   | 347/500 [22:27<10:21,  4.06s/it]Running Inference:  70%|██████▉   | 348/500 [22:31<10:12,  4.03s/it]Running Inference:  70%|██████▉   | 349/500 [22:34<10:02,  3.99s/it]Running Inference:  70%|███████   | 350/500 [22:40<11:30,  4.60s/it]Running Inference:  70%|███████   | 351/500 [22:45<11:14,  4.53s/it]Running Inference:  70%|███████   | 352/500 [22:49<10:34,  4.28s/it]Running Inference:  71%|███████   | 353/500 [22:52<10:09,  4.15s/it]Running Inference:  71%|███████   | 354/500 [22:56<09:58,  4.10s/it]Running Inference:  71%|███████   | 355/500 [23:03<11:46,  4.87s/it]Running Inference:  71%|███████   | 356/500 [23:07<10:58,  4.57s/it]Running Inference:  71%|███████▏  | 357/500 [23:11<10:40,  4.48s/it]Running Inference:  72%|███████▏  | 358/500 [23:15<10:09,  4.29s/it]Running Inference:  72%|███████▏  | 359/500 [23:19<09:45,  4.15s/it]Running Inference:  72%|███████▏  | 360/500 [23:23<09:28,  4.06s/it]Running Inference:  72%|███████▏  | 361/500 [23:27<09:20,  4.03s/it]Running Inference:  72%|███████▏  | 362/500 [23:31<09:17,  4.04s/it]Running Inference:  73%|███████▎  | 363/500 [23:34<08:44,  3.83s/it]Running Inference:  73%|███████▎  | 364/500 [23:38<08:49,  3.89s/it]Running Inference:  73%|███████▎  | 365/500 [23:43<09:31,  4.23s/it]Running Inference:  73%|███████▎  | 366/500 [23:47<09:29,  4.25s/it]Running Inference:  73%|███████▎  | 367/500 [23:51<09:13,  4.16s/it]Running Inference:  74%|███████▎  | 368/500 [23:55<09:04,  4.13s/it]Running Inference:  74%|███████▍  | 369/500 [24:00<09:04,  4.16s/it]Running Inference:  74%|███████▍  | 370/500 [24:04<09:10,  4.24s/it]Running Inference:  74%|███████▍  | 371/500 [24:08<08:52,  4.13s/it]Running Inference:  74%|███████▍  | 372/500 [24:12<08:44,  4.10s/it]Running Inference:  75%|███████▍  | 373/500 [24:16<08:32,  4.04s/it]Running Inference:  75%|███████▍  | 374/500 [24:20<08:26,  4.02s/it]Running Inference:  75%|███████▌  | 375/500 [24:24<08:14,  3.96s/it]Running Inference:  75%|███████▌  | 376/500 [24:28<08:10,  3.95s/it]Running Inference:  75%|███████▌  | 377/500 [24:31<08:03,  3.93s/it]Running Inference:  76%|███████▌  | 378/500 [24:35<07:59,  3.93s/it]Running Inference:  76%|███████▌  | 379/500 [24:40<08:01,  3.98s/it]Running Inference:  76%|███████▌  | 380/500 [24:43<07:53,  3.95s/it]Running Inference:  76%|███████▌  | 381/500 [24:47<07:47,  3.93s/it]Running Inference:  76%|███████▋  | 382/500 [24:52<07:54,  4.02s/it]Running Inference:  77%|███████▋  | 383/500 [24:56<07:51,  4.03s/it]Running Inference:  77%|███████▋  | 384/500 [24:59<07:41,  3.98s/it]Running Inference:  77%|███████▋  | 385/500 [25:04<07:42,  4.02s/it]Running Inference:  77%|███████▋  | 386/500 [25:08<07:39,  4.03s/it]Running Inference:  77%|███████▋  | 387/500 [25:12<07:34,  4.02s/it]Running Inference:  78%|███████▊  | 388/500 [25:15<07:25,  3.97s/it]Running Inference:  78%|███████▊  | 389/500 [25:19<07:20,  3.97s/it]Running Inference:  78%|███████▊  | 390/500 [25:23<07:13,  3.94s/it]Running Inference:  78%|███████▊  | 391/500 [25:27<07:07,  3.92s/it]Running Inference:  78%|███████▊  | 392/500 [25:31<07:01,  3.90s/it]Running Inference:  79%|███████▊  | 393/500 [25:35<06:53,  3.87s/it]Running Inference:  79%|███████▉  | 394/500 [25:39<06:51,  3.89s/it]Running Inference:  79%|███████▉  | 395/500 [25:43<06:49,  3.90s/it]Running Inference:  79%|███████▉  | 396/500 [25:46<06:42,  3.87s/it]Running Inference:  79%|███████▉  | 397/500 [25:51<06:47,  3.96s/it]Running Inference:  80%|███████▉  | 398/500 [25:54<06:39,  3.91s/it]Running Inference:  80%|███████▉  | 399/500 [25:58<06:32,  3.88s/it]Running Inference:  80%|████████  | 400/500 [26:02<06:26,  3.86s/it]Running Inference:  80%|████████  | 401/500 [26:06<06:20,  3.85s/it]Running Inference:  80%|████████  | 402/500 [26:10<06:15,  3.83s/it]Running Inference:  81%|████████  | 403/500 [26:14<06:12,  3.84s/it]Running Inference:  81%|████████  | 404/500 [26:17<06:07,  3.83s/it]Running Inference:  81%|████████  | 405/500 [26:21<06:05,  3.85s/it]Running Inference:  81%|████████  | 406/500 [26:25<06:02,  3.85s/it]Running Inference:  81%|████████▏ | 407/500 [26:29<05:58,  3.85s/it]Running Inference:  82%|████████▏ | 408/500 [26:33<05:54,  3.85s/it]Running Inference:  82%|████████▏ | 409/500 [26:37<05:50,  3.86s/it]Running Inference:  82%|████████▏ | 410/500 [26:41<05:48,  3.87s/it]Running Inference:  82%|████████▏ | 411/500 [26:45<05:46,  3.89s/it]Running Inference:  82%|████████▏ | 412/500 [26:48<05:42,  3.89s/it]Running Inference:  83%|████████▎ | 413/500 [26:52<05:36,  3.87s/it]Running Inference:  83%|████████▎ | 414/500 [26:56<05:32,  3.87s/it]Running Inference:  83%|████████▎ | 415/500 [27:00<05:28,  3.86s/it]Running Inference:  83%|████████▎ | 416/500 [27:04<05:22,  3.84s/it]Running Inference:  83%|████████▎ | 417/500 [27:08<05:17,  3.83s/it]Running Inference:  84%|████████▎ | 418/500 [27:11<05:15,  3.85s/it]Running Inference:  84%|████████▍ | 419/500 [27:15<05:11,  3.84s/it]Running Inference:  84%|████████▍ | 420/500 [27:19<05:07,  3.84s/it]Running Inference:  84%|████████▍ | 421/500 [27:23<05:03,  3.84s/it]Running Inference:  84%|████████▍ | 422/500 [27:27<04:59,  3.84s/it]Running Inference:  85%|████████▍ | 423/500 [27:31<04:54,  3.83s/it]Running Inference:  85%|████████▍ | 424/500 [27:34<04:51,  3.83s/it]Running Inference:  85%|████████▌ | 425/500 [27:39<04:54,  3.93s/it]Running Inference:  85%|████████▌ | 426/500 [27:42<04:48,  3.90s/it]Running Inference:  85%|████████▌ | 427/500 [27:46<04:41,  3.86s/it]Running Inference:  86%|████████▌ | 428/500 [27:50<04:36,  3.85s/it]Running Inference:  86%|████████▌ | 429/500 [27:54<04:32,  3.84s/it]Running Inference:  86%|████████▌ | 430/500 [27:58<04:37,  3.96s/it]Running Inference:  86%|████████▌ | 431/500 [28:02<04:31,  3.94s/it]Running Inference:  86%|████████▋ | 432/500 [28:06<04:32,  4.00s/it]Running Inference:  87%|████████▋ | 433/500 [28:10<04:30,  4.03s/it]Running Inference:  87%|████████▋ | 434/500 [28:14<04:26,  4.04s/it]Running Inference:  87%|████████▋ | 435/500 [28:18<04:18,  3.98s/it]Running Inference:  87%|████████▋ | 436/500 [28:22<04:13,  3.97s/it]Running Inference:  87%|████████▋ | 437/500 [28:26<04:08,  3.94s/it]Running Inference:  88%|████████▊ | 438/500 [28:30<04:02,  3.91s/it]Running Inference:  88%|████████▊ | 439/500 [28:34<03:58,  3.91s/it]Running Inference:  88%|████████▊ | 440/500 [28:38<03:54,  3.91s/it]Running Inference:  88%|████████▊ | 441/500 [28:41<03:48,  3.87s/it]Running Inference:  88%|████████▊ | 442/500 [28:45<03:43,  3.85s/it]Running Inference:  89%|████████▊ | 443/500 [28:49<03:39,  3.85s/it]Running Inference:  89%|████████▉ | 444/500 [28:53<03:37,  3.88s/it]Running Inference:  89%|████████▉ | 445/500 [28:57<03:34,  3.91s/it]Running Inference:  89%|████████▉ | 446/500 [29:01<03:32,  3.93s/it]Running Inference:  89%|████████▉ | 447/500 [29:05<03:27,  3.91s/it]Running Inference:  90%|████████▉ | 448/500 [29:09<03:23,  3.91s/it]Running Inference:  90%|████████▉ | 449/500 [29:13<03:19,  3.91s/it]Running Inference:  90%|█████████ | 450/500 [29:16<03:15,  3.90s/it]Running Inference:  90%|█████████ | 451/500 [29:20<03:10,  3.89s/it]Running Inference:  90%|█████████ | 452/500 [29:24<03:06,  3.89s/it]Running Inference:  91%|█████████ | 453/500 [29:28<03:03,  3.91s/it]Running Inference:  91%|█████████ | 454/500 [29:32<02:59,  3.89s/it]Running Inference:  91%|█████████ | 455/500 [29:36<02:54,  3.87s/it]Running Inference:  91%|█████████ | 456/500 [29:40<02:49,  3.85s/it]Running Inference:  91%|█████████▏| 457/500 [29:44<02:46,  3.87s/it]Running Inference:  92%|█████████▏| 458/500 [29:47<02:42,  3.86s/it]Running Inference:  92%|█████████▏| 459/500 [29:52<02:41,  3.95s/it]Running Inference:  92%|█████████▏| 460/500 [29:56<02:40,  4.01s/it]Running Inference:  92%|█████████▏| 461/500 [30:00<02:35,  4.00s/it]Running Inference:  92%|█████████▏| 462/500 [30:04<02:34,  4.08s/it]Running Inference:  93%|█████████▎| 463/500 [30:08<02:27,  4.00s/it]Running Inference:  93%|█████████▎| 464/500 [30:12<02:21,  3.94s/it]Running Inference:  93%|█████████▎| 465/500 [30:15<02:16,  3.89s/it]Running Inference:  93%|█████████▎| 466/500 [30:19<02:12,  3.89s/it]Running Inference:  93%|█████████▎| 467/500 [30:23<02:07,  3.86s/it]Running Inference:  94%|█████████▎| 468/500 [30:27<02:05,  3.91s/it]Running Inference:  94%|█████████▍| 469/500 [30:31<02:00,  3.89s/it]Running Inference:  94%|█████████▍| 470/500 [30:35<01:55,  3.86s/it]Running Inference:  94%|█████████▍| 471/500 [30:39<01:53,  3.90s/it]Running Inference:  94%|█████████▍| 472/500 [30:42<01:48,  3.88s/it]Running Inference:  95%|█████████▍| 473/500 [30:47<01:46,  3.96s/it]Running Inference:  95%|█████████▍| 474/500 [30:50<01:42,  3.93s/it]Running Inference:  95%|█████████▌| 475/500 [30:54<01:37,  3.91s/it]Running Inference:  95%|█████████▌| 476/500 [30:58<01:33,  3.90s/it]Running Inference:  95%|█████████▌| 477/500 [31:02<01:29,  3.90s/it]Running Inference:  96%|█████████▌| 478/500 [31:06<01:26,  3.95s/it]Running Inference:  96%|█████████▌| 479/500 [31:11<01:29,  4.24s/it]Running Inference:  96%|█████████▌| 480/500 [31:15<01:23,  4.16s/it]Running Inference:  96%|█████████▌| 481/500 [31:20<01:21,  4.30s/it]Running Inference:  96%|█████████▋| 482/500 [31:24<01:14,  4.16s/it]Running Inference:  97%|█████████▋| 483/500 [31:27<01:09,  4.07s/it]Running Inference:  97%|█████████▋| 484/500 [31:31<01:05,  4.08s/it]Running Inference:  97%|█████████▋| 485/500 [31:37<01:06,  4.43s/it]Running Inference:  97%|█████████▋| 486/500 [31:41<00:59,  4.25s/it]Running Inference:  97%|█████████▋| 487/500 [31:44<00:53,  4.13s/it]Running Inference:  98%|█████████▊| 488/500 [31:48<00:48,  4.02s/it]Running Inference:  98%|█████████▊| 489/500 [31:52<00:44,  4.00s/it]Running Inference:  98%|█████████▊| 490/500 [31:56<00:39,  3.99s/it]Running Inference:  98%|█████████▊| 491/500 [32:00<00:35,  3.95s/it]Running Inference:  98%|█████████▊| 492/500 [32:04<00:31,  3.96s/it]Running Inference:  99%|█████████▊| 493/500 [32:08<00:27,  3.91s/it]Running Inference:  99%|█████████▉| 494/500 [32:12<00:23,  3.93s/it]Running Inference:  99%|█████████▉| 495/500 [32:16<00:19,  3.92s/it]Running Inference:  99%|█████████▉| 496/500 [32:20<00:15,  3.93s/it]Running Inference:  99%|█████████▉| 497/500 [32:23<00:11,  3.91s/it]Running Inference: 100%|█████████▉| 498/500 [32:27<00:07,  3.91s/it]Running Inference: 100%|█████████▉| 499/500 [32:31<00:03,  3.89s/it]Running Inference: 100%|██████████| 500/500 [32:35<00:00,  3.97s/it]Running Inference: 100%|██████████| 500/500 [32:35<00:00,  3.91s/it]
2025-12-14 20:05:21,388 - INFO - Inference completed.
2025-12-14 20:05:21,404 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 20:05:21,404 - INFO - Calculating metrics for dataset: longbench
2025-12-14 20:05:21,406 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 20:05:21,406 - INFO - Metrics:
40.81
2025-12-14 20:05:21,407 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lcc, ratio=0.2) on GPU 2

----------------------------------------
Task: lcc | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 20:05:27,871 - INFO - Set deterministic seeds to 42
2025-12-14 20:05:27,871 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 20:05:27,871 - INFO - Starting evaluation run...
2025-12-14 20:05:27,871 - INFO - Output directory set to: longbenchresult
2025-12-14 20:05:27,872 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 20:05:27,872 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 20:05:27,872 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 20:05:27,872 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.08it/s]
Device set to use cuda:0
2025-12-14 20:05:44,544 - INFO - Model pipeline loaded.
2025-12-14 20:05:44,544 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 20:05:59,881 - INFO - Dataset loaded with 500 entries.
2025-12-14 20:05:59,882 - INFO - Dataset processed with 500 entries.
2025-12-14 20:05:59,896 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<39:59,  4.81s/it]Running Inference:   0%|          | 2/500 [00:08<35:12,  4.24s/it]Running Inference:   1%|          | 3/500 [00:12<35:17,  4.26s/it]Running Inference:   1%|          | 4/500 [00:16<33:46,  4.09s/it]Running Inference:   1%|          | 5/500 [00:20<32:55,  3.99s/it]Running Inference:   1%|          | 6/500 [00:24<32:56,  4.00s/it]Running Inference:   1%|▏         | 7/500 [00:28<32:33,  3.96s/it]Running Inference:   2%|▏         | 8/500 [00:32<32:01,  3.90s/it]Running Inference:   2%|▏         | 9/500 [00:36<31:37,  3.86s/it]Running Inference:   2%|▏         | 10/500 [00:39<31:20,  3.84s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:44<32:17,  3.96s/it]Running Inference:   2%|▏         | 12/500 [00:47<31:56,  3.93s/it]Running Inference:   3%|▎         | 13/500 [00:51<31:44,  3.91s/it]Running Inference:   3%|▎         | 14/500 [00:55<31:25,  3.88s/it]Running Inference:   3%|▎         | 15/500 [00:59<31:07,  3.85s/it]Running Inference:   3%|▎         | 16/500 [01:03<30:50,  3.82s/it]Running Inference:   3%|▎         | 17/500 [01:07<32:21,  4.02s/it]Running Inference:   4%|▎         | 18/500 [01:11<31:59,  3.98s/it]Running Inference:   4%|▍         | 19/500 [01:15<31:37,  3.94s/it]Running Inference:   4%|▍         | 20/500 [01:16<24:08,  3.02s/it]Running Inference:   4%|▍         | 21/500 [01:20<26:02,  3.26s/it]Running Inference:   4%|▍         | 22/500 [01:23<27:13,  3.42s/it]Running Inference:   5%|▍         | 23/500 [01:27<27:59,  3.52s/it]Running Inference:   5%|▍         | 24/500 [01:31<28:30,  3.59s/it]Running Inference:   5%|▌         | 25/500 [01:35<28:56,  3.66s/it]Running Inference:   5%|▌         | 26/500 [01:39<29:19,  3.71s/it]Running Inference:   5%|▌         | 27/500 [01:42<29:28,  3.74s/it]Running Inference:   6%|▌         | 28/500 [01:47<32:36,  4.15s/it]Running Inference:   6%|▌         | 29/500 [01:51<32:15,  4.11s/it]Running Inference:   6%|▌         | 30/500 [01:56<32:54,  4.20s/it]Running Inference:   6%|▌         | 31/500 [02:00<32:06,  4.11s/it]Running Inference:   6%|▋         | 32/500 [02:04<31:26,  4.03s/it]Running Inference:   7%|▋         | 33/500 [02:07<30:46,  3.95s/it]Running Inference:   7%|▋         | 34/500 [02:11<30:17,  3.90s/it]Running Inference:   7%|▋         | 35/500 [02:15<29:53,  3.86s/it]Running Inference:   7%|▋         | 36/500 [02:19<30:36,  3.96s/it]Running Inference:   7%|▋         | 37/500 [02:23<31:06,  4.03s/it]Running Inference:   8%|▊         | 38/500 [02:27<30:32,  3.97s/it]Running Inference:   8%|▊         | 39/500 [02:31<31:00,  4.03s/it]Running Inference:   8%|▊         | 40/500 [02:35<30:20,  3.96s/it]Running Inference:   8%|▊         | 41/500 [02:39<29:59,  3.92s/it]Running Inference:   8%|▊         | 42/500 [02:44<31:32,  4.13s/it]Running Inference:   9%|▊         | 43/500 [02:48<32:22,  4.25s/it]Running Inference:   9%|▉         | 44/500 [02:52<31:26,  4.14s/it]Running Inference:   9%|▉         | 45/500 [02:56<30:46,  4.06s/it]Running Inference:   9%|▉         | 46/500 [03:00<29:59,  3.96s/it]Running Inference:   9%|▉         | 47/500 [03:03<29:45,  3.94s/it]Running Inference:  10%|▉         | 48/500 [03:07<29:13,  3.88s/it]Running Inference:  10%|▉         | 49/500 [03:11<29:17,  3.90s/it]Running Inference:  10%|█         | 50/500 [03:16<30:37,  4.08s/it]Running Inference:  10%|█         | 51/500 [03:19<29:52,  3.99s/it]Running Inference:  10%|█         | 52/500 [03:23<29:23,  3.94s/it]Running Inference:  11%|█         | 53/500 [03:27<29:11,  3.92s/it]Running Inference:  11%|█         | 54/500 [03:31<28:52,  3.88s/it]Running Inference:  11%|█         | 55/500 [03:35<28:32,  3.85s/it]Running Inference:  11%|█         | 56/500 [03:38<28:20,  3.83s/it]Running Inference:  11%|█▏        | 57/500 [03:47<38:19,  5.19s/it]Running Inference:  12%|█▏        | 58/500 [03:51<35:02,  4.76s/it]Running Inference:  12%|█▏        | 59/500 [03:51<26:13,  3.57s/it]Running Inference:  12%|█▏        | 60/500 [03:54<24:28,  3.34s/it]Running Inference:  12%|█▏        | 61/500 [03:58<25:27,  3.48s/it]Running Inference:  12%|█▏        | 62/500 [04:02<26:01,  3.57s/it]Running Inference:  13%|█▎        | 63/500 [04:06<26:27,  3.63s/it]Running Inference:  13%|█▎        | 64/500 [04:07<21:35,  2.97s/it]Running Inference:  13%|█▎        | 65/500 [04:11<23:18,  3.21s/it]Running Inference:  13%|█▎        | 66/500 [04:15<25:08,  3.48s/it]Running Inference:  13%|█▎        | 67/500 [04:19<25:52,  3.59s/it]Running Inference:  14%|█▎        | 68/500 [04:22<26:17,  3.65s/it]Running Inference:  14%|█▍        | 69/500 [04:26<26:30,  3.69s/it]Running Inference:  14%|█▍        | 70/500 [04:30<26:38,  3.72s/it]Running Inference:  14%|█▍        | 71/500 [04:34<26:40,  3.73s/it]Running Inference:  14%|█▍        | 72/500 [04:38<27:13,  3.82s/it]Running Inference:  15%|█▍        | 73/500 [04:41<26:14,  3.69s/it]Running Inference:  15%|█▍        | 74/500 [04:45<26:29,  3.73s/it]Running Inference:  15%|█▌        | 75/500 [04:49<27:06,  3.83s/it]Running Inference:  15%|█▌        | 76/500 [04:53<27:16,  3.86s/it]Running Inference:  15%|█▌        | 77/500 [04:57<27:10,  3.86s/it]Running Inference:  16%|█▌        | 78/500 [05:00<26:28,  3.77s/it]Running Inference:  16%|█▌        | 79/500 [05:04<26:22,  3.76s/it]Running Inference:  16%|█▌        | 80/500 [05:08<26:25,  3.77s/it]Running Inference:  16%|█▌        | 81/500 [05:12<27:17,  3.91s/it]Running Inference:  16%|█▋        | 82/500 [05:16<28:03,  4.03s/it]Running Inference:  17%|█▋        | 83/500 [05:20<27:39,  3.98s/it]Running Inference:  17%|█▋        | 84/500 [05:24<27:18,  3.94s/it]Running Inference:  17%|█▋        | 85/500 [05:29<29:00,  4.19s/it]Running Inference:  17%|█▋        | 86/500 [05:33<28:03,  4.07s/it]Running Inference:  17%|█▋        | 87/500 [05:37<27:21,  3.98s/it]Running Inference:  18%|█▊        | 88/500 [05:40<26:55,  3.92s/it]Running Inference:  18%|█▊        | 89/500 [05:44<26:44,  3.90s/it]Running Inference:  18%|█▊        | 90/500 [05:48<26:29,  3.88s/it]Running Inference:  18%|█▊        | 91/500 [05:49<20:08,  2.96s/it]Running Inference:  18%|█▊        | 92/500 [05:53<21:57,  3.23s/it]Running Inference:  19%|█▊        | 93/500 [05:56<23:05,  3.40s/it]Running Inference:  19%|█▉        | 94/500 [06:00<23:46,  3.51s/it]Running Inference:  19%|█▉        | 95/500 [06:04<24:17,  3.60s/it]Running Inference:  19%|█▉        | 96/500 [06:08<24:45,  3.68s/it]Running Inference:  19%|█▉        | 97/500 [06:12<24:55,  3.71s/it]Running Inference:  20%|█▉        | 98/500 [06:15<24:56,  3.72s/it]Running Inference:  20%|█▉        | 99/500 [06:19<24:56,  3.73s/it]Running Inference:  20%|██        | 100/500 [06:21<21:46,  3.27s/it]Running Inference:  20%|██        | 101/500 [06:22<16:57,  2.55s/it]Running Inference:  20%|██        | 102/500 [06:26<19:18,  2.91s/it]Running Inference:  21%|██        | 103/500 [06:30<20:56,  3.16s/it]Running Inference:  21%|██        | 104/500 [06:36<25:58,  3.94s/it]Running Inference:  21%|██        | 105/500 [06:40<27:09,  4.13s/it]Running Inference:  21%|██        | 106/500 [06:45<29:26,  4.48s/it]Running Inference:  21%|██▏       | 107/500 [06:49<28:22,  4.33s/it]Running Inference:  22%|██▏       | 108/500 [06:53<27:14,  4.17s/it]Running Inference:  22%|██▏       | 109/500 [06:57<26:31,  4.07s/it]Running Inference:  22%|██▏       | 110/500 [07:01<26:05,  4.02s/it]Running Inference:  22%|██▏       | 111/500 [07:05<25:31,  3.94s/it]Running Inference:  22%|██▏       | 112/500 [07:08<25:10,  3.89s/it]Running Inference:  23%|██▎       | 113/500 [07:13<25:34,  3.96s/it]Running Inference:  23%|██▎       | 114/500 [07:18<27:45,  4.31s/it]Running Inference:  23%|██▎       | 115/500 [07:22<27:11,  4.24s/it]Running Inference:  23%|██▎       | 116/500 [07:26<26:59,  4.22s/it]Running Inference:  23%|██▎       | 117/500 [07:30<26:01,  4.08s/it]Running Inference:  24%|██▎       | 118/500 [07:34<26:35,  4.18s/it]Running Inference:  24%|██▍       | 119/500 [07:38<25:39,  4.04s/it]Running Inference:  24%|██▍       | 120/500 [07:42<25:47,  4.07s/it]Running Inference:  24%|██▍       | 121/500 [07:46<25:26,  4.03s/it]Running Inference:  24%|██▍       | 122/500 [07:50<24:45,  3.93s/it]Running Inference:  25%|██▍       | 123/500 [07:53<24:23,  3.88s/it]Running Inference:  25%|██▍       | 124/500 [07:57<24:05,  3.84s/it]Running Inference:  25%|██▌       | 125/500 [08:01<24:26,  3.91s/it]Running Inference:  25%|██▌       | 126/500 [08:05<24:05,  3.87s/it]Running Inference:  25%|██▌       | 127/500 [08:06<19:05,  3.07s/it]Running Inference:  26%|██▌       | 128/500 [08:10<20:41,  3.34s/it]Running Inference:  26%|██▌       | 129/500 [08:14<21:26,  3.47s/it]Running Inference:  26%|██▌       | 130/500 [08:18<22:09,  3.59s/it]Running Inference:  26%|██▌       | 131/500 [08:22<22:26,  3.65s/it]Running Inference:  26%|██▋       | 132/500 [08:25<22:34,  3.68s/it]Running Inference:  27%|██▋       | 133/500 [08:29<22:47,  3.73s/it]Running Inference:  27%|██▋       | 134/500 [08:33<22:52,  3.75s/it]Running Inference:  27%|██▋       | 135/500 [08:37<23:05,  3.79s/it]Running Inference:  27%|██▋       | 136/500 [08:41<23:09,  3.82s/it]Running Inference:  27%|██▋       | 137/500 [08:44<22:56,  3.79s/it]Running Inference:  28%|██▊       | 138/500 [08:48<23:04,  3.82s/it]Running Inference:  28%|██▊       | 139/500 [08:52<22:50,  3.80s/it]Running Inference:  28%|██▊       | 140/500 [08:56<22:56,  3.82s/it]Running Inference:  28%|██▊       | 141/500 [09:00<23:36,  3.94s/it]Running Inference:  28%|██▊       | 142/500 [09:04<23:41,  3.97s/it]Running Inference:  29%|██▊       | 143/500 [09:08<23:18,  3.92s/it]Running Inference:  29%|██▉       | 144/500 [09:12<22:57,  3.87s/it]Running Inference:  29%|██▉       | 145/500 [09:16<23:02,  3.89s/it]Running Inference:  29%|██▉       | 146/500 [09:20<23:10,  3.93s/it]Running Inference:  29%|██▉       | 147/500 [09:24<23:19,  3.96s/it]Running Inference:  30%|██▉       | 148/500 [09:27<22:51,  3.90s/it]Running Inference:  30%|██▉       | 149/500 [09:31<22:33,  3.85s/it]Running Inference:  30%|███       | 150/500 [09:35<22:15,  3.82s/it]Running Inference:  30%|███       | 151/500 [09:39<22:14,  3.82s/it]Running Inference:  30%|███       | 152/500 [09:43<22:04,  3.81s/it]Running Inference:  31%|███       | 153/500 [09:46<21:57,  3.80s/it]Running Inference:  31%|███       | 154/500 [09:50<21:49,  3.79s/it]Running Inference:  31%|███       | 155/500 [09:54<21:39,  3.77s/it]Running Inference:  31%|███       | 156/500 [09:58<21:37,  3.77s/it]Running Inference:  31%|███▏      | 157/500 [10:02<21:53,  3.83s/it]Running Inference:  32%|███▏      | 158/500 [10:05<21:47,  3.82s/it]Running Inference:  32%|███▏      | 159/500 [10:09<21:36,  3.80s/it]Running Inference:  32%|███▏      | 160/500 [10:13<21:27,  3.79s/it]Running Inference:  32%|███▏      | 161/500 [10:17<21:24,  3.79s/it]Running Inference:  32%|███▏      | 162/500 [10:21<21:31,  3.82s/it]Running Inference:  33%|███▎      | 163/500 [10:24<21:18,  3.79s/it]Running Inference:  33%|███▎      | 164/500 [10:28<21:13,  3.79s/it]Running Inference:  33%|███▎      | 165/500 [10:32<21:13,  3.80s/it]Running Inference:  33%|███▎      | 166/500 [10:36<21:00,  3.77s/it]Running Inference:  33%|███▎      | 167/500 [10:40<21:33,  3.88s/it]Running Inference:  34%|███▎      | 168/500 [10:44<21:20,  3.86s/it]Running Inference:  34%|███▍      | 169/500 [10:47<21:15,  3.85s/it]Running Inference:  34%|███▍      | 170/500 [10:51<21:02,  3.83s/it]Running Inference:  34%|███▍      | 171/500 [10:55<21:04,  3.84s/it]Running Inference:  34%|███▍      | 172/500 [10:59<20:52,  3.82s/it]Running Inference:  35%|███▍      | 173/500 [11:03<20:45,  3.81s/it]Running Inference:  35%|███▍      | 174/500 [11:07<21:06,  3.88s/it]Running Inference:  35%|███▌      | 175/500 [11:11<21:49,  4.03s/it]Running Inference:  35%|███▌      | 176/500 [11:15<21:22,  3.96s/it]Running Inference:  35%|███▌      | 177/500 [11:19<22:06,  4.11s/it]Running Inference:  36%|███▌      | 178/500 [11:23<21:22,  3.98s/it]Running Inference:  36%|███▌      | 179/500 [11:27<20:57,  3.92s/it]Running Inference:  36%|███▌      | 180/500 [11:31<20:46,  3.90s/it]Running Inference:  36%|███▌      | 181/500 [11:34<20:27,  3.85s/it]Running Inference:  36%|███▋      | 182/500 [11:38<20:12,  3.81s/it]Running Inference:  37%|███▋      | 183/500 [11:42<20:22,  3.86s/it]Running Inference:  37%|███▋      | 184/500 [11:46<20:19,  3.86s/it]Running Inference:  37%|███▋      | 185/500 [11:50<20:19,  3.87s/it]Running Inference:  37%|███▋      | 186/500 [11:54<20:09,  3.85s/it]Running Inference:  37%|███▋      | 187/500 [11:57<20:09,  3.86s/it]Running Inference:  38%|███▊      | 188/500 [12:01<19:56,  3.84s/it]Running Inference:  38%|███▊      | 189/500 [12:05<19:53,  3.84s/it]Running Inference:  38%|███▊      | 190/500 [12:09<20:17,  3.93s/it]Running Inference:  38%|███▊      | 191/500 [12:13<19:55,  3.87s/it]Running Inference:  38%|███▊      | 192/500 [12:17<19:48,  3.86s/it]Running Inference:  39%|███▊      | 193/500 [12:21<19:39,  3.84s/it]Running Inference:  39%|███▉      | 194/500 [12:24<19:31,  3.83s/it]Running Inference:  39%|███▉      | 195/500 [12:28<19:23,  3.81s/it]Running Inference:  39%|███▉      | 196/500 [12:32<19:16,  3.81s/it]Running Inference:  39%|███▉      | 197/500 [12:36<19:22,  3.84s/it]Running Inference:  40%|███▉      | 198/500 [12:40<19:08,  3.80s/it]Running Inference:  40%|███▉      | 199/500 [12:43<18:58,  3.78s/it]Running Inference:  40%|████      | 200/500 [12:47<19:02,  3.81s/it]Running Inference:  40%|████      | 201/500 [12:51<19:10,  3.85s/it]Running Inference:  40%|████      | 202/500 [12:55<18:58,  3.82s/it]Running Inference:  41%|████      | 203/500 [13:00<20:57,  4.23s/it]Running Inference:  41%|████      | 204/500 [13:04<20:15,  4.11s/it]Running Inference:  41%|████      | 205/500 [13:08<19:41,  4.01s/it]Running Inference:  41%|████      | 206/500 [13:11<19:13,  3.92s/it]Running Inference:  41%|████▏     | 207/500 [13:15<18:57,  3.88s/it]Running Inference:  42%|████▏     | 208/500 [13:19<19:15,  3.96s/it]Running Inference:  42%|████▏     | 209/500 [13:24<19:31,  4.03s/it]Running Inference:  42%|████▏     | 210/500 [13:27<19:04,  3.95s/it]Running Inference:  42%|████▏     | 211/500 [13:31<19:02,  3.95s/it]Running Inference:  42%|████▏     | 212/500 [13:35<18:38,  3.88s/it]Running Inference:  43%|████▎     | 213/500 [13:39<18:17,  3.82s/it]Running Inference:  43%|████▎     | 214/500 [13:42<18:10,  3.81s/it]Running Inference:  43%|████▎     | 215/500 [13:46<18:08,  3.82s/it]Running Inference:  43%|████▎     | 216/500 [13:50<18:00,  3.80s/it]Running Inference:  43%|████▎     | 217/500 [13:54<18:17,  3.88s/it]Running Inference:  44%|████▎     | 218/500 [13:58<18:22,  3.91s/it]Running Inference:  44%|████▍     | 219/500 [14:02<18:03,  3.85s/it]Running Inference:  44%|████▍     | 220/500 [14:06<17:54,  3.84s/it]Running Inference:  44%|████▍     | 221/500 [14:09<17:40,  3.80s/it]Running Inference:  44%|████▍     | 222/500 [14:13<17:31,  3.78s/it]Running Inference:  45%|████▍     | 223/500 [14:17<17:27,  3.78s/it]Running Inference:  45%|████▍     | 224/500 [14:21<17:23,  3.78s/it]Running Inference:  45%|████▌     | 225/500 [14:24<17:15,  3.77s/it]Running Inference:  45%|████▌     | 226/500 [14:28<17:08,  3.76s/it]Running Inference:  45%|████▌     | 227/500 [14:32<17:05,  3.76s/it]Running Inference:  46%|████▌     | 228/500 [14:35<16:54,  3.73s/it]Running Inference:  46%|████▌     | 229/500 [14:39<16:48,  3.72s/it]Running Inference:  46%|████▌     | 230/500 [14:43<16:46,  3.73s/it]Running Inference:  46%|████▌     | 231/500 [14:47<16:45,  3.74s/it]Running Inference:  46%|████▋     | 232/500 [14:50<16:44,  3.75s/it]Running Inference:  47%|████▋     | 233/500 [14:54<16:50,  3.78s/it]Running Inference:  47%|████▋     | 234/500 [14:58<16:48,  3.79s/it]Running Inference:  47%|████▋     | 235/500 [15:02<17:09,  3.89s/it]Running Inference:  47%|████▋     | 236/500 [15:06<16:50,  3.83s/it]Running Inference:  47%|████▋     | 237/500 [15:10<16:38,  3.80s/it]Running Inference:  48%|████▊     | 238/500 [15:13<16:31,  3.78s/it]Running Inference:  48%|████▊     | 239/500 [15:17<16:33,  3.81s/it]Running Inference:  48%|████▊     | 240/500 [15:21<16:27,  3.80s/it]Running Inference:  48%|████▊     | 241/500 [15:25<16:22,  3.79s/it]Running Inference:  48%|████▊     | 242/500 [15:29<16:11,  3.76s/it]Running Inference:  49%|████▊     | 243/500 [15:33<16:59,  3.97s/it]Running Inference:  49%|████▉     | 244/500 [15:37<17:14,  4.04s/it]Running Inference:  49%|████▉     | 245/500 [15:41<16:56,  3.99s/it]Running Inference:  49%|████▉     | 246/500 [15:45<16:29,  3.90s/it]Running Inference:  49%|████▉     | 247/500 [15:49<16:23,  3.89s/it]Running Inference:  50%|████▉     | 248/500 [15:52<16:11,  3.85s/it]Running Inference:  50%|████▉     | 249/500 [15:56<16:03,  3.84s/it]Running Inference:  50%|█████     | 250/500 [16:00<15:59,  3.84s/it]Running Inference:  50%|█████     | 251/500 [16:04<16:12,  3.91s/it]Running Inference:  50%|█████     | 252/500 [16:09<16:51,  4.08s/it]Running Inference:  51%|█████     | 253/500 [16:12<16:25,  3.99s/it]Running Inference:  51%|█████     | 254/500 [16:16<16:06,  3.93s/it]Running Inference:  51%|█████     | 255/500 [16:20<16:03,  3.93s/it]Running Inference:  51%|█████     | 256/500 [16:24<15:46,  3.88s/it]Running Inference:  51%|█████▏    | 257/500 [16:28<15:49,  3.91s/it]Running Inference:  52%|█████▏    | 258/500 [16:32<15:35,  3.87s/it]Running Inference:  52%|█████▏    | 259/500 [16:35<15:27,  3.85s/it]Running Inference:  52%|█████▏    | 260/500 [16:39<15:14,  3.81s/it]Running Inference:  52%|█████▏    | 261/500 [16:43<15:03,  3.78s/it]Running Inference:  52%|█████▏    | 262/500 [16:47<15:05,  3.81s/it]Running Inference:  53%|█████▎    | 263/500 [16:50<15:00,  3.80s/it]Running Inference:  53%|█████▎    | 264/500 [16:54<14:51,  3.78s/it]Running Inference:  53%|█████▎    | 265/500 [16:58<14:59,  3.83s/it]Running Inference:  53%|█████▎    | 266/500 [17:02<14:52,  3.81s/it]Running Inference:  53%|█████▎    | 267/500 [17:06<15:18,  3.94s/it]Running Inference:  54%|█████▎    | 268/500 [17:10<15:03,  3.90s/it]Running Inference:  54%|█████▍    | 269/500 [17:14<14:53,  3.87s/it]Running Inference:  54%|█████▍    | 270/500 [17:18<14:42,  3.84s/it]Running Inference:  54%|█████▍    | 271/500 [17:21<14:28,  3.79s/it]Running Inference:  54%|█████▍    | 272/500 [17:25<14:24,  3.79s/it]Running Inference:  55%|█████▍    | 273/500 [17:29<14:25,  3.81s/it]Running Inference:  55%|█████▍    | 274/500 [17:33<14:15,  3.79s/it]Running Inference:  55%|█████▌    | 275/500 [17:36<14:10,  3.78s/it]Running Inference:  55%|█████▌    | 276/500 [17:40<14:07,  3.79s/it]Running Inference:  55%|█████▌    | 277/500 [17:44<14:03,  3.78s/it]Running Inference:  56%|█████▌    | 278/500 [17:48<14:03,  3.80s/it]Running Inference:  56%|█████▌    | 279/500 [17:52<14:10,  3.85s/it]Running Inference:  56%|█████▌    | 280/500 [17:56<14:18,  3.90s/it]Running Inference:  56%|█████▌    | 281/500 [18:00<14:11,  3.89s/it]Running Inference:  56%|█████▋    | 282/500 [18:03<14:02,  3.87s/it]Running Inference:  57%|█████▋    | 283/500 [18:07<13:53,  3.84s/it]Running Inference:  57%|█████▋    | 284/500 [18:11<13:53,  3.86s/it]Running Inference:  57%|█████▋    | 285/500 [18:15<13:49,  3.86s/it]Running Inference:  57%|█████▋    | 286/500 [18:19<13:36,  3.82s/it]Running Inference:  57%|█████▋    | 287/500 [18:23<14:21,  4.05s/it]Running Inference:  58%|█████▊    | 288/500 [18:27<14:14,  4.03s/it]Running Inference:  58%|█████▊    | 289/500 [18:31<13:54,  3.96s/it]Running Inference:  58%|█████▊    | 290/500 [18:35<13:47,  3.94s/it]Running Inference:  58%|█████▊    | 291/500 [18:39<13:44,  3.95s/it]Running Inference:  58%|█████▊    | 292/500 [18:43<13:26,  3.88s/it]Running Inference:  59%|█████▊    | 293/500 [18:46<13:23,  3.88s/it]Running Inference:  59%|█████▉    | 294/500 [18:50<13:10,  3.84s/it]Running Inference:  59%|█████▉    | 295/500 [18:54<13:01,  3.81s/it]Running Inference:  59%|█████▉    | 296/500 [18:58<12:55,  3.80s/it]Running Inference:  59%|█████▉    | 297/500 [19:02<12:55,  3.82s/it]Running Inference:  60%|█████▉    | 298/500 [19:05<12:46,  3.79s/it]Running Inference:  60%|█████▉    | 299/500 [19:09<12:38,  3.77s/it]Running Inference:  60%|██████    | 300/500 [19:13<12:29,  3.75s/it]Running Inference:  60%|██████    | 301/500 [19:17<12:43,  3.84s/it]Running Inference:  60%|██████    | 302/500 [19:21<12:36,  3.82s/it]Running Inference:  61%|██████    | 303/500 [19:24<12:30,  3.81s/it]Running Inference:  61%|██████    | 304/500 [19:29<12:54,  3.95s/it]Running Inference:  61%|██████    | 305/500 [19:32<12:36,  3.88s/it]Running Inference:  61%|██████    | 306/500 [19:37<12:51,  3.98s/it]Running Inference:  61%|██████▏   | 307/500 [19:40<12:37,  3.93s/it]Running Inference:  62%|██████▏   | 308/500 [19:44<12:21,  3.86s/it]Running Inference:  62%|██████▏   | 309/500 [19:48<12:09,  3.82s/it]Running Inference:  62%|██████▏   | 310/500 [19:52<12:02,  3.80s/it]Running Inference:  62%|██████▏   | 311/500 [19:55<12:03,  3.83s/it]Running Inference:  62%|██████▏   | 312/500 [19:59<11:57,  3.81s/it]Running Inference:  63%|██████▎   | 313/500 [20:03<11:57,  3.84s/it]Running Inference:  63%|██████▎   | 314/500 [20:07<11:47,  3.81s/it]Running Inference:  63%|██████▎   | 315/500 [20:11<11:37,  3.77s/it]Running Inference:  63%|██████▎   | 316/500 [20:14<11:29,  3.75s/it]Running Inference:  63%|██████▎   | 317/500 [20:18<11:24,  3.74s/it]Running Inference:  64%|██████▎   | 318/500 [20:22<11:22,  3.75s/it]Running Inference:  64%|██████▍   | 319/500 [20:26<11:21,  3.76s/it]Running Inference:  64%|██████▍   | 320/500 [20:30<11:47,  3.93s/it]Running Inference:  64%|██████▍   | 321/500 [20:34<11:50,  3.97s/it]Running Inference:  64%|██████▍   | 322/500 [20:38<11:37,  3.92s/it]Running Inference:  65%|██████▍   | 323/500 [20:41<11:24,  3.87s/it]Running Inference:  65%|██████▍   | 324/500 [20:45<11:16,  3.85s/it]Running Inference:  65%|██████▌   | 325/500 [20:49<11:09,  3.83s/it]Running Inference:  65%|██████▌   | 326/500 [20:53<11:24,  3.93s/it]Running Inference:  65%|██████▌   | 327/500 [20:59<12:30,  4.34s/it]Running Inference:  66%|██████▌   | 328/500 [21:03<12:22,  4.31s/it]Running Inference:  66%|██████▌   | 329/500 [21:06<11:46,  4.13s/it]Running Inference:  66%|██████▌   | 330/500 [21:10<11:21,  4.01s/it]Running Inference:  66%|██████▌   | 331/500 [21:15<11:41,  4.15s/it]Running Inference:  66%|██████▋   | 332/500 [21:19<11:30,  4.11s/it]Running Inference:  67%|██████▋   | 333/500 [21:22<11:10,  4.01s/it]Running Inference:  67%|██████▋   | 334/500 [21:26<10:55,  3.95s/it]Running Inference:  67%|██████▋   | 335/500 [21:30<10:44,  3.91s/it]Running Inference:  67%|██████▋   | 336/500 [21:34<10:32,  3.86s/it]Running Inference:  67%|██████▋   | 337/500 [21:38<10:24,  3.83s/it]Running Inference:  68%|██████▊   | 338/500 [21:41<10:19,  3.82s/it]Running Inference:  68%|██████▊   | 339/500 [21:46<10:32,  3.93s/it]Running Inference:  68%|██████▊   | 340/500 [21:51<11:58,  4.49s/it]Running Inference:  68%|██████▊   | 341/500 [21:55<11:21,  4.28s/it]Running Inference:  68%|██████▊   | 342/500 [21:59<10:56,  4.15s/it]Running Inference:  69%|██████▊   | 343/500 [22:03<10:50,  4.14s/it]Running Inference:  69%|██████▉   | 344/500 [22:07<10:42,  4.12s/it]Running Inference:  69%|██████▉   | 345/500 [22:11<10:26,  4.04s/it]Running Inference:  69%|██████▉   | 346/500 [22:15<10:19,  4.02s/it]Running Inference:  69%|██████▉   | 347/500 [22:19<10:09,  3.99s/it]Running Inference:  70%|██████▉   | 348/500 [22:23<10:00,  3.95s/it]Running Inference:  70%|██████▉   | 349/500 [22:27<09:50,  3.91s/it]Running Inference:  70%|███████   | 350/500 [22:33<11:17,  4.52s/it]Running Inference:  70%|███████   | 351/500 [22:37<11:01,  4.44s/it]Running Inference:  70%|███████   | 352/500 [22:41<10:24,  4.22s/it]Running Inference:  71%|███████   | 353/500 [22:44<09:59,  4.08s/it]Running Inference:  71%|███████   | 354/500 [22:48<09:47,  4.02s/it]Running Inference:  71%|███████   | 355/500 [22:55<11:32,  4.78s/it]Running Inference:  71%|███████   | 356/500 [22:59<10:46,  4.49s/it]Running Inference:  71%|███████▏  | 357/500 [23:03<10:28,  4.39s/it]Running Inference:  72%|███████▏  | 358/500 [23:07<09:57,  4.21s/it]Running Inference:  72%|███████▏  | 359/500 [23:10<09:33,  4.07s/it]Running Inference:  72%|███████▏  | 360/500 [23:14<09:17,  3.98s/it]Running Inference:  72%|███████▏  | 361/500 [23:18<09:08,  3.94s/it]Running Inference:  72%|███████▏  | 362/500 [23:22<09:05,  3.95s/it]Running Inference:  73%|███████▎  | 363/500 [23:25<08:33,  3.75s/it]Running Inference:  73%|███████▎  | 364/500 [23:29<08:37,  3.81s/it]Running Inference:  73%|███████▎  | 365/500 [23:34<09:19,  4.14s/it]Running Inference:  73%|███████▎  | 366/500 [23:38<09:17,  4.16s/it]Running Inference:  73%|███████▎  | 367/500 [23:42<09:01,  4.07s/it]Running Inference:  74%|███████▎  | 368/500 [23:46<08:52,  4.04s/it]Running Inference:  74%|███████▍  | 369/500 [23:50<08:52,  4.07s/it]Running Inference:  74%|███████▍  | 370/500 [23:54<08:58,  4.15s/it]Running Inference:  74%|███████▍  | 371/500 [23:58<08:40,  4.04s/it]Running Inference:  74%|███████▍  | 372/500 [24:02<08:33,  4.01s/it]Running Inference:  75%|███████▍  | 373/500 [24:06<08:21,  3.95s/it]Running Inference:  75%|███████▍  | 374/500 [24:10<08:15,  3.93s/it]Running Inference:  75%|███████▌  | 375/500 [24:14<08:04,  3.88s/it]Running Inference:  75%|███████▌  | 376/500 [24:17<07:59,  3.86s/it]Running Inference:  75%|███████▌  | 377/500 [24:21<07:52,  3.84s/it]Running Inference:  76%|███████▌  | 378/500 [24:25<07:49,  3.84s/it]Running Inference:  76%|███████▌  | 379/500 [24:29<07:50,  3.89s/it]Running Inference:  76%|███████▌  | 380/500 [24:33<07:43,  3.86s/it]Running Inference:  76%|███████▌  | 381/500 [24:37<07:36,  3.84s/it]Running Inference:  76%|███████▋  | 382/500 [24:41<07:44,  3.93s/it]Running Inference:  77%|███████▋  | 383/500 [24:45<07:41,  3.94s/it]Running Inference:  77%|███████▋  | 384/500 [24:49<07:31,  3.89s/it]Running Inference:  77%|███████▋  | 385/500 [24:50<05:45,  3.00s/it]Running Inference:  77%|███████▋  | 386/500 [24:54<06:15,  3.30s/it]Running Inference:  77%|███████▋  | 387/500 [24:57<06:33,  3.48s/it]Running Inference:  78%|███████▊  | 388/500 [25:01<06:40,  3.58s/it]Running Inference:  78%|███████▊  | 389/500 [25:05<06:48,  3.68s/it]Running Inference:  78%|███████▊  | 390/500 [25:09<06:48,  3.71s/it]Running Inference:  78%|███████▊  | 391/500 [25:13<06:47,  3.74s/it]Running Inference:  78%|███████▊  | 392/500 [25:17<06:45,  3.75s/it]Running Inference:  79%|███████▊  | 393/500 [25:17<04:59,  2.80s/it]Running Inference:  79%|███████▉  | 394/500 [25:21<05:29,  3.11s/it]Running Inference:  79%|███████▉  | 395/500 [25:25<05:50,  3.33s/it]Running Inference:  79%|███████▉  | 396/500 [25:29<05:59,  3.46s/it]Running Inference:  79%|███████▉  | 397/500 [25:33<06:15,  3.65s/it]Running Inference:  80%|███████▉  | 398/500 [25:36<06:14,  3.68s/it]Running Inference:  80%|███████▉  | 399/500 [25:40<06:13,  3.69s/it]Running Inference:  80%|████████  | 400/500 [25:44<06:10,  3.71s/it]Running Inference:  80%|████████  | 401/500 [25:48<06:08,  3.72s/it]Running Inference:  80%|████████  | 402/500 [25:51<06:07,  3.75s/it]Running Inference:  81%|████████  | 403/500 [25:55<06:05,  3.76s/it]Running Inference:  81%|████████  | 404/500 [25:59<06:00,  3.76s/it]Running Inference:  81%|████████  | 405/500 [26:03<05:59,  3.78s/it]Running Inference:  81%|████████  | 406/500 [26:07<05:55,  3.78s/it]Running Inference:  81%|████████▏ | 407/500 [26:10<05:51,  3.78s/it]Running Inference:  82%|████████▏ | 408/500 [26:14<05:47,  3.78s/it]Running Inference:  82%|████████▏ | 409/500 [26:18<05:44,  3.78s/it]Running Inference:  82%|████████▏ | 410/500 [26:22<05:41,  3.79s/it]Running Inference:  82%|████████▏ | 411/500 [26:26<05:39,  3.82s/it]Running Inference:  82%|████████▏ | 412/500 [26:29<05:35,  3.81s/it]Running Inference:  83%|████████▎ | 413/500 [26:33<05:30,  3.79s/it]Running Inference:  83%|████████▎ | 414/500 [26:37<05:26,  3.80s/it]Running Inference:  83%|████████▎ | 415/500 [26:41<05:22,  3.79s/it]Running Inference:  83%|████████▎ | 416/500 [26:44<05:17,  3.77s/it]Running Inference:  83%|████████▎ | 417/500 [26:48<05:13,  3.77s/it]Running Inference:  84%|████████▎ | 418/500 [26:52<05:10,  3.79s/it]Running Inference:  84%|████████▍ | 419/500 [26:56<05:05,  3.77s/it]Running Inference:  84%|████████▍ | 420/500 [27:00<05:01,  3.76s/it]Running Inference:  84%|████████▍ | 421/500 [27:03<04:57,  3.76s/it]Running Inference:  84%|████████▍ | 422/500 [27:07<04:53,  3.76s/it]Running Inference:  85%|████████▍ | 423/500 [27:10<04:39,  3.63s/it]Running Inference:  85%|████████▍ | 424/500 [27:14<04:38,  3.67s/it]Running Inference:  85%|████████▌ | 425/500 [27:18<04:44,  3.79s/it]Running Inference:  85%|████████▌ | 426/500 [27:22<04:40,  3.79s/it]Running Inference:  85%|████████▌ | 427/500 [27:26<04:34,  3.76s/it]Running Inference:  86%|████████▌ | 428/500 [27:29<04:30,  3.75s/it]Running Inference:  86%|████████▌ | 429/500 [27:33<04:26,  3.75s/it]Running Inference:  86%|████████▌ | 430/500 [27:37<04:31,  3.87s/it]Running Inference:  86%|████████▌ | 431/500 [27:41<04:26,  3.86s/it]Running Inference:  86%|████████▋ | 432/500 [27:45<04:27,  3.93s/it]Running Inference:  87%|████████▋ | 433/500 [27:49<04:25,  3.96s/it]Running Inference:  87%|████████▋ | 434/500 [27:53<04:21,  3.97s/it]Running Inference:  87%|████████▋ | 435/500 [27:57<04:14,  3.91s/it]Running Inference:  87%|████████▋ | 436/500 [28:01<04:09,  3.89s/it]Running Inference:  87%|████████▋ | 437/500 [28:05<04:03,  3.87s/it]Running Inference:  88%|████████▊ | 438/500 [28:08<03:57,  3.83s/it]Running Inference:  88%|████████▊ | 439/500 [28:12<03:54,  3.84s/it]Running Inference:  88%|████████▊ | 440/500 [28:16<03:50,  3.84s/it]Running Inference:  88%|████████▊ | 441/500 [28:20<03:44,  3.80s/it]Running Inference:  88%|████████▊ | 442/500 [28:24<03:38,  3.77s/it]Running Inference:  89%|████████▊ | 443/500 [28:27<03:35,  3.77s/it]Running Inference:  89%|████████▉ | 444/500 [28:31<03:33,  3.82s/it]Running Inference:  89%|████████▉ | 445/500 [28:35<03:35,  3.91s/it]Running Inference:  89%|████████▉ | 446/500 [28:40<03:35,  3.99s/it]Running Inference:  89%|████████▉ | 447/500 [28:44<03:31,  3.99s/it]Running Inference:  90%|████████▉ | 448/500 [28:48<03:28,  4.01s/it]Running Inference:  90%|████████▉ | 449/500 [28:52<03:25,  4.02s/it]Running Inference:  90%|█████████ | 450/500 [28:56<03:21,  4.02s/it]Running Inference:  90%|█████████ | 451/500 [29:00<03:16,  4.01s/it]Running Inference:  90%|█████████ | 452/500 [29:04<03:12,  4.02s/it]Running Inference:  91%|█████████ | 453/500 [29:08<03:09,  4.04s/it]Running Inference:  91%|█████████ | 454/500 [29:12<03:05,  4.03s/it]Running Inference:  91%|█████████ | 455/500 [29:16<03:00,  4.00s/it]Running Inference:  91%|█████████ | 456/500 [29:20<02:55,  3.99s/it]Running Inference:  91%|█████████▏| 457/500 [29:24<02:52,  4.01s/it]Running Inference:  92%|█████████▏| 458/500 [29:28<02:48,  4.01s/it]Running Inference:  92%|█████████▏| 459/500 [29:32<02:47,  4.10s/it]Running Inference:  92%|█████████▏| 460/500 [29:36<02:46,  4.16s/it]Running Inference:  92%|█████████▏| 461/500 [29:40<02:41,  4.13s/it]Running Inference:  92%|█████████▏| 462/500 [29:45<02:40,  4.22s/it]Running Inference:  93%|█████████▎| 463/500 [29:49<02:33,  4.14s/it]Running Inference:  93%|█████████▎| 464/500 [29:53<02:26,  4.08s/it]Running Inference:  93%|█████████▎| 465/500 [29:57<02:21,  4.04s/it]Running Inference:  93%|█████████▎| 466/500 [30:01<02:17,  4.04s/it]Running Inference:  93%|█████████▎| 467/500 [30:05<02:12,  4.00s/it]Running Inference:  94%|█████████▎| 468/500 [30:09<02:09,  4.06s/it]Running Inference:  94%|█████████▍| 469/500 [30:13<02:04,  4.03s/it]Running Inference:  94%|█████████▍| 470/500 [30:17<02:00,  4.00s/it]Running Inference:  94%|█████████▍| 471/500 [30:21<01:57,  4.04s/it]Running Inference:  94%|█████████▍| 472/500 [30:25<01:52,  4.02s/it]Running Inference:  95%|█████████▍| 473/500 [30:29<01:50,  4.10s/it]Running Inference:  95%|█████████▍| 474/500 [30:33<01:46,  4.08s/it]Running Inference:  95%|█████████▌| 475/500 [30:37<01:41,  4.06s/it]Running Inference:  95%|█████████▌| 476/500 [30:41<01:37,  4.05s/it]Running Inference:  95%|█████████▌| 477/500 [30:45<01:32,  4.04s/it]Running Inference:  96%|█████████▌| 478/500 [30:49<01:29,  4.09s/it]Running Inference:  96%|█████████▌| 479/500 [30:55<01:32,  4.38s/it]Running Inference:  96%|█████████▌| 480/500 [30:59<01:26,  4.30s/it]Running Inference:  96%|█████████▌| 481/500 [31:03<01:24,  4.45s/it]Running Inference:  96%|█████████▋| 482/500 [31:07<01:17,  4.30s/it]Running Inference:  97%|█████████▋| 483/500 [31:11<01:11,  4.21s/it]Running Inference:  97%|█████████▋| 484/500 [31:16<01:07,  4.22s/it]Running Inference:  97%|█████████▋| 485/500 [31:21<01:08,  4.57s/it]Running Inference:  97%|█████████▋| 486/500 [31:25<01:01,  4.39s/it]Running Inference:  97%|█████████▋| 487/500 [31:29<00:55,  4.28s/it]Running Inference:  98%|█████████▊| 488/500 [31:33<00:49,  4.17s/it]Running Inference:  98%|█████████▊| 489/500 [31:37<00:45,  4.15s/it]Running Inference:  98%|█████████▊| 490/500 [31:41<00:41,  4.14s/it]Running Inference:  98%|█████████▊| 491/500 [31:45<00:36,  4.10s/it]Running Inference:  98%|█████████▊| 492/500 [31:49<00:32,  4.10s/it]Running Inference:  99%|█████████▊| 493/500 [31:53<00:28,  4.06s/it]Running Inference:  99%|█████████▉| 494/500 [31:57<00:24,  4.07s/it]Running Inference:  99%|█████████▉| 495/500 [32:01<00:20,  4.06s/it]Running Inference:  99%|█████████▉| 496/500 [32:05<00:16,  4.07s/it]Running Inference:  99%|█████████▉| 497/500 [32:09<00:12,  4.05s/it]Running Inference: 100%|█████████▉| 498/500 [32:14<00:08,  4.06s/it]Running Inference: 100%|█████████▉| 499/500 [32:18<00:04,  4.04s/it]Running Inference: 100%|██████████| 500/500 [32:22<00:00,  4.12s/it]Running Inference: 100%|██████████| 500/500 [32:22<00:00,  3.88s/it]
2025-12-14 20:38:22,214 - INFO - Inference completed.
2025-12-14 20:38:22,230 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 20:38:22,230 - INFO - Calculating metrics for dataset: longbench
2025-12-14 20:38:22,232 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 20:38:22,232 - INFO - Metrics:
38.02
2025-12-14 20:38:22,234 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lcc, ratio=0.3) on GPU 2

----------------------------------------
Task: lcc | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 20:38:28,842 - INFO - Set deterministic seeds to 42
2025-12-14 20:38:28,842 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lcc",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 20:38:28,842 - INFO - Starting evaluation run...
2025-12-14 20:38:28,842 - INFO - Output directory set to: longbenchresult
2025-12-14 20:38:28,842 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 20:38:28,842 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 20:38:28,842 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 20:38:28,842 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.43it/s]
Device set to use cuda:0
2025-12-14 20:38:48,707 - INFO - Model pipeline loaded.
2025-12-14 20:38:48,707 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lcc)
2025-12-14 20:39:15,392 - INFO - Dataset loaded with 500 entries.
2025-12-14 20:39:15,392 - INFO - Dataset processed with 500 entries.
2025-12-14 20:39:15,406 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:04<40:45,  4.90s/it]Running Inference:   0%|          | 2/500 [00:08<35:42,  4.30s/it]Running Inference:   1%|          | 3/500 [00:13<35:40,  4.31s/it]Running Inference:   1%|          | 4/500 [00:16<34:09,  4.13s/it]Running Inference:   1%|          | 5/500 [00:20<33:20,  4.04s/it]Running Inference:   1%|          | 6/500 [00:24<33:20,  4.05s/it]Running Inference:   1%|▏         | 7/500 [00:28<33:00,  4.02s/it]Running Inference:   2%|▏         | 8/500 [00:32<32:24,  3.95s/it]Running Inference:   2%|▏         | 9/500 [00:36<32:08,  3.93s/it]Running Inference:   2%|▏         | 10/500 [00:40<31:48,  3.89s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:44<32:43,  4.02s/it]Running Inference:   2%|▏         | 12/500 [00:48<32:20,  3.98s/it]Running Inference:   3%|▎         | 13/500 [00:52<31:58,  3.94s/it]Running Inference:   3%|▎         | 14/500 [00:56<31:43,  3.92s/it]Running Inference:   3%|▎         | 15/500 [01:00<31:31,  3.90s/it]Running Inference:   3%|▎         | 16/500 [01:03<31:18,  3.88s/it]Running Inference:   3%|▎         | 17/500 [01:08<32:57,  4.09s/it]Running Inference:   4%|▎         | 18/500 [01:12<32:34,  4.05s/it]Running Inference:   4%|▍         | 19/500 [01:16<32:04,  4.00s/it]Running Inference:   4%|▍         | 20/500 [01:17<24:28,  3.06s/it]Running Inference:   4%|▍         | 21/500 [01:21<26:19,  3.30s/it]Running Inference:   4%|▍         | 22/500 [01:24<27:33,  3.46s/it]Running Inference:   5%|▍         | 23/500 [01:28<28:21,  3.57s/it]Running Inference:   5%|▍         | 24/500 [01:32<28:54,  3.64s/it]Running Inference:   5%|▌         | 25/500 [01:36<29:24,  3.72s/it]Running Inference:   5%|▌         | 26/500 [01:40<29:49,  3.78s/it]Running Inference:   5%|▌         | 27/500 [01:44<30:06,  3.82s/it]Running Inference:   6%|▌         | 28/500 [01:49<33:12,  4.22s/it]Running Inference:   6%|▌         | 29/500 [01:53<32:49,  4.18s/it]Running Inference:   6%|▌         | 30/500 [01:58<33:25,  4.27s/it]Running Inference:   6%|▌         | 31/500 [02:01<32:36,  4.17s/it]Running Inference:   6%|▋         | 32/500 [02:05<31:56,  4.10s/it]Running Inference:   7%|▋         | 33/500 [02:09<31:19,  4.02s/it]Running Inference:   7%|▋         | 34/500 [02:13<30:49,  3.97s/it]Running Inference:   7%|▋         | 35/500 [02:17<30:30,  3.94s/it]Running Inference:   7%|▋         | 36/500 [02:21<31:11,  4.03s/it]Running Inference:   7%|▋         | 37/500 [02:23<25:52,  3.35s/it]Running Inference:   8%|▊         | 38/500 [02:27<27:01,  3.51s/it]Running Inference:   8%|▊         | 39/500 [02:31<28:42,  3.74s/it]Running Inference:   8%|▊         | 40/500 [02:35<28:53,  3.77s/it]Running Inference:   8%|▊         | 41/500 [02:39<29:08,  3.81s/it]Running Inference:   8%|▊         | 42/500 [02:44<31:08,  4.08s/it]Running Inference:   9%|▊         | 43/500 [02:48<32:15,  4.23s/it]Running Inference:   9%|▉         | 44/500 [02:52<31:35,  4.16s/it]Running Inference:   9%|▉         | 45/500 [02:56<31:00,  4.09s/it]Running Inference:   9%|▉         | 46/500 [03:00<30:16,  4.00s/it]Running Inference:   9%|▉         | 47/500 [03:04<30:06,  3.99s/it]Running Inference:  10%|▉         | 48/500 [03:08<29:35,  3.93s/it]Running Inference:  10%|▉         | 49/500 [03:12<29:41,  3.95s/it]Running Inference:  10%|█         | 50/500 [03:16<31:02,  4.14s/it]Running Inference:  10%|█         | 51/500 [03:20<30:20,  4.05s/it]Running Inference:  10%|█         | 52/500 [03:24<29:56,  4.01s/it]Running Inference:  11%|█         | 53/500 [03:28<29:41,  3.99s/it]Running Inference:  11%|█         | 54/500 [03:32<29:22,  3.95s/it]Running Inference:  11%|█         | 55/500 [03:36<29:02,  3.92s/it]Running Inference:  11%|█         | 56/500 [03:39<28:51,  3.90s/it]Running Inference:  11%|█▏        | 57/500 [03:48<38:32,  5.22s/it]Running Inference:  12%|█▏        | 58/500 [03:52<35:26,  4.81s/it]Running Inference:  12%|█▏        | 59/500 [03:52<26:03,  3.55s/it]Running Inference:  12%|█▏        | 60/500 [03:55<25:25,  3.47s/it]Running Inference:  12%|█▏        | 61/500 [03:59<26:24,  3.61s/it]Running Inference:  12%|█▏        | 62/500 [04:03<26:55,  3.69s/it]Running Inference:  13%|█▎        | 63/500 [04:07<27:21,  3.76s/it]Running Inference:  13%|█▎        | 64/500 [04:09<22:15,  3.06s/it]Running Inference:  13%|█▎        | 65/500 [04:13<23:56,  3.30s/it]Running Inference:  13%|█▎        | 66/500 [04:17<25:42,  3.55s/it]Running Inference:  13%|█▎        | 67/500 [04:21<26:26,  3.66s/it]Running Inference:  14%|█▎        | 68/500 [04:24<26:50,  3.73s/it]Running Inference:  14%|█▍        | 69/500 [04:28<27:02,  3.76s/it]Running Inference:  14%|█▍        | 70/500 [04:32<27:15,  3.80s/it]Running Inference:  14%|█▍        | 71/500 [04:33<20:23,  2.85s/it]Running Inference:  14%|█▍        | 72/500 [04:37<22:59,  3.22s/it]Running Inference:  15%|█▍        | 73/500 [04:39<19:42,  2.77s/it]Running Inference:  15%|█▍        | 74/500 [04:43<22:05,  3.11s/it]Running Inference:  15%|█▌        | 75/500 [04:47<24:11,  3.42s/it]Running Inference:  15%|█▌        | 76/500 [04:51<25:23,  3.59s/it]Running Inference:  15%|█▌        | 77/500 [04:55<26:00,  3.69s/it]Running Inference:  16%|█▌        | 78/500 [04:58<25:39,  3.65s/it]Running Inference:  16%|█▌        | 79/500 [05:02<25:55,  3.69s/it]Running Inference:  16%|█▌        | 80/500 [05:06<26:20,  3.76s/it]Running Inference:  16%|█▌        | 81/500 [05:10<27:22,  3.92s/it]Running Inference:  16%|█▋        | 82/500 [05:15<28:14,  4.05s/it]Running Inference:  17%|█▋        | 83/500 [05:16<22:32,  3.24s/it]Running Inference:  17%|█▋        | 84/500 [05:20<23:53,  3.45s/it]Running Inference:  17%|█▋        | 85/500 [05:25<26:46,  3.87s/it]Running Inference:  17%|█▋        | 86/500 [05:28<26:37,  3.86s/it]Running Inference:  17%|█▋        | 87/500 [05:32<26:29,  3.85s/it]Running Inference:  18%|█▊        | 88/500 [05:36<26:27,  3.85s/it]Running Inference:  18%|█▊        | 89/500 [05:40<26:42,  3.90s/it]Running Inference:  18%|█▊        | 90/500 [05:44<27:13,  3.98s/it]Running Inference:  18%|█▊        | 91/500 [05:48<27:23,  4.02s/it]Running Inference:  18%|█▊        | 92/500 [05:52<27:10,  4.00s/it]Running Inference:  19%|█▊        | 93/500 [05:54<21:26,  3.16s/it]Running Inference:  19%|█▉        | 94/500 [05:57<22:47,  3.37s/it]Running Inference:  19%|█▉        | 95/500 [06:01<23:46,  3.52s/it]Running Inference:  19%|█▉        | 96/500 [06:05<24:32,  3.64s/it]Running Inference:  19%|█▉        | 97/500 [06:09<24:55,  3.71s/it]Running Inference:  20%|█▉        | 98/500 [06:13<25:11,  3.76s/it]Running Inference:  20%|█▉        | 99/500 [06:17<25:14,  3.78s/it]Running Inference:  20%|██        | 100/500 [06:19<21:53,  3.28s/it]Running Inference:  20%|██        | 101/500 [06:20<17:02,  2.56s/it]Running Inference:  20%|██        | 102/500 [06:24<19:30,  2.94s/it]Running Inference:  21%|██        | 103/500 [06:27<21:12,  3.21s/it]Running Inference:  21%|██        | 104/500 [06:33<26:18,  3.99s/it]Running Inference:  21%|██        | 105/500 [06:38<27:32,  4.18s/it]Running Inference:  21%|██        | 106/500 [06:43<29:56,  4.56s/it]Running Inference:  21%|██▏       | 107/500 [06:47<28:58,  4.42s/it]Running Inference:  22%|██▏       | 108/500 [06:51<27:56,  4.28s/it]Running Inference:  22%|██▏       | 109/500 [06:55<27:04,  4.15s/it]Running Inference:  22%|██▏       | 110/500 [06:59<26:30,  4.08s/it]Running Inference:  22%|██▏       | 111/500 [07:03<25:51,  3.99s/it]Running Inference:  22%|██▏       | 112/500 [07:07<25:28,  3.94s/it]Running Inference:  23%|██▎       | 113/500 [07:11<25:49,  4.00s/it]Running Inference:  23%|██▎       | 114/500 [07:16<27:57,  4.35s/it]Running Inference:  23%|██▎       | 115/500 [07:20<27:31,  4.29s/it]Running Inference:  23%|██▎       | 116/500 [07:24<27:11,  4.25s/it]Running Inference:  23%|██▎       | 117/500 [07:28<26:09,  4.10s/it]Running Inference:  24%|██▎       | 118/500 [07:33<26:43,  4.20s/it]Running Inference:  24%|██▍       | 119/500 [07:36<25:47,  4.06s/it]Running Inference:  24%|██▍       | 120/500 [07:40<25:54,  4.09s/it]Running Inference:  24%|██▍       | 121/500 [07:44<25:28,  4.03s/it]Running Inference:  24%|██▍       | 122/500 [07:48<24:52,  3.95s/it]Running Inference:  25%|██▍       | 123/500 [07:52<24:31,  3.90s/it]Running Inference:  25%|██▍       | 124/500 [07:56<24:14,  3.87s/it]Running Inference:  25%|██▌       | 125/500 [08:00<24:02,  3.85s/it]Running Inference:  25%|██▌       | 126/500 [08:03<23:51,  3.83s/it]Running Inference:  25%|██▌       | 127/500 [08:05<18:55,  3.04s/it]Running Inference:  26%|██▌       | 128/500 [08:08<20:36,  3.32s/it]Running Inference:  26%|██▌       | 129/500 [08:12<21:24,  3.46s/it]Running Inference:  26%|██▌       | 130/500 [08:16<22:09,  3.59s/it]Running Inference:  26%|██▌       | 131/500 [08:20<22:29,  3.66s/it]Running Inference:  26%|██▋       | 132/500 [08:24<22:39,  3.69s/it]Running Inference:  27%|██▋       | 133/500 [08:28<22:58,  3.76s/it]Running Inference:  27%|██▋       | 134/500 [08:31<23:03,  3.78s/it]Running Inference:  27%|██▋       | 135/500 [08:35<23:14,  3.82s/it]Running Inference:  27%|██▋       | 136/500 [08:39<23:18,  3.84s/it]Running Inference:  27%|██▋       | 137/500 [08:43<23:04,  3.81s/it]Running Inference:  28%|██▊       | 138/500 [08:47<23:12,  3.85s/it]Running Inference:  28%|██▊       | 139/500 [08:51<22:58,  3.82s/it]Running Inference:  28%|██▊       | 140/500 [08:55<23:04,  3.85s/it]Running Inference:  28%|██▊       | 141/500 [08:59<23:44,  3.97s/it]Running Inference:  28%|██▊       | 142/500 [09:03<23:51,  4.00s/it]Running Inference:  29%|██▊       | 143/500 [09:04<19:16,  3.24s/it]Running Inference:  29%|██▉       | 144/500 [09:08<20:10,  3.40s/it]Running Inference:  29%|██▉       | 145/500 [09:12<21:06,  3.57s/it]Running Inference:  29%|██▉       | 146/500 [09:16<21:52,  3.71s/it]Running Inference:  29%|██▉       | 147/500 [09:20<22:28,  3.82s/it]Running Inference:  30%|██▉       | 148/500 [09:24<22:17,  3.80s/it]Running Inference:  30%|██▉       | 149/500 [09:28<22:12,  3.80s/it]Running Inference:  30%|███       | 150/500 [09:32<22:03,  3.78s/it]Running Inference:  30%|███       | 151/500 [09:35<22:12,  3.82s/it]Running Inference:  30%|███       | 152/500 [09:39<22:05,  3.81s/it]Running Inference:  31%|███       | 153/500 [09:43<22:00,  3.81s/it]Running Inference:  31%|███       | 154/500 [09:47<21:55,  3.80s/it]Running Inference:  31%|███       | 155/500 [09:51<21:46,  3.79s/it]Running Inference:  31%|███       | 156/500 [09:54<21:44,  3.79s/it]Running Inference:  31%|███▏      | 157/500 [09:58<22:01,  3.85s/it]Running Inference:  32%|███▏      | 158/500 [10:02<21:56,  3.85s/it]Running Inference:  32%|███▏      | 159/500 [10:06<21:46,  3.83s/it]Running Inference:  32%|███▏      | 160/500 [10:10<21:38,  3.82s/it]Running Inference:  32%|███▏      | 161/500 [10:14<21:36,  3.82s/it]Running Inference:  32%|███▏      | 162/500 [10:18<21:42,  3.85s/it]Running Inference:  33%|███▎      | 163/500 [10:21<21:27,  3.82s/it]Running Inference:  33%|███▎      | 164/500 [10:25<21:24,  3.82s/it]Running Inference:  33%|███▎      | 165/500 [10:29<21:25,  3.84s/it]Running Inference:  33%|███▎      | 166/500 [10:33<21:12,  3.81s/it]Running Inference:  33%|███▎      | 167/500 [10:37<21:45,  3.92s/it]Running Inference:  34%|███▎      | 168/500 [10:41<21:33,  3.90s/it]Running Inference:  34%|███▍      | 169/500 [10:45<21:28,  3.89s/it]Running Inference:  34%|███▍      | 170/500 [10:48<21:13,  3.86s/it]Running Inference:  34%|███▍      | 171/500 [10:52<21:15,  3.88s/it]Running Inference:  34%|███▍      | 172/500 [10:56<21:02,  3.85s/it]Running Inference:  35%|███▍      | 173/500 [11:00<20:57,  3.85s/it]Running Inference:  35%|███▍      | 174/500 [11:04<21:17,  3.92s/it]Running Inference:  35%|███▌      | 175/500 [11:06<18:43,  3.46s/it]Running Inference:  35%|███▌      | 176/500 [11:10<19:16,  3.57s/it]Running Inference:  35%|███▌      | 177/500 [11:15<20:42,  3.85s/it]Running Inference:  36%|███▌      | 178/500 [11:19<20:26,  3.81s/it]Running Inference:  36%|███▌      | 179/500 [11:22<20:20,  3.80s/it]Running Inference:  36%|███▌      | 180/500 [11:26<20:23,  3.82s/it]Running Inference:  36%|███▌      | 181/500 [11:30<20:12,  3.80s/it]Running Inference:  36%|███▋      | 182/500 [11:34<20:04,  3.79s/it]Running Inference:  37%|███▋      | 183/500 [11:38<20:18,  3.84s/it]Running Inference:  37%|███▋      | 184/500 [11:42<20:19,  3.86s/it]Running Inference:  37%|███▋      | 185/500 [11:45<20:21,  3.88s/it]Running Inference:  37%|███▋      | 186/500 [11:49<20:16,  3.88s/it]Running Inference:  37%|███▋      | 187/500 [11:53<20:17,  3.89s/it]Running Inference:  38%|███▊      | 188/500 [11:57<20:04,  3.86s/it]Running Inference:  38%|███▊      | 189/500 [12:01<20:01,  3.86s/it]Running Inference:  38%|███▊      | 190/500 [12:05<20:26,  3.96s/it]Running Inference:  38%|███▊      | 191/500 [12:09<20:03,  3.90s/it]Running Inference:  38%|███▊      | 192/500 [12:13<19:57,  3.89s/it]Running Inference:  39%|███▊      | 193/500 [12:17<19:50,  3.88s/it]Running Inference:  39%|███▉      | 194/500 [12:20<19:41,  3.86s/it]Running Inference:  39%|███▉      | 195/500 [12:24<19:36,  3.86s/it]Running Inference:  39%|███▉      | 196/500 [12:28<19:28,  3.84s/it]Running Inference:  39%|███▉      | 197/500 [12:32<19:32,  3.87s/it]Running Inference:  40%|███▉      | 198/500 [12:36<19:17,  3.83s/it]Running Inference:  40%|███▉      | 199/500 [12:39<19:06,  3.81s/it]Running Inference:  40%|████      | 200/500 [12:43<19:10,  3.83s/it]Running Inference:  40%|████      | 201/500 [12:47<19:18,  3.87s/it]Running Inference:  40%|████      | 202/500 [12:51<19:06,  3.85s/it]Running Inference:  41%|████      | 203/500 [12:56<21:08,  4.27s/it]Running Inference:  41%|████      | 204/500 [13:00<20:27,  4.15s/it]Running Inference:  41%|████      | 205/500 [13:04<19:51,  4.04s/it]Running Inference:  41%|████      | 206/500 [13:08<19:22,  3.95s/it]Running Inference:  41%|████▏     | 207/500 [13:12<19:06,  3.91s/it]Running Inference:  42%|████▏     | 208/500 [13:16<19:24,  3.99s/it]Running Inference:  42%|████▏     | 209/500 [13:20<19:45,  4.07s/it]Running Inference:  42%|████▏     | 210/500 [13:24<19:16,  3.99s/it]Running Inference:  42%|████▏     | 211/500 [13:28<19:15,  4.00s/it]Running Inference:  42%|████▏     | 212/500 [13:32<18:51,  3.93s/it]Running Inference:  43%|████▎     | 213/500 [13:35<18:29,  3.86s/it]Running Inference:  43%|████▎     | 214/500 [13:39<18:25,  3.86s/it]Running Inference:  43%|████▎     | 215/500 [13:43<18:23,  3.87s/it]Running Inference:  43%|████▎     | 216/500 [13:47<18:12,  3.85s/it]Running Inference:  43%|████▎     | 217/500 [13:51<18:27,  3.91s/it]Running Inference:  44%|████▎     | 218/500 [13:55<18:31,  3.94s/it]Running Inference:  44%|████▍     | 219/500 [13:59<18:12,  3.89s/it]Running Inference:  44%|████▍     | 220/500 [14:03<18:04,  3.87s/it]Running Inference:  44%|████▍     | 221/500 [14:06<17:50,  3.84s/it]Running Inference:  44%|████▍     | 222/500 [14:10<17:39,  3.81s/it]Running Inference:  45%|████▍     | 223/500 [14:14<17:34,  3.81s/it]Running Inference:  45%|████▍     | 224/500 [14:18<17:31,  3.81s/it]Running Inference:  45%|████▌     | 225/500 [14:21<17:22,  3.79s/it]Running Inference:  45%|████▌     | 226/500 [14:25<17:15,  3.78s/it]Running Inference:  45%|████▌     | 227/500 [14:29<17:15,  3.79s/it]Running Inference:  46%|████▌     | 228/500 [14:33<17:06,  3.77s/it]Running Inference:  46%|████▌     | 229/500 [14:36<17:00,  3.77s/it]Running Inference:  46%|████▌     | 230/500 [14:40<16:56,  3.77s/it]Running Inference:  46%|████▌     | 231/500 [14:44<16:55,  3.77s/it]Running Inference:  46%|████▋     | 232/500 [14:48<16:53,  3.78s/it]Running Inference:  47%|████▋     | 233/500 [14:52<16:59,  3.82s/it]Running Inference:  47%|████▋     | 234/500 [14:56<16:58,  3.83s/it]Running Inference:  47%|████▋     | 235/500 [15:00<17:20,  3.93s/it]Running Inference:  47%|████▋     | 236/500 [15:03<17:01,  3.87s/it]Running Inference:  47%|████▋     | 237/500 [15:07<16:48,  3.83s/it]Running Inference:  48%|████▊     | 238/500 [15:11<16:42,  3.83s/it]Running Inference:  48%|████▊     | 239/500 [15:15<16:44,  3.85s/it]Running Inference:  48%|████▊     | 240/500 [15:19<16:37,  3.84s/it]Running Inference:  48%|████▊     | 241/500 [15:23<16:32,  3.83s/it]Running Inference:  48%|████▊     | 242/500 [15:26<16:20,  3.80s/it]Running Inference:  49%|████▊     | 243/500 [15:31<17:08,  4.00s/it]Running Inference:  49%|████▉     | 244/500 [15:35<17:23,  4.08s/it]Running Inference:  49%|████▉     | 245/500 [15:39<17:07,  4.03s/it]Running Inference:  49%|████▉     | 246/500 [15:43<16:40,  3.94s/it]Running Inference:  49%|████▉     | 247/500 [15:47<16:33,  3.93s/it]Running Inference:  50%|████▉     | 248/500 [15:50<16:20,  3.89s/it]Running Inference:  50%|████▉     | 249/500 [15:54<16:12,  3.88s/it]Running Inference:  50%|█████     | 250/500 [15:58<16:08,  3.88s/it]Running Inference:  50%|█████     | 251/500 [16:02<16:22,  3.94s/it]Running Inference:  50%|█████     | 252/500 [16:07<17:00,  4.11s/it]Running Inference:  51%|█████     | 253/500 [16:11<16:33,  4.02s/it]Running Inference:  51%|█████     | 254/500 [16:14<16:14,  3.96s/it]Running Inference:  51%|█████     | 255/500 [16:18<16:10,  3.96s/it]Running Inference:  51%|█████     | 256/500 [16:22<15:54,  3.91s/it]Running Inference:  51%|█████▏    | 257/500 [16:26<15:56,  3.93s/it]Running Inference:  52%|█████▏    | 258/500 [16:30<15:41,  3.89s/it]Running Inference:  52%|█████▏    | 259/500 [16:34<15:33,  3.88s/it]Running Inference:  52%|█████▏    | 260/500 [16:37<15:21,  3.84s/it]Running Inference:  52%|█████▏    | 261/500 [16:41<15:10,  3.81s/it]Running Inference:  52%|█████▏    | 262/500 [16:45<15:13,  3.84s/it]Running Inference:  53%|█████▎    | 263/500 [16:49<15:07,  3.83s/it]Running Inference:  53%|█████▎    | 264/500 [16:53<14:57,  3.80s/it]Running Inference:  53%|█████▎    | 265/500 [16:57<15:03,  3.85s/it]Running Inference:  53%|█████▎    | 266/500 [17:00<14:57,  3.84s/it]Running Inference:  53%|█████▎    | 267/500 [17:05<15:24,  3.97s/it]Running Inference:  54%|█████▎    | 268/500 [17:08<15:09,  3.92s/it]Running Inference:  54%|█████▍    | 269/500 [17:12<14:59,  3.89s/it]Running Inference:  54%|█████▍    | 270/500 [17:16<14:48,  3.86s/it]Running Inference:  54%|█████▍    | 271/500 [17:20<14:37,  3.83s/it]Running Inference:  54%|█████▍    | 272/500 [17:24<14:32,  3.83s/it]Running Inference:  55%|█████▍    | 273/500 [17:28<14:34,  3.85s/it]Running Inference:  55%|█████▍    | 274/500 [17:31<14:26,  3.83s/it]Running Inference:  55%|█████▌    | 275/500 [17:35<14:18,  3.81s/it]Running Inference:  55%|█████▌    | 276/500 [17:39<14:14,  3.82s/it]Running Inference:  55%|█████▌    | 277/500 [17:43<14:09,  3.81s/it]Running Inference:  56%|█████▌    | 278/500 [17:47<14:10,  3.83s/it]Running Inference:  56%|█████▌    | 279/500 [17:51<14:16,  3.87s/it]Running Inference:  56%|█████▌    | 280/500 [17:55<14:24,  3.93s/it]Running Inference:  56%|█████▌    | 281/500 [17:59<14:16,  3.91s/it]Running Inference:  56%|█████▋    | 282/500 [18:02<14:07,  3.89s/it]Running Inference:  57%|█████▋    | 283/500 [18:06<13:57,  3.86s/it]Running Inference:  57%|█████▋    | 284/500 [18:10<13:56,  3.87s/it]Running Inference:  57%|█████▋    | 285/500 [18:14<13:53,  3.87s/it]Running Inference:  57%|█████▋    | 286/500 [18:18<13:40,  3.83s/it]Running Inference:  57%|█████▋    | 287/500 [18:22<14:25,  4.06s/it]Running Inference:  58%|█████▊    | 288/500 [18:26<14:20,  4.06s/it]Running Inference:  58%|█████▊    | 289/500 [18:30<13:59,  3.98s/it]Running Inference:  58%|█████▊    | 290/500 [18:34<13:51,  3.96s/it]Running Inference:  58%|█████▊    | 291/500 [18:38<13:48,  3.96s/it]Running Inference:  58%|█████▊    | 292/500 [18:42<13:29,  3.89s/it]Running Inference:  59%|█████▊    | 293/500 [18:46<13:26,  3.90s/it]Running Inference:  59%|█████▉    | 294/500 [18:49<13:13,  3.85s/it]Running Inference:  59%|█████▉    | 295/500 [18:53<13:04,  3.83s/it]Running Inference:  59%|█████▉    | 296/500 [18:57<12:59,  3.82s/it]Running Inference:  59%|█████▉    | 297/500 [19:01<13:00,  3.85s/it]Running Inference:  60%|█████▉    | 298/500 [19:05<12:50,  3.81s/it]Running Inference:  60%|█████▉    | 299/500 [19:08<12:41,  3.79s/it]Running Inference:  60%|██████    | 300/500 [19:12<12:32,  3.76s/it]Running Inference:  60%|██████    | 301/500 [19:16<12:46,  3.85s/it]Running Inference:  60%|██████    | 302/500 [19:20<12:39,  3.83s/it]Running Inference:  61%|██████    | 303/500 [19:24<12:33,  3.83s/it]Running Inference:  61%|██████    | 304/500 [19:28<12:58,  3.97s/it]Running Inference:  61%|██████    | 305/500 [19:32<12:40,  3.90s/it]Running Inference:  61%|██████    | 306/500 [19:36<12:57,  4.01s/it]Running Inference:  61%|██████▏   | 307/500 [19:40<12:43,  3.95s/it]Running Inference:  62%|██████▏   | 308/500 [19:44<12:26,  3.89s/it]Running Inference:  62%|██████▏   | 309/500 [19:47<12:14,  3.84s/it]Running Inference:  62%|██████▏   | 310/500 [19:51<12:06,  3.82s/it]Running Inference:  62%|██████▏   | 311/500 [19:55<12:07,  3.85s/it]Running Inference:  62%|██████▏   | 312/500 [19:59<12:00,  3.83s/it]Running Inference:  63%|██████▎   | 313/500 [20:03<12:00,  3.86s/it]Running Inference:  63%|██████▎   | 314/500 [20:06<11:51,  3.82s/it]Running Inference:  63%|██████▎   | 315/500 [20:10<11:41,  3.79s/it]Running Inference:  63%|██████▎   | 316/500 [20:14<11:32,  3.76s/it]Running Inference:  63%|██████▎   | 317/500 [20:18<11:27,  3.76s/it]Running Inference:  64%|██████▎   | 318/500 [20:21<11:23,  3.76s/it]Running Inference:  64%|██████▍   | 319/500 [20:25<11:23,  3.78s/it]Running Inference:  64%|██████▍   | 320/500 [20:30<11:49,  3.94s/it]Running Inference:  64%|██████▍   | 321/500 [20:34<11:53,  3.99s/it]Running Inference:  64%|██████▍   | 322/500 [20:37<11:39,  3.93s/it]Running Inference:  65%|██████▍   | 323/500 [20:41<11:27,  3.89s/it]Running Inference:  65%|██████▍   | 324/500 [20:45<11:19,  3.86s/it]Running Inference:  65%|██████▌   | 325/500 [20:49<11:12,  3.85s/it]Running Inference:  65%|██████▌   | 326/500 [20:53<11:27,  3.95s/it]Running Inference:  65%|██████▌   | 327/500 [20:58<12:33,  4.35s/it]Running Inference:  66%|██████▌   | 328/500 [21:03<12:23,  4.32s/it]Running Inference:  66%|██████▌   | 329/500 [21:06<11:49,  4.15s/it]Running Inference:  66%|██████▌   | 330/500 [21:10<11:23,  4.02s/it]Running Inference:  66%|██████▌   | 331/500 [21:14<11:41,  4.15s/it]Running Inference:  66%|██████▋   | 332/500 [21:18<11:30,  4.11s/it]Running Inference:  67%|██████▋   | 333/500 [21:22<11:09,  4.01s/it]Running Inference:  67%|██████▋   | 334/500 [21:26<10:55,  3.95s/it]Running Inference:  67%|██████▋   | 335/500 [21:30<10:43,  3.90s/it]Running Inference:  67%|██████▋   | 336/500 [21:34<10:31,  3.85s/it]Running Inference:  67%|██████▋   | 337/500 [21:37<10:24,  3.83s/it]Running Inference:  68%|██████▊   | 338/500 [21:41<10:19,  3.82s/it]Running Inference:  68%|██████▊   | 339/500 [21:45<10:33,  3.93s/it]Running Inference:  68%|██████▊   | 340/500 [21:51<11:58,  4.49s/it]Running Inference:  68%|██████▊   | 341/500 [21:55<11:20,  4.28s/it]Running Inference:  68%|██████▊   | 342/500 [21:59<10:55,  4.15s/it]Running Inference:  69%|██████▊   | 343/500 [22:03<10:50,  4.14s/it]Running Inference:  69%|██████▉   | 344/500 [22:07<10:42,  4.12s/it]Running Inference:  69%|██████▉   | 345/500 [22:11<10:27,  4.05s/it]Running Inference:  69%|██████▉   | 346/500 [22:15<10:20,  4.03s/it]Running Inference:  69%|██████▉   | 347/500 [22:19<10:09,  3.98s/it]Running Inference:  70%|██████▉   | 348/500 [22:23<09:59,  3.95s/it]Running Inference:  70%|██████▉   | 349/500 [22:26<09:50,  3.91s/it]Running Inference:  70%|███████   | 350/500 [22:32<11:17,  4.52s/it]Running Inference:  70%|███████   | 351/500 [22:37<11:01,  4.44s/it]Running Inference:  70%|███████   | 352/500 [22:40<10:25,  4.23s/it]Running Inference:  71%|███████   | 353/500 [22:44<10:00,  4.08s/it]Running Inference:  71%|███████   | 354/500 [22:48<09:48,  4.03s/it]Running Inference:  71%|███████   | 355/500 [22:54<11:31,  4.77s/it]Running Inference:  71%|███████   | 356/500 [22:58<10:44,  4.48s/it]Running Inference:  71%|███████▏  | 357/500 [23:02<10:27,  4.39s/it]Running Inference:  72%|███████▏  | 358/500 [23:06<09:57,  4.21s/it]Running Inference:  72%|███████▏  | 359/500 [23:10<09:33,  4.06s/it]Running Inference:  72%|███████▏  | 360/500 [23:14<09:16,  3.98s/it]Running Inference:  72%|███████▏  | 361/500 [23:18<09:09,  3.95s/it]Running Inference:  72%|███████▏  | 362/500 [23:22<09:06,  3.96s/it]Running Inference:  73%|███████▎  | 363/500 [23:25<08:34,  3.75s/it]Running Inference:  73%|███████▎  | 364/500 [23:29<08:37,  3.81s/it]Running Inference:  73%|███████▎  | 365/500 [23:34<09:20,  4.15s/it]Running Inference:  73%|███████▎  | 366/500 [23:38<09:18,  4.17s/it]Running Inference:  73%|███████▎  | 367/500 [23:42<09:02,  4.08s/it]Running Inference:  74%|███████▎  | 368/500 [23:46<08:54,  4.05s/it]Running Inference:  74%|███████▍  | 369/500 [23:50<08:53,  4.08s/it]Running Inference:  74%|███████▍  | 370/500 [23:54<08:59,  4.15s/it]Running Inference:  74%|███████▍  | 371/500 [23:58<08:42,  4.05s/it]Running Inference:  74%|███████▍  | 372/500 [24:02<08:34,  4.02s/it]Running Inference:  75%|███████▍  | 373/500 [24:06<08:23,  3.96s/it]Running Inference:  75%|███████▍  | 374/500 [24:10<08:17,  3.95s/it]Running Inference:  75%|███████▌  | 375/500 [24:14<08:05,  3.88s/it]Running Inference:  75%|███████▌  | 376/500 [24:17<08:00,  3.87s/it]Running Inference:  75%|███████▌  | 377/500 [24:21<07:53,  3.85s/it]Running Inference:  76%|███████▌  | 378/500 [24:24<07:03,  3.47s/it]Running Inference:  76%|███████▌  | 379/500 [24:28<07:19,  3.63s/it]Running Inference:  76%|███████▌  | 380/500 [24:32<07:22,  3.68s/it]Running Inference:  76%|███████▌  | 381/500 [24:35<07:22,  3.72s/it]Running Inference:  76%|███████▋  | 382/500 [24:40<07:35,  3.86s/it]Running Inference:  77%|███████▋  | 383/500 [24:44<07:35,  3.89s/it]Running Inference:  77%|███████▋  | 384/500 [24:47<07:27,  3.86s/it]Running Inference:  77%|███████▋  | 385/500 [24:51<07:29,  3.91s/it]Running Inference:  77%|███████▋  | 386/500 [24:55<07:27,  3.93s/it]Running Inference:  77%|███████▋  | 387/500 [24:59<07:23,  3.92s/it]Running Inference:  78%|███████▊  | 388/500 [25:03<07:14,  3.88s/it]Running Inference:  78%|███████▊  | 389/500 [25:07<07:11,  3.89s/it]Running Inference:  78%|███████▊  | 390/500 [25:11<07:04,  3.86s/it]Running Inference:  78%|███████▊  | 391/500 [25:14<06:58,  3.84s/it]Running Inference:  78%|███████▊  | 392/500 [25:18<06:52,  3.82s/it]Running Inference:  79%|███████▊  | 393/500 [25:19<05:03,  2.84s/it]Running Inference:  79%|███████▉  | 394/500 [25:23<05:32,  3.14s/it]Running Inference:  79%|███████▉  | 395/500 [25:27<05:52,  3.35s/it]Running Inference:  79%|███████▉  | 396/500 [25:30<06:00,  3.46s/it]Running Inference:  79%|███████▉  | 397/500 [25:34<06:15,  3.65s/it]Running Inference:  80%|███████▉  | 398/500 [25:38<06:14,  3.67s/it]Running Inference:  80%|███████▉  | 399/500 [25:42<06:12,  3.69s/it]Running Inference:  80%|████████  | 400/500 [25:45<06:10,  3.70s/it]Running Inference:  80%|████████  | 401/500 [25:49<06:07,  3.71s/it]Running Inference:  80%|████████  | 402/500 [25:53<06:03,  3.71s/it]Running Inference:  81%|████████  | 403/500 [25:57<06:03,  3.75s/it]Running Inference:  81%|████████  | 404/500 [26:01<06:00,  3.75s/it]Running Inference:  81%|████████  | 405/500 [26:04<05:59,  3.78s/it]Running Inference:  81%|████████  | 406/500 [26:08<05:55,  3.78s/it]Running Inference:  81%|████████▏ | 407/500 [26:12<05:52,  3.79s/it]Running Inference:  82%|████████▏ | 408/500 [26:16<05:49,  3.80s/it]Running Inference:  82%|████████▏ | 409/500 [26:20<05:45,  3.79s/it]Running Inference:  82%|████████▏ | 410/500 [26:23<05:42,  3.81s/it]Running Inference:  82%|████████▏ | 411/500 [26:27<05:40,  3.83s/it]Running Inference:  82%|████████▏ | 412/500 [26:31<05:36,  3.82s/it]Running Inference:  83%|████████▎ | 413/500 [26:35<05:30,  3.79s/it]Running Inference:  83%|████████▎ | 414/500 [26:39<05:26,  3.80s/it]Running Inference:  83%|████████▎ | 415/500 [26:42<05:22,  3.79s/it]Running Inference:  83%|████████▎ | 416/500 [26:46<05:17,  3.77s/it]Running Inference:  83%|████████▎ | 417/500 [26:50<05:12,  3.77s/it]Running Inference:  84%|████████▎ | 418/500 [26:54<05:10,  3.79s/it]Running Inference:  84%|████████▍ | 419/500 [26:57<05:05,  3.77s/it]Running Inference:  84%|████████▍ | 420/500 [27:01<05:02,  3.78s/it]Running Inference:  84%|████████▍ | 421/500 [27:05<04:58,  3.78s/it]Running Inference:  84%|████████▍ | 422/500 [27:09<04:53,  3.77s/it]Running Inference:  85%|████████▍ | 423/500 [27:10<03:48,  2.96s/it]Running Inference:  85%|████████▍ | 424/500 [27:14<04:03,  3.21s/it]Running Inference:  85%|████████▌ | 425/500 [27:18<04:19,  3.46s/it]Running Inference:  85%|████████▌ | 426/500 [27:21<04:22,  3.55s/it]Running Inference:  85%|████████▌ | 427/500 [27:25<04:22,  3.59s/it]Running Inference:  86%|████████▌ | 428/500 [27:29<04:21,  3.63s/it]Running Inference:  86%|████████▌ | 429/500 [27:33<04:19,  3.66s/it]Running Inference:  86%|████████▌ | 430/500 [27:37<04:26,  3.81s/it]Running Inference:  86%|████████▌ | 431/500 [27:41<04:23,  3.82s/it]Running Inference:  86%|████████▋ | 432/500 [27:45<04:24,  3.89s/it]Running Inference:  87%|████████▋ | 433/500 [27:49<04:23,  3.94s/it]Running Inference:  87%|████████▋ | 434/500 [27:53<04:20,  3.94s/it]Running Inference:  87%|████████▋ | 435/500 [27:56<04:12,  3.89s/it]Running Inference:  87%|████████▋ | 436/500 [28:00<04:08,  3.88s/it]Running Inference:  87%|████████▋ | 437/500 [28:04<04:03,  3.86s/it]Running Inference:  88%|████████▊ | 438/500 [28:08<03:56,  3.82s/it]Running Inference:  88%|████████▊ | 439/500 [28:12<03:53,  3.83s/it]Running Inference:  88%|████████▊ | 440/500 [28:15<03:49,  3.83s/it]Running Inference:  88%|████████▊ | 441/500 [28:19<03:43,  3.79s/it]Running Inference:  88%|████████▊ | 442/500 [28:23<03:38,  3.77s/it]Running Inference:  89%|████████▊ | 443/500 [28:27<03:34,  3.77s/it]Running Inference:  89%|████████▉ | 444/500 [28:31<03:32,  3.80s/it]Running Inference:  89%|████████▉ | 445/500 [28:34<03:30,  3.83s/it]Running Inference:  89%|████████▉ | 446/500 [28:38<03:27,  3.85s/it]Running Inference:  89%|████████▉ | 447/500 [28:42<03:22,  3.82s/it]Running Inference:  90%|████████▉ | 448/500 [28:46<03:19,  3.83s/it]Running Inference:  90%|████████▉ | 449/500 [28:50<03:14,  3.82s/it]Running Inference:  90%|█████████ | 450/500 [28:54<03:11,  3.82s/it]Running Inference:  90%|█████████ | 451/500 [28:57<03:06,  3.81s/it]Running Inference:  90%|█████████ | 452/500 [29:01<03:02,  3.81s/it]Running Inference:  91%|█████████ | 453/500 [29:05<02:59,  3.83s/it]Running Inference:  91%|█████████ | 454/500 [29:09<02:55,  3.81s/it]Running Inference:  91%|█████████ | 455/500 [29:13<02:50,  3.78s/it]Running Inference:  91%|█████████ | 456/500 [29:16<02:45,  3.76s/it]Running Inference:  91%|█████████▏| 457/500 [29:20<02:42,  3.78s/it]Running Inference:  92%|█████████▏| 458/500 [29:24<02:38,  3.78s/it]Running Inference:  92%|█████████▏| 459/500 [29:28<02:38,  3.87s/it]Running Inference:  92%|█████████▏| 460/500 [29:32<02:37,  3.94s/it]Running Inference:  92%|█████████▏| 461/500 [29:36<02:32,  3.92s/it]Running Inference:  92%|█████████▏| 462/500 [29:40<02:31,  4.00s/it]Running Inference:  93%|█████████▎| 463/500 [29:44<02:25,  3.92s/it]Running Inference:  93%|█████████▎| 464/500 [29:48<02:18,  3.86s/it]Running Inference:  93%|█████████▎| 465/500 [29:51<02:13,  3.82s/it]Running Inference:  93%|█████████▎| 466/500 [29:55<02:09,  3.82s/it]Running Inference:  93%|█████████▎| 467/500 [29:59<02:04,  3.78s/it]Running Inference:  94%|█████████▎| 468/500 [30:03<02:02,  3.84s/it]Running Inference:  94%|█████████▍| 469/500 [30:06<01:57,  3.81s/it]Running Inference:  94%|█████████▍| 470/500 [30:10<01:53,  3.78s/it]Running Inference:  94%|█████████▍| 471/500 [30:14<01:50,  3.81s/it]Running Inference:  94%|█████████▍| 472/500 [30:18<01:46,  3.79s/it]Running Inference:  95%|█████████▍| 473/500 [30:22<01:44,  3.87s/it]Running Inference:  95%|█████████▍| 474/500 [30:26<01:40,  3.85s/it]Running Inference:  95%|█████████▌| 475/500 [30:29<01:35,  3.83s/it]Running Inference:  95%|█████████▌| 476/500 [30:33<01:31,  3.83s/it]Running Inference:  95%|█████████▌| 477/500 [30:37<01:27,  3.82s/it]Running Inference:  96%|█████████▌| 478/500 [30:41<01:25,  3.87s/it]Running Inference:  96%|█████████▌| 479/500 [30:46<01:27,  4.16s/it]Running Inference:  96%|█████████▌| 480/500 [30:50<01:21,  4.08s/it]Running Inference:  96%|█████████▌| 481/500 [30:54<01:20,  4.22s/it]Running Inference:  96%|█████████▋| 482/500 [30:58<01:13,  4.08s/it]Running Inference:  97%|█████████▋| 483/500 [31:02<01:07,  4.00s/it]Running Inference:  97%|█████████▋| 484/500 [31:06<01:04,  4.00s/it]Running Inference:  97%|█████████▋| 485/500 [31:11<01:05,  4.35s/it]Running Inference:  97%|█████████▋| 486/500 [31:15<00:58,  4.17s/it]Running Inference:  97%|█████████▋| 487/500 [31:19<00:52,  4.06s/it]Running Inference:  98%|█████████▊| 488/500 [31:22<00:47,  3.95s/it]Running Inference:  98%|█████████▊| 489/500 [31:26<00:43,  3.93s/it]Running Inference:  98%|█████████▊| 490/500 [31:30<00:39,  3.91s/it]Running Inference:  98%|█████████▊| 491/500 [31:34<00:34,  3.88s/it]Running Inference:  98%|█████████▊| 492/500 [31:38<00:31,  3.88s/it]Running Inference:  99%|█████████▊| 493/500 [31:41<00:26,  3.83s/it]Running Inference:  99%|█████████▉| 494/500 [31:45<00:23,  3.85s/it]Running Inference:  99%|█████████▉| 495/500 [31:49<00:19,  3.84s/it]Running Inference:  99%|█████████▉| 496/500 [31:53<00:15,  3.85s/it]Running Inference:  99%|█████████▉| 497/500 [31:57<00:11,  3.83s/it]Running Inference: 100%|█████████▉| 498/500 [32:01<00:07,  3.83s/it]Running Inference: 100%|█████████▉| 499/500 [32:04<00:03,  3.81s/it]Running Inference: 100%|██████████| 500/500 [32:09<00:00,  3.89s/it]Running Inference: 100%|██████████| 500/500 [32:09<00:00,  3.86s/it]
2025-12-14 21:11:24,444 - INFO - Inference completed.
2025-12-14 21:11:24,461 - INFO - Results saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 21:11:24,461 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:11:24,463 - INFO - Metrics saved to longbenchresult/longbench__lcc__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 21:11:24,463 - INFO - Metrics:
36.18
2025-12-14 21:11:24,464 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lcc, ratio=0.5) on GPU 2


========================================
LongBench Task: lsht
========================================
----------------------------------------
Task: lsht | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:11:30,897 - INFO - Set deterministic seeds to 42
2025-12-14 21:11:30,897 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:11:30,897 - INFO - Starting evaluation run...
2025-12-14 21:11:30,897 - INFO - Output directory set to: longbenchresult
2025-12-14 21:11:30,897 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 21:11:30,897 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 21:11:30,897 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:11:30,897 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.54it/s]
Device set to use cuda:0
2025-12-14 21:11:48,253 - INFO - Model pipeline loaded.
2025-12-14 21:11:48,253 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 21:11:54,069 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:11:54,069 - INFO - Dataset processed with 200 entries.
2025-12-14 21:11:54,103 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<17:42,  5.34s/it]Running Inference:   1%|          | 2/200 [00:10<18:13,  5.52s/it]Running Inference:   2%|▏         | 3/200 [00:11<11:17,  3.44s/it]Running Inference:   2%|▏         | 4/200 [00:16<12:53,  3.95s/it]Running Inference:   2%|▎         | 5/200 [00:19<11:37,  3.58s/it]Running Inference:   3%|▎         | 6/200 [00:24<13:28,  4.17s/it]Running Inference:   4%|▎         | 7/200 [00:30<14:50,  4.61s/it]Running Inference:   4%|▍         | 8/200 [00:34<14:36,  4.56s/it]Running Inference:   4%|▍         | 9/200 [00:37<12:08,  3.81s/it]Running Inference:   5%|▌         | 10/200 [00:38<09:31,  3.01s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:43<11:43,  3.72s/it]Running Inference:   6%|▌         | 12/200 [00:45<09:26,  3.01s/it]Running Inference:   6%|▋         | 13/200 [00:50<11:58,  3.84s/it]Running Inference:   7%|▋         | 14/200 [00:52<09:34,  3.09s/it]Running Inference:   8%|▊         | 15/200 [00:55<09:28,  3.07s/it]Running Inference:   8%|▊         | 16/200 [00:59<10:52,  3.54s/it]Running Inference:   8%|▊         | 17/200 [01:06<13:33,  4.45s/it]Running Inference:   9%|▉         | 18/200 [01:11<14:36,  4.81s/it]Running Inference:  10%|▉         | 19/200 [01:13<11:14,  3.73s/it]Running Inference:  10%|█         | 20/200 [01:15<09:59,  3.33s/it]Running Inference:  10%|█         | 21/200 [01:21<11:53,  3.99s/it]Running Inference:  11%|█         | 22/200 [01:26<12:51,  4.33s/it]Running Inference:  12%|█▏        | 23/200 [01:27<09:49,  3.33s/it]Running Inference:  12%|█▏        | 24/200 [01:33<12:16,  4.18s/it]Running Inference:  12%|█▎        | 25/200 [01:37<12:09,  4.17s/it]Running Inference:  13%|█▎        | 26/200 [01:43<14:01,  4.84s/it]Running Inference:  14%|█▎        | 27/200 [01:46<12:14,  4.25s/it]Running Inference:  14%|█▍        | 28/200 [01:47<09:24,  3.28s/it]Running Inference:  14%|█▍        | 29/200 [01:52<10:20,  3.63s/it]Running Inference:  15%|█▌        | 30/200 [01:58<12:15,  4.33s/it]Running Inference:  16%|█▌        | 31/200 [02:04<13:31,  4.80s/it]Running Inference:  16%|█▌        | 32/200 [02:11<15:10,  5.42s/it]Running Inference:  16%|█▋        | 33/200 [02:17<15:34,  5.59s/it]Running Inference:  17%|█▋        | 34/200 [02:22<15:05,  5.46s/it]Running Inference:  18%|█▊        | 35/200 [02:29<16:48,  6.11s/it]Running Inference:  18%|█▊        | 36/200 [02:35<16:07,  5.90s/it]Running Inference:  18%|█▊        | 37/200 [02:37<13:03,  4.81s/it]Running Inference:  19%|█▉        | 38/200 [02:42<13:30,  5.01s/it]Running Inference:  20%|█▉        | 39/200 [02:48<13:34,  5.06s/it]Running Inference:  20%|██        | 40/200 [02:54<14:50,  5.56s/it]Running Inference:  20%|██        | 41/200 [03:01<15:28,  5.84s/it]Running Inference:  21%|██        | 42/200 [03:06<14:50,  5.63s/it]Running Inference:  22%|██▏       | 43/200 [03:11<14:07,  5.40s/it]Running Inference:  22%|██▏       | 44/200 [03:12<11:04,  4.26s/it]Running Inference:  22%|██▎       | 45/200 [03:17<11:33,  4.47s/it]Running Inference:  23%|██▎       | 46/200 [03:24<12:49,  5.00s/it]Running Inference:  24%|██▎       | 47/200 [03:29<12:39,  4.96s/it]Running Inference:  24%|██▍       | 48/200 [03:30<09:45,  3.85s/it]Running Inference:  24%|██▍       | 49/200 [03:36<11:25,  4.54s/it]Running Inference:  25%|██▌       | 50/200 [03:41<11:29,  4.60s/it]Running Inference:  26%|██▌       | 51/200 [03:42<08:39,  3.49s/it]Running Inference:  26%|██▌       | 52/200 [03:43<07:07,  2.89s/it]Running Inference:  26%|██▋       | 53/200 [03:48<08:34,  3.50s/it]Running Inference:  27%|██▋       | 54/200 [03:53<09:48,  4.03s/it]Running Inference:  28%|██▊       | 55/200 [04:00<11:39,  4.82s/it]Running Inference:  28%|██▊       | 56/200 [04:06<12:32,  5.23s/it]Running Inference:  28%|██▊       | 57/200 [04:12<13:02,  5.47s/it]Running Inference:  29%|██▉       | 58/200 [04:18<13:15,  5.60s/it]Running Inference:  30%|██▉       | 59/200 [04:22<12:18,  5.24s/it]Running Inference:  30%|███       | 60/200 [04:28<12:37,  5.41s/it]Running Inference:  30%|███       | 61/200 [04:29<09:21,  4.04s/it]Running Inference:  31%|███       | 62/200 [04:35<10:40,  4.64s/it]Running Inference:  32%|███▏      | 63/200 [04:40<10:33,  4.62s/it]Running Inference:  32%|███▏      | 64/200 [04:44<10:16,  4.53s/it]Running Inference:  32%|███▎      | 65/200 [04:48<10:01,  4.45s/it]Running Inference:  33%|███▎      | 66/200 [04:54<10:57,  4.91s/it]Running Inference:  34%|███▎      | 67/200 [04:55<08:15,  3.73s/it]Running Inference:  34%|███▍      | 68/200 [04:56<06:16,  2.85s/it]Running Inference:  34%|███▍      | 69/200 [05:02<08:09,  3.73s/it]Running Inference:  35%|███▌      | 70/200 [05:03<06:20,  2.93s/it]Running Inference:  36%|███▌      | 71/200 [05:04<04:58,  2.32s/it]Running Inference:  36%|███▌      | 72/200 [05:09<06:43,  3.16s/it]Running Inference:  36%|███▋      | 73/200 [05:14<08:15,  3.90s/it]Running Inference:  37%|███▋      | 74/200 [05:21<10:00,  4.76s/it]Running Inference:  38%|███▊      | 75/200 [05:26<09:57,  4.78s/it]Running Inference:  38%|███▊      | 76/200 [05:32<10:30,  5.09s/it]Running Inference:  38%|███▊      | 77/200 [05:34<08:17,  4.05s/it]Running Inference:  39%|███▉      | 78/200 [05:35<06:44,  3.32s/it]Running Inference:  40%|███▉      | 79/200 [05:36<05:16,  2.62s/it]Running Inference:  40%|████      | 80/200 [05:39<05:09,  2.58s/it]Running Inference:  40%|████      | 81/200 [05:44<06:46,  3.41s/it]Running Inference:  41%|████      | 82/200 [05:48<06:50,  3.48s/it]Running Inference:  42%|████▏     | 83/200 [05:53<07:40,  3.93s/it]Running Inference:  42%|████▏     | 84/200 [05:58<08:15,  4.27s/it]Running Inference:  42%|████▎     | 85/200 [06:00<07:18,  3.82s/it]Running Inference:  43%|████▎     | 86/200 [06:05<07:54,  4.16s/it]Running Inference:  44%|████▎     | 87/200 [06:11<08:30,  4.52s/it]Running Inference:  44%|████▍     | 88/200 [06:16<08:52,  4.75s/it]Running Inference:  44%|████▍     | 89/200 [06:21<08:42,  4.70s/it]Running Inference:  45%|████▌     | 90/200 [06:23<07:20,  4.00s/it]Running Inference:  46%|████▌     | 91/200 [06:28<07:54,  4.36s/it]Running Inference:  46%|████▌     | 92/200 [06:34<08:22,  4.66s/it]Running Inference:  46%|████▋     | 93/200 [06:39<08:52,  4.98s/it]Running Inference:  47%|████▋     | 94/200 [06:44<08:36,  4.88s/it]Running Inference:  48%|████▊     | 95/200 [06:50<09:14,  5.28s/it]Running Inference:  48%|████▊     | 96/200 [06:56<09:15,  5.34s/it]Running Inference:  48%|████▊     | 97/200 [07:02<09:52,  5.76s/it]Running Inference:  49%|████▉     | 98/200 [07:04<07:27,  4.39s/it]Running Inference:  50%|████▉     | 99/200 [07:09<08:03,  4.78s/it]Running Inference:  50%|█████     | 100/200 [07:10<06:01,  3.62s/it]Running Inference:  50%|█████     | 101/200 [07:15<06:49,  4.14s/it]Running Inference:  51%|█████     | 102/200 [07:16<05:09,  3.16s/it]Running Inference:  52%|█████▏    | 103/200 [07:23<06:57,  4.31s/it]Running Inference:  52%|█████▏    | 104/200 [07:30<08:03,  5.04s/it]Running Inference:  52%|█████▎    | 105/200 [07:35<07:59,  5.05s/it]Running Inference:  53%|█████▎    | 106/200 [07:42<08:52,  5.66s/it]Running Inference:  54%|█████▎    | 107/200 [07:47<08:13,  5.31s/it]Running Inference:  54%|█████▍    | 108/200 [07:52<08:01,  5.23s/it]Running Inference:  55%|█████▍    | 109/200 [07:53<06:13,  4.11s/it]Running Inference:  55%|█████▌    | 110/200 [07:58<06:38,  4.42s/it]Running Inference:  56%|█████▌    | 111/200 [08:04<07:01,  4.73s/it]Running Inference:  56%|█████▌    | 112/200 [08:09<07:18,  4.98s/it]Running Inference:  56%|█████▋    | 113/200 [08:14<07:14,  5.00s/it]Running Inference:  57%|█████▋    | 114/200 [08:19<07:00,  4.89s/it]Running Inference:  57%|█████▊    | 115/200 [08:25<07:18,  5.15s/it]Running Inference:  58%|█████▊    | 116/200 [08:31<07:37,  5.44s/it]Running Inference:  58%|█████▊    | 117/200 [08:38<08:04,  5.84s/it]Running Inference:  59%|█████▉    | 118/200 [08:39<06:11,  4.54s/it]Running Inference:  60%|█████▉    | 119/200 [08:44<06:01,  4.47s/it]Running Inference:  60%|██████    | 120/200 [08:49<06:13,  4.66s/it]Running Inference:  60%|██████    | 121/200 [08:55<06:36,  5.02s/it]Running Inference:  61%|██████    | 122/200 [09:00<06:31,  5.01s/it]Running Inference:  62%|██████▏   | 123/200 [09:00<04:46,  3.72s/it]Running Inference:  62%|██████▏   | 124/200 [09:03<04:23,  3.46s/it]Running Inference:  62%|██████▎   | 125/200 [09:08<04:57,  3.97s/it]Running Inference:  63%|██████▎   | 126/200 [09:13<05:11,  4.20s/it]Running Inference:  64%|██████▎   | 127/200 [09:18<05:16,  4.33s/it]Running Inference:  64%|██████▍   | 128/200 [09:24<06:04,  5.07s/it]Running Inference:  64%|██████▍   | 129/200 [09:29<05:44,  4.85s/it]Running Inference:  65%|██████▌   | 130/200 [09:34<05:52,  5.04s/it]Running Inference:  66%|██████▌   | 131/200 [09:35<04:27,  3.88s/it]Running Inference:  66%|██████▌   | 132/200 [09:40<04:39,  4.12s/it]Running Inference:  66%|██████▋   | 133/200 [09:45<04:53,  4.38s/it]Running Inference:  67%|██████▋   | 134/200 [09:46<03:48,  3.46s/it]Running Inference:  68%|██████▊   | 135/200 [09:51<04:10,  3.86s/it]Running Inference:  68%|██████▊   | 136/200 [09:52<03:14,  3.04s/it]Running Inference:  68%|██████▊   | 137/200 [09:59<04:28,  4.26s/it]Running Inference:  69%|██████▉   | 138/200 [10:05<04:50,  4.69s/it]Running Inference:  70%|██████▉   | 139/200 [10:12<05:19,  5.24s/it]Running Inference:  70%|███████   | 140/200 [10:17<05:08,  5.14s/it]Running Inference:  70%|███████   | 141/200 [10:22<05:16,  5.36s/it]Running Inference:  71%|███████   | 142/200 [10:27<05:02,  5.22s/it]Running Inference:  72%|███████▏  | 143/200 [10:32<04:40,  4.92s/it]Running Inference:  72%|███████▏  | 144/200 [10:37<04:45,  5.09s/it]Running Inference:  72%|███████▎  | 145/200 [10:43<04:48,  5.25s/it]Running Inference:  73%|███████▎  | 146/200 [10:48<04:38,  5.15s/it]Running Inference:  74%|███████▎  | 147/200 [10:54<04:52,  5.52s/it]Running Inference:  74%|███████▍  | 148/200 [11:00<04:50,  5.59s/it]Running Inference:  74%|███████▍  | 149/200 [11:01<03:39,  4.31s/it]Running Inference:  75%|███████▌  | 150/200 [11:06<03:45,  4.51s/it]Running Inference:  76%|███████▌  | 151/200 [11:12<04:00,  4.90s/it]Running Inference:  76%|███████▌  | 152/200 [11:13<02:58,  3.71s/it]Running Inference:  76%|███████▋  | 153/200 [11:19<03:35,  4.59s/it]Running Inference:  77%|███████▋  | 154/200 [11:24<03:26,  4.49s/it]Running Inference:  78%|███████▊  | 155/200 [11:29<03:28,  4.64s/it]Running Inference:  78%|███████▊  | 156/200 [11:34<03:28,  4.73s/it]Running Inference:  78%|███████▊  | 157/200 [11:35<02:36,  3.64s/it]Running Inference:  79%|███████▉  | 158/200 [11:36<01:57,  2.80s/it]Running Inference:  80%|███████▉  | 159/200 [11:40<02:21,  3.45s/it]Running Inference:  80%|████████  | 160/200 [11:45<02:34,  3.85s/it]Running Inference:  80%|████████  | 161/200 [11:51<02:53,  4.46s/it]Running Inference:  81%|████████  | 162/200 [11:56<02:48,  4.45s/it]Running Inference:  82%|████████▏ | 163/200 [12:00<02:49,  4.58s/it]Running Inference:  82%|████████▏ | 164/200 [12:02<02:11,  3.66s/it]Running Inference:  82%|████████▎ | 165/200 [12:07<02:22,  4.08s/it]Running Inference:  83%|████████▎ | 166/200 [12:08<01:47,  3.15s/it]Running Inference:  84%|████████▎ | 167/200 [12:13<01:58,  3.58s/it]Running Inference:  84%|████████▍ | 168/200 [12:14<01:30,  2.83s/it]Running Inference:  84%|████████▍ | 169/200 [12:20<02:02,  3.95s/it]Running Inference:  85%|████████▌ | 170/200 [12:21<01:31,  3.04s/it]Running Inference:  86%|████████▌ | 171/200 [12:26<01:44,  3.59s/it]Running Inference:  86%|████████▌ | 172/200 [12:31<01:48,  3.87s/it]Running Inference:  86%|████████▋ | 173/200 [12:36<01:59,  4.43s/it]Running Inference:  87%|████████▋ | 174/200 [12:42<02:03,  4.74s/it]Running Inference:  88%|████████▊ | 175/200 [12:47<02:01,  4.85s/it]Running Inference:  88%|████████▊ | 176/200 [12:52<01:56,  4.85s/it]Running Inference:  88%|████████▊ | 177/200 [12:57<01:55,  5.01s/it]Running Inference:  89%|████████▉ | 178/200 [13:02<01:50,  5.04s/it]Running Inference:  90%|████████▉ | 179/200 [13:03<01:22,  3.91s/it]Running Inference:  90%|█████████ | 180/200 [13:08<01:22,  4.14s/it]Running Inference:  90%|█████████ | 181/200 [13:13<01:24,  4.47s/it]Running Inference:  91%|█████████ | 182/200 [13:19<01:27,  4.85s/it]Running Inference:  92%|█████████▏| 183/200 [13:24<01:22,  4.83s/it]Running Inference:  92%|█████████▏| 184/200 [13:28<01:13,  4.61s/it]Running Inference:  92%|█████████▎| 185/200 [13:29<00:53,  3.60s/it]Running Inference:  93%|█████████▎| 186/200 [13:35<00:59,  4.22s/it]Running Inference:  94%|█████████▎| 187/200 [13:41<01:03,  4.91s/it]Running Inference:  94%|█████████▍| 188/200 [13:49<01:07,  5.60s/it]Running Inference:  94%|█████████▍| 189/200 [13:54<01:00,  5.47s/it]Running Inference:  95%|█████████▌| 190/200 [13:59<00:52,  5.27s/it]Running Inference:  96%|█████████▌| 191/200 [14:05<00:49,  5.47s/it]Running Inference:  96%|█████████▌| 192/200 [14:09<00:40,  5.02s/it]Running Inference:  96%|█████████▋| 193/200 [14:14<00:36,  5.21s/it]Running Inference:  97%|█████████▋| 194/200 [14:20<00:31,  5.31s/it]Running Inference:  98%|█████████▊| 195/200 [14:26<00:27,  5.50s/it]Running Inference:  98%|█████████▊| 196/200 [14:32<00:23,  5.79s/it]Running Inference:  98%|█████████▊| 197/200 [14:38<00:17,  5.75s/it]Running Inference:  99%|█████████▉| 198/200 [14:42<00:10,  5.42s/it]Running Inference: 100%|█████████▉| 199/200 [14:49<00:05,  5.68s/it]Running Inference: 100%|██████████| 200/200 [14:54<00:00,  5.46s/it]Running Inference: 100%|██████████| 200/200 [14:54<00:00,  4.47s/it]
2025-12-14 21:26:48,288 - INFO - Inference completed.
2025-12-14 21:26:48,307 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 21:26:48,307 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:26:48,309 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 21:26:48,309 - INFO - Metrics:
10.85
2025-12-14 21:26:48,310 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lsht, ratio=0.1) on GPU 2

----------------------------------------
Task: lsht | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:26:54,803 - INFO - Set deterministic seeds to 42
2025-12-14 21:26:54,803 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:26:54,803 - INFO - Starting evaluation run...
2025-12-14 21:26:54,803 - INFO - Output directory set to: longbenchresult
2025-12-14 21:26:54,803 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 21:26:54,803 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 21:26:54,803 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:26:54,803 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.62it/s]
Device set to use cuda:0
2025-12-14 21:27:09,252 - INFO - Model pipeline loaded.
2025-12-14 21:27:09,252 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 21:27:16,027 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:27:16,028 - INFO - Dataset processed with 200 entries.
2025-12-14 21:27:16,062 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<17:50,  5.38s/it]Running Inference:   1%|          | 2/200 [00:11<18:18,  5.55s/it]Running Inference:   2%|▏         | 3/200 [00:15<16:14,  4.95s/it]Running Inference:   2%|▏         | 4/200 [00:17<12:23,  3.79s/it]Running Inference:   2%|▎         | 5/200 [00:23<15:10,  4.67s/it]Running Inference:   3%|▎         | 6/200 [00:28<15:52,  4.91s/it]Running Inference:   4%|▎         | 7/200 [00:34<16:28,  5.12s/it]Running Inference:   4%|▍         | 8/200 [00:38<15:46,  4.93s/it]Running Inference:   4%|▍         | 9/200 [00:44<16:16,  5.11s/it]Running Inference:   5%|▌         | 10/200 [00:45<12:21,  3.90s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:51<13:43,  4.36s/it]Running Inference:   6%|▌         | 12/200 [00:52<10:55,  3.48s/it]Running Inference:   6%|▋         | 13/200 [00:58<13:00,  4.17s/it]Running Inference:   7%|▋         | 14/200 [00:59<10:17,  3.32s/it]Running Inference:   8%|▊         | 15/200 [01:02<09:57,  3.23s/it]Running Inference:   8%|▊         | 16/200 [01:07<11:15,  3.67s/it]Running Inference:   8%|▊         | 17/200 [01:13<13:48,  4.53s/it]Running Inference:   9%|▉         | 18/200 [01:19<14:47,  4.88s/it]Running Inference:  10%|▉         | 19/200 [01:24<14:29,  4.80s/it]Running Inference:  10%|█         | 20/200 [01:29<15:11,  5.06s/it]Running Inference:  10%|█         | 21/200 [01:35<15:32,  5.21s/it]Running Inference:  11%|█         | 22/200 [01:40<15:26,  5.20s/it]Running Inference:  12%|█▏        | 23/200 [01:41<11:41,  3.96s/it]Running Inference:  12%|█▏        | 24/200 [01:47<13:34,  4.63s/it]Running Inference:  12%|█▎        | 25/200 [01:52<13:07,  4.50s/it]Running Inference:  13%|█▎        | 26/200 [01:58<14:40,  5.06s/it]Running Inference:  14%|█▎        | 27/200 [02:01<12:41,  4.40s/it]Running Inference:  14%|█▍        | 28/200 [02:02<09:43,  3.39s/it]Running Inference:  14%|█▍        | 29/200 [02:06<10:36,  3.72s/it]Running Inference:  15%|█▌        | 30/200 [02:12<12:26,  4.39s/it]Running Inference:  16%|█▌        | 31/200 [02:18<13:38,  4.84s/it]Running Inference:  16%|█▌        | 32/200 [02:25<15:13,  5.44s/it]Running Inference:  16%|█▋        | 33/200 [02:31<15:36,  5.61s/it]Running Inference:  17%|█▋        | 34/200 [02:36<15:09,  5.48s/it]Running Inference:  18%|█▊        | 35/200 [02:44<16:50,  6.13s/it]Running Inference:  18%|█▊        | 36/200 [02:49<16:10,  5.92s/it]Running Inference:  18%|█▊        | 37/200 [02:52<13:05,  4.82s/it]Running Inference:  19%|█▉        | 38/200 [02:57<13:34,  5.03s/it]Running Inference:  20%|█▉        | 39/200 [03:02<13:39,  5.09s/it]Running Inference:  20%|██        | 40/200 [03:09<14:54,  5.59s/it]Running Inference:  20%|██        | 41/200 [03:16<15:30,  5.85s/it]Running Inference:  21%|██        | 42/200 [03:21<14:54,  5.66s/it]Running Inference:  22%|██▏       | 43/200 [03:26<14:14,  5.44s/it]Running Inference:  22%|██▏       | 44/200 [03:31<13:44,  5.29s/it]Running Inference:  22%|██▎       | 45/200 [03:36<13:27,  5.21s/it]Running Inference:  23%|██▎       | 46/200 [03:42<14:07,  5.50s/it]Running Inference:  24%|██▎       | 47/200 [03:47<13:36,  5.34s/it]Running Inference:  24%|██▍       | 48/200 [03:51<12:56,  5.11s/it]Running Inference:  24%|██▍       | 49/200 [03:57<13:37,  5.41s/it]Running Inference:  25%|██▌       | 50/200 [04:02<13:03,  5.22s/it]Running Inference:  26%|██▌       | 51/200 [04:06<12:15,  4.94s/it]Running Inference:  26%|██▌       | 52/200 [04:08<09:37,  3.90s/it]Running Inference:  26%|██▋       | 53/200 [04:13<10:20,  4.22s/it]Running Inference:  27%|██▋       | 54/200 [04:18<11:01,  4.53s/it]Running Inference:  28%|██▊       | 55/200 [04:25<12:27,  5.16s/it]Running Inference:  28%|██▊       | 56/200 [04:31<13:01,  5.43s/it]Running Inference:  28%|██▊       | 57/200 [04:37<13:19,  5.59s/it]Running Inference:  29%|██▉       | 58/200 [04:43<13:26,  5.68s/it]Running Inference:  30%|██▉       | 59/200 [04:47<12:26,  5.29s/it]Running Inference:  30%|███       | 60/200 [04:53<12:42,  5.45s/it]Running Inference:  30%|███       | 61/200 [04:54<09:27,  4.08s/it]Running Inference:  31%|███       | 62/200 [05:00<10:44,  4.67s/it]Running Inference:  32%|███▏      | 63/200 [05:04<10:36,  4.65s/it]Running Inference:  32%|███▏      | 64/200 [05:09<10:20,  4.56s/it]Running Inference:  32%|███▎      | 65/200 [05:13<10:05,  4.48s/it]Running Inference:  33%|███▎      | 66/200 [05:19<11:00,  4.93s/it]Running Inference:  34%|███▎      | 67/200 [05:24<10:34,  4.77s/it]Running Inference:  34%|███▍      | 68/200 [05:24<07:52,  3.58s/it]Running Inference:  34%|███▍      | 69/200 [05:30<09:15,  4.24s/it]Running Inference:  35%|███▌      | 70/200 [05:35<09:18,  4.29s/it]Running Inference:  36%|███▌      | 71/200 [05:35<06:58,  3.24s/it]Running Inference:  36%|███▌      | 72/200 [05:40<08:07,  3.81s/it]Running Inference:  36%|███▋      | 73/200 [05:46<09:13,  4.36s/it]Running Inference:  37%|███▋      | 74/200 [05:53<10:39,  5.08s/it]Running Inference:  38%|███▊      | 75/200 [05:58<10:26,  5.01s/it]Running Inference:  38%|███▊      | 76/200 [06:03<10:50,  5.25s/it]Running Inference:  38%|███▊      | 77/200 [06:08<10:31,  5.14s/it]Running Inference:  39%|███▉      | 78/200 [06:13<10:11,  5.02s/it]Running Inference:  40%|███▉      | 79/200 [06:14<07:40,  3.81s/it]Running Inference:  40%|████      | 80/200 [06:19<08:29,  4.25s/it]Running Inference:  40%|████      | 81/200 [06:25<09:06,  4.59s/it]Running Inference:  41%|████      | 82/200 [06:31<10:11,  5.18s/it]Running Inference:  42%|████▏     | 83/200 [06:36<10:00,  5.13s/it]Running Inference:  42%|████▏     | 84/200 [06:41<09:54,  5.12s/it]Running Inference:  42%|████▎     | 85/200 [06:48<10:22,  5.41s/it]Running Inference:  43%|████▎     | 86/200 [06:52<10:02,  5.28s/it]Running Inference:  44%|████▎     | 87/200 [06:58<09:59,  5.31s/it]Running Inference:  44%|████▍     | 88/200 [07:03<09:55,  5.31s/it]Running Inference:  44%|████▍     | 89/200 [07:08<09:26,  5.11s/it]Running Inference:  45%|████▌     | 90/200 [07:13<09:38,  5.26s/it]Running Inference:  46%|████▌     | 91/200 [07:19<09:31,  5.24s/it]Running Inference:  46%|████▌     | 92/200 [07:24<09:30,  5.28s/it]Running Inference:  46%|████▋     | 93/200 [07:30<09:39,  5.42s/it]Running Inference:  47%|████▋     | 94/200 [07:34<09:10,  5.19s/it]Running Inference:  48%|████▊     | 95/200 [07:41<09:36,  5.49s/it]Running Inference:  48%|████▊     | 96/200 [07:46<09:31,  5.50s/it]Running Inference:  48%|████▊     | 97/200 [07:53<10:03,  5.86s/it]Running Inference:  49%|████▉     | 98/200 [07:54<07:37,  4.48s/it]Running Inference:  50%|████▉     | 99/200 [08:00<08:09,  4.85s/it]Running Inference:  50%|█████     | 100/200 [08:04<07:49,  4.69s/it]Running Inference:  50%|█████     | 101/200 [08:10<08:05,  4.90s/it]Running Inference:  51%|█████     | 102/200 [08:10<06:01,  3.69s/it]Running Inference:  52%|█████▏    | 103/200 [08:17<07:33,  4.67s/it]Running Inference:  52%|█████▏    | 104/200 [08:24<08:28,  5.30s/it]Running Inference:  52%|█████▎    | 105/200 [08:29<08:17,  5.24s/it]Running Inference:  53%|█████▎    | 106/200 [08:36<09:05,  5.80s/it]Running Inference:  54%|█████▎    | 107/200 [08:41<08:25,  5.43s/it]Running Inference:  54%|█████▍    | 108/200 [08:46<08:10,  5.33s/it]Running Inference:  55%|█████▍    | 109/200 [08:47<06:19,  4.17s/it]Running Inference:  55%|█████▌    | 110/200 [08:53<06:43,  4.48s/it]Running Inference:  56%|█████▌    | 111/200 [08:58<07:05,  4.78s/it]Running Inference:  56%|█████▌    | 112/200 [09:04<07:21,  5.02s/it]Running Inference:  56%|█████▋    | 113/200 [09:09<07:17,  5.03s/it]Running Inference:  57%|█████▋    | 114/200 [09:13<07:03,  4.92s/it]Running Inference:  57%|█████▊    | 115/200 [09:19<07:19,  5.18s/it]Running Inference:  58%|█████▊    | 116/200 [09:25<07:38,  5.46s/it]Running Inference:  58%|█████▊    | 117/200 [09:32<08:04,  5.84s/it]Running Inference:  59%|█████▉    | 118/200 [09:34<06:14,  4.56s/it]Running Inference:  60%|█████▉    | 119/200 [09:38<06:04,  4.50s/it]Running Inference:  60%|██████    | 120/200 [09:43<06:17,  4.72s/it]Running Inference:  60%|██████    | 121/200 [09:49<06:39,  5.06s/it]Running Inference:  61%|██████    | 122/200 [09:54<06:33,  5.04s/it]Running Inference:  62%|██████▏   | 123/200 [09:55<04:48,  3.74s/it]Running Inference:  62%|██████▏   | 124/200 [10:01<05:39,  4.46s/it]Running Inference:  62%|██████▎   | 125/200 [10:06<05:50,  4.68s/it]Running Inference:  63%|██████▎   | 126/200 [10:11<05:48,  4.71s/it]Running Inference:  64%|██████▎   | 127/200 [10:16<05:42,  4.69s/it]Running Inference:  64%|██████▍   | 128/200 [10:22<06:22,  5.31s/it]Running Inference:  64%|██████▍   | 129/200 [10:27<05:57,  5.03s/it]Running Inference:  65%|██████▌   | 130/200 [10:32<06:01,  5.17s/it]Running Inference:  66%|██████▌   | 131/200 [10:33<04:33,  3.96s/it]Running Inference:  66%|██████▌   | 132/200 [10:38<04:44,  4.19s/it]Running Inference:  66%|██████▋   | 133/200 [10:43<04:57,  4.44s/it]Running Inference:  67%|██████▋   | 134/200 [10:44<03:50,  3.50s/it]Running Inference:  68%|██████▊   | 135/200 [10:49<04:13,  3.89s/it]Running Inference:  68%|██████▊   | 136/200 [10:54<04:20,  4.07s/it]Running Inference:  68%|██████▊   | 137/200 [11:01<05:12,  4.97s/it]Running Inference:  69%|██████▉   | 138/200 [11:06<05:21,  5.18s/it]Running Inference:  70%|██████▉   | 139/200 [11:13<05:40,  5.58s/it]Running Inference:  70%|███████   | 140/200 [11:18<05:22,  5.38s/it]Running Inference:  70%|███████   | 141/200 [11:24<05:26,  5.53s/it]Running Inference:  71%|███████   | 142/200 [11:29<05:09,  5.34s/it]Running Inference:  72%|███████▏  | 143/200 [11:33<04:46,  5.02s/it]Running Inference:  72%|███████▏  | 144/200 [11:38<04:49,  5.16s/it]Running Inference:  72%|███████▎  | 145/200 [11:44<04:51,  5.30s/it]Running Inference:  73%|███████▎  | 146/200 [11:49<04:40,  5.19s/it]Running Inference:  74%|███████▎  | 147/200 [11:55<04:53,  5.54s/it]Running Inference:  74%|███████▍  | 148/200 [12:01<04:51,  5.61s/it]Running Inference:  74%|███████▍  | 149/200 [12:06<04:29,  5.28s/it]Running Inference:  75%|███████▌  | 150/200 [12:11<04:19,  5.20s/it]Running Inference:  76%|███████▌  | 151/200 [12:16<04:23,  5.37s/it]Running Inference:  76%|███████▌  | 152/200 [12:17<03:15,  4.06s/it]Running Inference:  76%|███████▋  | 153/200 [12:24<03:47,  4.84s/it]Running Inference:  77%|███████▋  | 154/200 [12:28<03:34,  4.67s/it]Running Inference:  78%|███████▊  | 155/200 [12:33<03:35,  4.78s/it]Running Inference:  78%|███████▊  | 156/200 [12:38<03:32,  4.84s/it]Running Inference:  78%|███████▊  | 157/200 [12:39<02:39,  3.72s/it]Running Inference:  79%|███████▉  | 158/200 [12:40<01:59,  2.85s/it]Running Inference:  80%|███████▉  | 159/200 [12:45<02:23,  3.49s/it]Running Inference:  80%|████████  | 160/200 [12:50<02:35,  3.90s/it]Running Inference:  80%|████████  | 161/200 [12:56<02:54,  4.49s/it]Running Inference:  81%|████████  | 162/200 [13:00<02:50,  4.48s/it]Running Inference:  82%|████████▏ | 163/200 [13:05<02:50,  4.61s/it]Running Inference:  82%|████████▏ | 164/200 [13:07<02:13,  3.70s/it]Running Inference:  82%|████████▎ | 165/200 [13:12<02:24,  4.12s/it]Running Inference:  83%|████████▎ | 166/200 [13:16<02:22,  4.18s/it]Running Inference:  84%|████████▎ | 167/200 [13:21<02:22,  4.32s/it]Running Inference:  84%|████████▍ | 168/200 [13:22<01:47,  3.34s/it]Running Inference:  84%|████████▍ | 169/200 [13:29<02:13,  4.30s/it]Running Inference:  85%|████████▌ | 170/200 [13:33<02:08,  4.29s/it]Running Inference:  86%|████████▌ | 171/200 [13:38<02:09,  4.48s/it]Running Inference:  86%|████████▌ | 172/200 [13:42<02:06,  4.52s/it]Running Inference:  86%|████████▋ | 173/200 [13:48<02:11,  4.87s/it]Running Inference:  87%|████████▋ | 174/200 [13:54<02:11,  5.06s/it]Running Inference:  88%|████████▊ | 175/200 [13:59<02:07,  5.08s/it]Running Inference:  88%|████████▊ | 176/200 [14:04<02:00,  5.02s/it]Running Inference:  88%|████████▊ | 177/200 [14:09<01:58,  5.14s/it]Running Inference:  89%|████████▉ | 178/200 [14:14<01:53,  5.14s/it]Running Inference:  90%|████████▉ | 179/200 [14:15<01:23,  3.98s/it]Running Inference:  90%|█████████ | 180/200 [14:20<01:24,  4.20s/it]Running Inference:  90%|█████████ | 181/200 [14:25<01:25,  4.52s/it]Running Inference:  91%|█████████ | 182/200 [14:31<01:27,  4.88s/it]Running Inference:  92%|█████████▏| 183/200 [14:36<01:22,  4.85s/it]Running Inference:  92%|█████████▏| 184/200 [14:40<01:14,  4.63s/it]Running Inference:  92%|█████████▎| 185/200 [14:45<01:09,  4.64s/it]Running Inference:  93%|█████████▎| 186/200 [14:50<01:09,  4.95s/it]Running Inference:  94%|█████████▎| 187/200 [14:57<01:10,  5.41s/it]Running Inference:  94%|█████████▍| 188/200 [15:04<01:11,  5.94s/it]Running Inference:  94%|█████████▍| 189/200 [15:09<01:02,  5.71s/it]Running Inference:  95%|█████████▌| 190/200 [15:14<00:54,  5.45s/it]Running Inference:  96%|█████████▌| 191/200 [15:20<00:50,  5.61s/it]Running Inference:  96%|█████████▌| 192/200 [15:24<00:41,  5.13s/it]Running Inference:  96%|█████████▋| 193/200 [15:30<00:37,  5.30s/it]Running Inference:  97%|█████████▋| 194/200 [15:35<00:32,  5.38s/it]Running Inference:  98%|█████████▊| 195/200 [15:41<00:27,  5.55s/it]Running Inference:  98%|█████████▊| 196/200 [15:48<00:23,  5.83s/it]Running Inference:  98%|█████████▊| 197/200 [15:53<00:17,  5.78s/it]Running Inference:  99%|█████████▉| 198/200 [15:58<00:10,  5.46s/it]Running Inference: 100%|█████████▉| 199/200 [16:04<00:05,  5.72s/it]Running Inference: 100%|██████████| 200/200 [16:09<00:00,  5.50s/it]Running Inference: 100%|██████████| 200/200 [16:09<00:00,  4.85s/it]
2025-12-14 21:43:25,928 - INFO - Inference completed.
2025-12-14 21:43:25,947 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 21:43:25,947 - INFO - Calculating metrics for dataset: longbench
2025-12-14 21:43:25,948 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 21:43:25,948 - INFO - Metrics:
7.87
2025-12-14 21:43:25,950 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lsht, ratio=0.2) on GPU 2

----------------------------------------
Task: lsht | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 21:43:32,537 - INFO - Set deterministic seeds to 42
2025-12-14 21:43:32,537 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 21:43:32,538 - INFO - Starting evaluation run...
2025-12-14 21:43:32,538 - INFO - Output directory set to: longbenchresult
2025-12-14 21:43:32,538 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 21:43:32,538 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 21:43:32,538 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 21:43:32,538 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.70it/s]
Device set to use cuda:0
2025-12-14 21:43:53,568 - INFO - Model pipeline loaded.
2025-12-14 21:43:53,569 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 21:44:13,470 - INFO - Dataset loaded with 200 entries.
2025-12-14 21:44:13,470 - INFO - Dataset processed with 200 entries.
2025-12-14 21:44:13,505 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<18:01,  5.44s/it]Running Inference:   1%|          | 2/200 [00:11<18:25,  5.58s/it]Running Inference:   2%|▏         | 3/200 [00:12<11:27,  3.49s/it]Running Inference:   2%|▏         | 4/200 [00:14<09:34,  2.93s/it]Running Inference:   2%|▎         | 5/200 [00:20<13:19,  4.10s/it]Running Inference:   3%|▎         | 6/200 [00:25<14:38,  4.53s/it]Running Inference:   4%|▎         | 7/200 [00:31<15:41,  4.88s/it]Running Inference:   4%|▍         | 8/200 [00:35<15:15,  4.77s/it]Running Inference:   4%|▍         | 9/200 [00:41<15:56,  5.01s/it]Running Inference:   5%|▌         | 10/200 [00:42<12:08,  3.83s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:47<13:34,  4.31s/it]Running Inference:   6%|▌         | 12/200 [00:49<10:48,  3.45s/it]Running Inference:   6%|▋         | 13/200 [00:55<12:57,  4.16s/it]Running Inference:   7%|▋         | 14/200 [00:56<10:15,  3.31s/it]Running Inference:   8%|▊         | 15/200 [00:59<09:55,  3.22s/it]Running Inference:   8%|▊         | 16/200 [01:04<11:17,  3.68s/it]Running Inference:   8%|▊         | 17/200 [01:10<13:46,  4.52s/it]Running Inference:   9%|▉         | 18/200 [01:16<14:48,  4.88s/it]Running Inference:  10%|▉         | 19/200 [01:21<14:30,  4.81s/it]Running Inference:  10%|█         | 20/200 [01:26<15:14,  5.08s/it]Running Inference:  10%|█         | 21/200 [01:32<15:36,  5.23s/it]Running Inference:  11%|█         | 22/200 [01:37<15:28,  5.22s/it]Running Inference:  12%|█▏        | 23/200 [01:38<11:43,  3.97s/it]Running Inference:  12%|█▏        | 24/200 [01:44<13:33,  4.62s/it]Running Inference:  12%|█▎        | 25/200 [01:49<13:10,  4.52s/it]Running Inference:  13%|█▎        | 26/200 [01:55<14:39,  5.06s/it]Running Inference:  14%|█▎        | 27/200 [02:01<15:26,  5.36s/it]Running Inference:  14%|█▍        | 28/200 [02:05<14:34,  5.08s/it]Running Inference:  14%|█▍        | 29/200 [02:10<13:57,  4.90s/it]Running Inference:  15%|█▌        | 30/200 [02:13<11:58,  4.23s/it]Running Inference:  16%|█▌        | 31/200 [02:18<13:15,  4.71s/it]Running Inference:  16%|█▌        | 32/200 [02:25<14:52,  5.31s/it]Running Inference:  16%|█▋        | 33/200 [02:31<15:18,  5.50s/it]Running Inference:  17%|█▋        | 34/200 [02:36<14:54,  5.39s/it]Running Inference:  18%|█▊        | 35/200 [02:44<16:34,  6.03s/it]Running Inference:  18%|█▊        | 36/200 [02:49<15:57,  5.84s/it]Running Inference:  18%|█▊        | 37/200 [02:55<15:34,  5.73s/it]Running Inference:  19%|█▉        | 38/200 [03:00<15:16,  5.65s/it]Running Inference:  20%|█▉        | 39/200 [03:05<14:47,  5.51s/it]Running Inference:  20%|██        | 40/200 [03:12<15:35,  5.85s/it]Running Inference:  20%|██        | 41/200 [03:18<15:54,  6.00s/it]Running Inference:  21%|██        | 42/200 [03:23<15:08,  5.75s/it]Running Inference:  22%|██▏       | 43/200 [03:28<14:20,  5.48s/it]Running Inference:  22%|██▏       | 44/200 [03:33<13:47,  5.30s/it]Running Inference:  22%|██▎       | 45/200 [03:35<10:48,  4.18s/it]Running Inference:  23%|██▎       | 46/200 [03:41<12:12,  4.76s/it]Running Inference:  24%|██▎       | 47/200 [03:46<12:13,  4.80s/it]Running Inference:  24%|██▍       | 48/200 [03:50<11:56,  4.71s/it]Running Inference:  24%|██▍       | 49/200 [03:56<12:51,  5.11s/it]Running Inference:  25%|██▌       | 50/200 [04:01<12:28,  4.99s/it]Running Inference:  26%|██▌       | 51/200 [04:05<11:48,  4.75s/it]Running Inference:  26%|██▌       | 52/200 [04:07<09:18,  3.77s/it]Running Inference:  26%|██▋       | 53/200 [04:12<10:05,  4.12s/it]Running Inference:  27%|██▋       | 54/200 [04:17<10:47,  4.44s/it]Running Inference:  28%|██▊       | 55/200 [04:23<12:13,  5.06s/it]Running Inference:  28%|██▊       | 56/200 [04:29<12:48,  5.33s/it]Running Inference:  28%|██▊       | 57/200 [04:35<13:08,  5.51s/it]Running Inference:  29%|██▉       | 58/200 [04:41<13:17,  5.61s/it]Running Inference:  30%|██▉       | 59/200 [04:45<12:19,  5.25s/it]Running Inference:  30%|███       | 60/200 [04:51<12:35,  5.40s/it]Running Inference:  30%|███       | 61/200 [04:55<11:32,  4.98s/it]Running Inference:  31%|███       | 62/200 [05:01<12:07,  5.27s/it]Running Inference:  32%|███▏      | 63/200 [05:06<11:32,  5.05s/it]Running Inference:  32%|███▏      | 64/200 [05:07<08:41,  3.84s/it]Running Inference:  32%|███▎      | 65/200 [05:11<08:55,  3.96s/it]Running Inference:  33%|███▎      | 66/200 [05:17<10:09,  4.55s/it]Running Inference:  34%|███▎      | 67/200 [05:21<09:56,  4.49s/it]Running Inference:  34%|███▍      | 68/200 [05:25<09:40,  4.40s/it]Running Inference:  34%|███▍      | 69/200 [05:31<10:28,  4.79s/it]Running Inference:  35%|███▌      | 70/200 [05:35<10:06,  4.67s/it]Running Inference:  36%|███▌      | 71/200 [05:36<07:31,  3.50s/it]Running Inference:  36%|███▌      | 72/200 [05:41<08:29,  3.98s/it]Running Inference:  36%|███▋      | 73/200 [05:44<07:23,  3.49s/it]Running Inference:  37%|███▋      | 74/200 [05:50<09:21,  4.45s/it]Running Inference:  38%|███▊      | 75/200 [05:55<09:29,  4.56s/it]Running Inference:  38%|███▊      | 76/200 [06:01<10:09,  4.91s/it]Running Inference:  38%|███▊      | 77/200 [06:06<10:00,  4.88s/it]Running Inference:  39%|███▉      | 78/200 [06:10<09:48,  4.82s/it]Running Inference:  40%|███▉      | 79/200 [06:15<09:23,  4.66s/it]Running Inference:  40%|████      | 80/200 [06:20<09:39,  4.83s/it]Running Inference:  40%|████      | 81/200 [06:25<09:53,  4.99s/it]Running Inference:  41%|████      | 82/200 [06:32<10:40,  5.43s/it]Running Inference:  42%|████▏     | 83/200 [06:37<10:19,  5.29s/it]Running Inference:  42%|████▏     | 84/200 [06:42<10:05,  5.22s/it]Running Inference:  42%|████▎     | 85/200 [06:45<08:34,  4.47s/it]Running Inference:  43%|████▎     | 86/200 [06:49<08:46,  4.61s/it]Running Inference:  44%|████▎     | 87/200 [06:55<09:05,  4.83s/it]Running Inference:  44%|████▍     | 88/200 [07:00<09:15,  4.96s/it]Running Inference:  44%|████▍     | 89/200 [07:05<08:58,  4.85s/it]Running Inference:  45%|████▌     | 90/200 [07:10<09:16,  5.06s/it]Running Inference:  46%|████▌     | 91/200 [07:15<09:15,  5.09s/it]Running Inference:  46%|████▌     | 92/200 [07:21<09:17,  5.17s/it]Running Inference:  46%|████▋     | 93/200 [07:26<09:29,  5.32s/it]Running Inference:  47%|████▋     | 94/200 [07:31<09:02,  5.12s/it]Running Inference:  48%|████▊     | 95/200 [07:37<09:29,  5.42s/it]Running Inference:  48%|████▊     | 96/200 [07:43<09:25,  5.43s/it]Running Inference:  48%|████▊     | 97/200 [07:49<09:55,  5.78s/it]Running Inference:  49%|████▉     | 98/200 [07:51<07:31,  4.43s/it]Running Inference:  50%|████▉     | 99/200 [07:56<08:04,  4.80s/it]Running Inference:  50%|█████     | 100/200 [08:00<07:44,  4.65s/it]Running Inference:  50%|█████     | 101/200 [08:06<08:01,  4.86s/it]Running Inference:  51%|█████     | 102/200 [08:10<07:36,  4.65s/it]Running Inference:  52%|█████▏    | 103/200 [08:17<08:35,  5.31s/it]Running Inference:  52%|█████▏    | 104/200 [08:23<09:07,  5.70s/it]Running Inference:  52%|█████▎    | 105/200 [08:28<08:42,  5.51s/it]Running Inference:  53%|█████▎    | 106/200 [08:35<09:17,  5.94s/it]Running Inference:  54%|█████▎    | 107/200 [08:40<08:31,  5.50s/it]Running Inference:  54%|█████▍    | 108/200 [08:45<08:12,  5.36s/it]Running Inference:  55%|█████▍    | 109/200 [08:50<07:51,  5.18s/it]Running Inference:  55%|█████▌    | 110/200 [08:55<07:45,  5.18s/it]Running Inference:  56%|█████▌    | 111/200 [09:00<07:47,  5.25s/it]Running Inference:  56%|█████▌    | 112/200 [09:06<07:49,  5.34s/it]Running Inference:  56%|█████▋    | 113/200 [09:11<07:35,  5.24s/it]Running Inference:  57%|█████▋    | 114/200 [09:15<07:15,  5.06s/it]Running Inference:  57%|█████▊    | 115/200 [09:21<07:26,  5.26s/it]Running Inference:  58%|█████▊    | 116/200 [09:27<07:40,  5.48s/it]Running Inference:  58%|█████▊    | 117/200 [09:34<08:03,  5.82s/it]Running Inference:  59%|█████▉    | 118/200 [09:35<06:12,  4.55s/it]Running Inference:  60%|█████▉    | 119/200 [09:40<06:02,  4.48s/it]Running Inference:  60%|██████    | 120/200 [09:45<06:13,  4.67s/it]Running Inference:  60%|██████    | 121/200 [09:51<06:35,  5.01s/it]Running Inference:  61%|██████    | 122/200 [09:56<06:29,  5.00s/it]Running Inference:  62%|██████▏   | 123/200 [09:59<05:52,  4.57s/it]Running Inference:  62%|██████▏   | 124/200 [10:05<06:21,  5.02s/it]Running Inference:  62%|██████▎   | 125/200 [10:10<06:18,  5.05s/it]Running Inference:  63%|██████▎   | 126/200 [10:15<06:06,  4.96s/it]Running Inference:  64%|██████▎   | 127/200 [10:20<05:53,  4.85s/it]Running Inference:  64%|██████▍   | 128/200 [10:26<06:28,  5.39s/it]Running Inference:  64%|██████▍   | 129/200 [10:28<04:55,  4.16s/it]Running Inference:  65%|██████▌   | 130/200 [10:33<05:18,  4.54s/it]Running Inference:  66%|██████▌   | 131/200 [10:38<05:13,  4.54s/it]Running Inference:  66%|██████▌   | 132/200 [10:42<05:11,  4.58s/it]Running Inference:  66%|██████▋   | 133/200 [10:47<05:14,  4.70s/it]Running Inference:  67%|██████▋   | 134/200 [10:49<04:02,  3.68s/it]Running Inference:  68%|██████▊   | 135/200 [10:53<04:20,  4.01s/it]Running Inference:  68%|██████▊   | 136/200 [10:54<03:21,  3.14s/it]Running Inference:  68%|██████▊   | 137/200 [11:01<04:30,  4.29s/it]Running Inference:  69%|██████▉   | 138/200 [11:07<04:51,  4.69s/it]Running Inference:  70%|██████▉   | 139/200 [11:13<05:17,  5.21s/it]Running Inference:  70%|███████   | 140/200 [11:18<05:06,  5.11s/it]Running Inference:  70%|███████   | 141/200 [11:24<05:14,  5.32s/it]Running Inference:  71%|███████   | 142/200 [11:29<05:00,  5.19s/it]Running Inference:  72%|███████▏  | 143/200 [11:33<04:38,  4.89s/it]Running Inference:  72%|███████▏  | 144/200 [11:39<04:43,  5.06s/it]Running Inference:  72%|███████▎  | 145/200 [11:44<04:46,  5.21s/it]Running Inference:  73%|███████▎  | 146/200 [11:49<04:36,  5.13s/it]Running Inference:  74%|███████▎  | 147/200 [11:55<04:49,  5.47s/it]Running Inference:  74%|███████▍  | 148/200 [12:01<04:48,  5.56s/it]Running Inference:  74%|███████▍  | 149/200 [12:06<04:26,  5.23s/it]Running Inference:  75%|███████▌  | 150/200 [12:11<04:17,  5.15s/it]Running Inference:  76%|███████▌  | 151/200 [12:16<04:20,  5.32s/it]Running Inference:  76%|███████▌  | 152/200 [12:17<03:13,  4.03s/it]Running Inference:  76%|███████▋  | 153/200 [12:24<03:44,  4.78s/it]Running Inference:  77%|███████▋  | 154/200 [12:28<03:32,  4.62s/it]Running Inference:  78%|███████▊  | 155/200 [12:33<03:33,  4.74s/it]Running Inference:  78%|███████▊  | 156/200 [12:38<03:30,  4.79s/it]Running Inference:  78%|███████▊  | 157/200 [12:39<02:38,  3.68s/it]Running Inference:  79%|███████▉  | 158/200 [12:40<01:58,  2.82s/it]Running Inference:  80%|███████▉  | 159/200 [12:45<02:21,  3.46s/it]Running Inference:  80%|████████  | 160/200 [12:50<02:34,  3.86s/it]Running Inference:  80%|████████  | 161/200 [12:56<02:53,  4.44s/it]Running Inference:  81%|████████  | 162/200 [13:00<02:48,  4.43s/it]Running Inference:  82%|████████▏ | 163/200 [13:05<02:49,  4.57s/it]Running Inference:  82%|████████▏ | 164/200 [13:10<02:48,  4.67s/it]Running Inference:  82%|████████▎ | 165/200 [13:15<02:47,  4.78s/it]Running Inference:  83%|████████▎ | 166/200 [13:19<02:37,  4.63s/it]Running Inference:  84%|████████▎ | 167/200 [13:24<02:32,  4.61s/it]Running Inference:  84%|████████▍ | 168/200 [13:25<01:53,  3.55s/it]Running Inference:  84%|████████▍ | 169/200 [13:31<02:16,  4.42s/it]Running Inference:  85%|████████▌ | 170/200 [13:34<01:59,  3.99s/it]Running Inference:  86%|████████▌ | 171/200 [13:39<02:03,  4.25s/it]Running Inference:  86%|████████▌ | 172/200 [13:43<02:01,  4.33s/it]Running Inference:  86%|████████▋ | 173/200 [13:49<02:07,  4.73s/it]Running Inference:  87%|████████▋ | 174/200 [13:55<02:08,  4.95s/it]Running Inference:  88%|████████▊ | 175/200 [14:00<02:04,  4.99s/it]Running Inference:  88%|████████▊ | 176/200 [14:05<01:58,  4.94s/it]Running Inference:  88%|████████▊ | 177/200 [14:10<01:56,  5.07s/it]Running Inference:  89%|████████▉ | 178/200 [14:15<01:51,  5.08s/it]Running Inference:  90%|████████▉ | 179/200 [14:16<01:22,  3.94s/it]Running Inference:  90%|█████████ | 180/200 [14:21<01:23,  4.16s/it]Running Inference:  90%|█████████ | 181/200 [14:26<01:24,  4.46s/it]Running Inference:  91%|█████████ | 182/200 [14:32<01:26,  4.83s/it]Running Inference:  92%|█████████▏| 183/200 [14:37<01:21,  4.81s/it]Running Inference:  92%|█████████▏| 184/200 [14:41<01:13,  4.59s/it]Running Inference:  92%|█████████▎| 185/200 [14:45<01:09,  4.60s/it]Running Inference:  93%|█████████▎| 186/200 [14:51<01:08,  4.91s/it]Running Inference:  94%|█████████▎| 187/200 [14:57<01:09,  5.35s/it]Running Inference:  94%|█████████▍| 188/200 [15:04<01:10,  5.86s/it]Running Inference:  94%|█████████▍| 189/200 [15:09<01:01,  5.64s/it]Running Inference:  95%|█████████▌| 190/200 [15:14<00:53,  5.38s/it]Running Inference:  96%|█████████▌| 191/200 [15:20<00:49,  5.52s/it]Running Inference:  96%|█████████▌| 192/200 [15:24<00:40,  5.05s/it]Running Inference:  96%|█████████▋| 193/200 [15:30<00:36,  5.21s/it]Running Inference:  97%|█████████▋| 194/200 [15:35<00:31,  5.30s/it]Running Inference:  98%|█████████▊| 195/200 [15:41<00:27,  5.46s/it]Running Inference:  98%|█████████▊| 196/200 [15:47<00:22,  5.73s/it]Running Inference:  98%|█████████▊| 197/200 [15:53<00:17,  5.70s/it]Running Inference:  99%|█████████▉| 198/200 [15:58<00:10,  5.37s/it]Running Inference: 100%|█████████▉| 199/200 [16:04<00:05,  5.61s/it]Running Inference: 100%|██████████| 200/200 [16:09<00:00,  5.40s/it]Running Inference: 100%|██████████| 200/200 [16:09<00:00,  4.85s/it]
2025-12-14 22:00:22,606 - INFO - Inference completed.
2025-12-14 22:00:22,626 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 22:00:22,626 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:00:22,627 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 22:00:22,627 - INFO - Metrics:
6.86
2025-12-14 22:00:22,628 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lsht, ratio=0.3) on GPU 2

----------------------------------------
Task: lsht | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:00:29,122 - INFO - Set deterministic seeds to 42
2025-12-14 22:00:29,122 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "lsht",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:00:29,122 - INFO - Starting evaluation run...
2025-12-14 22:00:29,122 - INFO - Output directory set to: longbenchresult
2025-12-14 22:00:29,122 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 22:00:29,122 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 22:00:29,122 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:00:29,122 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.45it/s]
Device set to use cuda:0
2025-12-14 22:00:41,849 - INFO - Model pipeline loaded.
2025-12-14 22:00:41,849 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: lsht)
2025-12-14 22:00:50,768 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:00:50,768 - INFO - Dataset processed with 200 entries.
2025-12-14 22:00:50,803 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:05<17:49,  5.37s/it]Running Inference:   1%|          | 2/200 [00:10<18:08,  5.50s/it]Running Inference:   2%|▏         | 3/200 [00:11<11:08,  3.40s/it]Running Inference:   2%|▏         | 4/200 [00:16<12:44,  3.90s/it]Running Inference:   2%|▎         | 5/200 [00:19<11:22,  3.50s/it]Running Inference:   3%|▎         | 6/200 [00:24<13:16,  4.10s/it]Running Inference:   4%|▎         | 7/200 [00:30<14:38,  4.55s/it]Running Inference:   4%|▍         | 8/200 [00:34<14:26,  4.51s/it]Running Inference:   4%|▍         | 9/200 [00:39<14:56,  4.69s/it]Running Inference:   5%|▌         | 10/200 [00:40<11:25,  3.61s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:46<13:00,  4.13s/it]Running Inference:   6%|▌         | 12/200 [00:47<10:23,  3.32s/it]Running Inference:   6%|▋         | 13/200 [00:53<12:33,  4.03s/it]Running Inference:   7%|▋         | 14/200 [00:54<09:57,  3.21s/it]Running Inference:   8%|▊         | 15/200 [00:57<09:42,  3.15s/it]Running Inference:   8%|▊         | 16/200 [01:02<11:00,  3.59s/it]Running Inference:   8%|▊         | 17/200 [01:08<13:28,  4.42s/it]Running Inference:   9%|▉         | 18/200 [01:14<14:27,  4.77s/it]Running Inference:  10%|▉         | 19/200 [01:18<14:11,  4.70s/it]Running Inference:  10%|█         | 20/200 [01:23<14:13,  4.74s/it]Running Inference:  10%|█         | 21/200 [01:28<14:45,  4.95s/it]Running Inference:  11%|█         | 22/200 [01:33<14:48,  4.99s/it]Running Inference:  12%|█▏        | 23/200 [01:34<11:09,  3.78s/it]Running Inference:  12%|█▏        | 24/200 [01:40<13:05,  4.46s/it]Running Inference:  12%|█▎        | 25/200 [01:45<12:42,  4.36s/it]Running Inference:  13%|█▎        | 26/200 [01:51<14:14,  4.91s/it]Running Inference:  14%|█▎        | 27/200 [01:57<15:07,  5.24s/it]Running Inference:  14%|█▍        | 28/200 [02:01<14:17,  4.98s/it]Running Inference:  14%|█▍        | 29/200 [02:06<13:42,  4.81s/it]Running Inference:  15%|█▌        | 30/200 [02:08<11:47,  4.16s/it]Running Inference:  16%|█▌        | 31/200 [02:14<13:05,  4.65s/it]Running Inference:  16%|█▌        | 32/200 [02:21<14:41,  5.25s/it]Running Inference:  16%|█▋        | 33/200 [02:27<15:08,  5.44s/it]Running Inference:  17%|█▋        | 34/200 [02:32<14:45,  5.34s/it]Running Inference:  18%|█▊        | 35/200 [02:39<16:19,  5.94s/it]Running Inference:  18%|█▊        | 36/200 [02:44<15:45,  5.77s/it]Running Inference:  18%|█▊        | 37/200 [02:50<15:23,  5.66s/it]Running Inference:  19%|█▉        | 38/200 [02:55<15:04,  5.58s/it]Running Inference:  20%|█▉        | 39/200 [03:00<14:37,  5.45s/it]Running Inference:  20%|██        | 40/200 [03:07<15:24,  5.78s/it]Running Inference:  20%|██        | 41/200 [03:13<15:43,  5.94s/it]Running Inference:  21%|██        | 42/200 [03:18<14:58,  5.69s/it]Running Inference:  22%|██▏       | 43/200 [03:23<14:11,  5.42s/it]Running Inference:  22%|██▏       | 44/200 [03:28<13:38,  5.25s/it]Running Inference:  22%|██▎       | 45/200 [03:33<13:17,  5.15s/it]Running Inference:  23%|██▎       | 46/200 [03:39<13:54,  5.42s/it]Running Inference:  24%|██▎       | 47/200 [03:44<13:22,  5.25s/it]Running Inference:  24%|██▍       | 48/200 [03:48<12:43,  5.02s/it]Running Inference:  24%|██▍       | 49/200 [03:54<13:22,  5.31s/it]Running Inference:  25%|██▌       | 50/200 [03:59<12:49,  5.13s/it]Running Inference:  26%|██▌       | 51/200 [04:03<12:00,  4.84s/it]Running Inference:  26%|██▌       | 52/200 [04:08<11:52,  4.81s/it]Running Inference:  26%|██▋       | 53/200 [04:13<11:50,  4.83s/it]Running Inference:  27%|██▋       | 54/200 [04:18<12:00,  4.93s/it]Running Inference:  28%|██▊       | 55/200 [04:24<12:59,  5.38s/it]Running Inference:  28%|██▊       | 56/200 [04:27<10:56,  4.56s/it]Running Inference:  28%|██▊       | 57/200 [04:33<11:48,  4.96s/it]Running Inference:  29%|██▉       | 58/200 [04:39<12:18,  5.20s/it]Running Inference:  30%|██▉       | 59/200 [04:43<11:39,  4.96s/it]Running Inference:  30%|███       | 60/200 [04:49<12:06,  5.19s/it]Running Inference:  30%|███       | 61/200 [04:50<09:01,  3.90s/it]Running Inference:  31%|███       | 62/200 [04:56<10:24,  4.53s/it]Running Inference:  32%|███▏      | 63/200 [05:00<10:19,  4.52s/it]Running Inference:  32%|███▏      | 64/200 [05:01<07:50,  3.46s/it]Running Inference:  32%|███▎      | 65/200 [05:05<08:18,  3.69s/it]Running Inference:  33%|███▎      | 66/200 [05:11<09:40,  4.33s/it]Running Inference:  34%|███▎      | 67/200 [05:15<09:35,  4.32s/it]Running Inference:  34%|███▍      | 68/200 [05:20<09:23,  4.27s/it]Running Inference:  34%|███▍      | 69/200 [05:25<10:14,  4.69s/it]Running Inference:  35%|███▌      | 70/200 [05:30<09:55,  4.58s/it]Running Inference:  36%|███▌      | 71/200 [05:34<09:29,  4.42s/it]Running Inference:  36%|███▌      | 72/200 [05:39<09:49,  4.60s/it]Running Inference:  36%|███▋      | 73/200 [05:44<10:19,  4.87s/it]Running Inference:  37%|███▋      | 74/200 [05:51<11:17,  5.38s/it]Running Inference:  38%|███▊      | 75/200 [05:55<10:48,  5.19s/it]Running Inference:  38%|███▊      | 76/200 [06:01<11:02,  5.34s/it]Running Inference:  38%|███▊      | 77/200 [06:06<10:35,  5.17s/it]Running Inference:  39%|███▉      | 78/200 [06:11<10:10,  5.00s/it]Running Inference:  40%|███▉      | 79/200 [06:15<09:37,  4.77s/it]Running Inference:  40%|████      | 80/200 [06:20<09:47,  4.89s/it]Running Inference:  40%|████      | 81/200 [06:25<09:56,  5.01s/it]Running Inference:  41%|████      | 82/200 [06:32<10:40,  5.43s/it]Running Inference:  42%|████▏     | 83/200 [06:37<10:18,  5.28s/it]Running Inference:  42%|████▏     | 84/200 [06:42<10:02,  5.19s/it]Running Inference:  42%|████▎     | 85/200 [06:48<10:24,  5.43s/it]Running Inference:  43%|████▎     | 86/200 [06:52<10:00,  5.27s/it]Running Inference:  44%|████▎     | 87/200 [06:58<09:55,  5.27s/it]Running Inference:  44%|████▍     | 88/200 [07:03<09:48,  5.25s/it]Running Inference:  44%|████▍     | 89/200 [07:07<09:18,  5.04s/it]Running Inference:  45%|████▌     | 90/200 [07:13<09:29,  5.18s/it]Running Inference:  46%|████▌     | 91/200 [07:18<09:22,  5.16s/it]Running Inference:  46%|████▌     | 92/200 [07:23<09:21,  5.20s/it]Running Inference:  46%|████▋     | 93/200 [07:29<09:29,  5.33s/it]Running Inference:  47%|████▋     | 94/200 [07:34<09:00,  5.10s/it]Running Inference:  48%|████▊     | 95/200 [07:40<09:25,  5.38s/it]Running Inference:  48%|████▊     | 96/200 [07:45<09:20,  5.39s/it]Running Inference:  48%|████▊     | 97/200 [07:51<09:47,  5.71s/it]Running Inference:  49%|████▉     | 98/200 [07:53<07:26,  4.37s/it]Running Inference:  50%|████▉     | 99/200 [07:58<07:58,  4.74s/it]Running Inference:  50%|█████     | 100/200 [07:59<06:00,  3.61s/it]Running Inference:  50%|█████     | 101/200 [08:05<06:47,  4.11s/it]Running Inference:  51%|█████     | 102/200 [08:09<06:43,  4.11s/it]Running Inference:  52%|█████▏    | 103/200 [08:15<07:54,  4.89s/it]Running Inference:  52%|█████▏    | 104/200 [08:22<08:35,  5.37s/it]Running Inference:  52%|█████▎    | 105/200 [08:27<08:19,  5.26s/it]Running Inference:  53%|█████▎    | 106/200 [08:34<08:57,  5.72s/it]Running Inference:  54%|█████▎    | 107/200 [08:38<08:15,  5.33s/it]Running Inference:  54%|█████▍    | 108/200 [08:43<08:00,  5.23s/it]Running Inference:  55%|█████▍    | 109/200 [08:48<07:41,  5.07s/it]Running Inference:  55%|█████▌    | 110/200 [08:53<07:37,  5.08s/it]Running Inference:  56%|█████▌    | 111/200 [08:58<07:40,  5.17s/it]Running Inference:  56%|█████▌    | 112/200 [09:04<07:42,  5.26s/it]Running Inference:  56%|█████▋    | 113/200 [09:09<07:29,  5.16s/it]Running Inference:  57%|█████▋    | 114/200 [09:13<07:08,  4.99s/it]Running Inference:  57%|█████▊    | 115/200 [09:19<07:20,  5.19s/it]Running Inference:  58%|█████▊    | 116/200 [09:25<07:35,  5.42s/it]Running Inference:  58%|█████▊    | 117/200 [09:31<07:56,  5.74s/it]Running Inference:  59%|█████▉    | 118/200 [09:33<06:08,  4.49s/it]Running Inference:  60%|█████▉    | 119/200 [09:37<05:57,  4.41s/it]Running Inference:  60%|██████    | 120/200 [09:42<06:08,  4.61s/it]Running Inference:  60%|██████    | 121/200 [09:48<06:30,  4.94s/it]Running Inference:  61%|██████    | 122/200 [09:53<06:25,  4.94s/it]Running Inference:  62%|██████▏   | 123/200 [09:57<05:56,  4.63s/it]Running Inference:  62%|██████▏   | 124/200 [10:00<05:09,  4.07s/it]Running Inference:  62%|██████▎   | 125/200 [10:05<05:27,  4.37s/it]Running Inference:  63%|██████▎   | 126/200 [10:09<05:29,  4.46s/it]Running Inference:  64%|██████▎   | 127/200 [10:14<05:27,  4.48s/it]Running Inference:  64%|██████▍   | 128/200 [10:20<06:07,  5.10s/it]Running Inference:  64%|██████▍   | 129/200 [10:25<05:44,  4.86s/it]Running Inference:  65%|██████▌   | 130/200 [10:30<05:50,  5.01s/it]Running Inference:  66%|██████▌   | 131/200 [10:35<05:34,  4.85s/it]Running Inference:  66%|██████▌   | 132/200 [10:39<05:25,  4.78s/it]Running Inference:  66%|██████▋   | 133/200 [10:44<05:23,  4.83s/it]Running Inference:  67%|██████▋   | 134/200 [10:49<05:13,  4.74s/it]Running Inference:  68%|██████▊   | 135/200 [10:53<05:07,  4.74s/it]Running Inference:  68%|██████▊   | 136/200 [10:58<04:56,  4.63s/it]Running Inference:  68%|██████▊   | 137/200 [11:05<05:32,  5.29s/it]Running Inference:  69%|██████▉   | 138/200 [11:10<05:33,  5.37s/it]Running Inference:  70%|██████▉   | 139/200 [11:16<05:45,  5.66s/it]Running Inference:  70%|███████   | 140/200 [11:21<05:24,  5.41s/it]Running Inference:  70%|███████   | 141/200 [11:27<05:25,  5.52s/it]Running Inference:  71%|███████   | 142/200 [11:32<05:07,  5.30s/it]Running Inference:  72%|███████▏  | 143/200 [11:36<04:42,  4.96s/it]Running Inference:  72%|███████▏  | 144/200 [11:41<04:44,  5.08s/it]Running Inference:  72%|███████▎  | 145/200 [11:47<04:46,  5.21s/it]Running Inference:  73%|███████▎  | 146/200 [11:52<04:35,  5.10s/it]Running Inference:  74%|███████▎  | 147/200 [11:58<04:47,  5.42s/it]Running Inference:  74%|███████▍  | 148/200 [12:04<04:45,  5.48s/it]Running Inference:  74%|███████▍  | 149/200 [12:08<04:23,  5.17s/it]Running Inference:  75%|███████▌  | 150/200 [12:13<04:14,  5.08s/it]Running Inference:  76%|███████▌  | 151/200 [12:19<04:17,  5.26s/it]Running Inference:  76%|███████▌  | 152/200 [12:23<03:58,  4.96s/it]Running Inference:  76%|███████▋  | 153/200 [12:29<04:13,  5.40s/it]Running Inference:  77%|███████▋  | 154/200 [12:33<03:51,  5.04s/it]Running Inference:  78%|███████▊  | 155/200 [12:38<03:45,  5.01s/it]Running Inference:  78%|███████▊  | 156/200 [12:43<03:38,  4.96s/it]Running Inference:  78%|███████▊  | 157/200 [12:44<02:44,  3.83s/it]Running Inference:  79%|███████▉  | 158/200 [12:49<02:44,  3.93s/it]Running Inference:  80%|███████▉  | 159/200 [12:53<02:52,  4.21s/it]Running Inference:  80%|████████  | 160/200 [12:58<02:54,  4.37s/it]Running Inference:  80%|████████  | 161/200 [13:04<03:06,  4.79s/it]Running Inference:  81%|████████  | 162/200 [13:08<02:56,  4.65s/it]Running Inference:  82%|████████▏ | 163/200 [13:13<02:53,  4.70s/it]Running Inference:  82%|████████▏ | 164/200 [13:18<02:50,  4.74s/it]Running Inference:  82%|████████▎ | 165/200 [13:23<02:48,  4.81s/it]Running Inference:  83%|████████▎ | 166/200 [13:27<02:37,  4.64s/it]Running Inference:  84%|████████▎ | 167/200 [13:32<02:31,  4.60s/it]Running Inference:  84%|████████▍ | 168/200 [13:33<01:53,  3.53s/it]Running Inference:  84%|████████▍ | 169/200 [13:39<02:15,  4.38s/it]Running Inference:  85%|████████▌ | 170/200 [13:42<01:59,  4.00s/it]Running Inference:  86%|████████▌ | 171/200 [13:47<02:02,  4.23s/it]Running Inference:  86%|████████▌ | 172/200 [13:51<02:00,  4.31s/it]Running Inference:  86%|████████▋ | 173/200 [13:57<02:06,  4.69s/it]Running Inference:  87%|████████▋ | 174/200 [14:02<02:08,  4.92s/it]Running Inference:  88%|████████▊ | 175/200 [14:07<02:04,  4.96s/it]Running Inference:  88%|████████▊ | 176/200 [14:12<01:57,  4.91s/it]Running Inference:  88%|████████▊ | 177/200 [14:18<01:55,  5.02s/it]Running Inference:  89%|████████▉ | 178/200 [14:23<01:50,  5.04s/it]Running Inference:  90%|████████▉ | 179/200 [14:24<01:22,  3.91s/it]Running Inference:  90%|█████████ | 180/200 [14:29<01:22,  4.12s/it]Running Inference:  90%|█████████ | 181/200 [14:34<01:24,  4.43s/it]Running Inference:  91%|█████████ | 182/200 [14:39<01:26,  4.78s/it]Running Inference:  92%|█████████▏| 183/200 [14:44<01:20,  4.76s/it]Running Inference:  92%|█████████▏| 184/200 [14:48<01:12,  4.54s/it]Running Inference:  92%|█████████▎| 185/200 [14:50<00:56,  3.75s/it]Running Inference:  93%|█████████▎| 186/200 [14:55<01:00,  4.29s/it]Running Inference:  94%|█████████▎| 187/200 [15:02<01:03,  4.89s/it]Running Inference:  94%|█████████▍| 188/200 [15:09<01:05,  5.50s/it]Running Inference:  94%|█████████▍| 189/200 [15:14<00:58,  5.36s/it]Running Inference:  95%|█████████▌| 190/200 [15:18<00:51,  5.17s/it]Running Inference:  96%|█████████▌| 191/200 [15:24<00:48,  5.35s/it]Running Inference:  96%|█████████▌| 192/200 [15:28<00:39,  4.91s/it]Running Inference:  96%|█████████▋| 193/200 [15:34<00:35,  5.10s/it]Running Inference:  97%|█████████▋| 194/200 [15:39<00:31,  5.20s/it]Running Inference:  98%|█████████▊| 195/200 [15:45<00:26,  5.37s/it]Running Inference:  98%|█████████▊| 196/200 [15:51<00:22,  5.64s/it]Running Inference:  98%|█████████▊| 197/200 [15:57<00:16,  5.61s/it]Running Inference:  99%|█████████▉| 198/200 [16:01<00:10,  5.30s/it]Running Inference: 100%|█████████▉| 199/200 [16:07<00:05,  5.54s/it]Running Inference: 100%|██████████| 200/200 [16:12<00:00,  5.33s/it]Running Inference: 100%|██████████| 200/200 [16:12<00:00,  4.86s/it]
2025-12-14 22:17:03,496 - INFO - Inference completed.
2025-12-14 22:17:03,516 - INFO - Results saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 22:17:03,516 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:17:03,517 - INFO - Metrics saved to longbenchresult/longbench__lsht__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 22:17:03,517 - INFO - Metrics:
8.96
2025-12-14 22:17:03,519 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=lsht, ratio=0.5) on GPU 2


========================================
LongBench Task: multifieldqa_zh
========================================
----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:17:10,007 - INFO - Set deterministic seeds to 42
2025-12-14 22:17:10,008 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:17:10,008 - INFO - Starting evaluation run...
2025-12-14 22:17:10,008 - INFO - Output directory set to: longbenchresult
2025-12-14 22:17:10,008 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 22:17:10,008 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 22:17:10,008 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:17:10,008 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.56it/s]
Device set to use cuda:0
2025-12-14 22:17:21,978 - INFO - Model pipeline loaded.
2025-12-14 22:17:21,978 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 22:17:34,807 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:17:34,807 - INFO - Dataset processed with 200 entries.
2025-12-14 22:17:34,820 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:01<04:16,  1.82s/it]Running Inference:   1%|▏         | 2/142 [00:09<12:54,  5.53s/it]Running Inference:   2%|▏         | 3/142 [00:11<08:14,  3.55s/it]Running Inference:   3%|▎         | 4/142 [00:15<08:40,  3.77s/it]Running Inference:   4%|▎         | 5/142 [00:15<05:59,  2.62s/it]Running Inference:   4%|▍         | 6/142 [00:16<04:14,  1.87s/it]Running Inference:   5%|▍         | 7/142 [00:17<03:49,  1.70s/it]Running Inference:   6%|▌         | 8/142 [00:19<03:36,  1.61s/it]Running Inference:   6%|▋         | 9/142 [00:22<05:06,  2.30s/it]Running Inference:   7%|▋         | 10/142 [00:31<09:08,  4.15s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:33<07:36,  3.49s/it]Running Inference:   8%|▊         | 12/142 [00:34<06:03,  2.80s/it]Running Inference:   9%|▉         | 13/142 [00:35<05:04,  2.36s/it]Running Inference:  10%|▉         | 14/142 [00:40<06:39,  3.12s/it]Running Inference:  11%|█         | 15/142 [00:41<04:59,  2.36s/it]Running Inference:  11%|█▏        | 16/142 [00:46<06:31,  3.11s/it]Running Inference:  12%|█▏        | 17/142 [00:48<05:59,  2.88s/it]Running Inference:  13%|█▎        | 18/142 [00:52<06:33,  3.17s/it]Running Inference:  13%|█▎        | 19/142 [00:54<05:56,  2.90s/it]Running Inference:  14%|█▍        | 20/142 [00:55<04:37,  2.27s/it]Running Inference:  15%|█▍        | 21/142 [01:00<06:21,  3.16s/it]Running Inference:  15%|█▌        | 22/142 [01:04<06:57,  3.48s/it]Running Inference:  16%|█▌        | 23/142 [01:05<05:12,  2.62s/it]Running Inference:  17%|█▋        | 24/142 [01:09<05:56,  3.03s/it]Running Inference:  18%|█▊        | 25/142 [01:13<06:19,  3.25s/it]Running Inference:  18%|█▊        | 26/142 [01:17<06:50,  3.54s/it]Running Inference:  19%|█▉        | 27/142 [01:21<06:54,  3.61s/it]Running Inference:  20%|█▉        | 28/142 [01:25<07:14,  3.81s/it]Running Inference:  20%|██        | 29/142 [01:26<05:45,  3.05s/it]Running Inference:  21%|██        | 30/142 [01:27<04:30,  2.42s/it]Running Inference:  22%|██▏       | 31/142 [01:34<06:58,  3.77s/it]Running Inference:  23%|██▎       | 32/142 [01:38<07:11,  3.92s/it]Running Inference:  23%|██▎       | 33/142 [01:39<05:13,  2.87s/it]Running Inference:  24%|██▍       | 34/142 [01:39<04:02,  2.24s/it]Running Inference:  25%|██▍       | 35/142 [01:44<05:01,  2.81s/it]Running Inference:  25%|██▌       | 36/142 [01:46<04:43,  2.67s/it]Running Inference:  26%|██▌       | 37/142 [01:47<03:38,  2.08s/it]Running Inference:  27%|██▋       | 38/142 [01:48<03:14,  1.87s/it]Running Inference:  27%|██▋       | 39/142 [01:52<04:25,  2.58s/it]Running Inference:  28%|██▊       | 40/142 [01:56<05:02,  2.96s/it]Running Inference:  29%|██▉       | 41/142 [01:58<04:24,  2.62s/it]Running Inference:  30%|██▉       | 42/142 [01:59<03:43,  2.23s/it]Running Inference:  30%|███       | 43/142 [02:03<04:32,  2.75s/it]Running Inference:  31%|███       | 44/142 [02:05<03:45,  2.30s/it]Running Inference:  32%|███▏      | 45/142 [02:09<04:36,  2.85s/it]Running Inference:  32%|███▏      | 46/142 [02:09<03:32,  2.22s/it]Running Inference:  33%|███▎      | 47/142 [02:13<04:22,  2.77s/it]Running Inference:  34%|███▍      | 48/142 [02:14<03:15,  2.08s/it]Running Inference:  35%|███▍      | 49/142 [02:15<02:35,  1.68s/it]Running Inference:  35%|███▌      | 50/142 [02:16<02:19,  1.52s/it]Running Inference:  36%|███▌      | 51/142 [02:17<01:58,  1.31s/it]Running Inference:  37%|███▋      | 52/142 [02:18<02:00,  1.34s/it]Running Inference:  37%|███▋      | 53/142 [02:19<01:41,  1.14s/it]Running Inference:  38%|███▊      | 54/142 [02:20<01:31,  1.04s/it]Running Inference:  39%|███▊      | 55/142 [02:24<02:56,  2.03s/it]Running Inference:  39%|███▉      | 56/142 [02:28<03:53,  2.71s/it]Running Inference:  40%|████      | 57/142 [02:32<04:20,  3.07s/it]Running Inference:  41%|████      | 58/142 [02:33<03:18,  2.36s/it]Running Inference:  42%|████▏     | 59/142 [02:34<02:57,  2.14s/it]Running Inference:  42%|████▏     | 60/142 [02:35<02:15,  1.65s/it]Running Inference:  43%|████▎     | 61/142 [02:37<02:34,  1.91s/it]Running Inference:  44%|████▎     | 62/142 [02:38<02:13,  1.66s/it]Running Inference:  44%|████▍     | 63/142 [02:40<02:05,  1.59s/it]Running Inference:  45%|████▌     | 64/142 [02:44<03:08,  2.42s/it]Running Inference:  46%|████▌     | 65/142 [02:46<02:40,  2.08s/it]Running Inference:  46%|████▋     | 66/142 [02:50<03:29,  2.76s/it]Running Inference:  47%|████▋     | 67/142 [02:51<02:41,  2.16s/it]Running Inference:  48%|████▊     | 68/142 [02:51<02:02,  1.65s/it]Running Inference:  49%|████▊     | 69/142 [02:52<01:41,  1.39s/it]Running Inference:  49%|████▉     | 70/142 [02:53<01:41,  1.41s/it]Running Inference:  50%|█████     | 71/142 [02:58<02:39,  2.25s/it]Running Inference:  51%|█████     | 72/142 [02:58<01:59,  1.71s/it]Running Inference:  51%|█████▏    | 73/142 [02:59<01:44,  1.52s/it]Running Inference:  52%|█████▏    | 74/142 [03:00<01:32,  1.36s/it]Running Inference:  53%|█████▎    | 75/142 [03:00<01:10,  1.05s/it]Running Inference:  54%|█████▎    | 76/142 [03:01<01:00,  1.10it/s]Running Inference:  54%|█████▍    | 77/142 [03:03<01:20,  1.25s/it]Running Inference:  55%|█████▍    | 78/142 [03:07<02:11,  2.06s/it]Running Inference:  56%|█████▌    | 79/142 [03:12<02:59,  2.85s/it]Running Inference:  56%|█████▋    | 80/142 [03:13<02:29,  2.41s/it]Running Inference:  57%|█████▋    | 81/142 [03:15<02:12,  2.17s/it]Running Inference:  58%|█████▊    | 82/142 [03:19<02:41,  2.69s/it]Running Inference:  58%|█████▊    | 83/142 [03:23<03:07,  3.17s/it]Running Inference:  59%|█████▉    | 84/142 [03:27<03:25,  3.55s/it]Running Inference:  60%|█████▉    | 85/142 [03:28<02:32,  2.68s/it]Running Inference:  61%|██████    | 86/142 [03:29<01:57,  2.09s/it]Running Inference:  61%|██████▏   | 87/142 [03:30<01:37,  1.77s/it]Running Inference:  62%|██████▏   | 88/142 [03:31<01:24,  1.57s/it]Running Inference:  63%|██████▎   | 89/142 [03:32<01:18,  1.47s/it]Running Inference:  63%|██████▎   | 90/142 [03:33<01:15,  1.46s/it]Running Inference:  64%|██████▍   | 91/142 [03:38<01:58,  2.31s/it]Running Inference:  65%|██████▍   | 92/142 [03:42<02:23,  2.87s/it]Running Inference:  65%|██████▌   | 93/142 [03:44<02:02,  2.49s/it]Running Inference:  66%|██████▌   | 94/142 [03:44<01:33,  1.95s/it]Running Inference:  67%|██████▋   | 95/142 [03:46<01:33,  2.00s/it]Running Inference:  68%|██████▊   | 96/142 [03:47<01:17,  1.69s/it]Running Inference:  68%|██████▊   | 97/142 [03:51<01:47,  2.40s/it]Running Inference:  69%|██████▉   | 98/142 [03:52<01:23,  1.90s/it]Running Inference:  70%|██████▉   | 99/142 [03:56<01:53,  2.63s/it]Running Inference:  70%|███████   | 100/142 [04:00<02:08,  3.06s/it]Running Inference:  71%|███████   | 101/142 [04:01<01:36,  2.36s/it]Running Inference:  72%|███████▏  | 102/142 [04:05<01:54,  2.85s/it]Running Inference:  73%|███████▎  | 103/142 [04:08<01:50,  2.84s/it]Running Inference:  73%|███████▎  | 104/142 [04:12<02:04,  3.28s/it]Running Inference:  74%|███████▍  | 105/142 [04:17<02:14,  3.64s/it]Running Inference:  75%|███████▍  | 106/142 [04:23<02:33,  4.26s/it]Running Inference:  75%|███████▌  | 107/142 [04:25<02:11,  3.75s/it]Running Inference:  76%|███████▌  | 108/142 [04:26<01:34,  2.78s/it]Running Inference:  77%|███████▋  | 109/142 [04:30<01:44,  3.18s/it]Running Inference:  77%|███████▋  | 110/142 [04:32<01:28,  2.77s/it]Running Inference:  78%|███████▊  | 111/142 [04:32<01:06,  2.13s/it]Running Inference:  79%|███████▉  | 112/142 [04:34<00:58,  1.96s/it]Running Inference:  80%|███████▉  | 113/142 [04:34<00:44,  1.54s/it]Running Inference:  80%|████████  | 114/142 [04:35<00:37,  1.34s/it]Running Inference:  81%|████████  | 115/142 [04:36<00:30,  1.14s/it]Running Inference:  82%|████████▏ | 116/142 [04:40<00:50,  1.94s/it]Running Inference:  82%|████████▏ | 117/142 [04:41<00:42,  1.68s/it]Running Inference:  83%|████████▎ | 118/142 [04:42<00:37,  1.54s/it]Running Inference:  84%|████████▍ | 119/142 [04:42<00:27,  1.18s/it]Running Inference:  85%|████████▍ | 120/142 [04:44<00:28,  1.30s/it]Running Inference:  85%|████████▌ | 121/142 [04:48<00:46,  2.24s/it]Running Inference:  86%|████████▌ | 122/142 [04:49<00:38,  1.92s/it]Running Inference:  87%|████████▋ | 123/142 [04:53<00:47,  2.52s/it]Running Inference:  87%|████████▋ | 124/142 [04:54<00:34,  1.91s/it]Running Inference:  88%|████████▊ | 125/142 [04:55<00:30,  1.80s/it]Running Inference:  89%|████████▊ | 126/142 [04:56<00:24,  1.53s/it]Running Inference:  89%|████████▉ | 127/142 [05:00<00:34,  2.28s/it]Running Inference:  90%|█████████ | 128/142 [05:04<00:38,  2.76s/it]Running Inference:  91%|█████████ | 129/142 [05:05<00:28,  2.18s/it]Running Inference:  92%|█████████▏| 130/142 [05:10<00:35,  2.95s/it]Running Inference:  92%|█████████▏| 131/142 [05:10<00:24,  2.27s/it]Running Inference:  93%|█████████▎| 132/142 [05:11<00:17,  1.74s/it]Running Inference:  94%|█████████▎| 133/142 [05:19<00:33,  3.71s/it]Running Inference:  94%|█████████▍| 134/142 [05:24<00:31,  3.92s/it]Running Inference:  95%|█████████▌| 135/142 [05:24<00:20,  2.97s/it]Running Inference:  96%|█████████▌| 136/142 [05:26<00:15,  2.56s/it]Running Inference:  96%|█████████▋| 137/142 [05:27<00:11,  2.22s/it]Running Inference:  97%|█████████▋| 138/142 [05:28<00:06,  1.67s/it]Running Inference:  98%|█████████▊| 139/142 [05:29<00:04,  1.52s/it]Running Inference:  99%|█████████▊| 140/142 [05:30<00:02,  1.33s/it]Running Inference:  99%|█████████▉| 141/142 [05:32<00:01,  1.57s/it]Running Inference: 100%|██████████| 142/142 [05:36<00:00,  2.35s/it]Running Inference: 100%|██████████| 142/142 [05:36<00:00,  2.37s/it]
2025-12-14 22:23:11,551 - INFO - Inference completed.
2025-12-14 22:23:11,559 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 22:23:11,559 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.690 seconds.
Prefix dict has been built successfully.
2025-12-14 22:23:12,319 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 22:23:12,319 - INFO - Metrics:
38.15
2025-12-14 22:23:12,320 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_zh, ratio=0.1) on GPU 2

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:23:18,891 - INFO - Set deterministic seeds to 42
2025-12-14 22:23:18,891 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:23:18,891 - INFO - Starting evaluation run...
2025-12-14 22:23:18,892 - INFO - Output directory set to: longbenchresult
2025-12-14 22:23:18,892 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 22:23:18,892 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 22:23:18,892 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:23:18,892 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 101.58it/s]
Device set to use cuda:0
2025-12-14 22:23:33,852 - INFO - Model pipeline loaded.
2025-12-14 22:23:33,852 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 22:23:38,391 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:23:38,391 - INFO - Dataset processed with 200 entries.
2025-12-14 22:23:38,401 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:01<04:28,  1.90s/it]Running Inference:   1%|▏         | 2/142 [00:07<09:25,  4.04s/it]Running Inference:   2%|▏         | 3/142 [00:09<06:56,  3.00s/it]Running Inference:   3%|▎         | 4/142 [00:13<07:49,  3.40s/it]Running Inference:   4%|▎         | 5/142 [00:13<05:28,  2.40s/it]Running Inference:   4%|▍         | 6/142 [00:14<03:51,  1.71s/it]Running Inference:   5%|▍         | 7/142 [00:15<03:34,  1.59s/it]Running Inference:   6%|▌         | 8/142 [00:16<03:23,  1.52s/it]Running Inference:   6%|▋         | 9/142 [00:24<07:18,  3.30s/it]Running Inference:   7%|▋         | 10/142 [00:31<09:50,  4.47s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:33<08:04,  3.70s/it]Running Inference:   8%|▊         | 12/142 [00:34<06:21,  2.93s/it]Running Inference:   9%|▉         | 13/142 [00:38<07:17,  3.39s/it]Running Inference:  10%|▉         | 14/142 [00:43<08:12,  3.85s/it]Running Inference:  11%|█         | 15/142 [00:44<06:04,  2.87s/it]Running Inference:  11%|█▏        | 16/142 [00:52<09:04,  4.32s/it]Running Inference:  12%|█▏        | 17/142 [00:59<11:07,  5.34s/it]Running Inference:  13%|█▎        | 18/142 [01:03<10:05,  4.88s/it]Running Inference:  13%|█▎        | 19/142 [01:05<08:22,  4.08s/it]Running Inference:  14%|█▍        | 20/142 [01:06<06:18,  3.10s/it]Running Inference:  15%|█▍        | 21/142 [01:11<07:28,  3.70s/it]Running Inference:  15%|█▌        | 22/142 [01:15<07:40,  3.84s/it]Running Inference:  16%|█▌        | 23/142 [01:16<05:41,  2.87s/it]Running Inference:  17%|█▋        | 24/142 [01:20<06:14,  3.17s/it]Running Inference:  18%|█▊        | 25/142 [01:20<04:41,  2.41s/it]Running Inference:  18%|█▊        | 26/142 [01:25<05:38,  2.92s/it]Running Inference:  19%|█▉        | 27/142 [01:26<04:28,  2.33s/it]Running Inference:  20%|█▉        | 28/142 [01:29<04:51,  2.56s/it]Running Inference:  20%|██        | 29/142 [01:31<04:30,  2.39s/it]Running Inference:  21%|██        | 30/142 [01:31<03:35,  1.92s/it]Running Inference:  22%|██▏       | 31/142 [01:35<04:22,  2.37s/it]Running Inference:  23%|██▎       | 32/142 [01:36<03:33,  1.95s/it]Running Inference:  23%|██▎       | 33/142 [01:36<02:42,  1.49s/it]Running Inference:  24%|██▍       | 34/142 [01:37<02:17,  1.27s/it]Running Inference:  25%|██▍       | 35/142 [01:41<03:47,  2.12s/it]Running Inference:  25%|██▌       | 36/142 [01:43<03:51,  2.18s/it]Running Inference:  26%|██▌       | 37/142 [01:44<03:01,  1.73s/it]Running Inference:  27%|██▋       | 38/142 [01:46<03:03,  1.77s/it]Running Inference:  27%|██▋       | 39/142 [01:50<04:16,  2.49s/it]Running Inference:  28%|██▊       | 40/142 [01:52<03:43,  2.19s/it]Running Inference:  29%|██▉       | 41/142 [01:53<03:28,  2.07s/it]Running Inference:  30%|██▉       | 42/142 [01:55<03:04,  1.84s/it]Running Inference:  30%|███       | 43/142 [01:59<04:04,  2.47s/it]Running Inference:  31%|███       | 44/142 [02:00<03:26,  2.10s/it]Running Inference:  32%|███▏      | 45/142 [02:04<04:20,  2.69s/it]Running Inference:  32%|███▏      | 46/142 [02:05<03:21,  2.10s/it]Running Inference:  33%|███▎      | 47/142 [02:08<03:58,  2.51s/it]Running Inference:  34%|███▍      | 48/142 [02:09<02:58,  1.89s/it]Running Inference:  35%|███▍      | 49/142 [02:09<02:23,  1.54s/it]Running Inference:  35%|███▌      | 50/142 [02:10<02:10,  1.42s/it]Running Inference:  36%|███▌      | 51/142 [02:14<03:17,  2.17s/it]Running Inference:  37%|███▋      | 52/142 [02:16<03:00,  2.01s/it]Running Inference:  37%|███▋      | 53/142 [02:17<02:24,  1.62s/it]Running Inference:  38%|███▊      | 54/142 [02:18<02:01,  1.38s/it]Running Inference:  39%|███▊      | 55/142 [02:22<03:15,  2.25s/it]Running Inference:  39%|███▉      | 56/142 [02:24<03:06,  2.17s/it]Running Inference:  40%|████      | 57/142 [02:25<02:36,  1.84s/it]Running Inference:  41%|████      | 58/142 [02:26<02:05,  1.49s/it]Running Inference:  42%|████▏     | 59/142 [02:27<02:06,  1.53s/it]Running Inference:  42%|████▏     | 60/142 [02:28<01:40,  1.23s/it]Running Inference:  43%|████▎     | 61/142 [02:30<02:10,  1.61s/it]Running Inference:  44%|████▎     | 62/142 [02:31<01:56,  1.45s/it]Running Inference:  44%|████▍     | 63/142 [02:36<03:13,  2.45s/it]Running Inference:  45%|████▌     | 64/142 [02:40<03:55,  3.02s/it]Running Inference:  46%|████▌     | 65/142 [02:42<03:12,  2.50s/it]Running Inference:  46%|████▋     | 66/142 [02:46<03:56,  3.11s/it]Running Inference:  47%|████▋     | 67/142 [02:47<02:59,  2.40s/it]Running Inference:  48%|████▊     | 68/142 [02:47<02:14,  1.82s/it]Running Inference:  49%|████▊     | 69/142 [02:48<01:49,  1.50s/it]Running Inference:  49%|████▉     | 70/142 [02:49<01:27,  1.22s/it]Running Inference:  50%|█████     | 71/142 [02:53<02:29,  2.10s/it]Running Inference:  51%|█████     | 72/142 [02:54<01:57,  1.67s/it]Running Inference:  51%|█████▏    | 73/142 [02:55<01:42,  1.48s/it]Running Inference:  52%|█████▏    | 74/142 [02:56<01:31,  1.35s/it]Running Inference:  53%|█████▎    | 75/142 [02:56<01:10,  1.05s/it]Running Inference:  54%|█████▎    | 76/142 [02:57<01:00,  1.10it/s]Running Inference:  54%|█████▍    | 77/142 [02:59<01:21,  1.25s/it]Running Inference:  55%|█████▍    | 78/142 [03:01<01:38,  1.53s/it]Running Inference:  56%|█████▌    | 79/142 [03:03<01:43,  1.64s/it]Running Inference:  56%|█████▋    | 80/142 [03:03<01:20,  1.30s/it]Running Inference:  57%|█████▋    | 81/142 [03:05<01:24,  1.39s/it]Running Inference:  58%|█████▊    | 82/142 [03:05<01:05,  1.09s/it]Running Inference:  58%|█████▊    | 83/142 [03:09<01:55,  1.96s/it]Running Inference:  59%|█████▉    | 84/142 [03:14<02:34,  2.67s/it]Running Inference:  60%|█████▉    | 85/142 [03:14<01:57,  2.06s/it]Running Inference:  61%|██████    | 86/142 [03:15<01:32,  1.66s/it]Running Inference:  61%|██████▏   | 87/142 [03:16<01:26,  1.58s/it]Running Inference:  62%|██████▏   | 88/142 [03:17<01:17,  1.43s/it]Running Inference:  63%|██████▎   | 89/142 [03:20<01:30,  1.71s/it]Running Inference:  63%|██████▎   | 90/142 [03:21<01:23,  1.60s/it]Running Inference:  64%|██████▍   | 91/142 [03:25<02:02,  2.40s/it]Running Inference:  65%|██████▍   | 92/142 [03:29<02:20,  2.82s/it]Running Inference:  65%|██████▌   | 93/142 [03:31<02:07,  2.61s/it]Running Inference:  66%|██████▌   | 94/142 [03:32<01:36,  2.01s/it]Running Inference:  67%|██████▋   | 95/142 [03:33<01:19,  1.70s/it]Running Inference:  68%|██████▊   | 96/142 [03:34<01:07,  1.48s/it]Running Inference:  68%|██████▊   | 97/142 [03:38<01:40,  2.22s/it]Running Inference:  69%|██████▉   | 98/142 [03:38<01:17,  1.77s/it]Running Inference:  70%|██████▉   | 99/142 [03:43<01:48,  2.52s/it]Running Inference:  70%|███████   | 100/142 [03:47<02:03,  2.95s/it]Running Inference:  71%|███████   | 101/142 [03:47<01:33,  2.28s/it]Running Inference:  72%|███████▏  | 102/142 [03:51<01:50,  2.77s/it]Running Inference:  73%|███████▎  | 103/142 [03:54<01:47,  2.76s/it]Running Inference:  73%|███████▎  | 104/142 [03:58<02:01,  3.21s/it]Running Inference:  74%|███████▍  | 105/142 [04:03<02:11,  3.55s/it]Running Inference:  75%|███████▍  | 106/142 [04:11<03:02,  5.07s/it]Running Inference:  75%|███████▌  | 107/142 [04:16<02:51,  4.90s/it]Running Inference:  76%|███████▌  | 108/142 [04:16<02:01,  3.58s/it]Running Inference:  77%|███████▋  | 109/142 [04:20<02:02,  3.71s/it]Running Inference:  77%|███████▋  | 110/142 [04:22<01:40,  3.13s/it]Running Inference:  78%|███████▊  | 111/142 [04:23<01:13,  2.38s/it]Running Inference:  79%|███████▉  | 112/142 [04:23<00:55,  1.84s/it]Running Inference:  80%|███████▉  | 113/142 [04:24<00:42,  1.46s/it]Running Inference:  80%|████████  | 114/142 [04:25<00:34,  1.23s/it]Running Inference:  81%|████████  | 115/142 [04:25<00:28,  1.06s/it]Running Inference:  82%|████████▏ | 116/142 [04:29<00:48,  1.87s/it]Running Inference:  82%|████████▏ | 117/142 [04:30<00:40,  1.64s/it]Running Inference:  83%|████████▎ | 118/142 [04:31<00:35,  1.49s/it]Running Inference:  84%|████████▍ | 119/142 [04:32<00:26,  1.14s/it]Running Inference:  85%|████████▍ | 120/142 [04:32<00:23,  1.07s/it]Running Inference:  85%|████████▌ | 121/142 [04:37<00:43,  2.06s/it]Running Inference:  86%|████████▌ | 122/142 [04:41<00:52,  2.63s/it]Running Inference:  87%|████████▋ | 123/142 [04:45<00:56,  3.00s/it]Running Inference:  87%|████████▋ | 124/142 [04:45<00:40,  2.25s/it]Running Inference:  88%|████████▊ | 125/142 [04:47<00:34,  2.04s/it]Running Inference:  89%|████████▊ | 126/142 [04:51<00:42,  2.67s/it]Running Inference:  89%|████████▉ | 127/142 [04:53<00:35,  2.37s/it]Running Inference:  90%|█████████ | 128/142 [04:53<00:26,  1.89s/it]Running Inference:  91%|█████████ | 129/142 [04:54<00:20,  1.57s/it]Running Inference:  92%|█████████▏| 130/142 [04:59<00:29,  2.49s/it]Running Inference:  92%|█████████▏| 131/142 [04:59<00:21,  1.94s/it]Running Inference:  93%|█████████▎| 132/142 [05:00<00:15,  1.51s/it]Running Inference:  94%|█████████▎| 133/142 [05:12<00:40,  4.54s/it]Running Inference:  94%|█████████▍| 134/142 [05:16<00:35,  4.48s/it]Running Inference:  95%|█████████▌| 135/142 [05:17<00:23,  3.36s/it]Running Inference:  96%|█████████▌| 136/142 [05:18<00:16,  2.83s/it]Running Inference:  96%|█████████▋| 137/142 [05:20<00:12,  2.43s/it]Running Inference:  97%|█████████▋| 138/142 [05:20<00:07,  1.81s/it]Running Inference:  98%|█████████▊| 139/142 [05:21<00:04,  1.61s/it]Running Inference:  99%|█████████▊| 140/142 [05:22<00:02,  1.44s/it]Running Inference:  99%|█████████▉| 141/142 [05:24<00:01,  1.42s/it]Running Inference: 100%|██████████| 142/142 [05:28<00:00,  2.22s/it]Running Inference: 100%|██████████| 142/142 [05:28<00:00,  2.31s/it]
2025-12-14 22:29:06,626 - INFO - Inference completed.
2025-12-14 22:29:06,634 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 22:29:06,634 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.664 seconds.
Prefix dict has been built successfully.
2025-12-14 22:29:07,370 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 22:29:07,370 - INFO - Metrics:
33.31
2025-12-14 22:29:07,372 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_zh, ratio=0.2) on GPU 2

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:29:13,921 - INFO - Set deterministic seeds to 42
2025-12-14 22:29:13,922 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:29:13,922 - INFO - Starting evaluation run...
2025-12-14 22:29:13,922 - INFO - Output directory set to: longbenchresult
2025-12-14 22:29:13,922 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 22:29:13,922 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 22:29:13,922 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:29:13,922 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.19it/s]
Device set to use cuda:0
2025-12-14 22:29:54,934 - INFO - Model pipeline loaded.
2025-12-14 22:29:54,935 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 22:30:11,936 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:30:11,937 - INFO - Dataset processed with 200 entries.
2025-12-14 22:30:11,948 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:02<06:03,  2.58s/it]Running Inference:   1%|▏         | 2/142 [00:10<13:32,  5.80s/it]Running Inference:   2%|▏         | 3/142 [00:15<12:06,  5.23s/it]Running Inference:   3%|▎         | 4/142 [00:19<10:55,  4.75s/it]Running Inference:   4%|▎         | 5/142 [00:19<07:26,  3.26s/it]Running Inference:   4%|▍         | 6/142 [00:20<05:09,  2.28s/it]Running Inference:   5%|▍         | 7/142 [00:21<04:26,  1.97s/it]Running Inference:   6%|▌         | 8/142 [00:22<03:58,  1.78s/it]Running Inference:   6%|▋         | 9/142 [00:30<07:41,  3.47s/it]Running Inference:   7%|▋         | 10/142 [00:35<08:56,  4.06s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:37<07:27,  3.42s/it]Running Inference:   8%|▊         | 12/142 [00:40<07:05,  3.27s/it]Running Inference:   9%|▉         | 13/142 [00:44<07:47,  3.62s/it]Running Inference:  10%|▉         | 14/142 [00:46<06:19,  2.97s/it]Running Inference:  11%|█         | 15/142 [00:46<04:45,  2.25s/it]Running Inference:  11%|█▏        | 16/142 [00:52<07:11,  3.42s/it]Running Inference:  12%|█▏        | 17/142 [01:00<09:59,  4.80s/it]Running Inference:  13%|█▎        | 18/142 [01:04<09:18,  4.50s/it]Running Inference:  13%|█▎        | 19/142 [01:09<09:20,  4.56s/it]Running Inference:  14%|█▍        | 20/142 [01:10<06:58,  3.43s/it]Running Inference:  15%|█▍        | 21/142 [01:15<07:51,  3.90s/it]Running Inference:  15%|█▌        | 22/142 [01:19<07:58,  3.99s/it]Running Inference:  16%|█▌        | 23/142 [01:20<05:54,  2.98s/it]Running Inference:  17%|█▋        | 24/142 [01:23<06:24,  3.25s/it]Running Inference:  18%|█▊        | 25/142 [01:27<06:36,  3.39s/it]Running Inference:  18%|█▊        | 26/142 [01:31<07:00,  3.62s/it]Running Inference:  19%|█▉        | 27/142 [01:35<06:59,  3.65s/it]Running Inference:  20%|█▉        | 28/142 [01:38<06:42,  3.53s/it]Running Inference:  20%|██        | 29/142 [01:44<07:39,  4.07s/it]Running Inference:  21%|██        | 30/142 [01:44<05:46,  3.10s/it]Running Inference:  22%|██▏       | 31/142 [01:48<05:52,  3.17s/it]Running Inference:  23%|██▎       | 32/142 [01:49<04:48,  2.63s/it]Running Inference:  23%|██▎       | 33/142 [01:50<03:34,  1.96s/it]Running Inference:  24%|██▍       | 34/142 [01:50<02:53,  1.60s/it]Running Inference:  25%|██▍       | 35/142 [01:54<04:11,  2.35s/it]Running Inference:  25%|██▌       | 36/142 [01:57<04:08,  2.34s/it]Running Inference:  26%|██▌       | 37/142 [01:57<03:13,  1.84s/it]Running Inference:  27%|██▋       | 38/142 [02:00<03:36,  2.08s/it]Running Inference:  27%|██▋       | 39/142 [02:04<04:38,  2.70s/it]Running Inference:  28%|██▊       | 40/142 [02:06<03:59,  2.35s/it]Running Inference:  29%|██▉       | 41/142 [02:08<03:39,  2.17s/it]Running Inference:  30%|██▉       | 42/142 [02:09<03:18,  1.98s/it]Running Inference:  30%|███       | 43/142 [02:13<04:13,  2.56s/it]Running Inference:  31%|███       | 44/142 [02:17<04:53,  2.99s/it]Running Inference:  32%|███▏      | 45/142 [02:21<05:22,  3.32s/it]Running Inference:  32%|███▏      | 46/142 [02:22<04:04,  2.54s/it]Running Inference:  33%|███▎      | 47/142 [02:23<03:34,  2.26s/it]Running Inference:  34%|███▍      | 48/142 [02:24<02:41,  1.72s/it]Running Inference:  35%|███▍      | 49/142 [02:25<02:12,  1.42s/it]Running Inference:  35%|███▌      | 50/142 [02:26<02:01,  1.32s/it]Running Inference:  36%|███▌      | 51/142 [02:30<03:10,  2.10s/it]Running Inference:  37%|███▋      | 52/142 [02:31<02:56,  1.96s/it]Running Inference:  37%|███▋      | 53/142 [02:32<02:21,  1.59s/it]Running Inference:  38%|███▊      | 54/142 [02:33<01:59,  1.36s/it]Running Inference:  39%|███▊      | 55/142 [02:36<02:39,  1.83s/it]Running Inference:  39%|███▉      | 56/142 [02:38<02:41,  1.88s/it]Running Inference:  40%|████      | 57/142 [02:42<03:29,  2.46s/it]Running Inference:  41%|████      | 58/142 [02:42<02:42,  1.93s/it]Running Inference:  42%|████▏     | 59/142 [02:44<02:32,  1.83s/it]Running Inference:  42%|████▏     | 60/142 [02:44<01:58,  1.44s/it]Running Inference:  43%|████▎     | 61/142 [02:47<02:23,  1.77s/it]Running Inference:  44%|████▎     | 62/142 [02:48<02:06,  1.58s/it]Running Inference:  44%|████▍     | 63/142 [02:54<03:42,  2.81s/it]Running Inference:  45%|████▌     | 64/142 [02:59<04:28,  3.44s/it]Running Inference:  46%|████▌     | 65/142 [03:00<03:35,  2.79s/it]Running Inference:  46%|████▋     | 66/142 [03:04<04:12,  3.32s/it]Running Inference:  47%|████▋     | 67/142 [03:05<03:11,  2.55s/it]Running Inference:  48%|████▊     | 68/142 [03:06<02:23,  1.94s/it]Running Inference:  49%|████▊     | 69/142 [03:07<02:12,  1.82s/it]Running Inference:  49%|████▉     | 70/142 [03:08<01:43,  1.43s/it]Running Inference:  50%|█████     | 71/142 [03:12<02:39,  2.25s/it]Running Inference:  51%|█████     | 72/142 [03:13<02:04,  1.78s/it]Running Inference:  51%|█████▏    | 73/142 [03:14<01:46,  1.55s/it]Running Inference:  52%|█████▏    | 74/142 [03:15<01:46,  1.57s/it]Running Inference:  53%|█████▎    | 75/142 [03:16<01:19,  1.19s/it]Running Inference:  54%|█████▎    | 76/142 [03:16<01:06,  1.01s/it]Running Inference:  54%|█████▍    | 77/142 [03:17<00:57,  1.14it/s]Running Inference:  55%|█████▍    | 78/142 [03:21<01:54,  1.79s/it]Running Inference:  56%|█████▌    | 79/142 [03:23<01:55,  1.84s/it]Running Inference:  56%|█████▋    | 80/142 [03:23<01:28,  1.43s/it]Running Inference:  57%|█████▋    | 81/142 [03:24<01:27,  1.43s/it]Running Inference:  58%|█████▊    | 82/142 [03:25<01:07,  1.13s/it]Running Inference:  58%|█████▊    | 83/142 [03:29<01:58,  2.01s/it]Running Inference:  59%|█████▉    | 84/142 [03:30<01:42,  1.77s/it]Running Inference:  60%|█████▉    | 85/142 [03:31<01:22,  1.44s/it]Running Inference:  61%|██████    | 86/142 [03:35<02:00,  2.16s/it]Running Inference:  61%|██████▏   | 87/142 [03:36<01:46,  1.93s/it]Running Inference:  62%|██████▏   | 88/142 [03:37<01:30,  1.68s/it]Running Inference:  63%|██████▎   | 89/142 [03:39<01:26,  1.64s/it]Running Inference:  63%|██████▎   | 90/142 [03:41<01:36,  1.85s/it]Running Inference:  64%|██████▍   | 91/142 [03:45<02:11,  2.57s/it]Running Inference:  65%|██████▍   | 92/142 [03:47<01:48,  2.17s/it]Running Inference:  65%|██████▌   | 93/142 [03:48<01:38,  2.01s/it]Running Inference:  66%|██████▌   | 94/142 [03:49<01:14,  1.55s/it]Running Inference:  67%|██████▋   | 95/142 [03:50<01:05,  1.38s/it]Running Inference:  68%|██████▊   | 96/142 [03:51<00:57,  1.26s/it]Running Inference:  68%|██████▊   | 97/142 [03:55<01:33,  2.08s/it]Running Inference:  69%|██████▉   | 98/142 [03:55<01:13,  1.68s/it]Running Inference:  70%|██████▉   | 99/142 [04:00<01:45,  2.46s/it]Running Inference:  70%|███████   | 100/142 [04:04<02:02,  2.92s/it]Running Inference:  71%|███████   | 101/142 [04:04<01:32,  2.26s/it]Running Inference:  72%|███████▏  | 102/142 [04:05<01:09,  1.74s/it]Running Inference:  73%|███████▎  | 103/142 [04:08<01:19,  2.05s/it]Running Inference:  73%|███████▎  | 104/142 [04:12<01:43,  2.71s/it]Running Inference:  74%|███████▍  | 105/142 [04:19<02:33,  4.14s/it]Running Inference:  75%|███████▍  | 106/142 [04:23<02:20,  3.89s/it]Running Inference:  75%|███████▌  | 107/142 [04:27<02:22,  4.08s/it]Running Inference:  76%|███████▌  | 108/142 [04:28<01:42,  3.01s/it]Running Inference:  77%|███████▋  | 109/142 [04:29<01:21,  2.46s/it]Running Inference:  77%|███████▋  | 110/142 [04:31<01:12,  2.26s/it]Running Inference:  78%|███████▊  | 111/142 [04:31<00:54,  1.77s/it]Running Inference:  79%|███████▉  | 112/142 [04:32<00:41,  1.38s/it]Running Inference:  80%|███████▉  | 113/142 [04:32<00:32,  1.13s/it]Running Inference:  80%|████████  | 114/142 [04:36<00:56,  2.00s/it]Running Inference:  81%|████████  | 115/142 [04:37<00:43,  1.60s/it]Running Inference:  82%|████████▏ | 116/142 [04:37<00:32,  1.25s/it]Running Inference:  82%|████████▏ | 117/142 [04:39<00:29,  1.20s/it]Running Inference:  83%|████████▎ | 118/142 [04:40<00:28,  1.20s/it]Running Inference:  84%|████████▍ | 119/142 [04:40<00:21,  1.06it/s]Running Inference:  85%|████████▍ | 120/142 [04:41<00:19,  1.11it/s]Running Inference:  85%|████████▌ | 121/142 [04:45<00:40,  1.94s/it]Running Inference:  86%|████████▌ | 122/142 [04:49<00:50,  2.55s/it]Running Inference:  87%|████████▋ | 123/142 [04:53<00:55,  2.94s/it]Running Inference:  87%|████████▋ | 124/142 [04:54<00:39,  2.21s/it]Running Inference:  88%|████████▊ | 125/142 [04:55<00:34,  2.00s/it]Running Inference:  89%|████████▊ | 126/142 [04:59<00:42,  2.65s/it]Running Inference:  89%|████████▉ | 127/142 [05:01<00:35,  2.35s/it]Running Inference:  90%|█████████ | 128/142 [05:01<00:24,  1.75s/it]Running Inference:  91%|█████████ | 129/142 [05:02<00:19,  1.48s/it]Running Inference:  92%|█████████▏| 130/142 [05:07<00:29,  2.44s/it]Running Inference:  92%|█████████▏| 131/142 [05:07<00:20,  1.91s/it]Running Inference:  93%|█████████▎| 132/142 [05:08<00:14,  1.49s/it]Running Inference:  94%|█████████▎| 133/142 [05:20<00:40,  4.51s/it]Running Inference:  94%|█████████▍| 134/142 [05:24<00:35,  4.46s/it]Running Inference:  95%|█████████▌| 135/142 [05:25<00:23,  3.35s/it]Running Inference:  96%|█████████▌| 136/142 [05:26<00:16,  2.76s/it]Running Inference:  96%|█████████▋| 137/142 [05:27<00:11,  2.25s/it]Running Inference:  97%|█████████▋| 138/142 [05:27<00:06,  1.69s/it]Running Inference:  98%|█████████▊| 139/142 [05:28<00:04,  1.44s/it]Running Inference:  99%|█████████▊| 140/142 [05:29<00:02,  1.33s/it]Running Inference:  99%|█████████▉| 141/142 [05:31<00:01,  1.35s/it]Running Inference: 100%|██████████| 142/142 [05:33<00:00,  1.53s/it]Running Inference: 100%|██████████| 142/142 [05:33<00:00,  2.35s/it]
2025-12-14 22:35:45,198 - INFO - Inference completed.
2025-12-14 22:35:45,206 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 22:35:45,206 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.684 seconds.
Prefix dict has been built successfully.
2025-12-14 22:35:45,975 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 22:35:45,975 - INFO - Metrics:
30.42
2025-12-14 22:35:45,976 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_zh, ratio=0.3) on GPU 2

----------------------------------------
Task: multifieldqa_zh | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:35:52,489 - INFO - Set deterministic seeds to 42
2025-12-14 22:35:52,489 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "multifieldqa_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:35:52,489 - INFO - Starting evaluation run...
2025-12-14 22:35:52,489 - INFO - Output directory set to: longbenchresult
2025-12-14 22:35:52,490 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 22:35:52,490 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 22:35:52,490 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:35:52,490 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.68it/s]
Device set to use cuda:0
2025-12-14 22:36:06,046 - INFO - Model pipeline loaded.
2025-12-14 22:36:06,046 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: multifieldqa_zh)
2025-12-14 22:36:12,636 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:36:12,636 - INFO - Dataset processed with 200 entries.
2025-12-14 22:36:12,646 - INFO - Starting inference...
Running Inference:   0%|          | 0/142 [00:00<?, ?it/s]Running Inference:   1%|          | 1/142 [00:02<05:53,  2.51s/it]Running Inference:   1%|▏         | 2/142 [00:05<06:25,  2.75s/it]Running Inference:   2%|▏         | 3/142 [00:06<04:37,  2.00s/it]Running Inference:   3%|▎         | 4/142 [00:07<03:20,  1.45s/it]Running Inference:   4%|▎         | 5/142 [00:07<02:37,  1.15s/it]Running Inference:   4%|▍         | 6/142 [00:08<01:59,  1.14it/s]Running Inference:   5%|▍         | 7/142 [00:10<02:53,  1.28s/it]Running Inference:   6%|▌         | 8/142 [00:12<03:20,  1.49s/it]Running Inference:   6%|▋         | 9/142 [00:19<07:08,  3.22s/it]Running Inference:   7%|▋         | 10/142 [00:24<08:36,  3.91s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   8%|▊         | 11/142 [00:26<07:22,  3.38s/it]Running Inference:   8%|▊         | 12/142 [00:27<05:49,  2.69s/it]Running Inference:   9%|▉         | 13/142 [00:32<06:50,  3.18s/it]Running Inference:  10%|▉         | 14/142 [00:36<07:34,  3.55s/it]Running Inference:  11%|█         | 15/142 [00:37<05:37,  2.66s/it]Running Inference:  11%|█▏        | 16/142 [00:44<08:42,  4.14s/it]Running Inference:  12%|█▏        | 17/142 [00:50<09:25,  4.52s/it]Running Inference:  13%|█▎        | 18/142 [00:53<08:50,  4.27s/it]Running Inference:  13%|█▎        | 19/142 [00:58<08:56,  4.37s/it]Running Inference:  14%|█▍        | 20/142 [00:59<06:41,  3.29s/it]Running Inference:  15%|█▍        | 21/142 [01:10<11:37,  5.77s/it]Running Inference:  15%|█▌        | 22/142 [01:14<10:31,  5.27s/it]Running Inference:  16%|█▌        | 23/142 [01:15<07:40,  3.87s/it]Running Inference:  17%|█▋        | 24/142 [01:19<07:34,  3.85s/it]Running Inference:  18%|█▊        | 25/142 [01:19<05:26,  2.79s/it]Running Inference:  18%|█▊        | 26/142 [01:23<06:08,  3.17s/it]Running Inference:  19%|█▉        | 27/142 [01:24<04:36,  2.41s/it]Running Inference:  20%|█▉        | 28/142 [01:25<04:00,  2.11s/it]Running Inference:  20%|██        | 29/142 [01:30<05:21,  2.84s/it]Running Inference:  21%|██        | 30/142 [01:31<04:09,  2.23s/it]Running Inference:  22%|██▏       | 31/142 [01:34<04:39,  2.52s/it]Running Inference:  23%|██▎       | 32/142 [01:35<03:57,  2.16s/it]Running Inference:  23%|██▎       | 33/142 [01:36<02:58,  1.64s/it]Running Inference:  24%|██▍       | 34/142 [01:36<02:28,  1.37s/it]Running Inference:  25%|██▍       | 35/142 [01:40<03:49,  2.15s/it]Running Inference:  25%|██▌       | 36/142 [01:43<03:51,  2.18s/it]Running Inference:  26%|██▌       | 37/142 [01:43<03:02,  1.74s/it]Running Inference:  27%|██▋       | 38/142 [01:45<03:16,  1.89s/it]Running Inference:  27%|██▋       | 39/142 [01:50<04:21,  2.54s/it]Running Inference:  28%|██▊       | 40/142 [01:51<03:48,  2.24s/it]Running Inference:  29%|██▉       | 41/142 [01:55<04:44,  2.82s/it]Running Inference:  30%|██▉       | 42/142 [01:57<04:01,  2.42s/it]Running Inference:  30%|███       | 43/142 [02:01<04:41,  2.84s/it]Running Inference:  31%|███       | 44/142 [02:03<04:31,  2.77s/it]Running Inference:  32%|███▏      | 45/142 [02:08<05:18,  3.29s/it]Running Inference:  32%|███▏      | 46/142 [02:08<04:01,  2.52s/it]Running Inference:  33%|███▎      | 47/142 [02:12<04:39,  2.94s/it]Running Inference:  34%|███▍      | 48/142 [02:13<03:26,  2.19s/it]Running Inference:  35%|███▍      | 49/142 [02:13<02:42,  1.75s/it]Running Inference:  35%|███▌      | 50/142 [02:15<02:22,  1.55s/it]Running Inference:  36%|███▌      | 51/142 [02:18<03:23,  2.24s/it]Running Inference:  37%|███▋      | 52/142 [02:20<03:16,  2.18s/it]Running Inference:  37%|███▋      | 53/142 [02:21<02:34,  1.74s/it]Running Inference:  38%|███▊      | 54/142 [02:22<02:08,  1.46s/it]Running Inference:  39%|███▊      | 55/142 [02:26<03:08,  2.16s/it]Running Inference:  39%|███▉      | 56/142 [02:28<03:00,  2.10s/it]Running Inference:  40%|████      | 57/142 [02:31<03:40,  2.60s/it]Running Inference:  41%|████      | 58/142 [02:32<02:43,  1.95s/it]Running Inference:  42%|████▏     | 59/142 [02:33<02:32,  1.84s/it]Running Inference:  42%|████▏     | 60/142 [02:34<01:57,  1.44s/it]Running Inference:  43%|████▎     | 61/142 [02:38<03:02,  2.25s/it]Running Inference:  44%|████▎     | 62/142 [02:39<02:31,  1.89s/it]Running Inference:  44%|████▍     | 63/142 [02:45<04:05,  3.11s/it]Running Inference:  45%|████▌     | 64/142 [02:49<04:29,  3.46s/it]Running Inference:  46%|████▌     | 65/142 [02:51<03:35,  2.80s/it]Running Inference:  46%|████▋     | 66/142 [02:55<04:10,  3.29s/it]Running Inference:  47%|████▋     | 67/142 [02:56<03:23,  2.71s/it]Running Inference:  48%|████▊     | 68/142 [02:57<02:30,  2.04s/it]Running Inference:  49%|████▊     | 69/142 [02:58<02:17,  1.89s/it]Running Inference:  49%|████▉     | 70/142 [02:59<01:46,  1.48s/it]Running Inference:  50%|█████     | 71/142 [03:03<02:40,  2.26s/it]Running Inference:  51%|█████     | 72/142 [03:04<02:05,  1.79s/it]Running Inference:  51%|█████▏    | 73/142 [03:05<01:47,  1.55s/it]Running Inference:  52%|█████▏    | 74/142 [03:06<01:34,  1.39s/it]Running Inference:  53%|█████▎    | 75/142 [03:06<01:11,  1.07s/it]Running Inference:  54%|█████▎    | 76/142 [03:07<01:00,  1.09it/s]Running Inference:  54%|█████▍    | 77/142 [03:07<00:53,  1.23it/s]Running Inference:  55%|█████▍    | 78/142 [03:11<01:48,  1.69s/it]Running Inference:  56%|█████▌    | 79/142 [03:16<02:40,  2.55s/it]Running Inference:  56%|█████▋    | 80/142 [03:16<01:59,  1.93s/it]Running Inference:  57%|█████▋    | 81/142 [03:17<01:48,  1.78s/it]Running Inference:  58%|█████▊    | 82/142 [03:18<01:22,  1.37s/it]Running Inference:  58%|█████▊    | 83/142 [03:22<02:12,  2.24s/it]Running Inference:  59%|█████▉    | 84/142 [03:24<02:03,  2.13s/it]Running Inference:  60%|█████▉    | 85/142 [03:25<01:35,  1.68s/it]Running Inference:  61%|██████    | 86/142 [03:28<02:08,  2.30s/it]Running Inference:  61%|██████▏   | 87/142 [03:30<01:51,  2.02s/it]Running Inference:  62%|██████▏   | 88/142 [03:31<01:33,  1.74s/it]Running Inference:  63%|██████▎   | 89/142 [03:33<01:39,  1.88s/it]Running Inference:  63%|██████▎   | 90/142 [03:35<01:44,  2.01s/it]Running Inference:  64%|██████▍   | 91/142 [03:39<02:15,  2.65s/it]Running Inference:  65%|██████▍   | 92/142 [03:41<01:58,  2.37s/it]Running Inference:  65%|██████▌   | 93/142 [03:43<01:52,  2.30s/it]Running Inference:  66%|██████▌   | 94/142 [03:44<01:24,  1.75s/it]Running Inference:  67%|██████▋   | 95/142 [03:48<01:56,  2.48s/it]Running Inference:  68%|██████▊   | 96/142 [03:49<01:32,  2.02s/it]Running Inference:  68%|██████▊   | 97/142 [03:50<01:18,  1.73s/it]Running Inference:  69%|██████▉   | 98/142 [03:51<01:02,  1.43s/it]Running Inference:  70%|██████▉   | 99/142 [03:55<01:37,  2.26s/it]Running Inference:  70%|███████   | 100/142 [03:55<01:12,  1.74s/it]Running Inference:  71%|███████   | 101/142 [03:56<00:59,  1.44s/it]Running Inference:  72%|███████▏  | 102/142 [03:57<00:46,  1.16s/it]Running Inference:  73%|███████▎  | 103/142 [03:59<01:03,  1.63s/it]Running Inference:  73%|███████▎  | 104/142 [04:04<01:30,  2.39s/it]Running Inference:  74%|███████▍  | 105/142 [04:11<02:23,  3.87s/it]Running Inference:  75%|███████▍  | 106/142 [04:14<02:14,  3.74s/it]Running Inference:  75%|███████▌  | 107/142 [04:17<02:00,  3.43s/it]Running Inference:  76%|███████▌  | 108/142 [04:18<01:26,  2.55s/it]Running Inference:  77%|███████▋  | 109/142 [04:22<01:38,  2.98s/it]Running Inference:  77%|███████▋  | 110/142 [04:23<01:23,  2.61s/it]Running Inference:  78%|███████▊  | 111/142 [04:24<01:02,  2.02s/it]Running Inference:  79%|███████▉  | 112/142 [04:24<00:46,  1.55s/it]Running Inference:  80%|███████▉  | 113/142 [04:25<00:36,  1.26s/it]Running Inference:  80%|████████  | 114/142 [04:26<00:31,  1.12s/it]Running Inference:  81%|████████  | 115/142 [04:26<00:26,  1.02it/s]Running Inference:  82%|████████▏ | 116/142 [04:30<00:46,  1.79s/it]Running Inference:  82%|████████▏ | 117/142 [04:31<00:39,  1.58s/it]Running Inference:  83%|████████▎ | 118/142 [04:32<00:34,  1.45s/it]Running Inference:  84%|████████▍ | 119/142 [04:33<00:25,  1.12s/it]Running Inference:  85%|████████▍ | 120/142 [04:40<01:05,  2.98s/it]Running Inference:  85%|████████▌ | 121/142 [04:44<01:10,  3.37s/it]Running Inference:  86%|████████▌ | 122/142 [04:45<00:50,  2.52s/it]Running Inference:  87%|████████▋ | 123/142 [04:49<00:55,  2.90s/it]Running Inference:  87%|████████▋ | 124/142 [04:49<00:39,  2.18s/it]Running Inference:  88%|████████▊ | 125/142 [04:51<00:33,  1.98s/it]Running Inference:  89%|████████▊ | 126/142 [04:55<00:41,  2.61s/it]Running Inference:  89%|████████▉ | 127/142 [04:56<00:34,  2.32s/it]Running Inference:  90%|█████████ | 128/142 [04:57<00:24,  1.73s/it]Running Inference:  91%|█████████ | 129/142 [04:58<00:19,  1.48s/it]Running Inference:  92%|█████████▏| 130/142 [05:02<00:28,  2.42s/it]Running Inference:  92%|█████████▏| 131/142 [05:03<00:20,  1.89s/it]Running Inference:  93%|█████████▎| 132/142 [05:03<00:14,  1.47s/it]Running Inference:  94%|█████████▎| 133/142 [05:13<00:34,  3.80s/it]Running Inference:  94%|█████████▍| 134/142 [05:17<00:31,  3.94s/it]Running Inference:  95%|█████████▌| 135/142 [05:18<00:20,  2.98s/it]Running Inference:  96%|█████████▌| 136/142 [05:22<00:21,  3.53s/it]Running Inference:  96%|█████████▋| 137/142 [05:26<00:18,  3.68s/it]Running Inference:  97%|█████████▋| 138/142 [05:27<00:10,  2.68s/it]Running Inference:  98%|█████████▊| 139/142 [05:28<00:06,  2.14s/it]Running Inference:  99%|█████████▊| 140/142 [05:29<00:03,  1.85s/it]Running Inference:  99%|█████████▉| 141/142 [05:30<00:01,  1.70s/it]Running Inference: 100%|██████████| 142/142 [05:34<00:00,  2.41s/it]Running Inference: 100%|██████████| 142/142 [05:34<00:00,  2.36s/it]
2025-12-14 22:41:47,354 - INFO - Inference completed.
2025-12-14 22:41:47,363 - INFO - Results saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 22:41:47,363 - INFO - Calculating metrics for dataset: longbench
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.660 seconds.
Prefix dict has been built successfully.
2025-12-14 22:41:48,096 - INFO - Metrics saved to longbenchresult/longbench__multifieldqa_zh__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 22:41:48,096 - INFO - Metrics:
26.17
2025-12-14 22:41:48,097 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=multifieldqa_zh, ratio=0.5) on GPU 2


========================================
LongBench Task: passage_retrieval_zh
========================================
----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:41:54,671 - INFO - Set deterministic seeds to 42
2025-12-14 22:41:54,671 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:41:54,671 - INFO - Starting evaluation run...
2025-12-14 22:41:54,671 - INFO - Output directory set to: longbenchresult
2025-12-14 22:41:54,671 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 22:41:54,672 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 22:41:54,672 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:41:54,672 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.77it/s]
Device set to use cuda:0
2025-12-14 22:42:08,371 - INFO - Model pipeline loaded.
2025-12-14 22:42:08,372 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 22:42:20,467 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:42:20,467 - INFO - Dataset processed with 200 entries.
2025-12-14 22:42:20,479 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:48,  3.56s/it]Running Inference:   1%|          | 2/200 [00:06<10:05,  3.06s/it]Running Inference:   2%|▏         | 3/200 [00:09<09:38,  2.94s/it]Running Inference:   2%|▏         | 4/200 [00:11<09:20,  2.86s/it]Running Inference:   2%|▎         | 5/200 [00:14<09:07,  2.81s/it]Running Inference:   3%|▎         | 6/200 [00:17<09:00,  2.79s/it]Running Inference:   4%|▎         | 7/200 [00:20<08:56,  2.78s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:46,  2.74s/it]Running Inference:   4%|▍         | 9/200 [00:23<07:10,  2.26s/it]Running Inference:   5%|▌         | 10/200 [00:25<06:30,  2.06s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:28<07:10,  2.28s/it]Running Inference:   6%|▌         | 12/200 [00:31<07:39,  2.44s/it]Running Inference:   6%|▋         | 13/200 [00:31<05:58,  1.92s/it]Running Inference:   7%|▋         | 14/200 [00:34<06:39,  2.15s/it]Running Inference:   8%|▊         | 15/200 [00:37<07:04,  2.29s/it]Running Inference:   8%|▊         | 16/200 [00:39<07:24,  2.41s/it]Running Inference:   8%|▊         | 17/200 [00:42<07:34,  2.49s/it]Running Inference:   9%|▉         | 18/200 [00:45<07:43,  2.54s/it]Running Inference:  10%|▉         | 19/200 [00:47<07:53,  2.62s/it]Running Inference:  10%|█         | 20/200 [00:50<07:53,  2.63s/it]Running Inference:  10%|█         | 21/200 [00:53<07:53,  2.65s/it]Running Inference:  11%|█         | 22/200 [00:56<07:58,  2.69s/it]Running Inference:  12%|█▏        | 23/200 [00:58<07:57,  2.70s/it]Running Inference:  12%|█▏        | 24/200 [01:01<07:51,  2.68s/it]Running Inference:  12%|█▎        | 25/200 [01:04<07:54,  2.71s/it]Running Inference:  13%|█▎        | 26/200 [01:06<07:48,  2.69s/it]Running Inference:  14%|█▎        | 27/200 [01:09<07:43,  2.68s/it]Running Inference:  14%|█▍        | 28/200 [01:12<07:43,  2.69s/it]Running Inference:  14%|█▍        | 29/200 [01:14<07:39,  2.69s/it]Running Inference:  15%|█▌        | 30/200 [01:17<07:37,  2.69s/it]Running Inference:  16%|█▌        | 31/200 [01:20<07:36,  2.70s/it]Running Inference:  16%|█▌        | 32/200 [01:23<07:34,  2.71s/it]Running Inference:  16%|█▋        | 33/200 [01:25<07:30,  2.70s/it]Running Inference:  17%|█▋        | 34/200 [01:28<07:23,  2.67s/it]Running Inference:  18%|█▊        | 35/200 [01:30<07:21,  2.67s/it]Running Inference:  18%|█▊        | 36/200 [01:33<07:26,  2.72s/it]Running Inference:  18%|█▊        | 37/200 [01:36<07:26,  2.74s/it]Running Inference:  19%|█▉        | 38/200 [01:39<07:18,  2.71s/it]Running Inference:  20%|█▉        | 39/200 [01:41<07:14,  2.70s/it]Running Inference:  20%|██        | 40/200 [01:44<07:11,  2.70s/it]Running Inference:  20%|██        | 41/200 [01:46<06:21,  2.40s/it]Running Inference:  21%|██        | 42/200 [01:49<06:32,  2.48s/it]Running Inference:  22%|██▏       | 43/200 [01:51<06:26,  2.46s/it]Running Inference:  22%|██▏       | 44/200 [01:54<06:36,  2.54s/it]Running Inference:  22%|██▎       | 45/200 [01:56<06:37,  2.57s/it]Running Inference:  23%|██▎       | 46/200 [01:59<06:37,  2.58s/it]Running Inference:  24%|██▎       | 47/200 [02:02<06:42,  2.63s/it]Running Inference:  24%|██▍       | 48/200 [02:04<06:47,  2.68s/it]Running Inference:  24%|██▍       | 49/200 [02:07<06:46,  2.69s/it]Running Inference:  25%|██▌       | 50/200 [02:10<06:44,  2.69s/it]Running Inference:  26%|██▌       | 51/200 [02:13<06:43,  2.71s/it]Running Inference:  26%|██▌       | 52/200 [02:15<06:40,  2.71s/it]Running Inference:  26%|██▋       | 53/200 [02:18<06:37,  2.70s/it]Running Inference:  27%|██▋       | 54/200 [02:21<06:36,  2.71s/it]Running Inference:  28%|██▊       | 55/200 [02:23<06:34,  2.72s/it]Running Inference:  28%|██▊       | 56/200 [02:26<06:29,  2.70s/it]Running Inference:  28%|██▊       | 57/200 [02:29<06:24,  2.69s/it]Running Inference:  29%|██▉       | 58/200 [02:32<06:26,  2.72s/it]Running Inference:  30%|██▉       | 59/200 [02:34<06:27,  2.75s/it]Running Inference:  30%|███       | 60/200 [02:37<06:10,  2.65s/it]Running Inference:  30%|███       | 61/200 [02:39<06:07,  2.64s/it]Running Inference:  31%|███       | 62/200 [02:42<06:11,  2.69s/it]Running Inference:  32%|███▏      | 63/200 [02:45<06:08,  2.69s/it]Running Inference:  32%|███▏      | 64/200 [02:48<06:08,  2.71s/it]Running Inference:  32%|███▎      | 65/200 [02:50<06:06,  2.71s/it]Running Inference:  33%|███▎      | 66/200 [02:53<06:04,  2.72s/it]Running Inference:  34%|███▎      | 67/200 [02:56<06:06,  2.76s/it]Running Inference:  34%|███▍      | 68/200 [02:59<06:01,  2.74s/it]Running Inference:  34%|███▍      | 69/200 [03:01<05:55,  2.71s/it]Running Inference:  35%|███▌      | 70/200 [03:04<05:57,  2.75s/it]Running Inference:  36%|███▌      | 71/200 [03:07<05:53,  2.74s/it]Running Inference:  36%|███▌      | 72/200 [03:10<05:52,  2.75s/it]Running Inference:  36%|███▋      | 73/200 [03:12<05:43,  2.71s/it]Running Inference:  37%|███▋      | 74/200 [03:15<05:49,  2.77s/it]Running Inference:  38%|███▊      | 75/200 [03:18<05:43,  2.75s/it]Running Inference:  38%|███▊      | 76/200 [03:21<05:38,  2.73s/it]Running Inference:  38%|███▊      | 77/200 [03:23<05:35,  2.73s/it]Running Inference:  39%|███▉      | 78/200 [03:26<05:29,  2.70s/it]Running Inference:  40%|███▉      | 79/200 [03:29<05:24,  2.68s/it]Running Inference:  40%|████      | 80/200 [03:31<05:23,  2.69s/it]Running Inference:  40%|████      | 81/200 [03:34<05:22,  2.71s/it]Running Inference:  41%|████      | 82/200 [03:37<05:21,  2.73s/it]Running Inference:  42%|████▏     | 83/200 [03:40<05:19,  2.73s/it]Running Inference:  42%|████▏     | 84/200 [03:42<05:14,  2.71s/it]Running Inference:  42%|████▎     | 85/200 [03:45<05:10,  2.70s/it]Running Inference:  43%|████▎     | 86/200 [03:48<05:06,  2.69s/it]Running Inference:  44%|████▎     | 87/200 [03:50<05:01,  2.67s/it]Running Inference:  44%|████▍     | 88/200 [03:53<04:58,  2.67s/it]Running Inference:  44%|████▍     | 89/200 [03:56<05:00,  2.70s/it]Running Inference:  45%|████▌     | 90/200 [03:58<04:59,  2.73s/it]Running Inference:  46%|████▌     | 91/200 [04:01<04:56,  2.72s/it]Running Inference:  46%|████▌     | 92/200 [04:04<04:53,  2.72s/it]Running Inference:  46%|████▋     | 93/200 [04:06<04:49,  2.71s/it]Running Inference:  47%|████▋     | 94/200 [04:09<04:44,  2.68s/it]Running Inference:  48%|████▊     | 95/200 [04:12<04:41,  2.68s/it]Running Inference:  48%|████▊     | 96/200 [04:14<04:39,  2.69s/it]Running Inference:  48%|████▊     | 97/200 [04:17<04:37,  2.69s/it]Running Inference:  49%|████▉     | 98/200 [04:20<04:33,  2.68s/it]Running Inference:  50%|████▉     | 99/200 [04:23<04:32,  2.70s/it]Running Inference:  50%|█████     | 100/200 [04:25<04:32,  2.72s/it]Running Inference:  50%|█████     | 101/200 [04:28<04:26,  2.69s/it]Running Inference:  51%|█████     | 102/200 [04:31<04:27,  2.73s/it]Running Inference:  52%|█████▏    | 103/200 [04:34<04:24,  2.72s/it]Running Inference:  52%|█████▏    | 104/200 [04:36<04:23,  2.75s/it]Running Inference:  52%|█████▎    | 105/200 [04:39<04:20,  2.74s/it]Running Inference:  53%|█████▎    | 106/200 [04:42<04:15,  2.72s/it]Running Inference:  54%|█████▎    | 107/200 [04:44<04:11,  2.71s/it]Running Inference:  54%|█████▍    | 108/200 [04:47<04:09,  2.71s/it]Running Inference:  55%|█████▍    | 109/200 [04:48<03:14,  2.13s/it]Running Inference:  55%|█████▌    | 110/200 [04:51<03:29,  2.33s/it]Running Inference:  56%|█████▌    | 111/200 [04:53<03:39,  2.47s/it]Running Inference:  56%|█████▌    | 112/200 [04:56<03:47,  2.58s/it]Running Inference:  56%|█████▋    | 113/200 [04:59<03:47,  2.62s/it]Running Inference:  57%|█████▋    | 114/200 [05:02<03:49,  2.66s/it]Running Inference:  57%|█████▊    | 115/200 [05:03<03:07,  2.21s/it]Running Inference:  58%|█████▊    | 116/200 [05:06<03:16,  2.34s/it]Running Inference:  58%|█████▊    | 117/200 [05:08<03:23,  2.45s/it]Running Inference:  59%|█████▉    | 118/200 [05:10<02:57,  2.17s/it]Running Inference:  60%|█████▉    | 119/200 [05:13<03:10,  2.35s/it]Running Inference:  60%|██████    | 120/200 [05:15<03:14,  2.43s/it]Running Inference:  60%|██████    | 121/200 [05:18<03:18,  2.51s/it]Running Inference:  61%|██████    | 122/200 [05:19<02:48,  2.17s/it]Running Inference:  62%|██████▏   | 123/200 [05:22<02:57,  2.30s/it]Running Inference:  62%|██████▏   | 124/200 [05:25<03:04,  2.43s/it]Running Inference:  62%|██████▎   | 125/200 [05:27<03:08,  2.51s/it]Running Inference:  63%|██████▎   | 126/200 [05:30<03:09,  2.57s/it]Running Inference:  64%|██████▎   | 127/200 [05:33<03:10,  2.61s/it]Running Inference:  64%|██████▍   | 128/200 [05:35<03:08,  2.61s/it]Running Inference:  64%|██████▍   | 129/200 [05:38<03:08,  2.65s/it]Running Inference:  65%|██████▌   | 130/200 [05:41<03:07,  2.67s/it]Running Inference:  66%|██████▌   | 131/200 [05:44<03:07,  2.72s/it]Running Inference:  66%|██████▌   | 132/200 [05:46<03:05,  2.73s/it]Running Inference:  66%|██████▋   | 133/200 [05:49<03:02,  2.73s/it]Running Inference:  67%|██████▋   | 134/200 [05:52<02:59,  2.72s/it]Running Inference:  68%|██████▊   | 135/200 [05:55<02:57,  2.73s/it]Running Inference:  68%|██████▊   | 136/200 [05:57<02:53,  2.71s/it]Running Inference:  68%|██████▊   | 137/200 [06:00<02:50,  2.71s/it]Running Inference:  69%|██████▉   | 138/200 [06:03<02:47,  2.70s/it]Running Inference:  70%|██████▉   | 139/200 [06:05<02:44,  2.70s/it]Running Inference:  70%|███████   | 140/200 [06:08<02:43,  2.73s/it]Running Inference:  70%|███████   | 141/200 [06:11<02:40,  2.73s/it]Running Inference:  71%|███████   | 142/200 [06:14<02:37,  2.72s/it]Running Inference:  72%|███████▏  | 143/200 [06:16<02:33,  2.70s/it]Running Inference:  72%|███████▏  | 144/200 [06:19<02:29,  2.68s/it]Running Inference:  72%|███████▎  | 145/200 [06:22<02:27,  2.68s/it]Running Inference:  73%|███████▎  | 146/200 [06:24<02:22,  2.64s/it]Running Inference:  74%|███████▎  | 147/200 [06:27<02:21,  2.67s/it]Running Inference:  74%|███████▍  | 148/200 [06:29<02:18,  2.67s/it]Running Inference:  74%|███████▍  | 149/200 [06:32<02:16,  2.67s/it]Running Inference:  75%|███████▌  | 150/200 [06:35<02:14,  2.68s/it]Running Inference:  76%|███████▌  | 151/200 [06:37<02:10,  2.66s/it]Running Inference:  76%|███████▌  | 152/200 [06:40<02:08,  2.68s/it]Running Inference:  76%|███████▋  | 153/200 [06:43<02:05,  2.68s/it]Running Inference:  77%|███████▋  | 154/200 [06:45<01:56,  2.54s/it]Running Inference:  78%|███████▊  | 155/200 [06:48<01:58,  2.64s/it]Running Inference:  78%|███████▊  | 156/200 [06:51<01:56,  2.65s/it]Running Inference:  78%|███████▊  | 157/200 [06:53<01:54,  2.67s/it]Running Inference:  79%|███████▉  | 158/200 [06:56<01:52,  2.67s/it]Running Inference:  80%|███████▉  | 159/200 [06:59<01:50,  2.68s/it]Running Inference:  80%|████████  | 160/200 [07:01<01:47,  2.69s/it]Running Inference:  80%|████████  | 161/200 [07:04<01:45,  2.69s/it]Running Inference:  81%|████████  | 162/200 [07:05<01:18,  2.06s/it]Running Inference:  82%|████████▏ | 163/200 [07:07<01:23,  2.26s/it]Running Inference:  82%|████████▏ | 164/200 [07:10<01:26,  2.39s/it]Running Inference:  82%|████████▎ | 165/200 [07:12<01:13,  2.09s/it]Running Inference:  83%|████████▎ | 166/200 [07:14<01:17,  2.28s/it]Running Inference:  84%|████████▎ | 167/200 [07:17<01:19,  2.41s/it]Running Inference:  84%|████████▍ | 168/200 [07:20<01:20,  2.52s/it]Running Inference:  84%|████████▍ | 169/200 [07:22<01:19,  2.55s/it]Running Inference:  85%|████████▌ | 170/200 [07:25<01:18,  2.61s/it]Running Inference:  86%|████████▌ | 171/200 [07:28<01:15,  2.61s/it]Running Inference:  86%|████████▌ | 172/200 [07:30<01:14,  2.65s/it]Running Inference:  86%|████████▋ | 173/200 [07:33<01:10,  2.62s/it]Running Inference:  87%|████████▋ | 174/200 [07:36<01:08,  2.62s/it]Running Inference:  88%|████████▊ | 175/200 [07:38<01:06,  2.65s/it]Running Inference:  88%|████████▊ | 176/200 [07:41<01:04,  2.67s/it]Running Inference:  88%|████████▊ | 177/200 [07:44<01:01,  2.69s/it]Running Inference:  89%|████████▉ | 178/200 [07:47<00:59,  2.72s/it]Running Inference:  90%|████████▉ | 179/200 [07:49<00:57,  2.72s/it]Running Inference:  90%|█████████ | 180/200 [07:52<00:55,  2.75s/it]Running Inference:  90%|█████████ | 181/200 [07:55<00:52,  2.74s/it]Running Inference:  91%|█████████ | 182/200 [07:58<00:49,  2.76s/it]Running Inference:  92%|█████████▏| 183/200 [08:00<00:46,  2.75s/it]Running Inference:  92%|█████████▏| 184/200 [08:03<00:43,  2.73s/it]Running Inference:  92%|█████████▎| 185/200 [08:06<00:40,  2.71s/it]Running Inference:  93%|█████████▎| 186/200 [08:08<00:37,  2.70s/it]Running Inference:  94%|█████████▎| 187/200 [08:11<00:35,  2.72s/it]Running Inference:  94%|█████████▍| 188/200 [08:14<00:32,  2.70s/it]Running Inference:  94%|█████████▍| 189/200 [08:17<00:29,  2.70s/it]Running Inference:  95%|█████████▌| 190/200 [08:19<00:27,  2.72s/it]Running Inference:  96%|█████████▌| 191/200 [08:22<00:24,  2.72s/it]Running Inference:  96%|█████████▌| 192/200 [08:25<00:21,  2.72s/it]Running Inference:  96%|█████████▋| 193/200 [08:27<00:18,  2.69s/it]Running Inference:  97%|█████████▋| 194/200 [08:30<00:16,  2.73s/it]Running Inference:  98%|█████████▊| 195/200 [08:33<00:13,  2.72s/it]Running Inference:  98%|█████████▊| 196/200 [08:35<00:10,  2.69s/it]Running Inference:  98%|█████████▊| 197/200 [08:38<00:08,  2.72s/it]Running Inference:  99%|█████████▉| 198/200 [08:41<00:05,  2.69s/it]Running Inference: 100%|█████████▉| 199/200 [08:44<00:02,  2.70s/it]Running Inference: 100%|██████████| 200/200 [08:46<00:00,  2.70s/it]Running Inference: 100%|██████████| 200/200 [08:46<00:00,  2.63s/it]
2025-12-14 22:51:07,315 - INFO - Inference completed.
2025-12-14 22:51:07,324 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 22:51:07,324 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:51:07,326 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 22:51:07,326 - INFO - Metrics:
2.5
2025-12-14 22:51:07,328 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_zh, ratio=0.1) on GPU 2

----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 22:51:13,803 - INFO - Set deterministic seeds to 42
2025-12-14 22:51:13,803 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 22:51:13,803 - INFO - Starting evaluation run...
2025-12-14 22:51:13,803 - INFO - Output directory set to: longbenchresult
2025-12-14 22:51:13,804 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 22:51:13,804 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 22:51:13,804 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 22:51:13,804 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.25it/s]
Device set to use cuda:0
2025-12-14 22:51:24,963 - INFO - Model pipeline loaded.
2025-12-14 22:51:24,963 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 22:51:31,368 - INFO - Dataset loaded with 200 entries.
2025-12-14 22:51:31,368 - INFO - Dataset processed with 200 entries.
2025-12-14 22:51:31,380 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:21,  3.42s/it]Running Inference:   1%|          | 2/200 [00:06<09:51,  2.99s/it]Running Inference:   2%|▏         | 3/200 [00:08<09:28,  2.89s/it]Running Inference:   2%|▏         | 4/200 [00:11<09:11,  2.81s/it]Running Inference:   2%|▎         | 5/200 [00:14<08:59,  2.77s/it]Running Inference:   3%|▎         | 6/200 [00:16<08:52,  2.75s/it]Running Inference:   4%|▎         | 7/200 [00:19<08:47,  2.74s/it]Running Inference:   4%|▍         | 8/200 [00:22<08:36,  2.69s/it]Running Inference:   4%|▍         | 9/200 [00:23<07:03,  2.22s/it]Running Inference:   5%|▌         | 10/200 [00:26<07:26,  2.35s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:27<06:55,  2.20s/it]Running Inference:   6%|▌         | 12/200 [00:30<07:27,  2.38s/it]Running Inference:   6%|▋         | 13/200 [00:31<05:50,  1.87s/it]Running Inference:   7%|▋         | 14/200 [00:34<06:31,  2.10s/it]Running Inference:   8%|▊         | 15/200 [00:36<06:56,  2.25s/it]Running Inference:   8%|▊         | 16/200 [00:39<07:17,  2.38s/it]Running Inference:   8%|▊         | 17/200 [00:41<07:27,  2.45s/it]Running Inference:   9%|▉         | 18/200 [00:44<07:36,  2.51s/it]Running Inference:  10%|▉         | 19/200 [00:47<07:46,  2.58s/it]Running Inference:  10%|█         | 20/200 [00:48<06:06,  2.04s/it]Running Inference:  10%|█         | 21/200 [00:50<06:38,  2.22s/it]Running Inference:  11%|█         | 22/200 [00:53<07:04,  2.38s/it]Running Inference:  12%|█▏        | 23/200 [00:56<07:17,  2.47s/it]Running Inference:  12%|█▏        | 24/200 [00:57<06:12,  2.12s/it]Running Inference:  12%|█▎        | 25/200 [01:00<06:43,  2.31s/it]Running Inference:  13%|█▎        | 26/200 [01:02<06:57,  2.40s/it]Running Inference:  14%|█▎        | 27/200 [01:05<07:05,  2.46s/it]Running Inference:  14%|█▍        | 28/200 [01:08<07:15,  2.53s/it]Running Inference:  14%|█▍        | 29/200 [01:10<07:19,  2.57s/it]Running Inference:  15%|█▌        | 30/200 [01:13<07:21,  2.60s/it]Running Inference:  16%|█▌        | 31/200 [01:16<07:23,  2.62s/it]Running Inference:  16%|█▌        | 32/200 [01:18<07:23,  2.64s/it]Running Inference:  16%|█▋        | 33/200 [01:21<07:21,  2.64s/it]Running Inference:  17%|█▋        | 34/200 [01:24<07:14,  2.62s/it]Running Inference:  18%|█▊        | 35/200 [01:26<07:13,  2.63s/it]Running Inference:  18%|█▊        | 36/200 [01:29<07:18,  2.68s/it]Running Inference:  18%|█▊        | 37/200 [01:32<07:20,  2.70s/it]Running Inference:  19%|█▉        | 38/200 [01:34<06:33,  2.43s/it]Running Inference:  20%|█▉        | 39/200 [01:36<06:41,  2.49s/it]Running Inference:  20%|██        | 40/200 [01:39<06:45,  2.54s/it]Running Inference:  20%|██        | 41/200 [01:41<06:03,  2.28s/it]Running Inference:  21%|██        | 42/200 [01:43<06:18,  2.40s/it]Running Inference:  22%|██▏       | 43/200 [01:46<06:29,  2.48s/it]Running Inference:  22%|██▏       | 44/200 [01:49<06:36,  2.54s/it]Running Inference:  22%|██▎       | 45/200 [01:51<06:36,  2.56s/it]Running Inference:  23%|██▎       | 46/200 [01:54<06:35,  2.57s/it]Running Inference:  24%|██▎       | 47/200 [01:56<06:38,  2.60s/it]Running Inference:  24%|██▍       | 48/200 [01:59<06:43,  2.65s/it]Running Inference:  24%|██▍       | 49/200 [02:02<06:41,  2.66s/it]Running Inference:  25%|██▌       | 50/200 [02:05<06:38,  2.66s/it]Running Inference:  26%|██▌       | 51/200 [02:07<06:37,  2.67s/it]Running Inference:  26%|██▌       | 52/200 [02:10<06:34,  2.67s/it]Running Inference:  26%|██▋       | 53/200 [02:13<06:31,  2.66s/it]Running Inference:  27%|██▋       | 54/200 [02:15<06:30,  2.68s/it]Running Inference:  28%|██▊       | 55/200 [02:18<06:28,  2.68s/it]Running Inference:  28%|██▊       | 56/200 [02:21<06:24,  2.67s/it]Running Inference:  28%|██▊       | 57/200 [02:23<06:19,  2.65s/it]Running Inference:  29%|██▉       | 58/200 [02:26<06:21,  2.68s/it]Running Inference:  30%|██▉       | 59/200 [02:29<06:22,  2.71s/it]Running Inference:  30%|███       | 60/200 [02:31<06:16,  2.69s/it]Running Inference:  30%|███       | 61/200 [02:33<05:46,  2.49s/it]Running Inference:  31%|███       | 62/200 [02:35<05:03,  2.20s/it]Running Inference:  32%|███▏      | 63/200 [02:38<05:19,  2.33s/it]Running Inference:  32%|███▏      | 64/200 [02:40<05:32,  2.44s/it]Running Inference:  32%|███▎      | 65/200 [02:43<05:39,  2.51s/it]Running Inference:  33%|███▎      | 66/200 [02:46<05:43,  2.56s/it]Running Inference:  34%|███▎      | 67/200 [02:48<05:23,  2.43s/it]Running Inference:  34%|███▍      | 68/200 [02:50<05:29,  2.50s/it]Running Inference:  34%|███▍      | 69/200 [02:53<05:31,  2.53s/it]Running Inference:  35%|███▌      | 70/200 [02:56<05:39,  2.61s/it]Running Inference:  36%|███▌      | 71/200 [02:58<05:39,  2.63s/it]Running Inference:  36%|███▌      | 72/200 [03:01<05:41,  2.67s/it]Running Inference:  36%|███▋      | 73/200 [03:04<05:34,  2.64s/it]Running Inference:  37%|███▋      | 74/200 [03:07<05:41,  2.71s/it]Running Inference:  38%|███▊      | 75/200 [03:09<05:37,  2.70s/it]Running Inference:  38%|███▊      | 76/200 [03:12<05:34,  2.70s/it]Running Inference:  38%|███▊      | 77/200 [03:15<05:31,  2.70s/it]Running Inference:  39%|███▉      | 78/200 [03:17<05:25,  2.67s/it]Running Inference:  40%|███▉      | 79/200 [03:20<05:21,  2.65s/it]Running Inference:  40%|████      | 80/200 [03:23<05:16,  2.64s/it]Running Inference:  40%|████      | 81/200 [03:25<05:17,  2.67s/it]Running Inference:  41%|████      | 82/200 [03:27<04:42,  2.39s/it]Running Inference:  42%|████▏     | 83/200 [03:30<04:51,  2.49s/it]Running Inference:  42%|████▏     | 84/200 [03:32<04:53,  2.53s/it]Running Inference:  42%|████▎     | 85/200 [03:35<04:55,  2.57s/it]Running Inference:  43%|████▎     | 86/200 [03:38<04:54,  2.58s/it]Running Inference:  44%|████▎     | 87/200 [03:40<04:51,  2.58s/it]Running Inference:  44%|████▍     | 88/200 [03:43<04:50,  2.60s/it]Running Inference:  44%|████▍     | 89/200 [03:46<04:53,  2.65s/it]Running Inference:  45%|████▌     | 90/200 [03:48<04:53,  2.67s/it]Running Inference:  46%|████▌     | 91/200 [03:51<04:51,  2.67s/it]Running Inference:  46%|████▌     | 92/200 [03:54<04:49,  2.68s/it]Running Inference:  46%|████▋     | 93/200 [03:56<04:45,  2.67s/it]Running Inference:  47%|████▋     | 94/200 [03:59<04:39,  2.64s/it]Running Inference:  48%|████▊     | 95/200 [04:02<04:37,  2.65s/it]Running Inference:  48%|████▊     | 96/200 [04:04<04:35,  2.65s/it]Running Inference:  48%|████▊     | 97/200 [04:07<04:32,  2.65s/it]Running Inference:  49%|████▉     | 98/200 [04:10<04:29,  2.64s/it]Running Inference:  50%|████▉     | 99/200 [04:12<04:28,  2.66s/it]Running Inference:  50%|█████     | 100/200 [04:15<04:28,  2.68s/it]Running Inference:  50%|█████     | 101/200 [04:18<04:22,  2.65s/it]Running Inference:  51%|█████     | 102/200 [04:20<04:23,  2.69s/it]Running Inference:  52%|█████▏    | 103/200 [04:23<04:18,  2.67s/it]Running Inference:  52%|█████▏    | 104/200 [04:26<04:18,  2.69s/it]Running Inference:  52%|█████▎    | 105/200 [04:28<04:15,  2.69s/it]Running Inference:  53%|█████▎    | 106/200 [04:31<04:11,  2.67s/it]Running Inference:  54%|█████▎    | 107/200 [04:34<04:07,  2.66s/it]Running Inference:  54%|█████▍    | 108/200 [04:36<04:05,  2.67s/it]Running Inference:  55%|█████▍    | 109/200 [04:39<04:01,  2.65s/it]Running Inference:  55%|█████▌    | 110/200 [04:42<04:00,  2.68s/it]Running Inference:  56%|█████▌    | 111/200 [04:44<03:59,  2.69s/it]Running Inference:  56%|█████▌    | 112/200 [04:47<04:00,  2.73s/it]Running Inference:  56%|█████▋    | 113/200 [04:50<03:55,  2.71s/it]Running Inference:  57%|█████▋    | 114/200 [04:53<03:53,  2.72s/it]Running Inference:  57%|█████▊    | 115/200 [04:55<03:41,  2.61s/it]Running Inference:  58%|█████▊    | 116/200 [04:58<03:38,  2.60s/it]Running Inference:  58%|█████▊    | 117/200 [05:00<03:38,  2.63s/it]Running Inference:  59%|█████▉    | 118/200 [05:02<03:07,  2.29s/it]Running Inference:  60%|█████▉    | 119/200 [05:03<02:51,  2.12s/it]Running Inference:  60%|██████    | 120/200 [05:06<03:00,  2.26s/it]Running Inference:  60%|██████    | 121/200 [05:09<03:07,  2.38s/it]Running Inference:  61%|██████    | 122/200 [05:10<02:41,  2.07s/it]Running Inference:  62%|██████▏   | 123/200 [05:13<02:51,  2.22s/it]Running Inference:  62%|██████▏   | 124/200 [05:15<02:59,  2.37s/it]Running Inference:  62%|██████▎   | 125/200 [05:18<03:03,  2.45s/it]Running Inference:  63%|██████▎   | 126/200 [05:21<03:05,  2.51s/it]Running Inference:  64%|██████▎   | 127/200 [05:23<03:06,  2.56s/it]Running Inference:  64%|██████▍   | 128/200 [05:26<03:05,  2.57s/it]Running Inference:  64%|██████▍   | 129/200 [05:29<03:05,  2.61s/it]Running Inference:  65%|██████▌   | 130/200 [05:31<03:04,  2.63s/it]Running Inference:  66%|██████▌   | 131/200 [05:34<03:04,  2.67s/it]Running Inference:  66%|██████▌   | 132/200 [05:37<03:03,  2.69s/it]Running Inference:  66%|██████▋   | 133/200 [05:39<02:59,  2.69s/it]Running Inference:  67%|██████▋   | 134/200 [05:42<02:56,  2.68s/it]Running Inference:  68%|██████▊   | 135/200 [05:45<02:54,  2.68s/it]Running Inference:  68%|██████▊   | 136/200 [05:47<02:50,  2.67s/it]Running Inference:  68%|██████▊   | 137/200 [05:50<02:47,  2.66s/it]Running Inference:  69%|██████▉   | 138/200 [05:53<02:44,  2.65s/it]Running Inference:  70%|██████▉   | 139/200 [05:54<02:20,  2.31s/it]Running Inference:  70%|███████   | 140/200 [05:57<02:26,  2.44s/it]Running Inference:  70%|███████   | 141/200 [06:00<02:28,  2.51s/it]Running Inference:  71%|███████   | 142/200 [06:02<02:28,  2.56s/it]Running Inference:  72%|███████▏  | 143/200 [06:05<02:26,  2.58s/it]Running Inference:  72%|███████▏  | 144/200 [06:08<02:24,  2.58s/it]Running Inference:  72%|███████▎  | 145/200 [06:10<02:22,  2.60s/it]Running Inference:  73%|███████▎  | 146/200 [06:13<02:18,  2.57s/it]Running Inference:  74%|███████▎  | 147/200 [06:15<02:17,  2.60s/it]Running Inference:  74%|███████▍  | 148/200 [06:18<02:15,  2.61s/it]Running Inference:  74%|███████▍  | 149/200 [06:20<02:03,  2.41s/it]Running Inference:  75%|███████▌  | 150/200 [06:23<02:04,  2.49s/it]Running Inference:  76%|███████▌  | 151/200 [06:25<02:03,  2.51s/it]Running Inference:  76%|███████▌  | 152/200 [06:28<02:03,  2.56s/it]Running Inference:  76%|███████▋  | 153/200 [06:31<02:01,  2.58s/it]Running Inference:  77%|███████▋  | 154/200 [06:33<01:53,  2.46s/it]Running Inference:  78%|███████▊  | 155/200 [06:36<01:55,  2.57s/it]Running Inference:  78%|███████▊  | 156/200 [06:38<01:54,  2.60s/it]Running Inference:  78%|███████▊  | 157/200 [06:41<01:52,  2.62s/it]Running Inference:  79%|███████▉  | 158/200 [06:44<01:50,  2.63s/it]Running Inference:  80%|███████▉  | 159/200 [06:46<01:48,  2.64s/it]Running Inference:  80%|████████  | 160/200 [06:49<01:46,  2.65s/it]Running Inference:  80%|████████  | 161/200 [06:52<01:43,  2.65s/it]Running Inference:  81%|████████  | 162/200 [06:53<01:22,  2.18s/it]Running Inference:  82%|████████▏ | 163/200 [06:55<01:26,  2.33s/it]Running Inference:  82%|████████▏ | 164/200 [06:58<01:27,  2.43s/it]Running Inference:  82%|████████▎ | 165/200 [06:59<01:13,  2.11s/it]Running Inference:  83%|████████▎ | 166/200 [07:02<01:17,  2.29s/it]Running Inference:  84%|████████▎ | 167/200 [07:03<01:02,  1.90s/it]Running Inference:  84%|████████▍ | 168/200 [07:06<01:08,  2.15s/it]Running Inference:  84%|████████▍ | 169/200 [07:08<01:10,  2.29s/it]Running Inference:  85%|████████▌ | 170/200 [07:11<01:12,  2.42s/it]Running Inference:  86%|████████▌ | 171/200 [07:13<01:05,  2.25s/it]Running Inference:  86%|████████▌ | 172/200 [07:15<01:03,  2.26s/it]Running Inference:  86%|████████▋ | 173/200 [07:18<01:03,  2.34s/it]Running Inference:  87%|████████▋ | 174/200 [07:20<01:02,  2.41s/it]Running Inference:  88%|████████▊ | 175/200 [07:23<01:02,  2.48s/it]Running Inference:  88%|████████▊ | 176/200 [07:26<01:01,  2.55s/it]Running Inference:  88%|████████▊ | 177/200 [07:28<00:59,  2.58s/it]Running Inference:  89%|████████▉ | 178/200 [07:31<00:57,  2.63s/it]Running Inference:  90%|████████▉ | 179/200 [07:34<00:55,  2.65s/it]Running Inference:  90%|█████████ | 180/200 [07:37<00:53,  2.69s/it]Running Inference:  90%|█████████ | 181/200 [07:39<00:51,  2.69s/it]Running Inference:  91%|█████████ | 182/200 [07:42<00:48,  2.71s/it]Running Inference:  92%|█████████▏| 183/200 [07:45<00:45,  2.70s/it]Running Inference:  92%|█████████▏| 184/200 [07:47<00:42,  2.69s/it]Running Inference:  92%|█████████▎| 185/200 [07:50<00:40,  2.67s/it]Running Inference:  93%|█████████▎| 186/200 [07:53<00:37,  2.66s/it]Running Inference:  94%|█████████▎| 187/200 [07:55<00:34,  2.68s/it]Running Inference:  94%|█████████▍| 188/200 [07:58<00:31,  2.65s/it]Running Inference:  94%|█████████▍| 189/200 [08:01<00:29,  2.66s/it]Running Inference:  95%|█████████▌| 190/200 [08:03<00:26,  2.67s/it]Running Inference:  96%|█████████▌| 191/200 [08:05<00:21,  2.39s/it]Running Inference:  96%|█████████▌| 192/200 [08:08<00:19,  2.47s/it]Running Inference:  96%|█████████▋| 193/200 [08:09<00:14,  2.07s/it]Running Inference:  97%|█████████▋| 194/200 [08:12<00:13,  2.28s/it]Running Inference:  98%|█████████▊| 195/200 [08:14<00:11,  2.40s/it]Running Inference:  98%|█████████▊| 196/200 [08:17<00:09,  2.46s/it]Running Inference:  98%|█████████▊| 197/200 [08:20<00:07,  2.55s/it]Running Inference:  99%|█████████▉| 198/200 [08:21<00:04,  2.33s/it]Running Inference: 100%|█████████▉| 199/200 [08:24<00:02,  2.44s/it]Running Inference: 100%|██████████| 200/200 [08:27<00:00,  2.50s/it]Running Inference: 100%|██████████| 200/200 [08:27<00:00,  2.54s/it]
2025-12-14 22:59:58,668 - INFO - Inference completed.
2025-12-14 22:59:58,677 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-14 22:59:58,678 - INFO - Calculating metrics for dataset: longbench
2025-12-14 22:59:58,679 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-14 22:59:58,679 - INFO - Metrics:
0.5
2025-12-14 22:59:58,681 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_zh, ratio=0.2) on GPU 2

----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:00:05,153 - INFO - Set deterministic seeds to 42
2025-12-14 23:00:05,154 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:00:05,154 - INFO - Starting evaluation run...
2025-12-14 23:00:05,154 - INFO - Output directory set to: longbenchresult
2025-12-14 23:00:05,154 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-14 23:00:05,154 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 23:00:05,154 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:00:05,154 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 102.08it/s]
Device set to use cuda:0
2025-12-14 23:00:22,269 - INFO - Model pipeline loaded.
2025-12-14 23:00:22,270 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 23:00:28,770 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:00:28,770 - INFO - Dataset processed with 200 entries.
2025-12-14 23:00:28,783 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:01<05:24,  1.63s/it]Running Inference:   1%|          | 2/200 [00:04<07:25,  2.25s/it]Running Inference:   2%|▏         | 3/200 [00:07<08:08,  2.48s/it]Running Inference:   2%|▏         | 4/200 [00:09<08:24,  2.57s/it]Running Inference:   2%|▎         | 5/200 [00:12<08:30,  2.62s/it]Running Inference:   3%|▎         | 6/200 [00:15<08:34,  2.65s/it]Running Inference:   4%|▎         | 7/200 [00:17<08:37,  2.68s/it]Running Inference:   4%|▍         | 8/200 [00:20<08:30,  2.66s/it]Running Inference:   4%|▍         | 9/200 [00:21<07:05,  2.23s/it]Running Inference:   5%|▌         | 10/200 [00:24<07:28,  2.36s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:27<07:49,  2.48s/it]Running Inference:   6%|▌         | 12/200 [00:30<08:05,  2.58s/it]Running Inference:   6%|▋         | 13/200 [00:30<06:16,  2.01s/it]Running Inference:   7%|▋         | 14/200 [00:33<06:50,  2.21s/it]Running Inference:   8%|▊         | 15/200 [00:36<07:10,  2.32s/it]Running Inference:   8%|▊         | 16/200 [00:38<07:26,  2.43s/it]Running Inference:   8%|▊         | 17/200 [00:41<07:34,  2.49s/it]Running Inference:   9%|▉         | 18/200 [00:43<07:41,  2.53s/it]Running Inference:  10%|▉         | 19/200 [00:46<07:50,  2.60s/it]Running Inference:  10%|█         | 20/200 [00:47<06:09,  2.05s/it]Running Inference:  10%|█         | 21/200 [00:50<06:40,  2.24s/it]Running Inference:  11%|█         | 22/200 [00:52<07:07,  2.40s/it]Running Inference:  12%|█▏        | 23/200 [00:55<07:19,  2.49s/it]Running Inference:  12%|█▏        | 24/200 [00:57<06:26,  2.20s/it]Running Inference:  12%|█▎        | 25/200 [00:59<06:53,  2.36s/it]Running Inference:  13%|█▎        | 26/200 [01:02<07:05,  2.45s/it]Running Inference:  14%|█▎        | 27/200 [01:05<07:12,  2.50s/it]Running Inference:  14%|█▍        | 28/200 [01:07<07:20,  2.56s/it]Running Inference:  14%|█▍        | 29/200 [01:10<07:22,  2.59s/it]Running Inference:  15%|█▌        | 30/200 [01:13<07:24,  2.61s/it]Running Inference:  16%|█▌        | 31/200 [01:15<07:25,  2.64s/it]Running Inference:  16%|█▌        | 32/200 [01:18<07:25,  2.65s/it]Running Inference:  16%|█▋        | 33/200 [01:21<07:22,  2.65s/it]Running Inference:  17%|█▋        | 34/200 [01:23<07:16,  2.63s/it]Running Inference:  18%|█▊        | 35/200 [01:25<06:35,  2.40s/it]Running Inference:  18%|█▊        | 36/200 [01:28<06:52,  2.51s/it]Running Inference:  18%|█▊        | 37/200 [01:31<07:03,  2.60s/it]Running Inference:  19%|█▉        | 38/200 [01:33<06:31,  2.42s/it]Running Inference:  20%|█▉        | 39/200 [01:35<06:40,  2.49s/it]Running Inference:  20%|██        | 40/200 [01:38<06:46,  2.54s/it]Running Inference:  20%|██        | 41/200 [01:40<06:01,  2.27s/it]Running Inference:  21%|██        | 42/200 [01:42<06:17,  2.39s/it]Running Inference:  22%|██▏       | 43/200 [01:45<06:29,  2.48s/it]Running Inference:  22%|██▏       | 44/200 [01:48<06:36,  2.54s/it]Running Inference:  22%|██▎       | 45/200 [01:50<06:40,  2.58s/it]Running Inference:  23%|██▎       | 46/200 [01:52<05:38,  2.20s/it]Running Inference:  24%|██▎       | 47/200 [01:54<06:00,  2.36s/it]Running Inference:  24%|██▍       | 48/200 [01:57<06:17,  2.48s/it]Running Inference:  24%|██▍       | 49/200 [02:00<06:24,  2.55s/it]Running Inference:  25%|██▌       | 50/200 [02:03<06:28,  2.59s/it]Running Inference:  26%|██▌       | 51/200 [02:05<06:38,  2.68s/it]Running Inference:  26%|██▌       | 52/200 [02:08<06:36,  2.68s/it]Running Inference:  26%|██▋       | 53/200 [02:11<06:32,  2.67s/it]Running Inference:  27%|██▋       | 54/200 [02:13<06:30,  2.67s/it]Running Inference:  28%|██▊       | 55/200 [02:15<05:26,  2.25s/it]Running Inference:  28%|██▊       | 56/200 [02:17<05:40,  2.37s/it]Running Inference:  28%|██▊       | 57/200 [02:20<05:48,  2.43s/it]Running Inference:  29%|██▉       | 58/200 [02:23<06:00,  2.54s/it]Running Inference:  30%|██▉       | 59/200 [02:26<06:08,  2.61s/it]Running Inference:  30%|███       | 60/200 [02:28<05:53,  2.53s/it]Running Inference:  30%|███       | 61/200 [02:30<05:53,  2.54s/it]Running Inference:  31%|███       | 62/200 [02:32<05:08,  2.24s/it]Running Inference:  32%|███▏      | 63/200 [02:35<05:23,  2.36s/it]Running Inference:  32%|███▏      | 64/200 [02:37<05:34,  2.46s/it]Running Inference:  32%|███▎      | 65/200 [02:40<05:40,  2.52s/it]Running Inference:  33%|███▎      | 66/200 [02:43<05:43,  2.56s/it]Running Inference:  34%|███▎      | 67/200 [02:44<04:54,  2.21s/it]Running Inference:  34%|███▍      | 68/200 [02:47<05:08,  2.34s/it]Running Inference:  34%|███▍      | 69/200 [02:49<05:16,  2.41s/it]Running Inference:  35%|███▌      | 70/200 [02:52<05:28,  2.53s/it]Running Inference:  36%|███▌      | 71/200 [02:55<05:30,  2.56s/it]Running Inference:  36%|███▌      | 72/200 [02:57<05:34,  2.61s/it]Running Inference:  36%|███▋      | 73/200 [02:58<04:33,  2.16s/it]Running Inference:  37%|███▋      | 74/200 [03:01<04:58,  2.37s/it]Running Inference:  38%|███▊      | 75/200 [03:04<05:06,  2.45s/it]Running Inference:  38%|███▊      | 76/200 [03:07<05:10,  2.51s/it]Running Inference:  38%|███▊      | 77/200 [03:09<05:14,  2.56s/it]Running Inference:  39%|███▉      | 78/200 [03:12<05:12,  2.56s/it]Running Inference:  40%|███▉      | 79/200 [03:14<05:11,  2.57s/it]Running Inference:  40%|████      | 80/200 [03:17<05:08,  2.57s/it]Running Inference:  40%|████      | 81/200 [03:20<05:10,  2.61s/it]Running Inference:  41%|████      | 82/200 [03:21<04:24,  2.24s/it]Running Inference:  42%|████▏     | 83/200 [03:24<04:37,  2.37s/it]Running Inference:  42%|████▏     | 84/200 [03:26<04:43,  2.45s/it]Running Inference:  42%|████▎     | 85/200 [03:29<04:47,  2.50s/it]Running Inference:  43%|████▎     | 86/200 [03:32<04:48,  2.53s/it]Running Inference:  44%|████▎     | 87/200 [03:34<04:47,  2.54s/it]Running Inference:  44%|████▍     | 88/200 [03:37<04:46,  2.56s/it]Running Inference:  44%|████▍     | 89/200 [03:40<04:50,  2.61s/it]Running Inference:  45%|████▌     | 90/200 [03:42<04:51,  2.65s/it]Running Inference:  46%|████▌     | 91/200 [03:45<04:48,  2.65s/it]Running Inference:  46%|████▌     | 92/200 [03:48<04:46,  2.65s/it]Running Inference:  46%|████▋     | 93/200 [03:50<04:42,  2.64s/it]Running Inference:  47%|████▋     | 94/200 [03:53<04:37,  2.62s/it]Running Inference:  48%|████▊     | 95/200 [03:55<04:35,  2.63s/it]Running Inference:  48%|████▊     | 96/200 [03:58<04:33,  2.63s/it]Running Inference:  48%|████▊     | 97/200 [04:01<04:31,  2.63s/it]Running Inference:  49%|████▉     | 98/200 [04:03<04:27,  2.62s/it]Running Inference:  50%|████▉     | 99/200 [04:06<04:26,  2.64s/it]Running Inference:  50%|█████     | 100/200 [04:09<04:26,  2.66s/it]Running Inference:  50%|█████     | 101/200 [04:11<04:20,  2.64s/it]Running Inference:  51%|█████     | 102/200 [04:14<04:22,  2.68s/it]Running Inference:  52%|█████▏    | 103/200 [04:15<03:40,  2.28s/it]Running Inference:  52%|█████▏    | 104/200 [04:18<03:51,  2.42s/it]Running Inference:  52%|█████▎    | 105/200 [04:21<03:54,  2.47s/it]Running Inference:  53%|█████▎    | 106/200 [04:23<03:55,  2.51s/it]Running Inference:  54%|█████▎    | 107/200 [04:26<03:56,  2.54s/it]Running Inference:  54%|█████▍    | 108/200 [04:28<03:27,  2.25s/it]Running Inference:  55%|█████▍    | 109/200 [04:30<03:34,  2.36s/it]Running Inference:  55%|█████▌    | 110/200 [04:33<03:41,  2.46s/it]Running Inference:  56%|█████▌    | 111/200 [04:34<03:05,  2.09s/it]Running Inference:  56%|█████▌    | 112/200 [04:37<03:22,  2.30s/it]Running Inference:  56%|█████▋    | 113/200 [04:39<03:28,  2.40s/it]Running Inference:  57%|█████▋    | 114/200 [04:42<03:34,  2.49s/it]Running Inference:  57%|█████▊    | 115/200 [04:45<03:35,  2.54s/it]Running Inference:  58%|█████▊    | 116/200 [04:47<03:34,  2.55s/it]Running Inference:  58%|█████▊    | 117/200 [04:49<03:16,  2.37s/it]Running Inference:  59%|█████▉    | 118/200 [04:51<02:52,  2.10s/it]Running Inference:  60%|█████▉    | 119/200 [04:54<03:05,  2.29s/it]Running Inference:  60%|██████    | 120/200 [04:56<03:09,  2.37s/it]Running Inference:  60%|██████    | 121/200 [04:59<03:13,  2.45s/it]Running Inference:  61%|██████    | 122/200 [05:00<02:45,  2.12s/it]Running Inference:  62%|██████▏   | 123/200 [05:03<02:53,  2.25s/it]Running Inference:  62%|██████▏   | 124/200 [05:05<03:00,  2.38s/it]Running Inference:  62%|██████▎   | 125/200 [05:08<03:03,  2.45s/it]Running Inference:  63%|██████▎   | 126/200 [05:11<03:05,  2.51s/it]Running Inference:  64%|██████▎   | 127/200 [05:13<03:06,  2.55s/it]Running Inference:  64%|██████▍   | 128/200 [05:15<02:37,  2.19s/it]Running Inference:  64%|██████▍   | 129/200 [05:17<02:46,  2.34s/it]Running Inference:  65%|██████▌   | 130/200 [05:20<02:50,  2.44s/it]Running Inference:  66%|██████▌   | 131/200 [05:23<02:54,  2.53s/it]Running Inference:  66%|██████▌   | 132/200 [05:25<02:56,  2.59s/it]Running Inference:  66%|██████▋   | 133/200 [05:28<02:54,  2.61s/it]Running Inference:  67%|██████▋   | 134/200 [05:31<02:52,  2.62s/it]Running Inference:  68%|██████▊   | 135/200 [05:33<02:35,  2.40s/it]Running Inference:  68%|██████▊   | 136/200 [05:34<02:12,  2.07s/it]Running Inference:  68%|██████▊   | 137/200 [05:37<02:20,  2.24s/it]Running Inference:  69%|██████▉   | 138/200 [05:39<02:25,  2.35s/it]Running Inference:  70%|██████▉   | 139/200 [05:40<02:03,  2.02s/it]Running Inference:  70%|███████   | 140/200 [05:43<02:14,  2.24s/it]Running Inference:  70%|███████   | 141/200 [05:44<01:55,  1.95s/it]Running Inference:  71%|███████   | 142/200 [05:47<02:05,  2.16s/it]Running Inference:  72%|███████▏  | 143/200 [05:48<01:46,  1.87s/it]Running Inference:  72%|███████▏  | 144/200 [05:51<01:56,  2.08s/it]Running Inference:  72%|███████▎  | 145/200 [05:53<02:03,  2.25s/it]Running Inference:  73%|███████▎  | 146/200 [05:56<02:05,  2.32s/it]Running Inference:  74%|███████▎  | 147/200 [05:59<02:08,  2.42s/it]Running Inference:  74%|███████▍  | 148/200 [06:01<02:09,  2.48s/it]Running Inference:  74%|███████▍  | 149/200 [06:03<01:58,  2.32s/it]Running Inference:  75%|███████▌  | 150/200 [06:06<02:00,  2.42s/it]Running Inference:  76%|███████▌  | 151/200 [06:08<02:00,  2.46s/it]Running Inference:  76%|███████▌  | 152/200 [06:11<02:00,  2.52s/it]Running Inference:  76%|███████▋  | 153/200 [06:14<01:59,  2.55s/it]Running Inference:  77%|███████▋  | 154/200 [06:16<01:58,  2.57s/it]Running Inference:  78%|███████▊  | 155/200 [06:19<01:58,  2.64s/it]Running Inference:  78%|███████▊  | 156/200 [06:22<01:56,  2.64s/it]Running Inference:  78%|███████▊  | 157/200 [06:24<01:53,  2.65s/it]Running Inference:  79%|███████▉  | 158/200 [06:27<01:51,  2.64s/it]Running Inference:  80%|███████▉  | 159/200 [06:30<01:48,  2.65s/it]Running Inference:  80%|████████  | 160/200 [06:32<01:46,  2.65s/it]Running Inference:  80%|████████  | 161/200 [06:35<01:43,  2.65s/it]Running Inference:  81%|████████  | 162/200 [06:38<01:39,  2.62s/it]Running Inference:  82%|████████▏ | 163/200 [06:40<01:37,  2.64s/it]Running Inference:  82%|████████▏ | 164/200 [06:43<01:35,  2.64s/it]Running Inference:  82%|████████▎ | 165/200 [06:44<01:19,  2.26s/it]Running Inference:  83%|████████▎ | 166/200 [06:47<01:21,  2.39s/it]Running Inference:  84%|████████▎ | 167/200 [06:50<01:21,  2.46s/it]Running Inference:  84%|████████▍ | 168/200 [06:52<01:21,  2.54s/it]Running Inference:  84%|████████▍ | 169/200 [06:55<01:19,  2.55s/it]Running Inference:  85%|████████▌ | 170/200 [06:58<01:17,  2.60s/it]Running Inference:  86%|████████▌ | 171/200 [06:59<01:02,  2.16s/it]Running Inference:  86%|████████▌ | 172/200 [07:01<01:04,  2.32s/it]Running Inference:  86%|████████▋ | 173/200 [07:04<01:04,  2.38s/it]Running Inference:  87%|████████▋ | 174/200 [07:05<00:48,  1.85s/it]Running Inference:  88%|████████▊ | 175/200 [07:07<00:52,  2.08s/it]Running Inference:  88%|████████▊ | 176/200 [07:08<00:39,  1.66s/it]Running Inference:  88%|████████▊ | 177/200 [07:10<00:44,  1.94s/it]Running Inference:  89%|████████▉ | 178/200 [07:13<00:47,  2.18s/it]Running Inference:  90%|████████▉ | 179/200 [07:16<00:48,  2.32s/it]Running Inference:  90%|█████████ | 180/200 [07:19<00:49,  2.46s/it]Running Inference:  90%|█████████ | 181/200 [07:21<00:47,  2.52s/it]Running Inference:  91%|█████████ | 182/200 [07:23<00:40,  2.24s/it]Running Inference:  92%|█████████▏| 183/200 [07:26<00:40,  2.36s/it]Running Inference:  92%|█████████▏| 184/200 [07:27<00:32,  2.00s/it]Running Inference:  92%|█████████▎| 185/200 [07:29<00:32,  2.19s/it]Running Inference:  93%|█████████▎| 186/200 [07:32<00:32,  2.32s/it]Running Inference:  94%|█████████▎| 187/200 [07:35<00:31,  2.43s/it]Running Inference:  94%|█████████▍| 188/200 [07:37<00:29,  2.48s/it]Running Inference:  94%|█████████▍| 189/200 [07:40<00:27,  2.53s/it]Running Inference:  95%|█████████▌| 190/200 [07:43<00:25,  2.58s/it]Running Inference:  96%|█████████▌| 191/200 [07:45<00:23,  2.61s/it]Running Inference:  96%|█████████▌| 192/200 [07:48<00:20,  2.62s/it]Running Inference:  96%|█████████▋| 193/200 [07:49<00:15,  2.17s/it]Running Inference:  97%|█████████▋| 194/200 [07:52<00:14,  2.34s/it]Running Inference:  98%|█████████▊| 195/200 [07:53<00:10,  2.08s/it]Running Inference:  98%|█████████▊| 196/200 [07:56<00:08,  2.23s/it]Running Inference:  98%|█████████▊| 197/200 [07:59<00:07,  2.38s/it]Running Inference:  99%|█████████▉| 198/200 [08:00<00:04,  2.15s/it]Running Inference: 100%|█████████▉| 199/200 [08:03<00:02,  2.31s/it]Running Inference: 100%|██████████| 200/200 [08:05<00:00,  2.41s/it]Running Inference: 100%|██████████| 200/200 [08:05<00:00,  2.43s/it]
2025-12-14 23:08:34,729 - INFO - Inference completed.
2025-12-14 23:08:34,738 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-14 23:08:34,738 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:08:34,740 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-14 23:08:34,740 - INFO - Metrics:
1.0
2025-12-14 23:08:34,741 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_zh, ratio=0.3) on GPU 2

----------------------------------------
Task: passage_retrieval_zh | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:08:41,182 - INFO - Set deterministic seeds to 42
2025-12-14 23:08:41,182 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "passage_retrieval_zh",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:08:41,182 - INFO - Starting evaluation run...
2025-12-14 23:08:41,182 - INFO - Output directory set to: longbenchresult
2025-12-14 23:08:41,182 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-14 23:08:41,182 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 23:08:41,182 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:08:41,182 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.21it/s]
Device set to use cuda:0
2025-12-14 23:08:57,843 - INFO - Model pipeline loaded.
2025-12-14 23:08:57,843 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: passage_retrieval_zh)
2025-12-14 23:09:03,952 - INFO - Dataset loaded with 200 entries.
2025-12-14 23:09:03,953 - INFO - Dataset processed with 200 entries.
2025-12-14 23:09:03,970 - INFO - Starting inference...
Running Inference:   0%|          | 0/200 [00:00<?, ?it/s]Running Inference:   0%|          | 1/200 [00:03<11:31,  3.48s/it]Running Inference:   1%|          | 2/200 [00:06<09:53,  3.00s/it]Running Inference:   2%|▏         | 3/200 [00:08<08:15,  2.52s/it]Running Inference:   2%|▏         | 4/200 [00:10<08:27,  2.59s/it]Running Inference:   2%|▎         | 5/200 [00:13<08:30,  2.62s/it]Running Inference:   3%|▎         | 6/200 [00:16<08:33,  2.65s/it]Running Inference:   4%|▎         | 7/200 [00:18<08:35,  2.67s/it]Running Inference:   4%|▍         | 8/200 [00:19<06:35,  2.06s/it]Running Inference:   4%|▍         | 9/200 [00:20<05:40,  1.78s/it]Running Inference:   5%|▌         | 10/200 [00:23<06:28,  2.05s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   6%|▌         | 11/200 [00:26<07:06,  2.26s/it]Running Inference:   6%|▌         | 12/200 [00:28<07:33,  2.41s/it]Running Inference:   6%|▋         | 13/200 [00:31<07:48,  2.51s/it]Running Inference:   7%|▋         | 14/200 [00:34<07:54,  2.55s/it]Running Inference:   8%|▊         | 15/200 [00:36<07:54,  2.56s/it]Running Inference:   8%|▊         | 16/200 [00:39<07:56,  2.59s/it]Running Inference:   8%|▊         | 17/200 [00:42<07:54,  2.59s/it]Running Inference:   9%|▉         | 18/200 [00:44<07:54,  2.61s/it]Running Inference:  10%|▉         | 19/200 [00:47<07:59,  2.65s/it]Running Inference:  10%|█         | 20/200 [00:49<07:00,  2.34s/it]Running Inference:  10%|█         | 21/200 [00:50<06:00,  2.02s/it]Running Inference:  11%|█         | 22/200 [00:51<04:59,  1.68s/it]Running Inference:  12%|█▏        | 23/200 [00:53<05:23,  1.83s/it]Running Inference:  12%|█▏        | 24/200 [00:54<04:55,  1.68s/it]Running Inference:  12%|█▎        | 25/200 [00:57<05:48,  1.99s/it]Running Inference:  13%|█▎        | 26/200 [01:00<06:19,  2.18s/it]Running Inference:  14%|█▎        | 27/200 [01:02<06:39,  2.31s/it]Running Inference:  14%|█▍        | 28/200 [01:05<06:57,  2.43s/it]Running Inference:  14%|█▍        | 29/200 [01:06<05:55,  2.08s/it]Running Inference:  15%|█▌        | 30/200 [01:09<06:23,  2.25s/it]Running Inference:  16%|█▌        | 31/200 [01:12<06:42,  2.38s/it]Running Inference:  16%|█▌        | 32/200 [01:14<06:55,  2.47s/it]Running Inference:  16%|█▋        | 33/200 [01:17<07:01,  2.52s/it]Running Inference:  17%|█▋        | 34/200 [01:19<07:01,  2.54s/it]Running Inference:  18%|█▊        | 35/200 [01:21<06:30,  2.37s/it]Running Inference:  18%|█▊        | 36/200 [01:24<06:48,  2.49s/it]Running Inference:  18%|█▊        | 37/200 [01:27<06:58,  2.57s/it]Running Inference:  19%|█▉        | 38/200 [01:29<06:16,  2.32s/it]Running Inference:  20%|█▉        | 39/200 [01:31<06:29,  2.42s/it]Running Inference:  20%|██        | 40/200 [01:34<06:21,  2.39s/it]Running Inference:  20%|██        | 41/200 [01:35<05:43,  2.16s/it]Running Inference:  21%|██        | 42/200 [01:38<06:04,  2.31s/it]Running Inference:  22%|██▏       | 43/200 [01:41<06:19,  2.42s/it]Running Inference:  22%|██▏       | 44/200 [01:43<06:29,  2.50s/it]Running Inference:  22%|██▎       | 45/200 [01:46<06:31,  2.53s/it]Running Inference:  23%|██▎       | 46/200 [01:47<05:31,  2.15s/it]Running Inference:  24%|██▎       | 47/200 [01:50<05:54,  2.32s/it]Running Inference:  24%|██▍       | 48/200 [01:53<06:12,  2.45s/it]Running Inference:  24%|██▍       | 49/200 [01:55<06:20,  2.52s/it]Running Inference:  25%|██▌       | 50/200 [01:58<06:23,  2.56s/it]Running Inference:  26%|██▌       | 51/200 [02:01<06:26,  2.60s/it]Running Inference:  26%|██▌       | 52/200 [02:03<06:28,  2.63s/it]Running Inference:  26%|██▋       | 53/200 [02:06<06:27,  2.64s/it]Running Inference:  27%|██▋       | 54/200 [02:09<06:28,  2.66s/it]Running Inference:  28%|██▊       | 55/200 [02:11<06:28,  2.68s/it]Running Inference:  28%|██▊       | 56/200 [02:14<06:24,  2.67s/it]Running Inference:  28%|██▊       | 57/200 [02:17<06:19,  2.65s/it]Running Inference:  29%|██▉       | 58/200 [02:20<06:27,  2.73s/it]Running Inference:  30%|██▉       | 59/200 [02:22<06:27,  2.75s/it]Running Inference:  30%|███       | 60/200 [02:25<06:19,  2.71s/it]Running Inference:  30%|███       | 61/200 [02:27<05:52,  2.53s/it]Running Inference:  31%|███       | 62/200 [02:29<05:08,  2.24s/it]Running Inference:  32%|███▏      | 63/200 [02:31<05:23,  2.36s/it]Running Inference:  32%|███▏      | 64/200 [02:34<05:34,  2.46s/it]Running Inference:  32%|███▎      | 65/200 [02:37<05:40,  2.53s/it]Running Inference:  33%|███▎      | 66/200 [02:37<04:25,  1.98s/it]Running Inference:  34%|███▎      | 67/200 [02:40<04:56,  2.23s/it]Running Inference:  34%|███▍      | 68/200 [02:43<05:10,  2.35s/it]Running Inference:  34%|███▍      | 69/200 [02:46<05:18,  2.43s/it]Running Inference:  35%|███▌      | 70/200 [02:48<05:30,  2.54s/it]Running Inference:  36%|███▌      | 71/200 [02:51<05:33,  2.58s/it]Running Inference:  36%|███▌      | 72/200 [02:54<05:37,  2.63s/it]Running Inference:  36%|███▋      | 73/200 [02:55<04:35,  2.17s/it]Running Inference:  37%|███▋      | 74/200 [02:58<04:59,  2.38s/it]Running Inference:  38%|███▊      | 75/200 [03:00<05:08,  2.46s/it]Running Inference:  38%|███▊      | 76/200 [03:03<05:12,  2.52s/it]Running Inference:  38%|███▊      | 77/200 [03:06<05:15,  2.56s/it]Running Inference:  39%|███▉      | 78/200 [03:08<05:13,  2.57s/it]Running Inference:  40%|███▉      | 79/200 [03:11<05:12,  2.58s/it]Running Inference:  40%|████      | 80/200 [03:13<04:57,  2.48s/it]Running Inference:  40%|████      | 81/200 [03:16<05:03,  2.55s/it]Running Inference:  41%|████      | 82/200 [03:17<04:14,  2.15s/it]Running Inference:  42%|████▏     | 83/200 [03:20<04:31,  2.32s/it]Running Inference:  42%|████▏     | 84/200 [03:22<04:39,  2.41s/it]Running Inference:  42%|████▎     | 85/200 [03:25<04:45,  2.48s/it]Running Inference:  43%|████▎     | 86/200 [03:28<04:46,  2.52s/it]Running Inference:  44%|████▎     | 87/200 [03:29<04:05,  2.17s/it]Running Inference:  44%|████▍     | 88/200 [03:32<04:18,  2.31s/it]Running Inference:  44%|████▍     | 89/200 [03:34<04:30,  2.44s/it]Running Inference:  45%|████▌     | 90/200 [03:37<04:37,  2.53s/it]Running Inference:  46%|████▌     | 91/200 [03:40<04:39,  2.57s/it]Running Inference:  46%|████▌     | 92/200 [03:42<04:39,  2.58s/it]Running Inference:  46%|████▋     | 93/200 [03:45<04:28,  2.51s/it]Running Inference:  47%|████▋     | 94/200 [03:47<04:28,  2.53s/it]Running Inference:  48%|████▊     | 95/200 [03:50<04:29,  2.57s/it]Running Inference:  48%|████▊     | 96/200 [03:53<04:29,  2.59s/it]Running Inference:  48%|████▊     | 97/200 [03:55<04:29,  2.61s/it]Running Inference:  49%|████▉     | 98/200 [03:58<04:27,  2.62s/it]Running Inference:  50%|████▉     | 99/200 [04:00<04:03,  2.41s/it]Running Inference:  50%|█████     | 100/200 [04:03<04:10,  2.51s/it]Running Inference:  50%|█████     | 101/200 [04:05<04:10,  2.53s/it]Running Inference:  51%|█████     | 102/200 [04:08<04:15,  2.61s/it]Running Inference:  52%|█████▏    | 103/200 [04:09<03:37,  2.24s/it]Running Inference:  52%|█████▏    | 104/200 [04:12<03:49,  2.39s/it]Running Inference:  52%|█████▎    | 105/200 [04:15<03:55,  2.47s/it]Running Inference:  53%|█████▎    | 106/200 [04:17<03:56,  2.52s/it]Running Inference:  54%|█████▎    | 107/200 [04:18<03:06,  2.00s/it]Running Inference:  54%|█████▍    | 108/200 [04:21<03:22,  2.20s/it]Running Inference:  55%|█████▍    | 109/200 [04:23<03:31,  2.33s/it]Running Inference:  55%|█████▌    | 110/200 [04:26<03:40,  2.45s/it]Running Inference:  56%|█████▌    | 111/200 [04:28<03:12,  2.17s/it]Running Inference:  56%|█████▌    | 112/200 [04:30<03:27,  2.36s/it]Running Inference:  56%|█████▋    | 113/200 [04:33<03:32,  2.44s/it]Running Inference:  57%|█████▋    | 114/200 [04:36<03:37,  2.53s/it]Running Inference:  57%|█████▊    | 115/200 [04:39<03:37,  2.56s/it]Running Inference:  58%|█████▊    | 116/200 [04:41<03:35,  2.57s/it]Running Inference:  58%|█████▊    | 117/200 [04:42<02:56,  2.12s/it]Running Inference:  59%|█████▉    | 118/200 [04:44<02:39,  1.94s/it]Running Inference:  60%|█████▉    | 119/200 [04:46<02:56,  2.18s/it]Running Inference:  60%|██████    | 120/200 [04:49<03:03,  2.30s/it]Running Inference:  60%|██████    | 121/200 [04:52<03:09,  2.40s/it]Running Inference:  61%|██████    | 122/200 [04:53<02:42,  2.09s/it]Running Inference:  62%|██████▏   | 123/200 [04:56<02:52,  2.24s/it]Running Inference:  62%|██████▏   | 124/200 [04:58<03:00,  2.37s/it]Running Inference:  62%|██████▎   | 125/200 [05:01<03:03,  2.45s/it]Running Inference:  63%|██████▎   | 126/200 [05:04<03:05,  2.51s/it]Running Inference:  64%|██████▎   | 127/200 [05:06<03:06,  2.56s/it]Running Inference:  64%|██████▍   | 128/200 [05:08<02:39,  2.21s/it]Running Inference:  64%|██████▍   | 129/200 [05:09<02:16,  1.93s/it]Running Inference:  65%|██████▌   | 130/200 [05:10<02:04,  1.77s/it]Running Inference:  66%|██████▌   | 131/200 [05:13<02:22,  2.07s/it]Running Inference:  66%|██████▌   | 132/200 [05:16<02:34,  2.27s/it]Running Inference:  66%|██████▋   | 133/200 [05:18<02:40,  2.39s/it]Running Inference:  67%|██████▋   | 134/200 [05:21<02:42,  2.47s/it]Running Inference:  68%|██████▊   | 135/200 [05:23<02:25,  2.23s/it]Running Inference:  68%|██████▊   | 136/200 [05:24<02:02,  1.92s/it]Running Inference:  68%|██████▊   | 137/200 [05:27<02:14,  2.14s/it]Running Inference:  69%|██████▉   | 138/200 [05:29<02:21,  2.29s/it]Running Inference:  70%|██████▉   | 139/200 [05:31<02:00,  1.98s/it]Running Inference:  70%|███████   | 140/200 [05:33<02:12,  2.21s/it]Running Inference:  70%|███████   | 141/200 [05:35<02:00,  2.05s/it]Running Inference:  71%|███████   | 142/200 [05:38<02:09,  2.23s/it]Running Inference:  72%|███████▏  | 143/200 [05:39<01:49,  1.92s/it]Running Inference:  72%|███████▏  | 144/200 [05:41<01:58,  2.12s/it]Running Inference:  72%|███████▎  | 145/200 [05:44<02:04,  2.27s/it]Running Inference:  73%|███████▎  | 146/200 [05:46<02:06,  2.34s/it]Running Inference:  74%|███████▎  | 147/200 [05:48<01:47,  2.04s/it]Running Inference:  74%|███████▍  | 148/200 [05:50<01:52,  2.16s/it]Running Inference:  74%|███████▍  | 149/200 [05:52<01:47,  2.10s/it]Running Inference:  75%|███████▌  | 150/200 [05:55<01:53,  2.26s/it]Running Inference:  76%|███████▌  | 151/200 [05:57<01:55,  2.35s/it]Running Inference:  76%|███████▌  | 152/200 [06:00<01:57,  2.45s/it]Running Inference:  76%|███████▋  | 153/200 [06:03<01:57,  2.50s/it]Running Inference:  77%|███████▋  | 154/200 [06:05<01:57,  2.55s/it]Running Inference:  78%|███████▊  | 155/200 [06:08<01:58,  2.63s/it]Running Inference:  78%|███████▊  | 156/200 [06:11<01:55,  2.64s/it]Running Inference:  78%|███████▊  | 157/200 [06:14<01:53,  2.65s/it]Running Inference:  79%|███████▉  | 158/200 [06:16<01:51,  2.64s/it]Running Inference:  80%|███████▉  | 159/200 [06:19<01:48,  2.65s/it]Running Inference:  80%|████████  | 160/200 [06:22<01:46,  2.66s/it]Running Inference:  80%|████████  | 161/200 [06:24<01:43,  2.65s/it]Running Inference:  81%|████████  | 162/200 [06:27<01:40,  2.63s/it]Running Inference:  82%|████████▏ | 163/200 [06:29<01:38,  2.65s/it]Running Inference:  82%|████████▏ | 164/200 [06:32<01:35,  2.66s/it]Running Inference:  82%|████████▎ | 165/200 [06:33<01:19,  2.27s/it]Running Inference:  83%|████████▎ | 166/200 [06:36<01:19,  2.35s/it]Running Inference:  84%|████████▎ | 167/200 [06:39<01:20,  2.44s/it]Running Inference:  84%|████████▍ | 168/200 [06:41<01:20,  2.53s/it]Running Inference:  84%|████████▍ | 169/200 [06:43<01:13,  2.36s/it]Running Inference:  85%|████████▌ | 170/200 [06:46<01:13,  2.46s/it]Running Inference:  86%|████████▌ | 171/200 [06:49<01:12,  2.49s/it]Running Inference:  86%|████████▌ | 172/200 [06:51<01:10,  2.53s/it]Running Inference:  86%|████████▋ | 173/200 [06:54<01:08,  2.52s/it]Running Inference:  87%|████████▋ | 174/200 [06:55<00:57,  2.23s/it]Running Inference:  88%|████████▊ | 175/200 [06:58<00:58,  2.35s/it]Running Inference:  88%|████████▊ | 176/200 [07:01<00:58,  2.45s/it]Running Inference:  88%|████████▊ | 177/200 [07:03<00:57,  2.52s/it]Running Inference:  89%|████████▉ | 178/200 [07:06<00:56,  2.59s/it]Running Inference:  90%|████████▉ | 179/200 [07:09<00:54,  2.61s/it]Running Inference:  90%|█████████ | 180/200 [07:11<00:53,  2.66s/it]Running Inference:  90%|█████████ | 181/200 [07:14<00:50,  2.66s/it]Running Inference:  91%|█████████ | 182/200 [07:16<00:41,  2.31s/it]Running Inference:  92%|█████████▏| 183/200 [07:18<00:41,  2.42s/it]Running Inference:  92%|█████████▏| 184/200 [07:19<00:32,  2.04s/it]Running Inference:  92%|█████████▎| 185/200 [07:22<00:30,  2.04s/it]Running Inference:  93%|█████████▎| 186/200 [07:24<00:31,  2.22s/it]Running Inference:  94%|█████████▎| 187/200 [07:27<00:30,  2.38s/it]Running Inference:  94%|█████████▍| 188/200 [07:29<00:29,  2.44s/it]Running Inference:  94%|█████████▍| 189/200 [07:32<00:27,  2.51s/it]Running Inference:  95%|█████████▌| 190/200 [07:35<00:25,  2.57s/it]Running Inference:  96%|█████████▌| 191/200 [07:38<00:23,  2.60s/it]Running Inference:  96%|█████████▌| 192/200 [07:40<00:20,  2.62s/it]Running Inference:  96%|█████████▋| 193/200 [07:42<00:15,  2.25s/it]Running Inference:  97%|█████████▋| 194/200 [07:44<00:14,  2.40s/it]Running Inference:  98%|█████████▊| 195/200 [07:47<00:12,  2.48s/it]Running Inference:  98%|█████████▊| 196/200 [07:50<00:10,  2.51s/it]Running Inference:  98%|█████████▊| 197/200 [07:52<00:07,  2.58s/it]Running Inference:  99%|█████████▉| 198/200 [07:55<00:05,  2.58s/it]Running Inference: 100%|█████████▉| 199/200 [07:56<00:02,  2.18s/it]Running Inference: 100%|██████████| 200/200 [07:59<00:00,  2.32s/it]Running Inference: 100%|██████████| 200/200 [07:59<00:00,  2.40s/it]
2025-12-14 23:17:03,303 - INFO - Inference completed.
2025-12-14 23:17:03,312 - INFO - Results saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-14 23:17:03,312 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:17:03,313 - INFO - Metrics saved to longbenchresult/longbench__passage_retrieval_zh__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-14 23:17:03,313 - INFO - Metrics:
0.66
2025-12-14 23:17:03,315 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=passage_retrieval_zh, ratio=0.5) on GPU 2


========================================
LongBench Task: repobench-p
========================================
----------------------------------------
Task: repobench-p | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:17:09,822 - INFO - Set deterministic seeds to 42
2025-12-14 23:17:09,822 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:17:09,822 - INFO - Starting evaluation run...
2025-12-14 23:17:09,822 - INFO - Output directory set to: longbenchresult
2025-12-14 23:17:09,822 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-14 23:17:09,822 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 23:17:09,822 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:17:09,822 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.17it/s]
Device set to use cuda:0
2025-12-14 23:17:21,274 - INFO - Model pipeline loaded.
2025-12-14 23:17:21,274 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
2025-12-14 23:17:26,349 - INFO - Dataset loaded with 500 entries.
2025-12-14 23:17:26,349 - INFO - Dataset processed with 500 entries.
2025-12-14 23:17:26,393 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<42:25,  5.10s/it]Running Inference:   0%|          | 2/500 [00:11<47:45,  5.75s/it]Running Inference:   1%|          | 3/500 [00:17<49:25,  5.97s/it]Running Inference:   1%|          | 4/500 [00:21<43:11,  5.22s/it]Running Inference:   1%|          | 5/500 [00:26<41:08,  4.99s/it]Running Inference:   1%|          | 6/500 [00:30<38:43,  4.70s/it]Running Inference:   1%|▏         | 7/500 [00:34<37:06,  4.52s/it]Running Inference:   2%|▏         | 8/500 [00:38<35:44,  4.36s/it]Running Inference:   2%|▏         | 9/500 [00:42<34:51,  4.26s/it]Running Inference:   2%|▏         | 10/500 [00:50<42:57,  5.26s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:56<45:13,  5.55s/it]Running Inference:   2%|▏         | 12/500 [01:00<42:07,  5.18s/it]Running Inference:   3%|▎         | 13/500 [01:05<42:12,  5.20s/it]Running Inference:   3%|▎         | 14/500 [01:15<53:50,  6.65s/it]Running Inference:   3%|▎         | 15/500 [01:20<48:59,  6.06s/it]Running Inference:   3%|▎         | 16/500 [01:24<44:28,  5.51s/it]Running Inference:   3%|▎         | 17/500 [01:30<44:16,  5.50s/it]Running Inference:   4%|▎         | 18/500 [01:35<44:45,  5.57s/it]Running Inference:   4%|▍         | 19/500 [01:40<41:19,  5.15s/it]Running Inference:   4%|▍         | 20/500 [01:47<45:38,  5.71s/it]Running Inference:   4%|▍         | 21/500 [01:51<41:26,  5.19s/it]Running Inference:   4%|▍         | 22/500 [01:56<42:21,  5.32s/it]Running Inference:   5%|▍         | 23/500 [01:59<36:15,  4.56s/it]Running Inference:   5%|▍         | 24/500 [02:05<39:31,  4.98s/it]Running Inference:   5%|▌         | 25/500 [02:11<41:35,  5.25s/it]Running Inference:   5%|▌         | 26/500 [02:16<40:06,  5.08s/it]Running Inference:   5%|▌         | 27/500 [02:20<37:30,  4.76s/it]Running Inference:   6%|▌         | 28/500 [02:23<35:13,  4.48s/it]Running Inference:   6%|▌         | 29/500 [02:27<33:57,  4.33s/it]Running Inference:   6%|▌         | 30/500 [02:33<35:52,  4.58s/it]Running Inference:   6%|▌         | 31/500 [02:38<37:09,  4.75s/it]Running Inference:   6%|▋         | 32/500 [02:42<35:52,  4.60s/it]Running Inference:   7%|▋         | 33/500 [02:46<34:26,  4.42s/it]Running Inference:   7%|▋         | 34/500 [02:50<34:05,  4.39s/it]Running Inference:   7%|▋         | 35/500 [02:55<34:06,  4.40s/it]Running Inference:   7%|▋         | 36/500 [02:59<34:57,  4.52s/it]Running Inference:   7%|▋         | 37/500 [03:04<35:15,  4.57s/it]Running Inference:   8%|▊         | 38/500 [03:09<34:57,  4.54s/it]Running Inference:   8%|▊         | 39/500 [03:13<33:54,  4.41s/it]Running Inference:   8%|▊         | 40/500 [03:17<33:09,  4.33s/it]Running Inference:   8%|▊         | 41/500 [03:18<25:42,  3.36s/it]Running Inference:   8%|▊         | 42/500 [03:22<26:39,  3.49s/it]Running Inference:   9%|▊         | 43/500 [03:23<20:26,  2.68s/it]Running Inference:   9%|▉         | 44/500 [03:29<27:58,  3.68s/it]Running Inference:   9%|▉         | 45/500 [03:34<31:21,  4.14s/it]Running Inference:   9%|▉         | 46/500 [03:39<33:48,  4.47s/it]Running Inference:   9%|▉         | 47/500 [03:44<35:30,  4.70s/it]Running Inference:  10%|▉         | 48/500 [03:49<34:29,  4.58s/it]Running Inference:  10%|▉         | 49/500 [03:53<34:13,  4.55s/it]Running Inference:  10%|█         | 50/500 [03:59<36:52,  4.92s/it]Running Inference:  10%|█         | 51/500 [04:04<36:51,  4.93s/it]Running Inference:  10%|█         | 52/500 [04:08<35:48,  4.80s/it]Running Inference:  11%|█         | 53/500 [04:12<34:03,  4.57s/it]Running Inference:  11%|█         | 54/500 [04:16<32:57,  4.43s/it]Running Inference:  11%|█         | 55/500 [04:23<37:01,  4.99s/it]Running Inference:  11%|█         | 56/500 [04:27<35:52,  4.85s/it]Running Inference:  11%|█▏        | 57/500 [04:31<33:27,  4.53s/it]Running Inference:  12%|█▏        | 58/500 [04:35<32:11,  4.37s/it]Running Inference:  12%|█▏        | 59/500 [04:40<32:43,  4.45s/it]Running Inference:  12%|█▏        | 60/500 [04:44<31:45,  4.33s/it]Running Inference:  12%|█▏        | 61/500 [04:48<30:44,  4.20s/it]Running Inference:  12%|█▏        | 62/500 [04:53<34:08,  4.68s/it]Running Inference:  13%|█▎        | 63/500 [04:59<36:27,  5.01s/it]Running Inference:  13%|█▎        | 64/500 [05:04<35:01,  4.82s/it]Running Inference:  13%|█▎        | 65/500 [05:10<37:38,  5.19s/it]Running Inference:  13%|█▎        | 66/500 [05:13<34:30,  4.77s/it]Running Inference:  13%|█▎        | 67/500 [05:17<32:21,  4.48s/it]Running Inference:  14%|█▎        | 68/500 [05:23<34:11,  4.75s/it]Running Inference:  14%|█▍        | 69/500 [05:28<35:11,  4.90s/it]Running Inference:  14%|█▍        | 70/500 [05:36<42:41,  5.96s/it]Running Inference:  14%|█▍        | 71/500 [05:41<40:17,  5.64s/it]Running Inference:  14%|█▍        | 72/500 [05:46<38:20,  5.38s/it]Running Inference:  15%|█▍        | 73/500 [05:51<38:15,  5.38s/it]Running Inference:  15%|█▍        | 74/500 [05:56<37:23,  5.27s/it]Running Inference:  15%|█▌        | 75/500 [06:01<36:21,  5.13s/it]Running Inference:  15%|█▌        | 76/500 [06:06<36:08,  5.11s/it]Running Inference:  15%|█▌        | 77/500 [06:11<34:35,  4.91s/it]Running Inference:  16%|█▌        | 78/500 [06:17<37:52,  5.38s/it]Running Inference:  16%|█▌        | 79/500 [06:21<34:40,  4.94s/it]Running Inference:  16%|█▌        | 80/500 [06:25<33:19,  4.76s/it]Running Inference:  16%|█▌        | 81/500 [06:31<34:36,  4.96s/it]Running Inference:  16%|█▋        | 82/500 [06:33<29:14,  4.20s/it]Running Inference:  17%|█▋        | 83/500 [06:41<37:04,  5.33s/it]Running Inference:  17%|█▋        | 84/500 [06:46<35:52,  5.17s/it]Running Inference:  17%|█▋        | 85/500 [06:51<35:08,  5.08s/it]Running Inference:  17%|█▋        | 86/500 [06:56<34:26,  4.99s/it]Running Inference:  17%|█▋        | 87/500 [06:57<26:43,  3.88s/it]Running Inference:  18%|█▊        | 88/500 [07:01<27:09,  3.96s/it]Running Inference:  18%|█▊        | 89/500 [07:05<27:29,  4.01s/it]Running Inference:  18%|█▊        | 90/500 [07:09<27:42,  4.05s/it]Running Inference:  18%|█▊        | 91/500 [07:11<22:07,  3.25s/it]Running Inference:  18%|█▊        | 92/500 [07:15<24:58,  3.67s/it]Running Inference:  19%|█▊        | 93/500 [07:20<27:39,  4.08s/it]Running Inference:  19%|█▉        | 94/500 [07:24<27:15,  4.03s/it]Running Inference:  19%|█▉        | 95/500 [07:32<34:13,  5.07s/it]Running Inference:  19%|█▉        | 96/500 [07:35<30:31,  4.53s/it]Running Inference:  19%|█▉        | 97/500 [07:40<30:44,  4.58s/it]Running Inference:  20%|█▉        | 98/500 [07:42<25:00,  3.73s/it]Running Inference:  20%|█▉        | 99/500 [07:43<21:09,  3.17s/it]Running Inference:  20%|██        | 100/500 [07:45<18:23,  2.76s/it]Running Inference:  20%|██        | 101/500 [07:49<20:48,  3.13s/it]Running Inference:  20%|██        | 102/500 [07:54<23:07,  3.49s/it]Running Inference:  21%|██        | 103/500 [07:58<24:12,  3.66s/it]Running Inference:  21%|██        | 104/500 [08:03<26:57,  4.08s/it]Running Inference:  21%|██        | 105/500 [08:04<21:21,  3.24s/it]Running Inference:  21%|██        | 106/500 [08:06<18:11,  2.77s/it]Running Inference:  21%|██▏       | 107/500 [08:09<20:22,  3.11s/it]Running Inference:  22%|██▏       | 108/500 [08:14<23:04,  3.53s/it]Running Inference:  22%|██▏       | 109/500 [08:20<27:52,  4.28s/it]Running Inference:  22%|██▏       | 110/500 [08:26<31:15,  4.81s/it]Running Inference:  22%|██▏       | 111/500 [08:31<31:14,  4.82s/it]Running Inference:  22%|██▏       | 112/500 [08:35<30:12,  4.67s/it]Running Inference:  23%|██▎       | 113/500 [08:39<28:55,  4.49s/it]Running Inference:  23%|██▎       | 114/500 [08:44<28:28,  4.42s/it]Running Inference:  23%|██▎       | 115/500 [08:48<28:37,  4.46s/it]Running Inference:  23%|██▎       | 116/500 [08:52<27:59,  4.37s/it]Running Inference:  23%|██▎       | 117/500 [08:57<28:25,  4.45s/it]Running Inference:  24%|██▎       | 118/500 [09:02<29:14,  4.59s/it]Running Inference:  24%|██▍       | 119/500 [09:06<28:49,  4.54s/it]Running Inference:  24%|██▍       | 120/500 [09:10<27:46,  4.39s/it]Running Inference:  24%|██▍       | 121/500 [09:13<24:25,  3.87s/it]Running Inference:  24%|██▍       | 122/500 [09:17<24:10,  3.84s/it]Running Inference:  25%|██▍       | 123/500 [09:21<24:20,  3.87s/it]Running Inference:  25%|██▍       | 124/500 [09:25<24:28,  3.90s/it]Running Inference:  25%|██▌       | 125/500 [09:29<25:26,  4.07s/it]Running Inference:  25%|██▌       | 126/500 [09:31<20:54,  3.35s/it]Running Inference:  25%|██▌       | 127/500 [09:35<21:51,  3.52s/it]Running Inference:  26%|██▌       | 128/500 [09:39<22:26,  3.62s/it]Running Inference:  26%|██▌       | 129/500 [09:46<29:16,  4.73s/it]Running Inference:  26%|██▌       | 130/500 [09:53<33:30,  5.43s/it]Running Inference:  26%|██▌       | 131/500 [09:57<31:34,  5.13s/it]Running Inference:  26%|██▋       | 132/500 [09:59<25:03,  4.08s/it]Running Inference:  27%|██▋       | 133/500 [10:01<21:23,  3.50s/it]Running Inference:  27%|██▋       | 134/500 [10:06<23:40,  3.88s/it]Running Inference:  27%|██▋       | 135/500 [10:15<32:42,  5.38s/it]Running Inference:  27%|██▋       | 136/500 [10:21<34:25,  5.67s/it]Running Inference:  27%|██▋       | 137/500 [10:27<33:46,  5.58s/it]Running Inference:  28%|██▊       | 138/500 [10:30<29:49,  4.94s/it]Running Inference:  28%|██▊       | 139/500 [10:36<32:05,  5.33s/it]Running Inference:  28%|██▊       | 140/500 [10:39<27:30,  4.59s/it]Running Inference:  28%|██▊       | 141/500 [10:42<24:39,  4.12s/it]Running Inference:  28%|██▊       | 142/500 [10:43<18:59,  3.18s/it]Running Inference:  29%|██▊       | 143/500 [10:52<29:18,  4.93s/it]Running Inference:  29%|██▉       | 144/500 [10:56<27:29,  4.63s/it]Running Inference:  29%|██▉       | 145/500 [11:01<27:15,  4.61s/it]Running Inference:  29%|██▉       | 146/500 [11:05<26:00,  4.41s/it]Running Inference:  29%|██▉       | 147/500 [11:05<19:29,  3.31s/it]Running Inference:  30%|██▉       | 148/500 [11:11<23:00,  3.92s/it]Running Inference:  30%|██▉       | 149/500 [11:16<25:42,  4.40s/it]Running Inference:  30%|███       | 150/500 [11:22<27:24,  4.70s/it]Running Inference:  30%|███       | 151/500 [11:26<26:18,  4.52s/it]Running Inference:  30%|███       | 152/500 [11:27<20:54,  3.61s/it]Running Inference:  31%|███       | 153/500 [11:31<22:01,  3.81s/it]Running Inference:  31%|███       | 154/500 [11:37<24:45,  4.29s/it]Running Inference:  31%|███       | 155/500 [11:41<25:01,  4.35s/it]Running Inference:  31%|███       | 156/500 [11:45<23:01,  4.01s/it]Running Inference:  31%|███▏      | 157/500 [11:49<22:54,  4.01s/it]Running Inference:  32%|███▏      | 158/500 [11:50<18:21,  3.22s/it]Running Inference:  32%|███▏      | 159/500 [11:55<21:34,  3.80s/it]Running Inference:  32%|███▏      | 160/500 [12:00<22:52,  4.04s/it]Running Inference:  32%|███▏      | 161/500 [12:05<24:30,  4.34s/it]Running Inference:  32%|███▏      | 162/500 [12:09<24:16,  4.31s/it]Running Inference:  33%|███▎      | 163/500 [12:13<24:21,  4.34s/it]Running Inference:  33%|███▎      | 164/500 [12:19<26:47,  4.78s/it]Running Inference:  33%|███▎      | 165/500 [12:24<27:34,  4.94s/it]Running Inference:  33%|███▎      | 166/500 [12:30<27:39,  4.97s/it]Running Inference:  33%|███▎      | 167/500 [12:34<26:04,  4.70s/it]Running Inference:  34%|███▎      | 168/500 [12:36<22:12,  4.01s/it]Running Inference:  34%|███▍      | 169/500 [12:39<20:01,  3.63s/it]Running Inference:  34%|███▍      | 170/500 [12:43<20:43,  3.77s/it]Running Inference:  34%|███▍      | 171/500 [12:47<21:09,  3.86s/it]Running Inference:  34%|███▍      | 172/500 [12:51<21:22,  3.91s/it]Running Inference:  35%|███▍      | 173/500 [12:55<21:29,  3.94s/it]Running Inference:  35%|███▍      | 174/500 [12:59<21:28,  3.95s/it]Running Inference:  35%|███▌      | 175/500 [13:04<23:14,  4.29s/it]Running Inference:  35%|███▌      | 176/500 [13:09<24:29,  4.54s/it]Running Inference:  35%|███▌      | 177/500 [13:14<25:20,  4.71s/it]Running Inference:  36%|███▌      | 178/500 [13:20<26:48,  5.00s/it]Running Inference:  36%|███▌      | 179/500 [13:24<25:16,  4.72s/it]Running Inference:  36%|███▌      | 180/500 [13:28<24:25,  4.58s/it]Running Inference:  36%|███▌      | 181/500 [13:32<23:46,  4.47s/it]Running Inference:  36%|███▋      | 182/500 [13:37<23:26,  4.42s/it]Running Inference:  37%|███▋      | 183/500 [13:41<23:10,  4.39s/it]Running Inference:  37%|███▋      | 184/500 [13:45<22:25,  4.26s/it]Running Inference:  37%|███▋      | 185/500 [13:49<22:06,  4.21s/it]Running Inference:  37%|███▋      | 186/500 [13:54<23:32,  4.50s/it]Running Inference:  37%|███▋      | 187/500 [13:59<23:18,  4.47s/it]Running Inference:  38%|███▊      | 188/500 [14:03<22:40,  4.36s/it]Running Inference:  38%|███▊      | 189/500 [14:07<23:00,  4.44s/it]Running Inference:  38%|███▊      | 190/500 [14:16<28:56,  5.60s/it]Running Inference:  38%|███▊      | 191/500 [14:20<26:52,  5.22s/it]Running Inference:  38%|███▊      | 192/500 [14:25<25:47,  5.02s/it]Running Inference:  39%|███▊      | 193/500 [14:29<24:43,  4.83s/it]Running Inference:  39%|███▉      | 194/500 [14:33<24:04,  4.72s/it]Running Inference:  39%|███▉      | 195/500 [14:38<24:09,  4.75s/it]Running Inference:  39%|███▉      | 196/500 [14:42<22:57,  4.53s/it]Running Inference:  39%|███▉      | 197/500 [14:46<22:05,  4.37s/it]Running Inference:  40%|███▉      | 198/500 [14:50<21:32,  4.28s/it]Running Inference:  40%|███▉      | 199/500 [14:51<16:43,  3.33s/it]Running Inference:  40%|████      | 200/500 [14:56<17:47,  3.56s/it]Running Inference:  40%|████      | 201/500 [14:59<18:17,  3.67s/it]Running Inference:  40%|████      | 202/500 [15:05<21:41,  4.37s/it]Running Inference:  41%|████      | 203/500 [15:10<21:42,  4.38s/it]Running Inference:  41%|████      | 204/500 [15:14<21:33,  4.37s/it]Running Inference:  41%|████      | 205/500 [15:18<21:15,  4.32s/it]Running Inference:  41%|████      | 206/500 [15:23<22:02,  4.50s/it]Running Inference:  41%|████▏     | 207/500 [15:28<21:45,  4.46s/it]Running Inference:  42%|████▏     | 208/500 [15:32<20:49,  4.28s/it]Running Inference:  42%|████▏     | 209/500 [15:35<20:11,  4.16s/it]Running Inference:  42%|████▏     | 210/500 [15:39<19:48,  4.10s/it]Running Inference:  42%|████▏     | 211/500 [15:45<21:25,  4.45s/it]Running Inference:  42%|████▏     | 212/500 [15:49<21:03,  4.39s/it]Running Inference:  43%|████▎     | 213/500 [15:54<21:28,  4.49s/it]Running Inference:  43%|████▎     | 214/500 [15:57<20:26,  4.29s/it]Running Inference:  43%|████▎     | 215/500 [16:02<20:06,  4.23s/it]Running Inference:  43%|████▎     | 216/500 [16:06<19:42,  4.17s/it]Running Inference:  43%|████▎     | 217/500 [16:10<19:32,  4.14s/it]Running Inference:  44%|████▎     | 218/500 [16:14<19:22,  4.12s/it]Running Inference:  44%|████▍     | 219/500 [16:18<19:12,  4.10s/it]Running Inference:  44%|████▍     | 220/500 [16:22<19:01,  4.08s/it]Running Inference:  44%|████▍     | 221/500 [16:23<14:53,  3.20s/it]Running Inference:  44%|████▍     | 222/500 [16:27<16:02,  3.46s/it]Running Inference:  45%|████▍     | 223/500 [16:32<17:24,  3.77s/it]Running Inference:  45%|████▍     | 224/500 [16:36<18:33,  4.03s/it]Running Inference:  45%|████▌     | 225/500 [16:41<18:56,  4.13s/it]Running Inference:  45%|████▌     | 226/500 [16:44<17:36,  3.85s/it]Running Inference:  45%|████▌     | 227/500 [16:50<20:31,  4.51s/it]Running Inference:  46%|████▌     | 228/500 [16:56<22:38,  4.99s/it]Running Inference:  46%|████▌     | 229/500 [17:02<24:12,  5.36s/it]Running Inference:  46%|████▌     | 230/500 [17:04<20:02,  4.45s/it]Running Inference:  46%|████▌     | 231/500 [17:07<17:16,  3.85s/it]Running Inference:  46%|████▋     | 232/500 [17:10<16:04,  3.60s/it]Running Inference:  47%|████▋     | 233/500 [17:14<16:16,  3.66s/it]Running Inference:  47%|████▋     | 234/500 [17:15<12:33,  2.83s/it]Running Inference:  47%|████▋     | 235/500 [17:20<15:18,  3.47s/it]Running Inference:  47%|████▋     | 236/500 [17:22<13:14,  3.01s/it]Running Inference:  47%|████▋     | 237/500 [17:26<14:36,  3.33s/it]Running Inference:  48%|████▊     | 238/500 [17:31<17:25,  3.99s/it]Running Inference:  48%|████▊     | 239/500 [17:35<17:25,  4.01s/it]Running Inference:  48%|████▊     | 240/500 [17:40<18:33,  4.28s/it]Running Inference:  48%|████▊     | 241/500 [17:45<19:07,  4.43s/it]Running Inference:  48%|████▊     | 242/500 [17:48<16:52,  3.92s/it]Running Inference:  49%|████▊     | 243/500 [17:52<17:44,  4.14s/it]Running Inference:  49%|████▉     | 244/500 [17:55<15:48,  3.71s/it]Running Inference:  49%|████▉     | 245/500 [18:00<18:01,  4.24s/it]Running Inference:  49%|████▉     | 246/500 [18:02<14:30,  3.43s/it]Running Inference:  49%|████▉     | 247/500 [18:06<15:22,  3.65s/it]Running Inference:  50%|████▉     | 248/500 [18:11<16:43,  3.98s/it]Running Inference:  50%|████▉     | 249/500 [18:15<17:07,  4.10s/it]Running Inference:  50%|█████     | 250/500 [18:19<16:46,  4.03s/it]Running Inference:  50%|█████     | 251/500 [18:25<19:00,  4.58s/it]Running Inference:  50%|█████     | 252/500 [18:30<19:49,  4.80s/it]Running Inference:  51%|█████     | 253/500 [18:33<17:29,  4.25s/it]Running Inference:  51%|█████     | 254/500 [18:38<17:45,  4.33s/it]Running Inference:  51%|█████     | 255/500 [18:42<17:54,  4.39s/it]Running Inference:  51%|█████     | 256/500 [18:48<19:16,  4.74s/it]Running Inference:  51%|█████▏    | 257/500 [18:52<18:46,  4.64s/it]Running Inference:  52%|█████▏    | 258/500 [18:55<16:23,  4.06s/it]Running Inference:  52%|█████▏    | 259/500 [18:58<15:10,  3.78s/it]Running Inference:  52%|█████▏    | 260/500 [19:02<15:15,  3.82s/it]Running Inference:  52%|█████▏    | 261/500 [19:08<18:11,  4.57s/it]Running Inference:  52%|█████▏    | 262/500 [19:10<14:53,  3.75s/it]Running Inference:  53%|█████▎    | 263/500 [19:11<11:48,  2.99s/it]Running Inference:  53%|█████▎    | 264/500 [19:16<13:47,  3.50s/it]Running Inference:  53%|█████▎    | 265/500 [19:21<14:51,  3.79s/it]Running Inference:  53%|█████▎    | 266/500 [19:25<15:38,  4.01s/it]Running Inference:  53%|█████▎    | 267/500 [19:33<19:31,  5.03s/it]Running Inference:  54%|█████▎    | 268/500 [19:37<18:21,  4.75s/it]Running Inference:  54%|█████▍    | 269/500 [19:44<21:54,  5.69s/it]Running Inference:  54%|█████▍    | 270/500 [19:53<24:50,  6.48s/it]Running Inference:  54%|█████▍    | 271/500 [19:59<24:39,  6.46s/it]Running Inference:  54%|█████▍    | 272/500 [20:03<21:44,  5.72s/it]Running Inference:  55%|█████▍    | 273/500 [20:06<18:17,  4.84s/it]Running Inference:  55%|█████▍    | 274/500 [20:09<16:15,  4.32s/it]Running Inference:  55%|█████▌    | 275/500 [20:13<15:57,  4.26s/it]Running Inference:  55%|█████▌    | 276/500 [20:16<14:37,  3.92s/it]Running Inference:  55%|█████▌    | 277/500 [20:21<15:28,  4.16s/it]Running Inference:  56%|█████▌    | 278/500 [20:27<17:02,  4.61s/it]Running Inference:  56%|█████▌    | 279/500 [20:33<18:17,  4.96s/it]Running Inference:  56%|█████▌    | 280/500 [20:38<18:40,  5.09s/it]Running Inference:  56%|█████▌    | 281/500 [20:42<17:39,  4.84s/it]Running Inference:  56%|█████▋    | 282/500 [20:48<18:09,  5.00s/it]Running Inference:  57%|█████▋    | 283/500 [20:53<18:07,  5.01s/it]Running Inference:  57%|█████▋    | 284/500 [20:58<18:05,  5.02s/it]Running Inference:  57%|█████▋    | 285/500 [21:02<17:06,  4.77s/it]Running Inference:  57%|█████▋    | 286/500 [21:06<16:25,  4.61s/it]Running Inference:  57%|█████▋    | 287/500 [21:13<18:34,  5.23s/it]Running Inference:  58%|█████▊    | 288/500 [21:17<17:05,  4.84s/it]Running Inference:  58%|█████▊    | 289/500 [21:21<16:01,  4.56s/it]Running Inference:  58%|█████▊    | 290/500 [21:25<16:12,  4.63s/it]Running Inference:  58%|█████▊    | 291/500 [21:31<17:18,  4.97s/it]Running Inference:  58%|█████▊    | 292/500 [21:32<13:16,  3.83s/it]Running Inference:  59%|█████▊    | 293/500 [21:36<13:19,  3.86s/it]Running Inference:  59%|█████▉    | 294/500 [21:41<13:45,  4.01s/it]Running Inference:  59%|█████▉    | 295/500 [21:45<14:33,  4.26s/it]Running Inference:  59%|█████▉    | 296/500 [21:48<12:38,  3.72s/it]Running Inference:  59%|█████▉    | 297/500 [21:50<10:56,  3.23s/it]Running Inference:  60%|█████▉    | 298/500 [21:54<11:52,  3.53s/it]Running Inference:  60%|█████▉    | 299/500 [21:58<12:34,  3.75s/it]Running Inference:  60%|██████    | 300/500 [22:01<11:48,  3.54s/it]Running Inference:  60%|██████    | 301/500 [22:03<09:47,  2.95s/it]Running Inference:  60%|██████    | 302/500 [22:07<10:45,  3.26s/it]Running Inference:  61%|██████    | 303/500 [22:11<11:38,  3.55s/it]Running Inference:  61%|██████    | 304/500 [22:15<12:14,  3.75s/it]Running Inference:  61%|██████    | 305/500 [22:19<12:18,  3.79s/it]Running Inference:  61%|██████    | 306/500 [22:24<13:00,  4.02s/it]Running Inference:  61%|██████▏   | 307/500 [22:28<12:55,  4.02s/it]Running Inference:  62%|██████▏   | 308/500 [22:32<12:45,  3.99s/it]Running Inference:  62%|██████▏   | 309/500 [22:36<12:44,  4.00s/it]Running Inference:  62%|██████▏   | 310/500 [22:40<13:13,  4.18s/it]Running Inference:  62%|██████▏   | 311/500 [22:44<12:55,  4.11s/it]Running Inference:  62%|██████▏   | 312/500 [22:49<12:51,  4.10s/it]Running Inference:  63%|██████▎   | 313/500 [22:53<13:22,  4.29s/it]Running Inference:  63%|██████▎   | 314/500 [22:57<13:08,  4.24s/it]Running Inference:  63%|██████▎   | 315/500 [23:00<12:02,  3.91s/it]Running Inference:  63%|██████▎   | 316/500 [23:02<10:12,  3.33s/it]Running Inference:  63%|██████▎   | 317/500 [23:08<11:46,  3.86s/it]Running Inference:  64%|██████▎   | 318/500 [23:13<12:53,  4.25s/it]Running Inference:  64%|██████▍   | 319/500 [23:18<13:36,  4.51s/it]Running Inference:  64%|██████▍   | 320/500 [23:22<13:03,  4.35s/it]Running Inference:  64%|██████▍   | 321/500 [23:26<12:54,  4.32s/it]Running Inference:  64%|██████▍   | 322/500 [23:30<12:26,  4.20s/it]Running Inference:  65%|██████▍   | 323/500 [23:34<12:23,  4.20s/it]Running Inference:  65%|██████▍   | 324/500 [23:40<13:22,  4.56s/it]Running Inference:  65%|██████▌   | 325/500 [23:46<14:36,  5.01s/it]Running Inference:  65%|██████▌   | 326/500 [23:49<13:29,  4.65s/it]Running Inference:  65%|██████▌   | 327/500 [23:54<13:42,  4.75s/it]Running Inference:  66%|██████▌   | 328/500 [23:58<12:59,  4.53s/it]Running Inference:  66%|██████▌   | 329/500 [24:04<13:24,  4.70s/it]Running Inference:  66%|██████▌   | 330/500 [24:09<13:37,  4.81s/it]Running Inference:  66%|██████▌   | 331/500 [24:13<12:48,  4.55s/it]Running Inference:  66%|██████▋   | 332/500 [24:17<12:21,  4.41s/it]Running Inference:  67%|██████▋   | 333/500 [24:21<12:00,  4.31s/it]Running Inference:  67%|██████▋   | 334/500 [24:25<11:35,  4.19s/it]Running Inference:  67%|██████▋   | 335/500 [24:34<15:31,  5.65s/it]Running Inference:  67%|██████▋   | 336/500 [24:39<15:22,  5.63s/it]Running Inference:  67%|██████▋   | 337/500 [24:44<14:35,  5.37s/it]Running Inference:  68%|██████▊   | 338/500 [24:48<13:20,  4.94s/it]Running Inference:  68%|██████▊   | 339/500 [24:52<12:20,  4.60s/it]Running Inference:  68%|██████▊   | 340/500 [24:57<12:59,  4.87s/it]Running Inference:  68%|██████▊   | 341/500 [24:59<10:41,  4.03s/it]Running Inference:  68%|██████▊   | 342/500 [25:04<11:16,  4.28s/it]Running Inference:  69%|██████▊   | 343/500 [25:08<10:51,  4.15s/it]Running Inference:  69%|██████▉   | 344/500 [25:12<10:52,  4.18s/it]Running Inference:  69%|██████▉   | 345/500 [25:17<11:23,  4.41s/it]Running Inference:  69%|██████▉   | 346/500 [25:21<10:58,  4.27s/it]Running Inference:  69%|██████▉   | 347/500 [25:25<10:39,  4.18s/it]Running Inference:  70%|██████▉   | 348/500 [25:29<10:22,  4.09s/it]Running Inference:  70%|██████▉   | 349/500 [25:34<11:07,  4.42s/it]Running Inference:  70%|███████   | 350/500 [25:39<11:35,  4.64s/it]Running Inference:  70%|███████   | 351/500 [25:45<11:53,  4.79s/it]Running Inference:  70%|███████   | 352/500 [25:50<12:05,  4.90s/it]Running Inference:  71%|███████   | 353/500 [25:57<13:59,  5.71s/it]Running Inference:  71%|███████   | 354/500 [26:03<13:52,  5.70s/it]Running Inference:  71%|███████   | 355/500 [26:09<13:42,  5.67s/it]Running Inference:  71%|███████   | 356/500 [26:12<12:17,  5.12s/it]Running Inference:  71%|███████▏  | 357/500 [26:17<11:44,  4.92s/it]Running Inference:  72%|███████▏  | 358/500 [26:21<11:00,  4.65s/it]Running Inference:  72%|███████▏  | 359/500 [26:26<11:16,  4.80s/it]Running Inference:  72%|███████▏  | 360/500 [26:28<09:02,  3.88s/it]Running Inference:  72%|███████▏  | 361/500 [26:30<07:54,  3.42s/it]Running Inference:  72%|███████▏  | 362/500 [26:35<08:35,  3.73s/it]Running Inference:  73%|███████▎  | 363/500 [26:40<09:36,  4.21s/it]Running Inference:  73%|███████▎  | 364/500 [26:44<09:38,  4.25s/it]Running Inference:  73%|███████▎  | 365/500 [26:49<09:37,  4.28s/it]Running Inference:  73%|███████▎  | 366/500 [26:52<09:12,  4.13s/it]Running Inference:  73%|███████▎  | 367/500 [26:56<08:56,  4.03s/it]Running Inference:  74%|███████▎  | 368/500 [27:00<08:47,  4.00s/it]Running Inference:  74%|███████▍  | 369/500 [27:04<08:41,  3.98s/it]Running Inference:  74%|███████▍  | 370/500 [27:08<08:44,  4.04s/it]Running Inference:  74%|███████▍  | 371/500 [27:13<09:14,  4.30s/it]Running Inference:  74%|███████▍  | 372/500 [27:18<09:31,  4.47s/it]Running Inference:  75%|███████▍  | 373/500 [27:22<09:16,  4.38s/it]Running Inference:  75%|███████▍  | 374/500 [27:27<09:16,  4.41s/it]Running Inference:  75%|███████▌  | 375/500 [27:32<10:00,  4.81s/it]Running Inference:  75%|███████▌  | 376/500 [27:37<09:34,  4.63s/it]Running Inference:  75%|███████▌  | 377/500 [27:41<09:15,  4.51s/it]Running Inference:  76%|███████▌  | 378/500 [27:51<12:33,  6.18s/it]Running Inference:  76%|███████▌  | 379/500 [27:56<11:30,  5.70s/it]Running Inference:  76%|███████▌  | 380/500 [27:59<10:16,  5.14s/it]Running Inference:  76%|███████▌  | 381/500 [28:03<09:26,  4.76s/it]Running Inference:  76%|███████▋  | 382/500 [28:08<09:30,  4.83s/it]Running Inference:  77%|███████▋  | 383/500 [28:12<08:49,  4.52s/it]Running Inference:  77%|███████▋  | 384/500 [28:16<08:29,  4.39s/it]Running Inference:  77%|███████▋  | 385/500 [28:20<08:09,  4.26s/it]Running Inference:  77%|███████▋  | 386/500 [28:24<08:10,  4.31s/it]Running Inference:  77%|███████▋  | 387/500 [28:29<08:21,  4.43s/it]Running Inference:  78%|███████▊  | 388/500 [28:35<08:54,  4.78s/it]Running Inference:  78%|███████▊  | 389/500 [28:42<10:25,  5.64s/it]Running Inference:  78%|███████▊  | 390/500 [28:51<11:59,  6.54s/it]Running Inference:  78%|███████▊  | 391/500 [28:55<10:34,  5.82s/it]Running Inference:  78%|███████▊  | 392/500 [28:59<09:33,  5.31s/it]Running Inference:  79%|███████▊  | 393/500 [29:04<09:19,  5.23s/it]Running Inference:  79%|███████▉  | 394/500 [29:09<09:07,  5.17s/it]Running Inference:  79%|███████▉  | 395/500 [29:14<08:36,  4.92s/it]Running Inference:  79%|███████▉  | 396/500 [29:19<08:53,  5.13s/it]Running Inference:  79%|███████▉  | 397/500 [29:25<09:00,  5.25s/it]Running Inference:  80%|███████▉  | 398/500 [29:29<08:29,  4.99s/it]Running Inference:  80%|███████▉  | 399/500 [29:34<08:03,  4.79s/it]Running Inference:  80%|████████  | 400/500 [29:37<07:29,  4.50s/it]Running Inference:  80%|████████  | 401/500 [29:41<07:05,  4.30s/it]Running Inference:  80%|████████  | 402/500 [29:43<05:33,  3.41s/it]Running Inference:  81%|████████  | 403/500 [29:46<05:44,  3.56s/it]Running Inference:  81%|████████  | 404/500 [29:51<06:05,  3.80s/it]Running Inference:  81%|████████  | 405/500 [29:57<07:21,  4.65s/it]Running Inference:  81%|████████  | 406/500 [30:04<08:14,  5.26s/it]Running Inference:  81%|████████▏ | 407/500 [30:09<07:56,  5.12s/it]Running Inference:  82%|████████▏ | 408/500 [30:13<07:24,  4.83s/it]Running Inference:  82%|████████▏ | 409/500 [30:19<07:35,  5.00s/it]Running Inference:  82%|████████▏ | 410/500 [30:23<07:16,  4.85s/it]Running Inference:  82%|████████▏ | 411/500 [30:27<06:50,  4.62s/it]Running Inference:  82%|████████▏ | 412/500 [30:32<06:52,  4.69s/it]Running Inference:  83%|████████▎ | 413/500 [30:37<07:00,  4.83s/it]Running Inference:  83%|████████▎ | 414/500 [30:42<06:45,  4.71s/it]Running Inference:  83%|████████▎ | 415/500 [30:47<06:51,  4.84s/it]Running Inference:  83%|████████▎ | 416/500 [30:51<06:27,  4.62s/it]Running Inference:  83%|████████▎ | 417/500 [30:55<06:04,  4.40s/it]Running Inference:  84%|████████▎ | 418/500 [30:59<05:47,  4.24s/it]Running Inference:  84%|████████▍ | 419/500 [31:03<05:37,  4.17s/it]Running Inference:  84%|████████▍ | 420/500 [31:07<05:30,  4.13s/it]Running Inference:  84%|████████▍ | 421/500 [31:09<04:43,  3.59s/it]Running Inference:  84%|████████▍ | 422/500 [31:13<04:57,  3.82s/it]Running Inference:  85%|████████▍ | 423/500 [31:20<05:53,  4.60s/it]Running Inference:  85%|████████▍ | 424/500 [31:24<05:53,  4.65s/it]Running Inference:  85%|████████▌ | 425/500 [31:29<05:51,  4.69s/it]Running Inference:  85%|████████▌ | 426/500 [31:34<05:39,  4.59s/it]Running Inference:  85%|████████▌ | 427/500 [31:40<06:08,  5.05s/it]Running Inference:  86%|████████▌ | 428/500 [31:44<05:48,  4.85s/it]Running Inference:  86%|████████▌ | 429/500 [31:49<05:39,  4.79s/it]Running Inference:  86%|████████▌ | 430/500 [31:53<05:29,  4.70s/it]Running Inference:  86%|████████▌ | 431/500 [31:57<05:06,  4.44s/it]Running Inference:  86%|████████▋ | 432/500 [32:03<05:34,  4.92s/it]Running Inference:  87%|████████▋ | 433/500 [32:09<05:53,  5.28s/it]Running Inference:  87%|████████▋ | 434/500 [32:10<04:17,  3.91s/it]Running Inference:  87%|████████▋ | 435/500 [32:14<04:12,  3.89s/it]Running Inference:  87%|████████▋ | 436/500 [32:18<04:07,  3.87s/it]Running Inference:  87%|████████▋ | 437/500 [32:22<04:11,  4.00s/it]Running Inference:  88%|████████▊ | 438/500 [32:26<04:18,  4.18s/it]Running Inference:  88%|████████▊ | 439/500 [32:34<05:13,  5.14s/it]Running Inference:  88%|████████▊ | 440/500 [32:38<04:59,  4.99s/it]Running Inference:  88%|████████▊ | 441/500 [32:43<04:54,  4.99s/it]Running Inference:  88%|████████▊ | 442/500 [32:48<04:43,  4.89s/it]Running Inference:  89%|████████▊ | 443/500 [32:53<04:43,  4.98s/it]Running Inference:  89%|████████▉ | 444/500 [32:58<04:28,  4.80s/it]Running Inference:  89%|████████▉ | 445/500 [33:03<04:28,  4.88s/it]Running Inference:  89%|████████▉ | 446/500 [33:10<04:54,  5.46s/it]Running Inference:  89%|████████▉ | 447/500 [33:15<04:48,  5.44s/it]Running Inference:  90%|████████▉ | 448/500 [33:20<04:39,  5.37s/it]Running Inference:  90%|████████▉ | 449/500 [33:23<03:53,  4.58s/it]Running Inference:  90%|█████████ | 450/500 [33:27<03:42,  4.44s/it]Running Inference:  90%|█████████ | 451/500 [33:32<03:45,  4.60s/it]Running Inference:  90%|█████████ | 452/500 [33:37<03:50,  4.80s/it]Running Inference:  91%|█████████ | 453/500 [33:42<03:47,  4.84s/it]Running Inference:  91%|█████████ | 454/500 [33:47<03:35,  4.69s/it]Running Inference:  91%|█████████ | 455/500 [33:52<03:37,  4.84s/it]Running Inference:  91%|█████████ | 456/500 [33:56<03:26,  4.69s/it]Running Inference:  91%|█████████▏| 457/500 [34:00<03:12,  4.47s/it]Running Inference:  92%|█████████▏| 458/500 [34:05<03:19,  4.76s/it]Running Inference:  92%|█████████▏| 459/500 [34:11<03:22,  4.94s/it]Running Inference:  92%|█████████▏| 460/500 [34:16<03:24,  5.10s/it]Running Inference:  92%|█████████▏| 461/500 [34:21<03:13,  4.95s/it]Running Inference:  92%|█████████▏| 462/500 [34:26<03:10,  5.02s/it]Running Inference:  93%|█████████▎| 463/500 [34:31<02:59,  4.86s/it]Running Inference:  93%|█████████▎| 464/500 [34:35<02:51,  4.77s/it]Running Inference:  93%|█████████▎| 465/500 [34:37<02:16,  3.89s/it]Running Inference:  93%|█████████▎| 466/500 [34:41<02:16,  4.01s/it]Running Inference:  93%|█████████▎| 467/500 [34:45<02:13,  4.05s/it]Running Inference:  94%|█████████▎| 468/500 [34:49<02:07,  4.00s/it]Running Inference:  94%|█████████▍| 469/500 [34:53<02:02,  3.94s/it]Running Inference:  94%|█████████▍| 470/500 [34:55<01:36,  3.21s/it]Running Inference:  94%|█████████▍| 471/500 [34:59<01:40,  3.46s/it]Running Inference:  94%|█████████▍| 472/500 [35:03<01:41,  3.64s/it]Running Inference:  95%|█████████▍| 473/500 [35:08<01:50,  4.08s/it]Running Inference:  95%|█████████▍| 474/500 [35:13<01:53,  4.38s/it]Running Inference:  95%|█████████▌| 475/500 [35:18<01:53,  4.55s/it]Running Inference:  95%|█████████▌| 476/500 [35:22<01:44,  4.35s/it]Running Inference:  95%|█████████▌| 477/500 [35:26<01:36,  4.20s/it]Running Inference:  96%|█████████▌| 478/500 [35:32<01:44,  4.73s/it]Running Inference:  96%|█████████▌| 479/500 [35:39<01:56,  5.53s/it]Running Inference:  96%|█████████▌| 480/500 [35:40<01:23,  4.19s/it]Running Inference:  96%|█████████▌| 481/500 [35:45<01:24,  4.44s/it]Running Inference:  96%|█████████▋| 482/500 [35:46<01:03,  3.53s/it]Running Inference:  97%|█████████▋| 483/500 [35:51<01:04,  3.78s/it]Running Inference:  97%|█████████▋| 484/500 [35:55<01:01,  3.81s/it]Running Inference:  97%|█████████▋| 485/500 [35:59<01:01,  4.08s/it]Running Inference:  97%|█████████▋| 486/500 [36:04<00:57,  4.12s/it]Running Inference:  97%|█████████▋| 487/500 [36:08<00:55,  4.28s/it]Running Inference:  98%|█████████▊| 488/500 [36:12<00:50,  4.19s/it]Running Inference:  98%|█████████▊| 489/500 [36:17<00:49,  4.48s/it]Running Inference:  98%|█████████▊| 490/500 [36:22<00:45,  4.54s/it]Running Inference:  98%|█████████▊| 491/500 [36:27<00:41,  4.62s/it]Running Inference:  98%|█████████▊| 492/500 [36:33<00:39,  4.95s/it]Running Inference:  99%|█████████▊| 493/500 [36:37<00:32,  4.70s/it]Running Inference:  99%|█████████▉| 494/500 [36:41<00:27,  4.62s/it]Running Inference:  99%|█████████▉| 495/500 [36:46<00:22,  4.55s/it]Running Inference:  99%|█████████▉| 496/500 [36:50<00:18,  4.59s/it]Running Inference:  99%|█████████▉| 497/500 [36:55<00:13,  4.61s/it]Running Inference: 100%|█████████▉| 498/500 [36:59<00:09,  4.53s/it]Running Inference: 100%|█████████▉| 499/500 [37:03<00:04,  4.45s/it]Running Inference: 100%|██████████| 500/500 [37:08<00:00,  4.34s/it]Running Inference: 100%|██████████| 500/500 [37:08<00:00,  4.46s/it]
2025-12-14 23:54:34,523 - INFO - Inference completed.
2025-12-14 23:54:34,576 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-14 23:54:34,576 - INFO - Calculating metrics for dataset: longbench
2025-12-14 23:54:34,578 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-14 23:54:34,578 - INFO - Metrics:
13.29
2025-12-14 23:54:34,580 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=repobench-p, ratio=0.1) on GPU 2

----------------------------------------
Task: repobench-p | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-14 23:54:40,989 - INFO - Set deterministic seeds to 42
2025-12-14 23:54:40,989 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-14 23:54:40,989 - INFO - Starting evaluation run...
2025-12-14 23:54:40,990 - INFO - Output directory set to: longbenchresult
2025-12-14 23:54:40,990 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-14 23:54:40,990 - INFO - KV Press 'streaming_llm' setup.
2025-12-14 23:54:40,990 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-14 23:54:40,990 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.11it/s]
Device set to use cuda:0
2025-12-14 23:54:55,809 - INFO - Model pipeline loaded.
2025-12-14 23:54:55,809 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
2025-12-14 23:55:01,632 - INFO - Dataset loaded with 500 entries.
2025-12-14 23:55:01,632 - INFO - Dataset processed with 500 entries.
2025-12-14 23:55:01,677 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<42:55,  5.16s/it]Running Inference:   0%|          | 2/500 [00:11<47:47,  5.76s/it]Running Inference:   1%|          | 3/500 [00:14<37:32,  4.53s/it]Running Inference:   1%|          | 4/500 [00:18<36:06,  4.37s/it]Running Inference:   1%|          | 5/500 [00:23<36:36,  4.44s/it]Running Inference:   1%|          | 6/500 [00:27<35:53,  4.36s/it]Running Inference:   1%|▏         | 7/500 [00:28<28:20,  3.45s/it]Running Inference:   2%|▏         | 8/500 [00:32<29:44,  3.63s/it]Running Inference:   2%|▏         | 9/500 [00:36<30:46,  3.76s/it]Running Inference:   2%|▏         | 10/500 [00:44<40:01,  4.90s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:50<43:07,  5.29s/it]Running Inference:   2%|▏         | 12/500 [00:54<40:51,  5.02s/it]Running Inference:   3%|▎         | 13/500 [01:00<41:30,  5.11s/it]Running Inference:   3%|▎         | 14/500 [01:10<53:11,  6.57s/it]Running Inference:   3%|▎         | 15/500 [01:14<48:39,  6.02s/it]Running Inference:   3%|▎         | 16/500 [01:19<44:22,  5.50s/it]Running Inference:   3%|▎         | 17/500 [01:24<44:12,  5.49s/it]Running Inference:   4%|▎         | 18/500 [01:30<44:43,  5.57s/it]Running Inference:   4%|▍         | 19/500 [01:34<41:21,  5.16s/it]Running Inference:   4%|▍         | 20/500 [01:41<45:37,  5.70s/it]Running Inference:   4%|▍         | 21/500 [01:45<41:34,  5.21s/it]Running Inference:   4%|▍         | 22/500 [01:51<42:30,  5.34s/it]Running Inference:   5%|▍         | 23/500 [01:54<36:20,  4.57s/it]Running Inference:   5%|▍         | 24/500 [02:00<39:36,  4.99s/it]Running Inference:   5%|▌         | 25/500 [02:06<41:39,  5.26s/it]Running Inference:   5%|▌         | 26/500 [02:10<40:16,  5.10s/it]Running Inference:   5%|▌         | 27/500 [02:14<37:48,  4.80s/it]Running Inference:   6%|▌         | 28/500 [02:18<35:37,  4.53s/it]Running Inference:   6%|▌         | 29/500 [02:22<34:23,  4.38s/it]Running Inference:   6%|▌         | 30/500 [02:27<36:20,  4.64s/it]Running Inference:   6%|▌         | 31/500 [02:33<37:38,  4.82s/it]Running Inference:   6%|▋         | 32/500 [02:37<36:20,  4.66s/it]Running Inference:   7%|▋         | 33/500 [02:41<34:53,  4.48s/it]Running Inference:   7%|▋         | 34/500 [02:45<34:28,  4.44s/it]Running Inference:   7%|▋         | 35/500 [02:50<34:22,  4.44s/it]Running Inference:   7%|▋         | 36/500 [02:55<35:10,  4.55s/it]Running Inference:   7%|▋         | 37/500 [02:59<35:26,  4.59s/it]Running Inference:   8%|▊         | 38/500 [03:04<35:06,  4.56s/it]Running Inference:   8%|▊         | 39/500 [03:08<34:04,  4.44s/it]Running Inference:   8%|▊         | 40/500 [03:12<33:18,  4.34s/it]Running Inference:   8%|▊         | 41/500 [03:16<32:40,  4.27s/it]Running Inference:   8%|▊         | 42/500 [03:20<31:37,  4.14s/it]Running Inference:   9%|▊         | 43/500 [03:21<23:54,  3.14s/it]Running Inference:   9%|▉         | 44/500 [03:27<30:22,  4.00s/it]Running Inference:   9%|▉         | 45/500 [03:32<33:09,  4.37s/it]Running Inference:   9%|▉         | 46/500 [03:37<35:10,  4.65s/it]Running Inference:   9%|▉         | 47/500 [03:43<36:35,  4.85s/it]Running Inference:  10%|▉         | 48/500 [03:47<35:22,  4.69s/it]Running Inference:  10%|▉         | 49/500 [03:52<34:59,  4.66s/it]Running Inference:  10%|█         | 50/500 [03:57<37:25,  4.99s/it]Running Inference:  10%|█         | 51/500 [04:02<37:21,  4.99s/it]Running Inference:  10%|█         | 52/500 [04:07<36:17,  4.86s/it]Running Inference:  11%|█         | 53/500 [04:11<34:32,  4.64s/it]Running Inference:  11%|█         | 54/500 [04:15<33:25,  4.50s/it]Running Inference:  11%|█         | 55/500 [04:22<37:20,  5.03s/it]Running Inference:  11%|█         | 56/500 [04:26<36:14,  4.90s/it]Running Inference:  11%|█▏        | 57/500 [04:30<33:50,  4.58s/it]Running Inference:  12%|█▏        | 58/500 [04:34<32:34,  4.42s/it]Running Inference:  12%|█▏        | 59/500 [04:39<33:06,  4.50s/it]Running Inference:  12%|█▏        | 60/500 [04:43<32:09,  4.38s/it]Running Inference:  12%|█▏        | 61/500 [04:47<31:08,  4.26s/it]Running Inference:  12%|█▏        | 62/500 [04:53<34:26,  4.72s/it]Running Inference:  13%|█▎        | 63/500 [04:58<36:41,  5.04s/it]Running Inference:  13%|█▎        | 64/500 [05:03<35:20,  4.86s/it]Running Inference:  13%|█▎        | 65/500 [05:09<37:49,  5.22s/it]Running Inference:  13%|█▎        | 66/500 [05:13<34:43,  4.80s/it]Running Inference:  13%|█▎        | 67/500 [05:17<32:38,  4.52s/it]Running Inference:  14%|█▎        | 68/500 [05:22<34:34,  4.80s/it]Running Inference:  14%|█▍        | 69/500 [05:27<35:27,  4.94s/it]Running Inference:  14%|█▍        | 70/500 [05:36<42:40,  5.95s/it]Running Inference:  14%|█▍        | 71/500 [05:40<40:23,  5.65s/it]Running Inference:  14%|█▍        | 72/500 [05:45<38:31,  5.40s/it]Running Inference:  15%|█▍        | 73/500 [05:51<38:30,  5.41s/it]Running Inference:  15%|█▍        | 74/500 [05:56<37:40,  5.31s/it]Running Inference:  15%|█▌        | 75/500 [06:01<36:43,  5.18s/it]Running Inference:  15%|█▌        | 76/500 [06:06<36:26,  5.16s/it]Running Inference:  15%|█▌        | 77/500 [06:10<34:53,  4.95s/it]Running Inference:  16%|█▌        | 78/500 [06:17<38:04,  5.41s/it]Running Inference:  16%|█▌        | 79/500 [06:21<34:57,  4.98s/it]Running Inference:  16%|█▌        | 80/500 [06:25<33:39,  4.81s/it]Running Inference:  16%|█▌        | 81/500 [06:31<34:52,  4.99s/it]Running Inference:  16%|█▋        | 82/500 [06:36<35:19,  5.07s/it]Running Inference:  17%|█▋        | 83/500 [06:44<41:06,  5.91s/it]Running Inference:  17%|█▋        | 84/500 [06:49<38:44,  5.59s/it]Running Inference:  17%|█▋        | 85/500 [06:50<30:46,  4.45s/it]Running Inference:  17%|█▋        | 86/500 [06:55<31:27,  4.56s/it]Running Inference:  17%|█▋        | 87/500 [06:56<24:38,  3.58s/it]Running Inference:  18%|█▊        | 88/500 [07:01<25:43,  3.75s/it]Running Inference:  18%|█▊        | 89/500 [07:05<26:32,  3.87s/it]Running Inference:  18%|█▊        | 90/500 [07:09<27:07,  3.97s/it]Running Inference:  18%|█▊        | 91/500 [07:10<21:43,  3.19s/it]Running Inference:  18%|█▊        | 92/500 [07:14<22:38,  3.33s/it]Running Inference:  19%|█▊        | 93/500 [07:19<26:03,  3.84s/it]Running Inference:  19%|█▉        | 94/500 [07:23<25:21,  3.75s/it]Running Inference:  19%|█▉        | 95/500 [07:30<32:46,  4.85s/it]Running Inference:  19%|█▉        | 96/500 [07:33<28:05,  4.17s/it]Running Inference:  19%|█▉        | 97/500 [07:37<29:08,  4.34s/it]Running Inference:  20%|█▉        | 98/500 [07:39<23:48,  3.55s/it]Running Inference:  20%|█▉        | 99/500 [07:41<20:23,  3.05s/it]Running Inference:  20%|██        | 100/500 [07:46<23:33,  3.53s/it]Running Inference:  20%|██        | 101/500 [07:50<24:29,  3.68s/it]Running Inference:  20%|██        | 102/500 [07:51<20:04,  3.03s/it]Running Inference:  21%|██        | 103/500 [07:55<22:11,  3.36s/it]Running Inference:  21%|██        | 104/500 [08:00<25:41,  3.89s/it]Running Inference:  21%|██        | 105/500 [08:02<20:29,  3.11s/it]Running Inference:  21%|██        | 106/500 [08:03<17:36,  2.68s/it]Running Inference:  21%|██▏       | 107/500 [08:07<20:03,  3.06s/it]Running Inference:  22%|██▏       | 108/500 [08:12<22:56,  3.51s/it]Running Inference:  22%|██▏       | 109/500 [08:18<27:45,  4.26s/it]Running Inference:  22%|██▏       | 110/500 [08:24<31:09,  4.79s/it]Running Inference:  22%|██▏       | 111/500 [08:29<31:15,  4.82s/it]Running Inference:  22%|██▏       | 112/500 [08:33<30:20,  4.69s/it]Running Inference:  23%|██▎       | 113/500 [08:37<29:06,  4.51s/it]Running Inference:  23%|██▎       | 114/500 [08:42<28:40,  4.46s/it]Running Inference:  23%|██▎       | 115/500 [08:46<28:52,  4.50s/it]Running Inference:  23%|██▎       | 116/500 [08:50<28:16,  4.42s/it]Running Inference:  23%|██▎       | 117/500 [08:55<28:42,  4.50s/it]Running Inference:  24%|██▎       | 118/500 [09:00<29:32,  4.64s/it]Running Inference:  24%|██▍       | 119/500 [09:04<29:08,  4.59s/it]Running Inference:  24%|██▍       | 120/500 [09:09<28:07,  4.44s/it]Running Inference:  24%|██▍       | 121/500 [09:11<24:47,  3.93s/it]Running Inference:  24%|██▍       | 122/500 [09:15<24:32,  3.89s/it]Running Inference:  25%|██▍       | 123/500 [09:19<24:41,  3.93s/it]Running Inference:  25%|██▍       | 124/500 [09:23<24:48,  3.96s/it]Running Inference:  25%|██▌       | 125/500 [09:28<25:46,  4.12s/it]Running Inference:  25%|██▌       | 126/500 [09:29<21:09,  3.39s/it]Running Inference:  25%|██▌       | 127/500 [09:33<22:07,  3.56s/it]Running Inference:  26%|██▌       | 128/500 [09:37<22:43,  3.67s/it]Running Inference:  26%|██▌       | 129/500 [09:45<29:22,  4.75s/it]Running Inference:  26%|██▌       | 130/500 [09:52<33:31,  5.44s/it]Running Inference:  26%|██▌       | 131/500 [09:56<31:40,  5.15s/it]Running Inference:  26%|██▋       | 132/500 [09:58<25:08,  4.10s/it]Running Inference:  27%|██▋       | 133/500 [10:00<21:28,  3.51s/it]Running Inference:  27%|██▋       | 134/500 [10:05<23:49,  3.91s/it]Running Inference:  27%|██▋       | 135/500 [10:14<32:48,  5.39s/it]Running Inference:  27%|██▋       | 136/500 [10:20<34:28,  5.68s/it]Running Inference:  27%|██▋       | 137/500 [10:25<33:45,  5.58s/it]Running Inference:  28%|██▊       | 138/500 [10:29<29:47,  4.94s/it]Running Inference:  28%|██▊       | 139/500 [10:35<31:57,  5.31s/it]Running Inference:  28%|██▊       | 140/500 [10:38<27:25,  4.57s/it]Running Inference:  28%|██▊       | 141/500 [10:39<22:22,  3.74s/it]Running Inference:  28%|██▊       | 142/500 [10:43<22:32,  3.78s/it]Running Inference:  29%|██▊       | 143/500 [10:52<31:31,  5.30s/it]Running Inference:  29%|██▉       | 144/500 [10:56<29:05,  4.90s/it]Running Inference:  29%|██▉       | 145/500 [11:01<28:25,  4.80s/it]Running Inference:  29%|██▉       | 146/500 [11:05<26:51,  4.55s/it]Running Inference:  29%|██▉       | 147/500 [11:05<20:04,  3.41s/it]Running Inference:  30%|██▉       | 148/500 [11:11<23:28,  4.00s/it]Running Inference:  30%|██▉       | 149/500 [11:16<26:03,  4.45s/it]Running Inference:  30%|███       | 150/500 [11:22<27:43,  4.75s/it]Running Inference:  30%|███       | 151/500 [11:26<26:35,  4.57s/it]Running Inference:  30%|███       | 152/500 [11:27<21:07,  3.64s/it]Running Inference:  31%|███       | 153/500 [11:32<22:13,  3.84s/it]Running Inference:  31%|███       | 154/500 [11:37<24:57,  4.33s/it]Running Inference:  31%|███       | 155/500 [11:42<25:14,  4.39s/it]Running Inference:  31%|███       | 156/500 [11:47<27:09,  4.74s/it]Running Inference:  31%|███▏      | 157/500 [11:51<25:52,  4.53s/it]Running Inference:  32%|███▏      | 158/500 [11:53<20:26,  3.59s/it]Running Inference:  32%|███▏      | 159/500 [11:58<23:06,  4.07s/it]Running Inference:  32%|███▏      | 160/500 [12:03<24:00,  4.24s/it]Running Inference:  32%|███▏      | 161/500 [12:08<25:22,  4.49s/it]Running Inference:  32%|███▏      | 162/500 [12:12<24:48,  4.40s/it]Running Inference:  33%|███▎      | 163/500 [12:16<24:45,  4.41s/it]Running Inference:  33%|███▎      | 164/500 [12:22<27:03,  4.83s/it]Running Inference:  33%|███▎      | 165/500 [12:27<27:52,  4.99s/it]Running Inference:  33%|███▎      | 166/500 [12:32<27:55,  5.02s/it]Running Inference:  33%|███▎      | 167/500 [12:37<26:20,  4.75s/it]Running Inference:  34%|███▎      | 168/500 [12:39<22:22,  4.04s/it]Running Inference:  34%|███▍      | 169/500 [12:42<20:07,  3.65s/it]Running Inference:  34%|███▍      | 170/500 [12:46<20:50,  3.79s/it]Running Inference:  34%|███▍      | 171/500 [12:50<21:19,  3.89s/it]Running Inference:  34%|███▍      | 172/500 [12:54<21:33,  3.94s/it]Running Inference:  35%|███▍      | 173/500 [12:55<17:05,  3.13s/it]Running Inference:  35%|███▍      | 174/500 [12:59<18:31,  3.41s/it]Running Inference:  35%|███▌      | 175/500 [13:04<21:17,  3.93s/it]Running Inference:  35%|███▌      | 176/500 [13:10<23:13,  4.30s/it]Running Inference:  35%|███▌      | 177/500 [13:15<24:32,  4.56s/it]Running Inference:  36%|███▌      | 178/500 [13:20<26:15,  4.89s/it]Running Inference:  36%|███▌      | 179/500 [13:25<24:53,  4.65s/it]Running Inference:  36%|███▌      | 180/500 [13:29<24:13,  4.54s/it]Running Inference:  36%|███▌      | 181/500 [13:31<20:21,  3.83s/it]Running Inference:  36%|███▋      | 182/500 [13:35<21:05,  3.98s/it]Running Inference:  37%|███▋      | 183/500 [13:40<21:32,  4.08s/it]Running Inference:  37%|███▋      | 184/500 [13:44<21:19,  4.05s/it]Running Inference:  37%|███▋      | 185/500 [13:48<21:21,  4.07s/it]Running Inference:  37%|███▋      | 186/500 [13:53<22:59,  4.39s/it]Running Inference:  37%|███▋      | 187/500 [13:57<22:59,  4.41s/it]Running Inference:  38%|███▊      | 188/500 [14:02<22:32,  4.34s/it]Running Inference:  38%|███▊      | 189/500 [14:06<23:01,  4.44s/it]Running Inference:  38%|███▊      | 190/500 [14:12<24:31,  4.75s/it]Running Inference:  38%|███▊      | 191/500 [14:16<23:50,  4.63s/it]Running Inference:  38%|███▊      | 192/500 [14:21<23:46,  4.63s/it]Running Inference:  39%|███▊      | 193/500 [14:25<23:22,  4.57s/it]Running Inference:  39%|███▉      | 194/500 [14:27<18:36,  3.65s/it]Running Inference:  39%|███▉      | 195/500 [14:31<20:24,  4.02s/it]Running Inference:  39%|███▉      | 196/500 [14:36<20:26,  4.03s/it]Running Inference:  39%|███▉      | 197/500 [14:40<20:26,  4.05s/it]Running Inference:  40%|███▉      | 198/500 [14:42<17:13,  3.42s/it]Running Inference:  40%|███▉      | 199/500 [14:46<18:05,  3.61s/it]Running Inference:  40%|████      | 200/500 [14:50<18:48,  3.76s/it]Running Inference:  40%|████      | 201/500 [14:51<14:51,  2.98s/it]Running Inference:  40%|████      | 202/500 [14:57<19:17,  3.89s/it]Running Inference:  41%|████      | 203/500 [15:01<20:06,  4.06s/it]Running Inference:  41%|████      | 204/500 [15:06<20:33,  4.17s/it]Running Inference:  41%|████      | 205/500 [15:10<20:40,  4.21s/it]Running Inference:  41%|████      | 206/500 [15:15<21:42,  4.43s/it]Running Inference:  41%|████▏     | 207/500 [15:19<21:36,  4.43s/it]Running Inference:  42%|████▏     | 208/500 [15:23<20:49,  4.28s/it]Running Inference:  42%|████▏     | 209/500 [15:27<20:17,  4.18s/it]Running Inference:  42%|████▏     | 210/500 [15:31<19:57,  4.13s/it]Running Inference:  42%|████▏     | 211/500 [15:37<21:36,  4.49s/it]Running Inference:  42%|████▏     | 212/500 [15:41<21:15,  4.43s/it]Running Inference:  43%|████▎     | 213/500 [15:46<21:41,  4.53s/it]Running Inference:  43%|████▎     | 214/500 [15:50<20:38,  4.33s/it]Running Inference:  43%|████▎     | 215/500 [15:51<16:59,  3.58s/it]Running Inference:  43%|████▎     | 216/500 [15:55<17:33,  3.71s/it]Running Inference:  43%|████▎     | 217/500 [16:00<18:03,  3.83s/it]Running Inference:  44%|████▎     | 218/500 [16:04<18:23,  3.91s/it]Running Inference:  44%|████▍     | 219/500 [16:08<18:33,  3.96s/it]Running Inference:  44%|████▍     | 220/500 [16:12<18:38,  4.00s/it]Running Inference:  44%|████▍     | 221/500 [16:13<14:38,  3.15s/it]Running Inference:  44%|████▍     | 222/500 [16:17<15:58,  3.45s/it]Running Inference:  45%|████▍     | 223/500 [16:22<17:24,  3.77s/it]Running Inference:  45%|████▍     | 224/500 [16:26<18:37,  4.05s/it]Running Inference:  45%|████▌     | 225/500 [16:31<19:01,  4.15s/it]Running Inference:  45%|████▌     | 226/500 [16:37<21:22,  4.68s/it]Running Inference:  45%|████▌     | 227/500 [16:43<23:09,  5.09s/it]Running Inference:  46%|████▌     | 228/500 [16:46<20:48,  4.59s/it]Running Inference:  46%|████▌     | 229/500 [16:52<22:55,  5.08s/it]Running Inference:  46%|████▌     | 230/500 [16:55<19:08,  4.25s/it]Running Inference:  46%|████▌     | 231/500 [16:57<16:37,  3.71s/it]Running Inference:  46%|████▋     | 232/500 [17:00<15:37,  3.50s/it]Running Inference:  47%|████▋     | 233/500 [17:04<16:02,  3.61s/it]Running Inference:  47%|████▋     | 234/500 [17:08<16:20,  3.68s/it]Running Inference:  47%|████▋     | 235/500 [17:13<18:00,  4.08s/it]Running Inference:  47%|████▋     | 236/500 [17:18<19:08,  4.35s/it]Running Inference:  47%|████▋     | 237/500 [17:22<18:49,  4.29s/it]Running Inference:  48%|████▊     | 238/500 [17:28<20:24,  4.67s/it]Running Inference:  48%|████▊     | 239/500 [17:32<19:35,  4.50s/it]Running Inference:  48%|████▊     | 240/500 [17:37<20:08,  4.65s/it]Running Inference:  48%|████▊     | 241/500 [17:41<20:18,  4.71s/it]Running Inference:  48%|████▊     | 242/500 [17:47<21:17,  4.95s/it]Running Inference:  49%|████▊     | 243/500 [17:53<21:57,  5.13s/it]Running Inference:  49%|████▉     | 244/500 [17:55<18:45,  4.40s/it]Running Inference:  49%|████▉     | 245/500 [18:01<20:06,  4.73s/it]Running Inference:  49%|████▉     | 246/500 [18:02<15:59,  3.78s/it]Running Inference:  49%|████▉     | 247/500 [18:06<16:29,  3.91s/it]Running Inference:  50%|████▉     | 248/500 [18:11<17:34,  4.19s/it]Running Inference:  50%|████▉     | 249/500 [18:16<17:48,  4.26s/it]Running Inference:  50%|█████     | 250/500 [18:20<17:20,  4.16s/it]Running Inference:  50%|█████     | 251/500 [18:26<19:23,  4.67s/it]Running Inference:  50%|█████     | 252/500 [18:31<20:10,  4.88s/it]Running Inference:  51%|█████     | 253/500 [18:34<17:42,  4.30s/it]Running Inference:  51%|█████     | 254/500 [18:38<17:58,  4.39s/it]Running Inference:  51%|█████     | 255/500 [18:43<18:08,  4.44s/it]Running Inference:  51%|█████     | 256/500 [18:47<17:48,  4.38s/it]Running Inference:  51%|█████▏    | 257/500 [18:52<17:48,  4.40s/it]Running Inference:  52%|█████▏    | 258/500 [18:57<18:28,  4.58s/it]Running Inference:  52%|█████▏    | 259/500 [19:00<16:21,  4.07s/it]Running Inference:  52%|█████▏    | 260/500 [19:00<12:28,  3.12s/it]Running Inference:  52%|█████▏    | 261/500 [19:07<16:13,  4.07s/it]Running Inference:  52%|█████▏    | 262/500 [19:09<13:32,  3.41s/it]Running Inference:  53%|█████▎    | 263/500 [19:10<10:53,  2.76s/it]Running Inference:  53%|█████▎    | 264/500 [19:15<13:09,  3.34s/it]Running Inference:  53%|█████▎    | 265/500 [19:16<11:06,  2.84s/it]Running Inference:  53%|█████▎    | 266/500 [19:21<13:01,  3.34s/it]Running Inference:  53%|█████▎    | 267/500 [19:28<17:40,  4.55s/it]Running Inference:  54%|█████▎    | 268/500 [19:32<17:08,  4.43s/it]Running Inference:  54%|█████▍    | 269/500 [19:40<21:01,  5.46s/it]Running Inference:  54%|█████▍    | 270/500 [19:48<24:10,  6.31s/it]Running Inference:  54%|█████▍    | 271/500 [19:55<24:08,  6.32s/it]Running Inference:  54%|█████▍    | 272/500 [19:59<21:43,  5.72s/it]Running Inference:  55%|█████▍    | 273/500 [20:05<21:42,  5.74s/it]Running Inference:  55%|█████▍    | 274/500 [20:07<17:57,  4.77s/it]Running Inference:  55%|█████▌    | 275/500 [20:09<13:58,  3.73s/it]Running Inference:  55%|█████▌    | 276/500 [20:14<16:08,  4.32s/it]Running Inference:  55%|█████▌    | 277/500 [20:19<16:34,  4.46s/it]Running Inference:  56%|█████▌    | 278/500 [20:25<17:49,  4.82s/it]Running Inference:  56%|█████▌    | 279/500 [20:31<18:47,  5.10s/it]Running Inference:  56%|█████▌    | 280/500 [20:36<19:02,  5.19s/it]Running Inference:  56%|█████▌    | 281/500 [20:40<17:54,  4.91s/it]Running Inference:  56%|█████▋    | 282/500 [20:46<18:19,  5.04s/it]Running Inference:  57%|█████▋    | 283/500 [20:51<18:14,  5.05s/it]Running Inference:  57%|█████▋    | 284/500 [20:56<18:11,  5.05s/it]Running Inference:  57%|█████▋    | 285/500 [21:00<17:11,  4.80s/it]Running Inference:  57%|█████▋    | 286/500 [21:04<16:30,  4.63s/it]Running Inference:  57%|█████▋    | 287/500 [21:11<18:34,  5.23s/it]Running Inference:  58%|█████▊    | 288/500 [21:15<17:05,  4.84s/it]Running Inference:  58%|█████▊    | 289/500 [21:19<16:02,  4.56s/it]Running Inference:  58%|█████▊    | 290/500 [21:23<16:13,  4.63s/it]Running Inference:  58%|█████▊    | 291/500 [21:29<17:16,  4.96s/it]Running Inference:  58%|█████▊    | 292/500 [21:33<16:29,  4.76s/it]Running Inference:  59%|█████▊    | 293/500 [21:37<15:33,  4.51s/it]Running Inference:  59%|█████▉    | 294/500 [21:42<15:20,  4.47s/it]Running Inference:  59%|█████▉    | 295/500 [21:47<15:40,  4.59s/it]Running Inference:  59%|█████▉    | 296/500 [21:52<16:22,  4.82s/it]Running Inference:  59%|█████▉    | 297/500 [21:56<15:45,  4.66s/it]Running Inference:  60%|█████▉    | 298/500 [22:00<15:14,  4.53s/it]Running Inference:  60%|█████▉    | 299/500 [22:05<14:55,  4.46s/it]Running Inference:  60%|██████    | 300/500 [22:10<15:47,  4.74s/it]Running Inference:  60%|██████    | 301/500 [22:15<15:41,  4.73s/it]Running Inference:  60%|██████    | 302/500 [22:19<14:56,  4.53s/it]Running Inference:  61%|██████    | 303/500 [22:23<14:37,  4.45s/it]Running Inference:  61%|██████    | 304/500 [22:28<14:22,  4.40s/it]Running Inference:  61%|██████    | 305/500 [22:30<12:42,  3.91s/it]Running Inference:  61%|██████    | 306/500 [22:35<13:30,  4.18s/it]Running Inference:  61%|██████▏   | 307/500 [22:39<13:19,  4.14s/it]Running Inference:  62%|██████▏   | 308/500 [22:43<13:06,  4.10s/it]Running Inference:  62%|██████▏   | 309/500 [22:45<10:28,  3.29s/it]Running Inference:  62%|██████▏   | 310/500 [22:49<11:39,  3.68s/it]Running Inference:  62%|██████▏   | 311/500 [22:53<11:52,  3.77s/it]Running Inference:  62%|██████▏   | 312/500 [22:57<12:09,  3.88s/it]Running Inference:  63%|██████▎   | 313/500 [23:02<12:56,  4.15s/it]Running Inference:  63%|██████▎   | 314/500 [23:06<12:48,  4.13s/it]Running Inference:  63%|██████▎   | 315/500 [23:11<13:34,  4.40s/it]Running Inference:  63%|██████▎   | 316/500 [23:13<11:20,  3.70s/it]Running Inference:  63%|██████▎   | 317/500 [23:18<12:37,  4.14s/it]Running Inference:  64%|██████▎   | 318/500 [23:24<13:33,  4.47s/it]Running Inference:  64%|██████▍   | 319/500 [23:29<14:07,  4.68s/it]Running Inference:  64%|██████▍   | 320/500 [23:33<13:27,  4.49s/it]Running Inference:  64%|██████▍   | 321/500 [23:37<13:14,  4.44s/it]Running Inference:  64%|██████▍   | 322/500 [23:41<12:44,  4.29s/it]Running Inference:  65%|██████▍   | 323/500 [23:45<12:37,  4.28s/it]Running Inference:  65%|██████▍   | 324/500 [23:51<13:32,  4.62s/it]Running Inference:  65%|██████▌   | 325/500 [23:57<14:43,  5.05s/it]Running Inference:  65%|██████▌   | 326/500 [24:01<13:36,  4.70s/it]Running Inference:  65%|██████▌   | 327/500 [24:06<13:50,  4.80s/it]Running Inference:  66%|██████▌   | 328/500 [24:10<13:08,  4.58s/it]Running Inference:  66%|██████▌   | 329/500 [24:15<13:30,  4.74s/it]Running Inference:  66%|██████▌   | 330/500 [24:20<13:45,  4.86s/it]Running Inference:  66%|██████▌   | 331/500 [24:24<12:56,  4.60s/it]Running Inference:  66%|██████▋   | 332/500 [24:28<12:31,  4.47s/it]Running Inference:  67%|██████▋   | 333/500 [24:32<12:09,  4.37s/it]Running Inference:  67%|██████▋   | 334/500 [24:36<11:45,  4.25s/it]Running Inference:  67%|██████▋   | 335/500 [24:45<15:33,  5.66s/it]Running Inference:  67%|██████▋   | 336/500 [24:51<15:30,  5.67s/it]Running Inference:  67%|██████▋   | 337/500 [24:56<14:41,  5.41s/it]Running Inference:  68%|██████▊   | 338/500 [25:00<13:26,  4.98s/it]Running Inference:  68%|██████▊   | 339/500 [25:04<12:28,  4.65s/it]Running Inference:  68%|██████▊   | 340/500 [25:09<13:04,  4.91s/it]Running Inference:  68%|██████▊   | 341/500 [25:11<10:44,  4.05s/it]Running Inference:  68%|██████▊   | 342/500 [25:16<11:19,  4.30s/it]Running Inference:  69%|██████▊   | 343/500 [25:20<10:56,  4.18s/it]Running Inference:  69%|██████▉   | 344/500 [25:24<10:55,  4.20s/it]Running Inference:  69%|██████▉   | 345/500 [25:29<11:28,  4.44s/it]Running Inference:  69%|██████▉   | 346/500 [25:33<11:03,  4.31s/it]Running Inference:  69%|██████▉   | 347/500 [25:37<10:43,  4.21s/it]Running Inference:  70%|██████▉   | 348/500 [25:41<10:25,  4.12s/it]Running Inference:  70%|██████▉   | 349/500 [25:46<11:10,  4.44s/it]Running Inference:  70%|███████   | 350/500 [25:51<11:40,  4.67s/it]Running Inference:  70%|███████   | 351/500 [25:57<12:00,  4.84s/it]Running Inference:  70%|███████   | 352/500 [26:02<12:13,  4.96s/it]Running Inference:  71%|███████   | 353/500 [26:09<14:03,  5.74s/it]Running Inference:  71%|███████   | 354/500 [26:15<13:54,  5.72s/it]Running Inference:  71%|███████   | 355/500 [26:21<13:43,  5.68s/it]Running Inference:  71%|███████   | 356/500 [26:25<12:20,  5.14s/it]Running Inference:  71%|███████▏  | 357/500 [26:29<11:48,  4.96s/it]Running Inference:  72%|███████▏  | 358/500 [26:33<11:05,  4.69s/it]Running Inference:  72%|███████▏  | 359/500 [26:38<11:22,  4.84s/it]Running Inference:  72%|███████▏  | 360/500 [26:40<09:07,  3.91s/it]Running Inference:  72%|███████▏  | 361/500 [26:44<09:16,  4.00s/it]Running Inference:  72%|███████▏  | 362/500 [26:49<09:34,  4.16s/it]Running Inference:  73%|███████▎  | 363/500 [26:54<10:20,  4.53s/it]Running Inference:  73%|███████▎  | 364/500 [26:59<10:11,  4.49s/it]Running Inference:  73%|███████▎  | 365/500 [27:03<10:02,  4.46s/it]Running Inference:  73%|███████▎  | 366/500 [27:07<09:32,  4.27s/it]Running Inference:  73%|███████▎  | 367/500 [27:11<09:11,  4.15s/it]Running Inference:  74%|███████▎  | 368/500 [27:15<09:00,  4.10s/it]Running Inference:  74%|███████▍  | 369/500 [27:19<08:53,  4.08s/it]Running Inference:  74%|███████▍  | 370/500 [27:23<08:55,  4.12s/it]Running Inference:  74%|███████▍  | 371/500 [27:28<09:23,  4.37s/it]Running Inference:  74%|███████▍  | 372/500 [27:33<09:40,  4.53s/it]Running Inference:  75%|███████▍  | 373/500 [27:37<09:24,  4.45s/it]Running Inference:  75%|███████▍  | 374/500 [27:42<09:23,  4.47s/it]Running Inference:  75%|███████▌  | 375/500 [27:47<10:06,  4.85s/it]Running Inference:  75%|███████▌  | 376/500 [27:52<09:38,  4.66s/it]Running Inference:  75%|███████▌  | 377/500 [27:56<09:07,  4.45s/it]Running Inference:  76%|███████▌  | 378/500 [28:06<12:24,  6.10s/it]Running Inference:  76%|███████▌  | 379/500 [28:10<11:24,  5.66s/it]Running Inference:  76%|███████▌  | 380/500 [28:14<10:12,  5.11s/it]Running Inference:  76%|███████▌  | 381/500 [28:15<07:46,  3.92s/it]Running Inference:  76%|███████▋  | 382/500 [28:20<08:22,  4.26s/it]Running Inference:  77%|███████▋  | 383/500 [28:24<08:04,  4.14s/it]Running Inference:  77%|███████▋  | 384/500 [28:28<08:00,  4.14s/it]Running Inference:  77%|███████▋  | 385/500 [28:32<07:50,  4.09s/it]Running Inference:  77%|███████▋  | 386/500 [28:37<07:59,  4.21s/it]Running Inference:  77%|███████▋  | 387/500 [28:41<08:13,  4.37s/it]Running Inference:  78%|███████▊  | 388/500 [28:47<08:49,  4.73s/it]Running Inference:  78%|███████▊  | 389/500 [28:51<08:36,  4.65s/it]Running Inference:  78%|███████▊  | 390/500 [29:00<10:42,  5.84s/it]Running Inference:  78%|███████▊  | 391/500 [29:07<11:11,  6.16s/it]Running Inference:  78%|███████▊  | 392/500 [29:11<09:59,  5.55s/it]Running Inference:  79%|███████▊  | 393/500 [29:16<09:37,  5.39s/it]Running Inference:  79%|███████▉  | 394/500 [29:21<09:20,  5.29s/it]Running Inference:  79%|███████▉  | 395/500 [29:25<08:46,  5.01s/it]Running Inference:  79%|███████▉  | 396/500 [29:31<09:00,  5.20s/it]Running Inference:  79%|███████▉  | 397/500 [29:37<09:06,  5.30s/it]Running Inference:  80%|███████▉  | 398/500 [29:41<08:33,  5.04s/it]Running Inference:  80%|███████▉  | 399/500 [29:45<08:07,  4.83s/it]Running Inference:  80%|████████  | 400/500 [29:49<07:34,  4.54s/it]Running Inference:  80%|████████  | 401/500 [29:53<07:09,  4.34s/it]Running Inference:  80%|████████  | 402/500 [29:54<05:32,  3.39s/it]Running Inference:  81%|████████  | 403/500 [29:58<05:43,  3.54s/it]Running Inference:  81%|████████  | 404/500 [30:03<06:05,  3.81s/it]Running Inference:  81%|████████  | 405/500 [30:09<07:20,  4.64s/it]Running Inference:  81%|████████  | 406/500 [30:16<08:13,  5.25s/it]Running Inference:  81%|████████▏ | 407/500 [30:21<07:56,  5.12s/it]Running Inference:  82%|████████▏ | 408/500 [30:25<07:26,  4.85s/it]Running Inference:  82%|████████▏ | 409/500 [30:30<07:36,  5.02s/it]Running Inference:  82%|████████▏ | 410/500 [30:35<07:18,  4.87s/it]Running Inference:  82%|████████▏ | 411/500 [30:39<06:53,  4.65s/it]Running Inference:  82%|████████▏ | 412/500 [30:44<06:56,  4.73s/it]Running Inference:  83%|████████▎ | 413/500 [30:49<07:03,  4.87s/it]Running Inference:  83%|████████▎ | 414/500 [30:54<06:48,  4.75s/it]Running Inference:  83%|████████▎ | 415/500 [30:59<06:54,  4.88s/it]Running Inference:  83%|████████▎ | 416/500 [31:03<06:30,  4.65s/it]Running Inference:  83%|████████▎ | 417/500 [31:07<06:07,  4.43s/it]Running Inference:  84%|████████▎ | 418/500 [31:11<05:50,  4.28s/it]Running Inference:  84%|████████▍ | 419/500 [31:15<05:41,  4.22s/it]Running Inference:  84%|████████▍ | 420/500 [31:19<05:34,  4.19s/it]Running Inference:  84%|████████▍ | 421/500 [31:23<05:35,  4.24s/it]Running Inference:  84%|████████▍ | 422/500 [31:28<05:34,  4.29s/it]Running Inference:  85%|████████▍ | 423/500 [31:34<06:19,  4.92s/it]Running Inference:  85%|████████▍ | 424/500 [31:39<06:11,  4.89s/it]Running Inference:  85%|████████▌ | 425/500 [31:44<06:05,  4.87s/it]Running Inference:  85%|████████▌ | 426/500 [31:48<05:50,  4.73s/it]Running Inference:  85%|████████▌ | 427/500 [31:54<06:16,  5.16s/it]Running Inference:  86%|████████▌ | 428/500 [31:59<05:55,  4.93s/it]Running Inference:  86%|████████▌ | 429/500 [32:03<05:45,  4.86s/it]Running Inference:  86%|████████▌ | 430/500 [32:08<05:33,  4.77s/it]Running Inference:  86%|████████▌ | 431/500 [32:12<05:10,  4.50s/it]Running Inference:  86%|████████▋ | 432/500 [32:18<05:37,  4.97s/it]Running Inference:  87%|████████▋ | 433/500 [32:24<05:55,  5.30s/it]Running Inference:  87%|████████▋ | 434/500 [32:25<04:20,  3.95s/it]Running Inference:  87%|████████▋ | 435/500 [32:29<04:15,  3.93s/it]Running Inference:  87%|████████▋ | 436/500 [32:33<04:10,  3.91s/it]Running Inference:  87%|████████▋ | 437/500 [32:37<04:13,  4.03s/it]Running Inference:  88%|████████▊ | 438/500 [32:41<04:20,  4.20s/it]Running Inference:  88%|████████▊ | 439/500 [32:49<05:12,  5.12s/it]Running Inference:  88%|████████▊ | 440/500 [32:52<04:41,  4.69s/it]Running Inference:  88%|████████▊ | 441/500 [32:57<04:42,  4.80s/it]Running Inference:  88%|████████▊ | 442/500 [33:02<04:36,  4.76s/it]Running Inference:  89%|████████▊ | 443/500 [33:07<04:38,  4.89s/it]Running Inference:  89%|████████▉ | 444/500 [33:12<04:25,  4.75s/it]Running Inference:  89%|████████▉ | 445/500 [33:17<04:26,  4.84s/it]Running Inference:  89%|████████▉ | 446/500 [33:24<04:52,  5.42s/it]Running Inference:  89%|████████▉ | 447/500 [33:29<04:47,  5.42s/it]Running Inference:  90%|████████▉ | 448/500 [33:34<04:38,  5.36s/it]Running Inference:  90%|████████▉ | 449/500 [33:37<03:53,  4.57s/it]Running Inference:  90%|█████████ | 450/500 [33:41<03:41,  4.43s/it]Running Inference:  90%|█████████ | 451/500 [33:46<03:44,  4.59s/it]Running Inference:  90%|█████████ | 452/500 [33:51<03:50,  4.80s/it]Running Inference:  91%|█████████ | 453/500 [33:56<03:48,  4.85s/it]Running Inference:  91%|█████████ | 454/500 [34:01<03:36,  4.71s/it]Running Inference:  91%|█████████ | 455/500 [34:06<03:38,  4.87s/it]Running Inference:  91%|█████████ | 456/500 [34:10<03:27,  4.72s/it]Running Inference:  91%|█████████▏| 457/500 [34:14<03:13,  4.50s/it]Running Inference:  92%|█████████▏| 458/500 [34:20<03:20,  4.78s/it]Running Inference:  92%|█████████▏| 459/500 [34:25<03:23,  4.96s/it]Running Inference:  92%|█████████▏| 460/500 [34:31<03:25,  5.13s/it]Running Inference:  92%|█████████▏| 461/500 [34:35<03:14,  4.99s/it]Running Inference:  92%|█████████▏| 462/500 [34:40<03:12,  5.06s/it]Running Inference:  93%|█████████▎| 463/500 [34:45<03:01,  4.90s/it]Running Inference:  93%|█████████▎| 464/500 [34:50<02:53,  4.81s/it]Running Inference:  93%|█████████▎| 465/500 [34:54<02:39,  4.57s/it]Running Inference:  93%|█████████▎| 466/500 [34:58<02:32,  4.49s/it]Running Inference:  93%|█████████▎| 467/500 [35:02<02:25,  4.39s/it]Running Inference:  94%|█████████▎| 468/500 [35:06<02:16,  4.26s/it]Running Inference:  94%|█████████▍| 469/500 [35:10<02:08,  4.14s/it]Running Inference:  94%|█████████▍| 470/500 [35:12<01:42,  3.43s/it]Running Inference:  94%|█████████▍| 471/500 [35:16<01:45,  3.63s/it]Running Inference:  94%|█████████▍| 472/500 [35:20<01:45,  3.77s/it]Running Inference:  95%|█████████▍| 473/500 [35:25<01:52,  4.18s/it]Running Inference:  95%|█████████▍| 474/500 [35:30<01:56,  4.46s/it]Running Inference:  95%|█████████▌| 475/500 [35:35<01:55,  4.62s/it]Running Inference:  95%|█████████▌| 476/500 [35:39<01:45,  4.41s/it]Running Inference:  95%|█████████▌| 477/500 [35:43<01:37,  4.25s/it]Running Inference:  96%|█████████▌| 478/500 [35:49<01:44,  4.76s/it]Running Inference:  96%|█████████▌| 479/500 [35:56<01:56,  5.54s/it]Running Inference:  96%|█████████▌| 480/500 [35:57<01:23,  4.20s/it]Running Inference:  96%|█████████▌| 481/500 [36:02<01:24,  4.44s/it]Running Inference:  96%|█████████▋| 482/500 [36:07<01:20,  4.45s/it]Running Inference:  97%|█████████▋| 483/500 [36:11<01:15,  4.41s/it]Running Inference:  97%|█████████▋| 484/500 [36:12<00:54,  3.40s/it]Running Inference:  97%|█████████▋| 485/500 [36:17<00:56,  3.79s/it]Running Inference:  97%|█████████▋| 486/500 [36:21<00:54,  3.93s/it]Running Inference:  97%|█████████▋| 487/500 [36:26<00:53,  4.14s/it]Running Inference:  98%|█████████▊| 488/500 [36:30<00:49,  4.11s/it]Running Inference:  98%|█████████▊| 489/500 [36:35<00:48,  4.43s/it]Running Inference:  98%|█████████▊| 490/500 [36:40<00:45,  4.51s/it]Running Inference:  98%|█████████▊| 491/500 [36:44<00:41,  4.61s/it]Running Inference:  98%|█████████▊| 492/500 [36:50<00:39,  4.93s/it]Running Inference:  99%|█████████▊| 493/500 [36:54<00:32,  4.70s/it]Running Inference:  99%|█████████▉| 494/500 [36:59<00:27,  4.63s/it]Running Inference:  99%|█████████▉| 495/500 [37:03<00:22,  4.57s/it]Running Inference:  99%|█████████▉| 496/500 [37:08<00:18,  4.62s/it]Running Inference:  99%|█████████▉| 497/500 [37:13<00:13,  4.64s/it]Running Inference: 100%|█████████▉| 498/500 [37:17<00:09,  4.56s/it]Running Inference: 100%|█████████▉| 499/500 [37:21<00:04,  4.48s/it]Running Inference: 100%|██████████| 500/500 [37:25<00:00,  4.38s/it]Running Inference: 100%|██████████| 500/500 [37:25<00:00,  4.49s/it]
2025-12-15 00:32:27,627 - INFO - Inference completed.
2025-12-15 00:32:27,679 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-15 00:32:27,679 - INFO - Calculating metrics for dataset: longbench
2025-12-15 00:32:27,681 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-15 00:32:27,681 - INFO - Metrics:
14.73
2025-12-15 00:32:27,682 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=repobench-p, ratio=0.2) on GPU 2

----------------------------------------
Task: repobench-p | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 00:32:34,188 - INFO - Set deterministic seeds to 42
2025-12-15 00:32:34,188 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 00:32:34,188 - INFO - Starting evaluation run...
2025-12-15 00:32:34,188 - INFO - Output directory set to: longbenchresult
2025-12-15 00:32:34,189 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-15 00:32:34,189 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 00:32:34,189 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 00:32:34,189 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.76it/s]
Device set to use cuda:0
2025-12-15 00:32:57,872 - INFO - Model pipeline loaded.
2025-12-15 00:32:57,872 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
2025-12-15 00:33:05,627 - INFO - Dataset loaded with 500 entries.
2025-12-15 00:33:05,628 - INFO - Dataset processed with 500 entries.
2025-12-15 00:33:05,671 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<43:04,  5.18s/it]Running Inference:   0%|          | 2/500 [00:11<47:38,  5.74s/it]Running Inference:   1%|          | 3/500 [00:17<49:00,  5.92s/it]Running Inference:   1%|          | 4/500 [00:21<43:09,  5.22s/it]Running Inference:   1%|          | 5/500 [00:26<41:07,  4.98s/it]Running Inference:   1%|          | 6/500 [00:30<38:55,  4.73s/it]Running Inference:   1%|▏         | 7/500 [00:31<30:22,  3.70s/it]Running Inference:   2%|▏         | 8/500 [00:36<31:12,  3.81s/it]Running Inference:   2%|▏         | 9/500 [00:40<31:50,  3.89s/it]Running Inference:   2%|▏         | 10/500 [00:47<40:38,  4.98s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:53<43:30,  5.34s/it]Running Inference:   2%|▏         | 12/500 [00:58<41:10,  5.06s/it]Running Inference:   3%|▎         | 13/500 [01:03<41:46,  5.15s/it]Running Inference:   3%|▎         | 14/500 [01:13<53:15,  6.58s/it]Running Inference:   3%|▎         | 15/500 [01:18<48:48,  6.04s/it]Running Inference:   3%|▎         | 16/500 [01:22<44:33,  5.52s/it]Running Inference:   3%|▎         | 17/500 [01:27<44:23,  5.51s/it]Running Inference:   4%|▎         | 18/500 [01:33<44:49,  5.58s/it]Running Inference:   4%|▍         | 19/500 [01:37<41:31,  5.18s/it]Running Inference:   4%|▍         | 20/500 [01:44<45:38,  5.71s/it]Running Inference:   4%|▍         | 21/500 [01:48<41:40,  5.22s/it]Running Inference:   4%|▍         | 22/500 [01:54<42:38,  5.35s/it]Running Inference:   5%|▍         | 23/500 [02:00<43:50,  5.51s/it]Running Inference:   5%|▍         | 24/500 [02:06<44:49,  5.65s/it]Running Inference:   5%|▌         | 25/500 [02:09<39:20,  4.97s/it]Running Inference:   5%|▌         | 26/500 [02:14<38:43,  4.90s/it]Running Inference:   5%|▌         | 27/500 [02:18<36:46,  4.67s/it]Running Inference:   6%|▌         | 28/500 [02:22<34:57,  4.44s/it]Running Inference:   6%|▌         | 29/500 [02:26<34:00,  4.33s/it]Running Inference:   6%|▌         | 30/500 [02:31<36:07,  4.61s/it]Running Inference:   6%|▌         | 31/500 [02:37<37:31,  4.80s/it]Running Inference:   6%|▋         | 32/500 [02:41<36:20,  4.66s/it]Running Inference:   7%|▋         | 33/500 [02:45<34:57,  4.49s/it]Running Inference:   7%|▋         | 34/500 [02:49<34:35,  4.45s/it]Running Inference:   7%|▋         | 35/500 [02:54<34:32,  4.46s/it]Running Inference:   7%|▋         | 36/500 [02:59<35:19,  4.57s/it]Running Inference:   7%|▋         | 37/500 [03:03<35:36,  4.61s/it]Running Inference:   8%|▊         | 38/500 [03:08<35:18,  4.59s/it]Running Inference:   8%|▊         | 39/500 [03:12<34:15,  4.46s/it]Running Inference:   8%|▊         | 40/500 [03:16<33:29,  4.37s/it]Running Inference:   8%|▊         | 41/500 [03:20<32:52,  4.30s/it]Running Inference:   8%|▊         | 42/500 [03:24<31:50,  4.17s/it]Running Inference:   9%|▊         | 43/500 [03:25<24:03,  3.16s/it]Running Inference:   9%|▉         | 44/500 [03:31<30:29,  4.01s/it]Running Inference:   9%|▉         | 45/500 [03:36<33:19,  4.39s/it]Running Inference:   9%|▉         | 46/500 [03:42<35:21,  4.67s/it]Running Inference:   9%|▉         | 47/500 [03:47<36:45,  4.87s/it]Running Inference:  10%|▉         | 48/500 [03:51<35:34,  4.72s/it]Running Inference:  10%|▉         | 49/500 [03:56<35:12,  4.68s/it]Running Inference:  10%|█         | 50/500 [04:01<35:47,  4.77s/it]Running Inference:  10%|█         | 51/500 [04:06<36:15,  4.85s/it]Running Inference:  10%|█         | 52/500 [04:11<35:35,  4.77s/it]Running Inference:  11%|█         | 53/500 [04:15<34:05,  4.58s/it]Running Inference:  11%|█         | 54/500 [04:19<33:10,  4.46s/it]Running Inference:  11%|█         | 55/500 [04:25<37:06,  5.00s/it]Running Inference:  11%|█         | 56/500 [04:30<36:08,  4.88s/it]Running Inference:  11%|█▏        | 57/500 [04:34<33:52,  4.59s/it]Running Inference:  12%|█▏        | 58/500 [04:38<32:40,  4.44s/it]Running Inference:  12%|█▏        | 59/500 [04:43<33:12,  4.52s/it]Running Inference:  12%|█▏        | 60/500 [04:47<32:16,  4.40s/it]Running Inference:  12%|█▏        | 61/500 [04:51<31:16,  4.27s/it]Running Inference:  12%|█▏        | 62/500 [04:56<34:33,  4.73s/it]Running Inference:  13%|█▎        | 63/500 [05:02<36:47,  5.05s/it]Running Inference:  13%|█▎        | 64/500 [05:07<35:26,  4.88s/it]Running Inference:  13%|█▎        | 65/500 [05:13<37:51,  5.22s/it]Running Inference:  13%|█▎        | 66/500 [05:17<34:44,  4.80s/it]Running Inference:  13%|█▎        | 67/500 [05:20<32:35,  4.52s/it]Running Inference:  14%|█▎        | 68/500 [05:26<34:29,  4.79s/it]Running Inference:  14%|█▍        | 69/500 [05:31<35:26,  4.93s/it]Running Inference:  14%|█▍        | 70/500 [05:39<42:32,  5.94s/it]Running Inference:  14%|█▍        | 71/500 [05:44<40:20,  5.64s/it]Running Inference:  14%|█▍        | 72/500 [05:49<38:33,  5.41s/it]Running Inference:  15%|█▍        | 73/500 [05:55<38:34,  5.42s/it]Running Inference:  15%|█▍        | 74/500 [06:00<37:48,  5.32s/it]Running Inference:  15%|█▌        | 75/500 [06:05<36:49,  5.20s/it]Running Inference:  15%|█▌        | 76/500 [06:10<36:33,  5.17s/it]Running Inference:  15%|█▌        | 77/500 [06:14<35:02,  4.97s/it]Running Inference:  16%|█▌        | 78/500 [06:21<38:07,  5.42s/it]Running Inference:  16%|█▌        | 79/500 [06:25<35:01,  4.99s/it]Running Inference:  16%|█▌        | 80/500 [06:29<33:45,  4.82s/it]Running Inference:  16%|█▌        | 81/500 [06:35<35:00,  5.01s/it]Running Inference:  16%|█▋        | 82/500 [06:40<35:28,  5.09s/it]Running Inference:  17%|█▋        | 83/500 [06:48<41:08,  5.92s/it]Running Inference:  17%|█▋        | 84/500 [06:53<38:48,  5.60s/it]Running Inference:  17%|█▋        | 85/500 [06:57<37:16,  5.39s/it]Running Inference:  17%|█▋        | 86/500 [07:02<36:01,  5.22s/it]Running Inference:  17%|█▋        | 87/500 [07:04<27:50,  4.04s/it]Running Inference:  18%|█▊        | 88/500 [07:08<28:03,  4.09s/it]Running Inference:  18%|█▊        | 89/500 [07:12<28:14,  4.12s/it]Running Inference:  18%|█▊        | 90/500 [07:16<28:22,  4.15s/it]Running Inference:  18%|█▊        | 91/500 [07:18<22:36,  3.32s/it]Running Inference:  18%|█▊        | 92/500 [07:22<25:22,  3.73s/it]Running Inference:  19%|█▊        | 93/500 [07:27<27:59,  4.13s/it]Running Inference:  19%|█▉        | 94/500 [07:29<22:21,  3.30s/it]Running Inference:  19%|█▉        | 95/500 [07:36<30:34,  4.53s/it]Running Inference:  19%|█▉        | 96/500 [07:39<26:34,  3.95s/it]Running Inference:  19%|█▉        | 97/500 [07:43<28:08,  4.19s/it]Running Inference:  20%|█▉        | 98/500 [07:45<23:06,  3.45s/it]Running Inference:  20%|█▉        | 99/500 [07:47<19:48,  2.96s/it]Running Inference:  20%|██        | 100/500 [07:52<23:13,  3.48s/it]Running Inference:  20%|██        | 101/500 [07:56<24:20,  3.66s/it]Running Inference:  20%|██        | 102/500 [08:00<25:44,  3.88s/it]Running Inference:  21%|██        | 103/500 [08:04<26:12,  3.96s/it]Running Inference:  21%|██        | 104/500 [08:09<28:29,  4.32s/it]Running Inference:  21%|██        | 105/500 [08:11<22:26,  3.41s/it]Running Inference:  21%|██        | 106/500 [08:15<23:55,  3.64s/it]Running Inference:  21%|██▏       | 107/500 [08:19<24:31,  3.75s/it]Running Inference:  22%|██▏       | 108/500 [08:23<26:08,  4.00s/it]Running Inference:  22%|██▏       | 109/500 [08:29<29:57,  4.60s/it]Running Inference:  22%|██▏       | 110/500 [08:36<32:39,  5.02s/it]Running Inference:  22%|██▏       | 111/500 [08:40<32:19,  4.99s/it]Running Inference:  22%|██▏       | 112/500 [08:45<31:06,  4.81s/it]Running Inference:  23%|██▎       | 113/500 [08:49<29:43,  4.61s/it]Running Inference:  23%|██▎       | 114/500 [08:53<29:10,  4.54s/it]Running Inference:  23%|██▎       | 115/500 [08:58<29:17,  4.56s/it]Running Inference:  23%|██▎       | 116/500 [09:02<28:36,  4.47s/it]Running Inference:  23%|██▎       | 117/500 [09:07<29:00,  4.54s/it]Running Inference:  24%|██▎       | 118/500 [09:12<29:46,  4.68s/it]Running Inference:  24%|██▍       | 119/500 [09:16<29:21,  4.62s/it]Running Inference:  24%|██▍       | 120/500 [09:21<28:19,  4.47s/it]Running Inference:  24%|██▍       | 121/500 [09:23<24:54,  3.94s/it]Running Inference:  24%|██▍       | 122/500 [09:27<24:40,  3.92s/it]Running Inference:  25%|██▍       | 123/500 [09:31<24:51,  3.96s/it]Running Inference:  25%|██▍       | 124/500 [09:35<24:59,  3.99s/it]Running Inference:  25%|██▌       | 125/500 [09:40<26:01,  4.16s/it]Running Inference:  25%|██▌       | 126/500 [09:44<26:48,  4.30s/it]Running Inference:  25%|██▌       | 127/500 [09:48<26:09,  4.21s/it]Running Inference:  26%|██▌       | 128/500 [09:52<25:40,  4.14s/it]Running Inference:  26%|██▌       | 129/500 [10:00<31:21,  5.07s/it]Running Inference:  26%|██▌       | 130/500 [10:07<34:50,  5.65s/it]Running Inference:  26%|██▌       | 131/500 [10:11<32:39,  5.31s/it]Running Inference:  26%|██▋       | 132/500 [10:13<25:51,  4.22s/it]Running Inference:  27%|██▋       | 133/500 [10:17<25:45,  4.21s/it]Running Inference:  27%|██▋       | 134/500 [10:22<26:52,  4.41s/it]Running Inference:  27%|██▋       | 135/500 [10:31<34:43,  5.71s/it]Running Inference:  27%|██▋       | 136/500 [10:37<35:46,  5.90s/it]Running Inference:  27%|██▋       | 137/500 [10:42<34:42,  5.74s/it]Running Inference:  28%|██▊       | 138/500 [10:49<35:43,  5.92s/it]Running Inference:  28%|██▊       | 139/500 [10:55<36:04,  6.00s/it]Running Inference:  28%|██▊       | 140/500 [11:00<34:32,  5.76s/it]Running Inference:  28%|██▊       | 141/500 [11:04<31:30,  5.26s/it]Running Inference:  28%|██▊       | 142/500 [11:08<28:58,  4.86s/it]Running Inference:  29%|██▊       | 143/500 [11:17<35:53,  6.03s/it]Running Inference:  29%|██▉       | 144/500 [11:21<32:12,  5.43s/it]Running Inference:  29%|██▉       | 145/500 [11:25<30:39,  5.18s/it]Running Inference:  29%|██▉       | 146/500 [11:29<28:29,  4.83s/it]Running Inference:  29%|██▉       | 147/500 [11:30<21:13,  3.61s/it]Running Inference:  30%|██▉       | 148/500 [11:36<24:20,  4.15s/it]Running Inference:  30%|██▉       | 149/500 [11:41<26:42,  4.56s/it]Running Inference:  30%|███       | 150/500 [11:47<28:12,  4.84s/it]Running Inference:  30%|███       | 151/500 [11:51<26:59,  4.64s/it]Running Inference:  30%|███       | 152/500 [11:55<25:47,  4.45s/it]Running Inference:  31%|███       | 153/500 [11:59<25:33,  4.42s/it]Running Inference:  31%|███       | 154/500 [12:05<27:20,  4.74s/it]Running Inference:  31%|███       | 155/500 [12:09<26:58,  4.69s/it]Running Inference:  31%|███       | 156/500 [12:15<28:26,  4.96s/it]Running Inference:  31%|███▏      | 157/500 [12:19<26:50,  4.70s/it]Running Inference:  32%|███▏      | 158/500 [12:22<24:10,  4.24s/it]Running Inference:  32%|███▏      | 159/500 [12:27<25:46,  4.53s/it]Running Inference:  32%|███▏      | 160/500 [12:32<25:55,  4.57s/it]Running Inference:  32%|███▏      | 161/500 [12:37<26:45,  4.74s/it]Running Inference:  32%|███▏      | 162/500 [12:42<26:37,  4.73s/it]Running Inference:  33%|███▎      | 163/500 [12:46<26:05,  4.65s/it]Running Inference:  33%|███▎      | 164/500 [12:49<22:37,  4.04s/it]Running Inference:  33%|███▎      | 165/500 [12:54<24:49,  4.45s/it]Running Inference:  33%|███▎      | 166/500 [12:56<20:35,  3.70s/it]Running Inference:  33%|███▎      | 167/500 [13:00<21:15,  3.83s/it]Running Inference:  34%|███▎      | 168/500 [13:06<24:25,  4.42s/it]Running Inference:  34%|███▍      | 169/500 [13:09<21:32,  3.91s/it]Running Inference:  34%|███▍      | 170/500 [13:13<21:52,  3.98s/it]Running Inference:  34%|███▍      | 171/500 [13:17<22:05,  4.03s/it]Running Inference:  34%|███▍      | 172/500 [13:21<22:09,  4.05s/it]Running Inference:  35%|███▍      | 173/500 [13:25<22:10,  4.07s/it]Running Inference:  35%|███▍      | 174/500 [13:29<22:06,  4.07s/it]Running Inference:  35%|███▌      | 175/500 [13:35<23:49,  4.40s/it]Running Inference:  35%|███▌      | 176/500 [13:40<25:03,  4.64s/it]Running Inference:  35%|███▌      | 177/500 [13:45<25:52,  4.81s/it]Running Inference:  36%|███▌      | 178/500 [13:51<27:13,  5.07s/it]Running Inference:  36%|███▌      | 179/500 [13:55<25:38,  4.79s/it]Running Inference:  36%|███▌      | 180/500 [13:59<24:48,  4.65s/it]Running Inference:  36%|███▌      | 181/500 [14:01<20:42,  3.89s/it]Running Inference:  36%|███▋      | 182/500 [14:06<21:22,  4.03s/it]Running Inference:  37%|███▋      | 183/500 [14:10<21:48,  4.13s/it]Running Inference:  37%|███▋      | 184/500 [14:14<21:34,  4.10s/it]Running Inference:  37%|███▋      | 185/500 [14:18<21:35,  4.11s/it]Running Inference:  37%|███▋      | 186/500 [14:23<23:11,  4.43s/it]Running Inference:  37%|███▋      | 187/500 [14:28<23:12,  4.45s/it]Running Inference:  38%|███▊      | 188/500 [14:32<22:44,  4.37s/it]Running Inference:  38%|███▊      | 189/500 [14:37<23:11,  4.47s/it]Running Inference:  38%|███▊      | 190/500 [14:42<24:35,  4.76s/it]Running Inference:  38%|███▊      | 191/500 [14:47<23:57,  4.65s/it]Running Inference:  38%|███▊      | 192/500 [14:51<23:55,  4.66s/it]Running Inference:  39%|███▊      | 193/500 [14:56<23:33,  4.60s/it]Running Inference:  39%|███▉      | 194/500 [14:57<18:44,  3.68s/it]Running Inference:  39%|███▉      | 195/500 [15:02<20:34,  4.05s/it]Running Inference:  39%|███▉      | 196/500 [15:06<20:35,  4.07s/it]Running Inference:  39%|███▉      | 197/500 [15:10<20:35,  4.08s/it]Running Inference:  40%|███▉      | 198/500 [15:14<20:37,  4.10s/it]Running Inference:  40%|███▉      | 199/500 [15:19<20:31,  4.09s/it]Running Inference:  40%|████      | 200/500 [15:23<20:33,  4.11s/it]Running Inference:  40%|████      | 201/500 [15:27<20:21,  4.09s/it]Running Inference:  40%|████      | 202/500 [15:33<23:06,  4.65s/it]Running Inference:  41%|████      | 203/500 [15:37<22:49,  4.61s/it]Running Inference:  41%|████      | 204/500 [15:42<22:29,  4.56s/it]Running Inference:  41%|████      | 205/500 [15:46<22:13,  4.52s/it]Running Inference:  41%|████      | 206/500 [15:51<22:48,  4.65s/it]Running Inference:  41%|████▏     | 207/500 [15:55<22:24,  4.59s/it]Running Inference:  42%|████▏     | 208/500 [15:59<21:25,  4.40s/it]Running Inference:  42%|████▏     | 209/500 [16:03<20:44,  4.28s/it]Running Inference:  42%|████▏     | 210/500 [16:05<16:40,  3.45s/it]Running Inference:  42%|████▏     | 211/500 [16:10<19:21,  4.02s/it]Running Inference:  42%|████▏     | 212/500 [16:15<19:44,  4.11s/it]Running Inference:  43%|████▎     | 213/500 [16:19<20:39,  4.32s/it]Running Inference:  43%|████▎     | 214/500 [16:23<19:58,  4.19s/it]Running Inference:  43%|████▎     | 215/500 [16:25<16:32,  3.48s/it]Running Inference:  43%|████▎     | 216/500 [16:29<17:16,  3.65s/it]Running Inference:  43%|████▎     | 217/500 [16:33<17:54,  3.80s/it]Running Inference:  44%|████▎     | 218/500 [16:37<18:19,  3.90s/it]Running Inference:  44%|████▍     | 219/500 [16:42<18:33,  3.96s/it]Running Inference:  44%|████▍     | 220/500 [16:46<18:41,  4.00s/it]Running Inference:  44%|████▍     | 221/500 [16:47<14:40,  3.15s/it]Running Inference:  44%|████▍     | 222/500 [16:51<16:00,  3.46s/it]Running Inference:  45%|████▍     | 223/500 [16:56<17:29,  3.79s/it]Running Inference:  45%|████▍     | 224/500 [17:00<18:44,  4.07s/it]Running Inference:  45%|████▌     | 225/500 [17:05<19:10,  4.18s/it]Running Inference:  45%|████▌     | 226/500 [17:11<21:28,  4.70s/it]Running Inference:  45%|████▌     | 227/500 [17:17<23:11,  5.10s/it]Running Inference:  46%|████▌     | 228/500 [17:20<20:48,  4.59s/it]Running Inference:  46%|████▌     | 229/500 [17:26<22:55,  5.07s/it]Running Inference:  46%|████▌     | 230/500 [17:32<23:39,  5.26s/it]Running Inference:  46%|████▌     | 231/500 [17:34<19:46,  4.41s/it]Running Inference:  46%|████▋     | 232/500 [17:37<17:48,  3.99s/it]Running Inference:  47%|████▋     | 233/500 [17:41<17:36,  3.96s/it]Running Inference:  47%|████▋     | 234/500 [17:45<17:27,  3.94s/it]Running Inference:  47%|████▋     | 235/500 [17:50<18:50,  4.27s/it]Running Inference:  47%|████▋     | 236/500 [17:55<19:45,  4.49s/it]Running Inference:  47%|████▋     | 237/500 [17:59<19:16,  4.40s/it]Running Inference:  48%|████▊     | 238/500 [18:05<20:45,  4.75s/it]Running Inference:  48%|████▊     | 239/500 [18:09<19:52,  4.57s/it]Running Inference:  48%|████▊     | 240/500 [18:14<20:23,  4.71s/it]Running Inference:  48%|████▊     | 241/500 [18:19<20:31,  4.75s/it]Running Inference:  48%|████▊     | 242/500 [18:25<21:27,  4.99s/it]Running Inference:  49%|████▊     | 243/500 [18:30<22:05,  5.16s/it]Running Inference:  49%|████▉     | 244/500 [18:33<18:50,  4.42s/it]Running Inference:  49%|████▉     | 245/500 [18:38<20:12,  4.75s/it]Running Inference:  49%|████▉     | 246/500 [18:40<16:03,  3.79s/it]Running Inference:  49%|████▉     | 247/500 [18:44<16:34,  3.93s/it]Running Inference:  50%|████▉     | 248/500 [18:49<17:40,  4.21s/it]Running Inference:  50%|████▉     | 249/500 [18:53<17:54,  4.28s/it]Running Inference:  50%|█████     | 250/500 [18:57<17:26,  4.19s/it]Running Inference:  50%|█████     | 251/500 [19:03<19:28,  4.69s/it]Running Inference:  50%|█████     | 252/500 [19:09<20:15,  4.90s/it]Running Inference:  51%|█████     | 253/500 [19:12<17:47,  4.32s/it]Running Inference:  51%|█████     | 254/500 [19:16<18:04,  4.41s/it]Running Inference:  51%|█████     | 255/500 [19:21<18:13,  4.46s/it]Running Inference:  51%|█████     | 256/500 [19:26<19:32,  4.81s/it]Running Inference:  51%|█████▏    | 257/500 [19:31<19:02,  4.70s/it]Running Inference:  52%|█████▏    | 258/500 [19:36<19:22,  4.80s/it]Running Inference:  52%|█████▏    | 259/500 [19:39<17:14,  4.29s/it]Running Inference:  52%|█████▏    | 260/500 [19:43<16:48,  4.20s/it]Running Inference:  52%|█████▏    | 261/500 [19:49<19:14,  4.83s/it]Running Inference:  52%|█████▏    | 262/500 [19:51<15:38,  3.94s/it]Running Inference:  53%|█████▎    | 263/500 [19:52<12:21,  3.13s/it]Running Inference:  53%|█████▎    | 264/500 [19:57<14:12,  3.61s/it]Running Inference:  53%|█████▎    | 265/500 [20:02<15:12,  3.88s/it]Running Inference:  53%|█████▎    | 266/500 [20:06<15:55,  4.08s/it]Running Inference:  53%|█████▎    | 267/500 [20:14<19:38,  5.06s/it]Running Inference:  54%|█████▎    | 268/500 [20:18<18:32,  4.80s/it]Running Inference:  54%|█████▍    | 269/500 [20:26<21:57,  5.70s/it]Running Inference:  54%|█████▍    | 270/500 [20:34<24:49,  6.48s/it]Running Inference:  54%|█████▍    | 271/500 [20:40<24:35,  6.44s/it]Running Inference:  54%|█████▍    | 272/500 [20:45<22:04,  5.81s/it]Running Inference:  55%|█████▍    | 273/500 [20:47<18:39,  4.93s/it]Running Inference:  55%|█████▍    | 274/500 [20:51<16:31,  4.39s/it]Running Inference:  55%|█████▌    | 275/500 [20:55<16:15,  4.34s/it]Running Inference:  55%|█████▌    | 276/500 [20:58<15:16,  4.09s/it]Running Inference:  55%|█████▌    | 277/500 [21:03<16:00,  4.31s/it]Running Inference:  56%|█████▌    | 278/500 [21:09<17:28,  4.72s/it]Running Inference:  56%|█████▌    | 279/500 [21:15<18:33,  5.04s/it]Running Inference:  56%|█████▌    | 280/500 [21:20<18:52,  5.15s/it]Running Inference:  56%|█████▌    | 281/500 [21:24<17:50,  4.89s/it]Running Inference:  56%|█████▋    | 282/500 [21:30<18:17,  5.03s/it]Running Inference:  57%|█████▋    | 283/500 [21:35<18:15,  5.05s/it]Running Inference:  57%|█████▋    | 284/500 [21:40<18:12,  5.06s/it]Running Inference:  57%|█████▋    | 285/500 [21:44<17:14,  4.81s/it]Running Inference:  57%|█████▋    | 286/500 [21:48<16:34,  4.65s/it]Running Inference:  57%|█████▋    | 287/500 [21:55<18:35,  5.24s/it]Running Inference:  58%|█████▊    | 288/500 [21:59<17:08,  4.85s/it]Running Inference:  58%|█████▊    | 289/500 [22:03<16:06,  4.58s/it]Running Inference:  58%|█████▊    | 290/500 [22:08<16:17,  4.66s/it]Running Inference:  58%|█████▊    | 291/500 [22:13<17:20,  4.98s/it]Running Inference:  58%|█████▊    | 292/500 [22:18<16:35,  4.78s/it]Running Inference:  59%|█████▊    | 293/500 [22:22<15:40,  4.54s/it]Running Inference:  59%|█████▉    | 294/500 [22:24<13:06,  3.82s/it]Running Inference:  59%|█████▉    | 295/500 [22:29<14:08,  4.14s/it]Running Inference:  59%|█████▉    | 296/500 [22:31<12:20,  3.63s/it]Running Inference:  59%|█████▉    | 297/500 [22:35<12:59,  3.84s/it]Running Inference:  60%|█████▉    | 298/500 [22:40<13:20,  3.96s/it]Running Inference:  60%|█████▉    | 299/500 [22:44<13:38,  4.07s/it]Running Inference:  60%|██████    | 300/500 [22:49<14:55,  4.48s/it]Running Inference:  60%|██████    | 301/500 [22:51<11:57,  3.61s/it]Running Inference:  60%|██████    | 302/500 [22:55<12:19,  3.74s/it]Running Inference:  61%|██████    | 303/500 [22:59<12:49,  3.91s/it]Running Inference:  61%|██████    | 304/500 [23:02<11:41,  3.58s/it]Running Inference:  61%|██████    | 305/500 [23:06<12:01,  3.70s/it]Running Inference:  61%|██████    | 306/500 [23:11<12:54,  3.99s/it]Running Inference:  61%|██████▏   | 307/500 [23:15<12:56,  4.02s/it]Running Inference:  62%|██████▏   | 308/500 [23:19<12:51,  4.02s/it]Running Inference:  62%|██████▏   | 309/500 [23:23<12:53,  4.05s/it]Running Inference:  62%|██████▏   | 310/500 [23:28<13:20,  4.21s/it]Running Inference:  62%|██████▏   | 311/500 [23:32<13:04,  4.15s/it]Running Inference:  62%|██████▏   | 312/500 [23:36<13:01,  4.16s/it]Running Inference:  63%|██████▎   | 313/500 [23:41<13:33,  4.35s/it]Running Inference:  63%|██████▎   | 314/500 [23:45<13:14,  4.27s/it]Running Inference:  63%|██████▎   | 315/500 [23:50<13:54,  4.51s/it]Running Inference:  63%|██████▎   | 316/500 [23:52<11:29,  3.75s/it]Running Inference:  63%|██████▎   | 317/500 [23:57<12:45,  4.18s/it]Running Inference:  64%|██████▎   | 318/500 [24:02<13:38,  4.50s/it]Running Inference:  64%|██████▍   | 319/500 [24:07<14:12,  4.71s/it]Running Inference:  64%|██████▍   | 320/500 [24:11<13:32,  4.51s/it]Running Inference:  64%|██████▍   | 321/500 [24:16<13:19,  4.47s/it]Running Inference:  64%|██████▍   | 322/500 [24:20<12:49,  4.32s/it]Running Inference:  65%|██████▍   | 323/500 [24:24<12:42,  4.31s/it]Running Inference:  65%|██████▍   | 324/500 [24:30<13:37,  4.65s/it]Running Inference:  65%|██████▌   | 325/500 [24:36<14:45,  5.06s/it]Running Inference:  65%|██████▌   | 326/500 [24:39<13:39,  4.71s/it]Running Inference:  65%|██████▌   | 327/500 [24:45<13:53,  4.82s/it]Running Inference:  66%|██████▌   | 328/500 [24:49<13:11,  4.60s/it]Running Inference:  66%|██████▌   | 329/500 [24:54<13:35,  4.77s/it]Running Inference:  66%|██████▌   | 330/500 [24:56<11:28,  4.05s/it]Running Inference:  66%|██████▌   | 331/500 [25:00<11:23,  4.04s/it]Running Inference:  66%|██████▋   | 332/500 [25:04<11:26,  4.09s/it]Running Inference:  67%|██████▋   | 333/500 [25:09<11:26,  4.11s/it]Running Inference:  67%|██████▋   | 334/500 [25:13<11:16,  4.07s/it]Running Inference:  67%|██████▋   | 335/500 [25:21<15:09,  5.51s/it]Running Inference:  67%|██████▋   | 336/500 [25:28<16:08,  5.90s/it]Running Inference:  67%|██████▋   | 337/500 [25:33<15:08,  5.58s/it]Running Inference:  68%|██████▊   | 338/500 [25:37<13:47,  5.11s/it]Running Inference:  68%|██████▊   | 339/500 [25:41<12:43,  4.74s/it]Running Inference:  68%|██████▊   | 340/500 [25:46<13:16,  4.98s/it]Running Inference:  68%|██████▊   | 341/500 [25:48<10:46,  4.06s/it]Running Inference:  68%|██████▊   | 342/500 [25:53<11:21,  4.31s/it]Running Inference:  69%|██████▊   | 343/500 [25:57<10:59,  4.20s/it]Running Inference:  69%|██████▉   | 344/500 [26:02<10:59,  4.23s/it]Running Inference:  69%|██████▉   | 345/500 [26:07<11:32,  4.47s/it]Running Inference:  69%|██████▉   | 346/500 [26:11<11:07,  4.34s/it]Running Inference:  69%|██████▉   | 347/500 [26:15<10:48,  4.24s/it]Running Inference:  70%|██████▉   | 348/500 [26:18<10:30,  4.15s/it]Running Inference:  70%|██████▉   | 349/500 [26:24<11:14,  4.47s/it]Running Inference:  70%|███████   | 350/500 [26:29<11:43,  4.69s/it]Running Inference:  70%|███████   | 351/500 [26:34<12:02,  4.85s/it]Running Inference:  70%|███████   | 352/500 [26:39<12:15,  4.97s/it]Running Inference:  71%|███████   | 353/500 [26:47<14:02,  5.73s/it]Running Inference:  71%|███████   | 354/500 [26:52<13:35,  5.59s/it]Running Inference:  71%|███████   | 355/500 [26:58<13:22,  5.54s/it]Running Inference:  71%|███████   | 356/500 [27:02<12:08,  5.06s/it]Running Inference:  71%|███████▏  | 357/500 [27:06<11:41,  4.90s/it]Running Inference:  72%|███████▏  | 358/500 [27:10<11:02,  4.66s/it]Running Inference:  72%|███████▏  | 359/500 [27:15<11:19,  4.82s/it]Running Inference:  72%|███████▏  | 360/500 [27:17<09:06,  3.90s/it]Running Inference:  72%|███████▏  | 361/500 [27:21<09:16,  4.00s/it]Running Inference:  72%|███████▏  | 362/500 [27:26<09:35,  4.17s/it]Running Inference:  73%|███████▎  | 363/500 [27:31<10:21,  4.53s/it]Running Inference:  73%|███████▎  | 364/500 [27:36<10:12,  4.50s/it]Running Inference:  73%|███████▎  | 365/500 [27:40<10:05,  4.48s/it]Running Inference:  73%|███████▎  | 366/500 [27:44<09:35,  4.29s/it]Running Inference:  73%|███████▎  | 367/500 [27:48<09:15,  4.17s/it]Running Inference:  74%|███████▎  | 368/500 [27:52<09:04,  4.12s/it]Running Inference:  74%|███████▍  | 369/500 [27:56<08:57,  4.10s/it]Running Inference:  74%|███████▍  | 370/500 [28:00<08:58,  4.14s/it]Running Inference:  74%|███████▍  | 371/500 [28:05<09:26,  4.39s/it]Running Inference:  74%|███████▍  | 372/500 [28:10<09:43,  4.56s/it]Running Inference:  75%|███████▍  | 373/500 [28:14<09:27,  4.47s/it]Running Inference:  75%|███████▍  | 374/500 [28:19<09:26,  4.50s/it]Running Inference:  75%|███████▌  | 375/500 [28:25<10:08,  4.87s/it]Running Inference:  75%|███████▌  | 376/500 [28:29<09:40,  4.68s/it]Running Inference:  75%|███████▌  | 377/500 [28:33<09:10,  4.47s/it]Running Inference:  76%|███████▌  | 378/500 [28:40<10:51,  5.34s/it]Running Inference:  76%|███████▌  | 379/500 [28:45<10:21,  5.13s/it]Running Inference:  76%|███████▌  | 380/500 [28:49<09:29,  4.75s/it]Running Inference:  76%|███████▌  | 381/500 [28:53<08:56,  4.51s/it]Running Inference:  76%|███████▋  | 382/500 [28:58<09:12,  4.68s/it]Running Inference:  77%|███████▋  | 383/500 [29:02<08:40,  4.45s/it]Running Inference:  77%|███████▋  | 384/500 [29:06<08:26,  4.37s/it]Running Inference:  77%|███████▋  | 385/500 [29:10<08:09,  4.26s/it]Running Inference:  77%|███████▋  | 386/500 [29:14<08:13,  4.33s/it]Running Inference:  77%|███████▋  | 387/500 [29:19<08:24,  4.46s/it]Running Inference:  78%|███████▊  | 388/500 [29:25<08:57,  4.80s/it]Running Inference:  78%|███████▊  | 389/500 [29:32<10:24,  5.62s/it]Running Inference:  78%|███████▊  | 390/500 [29:41<11:53,  6.49s/it]Running Inference:  78%|███████▊  | 391/500 [29:48<11:59,  6.61s/it]Running Inference:  78%|███████▊  | 392/500 [29:52<10:33,  5.87s/it]Running Inference:  79%|███████▊  | 393/500 [29:57<10:02,  5.63s/it]Running Inference:  79%|███████▉  | 394/500 [30:02<09:38,  5.46s/it]Running Inference:  79%|███████▉  | 395/500 [30:06<08:59,  5.14s/it]Running Inference:  79%|███████▉  | 396/500 [30:12<09:13,  5.32s/it]Running Inference:  79%|███████▉  | 397/500 [30:18<09:15,  5.40s/it]Running Inference:  80%|███████▉  | 398/500 [30:22<08:43,  5.13s/it]Running Inference:  80%|███████▉  | 399/500 [30:27<08:16,  4.91s/it]Running Inference:  80%|████████  | 400/500 [30:31<07:41,  4.61s/it]Running Inference:  80%|████████  | 401/500 [30:34<07:15,  4.40s/it]Running Inference:  80%|████████  | 402/500 [30:36<05:38,  3.46s/it]Running Inference:  81%|████████  | 403/500 [30:40<05:51,  3.63s/it]Running Inference:  81%|████████  | 404/500 [30:44<06:11,  3.87s/it]Running Inference:  81%|████████  | 405/500 [30:51<07:24,  4.68s/it]Running Inference:  81%|████████  | 406/500 [30:57<08:14,  5.26s/it]Running Inference:  81%|████████▏ | 407/500 [31:02<07:57,  5.14s/it]Running Inference:  82%|████████▏ | 408/500 [31:06<07:27,  4.87s/it]Running Inference:  82%|████████▏ | 409/500 [31:12<07:38,  5.04s/it]Running Inference:  82%|████████▏ | 410/500 [31:16<07:20,  4.90s/it]Running Inference:  82%|████████▏ | 411/500 [31:21<06:56,  4.68s/it]Running Inference:  82%|████████▏ | 412/500 [31:26<06:58,  4.76s/it]Running Inference:  83%|████████▎ | 413/500 [31:31<07:05,  4.89s/it]Running Inference:  83%|████████▎ | 414/500 [31:35<06:50,  4.77s/it]Running Inference:  83%|████████▎ | 415/500 [31:40<06:56,  4.90s/it]Running Inference:  83%|████████▎ | 416/500 [31:45<06:32,  4.67s/it]Running Inference:  83%|████████▎ | 417/500 [31:49<06:09,  4.45s/it]Running Inference:  84%|████████▎ | 418/500 [31:52<05:52,  4.30s/it]Running Inference:  84%|████████▍ | 419/500 [31:57<05:43,  4.24s/it]Running Inference:  84%|████████▍ | 420/500 [32:01<05:38,  4.23s/it]Running Inference:  84%|████████▍ | 421/500 [32:05<05:38,  4.29s/it]Running Inference:  84%|████████▍ | 422/500 [32:10<05:38,  4.33s/it]Running Inference:  85%|████████▍ | 423/500 [32:16<06:20,  4.95s/it]Running Inference:  85%|████████▍ | 424/500 [32:21<06:13,  4.92s/it]Running Inference:  85%|████████▌ | 425/500 [32:26<06:07,  4.89s/it]Running Inference:  85%|████████▌ | 426/500 [32:30<05:51,  4.75s/it]Running Inference:  85%|████████▌ | 427/500 [32:36<06:16,  5.16s/it]Running Inference:  86%|████████▌ | 428/500 [32:41<05:55,  4.94s/it]Running Inference:  86%|████████▌ | 429/500 [32:45<05:46,  4.88s/it]Running Inference:  86%|████████▌ | 430/500 [32:50<05:35,  4.79s/it]Running Inference:  86%|████████▌ | 431/500 [32:54<05:12,  4.52s/it]Running Inference:  86%|████████▋ | 432/500 [33:00<05:38,  4.98s/it]Running Inference:  87%|████████▋ | 433/500 [33:06<05:55,  5.30s/it]Running Inference:  87%|████████▋ | 434/500 [33:07<04:19,  3.93s/it]Running Inference:  87%|████████▋ | 435/500 [33:11<04:14,  3.92s/it]Running Inference:  87%|████████▋ | 436/500 [33:14<04:10,  3.91s/it]Running Inference:  87%|████████▋ | 437/500 [33:19<04:14,  4.04s/it]Running Inference:  88%|████████▊ | 438/500 [33:23<04:21,  4.22s/it]Running Inference:  88%|████████▊ | 439/500 [33:31<05:13,  5.13s/it]Running Inference:  88%|████████▊ | 440/500 [33:35<05:00,  5.01s/it]Running Inference:  88%|████████▊ | 441/500 [33:41<04:56,  5.03s/it]Running Inference:  88%|████████▊ | 442/500 [33:45<04:46,  4.94s/it]Running Inference:  89%|████████▊ | 443/500 [33:51<04:46,  5.03s/it]Running Inference:  89%|████████▉ | 444/500 [33:55<04:31,  4.85s/it]Running Inference:  89%|████████▉ | 445/500 [34:00<04:30,  4.92s/it]Running Inference:  89%|████████▉ | 446/500 [34:07<04:54,  5.46s/it]Running Inference:  89%|████████▉ | 447/500 [34:12<04:49,  5.45s/it]Running Inference:  90%|████████▉ | 448/500 [34:17<04:41,  5.40s/it]Running Inference:  90%|████████▉ | 449/500 [34:23<04:36,  5.43s/it]Running Inference:  90%|█████████ | 450/500 [34:27<04:12,  5.05s/it]Running Inference:  90%|█████████ | 451/500 [34:32<04:06,  5.03s/it]Running Inference:  90%|█████████ | 452/500 [34:37<04:05,  5.12s/it]Running Inference:  91%|█████████ | 453/500 [34:42<03:59,  5.09s/it]Running Inference:  91%|█████████ | 454/500 [34:47<03:44,  4.89s/it]Running Inference:  91%|█████████ | 455/500 [34:52<03:45,  5.00s/it]Running Inference:  91%|█████████ | 456/500 [34:57<03:32,  4.83s/it]Running Inference:  91%|█████████▏| 457/500 [35:00<03:09,  4.40s/it]Running Inference:  92%|█████████▏| 458/500 [35:05<03:18,  4.72s/it]Running Inference:  92%|█████████▏| 459/500 [35:11<03:22,  4.93s/it]Running Inference:  92%|█████████▏| 460/500 [35:16<03:24,  5.11s/it]Running Inference:  92%|█████████▏| 461/500 [35:21<03:14,  4.98s/it]Running Inference:  92%|█████████▏| 462/500 [35:26<03:12,  5.06s/it]Running Inference:  93%|█████████▎| 463/500 [35:31<03:01,  4.92s/it]Running Inference:  93%|█████████▎| 464/500 [35:36<02:54,  4.83s/it]Running Inference:  93%|█████████▎| 465/500 [35:40<02:43,  4.67s/it]Running Inference:  93%|█████████▎| 466/500 [35:44<02:35,  4.58s/it]Running Inference:  93%|█████████▎| 467/500 [35:48<02:27,  4.47s/it]Running Inference:  94%|█████████▎| 468/500 [35:52<02:18,  4.32s/it]Running Inference:  94%|█████████▍| 469/500 [35:56<02:10,  4.20s/it]Running Inference:  94%|█████████▍| 470/500 [36:00<02:04,  4.16s/it]Running Inference:  94%|█████████▍| 471/500 [36:05<02:00,  4.15s/it]Running Inference:  94%|█████████▍| 472/500 [36:09<01:56,  4.15s/it]Running Inference:  95%|█████████▍| 473/500 [36:14<02:00,  4.46s/it]Running Inference:  95%|█████████▍| 474/500 [36:19<02:01,  4.67s/it]Running Inference:  95%|█████████▌| 475/500 [36:24<01:59,  4.78s/it]Running Inference:  95%|█████████▌| 476/500 [36:28<01:48,  4.54s/it]Running Inference:  95%|█████████▌| 477/500 [36:32<01:40,  4.36s/it]Running Inference:  96%|█████████▌| 478/500 [36:38<01:46,  4.83s/it]Running Inference:  96%|█████████▌| 479/500 [36:45<01:57,  5.58s/it]Running Inference:  96%|█████████▌| 480/500 [36:49<01:42,  5.12s/it]Running Inference:  96%|█████████▌| 481/500 [36:54<01:36,  5.10s/it]Running Inference:  96%|█████████▋| 482/500 [36:59<01:28,  4.92s/it]Running Inference:  97%|█████████▋| 483/500 [37:03<01:20,  4.76s/it]Running Inference:  97%|█████████▋| 484/500 [37:04<00:58,  3.66s/it]Running Inference:  97%|█████████▋| 485/500 [37:09<00:59,  3.99s/it]Running Inference:  97%|█████████▋| 486/500 [37:13<00:57,  4.08s/it]Running Inference:  97%|█████████▋| 487/500 [37:18<00:55,  4.26s/it]Running Inference:  98%|█████████▊| 488/500 [37:22<00:50,  4.21s/it]Running Inference:  98%|█████████▊| 489/500 [37:27<00:49,  4.51s/it]Running Inference:  98%|█████████▊| 490/500 [37:32<00:45,  4.58s/it]Running Inference:  98%|█████████▊| 491/500 [37:37<00:42,  4.67s/it]Running Inference:  98%|█████████▊| 492/500 [37:43<00:39,  4.99s/it]Running Inference:  99%|█████████▊| 493/500 [37:47<00:33,  4.76s/it]Running Inference:  99%|█████████▉| 494/500 [37:51<00:28,  4.68s/it]Running Inference:  99%|█████████▉| 495/500 [37:56<00:23,  4.62s/it]Running Inference:  99%|█████████▉| 496/500 [38:01<00:18,  4.66s/it]Running Inference:  99%|█████████▉| 497/500 [38:05<00:14,  4.68s/it]Running Inference: 100%|█████████▉| 498/500 [38:10<00:09,  4.60s/it]Running Inference: 100%|█████████▉| 499/500 [38:14<00:04,  4.52s/it]Running Inference: 100%|██████████| 500/500 [38:18<00:00,  4.42s/it]Running Inference: 100%|██████████| 500/500 [38:18<00:00,  4.60s/it]
2025-12-15 01:11:24,519 - INFO - Inference completed.
2025-12-15 01:11:24,571 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-15 01:11:24,572 - INFO - Calculating metrics for dataset: longbench
2025-12-15 01:11:24,573 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-15 01:11:24,573 - INFO - Metrics:
14.18
2025-12-15 01:11:24,575 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=repobench-p, ratio=0.3) on GPU 2

----------------------------------------
Task: repobench-p | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 01:11:31,030 - INFO - Set deterministic seeds to 42
2025-12-15 01:11:31,030 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "repobench-p",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 01:11:31,030 - INFO - Starting evaluation run...
2025-12-15 01:11:31,030 - INFO - Output directory set to: longbenchresult
2025-12-15 01:11:31,030 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-15 01:11:31,030 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 01:11:31,030 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 01:11:31,030 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.42it/s]
Device set to use cuda:0
2025-12-15 01:11:42,413 - INFO - Model pipeline loaded.
2025-12-15 01:11:42,413 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: repobench-p)
2025-12-15 01:11:46,638 - INFO - Dataset loaded with 500 entries.
2025-12-15 01:11:46,638 - INFO - Dataset processed with 500 entries.
2025-12-15 01:11:46,684 - INFO - Starting inference...
Running Inference:   0%|          | 0/500 [00:00<?, ?it/s]Running Inference:   0%|          | 1/500 [00:05<42:38,  5.13s/it]Running Inference:   0%|          | 2/500 [00:11<47:16,  5.70s/it]Running Inference:   1%|          | 3/500 [00:17<48:34,  5.87s/it]Running Inference:   1%|          | 4/500 [00:21<42:41,  5.16s/it]Running Inference:   1%|          | 5/500 [00:25<40:40,  4.93s/it]Running Inference:   1%|          | 6/500 [00:30<38:31,  4.68s/it]Running Inference:   1%|▏         | 7/500 [00:31<30:04,  3.66s/it]Running Inference:   2%|▏         | 8/500 [00:35<30:53,  3.77s/it]Running Inference:   2%|▏         | 9/500 [00:39<31:31,  3.85s/it]Running Inference:   2%|▏         | 10/500 [00:46<39:55,  4.89s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   2%|▏         | 11/500 [00:52<42:49,  5.25s/it]Running Inference:   2%|▏         | 12/500 [00:57<40:43,  5.01s/it]Running Inference:   3%|▎         | 13/500 [01:02<41:22,  5.10s/it]Running Inference:   3%|▎         | 14/500 [01:12<52:21,  6.46s/it]Running Inference:   3%|▎         | 15/500 [01:17<48:01,  5.94s/it]Running Inference:   3%|▎         | 16/500 [01:21<43:53,  5.44s/it]Running Inference:   3%|▎         | 17/500 [01:26<43:45,  5.44s/it]Running Inference:   4%|▎         | 18/500 [01:32<44:15,  5.51s/it]Running Inference:   4%|▍         | 19/500 [01:36<41:01,  5.12s/it]Running Inference:   4%|▍         | 20/500 [01:43<44:55,  5.62s/it]Running Inference:   4%|▍         | 21/500 [01:47<41:04,  5.14s/it]Running Inference:   4%|▍         | 22/500 [01:53<42:05,  5.28s/it]Running Inference:   5%|▍         | 23/500 [01:58<43:18,  5.45s/it]Running Inference:   5%|▍         | 24/500 [02:04<44:18,  5.58s/it]Running Inference:   5%|▌         | 25/500 [02:10<44:49,  5.66s/it]Running Inference:   5%|▌         | 26/500 [02:15<42:25,  5.37s/it]Running Inference:   5%|▌         | 27/500 [02:19<39:14,  4.98s/it]Running Inference:   6%|▌         | 28/500 [02:23<36:35,  4.65s/it]Running Inference:   6%|▌         | 29/500 [02:27<35:02,  4.46s/it]Running Inference:   6%|▌         | 30/500 [02:32<36:42,  4.69s/it]Running Inference:   6%|▌         | 31/500 [02:37<37:47,  4.84s/it]Running Inference:   6%|▋         | 32/500 [02:41<36:23,  4.67s/it]Running Inference:   7%|▋         | 33/500 [02:46<34:54,  4.48s/it]Running Inference:   7%|▋         | 34/500 [02:50<34:25,  4.43s/it]Running Inference:   7%|▋         | 35/500 [02:54<34:18,  4.43s/it]Running Inference:   7%|▋         | 36/500 [02:59<35:01,  4.53s/it]Running Inference:   7%|▋         | 37/500 [03:04<35:17,  4.57s/it]Running Inference:   8%|▊         | 38/500 [03:08<34:58,  4.54s/it]Running Inference:   8%|▊         | 39/500 [03:12<33:54,  4.41s/it]Running Inference:   8%|▊         | 40/500 [03:16<33:09,  4.32s/it]Running Inference:   8%|▊         | 41/500 [03:18<25:58,  3.40s/it]Running Inference:   8%|▊         | 42/500 [03:21<26:56,  3.53s/it]Running Inference:   9%|▊         | 43/500 [03:22<20:37,  2.71s/it]Running Inference:   9%|▉         | 44/500 [03:28<27:58,  3.68s/it]Running Inference:   9%|▉         | 45/500 [03:33<31:27,  4.15s/it]Running Inference:   9%|▉         | 46/500 [03:39<33:55,  4.48s/it]Running Inference:   9%|▉         | 47/500 [03:44<35:38,  4.72s/it]Running Inference:  10%|▉         | 48/500 [03:48<34:38,  4.60s/it]Running Inference:  10%|▉         | 49/500 [03:53<34:26,  4.58s/it]Running Inference:  10%|█         | 50/500 [03:58<35:13,  4.70s/it]Running Inference:  10%|█         | 51/500 [04:03<35:44,  4.78s/it]Running Inference:  10%|█         | 52/500 [04:07<35:06,  4.70s/it]Running Inference:  11%|█         | 53/500 [04:11<33:38,  4.52s/it]Running Inference:  11%|█         | 54/500 [04:16<32:45,  4.41s/it]Running Inference:  11%|█         | 55/500 [04:22<36:39,  4.94s/it]Running Inference:  11%|█         | 56/500 [04:26<35:44,  4.83s/it]Running Inference:  11%|█▏        | 57/500 [04:30<33:29,  4.54s/it]Running Inference:  12%|█▏        | 58/500 [04:34<32:19,  4.39s/it]Running Inference:  12%|█▏        | 59/500 [04:39<32:51,  4.47s/it]Running Inference:  12%|█▏        | 60/500 [04:43<31:56,  4.36s/it]Running Inference:  12%|█▏        | 61/500 [04:47<30:56,  4.23s/it]Running Inference:  12%|█▏        | 62/500 [04:53<34:10,  4.68s/it]Running Inference:  13%|█▎        | 63/500 [04:58<36:22,  4.99s/it]Running Inference:  13%|█▎        | 64/500 [05:03<35:02,  4.82s/it]Running Inference:  13%|█▎        | 65/500 [05:09<37:26,  5.16s/it]Running Inference:  13%|█▎        | 66/500 [05:13<34:22,  4.75s/it]Running Inference:  13%|█▎        | 67/500 [05:16<32:15,  4.47s/it]Running Inference:  14%|█▎        | 68/500 [05:22<34:08,  4.74s/it]Running Inference:  14%|█▍        | 69/500 [05:27<35:05,  4.89s/it]Running Inference:  14%|█▍        | 70/500 [05:35<41:52,  5.84s/it]Running Inference:  14%|█▍        | 71/500 [05:40<39:45,  5.56s/it]Running Inference:  14%|█▍        | 72/500 [05:45<38:02,  5.33s/it]Running Inference:  15%|█▍        | 73/500 [05:50<38:03,  5.35s/it]Running Inference:  15%|█▍        | 74/500 [05:55<37:18,  5.26s/it]Running Inference:  15%|█▌        | 75/500 [06:00<36:22,  5.14s/it]Running Inference:  15%|█▌        | 76/500 [06:05<36:08,  5.11s/it]Running Inference:  15%|█▌        | 77/500 [06:10<34:37,  4.91s/it]Running Inference:  16%|█▌        | 78/500 [06:16<37:38,  5.35s/it]Running Inference:  16%|█▌        | 79/500 [06:20<34:35,  4.93s/it]Running Inference:  16%|█▌        | 80/500 [06:24<33:20,  4.76s/it]Running Inference:  16%|█▌        | 81/500 [06:30<34:38,  4.96s/it]Running Inference:  16%|█▋        | 82/500 [06:35<35:07,  5.04s/it]Running Inference:  17%|█▋        | 83/500 [06:42<40:26,  5.82s/it]Running Inference:  17%|█▋        | 84/500 [06:47<38:12,  5.51s/it]Running Inference:  17%|█▋        | 85/500 [06:52<36:43,  5.31s/it]Running Inference:  17%|█▋        | 86/500 [06:57<35:32,  5.15s/it]Running Inference:  17%|█▋        | 87/500 [07:01<33:33,  4.88s/it]Running Inference:  18%|█▊        | 88/500 [07:05<31:54,  4.65s/it]Running Inference:  18%|█▊        | 89/500 [07:09<30:48,  4.50s/it]Running Inference:  18%|█▊        | 90/500 [07:13<29:32,  4.32s/it]Running Inference:  18%|█▊        | 91/500 [07:15<23:17,  3.42s/it]Running Inference:  18%|█▊        | 92/500 [07:19<25:44,  3.78s/it]Running Inference:  19%|█▊        | 93/500 [07:24<28:08,  4.15s/it]Running Inference:  19%|█▉        | 94/500 [07:26<22:26,  3.32s/it]Running Inference:  19%|█▉        | 95/500 [07:33<30:14,  4.48s/it]Running Inference:  19%|█▉        | 96/500 [07:38<30:42,  4.56s/it]Running Inference:  19%|█▉        | 97/500 [07:40<25:19,  3.77s/it]Running Inference:  20%|█▉        | 98/500 [07:44<26:10,  3.91s/it]Running Inference:  20%|█▉        | 99/500 [07:46<22:15,  3.33s/it]Running Inference:  20%|██        | 100/500 [07:50<24:49,  3.72s/it]Running Inference:  20%|██        | 101/500 [07:54<25:20,  3.81s/it]Running Inference:  20%|██        | 102/500 [07:59<26:20,  3.97s/it]Running Inference:  21%|██        | 103/500 [08:03<26:31,  4.01s/it]Running Inference:  21%|██        | 104/500 [08:08<28:37,  4.34s/it]Running Inference:  21%|██        | 105/500 [08:09<22:30,  3.42s/it]Running Inference:  21%|██        | 106/500 [08:13<23:53,  3.64s/it]Running Inference:  21%|██▏       | 107/500 [08:17<24:25,  3.73s/it]Running Inference:  22%|██▏       | 108/500 [08:22<25:57,  3.97s/it]Running Inference:  22%|██▏       | 109/500 [08:28<29:43,  4.56s/it]Running Inference:  22%|██▏       | 110/500 [08:34<32:22,  4.98s/it]Running Inference:  22%|██▏       | 111/500 [08:39<32:03,  4.94s/it]Running Inference:  22%|██▏       | 112/500 [08:43<30:48,  4.77s/it]Running Inference:  23%|██▎       | 113/500 [08:47<29:25,  4.56s/it]Running Inference:  23%|██▎       | 114/500 [08:51<28:52,  4.49s/it]Running Inference:  23%|██▎       | 115/500 [08:56<28:59,  4.52s/it]Running Inference:  23%|██▎       | 116/500 [09:00<28:17,  4.42s/it]Running Inference:  23%|██▎       | 117/500 [09:05<28:40,  4.49s/it]Running Inference:  24%|██▎       | 118/500 [09:10<29:27,  4.63s/it]Running Inference:  24%|██▍       | 119/500 [09:14<29:01,  4.57s/it]Running Inference:  24%|██▍       | 120/500 [09:18<27:59,  4.42s/it]Running Inference:  24%|██▍       | 121/500 [09:22<26:45,  4.24s/it]Running Inference:  24%|██▍       | 122/500 [09:26<25:52,  4.11s/it]Running Inference:  25%|██▍       | 123/500 [09:30<25:37,  4.08s/it]Running Inference:  25%|██▍       | 124/500 [09:34<25:26,  4.06s/it]Running Inference:  25%|██▌       | 125/500 [09:38<26:09,  4.18s/it]Running Inference:  25%|██▌       | 126/500 [09:43<26:44,  4.29s/it]Running Inference:  25%|██▌       | 127/500 [09:47<25:59,  4.18s/it]Running Inference:  26%|██▌       | 128/500 [09:51<25:25,  4.10s/it]Running Inference:  26%|██▌       | 129/500 [09:58<30:49,  4.99s/it]Running Inference:  26%|██▌       | 130/500 [10:05<34:09,  5.54s/it]Running Inference:  26%|██▌       | 131/500 [10:08<29:44,  4.84s/it]Running Inference:  26%|██▋       | 132/500 [10:12<28:09,  4.59s/it]Running Inference:  27%|██▋       | 133/500 [10:16<27:18,  4.46s/it]Running Inference:  27%|██▋       | 134/500 [10:21<27:51,  4.57s/it]Running Inference:  27%|██▋       | 135/500 [10:29<35:03,  5.76s/it]Running Inference:  27%|██▋       | 136/500 [10:36<35:50,  5.91s/it]Running Inference:  27%|██▋       | 137/500 [10:41<34:36,  5.72s/it]Running Inference:  28%|██▊       | 138/500 [10:47<35:28,  5.88s/it]Running Inference:  28%|██▊       | 139/500 [10:53<35:45,  5.94s/it]Running Inference:  28%|██▊       | 140/500 [10:57<31:51,  5.31s/it]Running Inference:  28%|██▊       | 141/500 [11:01<29:30,  4.93s/it]Running Inference:  28%|██▊       | 142/500 [11:02<22:18,  3.74s/it]Running Inference:  29%|██▊       | 143/500 [11:11<30:48,  5.18s/it]Running Inference:  29%|██▉       | 144/500 [11:15<28:33,  4.81s/it]Running Inference:  29%|██▉       | 145/500 [11:19<28:00,  4.73s/it]Running Inference:  29%|██▉       | 146/500 [11:23<26:32,  4.50s/it]Running Inference:  29%|██▉       | 147/500 [11:24<19:51,  3.37s/it]Running Inference:  30%|██▉       | 148/500 [11:29<23:14,  3.96s/it]Running Inference:  30%|██▉       | 149/500 [11:35<25:49,  4.41s/it]Running Inference:  30%|███       | 150/500 [11:40<27:27,  4.71s/it]Running Inference:  30%|███       | 151/500 [11:44<26:22,  4.53s/it]Running Inference:  30%|███       | 152/500 [11:48<25:16,  4.36s/it]Running Inference:  31%|███       | 153/500 [11:52<25:03,  4.33s/it]Running Inference:  31%|███       | 154/500 [11:58<26:53,  4.66s/it]Running Inference:  31%|███       | 155/500 [12:02<26:31,  4.61s/it]Running Inference:  31%|███       | 156/500 [12:08<28:00,  4.88s/it]Running Inference:  31%|███▏      | 157/500 [12:12<26:26,  4.63s/it]Running Inference:  32%|███▏      | 158/500 [12:15<24:07,  4.23s/it]Running Inference:  32%|███▏      | 159/500 [12:20<25:36,  4.51s/it]Running Inference:  32%|███▏      | 160/500 [12:25<25:42,  4.54s/it]Running Inference:  32%|███▏      | 161/500 [12:30<26:28,  4.69s/it]Running Inference:  32%|███▏      | 162/500 [12:35<26:19,  4.67s/it]Running Inference:  33%|███▎      | 163/500 [12:39<25:47,  4.59s/it]Running Inference:  33%|███▎      | 164/500 [12:42<22:29,  4.02s/it]Running Inference:  33%|███▎      | 165/500 [12:45<21:08,  3.79s/it]Running Inference:  33%|███▎      | 166/500 [12:47<17:59,  3.23s/it]Running Inference:  33%|███▎      | 167/500 [12:51<19:21,  3.49s/it]Running Inference:  34%|███▎      | 168/500 [12:57<22:59,  4.15s/it]Running Inference:  34%|███▍      | 169/500 [12:59<20:28,  3.71s/it]Running Inference:  34%|███▍      | 170/500 [13:03<21:02,  3.83s/it]Running Inference:  34%|███▍      | 171/500 [13:07<21:26,  3.91s/it]Running Inference:  34%|███▍      | 172/500 [13:12<21:36,  3.95s/it]Running Inference:  35%|███▍      | 173/500 [13:16<21:43,  3.99s/it]Running Inference:  35%|███▍      | 174/500 [13:20<21:43,  4.00s/it]Running Inference:  35%|███▌      | 175/500 [13:25<23:28,  4.33s/it]Running Inference:  35%|███▌      | 176/500 [13:30<24:42,  4.58s/it]Running Inference:  35%|███▌      | 177/500 [13:35<25:32,  4.74s/it]Running Inference:  36%|███▌      | 178/500 [13:41<26:53,  5.01s/it]Running Inference:  36%|███▌      | 179/500 [13:45<25:19,  4.73s/it]Running Inference:  36%|███▌      | 180/500 [13:49<24:29,  4.59s/it]Running Inference:  36%|███▌      | 181/500 [13:53<23:51,  4.49s/it]Running Inference:  36%|███▋      | 182/500 [13:58<23:30,  4.43s/it]Running Inference:  37%|███▋      | 183/500 [14:02<23:12,  4.39s/it]Running Inference:  37%|███▋      | 184/500 [14:06<22:28,  4.27s/it]Running Inference:  37%|███▋      | 185/500 [14:10<22:08,  4.22s/it]Running Inference:  37%|███▋      | 186/500 [14:15<23:30,  4.49s/it]Running Inference:  37%|███▋      | 187/500 [14:20<23:20,  4.48s/it]Running Inference:  38%|███▊      | 188/500 [14:24<22:46,  4.38s/it]Running Inference:  38%|███▊      | 189/500 [14:28<23:06,  4.46s/it]Running Inference:  38%|███▊      | 190/500 [14:34<24:23,  4.72s/it]Running Inference:  38%|███▊      | 191/500 [14:38<23:43,  4.61s/it]Running Inference:  38%|███▊      | 192/500 [14:43<23:39,  4.61s/it]Running Inference:  39%|███▊      | 193/500 [14:47<23:16,  4.55s/it]Running Inference:  39%|███▉      | 194/500 [14:51<23:05,  4.53s/it]Running Inference:  39%|███▉      | 195/500 [14:56<23:30,  4.62s/it]Running Inference:  39%|███▉      | 196/500 [15:00<22:33,  4.45s/it]Running Inference:  39%|███▉      | 197/500 [15:04<21:52,  4.33s/it]Running Inference:  40%|███▉      | 198/500 [15:06<18:12,  3.62s/it]Running Inference:  40%|███▉      | 199/500 [15:10<18:46,  3.74s/it]Running Inference:  40%|████      | 200/500 [15:15<19:15,  3.85s/it]Running Inference:  40%|████      | 201/500 [15:18<19:22,  3.89s/it]Running Inference:  40%|████      | 202/500 [15:24<22:19,  4.49s/it]Running Inference:  41%|████      | 203/500 [15:26<17:59,  3.63s/it]Running Inference:  41%|████      | 204/500 [15:30<19:01,  3.86s/it]Running Inference:  41%|████      | 205/500 [15:35<19:44,  4.02s/it]Running Inference:  41%|████      | 206/500 [15:40<20:59,  4.28s/it]Running Inference:  41%|████▏     | 207/500 [15:44<21:02,  4.31s/it]Running Inference:  42%|████▏     | 208/500 [15:48<20:23,  4.19s/it]Running Inference:  42%|████▏     | 209/500 [15:52<19:56,  4.11s/it]Running Inference:  42%|████▏     | 210/500 [15:53<16:05,  3.33s/it]Running Inference:  42%|████▏     | 211/500 [15:59<18:51,  3.92s/it]Running Inference:  42%|████▏     | 212/500 [16:03<19:19,  4.02s/it]Running Inference:  43%|████▎     | 213/500 [16:08<20:16,  4.24s/it]Running Inference:  43%|████▎     | 214/500 [16:12<19:38,  4.12s/it]Running Inference:  43%|████▎     | 215/500 [16:16<19:23,  4.08s/it]Running Inference:  43%|████▎     | 216/500 [16:20<19:12,  4.06s/it]Running Inference:  43%|████▎     | 217/500 [16:24<19:10,  4.07s/it]Running Inference:  44%|████▎     | 218/500 [16:28<19:07,  4.07s/it]Running Inference:  44%|████▍     | 219/500 [16:32<19:02,  4.07s/it]Running Inference:  44%|████▍     | 220/500 [16:36<18:57,  4.06s/it]Running Inference:  44%|████▍     | 221/500 [16:40<18:53,  4.06s/it]Running Inference:  44%|████▍     | 222/500 [16:44<18:52,  4.07s/it]Running Inference:  45%|████▍     | 223/500 [16:48<19:23,  4.20s/it]Running Inference:  45%|████▍     | 224/500 [16:53<19:58,  4.34s/it]Running Inference:  45%|████▌     | 225/500 [16:58<19:56,  4.35s/it]Running Inference:  45%|████▌     | 226/500 [17:03<21:54,  4.80s/it]Running Inference:  45%|████▌     | 227/500 [17:09<23:22,  5.14s/it]Running Inference:  46%|████▌     | 228/500 [17:12<19:58,  4.41s/it]Running Inference:  46%|████▌     | 229/500 [17:18<22:13,  4.92s/it]Running Inference:  46%|████▌     | 230/500 [17:24<23:05,  5.13s/it]Running Inference:  46%|████▌     | 231/500 [17:29<23:31,  5.25s/it]Running Inference:  46%|████▋     | 232/500 [17:35<24:42,  5.53s/it]Running Inference:  47%|████▋     | 233/500 [17:39<22:21,  5.02s/it]Running Inference:  47%|████▋     | 234/500 [17:43<20:42,  4.67s/it]Running Inference:  47%|████▋     | 235/500 [17:48<21:00,  4.76s/it]Running Inference:  47%|████▋     | 236/500 [17:53<21:11,  4.82s/it]Running Inference:  47%|████▋     | 237/500 [17:57<20:11,  4.61s/it]Running Inference:  48%|████▊     | 238/500 [18:03<21:18,  4.88s/it]Running Inference:  48%|████▊     | 239/500 [18:07<20:10,  4.64s/it]Running Inference:  48%|████▊     | 240/500 [18:12<20:31,  4.74s/it]Running Inference:  48%|████▊     | 241/500 [18:17<20:33,  4.76s/it]Running Inference:  48%|████▊     | 242/500 [18:22<21:23,  4.98s/it]Running Inference:  49%|████▊     | 243/500 [18:25<18:20,  4.28s/it]Running Inference:  49%|████▉     | 244/500 [18:27<16:11,  3.80s/it]Running Inference:  49%|████▉     | 245/500 [18:33<18:17,  4.30s/it]Running Inference:  49%|████▉     | 246/500 [18:34<14:41,  3.47s/it]Running Inference:  49%|████▉     | 247/500 [18:39<15:34,  3.69s/it]Running Inference:  50%|████▉     | 248/500 [18:43<16:54,  4.03s/it]Running Inference:  50%|████▉     | 249/500 [18:48<17:18,  4.14s/it]Running Inference:  50%|█████     | 250/500 [18:52<16:57,  4.07s/it]Running Inference:  50%|█████     | 251/500 [18:57<19:02,  4.59s/it]Running Inference:  50%|█████     | 252/500 [19:03<19:52,  4.81s/it]Running Inference:  51%|█████     | 253/500 [19:09<21:01,  5.11s/it]Running Inference:  51%|█████     | 254/500 [19:13<20:15,  4.94s/it]Running Inference:  51%|█████     | 255/500 [19:18<19:41,  4.82s/it]Running Inference:  51%|█████     | 256/500 [19:23<20:29,  5.04s/it]Running Inference:  51%|█████▏    | 257/500 [19:28<19:39,  4.86s/it]Running Inference:  52%|█████▏    | 258/500 [19:33<19:46,  4.90s/it]Running Inference:  52%|█████▏    | 259/500 [19:39<20:56,  5.21s/it]Running Inference:  52%|█████▏    | 260/500 [19:43<19:24,  4.85s/it]Running Inference:  52%|█████▏    | 261/500 [19:49<20:58,  5.27s/it]Running Inference:  52%|█████▏    | 262/500 [19:51<16:49,  4.24s/it]Running Inference:  53%|█████▎    | 263/500 [19:52<13:10,  3.33s/it]Running Inference:  53%|█████▎    | 264/500 [19:57<14:42,  3.74s/it]Running Inference:  53%|█████▎    | 265/500 [20:01<15:30,  3.96s/it]Running Inference:  53%|█████▎    | 266/500 [20:06<16:03,  4.12s/it]Running Inference:  53%|█████▎    | 267/500 [20:13<19:32,  5.03s/it]Running Inference:  54%|█████▎    | 268/500 [20:17<18:25,  4.76s/it]Running Inference:  54%|█████▍    | 269/500 [20:25<21:37,  5.62s/it]Running Inference:  54%|█████▍    | 270/500 [20:33<24:18,  6.34s/it]Running Inference:  54%|█████▍    | 271/500 [20:39<24:05,  6.31s/it]Running Inference:  54%|█████▍    | 272/500 [20:43<21:39,  5.70s/it]Running Inference:  55%|█████▍    | 273/500 [20:47<20:02,  5.30s/it]Running Inference:  55%|█████▍    | 274/500 [20:53<20:22,  5.41s/it]Running Inference:  55%|█████▌    | 275/500 [20:57<18:51,  5.03s/it]Running Inference:  55%|█████▌    | 276/500 [21:03<19:28,  5.22s/it]Running Inference:  55%|█████▌    | 277/500 [21:08<18:52,  5.08s/it]Running Inference:  56%|█████▌    | 278/500 [21:13<19:23,  5.24s/it]Running Inference:  56%|█████▌    | 279/500 [21:18<18:53,  5.13s/it]Running Inference:  56%|█████▌    | 280/500 [21:23<19:01,  5.19s/it]Running Inference:  56%|█████▌    | 281/500 [21:28<17:53,  4.90s/it]Running Inference:  56%|█████▋    | 282/500 [21:33<18:14,  5.02s/it]Running Inference:  57%|█████▋    | 283/500 [21:38<18:09,  5.02s/it]Running Inference:  57%|█████▋    | 284/500 [21:43<18:05,  5.02s/it]Running Inference:  57%|█████▋    | 285/500 [21:47<17:05,  4.77s/it]Running Inference:  57%|█████▋    | 286/500 [21:51<16:24,  4.60s/it]Running Inference:  57%|█████▋    | 287/500 [21:58<18:20,  5.17s/it]Running Inference:  58%|█████▊    | 288/500 [22:02<16:55,  4.79s/it]Running Inference:  58%|█████▊    | 289/500 [22:06<15:54,  4.52s/it]Running Inference:  58%|█████▊    | 290/500 [22:10<16:05,  4.60s/it]Running Inference:  58%|█████▊    | 291/500 [22:16<17:07,  4.92s/it]Running Inference:  58%|█████▊    | 292/500 [22:20<16:22,  4.72s/it]Running Inference:  59%|█████▊    | 293/500 [22:24<15:27,  4.48s/it]Running Inference:  59%|█████▉    | 294/500 [22:27<13:06,  3.82s/it]Running Inference:  59%|█████▉    | 295/500 [22:31<14:05,  4.12s/it]Running Inference:  59%|█████▉    | 296/500 [22:34<12:05,  3.56s/it]Running Inference:  59%|█████▉    | 297/500 [22:38<12:45,  3.77s/it]Running Inference:  60%|█████▉    | 298/500 [22:42<13:07,  3.90s/it]Running Inference:  60%|█████▉    | 299/500 [22:46<13:25,  4.01s/it]Running Inference:  60%|██████    | 300/500 [22:52<14:42,  4.41s/it]Running Inference:  60%|██████    | 301/500 [22:53<11:47,  3.56s/it]Running Inference:  60%|██████    | 302/500 [22:57<12:09,  3.68s/it]Running Inference:  61%|██████    | 303/500 [23:02<12:38,  3.85s/it]Running Inference:  61%|██████    | 304/500 [23:03<10:02,  3.07s/it]Running Inference:  61%|██████    | 305/500 [23:07<10:49,  3.33s/it]Running Inference:  61%|██████    | 306/500 [23:11<12:00,  3.72s/it]Running Inference:  61%|██████▏   | 307/500 [23:15<12:15,  3.81s/it]Running Inference:  62%|██████▏   | 308/500 [23:19<12:20,  3.86s/it]Running Inference:  62%|██████▏   | 309/500 [23:23<12:29,  3.92s/it]Running Inference:  62%|██████▏   | 310/500 [23:28<13:00,  4.11s/it]Running Inference:  62%|██████▏   | 311/500 [23:32<12:47,  4.06s/it]Running Inference:  62%|██████▏   | 312/500 [23:36<12:47,  4.08s/it]Running Inference:  63%|██████▎   | 313/500 [23:41<13:20,  4.28s/it]Running Inference:  63%|██████▎   | 314/500 [23:45<13:03,  4.21s/it]Running Inference:  63%|██████▎   | 315/500 [23:50<13:42,  4.45s/it]Running Inference:  63%|██████▎   | 316/500 [23:55<14:10,  4.62s/it]Running Inference:  63%|██████▎   | 317/500 [24:00<14:33,  4.78s/it]Running Inference:  64%|██████▎   | 318/500 [24:05<14:51,  4.90s/it]Running Inference:  64%|██████▍   | 319/500 [24:10<14:59,  4.97s/it]Running Inference:  64%|██████▍   | 320/500 [24:14<14:02,  4.68s/it]Running Inference:  64%|██████▍   | 321/500 [24:19<13:37,  4.57s/it]Running Inference:  64%|██████▍   | 322/500 [24:23<12:59,  4.38s/it]Running Inference:  65%|██████▍   | 323/500 [24:27<12:46,  4.33s/it]Running Inference:  65%|██████▍   | 324/500 [24:32<13:36,  4.64s/it]Running Inference:  65%|██████▌   | 325/500 [24:38<14:40,  5.03s/it]Running Inference:  65%|██████▌   | 326/500 [24:42<13:34,  4.68s/it]Running Inference:  65%|██████▌   | 327/500 [24:47<13:46,  4.78s/it]Running Inference:  66%|██████▌   | 328/500 [24:51<13:03,  4.56s/it]Running Inference:  66%|██████▌   | 329/500 [24:56<13:27,  4.72s/it]Running Inference:  66%|██████▌   | 330/500 [25:01<13:41,  4.83s/it]Running Inference:  66%|██████▌   | 331/500 [25:05<12:52,  4.57s/it]Running Inference:  66%|██████▋   | 332/500 [25:09<12:26,  4.44s/it]Running Inference:  67%|██████▋   | 333/500 [25:13<12:05,  4.34s/it]Running Inference:  67%|██████▋   | 334/500 [25:17<11:41,  4.22s/it]Running Inference:  67%|██████▋   | 335/500 [25:26<15:15,  5.55s/it]Running Inference:  67%|██████▋   | 336/500 [25:33<16:06,  5.89s/it]Running Inference:  67%|██████▋   | 337/500 [25:37<15:04,  5.55s/it]Running Inference:  68%|██████▊   | 338/500 [25:41<13:42,  5.07s/it]Running Inference:  68%|██████▊   | 339/500 [25:45<12:37,  4.71s/it]Running Inference:  68%|██████▊   | 340/500 [25:51<13:08,  4.93s/it]Running Inference:  68%|██████▊   | 341/500 [25:53<10:39,  4.02s/it]Running Inference:  68%|██████▊   | 342/500 [25:57<11:14,  4.27s/it]Running Inference:  69%|██████▊   | 343/500 [26:01<10:51,  4.15s/it]Running Inference:  69%|██████▉   | 344/500 [26:06<10:52,  4.18s/it]Running Inference:  69%|██████▉   | 345/500 [26:11<11:25,  4.42s/it]Running Inference:  69%|██████▉   | 346/500 [26:15<11:00,  4.29s/it]Running Inference:  69%|██████▉   | 347/500 [26:19<10:40,  4.19s/it]Running Inference:  70%|██████▉   | 348/500 [26:22<10:22,  4.10s/it]Running Inference:  70%|██████▉   | 349/500 [26:28<11:06,  4.42s/it]Running Inference:  70%|███████   | 350/500 [26:33<11:36,  4.64s/it]Running Inference:  70%|███████   | 351/500 [26:38<11:55,  4.80s/it]Running Inference:  70%|███████   | 352/500 [26:43<12:07,  4.91s/it]Running Inference:  71%|███████   | 353/500 [26:50<13:49,  5.64s/it]Running Inference:  71%|███████   | 354/500 [26:56<13:23,  5.50s/it]Running Inference:  71%|███████   | 355/500 [27:01<13:18,  5.51s/it]Running Inference:  71%|███████   | 356/500 [27:05<12:02,  5.02s/it]Running Inference:  71%|███████▏  | 357/500 [27:09<11:34,  4.86s/it]Running Inference:  72%|███████▏  | 358/500 [27:14<10:55,  4.62s/it]Running Inference:  72%|███████▏  | 359/500 [27:19<11:12,  4.77s/it]Running Inference:  72%|███████▏  | 360/500 [27:20<09:00,  3.86s/it]Running Inference:  72%|███████▏  | 361/500 [27:25<09:10,  3.96s/it]Running Inference:  72%|███████▏  | 362/500 [27:29<09:29,  4.12s/it]Running Inference:  73%|███████▎  | 363/500 [27:34<10:14,  4.49s/it]Running Inference:  73%|███████▎  | 364/500 [27:39<10:06,  4.46s/it]Running Inference:  73%|███████▎  | 365/500 [27:43<09:58,  4.43s/it]Running Inference:  73%|███████▎  | 366/500 [27:47<09:28,  4.24s/it]Running Inference:  73%|███████▎  | 367/500 [27:51<09:08,  4.13s/it]Running Inference:  74%|███████▎  | 368/500 [27:55<08:58,  4.08s/it]Running Inference:  74%|███████▍  | 369/500 [27:59<08:50,  4.05s/it]Running Inference:  74%|███████▍  | 370/500 [28:03<08:52,  4.09s/it]Running Inference:  74%|███████▍  | 371/500 [28:08<09:20,  4.34s/it]Running Inference:  74%|███████▍  | 372/500 [28:13<09:36,  4.51s/it]Running Inference:  75%|███████▍  | 373/500 [28:17<09:21,  4.42s/it]Running Inference:  75%|███████▍  | 374/500 [28:22<09:20,  4.45s/it]Running Inference:  75%|███████▌  | 375/500 [28:27<10:02,  4.82s/it]Running Inference:  75%|███████▌  | 376/500 [28:31<09:34,  4.64s/it]Running Inference:  75%|███████▌  | 377/500 [28:36<09:15,  4.51s/it]Running Inference:  76%|███████▌  | 378/500 [28:45<12:17,  6.04s/it]Running Inference:  76%|███████▌  | 379/500 [28:50<11:18,  5.61s/it]Running Inference:  76%|███████▌  | 380/500 [28:54<10:08,  5.07s/it]Running Inference:  76%|███████▌  | 381/500 [28:58<09:21,  4.72s/it]Running Inference:  76%|███████▋  | 382/500 [29:03<09:27,  4.81s/it]Running Inference:  77%|███████▋  | 383/500 [29:06<08:48,  4.52s/it]Running Inference:  77%|███████▋  | 384/500 [29:11<08:31,  4.41s/it]Running Inference:  77%|███████▋  | 385/500 [29:15<08:11,  4.27s/it]Running Inference:  77%|███████▋  | 386/500 [29:19<08:13,  4.33s/it]Running Inference:  77%|███████▋  | 387/500 [29:24<08:22,  4.45s/it]Running Inference:  78%|███████▊  | 388/500 [29:29<08:53,  4.77s/it]Running Inference:  78%|███████▊  | 389/500 [29:37<10:15,  5.54s/it]Running Inference:  78%|███████▊  | 390/500 [29:45<11:40,  6.37s/it]Running Inference:  78%|███████▊  | 391/500 [29:52<11:46,  6.48s/it]Running Inference:  78%|███████▊  | 392/500 [29:56<10:22,  5.77s/it]Running Inference:  79%|███████▊  | 393/500 [30:01<09:52,  5.54s/it]Running Inference:  79%|███████▉  | 394/500 [30:06<09:30,  5.38s/it]Running Inference:  79%|███████▉  | 395/500 [30:10<08:52,  5.07s/it]Running Inference:  79%|███████▉  | 396/500 [30:16<09:03,  5.23s/it]Running Inference:  79%|███████▉  | 397/500 [30:21<09:06,  5.31s/it]Running Inference:  80%|███████▉  | 398/500 [30:26<08:33,  5.04s/it]Running Inference:  80%|███████▉  | 399/500 [30:30<08:06,  4.82s/it]Running Inference:  80%|████████  | 400/500 [30:34<07:32,  4.53s/it]Running Inference:  80%|████████  | 401/500 [30:38<07:08,  4.33s/it]Running Inference:  80%|████████  | 402/500 [30:39<05:35,  3.43s/it]Running Inference:  81%|████████  | 403/500 [30:43<05:45,  3.56s/it]Running Inference:  81%|████████  | 404/500 [30:47<06:05,  3.81s/it]Running Inference:  81%|████████  | 405/500 [30:51<05:49,  3.68s/it]Running Inference:  81%|████████  | 406/500 [30:57<07:05,  4.53s/it]Running Inference:  81%|████████▏ | 407/500 [31:02<07:08,  4.61s/it]Running Inference:  82%|████████▏ | 408/500 [31:06<06:52,  4.48s/it]Running Inference:  82%|████████▏ | 409/500 [31:11<07:12,  4.75s/it]Running Inference:  82%|████████▏ | 410/500 [31:16<07:01,  4.68s/it]Running Inference:  82%|████████▏ | 411/500 [31:20<06:41,  4.51s/it]Running Inference:  82%|████████▏ | 412/500 [31:25<06:47,  4.63s/it]Running Inference:  83%|████████▎ | 413/500 [31:30<06:56,  4.79s/it]Running Inference:  83%|████████▎ | 414/500 [31:35<06:43,  4.69s/it]Running Inference:  83%|████████▎ | 415/500 [31:40<06:50,  4.82s/it]Running Inference:  83%|████████▎ | 416/500 [31:44<06:27,  4.61s/it]Running Inference:  83%|████████▎ | 417/500 [31:48<06:04,  4.39s/it]Running Inference:  84%|████████▎ | 418/500 [31:52<05:48,  4.25s/it]Running Inference:  84%|████████▍ | 419/500 [31:56<05:39,  4.19s/it]Running Inference:  84%|████████▍ | 420/500 [32:00<05:33,  4.16s/it]Running Inference:  84%|████████▍ | 421/500 [32:04<05:33,  4.22s/it]Running Inference:  84%|████████▍ | 422/500 [32:09<05:32,  4.27s/it]Running Inference:  85%|████████▍ | 423/500 [32:15<06:14,  4.87s/it]Running Inference:  85%|████████▍ | 424/500 [32:20<06:08,  4.85s/it]Running Inference:  85%|████████▌ | 425/500 [32:24<06:02,  4.84s/it]Running Inference:  85%|████████▌ | 426/500 [32:29<05:47,  4.70s/it]Running Inference:  85%|████████▌ | 427/500 [32:35<06:12,  5.10s/it]Running Inference:  86%|████████▌ | 428/500 [32:39<05:51,  4.89s/it]Running Inference:  86%|████████▌ | 429/500 [32:44<05:42,  4.83s/it]Running Inference:  86%|████████▌ | 430/500 [32:48<05:31,  4.74s/it]Running Inference:  86%|████████▌ | 431/500 [32:52<05:08,  4.47s/it]Running Inference:  86%|████████▋ | 432/500 [32:58<05:34,  4.92s/it]Running Inference:  87%|████████▋ | 433/500 [33:04<05:51,  5.25s/it]Running Inference:  87%|████████▋ | 434/500 [33:05<04:16,  3.89s/it]Running Inference:  87%|████████▋ | 435/500 [33:09<04:12,  3.88s/it]Running Inference:  87%|████████▋ | 436/500 [33:13<04:07,  3.87s/it]Running Inference:  87%|████████▋ | 437/500 [33:17<04:11,  4.00s/it]Running Inference:  88%|████████▊ | 438/500 [33:22<04:18,  4.18s/it]Running Inference:  88%|████████▊ | 439/500 [33:29<05:07,  5.05s/it]Running Inference:  88%|████████▊ | 440/500 [33:32<04:37,  4.63s/it]Running Inference:  88%|████████▊ | 441/500 [33:37<04:40,  4.75s/it]Running Inference:  88%|████████▊ | 442/500 [33:42<04:34,  4.73s/it]Running Inference:  89%|████████▊ | 443/500 [33:47<04:37,  4.87s/it]Running Inference:  89%|████████▉ | 444/500 [33:52<04:24,  4.72s/it]Running Inference:  89%|████████▉ | 445/500 [33:57<04:24,  4.81s/it]Running Inference:  89%|████████▉ | 446/500 [34:03<04:49,  5.35s/it]Running Inference:  89%|████████▉ | 447/500 [34:09<04:44,  5.36s/it]Running Inference:  90%|████████▉ | 448/500 [34:14<04:36,  5.32s/it]Running Inference:  90%|████████▉ | 449/500 [34:19<04:33,  5.35s/it]Running Inference:  90%|█████████ | 450/500 [34:23<04:09,  4.98s/it]Running Inference:  90%|█████████ | 451/500 [34:28<04:03,  4.96s/it]Running Inference:  90%|█████████ | 452/500 [34:34<04:02,  5.06s/it]Running Inference:  91%|█████████ | 453/500 [34:39<03:56,  5.03s/it]Running Inference:  91%|█████████ | 454/500 [34:43<03:42,  4.83s/it]Running Inference:  91%|█████████ | 455/500 [34:48<03:42,  4.95s/it]Running Inference:  91%|█████████ | 456/500 [34:53<03:30,  4.78s/it]Running Inference:  91%|█████████▏| 457/500 [34:56<03:15,  4.54s/it]Running Inference:  92%|█████████▏| 458/500 [35:02<03:21,  4.80s/it]Running Inference:  92%|█████████▏| 459/500 [35:07<03:24,  4.98s/it]Running Inference:  92%|█████████▏| 460/500 [35:13<03:25,  5.13s/it]Running Inference:  92%|█████████▏| 461/500 [35:17<03:14,  4.97s/it]Running Inference:  92%|█████████▏| 462/500 [35:23<03:11,  5.04s/it]Running Inference:  93%|█████████▎| 463/500 [35:27<03:00,  4.89s/it]Running Inference:  93%|█████████▎| 464/500 [35:32<02:52,  4.80s/it]Running Inference:  93%|█████████▎| 465/500 [35:36<02:42,  4.64s/it]Running Inference:  93%|█████████▎| 466/500 [35:40<02:34,  4.54s/it]Running Inference:  93%|█████████▎| 467/500 [35:44<02:26,  4.43s/it]Running Inference:  94%|█████████▎| 468/500 [35:48<02:16,  4.28s/it]Running Inference:  94%|█████████▍| 469/500 [35:52<02:08,  4.15s/it]Running Inference:  94%|█████████▍| 470/500 [35:56<02:03,  4.11s/it]Running Inference:  94%|█████████▍| 471/500 [36:00<01:59,  4.10s/it]Running Inference:  94%|█████████▍| 472/500 [36:04<01:54,  4.10s/it]Running Inference:  95%|█████████▍| 473/500 [36:10<01:59,  4.41s/it]Running Inference:  95%|█████████▍| 474/500 [36:15<01:59,  4.62s/it]Running Inference:  95%|█████████▌| 475/500 [36:20<01:58,  4.72s/it]Running Inference:  95%|█████████▌| 476/500 [36:24<01:47,  4.48s/it]Running Inference:  95%|█████████▌| 477/500 [36:27<01:38,  4.30s/it]Running Inference:  96%|█████████▌| 478/500 [36:33<01:44,  4.77s/it]Running Inference:  96%|█████████▌| 479/500 [36:40<01:55,  5.48s/it]Running Inference:  96%|█████████▌| 480/500 [36:44<01:40,  5.04s/it]Running Inference:  96%|█████████▌| 481/500 [36:49<01:35,  5.02s/it]Running Inference:  96%|█████████▋| 482/500 [36:51<01:11,  3.94s/it]Running Inference:  97%|█████████▋| 483/500 [36:55<01:09,  4.06s/it]Running Inference:  97%|█████████▋| 484/500 [36:59<01:04,  4.01s/it]Running Inference:  97%|█████████▋| 485/500 [37:04<01:03,  4.22s/it]Running Inference:  97%|█████████▋| 486/500 [37:08<00:59,  4.22s/it]Running Inference:  97%|█████████▋| 487/500 [37:13<00:56,  4.34s/it]Running Inference:  98%|█████████▊| 488/500 [37:17<00:51,  4.26s/it]Running Inference:  98%|█████████▊| 489/500 [37:22<00:49,  4.53s/it]Running Inference:  98%|█████████▊| 490/500 [37:27<00:45,  4.57s/it]Running Inference:  98%|█████████▊| 491/500 [37:31<00:41,  4.65s/it]Running Inference:  98%|█████████▊| 492/500 [37:37<00:39,  4.96s/it]Running Inference:  99%|█████████▊| 493/500 [37:41<00:33,  4.72s/it]Running Inference:  99%|█████████▉| 494/500 [37:46<00:27,  4.64s/it]Running Inference:  99%|█████████▉| 495/500 [37:50<00:22,  4.58s/it]Running Inference:  99%|█████████▉| 496/500 [37:55<00:18,  4.61s/it]Running Inference:  99%|█████████▉| 497/500 [37:59<00:13,  4.63s/it]Running Inference: 100%|█████████▉| 498/500 [38:04<00:09,  4.55s/it]Running Inference: 100%|█████████▉| 499/500 [38:08<00:04,  4.47s/it]Running Inference: 100%|██████████| 500/500 [38:12<00:00,  4.37s/it]Running Inference: 100%|██████████| 500/500 [38:12<00:00,  4.59s/it]
2025-12-15 01:49:59,491 - INFO - Inference completed.
2025-12-15 01:49:59,543 - INFO - Results saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-15 01:49:59,543 - INFO - Calculating metrics for dataset: longbench
2025-12-15 01:49:59,545 - INFO - Metrics saved to longbenchresult/longbench__repobench-p__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-15 01:49:59,545 - INFO - Metrics:
15.97
2025-12-15 01:49:59,546 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=repobench-p, ratio=0.5) on GPU 2


========================================
LongBench Task: 2wikimqa_e
========================================
----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 01:50:06,031 - INFO - Set deterministic seeds to 42
2025-12-15 01:50:06,031 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 01:50:06,031 - INFO - Starting evaluation run...
2025-12-15 01:50:06,031 - INFO - Output directory set to: longbenchresult
2025-12-15 01:50:06,031 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-15 01:50:06,031 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 01:50:06,031 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 01:50:06,031 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 103.32it/s]
Device set to use cuda:0
2025-12-15 01:50:25,928 - INFO - Model pipeline loaded.
2025-12-15 01:50:25,929 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
2025-12-15 01:50:34,371 - INFO - Dataset loaded with 300 entries.
2025-12-15 01:50:34,371 - INFO - Dataset processed with 300 entries.
2025-12-15 01:50:34,404 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:02<10:14,  2.06s/it]Running Inference:   1%|          | 2/300 [00:03<07:25,  1.50s/it]Running Inference:   1%|          | 3/300 [00:05<10:07,  2.05s/it]Running Inference:   1%|▏         | 4/300 [00:09<13:09,  2.67s/it]Running Inference:   2%|▏         | 5/300 [00:10<10:04,  2.05s/it]Running Inference:   2%|▏         | 6/300 [00:12<10:17,  2.10s/it]Running Inference:   2%|▏         | 7/300 [00:16<13:10,  2.70s/it]Running Inference:   3%|▎         | 8/300 [00:17<11:06,  2.28s/it]Running Inference:   3%|▎         | 9/300 [00:20<11:59,  2.47s/it]Running Inference:   3%|▎         | 10/300 [00:23<12:27,  2.58s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:24<09:54,  2.06s/it]Running Inference:   4%|▍         | 12/300 [00:26<10:28,  2.18s/it]Running Inference:   4%|▍         | 13/300 [00:28<08:46,  1.84s/it]Running Inference:   5%|▍         | 14/300 [00:32<12:14,  2.57s/it]Running Inference:   5%|▌         | 15/300 [00:34<12:13,  2.57s/it]Running Inference:   5%|▌         | 16/300 [00:37<11:47,  2.49s/it]Running Inference:   6%|▌         | 17/300 [00:40<13:23,  2.84s/it]Running Inference:   6%|▌         | 18/300 [00:43<13:12,  2.81s/it]Running Inference:   6%|▋         | 19/300 [00:47<14:49,  3.16s/it]Running Inference:   7%|▋         | 20/300 [00:47<10:54,  2.34s/it]Running Inference:   7%|▋         | 21/300 [00:52<13:17,  2.86s/it]Running Inference:   7%|▋         | 22/300 [00:53<11:48,  2.55s/it]Running Inference:   8%|▊         | 23/300 [00:55<10:05,  2.19s/it]Running Inference:   8%|▊         | 24/300 [00:58<11:09,  2.42s/it]Running Inference:   8%|▊         | 25/300 [01:00<11:00,  2.40s/it]Running Inference:   9%|▊         | 26/300 [01:03<11:59,  2.63s/it]Running Inference:   9%|▉         | 27/300 [01:04<09:36,  2.11s/it]Running Inference:   9%|▉         | 28/300 [01:07<10:50,  2.39s/it]Running Inference:  10%|▉         | 29/300 [01:08<09:08,  2.02s/it]Running Inference:  10%|█         | 30/300 [01:12<11:23,  2.53s/it]Running Inference:  10%|█         | 31/300 [01:15<12:27,  2.78s/it]Running Inference:  11%|█         | 32/300 [01:19<13:55,  3.12s/it]Running Inference:  11%|█         | 33/300 [01:23<15:16,  3.43s/it]Running Inference:  11%|█▏        | 34/300 [01:26<13:29,  3.04s/it]Running Inference:  12%|█▏        | 35/300 [01:27<10:44,  2.43s/it]Running Inference:  12%|█▏        | 36/300 [01:31<12:51,  2.92s/it]Running Inference:  12%|█▏        | 37/300 [01:32<11:21,  2.59s/it]Running Inference:  13%|█▎        | 38/300 [01:34<10:01,  2.30s/it]Running Inference:  13%|█▎        | 39/300 [01:35<08:12,  1.89s/it]Running Inference:  13%|█▎        | 40/300 [01:37<08:00,  1.85s/it]Running Inference:  14%|█▎        | 41/300 [01:40<10:05,  2.34s/it]Running Inference:  14%|█▍        | 42/300 [01:41<08:31,  1.98s/it]Running Inference:  14%|█▍        | 43/300 [01:43<08:31,  1.99s/it]Running Inference:  15%|█▍        | 44/300 [01:44<07:14,  1.70s/it]Running Inference:  15%|█▌        | 45/300 [01:49<10:15,  2.41s/it]Running Inference:  15%|█▌        | 46/300 [01:50<09:37,  2.27s/it]Running Inference:  16%|█▌        | 47/300 [01:53<09:51,  2.34s/it]Running Inference:  16%|█▌        | 48/300 [01:56<10:39,  2.54s/it]Running Inference:  16%|█▋        | 49/300 [01:58<10:24,  2.49s/it]Running Inference:  17%|█▋        | 50/300 [02:00<09:10,  2.20s/it]Running Inference:  17%|█▋        | 51/300 [02:03<10:12,  2.46s/it]Running Inference:  17%|█▋        | 52/300 [02:06<10:46,  2.61s/it]Running Inference:  18%|█▊        | 53/300 [02:09<11:09,  2.71s/it]Running Inference:  18%|█▊        | 54/300 [02:10<09:29,  2.32s/it]Running Inference:  18%|█▊        | 55/300 [02:13<09:23,  2.30s/it]Running Inference:  19%|█▊        | 56/300 [02:15<09:55,  2.44s/it]Running Inference:  19%|█▉        | 57/300 [02:18<10:08,  2.51s/it]Running Inference:  19%|█▉        | 58/300 [02:21<10:49,  2.68s/it]Running Inference:  20%|█▉        | 59/300 [02:22<08:21,  2.08s/it]Running Inference:  20%|██        | 60/300 [02:25<09:31,  2.38s/it]Running Inference:  20%|██        | 61/300 [02:26<08:29,  2.13s/it]Running Inference:  21%|██        | 62/300 [02:28<08:23,  2.11s/it]Running Inference:  21%|██        | 63/300 [02:31<09:06,  2.31s/it]Running Inference:  21%|██▏       | 64/300 [02:34<09:18,  2.37s/it]Running Inference:  22%|██▏       | 65/300 [02:36<08:47,  2.25s/it]Running Inference:  22%|██▏       | 66/300 [02:38<09:22,  2.41s/it]Running Inference:  22%|██▏       | 67/300 [02:41<09:54,  2.55s/it]Running Inference:  23%|██▎       | 68/300 [02:43<09:20,  2.42s/it]Running Inference:  23%|██▎       | 69/300 [02:45<08:09,  2.12s/it]Running Inference:  23%|██▎       | 70/300 [02:46<06:48,  1.77s/it]Running Inference:  24%|██▎       | 71/300 [02:47<05:59,  1.57s/it]Running Inference:  24%|██▍       | 72/300 [02:48<05:25,  1.43s/it]Running Inference:  24%|██▍       | 73/300 [02:49<05:09,  1.37s/it]Running Inference:  25%|██▍       | 74/300 [02:50<04:41,  1.24s/it]Running Inference:  25%|██▌       | 75/300 [02:51<04:11,  1.12s/it]Running Inference:  25%|██▌       | 76/300 [02:52<03:46,  1.01s/it]Running Inference:  26%|██▌       | 77/300 [02:54<05:04,  1.37s/it]Running Inference:  26%|██▌       | 78/300 [02:58<07:30,  2.03s/it]Running Inference:  26%|██▋       | 79/300 [03:00<08:26,  2.29s/it]Running Inference:  27%|██▋       | 80/300 [03:02<07:08,  1.95s/it]Running Inference:  27%|██▋       | 81/300 [03:03<06:39,  1.83s/it]Running Inference:  27%|██▋       | 82/300 [03:06<07:31,  2.07s/it]Running Inference:  28%|██▊       | 83/300 [03:07<06:43,  1.86s/it]Running Inference:  28%|██▊       | 84/300 [03:09<06:52,  1.91s/it]Running Inference:  28%|██▊       | 85/300 [03:13<09:10,  2.56s/it]Running Inference:  29%|██▊       | 86/300 [03:14<07:34,  2.13s/it]Running Inference:  29%|██▉       | 87/300 [03:17<07:46,  2.19s/it]Running Inference:  29%|██▉       | 88/300 [03:21<09:35,  2.71s/it]Running Inference:  30%|██▉       | 89/300 [03:23<09:12,  2.62s/it]Running Inference:  30%|███       | 90/300 [03:25<07:58,  2.28s/it]Running Inference:  30%|███       | 91/300 [03:28<08:46,  2.52s/it]Running Inference:  31%|███       | 92/300 [03:28<06:54,  1.99s/it]Running Inference:  31%|███       | 93/300 [03:30<06:44,  1.96s/it]Running Inference:  31%|███▏      | 94/300 [03:31<05:54,  1.72s/it]Running Inference:  32%|███▏      | 95/300 [03:36<08:24,  2.46s/it]Running Inference:  32%|███▏      | 96/300 [03:37<07:23,  2.17s/it]Running Inference:  32%|███▏      | 97/300 [03:38<06:13,  1.84s/it]Running Inference:  33%|███▎      | 98/300 [03:39<05:28,  1.62s/it]Running Inference:  33%|███▎      | 99/300 [03:41<06:00,  1.79s/it]Running Inference:  33%|███▎      | 100/300 [03:42<05:05,  1.53s/it]Running Inference:  34%|███▎      | 101/300 [03:45<06:24,  1.93s/it]Running Inference:  34%|███▍      | 102/300 [03:48<07:25,  2.25s/it]Running Inference:  34%|███▍      | 103/300 [03:50<07:11,  2.19s/it]Running Inference:  35%|███▍      | 104/300 [03:53<08:05,  2.48s/it]Running Inference:  35%|███▌      | 105/300 [03:56<08:27,  2.60s/it]Running Inference:  35%|███▌      | 106/300 [03:59<08:40,  2.68s/it]Running Inference:  36%|███▌      | 107/300 [04:02<08:33,  2.66s/it]Running Inference:  36%|███▌      | 108/300 [04:05<08:49,  2.76s/it]Running Inference:  36%|███▋      | 109/300 [04:08<08:52,  2.79s/it]Running Inference:  37%|███▋      | 110/300 [04:10<08:45,  2.76s/it]Running Inference:  37%|███▋      | 111/300 [04:15<10:01,  3.18s/it]Running Inference:  37%|███▋      | 112/300 [04:15<07:44,  2.47s/it]Running Inference:  38%|███▊      | 113/300 [04:17<06:42,  2.15s/it]Running Inference:  38%|███▊      | 114/300 [04:20<07:37,  2.46s/it]Running Inference:  38%|███▊      | 115/300 [04:24<09:05,  2.95s/it]Running Inference:  39%|███▊      | 116/300 [04:28<09:46,  3.19s/it]Running Inference:  39%|███▉      | 117/300 [04:30<09:18,  3.05s/it]Running Inference:  39%|███▉      | 118/300 [04:32<08:02,  2.65s/it]Running Inference:  40%|███▉      | 119/300 [04:34<06:51,  2.27s/it]Running Inference:  40%|████      | 120/300 [04:36<07:08,  2.38s/it]Running Inference:  40%|████      | 121/300 [04:38<06:32,  2.19s/it]Running Inference:  41%|████      | 122/300 [04:41<07:00,  2.36s/it]Running Inference:  41%|████      | 123/300 [04:43<06:38,  2.25s/it]Running Inference:  41%|████▏     | 124/300 [04:46<07:49,  2.67s/it]Running Inference:  42%|████▏     | 125/300 [04:48<07:02,  2.42s/it]Running Inference:  42%|████▏     | 126/300 [04:51<07:14,  2.50s/it]Running Inference:  42%|████▏     | 127/300 [04:55<08:41,  3.02s/it]Running Inference:  43%|████▎     | 128/300 [04:58<08:22,  2.92s/it]Running Inference:  43%|████▎     | 129/300 [04:59<06:45,  2.37s/it]Running Inference:  43%|████▎     | 130/300 [05:00<05:41,  2.01s/it]Running Inference:  44%|████▎     | 131/300 [05:03<06:16,  2.23s/it]Running Inference:  44%|████▍     | 132/300 [05:04<05:00,  1.79s/it]Running Inference:  44%|████▍     | 133/300 [05:05<04:46,  1.71s/it]Running Inference:  45%|████▍     | 134/300 [05:08<05:20,  1.93s/it]Running Inference:  45%|████▌     | 135/300 [05:08<04:13,  1.54s/it]Running Inference:  45%|████▌     | 136/300 [05:11<05:36,  2.05s/it]Running Inference:  46%|████▌     | 137/300 [05:15<06:48,  2.50s/it]Running Inference:  46%|████▌     | 138/300 [05:18<07:07,  2.64s/it]Running Inference:  46%|████▋     | 139/300 [05:19<05:51,  2.18s/it]Running Inference:  47%|████▋     | 140/300 [05:20<04:48,  1.80s/it]Running Inference:  47%|████▋     | 141/300 [05:22<04:43,  1.79s/it]Running Inference:  47%|████▋     | 142/300 [05:23<04:27,  1.69s/it]Running Inference:  48%|████▊     | 143/300 [05:24<04:00,  1.53s/it]Running Inference:  48%|████▊     | 144/300 [05:29<06:04,  2.33s/it]Running Inference:  48%|████▊     | 145/300 [05:30<05:06,  1.98s/it]Running Inference:  49%|████▊     | 146/300 [05:33<06:01,  2.35s/it]Running Inference:  49%|████▉     | 147/300 [05:36<06:11,  2.43s/it]Running Inference:  49%|████▉     | 148/300 [05:40<07:33,  2.98s/it]Running Inference:  50%|████▉     | 149/300 [05:42<06:49,  2.71s/it]Running Inference:  50%|█████     | 150/300 [05:43<05:40,  2.27s/it]Running Inference:  50%|█████     | 151/300 [05:44<04:40,  1.88s/it]Running Inference:  51%|█████     | 152/300 [05:47<05:24,  2.20s/it]Running Inference:  51%|█████     | 153/300 [05:50<06:09,  2.51s/it]Running Inference:  51%|█████▏    | 154/300 [05:53<05:59,  2.46s/it]Running Inference:  52%|█████▏    | 155/300 [05:54<04:48,  1.99s/it]Running Inference:  52%|█████▏    | 156/300 [05:56<05:16,  2.20s/it]Running Inference:  52%|█████▏    | 157/300 [05:57<04:36,  1.93s/it]Running Inference:  53%|█████▎    | 158/300 [06:00<05:17,  2.24s/it]Running Inference:  53%|█████▎    | 159/300 [06:01<04:23,  1.87s/it]Running Inference:  53%|█████▎    | 160/300 [06:04<04:47,  2.05s/it]Running Inference:  54%|█████▎    | 161/300 [06:08<06:15,  2.70s/it]Running Inference:  54%|█████▍    | 162/300 [06:12<06:52,  2.99s/it]Running Inference:  54%|█████▍    | 163/300 [06:14<06:05,  2.67s/it]Running Inference:  55%|█████▍    | 164/300 [06:18<07:02,  3.11s/it]Running Inference:  55%|█████▌    | 165/300 [06:19<05:21,  2.38s/it]Running Inference:  55%|█████▌    | 166/300 [06:20<04:21,  1.95s/it]Running Inference:  56%|█████▌    | 167/300 [06:22<04:54,  2.21s/it]Running Inference:  56%|█████▌    | 168/300 [06:23<04:07,  1.87s/it]Running Inference:  56%|█████▋    | 169/300 [06:27<05:28,  2.51s/it]Running Inference:  57%|█████▋    | 170/300 [06:29<04:35,  2.12s/it]Running Inference:  57%|█████▋    | 171/300 [06:33<05:55,  2.75s/it]Running Inference:  57%|█████▋    | 172/300 [06:35<05:30,  2.58s/it]Running Inference:  58%|█████▊    | 173/300 [06:38<05:27,  2.58s/it]Running Inference:  58%|█████▊    | 174/300 [06:38<04:21,  2.07s/it]Running Inference:  58%|█████▊    | 175/300 [06:39<03:37,  1.74s/it]Running Inference:  59%|█████▊    | 176/300 [06:42<03:50,  1.86s/it]Running Inference:  59%|█████▉    | 177/300 [06:46<05:05,  2.49s/it]Running Inference:  59%|█████▉    | 178/300 [06:47<04:37,  2.27s/it]Running Inference:  60%|█████▉    | 179/300 [06:48<03:39,  1.82s/it]Running Inference:  60%|██████    | 180/300 [06:51<04:24,  2.20s/it]Running Inference:  60%|██████    | 181/300 [06:54<04:35,  2.32s/it]Running Inference:  61%|██████    | 182/300 [06:57<05:08,  2.62s/it]Running Inference:  61%|██████    | 183/300 [06:58<04:21,  2.24s/it]Running Inference:  61%|██████▏   | 184/300 [07:03<05:27,  2.82s/it]Running Inference:  62%|██████▏   | 185/300 [07:06<05:38,  2.94s/it]Running Inference:  62%|██████▏   | 186/300 [07:08<05:03,  2.66s/it]Running Inference:  62%|██████▏   | 187/300 [07:09<03:56,  2.09s/it]Running Inference:  63%|██████▎   | 188/300 [07:11<04:19,  2.31s/it]Running Inference:  63%|██████▎   | 189/300 [07:12<03:21,  1.82s/it]Running Inference:  63%|██████▎   | 190/300 [07:14<03:06,  1.69s/it]Running Inference:  64%|██████▎   | 191/300 [07:14<02:35,  1.43s/it]Running Inference:  64%|██████▍   | 192/300 [07:17<03:17,  1.83s/it]Running Inference:  64%|██████▍   | 193/300 [07:20<03:58,  2.23s/it]Running Inference:  65%|██████▍   | 194/300 [07:21<03:22,  1.91s/it]Running Inference:  65%|██████▌   | 195/300 [07:24<03:44,  2.14s/it]Running Inference:  65%|██████▌   | 196/300 [07:27<03:54,  2.25s/it]Running Inference:  66%|██████▌   | 197/300 [07:28<03:30,  2.04s/it]Running Inference:  66%|██████▌   | 198/300 [07:29<02:54,  1.71s/it]Running Inference:  66%|██████▋   | 199/300 [07:33<04:09,  2.47s/it]Running Inference:  67%|██████▋   | 200/300 [07:38<04:59,  2.99s/it]Running Inference:  67%|██████▋   | 201/300 [07:40<04:51,  2.94s/it]Running Inference:  67%|██████▋   | 202/300 [07:43<04:46,  2.92s/it]Running Inference:  68%|██████▊   | 203/300 [07:44<03:44,  2.32s/it]Running Inference:  68%|██████▊   | 204/300 [07:46<03:36,  2.25s/it]Running Inference:  68%|██████▊   | 205/300 [07:47<02:58,  1.88s/it]Running Inference:  69%|██████▊   | 206/300 [07:49<02:55,  1.87s/it]Running Inference:  69%|██████▉   | 207/300 [07:52<03:28,  2.24s/it]Running Inference:  69%|██████▉   | 208/300 [07:55<03:38,  2.37s/it]Running Inference:  70%|██████▉   | 209/300 [07:58<03:51,  2.54s/it]Running Inference:  70%|███████   | 210/300 [08:01<04:05,  2.73s/it]Running Inference:  70%|███████   | 211/300 [08:03<03:35,  2.43s/it]Running Inference:  71%|███████   | 212/300 [08:06<03:55,  2.68s/it]Running Inference:  71%|███████   | 213/300 [08:07<03:00,  2.08s/it]Running Inference:  71%|███████▏  | 214/300 [08:10<03:27,  2.41s/it]Running Inference:  72%|███████▏  | 215/300 [08:14<04:08,  2.92s/it]Running Inference:  72%|███████▏  | 216/300 [08:16<03:32,  2.54s/it]Running Inference:  72%|███████▏  | 217/300 [08:18<03:23,  2.45s/it]Running Inference:  73%|███████▎  | 218/300 [08:19<02:56,  2.16s/it]Running Inference:  73%|███████▎  | 219/300 [08:20<02:28,  1.84s/it]Running Inference:  73%|███████▎  | 220/300 [08:21<02:04,  1.55s/it]Running Inference:  74%|███████▎  | 221/300 [08:23<01:58,  1.50s/it]Running Inference:  74%|███████▍  | 222/300 [08:24<02:04,  1.59s/it]Running Inference:  74%|███████▍  | 223/300 [08:25<01:46,  1.38s/it]Running Inference:  75%|███████▍  | 224/300 [08:28<02:16,  1.79s/it]Running Inference:  75%|███████▌  | 225/300 [08:32<03:05,  2.47s/it]Running Inference:  75%|███████▌  | 226/300 [08:35<03:04,  2.49s/it]Running Inference:  76%|███████▌  | 227/300 [08:38<03:11,  2.63s/it]Running Inference:  76%|███████▌  | 228/300 [08:39<02:47,  2.33s/it]Running Inference:  76%|███████▋  | 229/300 [08:42<02:55,  2.47s/it]Running Inference:  77%|███████▋  | 230/300 [08:46<03:18,  2.83s/it]Running Inference:  77%|███████▋  | 231/300 [08:46<02:28,  2.16s/it]Running Inference:  77%|███████▋  | 232/300 [08:49<02:36,  2.30s/it]Running Inference:  78%|███████▊  | 233/300 [08:50<02:15,  2.02s/it]Running Inference:  78%|███████▊  | 234/300 [08:53<02:27,  2.23s/it]Running Inference:  78%|███████▊  | 235/300 [08:56<02:41,  2.49s/it]Running Inference:  79%|███████▊  | 236/300 [08:59<02:51,  2.67s/it]Running Inference:  79%|███████▉  | 237/300 [09:01<02:33,  2.43s/it]Running Inference:  79%|███████▉  | 238/300 [09:04<02:37,  2.55s/it]Running Inference:  80%|███████▉  | 239/300 [09:07<02:50,  2.79s/it]Running Inference:  80%|████████  | 240/300 [09:11<03:01,  3.03s/it]Running Inference:  80%|████████  | 241/300 [09:13<02:45,  2.80s/it]Running Inference:  81%|████████  | 242/300 [09:14<02:10,  2.24s/it]Running Inference:  81%|████████  | 243/300 [09:17<02:14,  2.36s/it]Running Inference:  81%|████████▏ | 244/300 [09:17<01:40,  1.80s/it]Running Inference:  82%|████████▏ | 245/300 [09:21<02:10,  2.37s/it]Running Inference:  82%|████████▏ | 246/300 [09:22<01:45,  1.96s/it]Running Inference:  82%|████████▏ | 247/300 [09:22<01:20,  1.52s/it]Running Inference:  83%|████████▎ | 248/300 [09:25<01:29,  1.72s/it]Running Inference:  83%|████████▎ | 249/300 [09:26<01:17,  1.52s/it]Running Inference:  83%|████████▎ | 250/300 [09:29<01:45,  2.11s/it]Running Inference:  84%|████████▎ | 251/300 [09:31<01:38,  2.01s/it]Running Inference:  84%|████████▍ | 252/300 [09:33<01:44,  2.17s/it]Running Inference:  84%|████████▍ | 253/300 [09:36<01:44,  2.23s/it]Running Inference:  85%|████████▍ | 254/300 [09:37<01:21,  1.77s/it]Running Inference:  85%|████████▌ | 255/300 [09:39<01:31,  2.04s/it]Running Inference:  85%|████████▌ | 256/300 [09:40<01:11,  1.63s/it]Running Inference:  86%|████████▌ | 257/300 [09:43<01:28,  2.05s/it]Running Inference:  86%|████████▌ | 258/300 [09:45<01:26,  2.05s/it]Running Inference:  86%|████████▋ | 259/300 [09:48<01:38,  2.40s/it]Running Inference:  87%|████████▋ | 260/300 [09:51<01:36,  2.41s/it]Running Inference:  87%|████████▋ | 261/300 [09:52<01:17,  1.97s/it]Running Inference:  87%|████████▋ | 262/300 [09:54<01:22,  2.18s/it]Running Inference:  88%|████████▊ | 263/300 [09:55<01:05,  1.78s/it]Running Inference:  88%|████████▊ | 264/300 [09:58<01:14,  2.08s/it]Running Inference:  88%|████████▊ | 265/300 [10:01<01:20,  2.31s/it]Running Inference:  89%|████████▊ | 266/300 [10:02<01:06,  1.97s/it]Running Inference:  89%|████████▉ | 267/300 [10:02<00:51,  1.55s/it]Running Inference:  89%|████████▉ | 268/300 [10:06<01:06,  2.07s/it]Running Inference:  90%|████████▉ | 269/300 [10:10<01:22,  2.67s/it]Running Inference:  90%|█████████ | 270/300 [10:11<01:02,  2.10s/it]Running Inference:  90%|█████████ | 271/300 [10:12<00:57,  1.97s/it]Running Inference:  91%|█████████ | 272/300 [10:16<01:09,  2.49s/it]Running Inference:  91%|█████████ | 273/300 [10:18<01:04,  2.40s/it]Running Inference:  91%|█████████▏| 274/300 [10:21<01:06,  2.54s/it]Running Inference:  92%|█████████▏| 275/300 [10:22<00:50,  2.02s/it]Running Inference:  92%|█████████▏| 276/300 [10:24<00:47,  1.96s/it]Running Inference:  92%|█████████▏| 277/300 [10:27<00:57,  2.50s/it]Running Inference:  93%|█████████▎| 278/300 [10:30<00:55,  2.52s/it]Running Inference:  93%|█████████▎| 279/300 [10:32<00:52,  2.51s/it]Running Inference:  93%|█████████▎| 280/300 [10:35<00:49,  2.46s/it]Running Inference:  94%|█████████▎| 281/300 [10:39<00:56,  2.97s/it]Running Inference:  94%|█████████▍| 282/300 [10:41<00:49,  2.76s/it]Running Inference:  94%|█████████▍| 283/300 [10:43<00:44,  2.59s/it]Running Inference:  95%|█████████▍| 284/300 [10:48<00:49,  3.08s/it]Running Inference:  95%|█████████▌| 285/300 [10:49<00:37,  2.51s/it]Running Inference:  95%|█████████▌| 286/300 [10:50<00:28,  2.00s/it]Running Inference:  96%|█████████▌| 287/300 [10:54<00:34,  2.66s/it]Running Inference:  96%|█████████▌| 288/300 [10:57<00:32,  2.69s/it]Running Inference:  96%|█████████▋| 289/300 [11:01<00:34,  3.11s/it]Running Inference:  97%|█████████▋| 290/300 [11:02<00:26,  2.69s/it]Running Inference:  97%|█████████▋| 291/300 [11:05<00:23,  2.63s/it]Running Inference:  97%|█████████▋| 292/300 [11:08<00:23,  2.92s/it]Running Inference:  98%|█████████▊| 293/300 [11:12<00:20,  2.99s/it]Running Inference:  98%|█████████▊| 294/300 [11:15<00:19,  3.18s/it]Running Inference:  98%|█████████▊| 295/300 [11:17<00:13,  2.70s/it]Running Inference:  99%|█████████▊| 296/300 [11:18<00:08,  2.15s/it]Running Inference:  99%|█████████▉| 297/300 [11:19<00:05,  1.92s/it]Running Inference:  99%|█████████▉| 298/300 [11:22<00:04,  2.31s/it]Running Inference: 100%|█████████▉| 299/300 [11:25<00:02,  2.54s/it]Running Inference: 100%|██████████| 300/300 [11:29<00:00,  2.94s/it]Running Inference: 100%|██████████| 300/300 [11:29<00:00,  2.30s/it]
2025-12-15 02:02:04,195 - INFO - Inference completed.
2025-12-15 02:02:04,205 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-15 02:02:04,205 - INFO - Calculating metrics for dataset: longbench
2025-12-15 02:02:04,213 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-15 02:02:04,213 - INFO - Metrics:
20.01
2025-12-15 02:02:04,214 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa_e, ratio=0.1) on GPU 2

----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 02:02:10,628 - INFO - Set deterministic seeds to 42
2025-12-15 02:02:10,628 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 02:02:10,629 - INFO - Starting evaluation run...
2025-12-15 02:02:10,629 - INFO - Output directory set to: longbenchresult
2025-12-15 02:02:10,629 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-15 02:02:10,629 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 02:02:10,629 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 02:02:10,629 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 104.64it/s]
Device set to use cuda:0
2025-12-15 02:02:22,365 - INFO - Model pipeline loaded.
2025-12-15 02:02:22,365 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
2025-12-15 02:02:25,588 - INFO - Dataset loaded with 300 entries.
2025-12-15 02:02:25,588 - INFO - Dataset processed with 300 entries.
2025-12-15 02:02:25,622 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:01<07:01,  1.41s/it]Running Inference:   1%|          | 2/300 [00:02<06:04,  1.22s/it]Running Inference:   1%|          | 3/300 [00:05<09:25,  1.90s/it]Running Inference:   1%|▏         | 4/300 [00:08<12:47,  2.59s/it]Running Inference:   2%|▏         | 5/300 [00:09<09:50,  2.00s/it]Running Inference:   2%|▏         | 6/300 [00:12<10:46,  2.20s/it]Running Inference:   2%|▏         | 7/300 [00:15<11:31,  2.36s/it]Running Inference:   3%|▎         | 8/300 [00:16<09:36,  1.97s/it]Running Inference:   3%|▎         | 9/300 [00:19<10:59,  2.27s/it]Running Inference:   3%|▎         | 10/300 [00:21<11:47,  2.44s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:23<10:34,  2.19s/it]Running Inference:   4%|▍         | 12/300 [00:26<10:57,  2.28s/it]Running Inference:   4%|▍         | 13/300 [00:27<09:07,  1.91s/it]Running Inference:   5%|▍         | 14/300 [00:31<12:29,  2.62s/it]Running Inference:   5%|▌         | 15/300 [00:34<13:07,  2.76s/it]Running Inference:   5%|▌         | 16/300 [00:36<12:26,  2.63s/it]Running Inference:   6%|▌         | 17/300 [00:40<13:51,  2.94s/it]Running Inference:   6%|▌         | 18/300 [00:43<13:26,  2.86s/it]Running Inference:   6%|▋         | 19/300 [00:47<15:00,  3.21s/it]Running Inference:   7%|▋         | 20/300 [00:47<11:02,  2.37s/it]Running Inference:   7%|▋         | 21/300 [00:51<13:24,  2.88s/it]Running Inference:   7%|▋         | 22/300 [00:53<11:54,  2.57s/it]Running Inference:   8%|▊         | 23/300 [00:56<12:37,  2.74s/it]Running Inference:   8%|▊         | 24/300 [00:59<12:56,  2.81s/it]Running Inference:   8%|▊         | 25/300 [01:02<12:40,  2.77s/it]Running Inference:   9%|▊         | 26/300 [01:05<13:11,  2.89s/it]Running Inference:   9%|▉         | 27/300 [01:06<10:26,  2.30s/it]Running Inference:   9%|▉         | 28/300 [01:09<11:26,  2.52s/it]Running Inference:  10%|▉         | 29/300 [01:10<09:33,  2.12s/it]Running Inference:  10%|█         | 30/300 [01:14<11:42,  2.60s/it]Running Inference:  10%|█         | 31/300 [01:17<12:42,  2.83s/it]Running Inference:  11%|█         | 32/300 [01:21<14:06,  3.16s/it]Running Inference:  11%|█         | 33/300 [01:25<15:24,  3.46s/it]Running Inference:  11%|█▏        | 34/300 [01:28<14:56,  3.37s/it]Running Inference:  12%|█▏        | 35/300 [01:30<12:08,  2.75s/it]Running Inference:  12%|█▏        | 36/300 [01:34<13:52,  3.15s/it]Running Inference:  12%|█▏        | 37/300 [01:36<12:02,  2.75s/it]Running Inference:  13%|█▎        | 38/300 [01:37<10:30,  2.41s/it]Running Inference:  13%|█▎        | 39/300 [01:38<08:33,  1.97s/it]Running Inference:  13%|█▎        | 40/300 [01:40<08:15,  1.90s/it]Running Inference:  14%|█▎        | 41/300 [01:42<08:45,  2.03s/it]Running Inference:  14%|█▍        | 42/300 [01:43<07:38,  1.78s/it]Running Inference:  14%|█▍        | 43/300 [01:45<07:55,  1.85s/it]Running Inference:  15%|█▍        | 44/300 [01:46<06:49,  1.60s/it]Running Inference:  15%|█▌        | 45/300 [01:51<09:58,  2.35s/it]Running Inference:  15%|█▌        | 46/300 [01:53<09:24,  2.22s/it]Running Inference:  16%|█▌        | 47/300 [01:53<07:39,  1.81s/it]Running Inference:  16%|█▌        | 48/300 [01:56<09:08,  2.18s/it]Running Inference:  16%|█▋        | 49/300 [01:59<09:21,  2.24s/it]Running Inference:  17%|█▋        | 50/300 [02:01<09:51,  2.37s/it]Running Inference:  17%|█▋        | 51/300 [02:05<10:43,  2.58s/it]Running Inference:  17%|█▋        | 52/300 [02:08<11:09,  2.70s/it]Running Inference:  18%|█▊        | 53/300 [02:10<11:25,  2.78s/it]Running Inference:  18%|█▊        | 54/300 [02:13<11:18,  2.76s/it]Running Inference:  18%|█▊        | 55/300 [02:15<10:40,  2.61s/it]Running Inference:  19%|█▊        | 56/300 [02:18<11:01,  2.71s/it]Running Inference:  19%|█▉        | 57/300 [02:21<10:55,  2.70s/it]Running Inference:  19%|█▉        | 58/300 [02:24<11:23,  2.82s/it]Running Inference:  20%|█▉        | 59/300 [02:25<08:45,  2.18s/it]Running Inference:  20%|██        | 60/300 [02:28<09:49,  2.46s/it]Running Inference:  20%|██        | 61/300 [02:31<10:21,  2.60s/it]Running Inference:  21%|██        | 62/300 [02:34<11:23,  2.87s/it]Running Inference:  21%|██        | 63/300 [02:37<11:13,  2.84s/it]Running Inference:  21%|██▏       | 64/300 [02:40<10:48,  2.75s/it]Running Inference:  22%|██▏       | 65/300 [02:44<12:10,  3.11s/it]Running Inference:  22%|██▏       | 66/300 [02:46<11:45,  3.01s/it]Running Inference:  22%|██▏       | 67/300 [02:48<09:42,  2.50s/it]Running Inference:  23%|██▎       | 68/300 [02:50<09:31,  2.46s/it]Running Inference:  23%|██▎       | 69/300 [02:53<10:28,  2.72s/it]Running Inference:  23%|██▎       | 70/300 [02:54<08:25,  2.20s/it]Running Inference:  24%|██▎       | 71/300 [02:56<07:07,  1.87s/it]Running Inference:  24%|██▍       | 72/300 [02:57<06:12,  1.63s/it]Running Inference:  24%|██▍       | 73/300 [02:58<05:43,  1.51s/it]Running Inference:  25%|██▍       | 74/300 [02:59<05:04,  1.35s/it]Running Inference:  25%|██▌       | 75/300 [03:00<04:28,  1.19s/it]Running Inference:  25%|██▌       | 76/300 [03:00<03:58,  1.07s/it]Running Inference:  26%|██▌       | 77/300 [03:03<05:12,  1.40s/it]Running Inference:  26%|██▌       | 78/300 [03:06<07:36,  2.06s/it]Running Inference:  26%|██▋       | 79/300 [03:09<08:30,  2.31s/it]Running Inference:  27%|██▋       | 80/300 [03:10<06:55,  1.89s/it]Running Inference:  27%|██▋       | 81/300 [03:12<06:30,  1.78s/it]Running Inference:  27%|██▋       | 82/300 [03:14<06:53,  1.89s/it]Running Inference:  28%|██▊       | 83/300 [03:15<06:16,  1.74s/it]Running Inference:  28%|██▊       | 84/300 [03:19<08:39,  2.41s/it]Running Inference:  28%|██▊       | 85/300 [03:22<09:42,  2.71s/it]Running Inference:  29%|██▊       | 86/300 [03:24<07:59,  2.24s/it]Running Inference:  29%|██▉       | 87/300 [03:26<08:03,  2.27s/it]Running Inference:  29%|██▉       | 88/300 [03:30<09:48,  2.78s/it]Running Inference:  30%|██▉       | 89/300 [03:32<09:20,  2.66s/it]Running Inference:  30%|███       | 90/300 [03:34<08:04,  2.31s/it]Running Inference:  30%|███       | 91/300 [03:36<07:52,  2.26s/it]Running Inference:  31%|███       | 92/300 [03:37<06:22,  1.84s/it]Running Inference:  31%|███       | 93/300 [03:39<06:22,  1.85s/it]Running Inference:  31%|███▏      | 94/300 [03:40<05:39,  1.65s/it]Running Inference:  32%|███▏      | 95/300 [03:44<08:13,  2.41s/it]Running Inference:  32%|███▏      | 96/300 [03:45<07:16,  2.14s/it]Running Inference:  32%|███▏      | 97/300 [03:47<06:09,  1.82s/it]Running Inference:  33%|███▎      | 98/300 [03:50<07:22,  2.19s/it]Running Inference:  33%|███▎      | 99/300 [03:52<07:22,  2.20s/it]Running Inference:  33%|███▎      | 100/300 [03:53<06:03,  1.82s/it]Running Inference:  34%|███▎      | 101/300 [03:56<07:05,  2.14s/it]Running Inference:  34%|███▍      | 102/300 [03:59<07:54,  2.40s/it]Running Inference:  34%|███▍      | 103/300 [04:01<08:07,  2.48s/it]Running Inference:  35%|███▍      | 104/300 [04:04<08:45,  2.68s/it]Running Inference:  35%|███▌      | 105/300 [04:07<08:56,  2.75s/it]Running Inference:  35%|███▌      | 106/300 [04:10<09:02,  2.79s/it]Running Inference:  36%|███▌      | 107/300 [04:13<08:48,  2.74s/it]Running Inference:  36%|███▌      | 108/300 [04:16<09:13,  2.88s/it]Running Inference:  36%|███▋      | 109/300 [04:19<09:10,  2.88s/it]Running Inference:  37%|███▋      | 110/300 [04:22<09:26,  2.98s/it]Running Inference:  37%|███▋      | 111/300 [04:26<10:31,  3.34s/it]Running Inference:  37%|███▋      | 112/300 [04:27<08:16,  2.64s/it]Running Inference:  38%|███▊      | 113/300 [04:29<07:05,  2.28s/it]Running Inference:  38%|███▊      | 114/300 [04:32<07:53,  2.55s/it]Running Inference:  38%|███▊      | 115/300 [04:36<09:17,  3.02s/it]Running Inference:  39%|███▊      | 116/300 [04:40<09:56,  3.24s/it]Running Inference:  39%|███▉      | 117/300 [04:43<09:26,  3.09s/it]Running Inference:  39%|███▉      | 118/300 [04:44<08:08,  2.68s/it]Running Inference:  40%|███▉      | 119/300 [04:46<06:55,  2.29s/it]Running Inference:  40%|████      | 120/300 [04:48<07:11,  2.40s/it]Running Inference:  40%|████      | 121/300 [04:51<07:30,  2.52s/it]Running Inference:  41%|████      | 122/300 [04:54<07:41,  2.59s/it]Running Inference:  41%|████      | 123/300 [04:56<07:20,  2.49s/it]Running Inference:  41%|████▏     | 124/300 [05:00<08:19,  2.84s/it]Running Inference:  42%|████▏     | 125/300 [05:03<08:27,  2.90s/it]Running Inference:  42%|████▏     | 126/300 [05:06<08:13,  2.84s/it]Running Inference:  42%|████▏     | 127/300 [05:10<09:23,  3.26s/it]Running Inference:  43%|████▎     | 128/300 [05:13<08:52,  3.09s/it]Running Inference:  43%|████▎     | 129/300 [05:14<07:06,  2.49s/it]Running Inference:  43%|████▎     | 130/300 [05:15<05:56,  2.10s/it]Running Inference:  44%|████▎     | 131/300 [05:18<06:27,  2.29s/it]Running Inference:  44%|████▍     | 132/300 [05:18<05:08,  1.84s/it]Running Inference:  44%|████▍     | 133/300 [05:20<04:51,  1.75s/it]Running Inference:  45%|████▍     | 134/300 [05:22<05:24,  1.96s/it]Running Inference:  45%|████▌     | 135/300 [05:23<04:16,  1.55s/it]Running Inference:  45%|████▌     | 136/300 [05:26<05:39,  2.07s/it]Running Inference:  46%|████▌     | 137/300 [05:30<06:51,  2.52s/it]Running Inference:  46%|████▌     | 138/300 [05:31<06:01,  2.23s/it]Running Inference:  46%|████▋     | 139/300 [05:32<05:05,  1.90s/it]Running Inference:  47%|████▋     | 140/300 [05:33<04:16,  1.60s/it]Running Inference:  47%|████▋     | 141/300 [05:35<04:21,  1.65s/it]Running Inference:  47%|████▋     | 142/300 [05:37<04:12,  1.60s/it]Running Inference:  48%|████▊     | 143/300 [05:38<03:49,  1.46s/it]Running Inference:  48%|████▊     | 144/300 [05:42<05:56,  2.28s/it]Running Inference:  48%|████▊     | 145/300 [05:43<05:01,  1.95s/it]Running Inference:  49%|████▊     | 146/300 [05:44<04:28,  1.74s/it]Running Inference:  49%|████▉     | 147/300 [05:47<05:13,  2.05s/it]Running Inference:  49%|████▉     | 148/300 [05:51<06:52,  2.71s/it]Running Inference:  50%|████▉     | 149/300 [05:53<06:20,  2.52s/it]Running Inference:  50%|█████     | 150/300 [05:55<05:22,  2.15s/it]Running Inference:  50%|█████     | 151/300 [05:56<04:20,  1.75s/it]Running Inference:  51%|█████     | 152/300 [05:56<03:40,  1.49s/it]Running Inference:  51%|█████     | 153/300 [06:00<04:57,  2.02s/it]Running Inference:  51%|█████▏    | 154/300 [06:02<05:09,  2.12s/it]Running Inference:  52%|█████▏    | 155/300 [06:03<04:24,  1.83s/it]Running Inference:  52%|█████▏    | 156/300 [06:06<05:00,  2.09s/it]Running Inference:  52%|█████▏    | 157/300 [06:07<04:25,  1.86s/it]Running Inference:  53%|█████▎    | 158/300 [06:11<05:46,  2.44s/it]Running Inference:  53%|█████▎    | 159/300 [06:12<04:43,  2.01s/it]Running Inference:  53%|█████▎    | 160/300 [06:13<04:10,  1.79s/it]Running Inference:  54%|█████▎    | 161/300 [06:18<05:50,  2.52s/it]Running Inference:  54%|█████▍    | 162/300 [06:21<06:35,  2.87s/it]Running Inference:  54%|█████▍    | 163/300 [06:23<05:54,  2.58s/it]Running Inference:  55%|█████▍    | 164/300 [06:27<06:27,  2.85s/it]Running Inference:  55%|█████▌    | 165/300 [06:29<06:22,  2.84s/it]Running Inference:  55%|█████▌    | 166/300 [06:30<05:03,  2.27s/it]Running Inference:  56%|█████▌    | 167/300 [06:33<05:24,  2.44s/it]Running Inference:  56%|█████▌    | 168/300 [06:34<04:23,  1.99s/it]Running Inference:  56%|█████▋    | 169/300 [06:36<04:33,  2.09s/it]Running Inference:  57%|█████▋    | 170/300 [06:38<03:57,  1.83s/it]Running Inference:  57%|█████▋    | 171/300 [06:42<05:28,  2.55s/it]Running Inference:  57%|█████▋    | 172/300 [06:44<05:11,  2.44s/it]Running Inference:  58%|█████▊    | 173/300 [06:47<05:14,  2.47s/it]Running Inference:  58%|█████▊    | 174/300 [06:48<04:12,  2.00s/it]Running Inference:  58%|█████▊    | 175/300 [06:49<03:31,  1.69s/it]Running Inference:  59%|█████▊    | 176/300 [06:52<04:51,  2.35s/it]Running Inference:  59%|█████▉    | 177/300 [06:56<05:48,  2.83s/it]Running Inference:  59%|█████▉    | 178/300 [06:58<05:06,  2.51s/it]Running Inference:  60%|█████▉    | 179/300 [06:59<04:00,  1.99s/it]Running Inference:  60%|██████    | 180/300 [07:02<04:39,  2.33s/it]Running Inference:  60%|██████    | 181/300 [07:04<04:08,  2.09s/it]Running Inference:  61%|██████    | 182/300 [07:07<04:50,  2.46s/it]Running Inference:  61%|██████    | 183/300 [07:08<04:09,  2.13s/it]Running Inference:  61%|██████▏   | 184/300 [07:12<05:18,  2.75s/it]Running Inference:  62%|██████▏   | 185/300 [07:16<05:33,  2.90s/it]Running Inference:  62%|██████▏   | 186/300 [07:18<05:00,  2.63s/it]Running Inference:  62%|██████▏   | 187/300 [07:18<03:54,  2.07s/it]Running Inference:  63%|██████▎   | 188/300 [07:21<04:17,  2.30s/it]Running Inference:  63%|██████▎   | 189/300 [07:22<03:20,  1.81s/it]Running Inference:  63%|██████▎   | 190/300 [07:23<03:05,  1.69s/it]Running Inference:  64%|██████▎   | 191/300 [07:24<02:35,  1.43s/it]Running Inference:  64%|██████▍   | 192/300 [07:28<03:52,  2.15s/it]Running Inference:  64%|██████▍   | 193/300 [07:31<04:22,  2.45s/it]Running Inference:  65%|██████▍   | 194/300 [07:32<03:34,  2.02s/it]Running Inference:  65%|██████▌   | 195/300 [07:33<02:53,  1.66s/it]Running Inference:  65%|██████▌   | 196/300 [07:35<03:19,  1.92s/it]Running Inference:  66%|██████▌   | 197/300 [07:37<03:06,  1.81s/it]Running Inference:  66%|██████▌   | 198/300 [07:38<02:38,  1.55s/it]Running Inference:  66%|██████▋   | 199/300 [07:42<03:57,  2.36s/it]Running Inference:  67%|██████▋   | 200/300 [07:46<04:51,  2.91s/it]Running Inference:  67%|██████▋   | 201/300 [07:49<04:46,  2.89s/it]Running Inference:  67%|██████▋   | 202/300 [07:52<04:44,  2.90s/it]Running Inference:  68%|██████▊   | 203/300 [07:55<04:44,  2.93s/it]Running Inference:  68%|██████▊   | 204/300 [07:57<04:17,  2.68s/it]Running Inference:  68%|██████▊   | 205/300 [07:58<03:26,  2.18s/it]Running Inference:  69%|██████▊   | 206/300 [08:00<03:15,  2.08s/it]Running Inference:  69%|██████▉   | 207/300 [08:03<03:41,  2.38s/it]Running Inference:  69%|██████▉   | 208/300 [08:06<03:47,  2.47s/it]Running Inference:  70%|██████▉   | 209/300 [08:09<03:58,  2.62s/it]Running Inference:  70%|███████   | 210/300 [08:12<04:10,  2.78s/it]Running Inference:  70%|███████   | 211/300 [08:15<04:13,  2.84s/it]Running Inference:  71%|███████   | 212/300 [08:18<04:22,  2.98s/it]Running Inference:  71%|███████   | 213/300 [08:19<03:18,  2.29s/it]Running Inference:  71%|███████▏  | 214/300 [08:22<03:40,  2.56s/it]Running Inference:  72%|███████▏  | 215/300 [08:26<04:17,  3.02s/it]Running Inference:  72%|███████▏  | 216/300 [08:28<03:39,  2.61s/it]Running Inference:  72%|███████▏  | 217/300 [08:31<03:41,  2.67s/it]Running Inference:  73%|███████▎  | 218/300 [08:32<03:14,  2.37s/it]Running Inference:  73%|███████▎  | 219/300 [08:34<02:41,  1.99s/it]Running Inference:  73%|███████▎  | 220/300 [08:34<02:13,  1.66s/it]Running Inference:  74%|███████▎  | 221/300 [08:36<02:04,  1.57s/it]Running Inference:  74%|███████▍  | 222/300 [08:38<02:08,  1.65s/it]Running Inference:  74%|███████▍  | 223/300 [08:38<01:48,  1.41s/it]Running Inference:  75%|███████▍  | 224/300 [08:41<02:18,  1.82s/it]Running Inference:  75%|███████▌  | 225/300 [08:44<02:39,  2.12s/it]Running Inference:  75%|███████▌  | 226/300 [08:47<02:46,  2.25s/it]Running Inference:  76%|███████▌  | 227/300 [08:50<02:59,  2.46s/it]Running Inference:  76%|███████▌  | 228/300 [08:51<02:44,  2.29s/it]Running Inference:  76%|███████▋  | 229/300 [08:53<02:34,  2.17s/it]Running Inference:  77%|███████▋  | 230/300 [08:57<03:11,  2.74s/it]Running Inference:  77%|███████▋  | 231/300 [08:58<02:24,  2.09s/it]Running Inference:  77%|███████▋  | 232/300 [09:01<02:33,  2.26s/it]Running Inference:  78%|███████▊  | 233/300 [09:02<02:13,  1.99s/it]Running Inference:  78%|███████▊  | 234/300 [09:05<02:26,  2.22s/it]Running Inference:  78%|███████▊  | 235/300 [09:08<02:41,  2.48s/it]Running Inference:  79%|███████▊  | 236/300 [09:11<02:51,  2.67s/it]Running Inference:  79%|███████▉  | 237/300 [09:15<03:07,  2.98s/it]Running Inference:  79%|███████▉  | 238/300 [09:18<03:01,  2.93s/it]Running Inference:  80%|███████▉  | 239/300 [09:20<02:42,  2.66s/it]Running Inference:  80%|████████  | 240/300 [09:23<02:56,  2.94s/it]Running Inference:  80%|████████  | 241/300 [09:25<02:41,  2.74s/it]Running Inference:  81%|████████  | 242/300 [09:28<02:41,  2.78s/it]Running Inference:  81%|████████  | 243/300 [09:31<02:36,  2.74s/it]Running Inference:  81%|████████▏ | 244/300 [09:31<01:55,  2.07s/it]Running Inference:  82%|████████▏ | 245/300 [09:35<02:20,  2.56s/it]Running Inference:  82%|████████▏ | 246/300 [09:36<01:52,  2.08s/it]Running Inference:  82%|████████▏ | 247/300 [09:37<01:25,  1.61s/it]Running Inference:  83%|████████▎ | 248/300 [09:39<01:32,  1.78s/it]Running Inference:  83%|████████▎ | 249/300 [09:40<01:20,  1.57s/it]Running Inference:  83%|████████▎ | 250/300 [09:43<01:47,  2.15s/it]Running Inference:  84%|████████▎ | 251/300 [09:45<01:39,  2.04s/it]Running Inference:  84%|████████▍ | 252/300 [09:48<01:45,  2.20s/it]Running Inference:  84%|████████▍ | 253/300 [09:50<01:45,  2.25s/it]Running Inference:  85%|████████▍ | 254/300 [09:51<01:22,  1.79s/it]Running Inference:  85%|████████▌ | 255/300 [09:53<01:32,  2.05s/it]Running Inference:  85%|████████▌ | 256/300 [09:54<01:12,  1.65s/it]Running Inference:  86%|████████▌ | 257/300 [09:57<01:28,  2.07s/it]Running Inference:  86%|████████▌ | 258/300 [09:59<01:26,  2.06s/it]Running Inference:  86%|████████▋ | 259/300 [10:02<01:37,  2.38s/it]Running Inference:  87%|████████▋ | 260/300 [10:05<01:36,  2.40s/it]Running Inference:  87%|████████▋ | 261/300 [10:06<01:16,  1.97s/it]Running Inference:  87%|████████▋ | 262/300 [10:07<01:11,  1.88s/it]Running Inference:  88%|████████▊ | 263/300 [10:08<00:57,  1.57s/it]Running Inference:  88%|████████▊ | 264/300 [10:11<01:09,  1.94s/it]Running Inference:  88%|████████▊ | 265/300 [10:14<01:17,  2.21s/it]Running Inference:  89%|████████▊ | 266/300 [10:17<01:20,  2.37s/it]Running Inference:  89%|████████▉ | 267/300 [10:17<01:00,  1.83s/it]Running Inference:  89%|████████▉ | 268/300 [10:21<01:12,  2.27s/it]Running Inference:  90%|████████▉ | 269/300 [10:25<01:27,  2.82s/it]Running Inference:  90%|█████████ | 270/300 [10:25<01:05,  2.20s/it]Running Inference:  90%|█████████ | 271/300 [10:27<00:59,  2.04s/it]Running Inference:  91%|█████████ | 272/300 [10:31<01:14,  2.66s/it]Running Inference:  91%|█████████ | 273/300 [10:34<01:16,  2.82s/it]Running Inference:  91%|█████████▏| 274/300 [10:37<01:13,  2.84s/it]Running Inference:  92%|█████████▏| 275/300 [10:38<00:55,  2.23s/it]Running Inference:  92%|█████████▏| 276/300 [10:40<00:50,  2.11s/it]Running Inference:  92%|█████████▏| 277/300 [10:42<00:50,  2.18s/it]Running Inference:  93%|█████████▎| 278/300 [10:45<00:50,  2.31s/it]Running Inference:  93%|█████████▎| 279/300 [10:47<00:49,  2.37s/it]Running Inference:  93%|█████████▎| 280/300 [10:50<00:46,  2.30s/it]Running Inference:  94%|█████████▎| 281/300 [10:54<00:54,  2.86s/it]Running Inference:  94%|█████████▍| 282/300 [10:56<00:48,  2.69s/it]Running Inference:  94%|█████████▍| 283/300 [10:58<00:40,  2.41s/it]Running Inference:  95%|█████████▍| 284/300 [11:02<00:47,  2.95s/it]Running Inference:  95%|█████████▌| 285/300 [11:03<00:36,  2.42s/it]Running Inference:  95%|█████████▌| 286/300 [11:04<00:27,  1.94s/it]Running Inference:  96%|█████████▌| 287/300 [11:08<00:33,  2.61s/it]Running Inference:  96%|█████████▌| 288/300 [11:11<00:32,  2.67s/it]Running Inference:  96%|█████████▋| 289/300 [11:15<00:34,  3.09s/it]Running Inference:  97%|█████████▋| 290/300 [11:17<00:26,  2.69s/it]Running Inference:  97%|█████████▋| 291/300 [11:19<00:23,  2.64s/it]Running Inference:  97%|█████████▋| 292/300 [11:23<00:23,  2.92s/it]Running Inference:  98%|█████████▊| 293/300 [11:27<00:23,  3.30s/it]Running Inference:  98%|█████████▊| 294/300 [11:31<00:20,  3.40s/it]Running Inference:  98%|█████████▊| 295/300 [11:32<00:14,  2.85s/it]Running Inference:  99%|█████████▊| 296/300 [11:33<00:09,  2.26s/it]Running Inference:  99%|█████████▉| 297/300 [11:35<00:05,  1.99s/it]Running Inference:  99%|█████████▉| 298/300 [11:38<00:04,  2.37s/it]Running Inference: 100%|█████████▉| 299/300 [11:41<00:02,  2.59s/it]Running Inference: 100%|██████████| 300/300 [11:45<00:00,  2.97s/it]Running Inference: 100%|██████████| 300/300 [11:45<00:00,  2.35s/it]
2025-12-15 02:14:10,898 - INFO - Inference completed.
2025-12-15 02:14:10,908 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.20/predictions.csv
2025-12-15 02:14:10,908 - INFO - Calculating metrics for dataset: longbench
2025-12-15 02:14:10,916 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.20/metrics.json
2025-12-15 02:14:10,916 - INFO - Metrics:
18.83
2025-12-15 02:14:10,917 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa_e, ratio=0.2) on GPU 2

----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.3
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 02:14:17,348 - INFO - Set deterministic seeds to 42
2025-12-15 02:14:17,348 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.3,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 02:14:17,348 - INFO - Starting evaluation run...
2025-12-15 02:14:17,348 - INFO - Output directory set to: longbenchresult
2025-12-15 02:14:17,348 - INFO - Set StreamingLLMPress compression_ratio to 0.3
2025-12-15 02:14:17,348 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 02:14:17,348 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 02:14:17,348 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 106.29it/s]
Device set to use cuda:0
2025-12-15 02:14:28,232 - INFO - Model pipeline loaded.
2025-12-15 02:14:28,233 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
2025-12-15 02:14:31,325 - INFO - Dataset loaded with 300 entries.
2025-12-15 02:14:31,325 - INFO - Dataset processed with 300 entries.
2025-12-15 02:14:31,361 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:01<07:04,  1.42s/it]Running Inference:   1%|          | 2/300 [00:02<06:04,  1.22s/it]Running Inference:   1%|          | 3/300 [00:05<09:24,  1.90s/it]Running Inference:   1%|▏         | 4/300 [00:08<12:45,  2.59s/it]Running Inference:   2%|▏         | 5/300 [00:09<09:49,  2.00s/it]Running Inference:   2%|▏         | 6/300 [00:10<08:01,  1.64s/it]Running Inference:   2%|▏         | 7/300 [00:14<10:36,  2.17s/it]Running Inference:   3%|▎         | 8/300 [00:15<08:59,  1.85s/it]Running Inference:   3%|▎         | 9/300 [00:18<10:33,  2.18s/it]Running Inference:   3%|▎         | 10/300 [00:20<11:28,  2.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:23<12:01,  2.50s/it]Running Inference:   4%|▍         | 12/300 [00:24<09:37,  2.01s/it]Running Inference:   4%|▍         | 13/300 [00:25<08:11,  1.71s/it]Running Inference:   5%|▍         | 14/300 [00:29<11:50,  2.48s/it]Running Inference:   5%|▌         | 15/300 [00:32<12:40,  2.67s/it]Running Inference:   5%|▌         | 16/300 [00:37<14:45,  3.12s/it]Running Inference:   6%|▌         | 17/300 [00:40<15:28,  3.28s/it]Running Inference:   6%|▌         | 18/300 [00:43<14:41,  3.13s/it]Running Inference:   6%|▋         | 19/300 [00:47<15:52,  3.39s/it]Running Inference:   7%|▋         | 20/300 [00:47<11:38,  2.50s/it]Running Inference:   7%|▋         | 21/300 [00:52<13:48,  2.97s/it]Running Inference:   7%|▋         | 22/300 [00:53<12:11,  2.63s/it]Running Inference:   8%|▊         | 23/300 [00:56<12:49,  2.78s/it]Running Inference:   8%|▊         | 24/300 [00:59<13:03,  2.84s/it]Running Inference:   8%|▊         | 25/300 [01:02<12:38,  2.76s/it]Running Inference:   9%|▊         | 26/300 [01:05<13:09,  2.88s/it]Running Inference:   9%|▉         | 27/300 [01:06<10:24,  2.29s/it]Running Inference:   9%|▉         | 28/300 [01:09<11:24,  2.52s/it]Running Inference:  10%|▉         | 29/300 [01:10<09:35,  2.12s/it]Running Inference:  10%|█         | 30/300 [01:14<11:43,  2.61s/it]Running Inference:  10%|█         | 31/300 [01:17<12:42,  2.84s/it]Running Inference:  11%|█         | 32/300 [01:21<14:06,  3.16s/it]Running Inference:  11%|█         | 33/300 [01:26<15:23,  3.46s/it]Running Inference:  11%|█▏        | 34/300 [01:29<15:15,  3.44s/it]Running Inference:  12%|█▏        | 35/300 [01:31<12:58,  2.94s/it]Running Inference:  12%|█▏        | 36/300 [01:35<14:25,  3.28s/it]Running Inference:  12%|█▏        | 37/300 [01:37<12:26,  2.84s/it]Running Inference:  13%|█▎        | 38/300 [01:38<10:43,  2.46s/it]Running Inference:  13%|█▎        | 39/300 [01:39<08:42,  2.00s/it]Running Inference:  13%|█▎        | 40/300 [01:41<08:21,  1.93s/it]Running Inference:  14%|█▎        | 41/300 [01:44<09:28,  2.20s/it]Running Inference:  14%|█▍        | 42/300 [01:45<08:08,  1.89s/it]Running Inference:  14%|█▍        | 43/300 [01:47<08:15,  1.93s/it]Running Inference:  15%|█▍        | 44/300 [01:48<06:53,  1.62s/it]Running Inference:  15%|█▌        | 45/300 [01:52<10:00,  2.36s/it]Running Inference:  15%|█▌        | 46/300 [01:54<09:25,  2.23s/it]Running Inference:  16%|█▌        | 47/300 [01:56<09:44,  2.31s/it]Running Inference:  16%|█▌        | 48/300 [01:59<10:35,  2.52s/it]Running Inference:  16%|█▋        | 49/300 [02:02<10:37,  2.54s/it]Running Inference:  17%|█▋        | 50/300 [02:04<09:37,  2.31s/it]Running Inference:  17%|█▋        | 51/300 [02:06<09:23,  2.26s/it]Running Inference:  17%|█▋        | 52/300 [02:09<10:13,  2.47s/it]Running Inference:  18%|█▊        | 53/300 [02:12<10:46,  2.62s/it]Running Inference:  18%|█▊        | 54/300 [02:14<10:50,  2.64s/it]Running Inference:  18%|█▊        | 55/300 [02:17<10:17,  2.52s/it]Running Inference:  19%|█▊        | 56/300 [02:19<10:35,  2.61s/it]Running Inference:  19%|█▉        | 57/300 [02:22<10:37,  2.62s/it]Running Inference:  19%|█▉        | 58/300 [02:25<11:09,  2.77s/it]Running Inference:  20%|█▉        | 59/300 [02:26<08:35,  2.14s/it]Running Inference:  20%|██        | 60/300 [02:29<09:42,  2.43s/it]Running Inference:  20%|██        | 61/300 [02:31<08:37,  2.16s/it]Running Inference:  21%|██        | 62/300 [02:34<10:09,  2.56s/it]Running Inference:  21%|██        | 63/300 [02:37<10:21,  2.62s/it]Running Inference:  21%|██▏       | 64/300 [02:39<10:11,  2.59s/it]Running Inference:  22%|██▏       | 65/300 [02:43<10:53,  2.78s/it]Running Inference:  22%|██▏       | 66/300 [02:45<10:50,  2.78s/it]Running Inference:  22%|██▏       | 67/300 [02:48<10:55,  2.81s/it]Running Inference:  23%|██▎       | 68/300 [02:51<10:18,  2.67s/it]Running Inference:  23%|██▎       | 69/300 [02:54<11:01,  2.86s/it]Running Inference:  23%|██▎       | 70/300 [02:55<08:47,  2.30s/it]Running Inference:  24%|██▎       | 71/300 [02:56<07:23,  1.94s/it]Running Inference:  24%|██▍       | 72/300 [02:57<06:23,  1.68s/it]Running Inference:  24%|██▍       | 73/300 [02:58<05:51,  1.55s/it]Running Inference:  25%|██▍       | 74/300 [02:59<05:10,  1.37s/it]Running Inference:  25%|██▌       | 75/300 [03:00<04:31,  1.21s/it]Running Inference:  25%|██▌       | 76/300 [03:01<04:04,  1.09s/it]Running Inference:  26%|██▌       | 77/300 [03:03<05:15,  1.41s/it]Running Inference:  26%|██▌       | 78/300 [03:07<07:38,  2.07s/it]Running Inference:  26%|██▋       | 79/300 [03:10<08:31,  2.32s/it]Running Inference:  27%|██▋       | 80/300 [03:10<06:55,  1.89s/it]Running Inference:  27%|██▋       | 81/300 [03:12<06:30,  1.78s/it]Running Inference:  27%|██▋       | 82/300 [03:15<07:26,  2.05s/it]Running Inference:  28%|██▊       | 83/300 [03:16<07:04,  1.95s/it]Running Inference:  28%|██▊       | 84/300 [03:18<07:06,  1.98s/it]Running Inference:  28%|██▊       | 85/300 [03:20<07:05,  1.98s/it]Running Inference:  29%|██▊       | 86/300 [03:22<06:10,  1.73s/it]Running Inference:  29%|██▉       | 87/300 [03:24<06:44,  1.90s/it]Running Inference:  29%|██▉       | 88/300 [03:28<08:53,  2.51s/it]Running Inference:  30%|██▉       | 89/300 [03:30<08:42,  2.48s/it]Running Inference:  30%|███       | 90/300 [03:32<07:37,  2.18s/it]Running Inference:  30%|███       | 91/300 [03:34<07:17,  2.10s/it]Running Inference:  31%|███       | 92/300 [03:34<05:58,  1.72s/it]Running Inference:  31%|███       | 93/300 [03:36<06:05,  1.77s/it]Running Inference:  31%|███▏      | 94/300 [03:37<05:27,  1.59s/it]Running Inference:  32%|███▏      | 95/300 [03:42<08:00,  2.34s/it]Running Inference:  32%|███▏      | 96/300 [03:43<07:01,  2.07s/it]Running Inference:  32%|███▏      | 97/300 [03:46<07:53,  2.33s/it]Running Inference:  33%|███▎      | 98/300 [03:49<08:34,  2.55s/it]Running Inference:  33%|███▎      | 99/300 [03:51<08:12,  2.45s/it]Running Inference:  33%|███▎      | 100/300 [03:52<06:37,  1.99s/it]Running Inference:  34%|███▎      | 101/300 [03:55<07:29,  2.26s/it]Running Inference:  34%|███▍      | 102/300 [03:58<08:10,  2.48s/it]Running Inference:  34%|███▍      | 103/300 [04:01<08:17,  2.53s/it]Running Inference:  35%|███▍      | 104/300 [04:04<08:52,  2.71s/it]Running Inference:  35%|███▌      | 105/300 [04:07<08:59,  2.77s/it]Running Inference:  35%|███▌      | 106/300 [04:10<09:04,  2.81s/it]Running Inference:  36%|███▌      | 107/300 [04:12<08:50,  2.75s/it]Running Inference:  36%|███▌      | 108/300 [04:15<09:13,  2.88s/it]Running Inference:  36%|███▋      | 109/300 [04:18<09:09,  2.88s/it]Running Inference:  37%|███▋      | 110/300 [04:21<08:57,  2.83s/it]Running Inference:  37%|███▋      | 111/300 [04:25<10:10,  3.23s/it]Running Inference:  37%|███▋      | 112/300 [04:26<07:50,  2.50s/it]Running Inference:  38%|███▊      | 113/300 [04:28<07:19,  2.35s/it]Running Inference:  38%|███▊      | 114/300 [04:30<06:55,  2.23s/it]Running Inference:  38%|███▊      | 115/300 [04:34<08:37,  2.80s/it]Running Inference:  39%|███▊      | 116/300 [04:38<09:27,  3.08s/it]Running Inference:  39%|███▉      | 117/300 [04:40<09:05,  2.98s/it]Running Inference:  39%|███▉      | 118/300 [04:42<07:51,  2.59s/it]Running Inference:  40%|███▉      | 119/300 [04:44<06:43,  2.23s/it]Running Inference:  40%|████      | 120/300 [04:46<07:03,  2.35s/it]Running Inference:  40%|████      | 121/300 [04:49<07:24,  2.48s/it]Running Inference:  41%|████      | 122/300 [04:52<07:36,  2.56s/it]Running Inference:  41%|████      | 123/300 [04:54<07:16,  2.47s/it]Running Inference:  41%|████▏     | 124/300 [04:58<08:16,  2.82s/it]Running Inference:  42%|████▏     | 125/300 [05:01<08:25,  2.89s/it]Running Inference:  42%|████▏     | 126/300 [05:03<07:36,  2.62s/it]Running Inference:  42%|████▏     | 127/300 [05:05<07:07,  2.47s/it]Running Inference:  43%|████▎     | 128/300 [05:05<05:29,  1.91s/it]Running Inference:  43%|████▎     | 129/300 [05:06<04:44,  1.67s/it]Running Inference:  43%|████▎     | 130/300 [05:08<04:17,  1.52s/it]Running Inference:  44%|████▎     | 131/300 [05:09<03:53,  1.38s/it]Running Inference:  44%|████▍     | 132/300 [05:09<03:21,  1.20s/it]Running Inference:  44%|████▍     | 133/300 [05:11<03:35,  1.29s/it]Running Inference:  45%|████▍     | 134/300 [05:13<04:31,  1.63s/it]Running Inference:  45%|████▌     | 135/300 [05:14<03:38,  1.33s/it]Running Inference:  45%|████▌     | 136/300 [05:17<05:12,  1.91s/it]Running Inference:  46%|████▌     | 137/300 [05:19<05:18,  1.95s/it]Running Inference:  46%|████▌     | 138/300 [05:21<04:56,  1.83s/it]Running Inference:  46%|████▋     | 139/300 [05:22<04:19,  1.61s/it]Running Inference:  47%|████▋     | 140/300 [05:23<03:45,  1.41s/it]Running Inference:  47%|████▋     | 141/300 [05:25<03:59,  1.51s/it]Running Inference:  47%|████▋     | 142/300 [05:26<03:56,  1.50s/it]Running Inference:  48%|████▊     | 143/300 [05:27<03:39,  1.40s/it]Running Inference:  48%|████▊     | 144/300 [05:31<05:48,  2.24s/it]Running Inference:  48%|████▊     | 145/300 [05:33<04:56,  1.91s/it]Running Inference:  49%|████▊     | 146/300 [05:34<04:25,  1.72s/it]Running Inference:  49%|████▉     | 147/300 [05:37<05:11,  2.04s/it]Running Inference:  49%|████▉     | 148/300 [05:41<06:51,  2.70s/it]Running Inference:  50%|████▉     | 149/300 [05:45<07:30,  2.98s/it]Running Inference:  50%|█████     | 150/300 [05:46<06:10,  2.47s/it]Running Inference:  50%|█████     | 151/300 [05:47<04:54,  1.97s/it]Running Inference:  51%|█████     | 152/300 [05:48<04:10,  1.69s/it]Running Inference:  51%|█████     | 153/300 [05:51<05:18,  2.16s/it]Running Inference:  51%|█████▏    | 154/300 [05:53<05:22,  2.21s/it]Running Inference:  52%|█████▏    | 155/300 [05:55<04:44,  1.96s/it]Running Inference:  52%|█████▏    | 156/300 [05:57<05:14,  2.18s/it]Running Inference:  52%|█████▏    | 157/300 [05:58<04:20,  1.82s/it]Running Inference:  53%|█████▎    | 158/300 [06:02<05:42,  2.42s/it]Running Inference:  53%|█████▎    | 159/300 [06:03<04:40,  1.99s/it]Running Inference:  53%|█████▎    | 160/300 [06:04<04:08,  1.77s/it]Running Inference:  54%|█████▎    | 161/300 [06:09<05:48,  2.51s/it]Running Inference:  54%|█████▍    | 162/300 [06:12<06:34,  2.86s/it]Running Inference:  54%|█████▍    | 163/300 [06:14<05:52,  2.57s/it]Running Inference:  55%|█████▍    | 164/300 [06:18<06:51,  3.03s/it]Running Inference:  55%|█████▌    | 165/300 [06:21<06:39,  2.96s/it]Running Inference:  55%|█████▌    | 166/300 [06:22<05:15,  2.35s/it]Running Inference:  56%|█████▌    | 167/300 [06:24<04:58,  2.24s/it]Running Inference:  56%|█████▌    | 168/300 [06:25<04:13,  1.92s/it]Running Inference:  56%|█████▋    | 169/300 [06:28<04:27,  2.04s/it]Running Inference:  57%|█████▋    | 170/300 [06:29<03:52,  1.79s/it]Running Inference:  57%|█████▋    | 171/300 [06:33<05:25,  2.52s/it]Running Inference:  57%|█████▋    | 172/300 [06:35<05:09,  2.42s/it]Running Inference:  58%|█████▊    | 173/300 [06:38<05:15,  2.49s/it]Running Inference:  58%|█████▊    | 174/300 [06:39<04:13,  2.01s/it]Running Inference:  58%|█████▊    | 175/300 [06:40<03:32,  1.70s/it]Running Inference:  59%|█████▊    | 176/300 [06:44<04:51,  2.35s/it]Running Inference:  59%|█████▉    | 177/300 [06:47<05:47,  2.83s/it]Running Inference:  59%|█████▉    | 178/300 [06:49<05:06,  2.51s/it]Running Inference:  60%|█████▉    | 179/300 [06:50<04:00,  1.98s/it]Running Inference:  60%|██████    | 180/300 [06:53<04:38,  2.32s/it]Running Inference:  60%|██████    | 181/300 [06:55<04:08,  2.09s/it]Running Inference:  61%|██████    | 182/300 [06:58<04:50,  2.46s/it]Running Inference:  61%|██████    | 183/300 [06:59<04:08,  2.12s/it]Running Inference:  61%|██████▏   | 184/300 [07:03<05:18,  2.74s/it]Running Inference:  62%|██████▏   | 185/300 [07:07<05:32,  2.89s/it]Running Inference:  62%|██████▏   | 186/300 [07:09<04:59,  2.63s/it]Running Inference:  62%|██████▏   | 187/300 [07:10<03:53,  2.07s/it]Running Inference:  63%|██████▎   | 188/300 [07:12<04:17,  2.30s/it]Running Inference:  63%|██████▎   | 189/300 [07:13<03:20,  1.81s/it]Running Inference:  63%|██████▎   | 190/300 [07:15<03:24,  1.86s/it]Running Inference:  64%|██████▎   | 191/300 [07:16<02:48,  1.54s/it]Running Inference:  64%|██████▍   | 192/300 [07:20<04:01,  2.23s/it]Running Inference:  64%|██████▍   | 193/300 [07:23<04:28,  2.51s/it]Running Inference:  65%|██████▍   | 194/300 [07:24<03:38,  2.06s/it]Running Inference:  65%|██████▌   | 195/300 [07:26<03:55,  2.25s/it]Running Inference:  65%|██████▌   | 196/300 [07:29<04:02,  2.33s/it]Running Inference:  66%|██████▌   | 197/300 [07:31<03:35,  2.10s/it]Running Inference:  66%|██████▌   | 198/300 [07:32<03:09,  1.85s/it]Running Inference:  66%|██████▋   | 199/300 [07:36<04:19,  2.57s/it]Running Inference:  67%|██████▋   | 200/300 [07:40<05:05,  3.06s/it]Running Inference:  67%|██████▋   | 201/300 [07:43<04:55,  2.99s/it]Running Inference:  67%|██████▋   | 202/300 [07:46<04:50,  2.96s/it]Running Inference:  68%|██████▊   | 203/300 [07:48<04:25,  2.73s/it]Running Inference:  68%|██████▊   | 204/300 [07:50<04:04,  2.54s/it]Running Inference:  68%|██████▊   | 205/300 [07:51<03:17,  2.08s/it]Running Inference:  69%|██████▊   | 206/300 [07:53<03:08,  2.01s/it]Running Inference:  69%|██████▉   | 207/300 [07:56<03:37,  2.34s/it]Running Inference:  69%|██████▉   | 208/300 [07:59<03:44,  2.44s/it]Running Inference:  70%|██████▉   | 209/300 [08:01<03:23,  2.23s/it]Running Inference:  70%|███████   | 210/300 [08:04<03:46,  2.51s/it]Running Inference:  70%|███████   | 211/300 [08:07<03:56,  2.66s/it]Running Inference:  71%|███████   | 212/300 [08:10<04:10,  2.85s/it]Running Inference:  71%|███████   | 213/300 [08:11<03:10,  2.19s/it]Running Inference:  71%|███████▏  | 214/300 [08:14<03:34,  2.50s/it]Running Inference:  72%|███████▏  | 215/300 [08:18<04:12,  2.98s/it]Running Inference:  72%|███████▏  | 216/300 [08:20<03:36,  2.57s/it]Running Inference:  72%|███████▏  | 217/300 [08:22<03:18,  2.39s/it]Running Inference:  73%|███████▎  | 218/300 [08:23<02:58,  2.18s/it]Running Inference:  73%|███████▎  | 219/300 [08:24<02:30,  1.85s/it]Running Inference:  73%|███████▎  | 220/300 [08:25<02:05,  1.57s/it]Running Inference:  74%|███████▎  | 221/300 [08:27<01:58,  1.50s/it]Running Inference:  74%|███████▍  | 222/300 [08:29<02:04,  1.60s/it]Running Inference:  74%|███████▍  | 223/300 [08:29<01:46,  1.38s/it]Running Inference:  75%|███████▍  | 224/300 [08:32<02:16,  1.80s/it]Running Inference:  75%|███████▌  | 225/300 [08:36<02:56,  2.36s/it]Running Inference:  75%|███████▌  | 226/300 [08:38<02:58,  2.41s/it]Running Inference:  76%|███████▌  | 227/300 [08:41<03:07,  2.57s/it]Running Inference:  76%|███████▌  | 228/300 [08:44<03:02,  2.53s/it]Running Inference:  76%|███████▋  | 229/300 [08:46<02:46,  2.34s/it]Running Inference:  77%|███████▋  | 230/300 [08:50<03:19,  2.86s/it]Running Inference:  77%|███████▋  | 231/300 [08:50<02:30,  2.18s/it]Running Inference:  77%|███████▋  | 232/300 [08:53<02:37,  2.32s/it]Running Inference:  78%|███████▊  | 233/300 [08:54<02:15,  2.03s/it]Running Inference:  78%|███████▊  | 234/300 [08:57<02:28,  2.24s/it]Running Inference:  78%|███████▊  | 235/300 [09:00<02:42,  2.50s/it]Running Inference:  79%|███████▊  | 236/300 [09:03<02:51,  2.68s/it]Running Inference:  79%|███████▉  | 237/300 [09:06<02:44,  2.61s/it]Running Inference:  79%|███████▉  | 238/300 [09:09<02:45,  2.68s/it]Running Inference:  80%|███████▉  | 239/300 [09:11<02:44,  2.70s/it]Running Inference:  80%|████████  | 240/300 [09:15<02:57,  2.96s/it]Running Inference:  80%|████████  | 241/300 [09:17<02:42,  2.76s/it]Running Inference:  81%|████████  | 242/300 [09:20<02:41,  2.79s/it]Running Inference:  81%|████████  | 243/300 [09:23<02:36,  2.75s/it]Running Inference:  81%|████████▏ | 244/300 [09:23<01:56,  2.07s/it]Running Inference:  82%|████████▏ | 245/300 [09:27<02:21,  2.56s/it]Running Inference:  82%|████████▏ | 246/300 [09:28<01:52,  2.08s/it]Running Inference:  82%|████████▏ | 247/300 [09:28<01:25,  1.61s/it]Running Inference:  83%|████████▎ | 248/300 [09:30<01:30,  1.74s/it]Running Inference:  83%|████████▎ | 249/300 [09:31<01:18,  1.54s/it]Running Inference:  83%|████████▎ | 250/300 [09:35<01:46,  2.13s/it]Running Inference:  84%|████████▎ | 251/300 [09:37<01:39,  2.02s/it]Running Inference:  84%|████████▍ | 252/300 [09:39<01:44,  2.18s/it]Running Inference:  84%|████████▍ | 253/300 [09:42<01:45,  2.24s/it]Running Inference:  85%|████████▍ | 254/300 [09:42<01:21,  1.78s/it]Running Inference:  85%|████████▌ | 255/300 [09:45<01:31,  2.04s/it]Running Inference:  85%|████████▌ | 256/300 [09:46<01:11,  1.63s/it]Running Inference:  86%|████████▌ | 257/300 [09:48<01:16,  1.79s/it]Running Inference:  86%|████████▌ | 258/300 [09:50<01:18,  1.87s/it]Running Inference:  86%|████████▋ | 259/300 [09:53<01:31,  2.24s/it]Running Inference:  87%|████████▋ | 260/300 [09:55<01:32,  2.30s/it]Running Inference:  87%|████████▋ | 261/300 [09:56<01:14,  1.90s/it]Running Inference:  87%|████████▋ | 262/300 [09:59<01:21,  2.13s/it]Running Inference:  88%|████████▊ | 263/300 [10:00<01:04,  1.74s/it]Running Inference:  88%|████████▊ | 264/300 [10:03<01:14,  2.06s/it]Running Inference:  88%|████████▊ | 265/300 [10:06<01:20,  2.30s/it]Running Inference:  89%|████████▊ | 266/300 [10:08<01:22,  2.43s/it]Running Inference:  89%|████████▉ | 267/300 [10:09<01:01,  1.87s/it]Running Inference:  89%|████████▉ | 268/300 [10:12<01:13,  2.30s/it]Running Inference:  90%|████████▉ | 269/300 [10:16<01:27,  2.83s/it]Running Inference:  90%|█████████ | 270/300 [10:17<01:06,  2.21s/it]Running Inference:  90%|█████████ | 271/300 [10:19<00:59,  2.05s/it]Running Inference:  91%|█████████ | 272/300 [10:23<01:14,  2.66s/it]Running Inference:  91%|█████████ | 273/300 [10:25<01:07,  2.52s/it]Running Inference:  91%|█████████▏| 274/300 [10:28<01:08,  2.62s/it]Running Inference:  92%|█████████▏| 275/300 [10:29<00:52,  2.08s/it]Running Inference:  92%|█████████▏| 276/300 [10:30<00:48,  2.00s/it]Running Inference:  92%|█████████▏| 277/300 [10:33<00:48,  2.10s/it]Running Inference:  93%|█████████▎| 278/300 [10:35<00:49,  2.25s/it]Running Inference:  93%|█████████▎| 279/300 [10:38<00:48,  2.33s/it]Running Inference:  93%|█████████▎| 280/300 [10:40<00:45,  2.28s/it]Running Inference:  94%|█████████▎| 281/300 [10:44<00:54,  2.84s/it]Running Inference:  94%|█████████▍| 282/300 [10:46<00:48,  2.67s/it]Running Inference:  94%|█████████▍| 283/300 [10:50<00:50,  2.97s/it]Running Inference:  95%|█████████▍| 284/300 [10:54<00:53,  3.33s/it]Running Inference:  95%|█████████▌| 285/300 [10:58<00:49,  3.32s/it]Running Inference:  95%|█████████▌| 286/300 [10:58<00:36,  2.58s/it]Running Inference:  96%|█████████▌| 287/300 [11:03<00:39,  3.06s/it]Running Inference:  96%|█████████▌| 288/300 [11:05<00:35,  2.98s/it]Running Inference:  96%|█████████▋| 289/300 [11:10<00:36,  3.31s/it]Running Inference:  97%|█████████▋| 290/300 [11:11<00:28,  2.85s/it]Running Inference:  97%|█████████▋| 291/300 [11:14<00:24,  2.75s/it]Running Inference:  97%|█████████▋| 292/300 [11:17<00:23,  3.00s/it]Running Inference:  98%|█████████▊| 293/300 [11:22<00:23,  3.35s/it]Running Inference:  98%|█████████▊| 294/300 [11:25<00:20,  3.43s/it]Running Inference:  98%|█████████▊| 295/300 [11:27<00:14,  2.87s/it]Running Inference:  99%|█████████▊| 296/300 [11:30<00:11,  2.84s/it]Running Inference:  99%|█████████▉| 297/300 [11:31<00:07,  2.40s/it]Running Inference:  99%|█████████▉| 298/300 [11:34<00:05,  2.65s/it]Running Inference: 100%|█████████▉| 299/300 [11:35<00:02,  2.24s/it]Running Inference: 100%|██████████| 300/300 [11:39<00:00,  2.73s/it]Running Inference: 100%|██████████| 300/300 [11:39<00:00,  2.33s/it]
2025-12-15 02:26:11,156 - INFO - Inference completed.
2025-12-15 02:26:11,166 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.30/predictions.csv
2025-12-15 02:26:11,166 - INFO - Calculating metrics for dataset: longbench
2025-12-15 02:26:11,174 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.30/metrics.json
2025-12-15 02:26:11,174 - INFO - Metrics:
19.01
2025-12-15 02:26:11,175 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa_e, ratio=0.3) on GPU 2

----------------------------------------
Task: 2wikimqa_e | Compression Ratio: 0.5
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 02:26:17,624 - INFO - Set deterministic seeds to 42
2025-12-15 02:26:17,624 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "2wikimqa_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.5,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 02:26:17,624 - INFO - Starting evaluation run...
2025-12-15 02:26:17,624 - INFO - Output directory set to: longbenchresult
2025-12-15 02:26:17,625 - INFO - Set StreamingLLMPress compression_ratio to 0.5
2025-12-15 02:26:17,625 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 02:26:17,625 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 02:26:17,625 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.70it/s]
Device set to use cuda:0
2025-12-15 02:26:28,814 - INFO - Model pipeline loaded.
2025-12-15 02:26:28,814 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: 2wikimqa_e)
2025-12-15 02:26:31,718 - INFO - Dataset loaded with 300 entries.
2025-12-15 02:26:31,719 - INFO - Dataset processed with 300 entries.
2025-12-15 02:26:31,753 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:01<06:59,  1.40s/it]Running Inference:   1%|          | 2/300 [00:02<06:02,  1.22s/it]Running Inference:   1%|          | 3/300 [00:05<09:22,  1.89s/it]Running Inference:   1%|▏         | 4/300 [00:08<12:43,  2.58s/it]Running Inference:   2%|▏         | 5/300 [00:09<09:51,  2.01s/it]Running Inference:   2%|▏         | 6/300 [00:10<08:03,  1.64s/it]Running Inference:   2%|▏         | 7/300 [00:14<10:36,  2.17s/it]Running Inference:   3%|▎         | 8/300 [00:15<09:45,  2.01s/it]Running Inference:   3%|▎         | 9/300 [00:18<11:04,  2.28s/it]Running Inference:   3%|▎         | 10/300 [00:21<11:49,  2.45s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [00:22<09:39,  2.00s/it]Running Inference:   4%|▍         | 12/300 [00:23<07:58,  1.66s/it]Running Inference:   4%|▍         | 13/300 [00:26<10:05,  2.11s/it]Running Inference:   5%|▍         | 14/300 [00:30<13:08,  2.76s/it]Running Inference:   5%|▌         | 15/300 [00:33<13:34,  2.86s/it]Running Inference:   5%|▌         | 16/300 [00:37<15:21,  3.25s/it]Running Inference:   6%|▌         | 17/300 [00:40<14:20,  3.04s/it]Running Inference:   6%|▌         | 18/300 [00:43<13:52,  2.95s/it]Running Inference:   6%|▋         | 19/300 [00:47<15:17,  3.26s/it]Running Inference:   7%|▋         | 20/300 [00:47<11:13,  2.41s/it]Running Inference:   7%|▋         | 21/300 [00:51<13:30,  2.90s/it]Running Inference:   7%|▋         | 22/300 [00:53<11:57,  2.58s/it]Running Inference:   8%|▊         | 23/300 [00:56<12:38,  2.74s/it]Running Inference:   8%|▊         | 24/300 [00:59<12:55,  2.81s/it]Running Inference:   8%|▊         | 25/300 [01:02<12:38,  2.76s/it]Running Inference:   9%|▊         | 26/300 [01:05<13:08,  2.88s/it]Running Inference:   9%|▉         | 27/300 [01:06<10:23,  2.28s/it]Running Inference:   9%|▉         | 28/300 [01:09<11:23,  2.51s/it]Running Inference:  10%|▉         | 29/300 [01:10<09:34,  2.12s/it]Running Inference:  10%|█         | 30/300 [01:12<09:09,  2.03s/it]Running Inference:  10%|█         | 31/300 [01:15<10:54,  2.43s/it]Running Inference:  11%|█         | 32/300 [01:19<12:50,  2.87s/it]Running Inference:  11%|█         | 33/300 [01:23<14:29,  3.26s/it]Running Inference:  11%|█▏        | 34/300 [01:27<15:40,  3.54s/it]Running Inference:  12%|█▏        | 35/300 [01:29<13:16,  3.01s/it]Running Inference:  12%|█▏        | 36/300 [01:33<14:37,  3.32s/it]Running Inference:  12%|█▏        | 37/300 [01:37<15:16,  3.49s/it]Running Inference:  13%|█▎        | 38/300 [01:39<12:41,  2.91s/it]Running Inference:  13%|█▎        | 39/300 [01:40<10:04,  2.31s/it]Running Inference:  13%|█▎        | 40/300 [01:42<10:30,  2.43s/it]Running Inference:  14%|█▎        | 41/300 [01:46<11:50,  2.74s/it]Running Inference:  14%|█▍        | 42/300 [01:47<09:33,  2.22s/it]Running Inference:  14%|█▍        | 43/300 [01:49<09:14,  2.16s/it]Running Inference:  15%|█▍        | 44/300 [01:50<07:43,  1.81s/it]Running Inference:  15%|█▌        | 45/300 [01:54<10:35,  2.49s/it]Running Inference:  15%|█▌        | 46/300 [01:56<10:15,  2.42s/it]Running Inference:  16%|█▌        | 47/300 [01:59<10:18,  2.45s/it]Running Inference:  16%|█▌        | 48/300 [02:02<10:59,  2.62s/it]Running Inference:  16%|█▋        | 49/300 [02:04<10:37,  2.54s/it]Running Inference:  17%|█▋        | 50/300 [02:07<10:53,  2.61s/it]Running Inference:  17%|█▋        | 51/300 [02:08<09:38,  2.32s/it]Running Inference:  17%|█▋        | 52/300 [02:11<10:23,  2.51s/it]Running Inference:  18%|█▊        | 53/300 [02:14<10:53,  2.64s/it]Running Inference:  18%|█▊        | 54/300 [02:17<10:54,  2.66s/it]Running Inference:  18%|█▊        | 55/300 [02:21<12:01,  2.94s/it]Running Inference:  19%|█▊        | 56/300 [02:24<11:57,  2.94s/it]Running Inference:  19%|█▉        | 57/300 [02:25<09:49,  2.43s/it]Running Inference:  19%|█▉        | 58/300 [02:28<10:36,  2.63s/it]Running Inference:  20%|█▉        | 59/300 [02:31<10:35,  2.64s/it]Running Inference:  20%|██        | 60/300 [02:34<11:04,  2.77s/it]Running Inference:  20%|██        | 61/300 [02:35<09:34,  2.40s/it]Running Inference:  21%|██        | 62/300 [02:37<09:07,  2.30s/it]Running Inference:  21%|██        | 63/300 [02:39<08:26,  2.14s/it]Running Inference:  21%|██▏       | 64/300 [02:41<08:03,  2.05s/it]Running Inference:  22%|██▏       | 65/300 [02:43<07:54,  2.02s/it]Running Inference:  22%|██▏       | 66/300 [02:46<08:46,  2.25s/it]Running Inference:  22%|██▏       | 67/300 [02:49<09:28,  2.44s/it]Running Inference:  23%|██▎       | 68/300 [02:51<09:41,  2.50s/it]Running Inference:  23%|██▎       | 69/300 [02:54<10:34,  2.75s/it]Running Inference:  23%|██▎       | 70/300 [02:55<08:28,  2.21s/it]Running Inference:  24%|██▎       | 71/300 [02:58<09:10,  2.41s/it]Running Inference:  24%|██▍       | 72/300 [02:59<07:38,  2.01s/it]Running Inference:  24%|██▍       | 73/300 [03:02<07:51,  2.08s/it]Running Inference:  25%|██▍       | 74/300 [03:03<06:33,  1.74s/it]Running Inference:  25%|██▌       | 75/300 [03:03<05:30,  1.47s/it]Running Inference:  25%|██▌       | 76/300 [03:04<04:41,  1.26s/it]Running Inference:  26%|██▌       | 77/300 [03:06<05:41,  1.53s/it]Running Inference:  26%|██▌       | 78/300 [03:10<07:56,  2.15s/it]Running Inference:  26%|██▋       | 79/300 [03:13<08:43,  2.37s/it]Running Inference:  27%|██▋       | 80/300 [03:14<07:04,  1.93s/it]Running Inference:  27%|██▋       | 81/300 [03:15<06:36,  1.81s/it]Running Inference:  27%|██▋       | 82/300 [03:16<05:17,  1.46s/it]Running Inference:  28%|██▊       | 83/300 [03:19<07:07,  1.97s/it]Running Inference:  28%|██▊       | 84/300 [03:23<09:14,  2.57s/it]Running Inference:  28%|██▊       | 85/300 [03:25<08:33,  2.39s/it]Running Inference:  29%|██▊       | 86/300 [03:26<07:11,  2.02s/it]Running Inference:  29%|██▉       | 87/300 [03:28<07:26,  2.10s/it]Running Inference:  29%|██▉       | 88/300 [03:32<09:21,  2.65s/it]Running Inference:  30%|██▉       | 89/300 [03:35<09:02,  2.57s/it]Running Inference:  30%|███       | 90/300 [03:36<07:51,  2.24s/it]Running Inference:  30%|███       | 91/300 [03:38<07:25,  2.13s/it]Running Inference:  31%|███       | 92/300 [03:39<06:03,  1.75s/it]Running Inference:  31%|███       | 93/300 [03:41<06:08,  1.78s/it]Running Inference:  31%|███▏      | 94/300 [03:42<05:29,  1.60s/it]Running Inference:  32%|███▏      | 95/300 [03:44<06:15,  1.83s/it]Running Inference:  32%|███▏      | 96/300 [03:46<05:48,  1.71s/it]Running Inference:  32%|███▏      | 97/300 [03:49<07:03,  2.09s/it]Running Inference:  33%|███▎      | 98/300 [03:50<06:38,  1.97s/it]Running Inference:  33%|███▎      | 99/300 [03:53<06:51,  2.05s/it]Running Inference:  33%|███▎      | 100/300 [03:54<05:40,  1.70s/it]Running Inference:  34%|███▎      | 101/300 [03:56<06:49,  2.06s/it]Running Inference:  34%|███▍      | 102/300 [03:59<07:42,  2.34s/it]Running Inference:  34%|███▍      | 103/300 [04:02<07:58,  2.43s/it]Running Inference:  35%|███▍      | 104/300 [04:05<08:38,  2.64s/it]Running Inference:  35%|███▌      | 105/300 [04:08<08:49,  2.72s/it]Running Inference:  35%|███▌      | 106/300 [04:11<08:56,  2.77s/it]Running Inference:  36%|███▌      | 107/300 [04:14<08:44,  2.72s/it]Running Inference:  36%|███▌      | 108/300 [04:18<09:54,  3.10s/it]Running Inference:  36%|███▋      | 109/300 [04:19<08:02,  2.52s/it]Running Inference:  37%|███▋      | 110/300 [04:23<09:11,  2.90s/it]Running Inference:  37%|███▋      | 111/300 [04:27<10:19,  3.28s/it]Running Inference:  37%|███▋      | 112/300 [04:28<07:56,  2.53s/it]Running Inference:  38%|███▊      | 113/300 [04:30<08:01,  2.58s/it]Running Inference:  38%|███▊      | 114/300 [04:33<08:31,  2.75s/it]Running Inference:  38%|███▊      | 115/300 [04:37<09:42,  3.15s/it]Running Inference:  39%|███▊      | 116/300 [04:41<10:12,  3.33s/it]Running Inference:  39%|███▉      | 117/300 [04:44<09:37,  3.15s/it]Running Inference:  39%|███▉      | 118/300 [04:47<09:19,  3.07s/it]Running Inference:  40%|███▉      | 119/300 [04:48<07:44,  2.57s/it]Running Inference:  40%|████      | 120/300 [04:51<07:45,  2.59s/it]Running Inference:  40%|████      | 121/300 [04:53<07:12,  2.42s/it]Running Inference:  41%|████      | 122/300 [04:56<07:28,  2.52s/it]Running Inference:  41%|████      | 123/300 [04:58<07:17,  2.47s/it]Running Inference:  41%|████▏     | 124/300 [05:00<07:14,  2.47s/it]Running Inference:  42%|████▏     | 125/300 [05:04<08:18,  2.85s/it]Running Inference:  42%|████▏     | 126/300 [05:07<08:07,  2.80s/it]Running Inference:  42%|████▏     | 127/300 [05:09<07:28,  2.60s/it]Running Inference:  43%|████▎     | 128/300 [05:10<05:43,  2.00s/it]Running Inference:  43%|████▎     | 129/300 [05:11<04:54,  1.72s/it]Running Inference:  43%|████▎     | 130/300 [05:12<04:24,  1.56s/it]Running Inference:  44%|████▎     | 131/300 [05:14<04:36,  1.64s/it]Running Inference:  44%|████▍     | 132/300 [05:14<03:51,  1.38s/it]Running Inference:  44%|████▍     | 133/300 [05:16<03:53,  1.40s/it]Running Inference:  45%|████▍     | 134/300 [05:18<04:43,  1.71s/it]Running Inference:  45%|████▌     | 135/300 [05:19<03:47,  1.38s/it]Running Inference:  45%|████▌     | 136/300 [05:22<05:18,  1.94s/it]Running Inference:  46%|████▌     | 137/300 [05:26<06:35,  2.43s/it]Running Inference:  46%|████▌     | 138/300 [05:27<05:49,  2.16s/it]Running Inference:  46%|████▋     | 139/300 [05:28<04:56,  1.84s/it]Running Inference:  47%|████▋     | 140/300 [05:29<04:10,  1.57s/it]Running Inference:  47%|████▋     | 141/300 [05:33<05:55,  2.24s/it]Running Inference:  47%|████▋     | 142/300 [05:35<05:17,  2.01s/it]Running Inference:  48%|████▊     | 143/300 [05:36<04:34,  1.75s/it]Running Inference:  48%|████▊     | 144/300 [05:40<06:26,  2.48s/it]Running Inference:  48%|████▊     | 145/300 [05:41<05:22,  2.08s/it]Running Inference:  49%|████▊     | 146/300 [05:43<05:20,  2.08s/it]Running Inference:  49%|████▉     | 147/300 [05:45<05:12,  2.04s/it]Running Inference:  49%|████▉     | 148/300 [05:49<06:50,  2.70s/it]Running Inference:  50%|████▉     | 149/300 [05:53<07:46,  3.09s/it]Running Inference:  50%|█████     | 150/300 [05:55<06:21,  2.55s/it]Running Inference:  50%|█████     | 151/300 [05:55<05:01,  2.02s/it]Running Inference:  51%|█████     | 152/300 [05:56<04:16,  1.73s/it]Running Inference:  51%|█████     | 153/300 [06:00<05:21,  2.19s/it]Running Inference:  51%|█████▏    | 154/300 [06:04<06:45,  2.78s/it]Running Inference:  52%|█████▏    | 155/300 [06:05<05:42,  2.36s/it]Running Inference:  52%|█████▏    | 156/300 [06:08<05:54,  2.46s/it]Running Inference:  52%|█████▏    | 157/300 [06:09<04:46,  2.00s/it]Running Inference:  53%|█████▎    | 158/300 [06:12<05:12,  2.20s/it]Running Inference:  53%|█████▎    | 159/300 [06:14<05:42,  2.43s/it]Running Inference:  53%|█████▎    | 160/300 [06:17<05:49,  2.50s/it]Running Inference:  54%|█████▎    | 161/300 [06:21<06:58,  3.01s/it]Running Inference:  54%|█████▍    | 162/300 [06:24<06:25,  2.79s/it]Running Inference:  54%|█████▍    | 163/300 [06:26<05:46,  2.53s/it]Running Inference:  55%|█████▍    | 164/300 [06:30<06:47,  2.99s/it]Running Inference:  55%|█████▌    | 165/300 [06:32<06:35,  2.93s/it]Running Inference:  55%|█████▌    | 166/300 [06:33<05:12,  2.33s/it]Running Inference:  56%|█████▌    | 167/300 [06:34<04:11,  1.89s/it]Running Inference:  56%|█████▌    | 168/300 [06:35<03:40,  1.67s/it]Running Inference:  56%|█████▋    | 169/300 [06:39<05:09,  2.37s/it]Running Inference:  57%|█████▋    | 170/300 [06:41<04:22,  2.02s/it]Running Inference:  57%|█████▋    | 171/300 [06:45<05:44,  2.67s/it]Running Inference:  57%|█████▋    | 172/300 [06:47<05:24,  2.54s/it]Running Inference:  58%|█████▊    | 173/300 [06:49<05:17,  2.50s/it]Running Inference:  58%|█████▊    | 174/300 [06:50<04:14,  2.02s/it]Running Inference:  58%|█████▊    | 175/300 [06:51<03:33,  1.71s/it]Running Inference:  59%|█████▊    | 176/300 [06:53<03:34,  1.73s/it]Running Inference:  59%|█████▉    | 177/300 [06:57<04:54,  2.39s/it]Running Inference:  59%|█████▉    | 178/300 [06:59<04:29,  2.21s/it]Running Inference:  60%|█████▉    | 179/300 [06:59<03:33,  1.77s/it]Running Inference:  60%|██████    | 180/300 [07:01<03:27,  1.73s/it]Running Inference:  60%|██████    | 181/300 [07:03<03:39,  1.85s/it]Running Inference:  61%|██████    | 182/300 [07:05<03:33,  1.81s/it]Running Inference:  61%|██████    | 183/300 [07:06<03:15,  1.67s/it]Running Inference:  61%|██████▏   | 184/300 [07:10<04:40,  2.42s/it]Running Inference:  62%|██████▏   | 185/300 [07:14<05:11,  2.71s/it]Running Inference:  62%|██████▏   | 186/300 [07:17<05:23,  2.84s/it]Running Inference:  62%|██████▏   | 187/300 [07:18<04:10,  2.22s/it]Running Inference:  63%|██████▎   | 188/300 [07:21<04:28,  2.40s/it]Running Inference:  63%|██████▎   | 189/300 [07:21<03:28,  1.88s/it]Running Inference:  63%|██████▎   | 190/300 [07:24<03:43,  2.04s/it]Running Inference:  64%|██████▎   | 191/300 [07:24<03:01,  1.67s/it]Running Inference:  64%|██████▍   | 192/300 [07:28<04:09,  2.31s/it]Running Inference:  64%|██████▍   | 193/300 [07:31<04:34,  2.56s/it]Running Inference:  65%|██████▍   | 194/300 [07:32<03:42,  2.10s/it]Running Inference:  65%|██████▌   | 195/300 [07:33<02:59,  1.71s/it]Running Inference:  65%|██████▌   | 196/300 [07:34<02:22,  1.37s/it]Running Inference:  66%|██████▌   | 197/300 [07:35<02:26,  1.42s/it]Running Inference:  66%|██████▌   | 198/300 [07:37<02:22,  1.39s/it]Running Inference:  66%|██████▋   | 199/300 [07:41<03:46,  2.24s/it]Running Inference:  67%|██████▋   | 200/300 [07:45<04:42,  2.83s/it]Running Inference:  67%|██████▋   | 201/300 [07:48<04:39,  2.83s/it]Running Inference:  67%|██████▋   | 202/300 [07:51<04:39,  2.85s/it]Running Inference:  68%|██████▊   | 203/300 [07:54<04:40,  2.89s/it]Running Inference:  68%|██████▊   | 204/300 [07:56<04:14,  2.65s/it]Running Inference:  68%|██████▊   | 205/300 [07:57<03:24,  2.15s/it]Running Inference:  69%|██████▊   | 206/300 [07:59<03:13,  2.06s/it]Running Inference:  69%|██████▉   | 207/300 [08:00<02:43,  1.75s/it]Running Inference:  69%|██████▉   | 208/300 [08:02<03:06,  2.03s/it]Running Inference:  70%|██████▉   | 209/300 [08:04<02:53,  1.90s/it]Running Inference:  70%|███████   | 210/300 [08:05<02:31,  1.69s/it]Running Inference:  70%|███████   | 211/300 [08:08<03:04,  2.07s/it]Running Inference:  71%|███████   | 212/300 [08:12<03:34,  2.44s/it]Running Inference:  71%|███████   | 213/300 [08:12<02:45,  1.90s/it]Running Inference:  71%|███████▏  | 214/300 [08:15<02:55,  2.04s/it]Running Inference:  72%|███████▏  | 215/300 [08:19<03:45,  2.65s/it]Running Inference:  72%|███████▏  | 216/300 [08:20<03:15,  2.33s/it]Running Inference:  72%|███████▏  | 217/300 [08:22<03:08,  2.28s/it]Running Inference:  73%|███████▎  | 218/300 [08:23<02:25,  1.78s/it]Running Inference:  73%|███████▎  | 219/300 [08:24<02:07,  1.57s/it]Running Inference:  73%|███████▎  | 220/300 [08:25<01:49,  1.37s/it]Running Inference:  74%|███████▎  | 221/300 [08:26<01:47,  1.36s/it]Running Inference:  74%|███████▍  | 222/300 [08:28<01:56,  1.50s/it]Running Inference:  74%|███████▍  | 223/300 [08:29<01:40,  1.31s/it]Running Inference:  75%|███████▍  | 224/300 [08:32<02:12,  1.75s/it]Running Inference:  75%|███████▌  | 225/300 [08:35<02:34,  2.05s/it]Running Inference:  75%|███████▌  | 226/300 [08:37<02:34,  2.09s/it]Running Inference:  76%|███████▌  | 227/300 [08:40<02:51,  2.35s/it]Running Inference:  76%|███████▌  | 228/300 [08:40<02:06,  1.76s/it]Running Inference:  76%|███████▋  | 229/300 [08:41<01:42,  1.45s/it]Running Inference:  77%|███████▋  | 230/300 [08:45<02:35,  2.23s/it]Running Inference:  77%|███████▋  | 231/300 [08:45<01:59,  1.74s/it]Running Inference:  77%|███████▋  | 232/300 [08:48<02:16,  2.01s/it]Running Inference:  78%|███████▊  | 233/300 [08:49<02:01,  1.81s/it]Running Inference:  78%|███████▊  | 234/300 [08:52<02:17,  2.09s/it]Running Inference:  78%|███████▊  | 235/300 [08:55<02:35,  2.39s/it]Running Inference:  79%|███████▊  | 236/300 [08:58<02:46,  2.60s/it]Running Inference:  79%|███████▉  | 237/300 [09:02<03:04,  2.92s/it]Running Inference:  79%|███████▉  | 238/300 [09:04<02:36,  2.52s/it]Running Inference:  80%|███████▉  | 239/300 [09:06<02:37,  2.59s/it]Running Inference:  80%|████████  | 240/300 [09:10<02:52,  2.88s/it]Running Inference:  80%|████████  | 241/300 [09:12<02:39,  2.70s/it]Running Inference:  81%|████████  | 242/300 [09:15<02:39,  2.75s/it]Running Inference:  81%|████████  | 243/300 [09:18<02:34,  2.72s/it]Running Inference:  81%|████████▏ | 244/300 [09:20<02:27,  2.64s/it]Running Inference:  82%|████████▏ | 245/300 [09:24<02:42,  2.96s/it]Running Inference:  82%|████████▏ | 246/300 [09:25<02:07,  2.37s/it]Running Inference:  82%|████████▏ | 247/300 [09:27<02:07,  2.41s/it]Running Inference:  83%|████████▎ | 248/300 [09:29<01:59,  2.30s/it]Running Inference:  83%|████████▎ | 249/300 [09:30<01:38,  1.94s/it]Running Inference:  83%|████████▎ | 250/300 [09:34<02:00,  2.40s/it]Running Inference:  84%|████████▎ | 251/300 [09:36<01:48,  2.21s/it]Running Inference:  84%|████████▍ | 252/300 [09:38<01:51,  2.32s/it]Running Inference:  84%|████████▍ | 253/300 [09:41<01:52,  2.40s/it]Running Inference:  85%|████████▍ | 254/300 [09:42<01:27,  1.89s/it]Running Inference:  85%|████████▌ | 255/300 [09:44<01:35,  2.12s/it]Running Inference:  85%|████████▌ | 256/300 [09:45<01:14,  1.69s/it]Running Inference:  86%|████████▌ | 257/300 [09:47<01:18,  1.83s/it]Running Inference:  86%|████████▌ | 258/300 [09:49<01:19,  1.89s/it]Running Inference:  86%|████████▋ | 259/300 [09:52<01:32,  2.26s/it]Running Inference:  87%|████████▋ | 260/300 [09:55<01:32,  2.31s/it]Running Inference:  87%|████████▋ | 261/300 [09:56<01:14,  1.91s/it]Running Inference:  87%|████████▋ | 262/300 [09:58<01:21,  2.14s/it]Running Inference:  88%|████████▊ | 263/300 [09:59<01:04,  1.74s/it]Running Inference:  88%|████████▊ | 264/300 [10:02<01:14,  2.06s/it]Running Inference:  88%|████████▊ | 265/300 [10:05<01:20,  2.29s/it]Running Inference:  89%|████████▊ | 266/300 [10:07<01:22,  2.42s/it]Running Inference:  89%|████████▉ | 267/300 [10:10<01:21,  2.47s/it]Running Inference:  89%|████████▉ | 268/300 [10:13<01:26,  2.72s/it]Running Inference:  90%|████████▉ | 269/300 [10:17<01:36,  3.12s/it]Running Inference:  90%|█████████ | 270/300 [10:18<01:12,  2.41s/it]Running Inference:  90%|█████████ | 271/300 [10:21<01:17,  2.67s/it]Running Inference:  91%|█████████ | 272/300 [10:25<01:26,  3.09s/it]Running Inference:  91%|█████████ | 273/300 [10:28<01:18,  2.89s/it]Running Inference:  91%|█████████▏| 274/300 [10:31<01:15,  2.89s/it]Running Inference:  92%|█████████▏| 275/300 [10:32<00:59,  2.39s/it]Running Inference:  92%|█████████▏| 276/300 [10:34<00:53,  2.22s/it]Running Inference:  92%|█████████▏| 277/300 [10:36<00:51,  2.25s/it]Running Inference:  93%|█████████▎| 278/300 [10:39<00:51,  2.35s/it]Running Inference:  93%|█████████▎| 279/300 [10:41<00:50,  2.40s/it]Running Inference:  93%|█████████▎| 280/300 [10:43<00:44,  2.25s/it]Running Inference:  94%|█████████▎| 281/300 [10:47<00:53,  2.82s/it]Running Inference:  94%|█████████▍| 282/300 [10:50<00:47,  2.65s/it]Running Inference:  94%|█████████▍| 283/300 [10:51<00:39,  2.34s/it]Running Inference:  95%|█████████▍| 284/300 [10:55<00:46,  2.90s/it]Running Inference:  95%|█████████▌| 285/300 [10:57<00:35,  2.38s/it]Running Inference:  95%|█████████▌| 286/300 [10:59<00:35,  2.50s/it]Running Inference:  96%|█████████▌| 287/300 [11:04<00:39,  3.00s/it]Running Inference:  96%|█████████▌| 288/300 [11:05<00:32,  2.69s/it]Running Inference:  96%|█████████▋| 289/300 [11:10<00:34,  3.10s/it]Running Inference:  97%|█████████▋| 290/300 [11:11<00:26,  2.70s/it]Running Inference:  97%|█████████▋| 291/300 [11:14<00:23,  2.64s/it]Running Inference:  97%|█████████▋| 292/300 [11:17<00:23,  2.92s/it]Running Inference:  98%|█████████▊| 293/300 [11:22<00:23,  3.29s/it]Running Inference:  98%|█████████▊| 294/300 [11:25<00:20,  3.39s/it]Running Inference:  98%|█████████▊| 295/300 [11:27<00:14,  2.86s/it]Running Inference:  99%|█████████▊| 296/300 [11:30<00:11,  2.83s/it]Running Inference:  99%|█████████▉| 297/300 [11:31<00:07,  2.39s/it]Running Inference:  99%|█████████▉| 298/300 [11:34<00:05,  2.64s/it]Running Inference: 100%|█████████▉| 299/300 [11:35<00:02,  2.23s/it]Running Inference: 100%|██████████| 300/300 [11:38<00:00,  2.21s/it]Running Inference: 100%|██████████| 300/300 [11:38<00:00,  2.33s/it]
2025-12-15 02:38:09,811 - INFO - Inference completed.
2025-12-15 02:38:09,821 - INFO - Results saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.50/predictions.csv
2025-12-15 02:38:09,821 - INFO - Calculating metrics for dataset: longbench
2025-12-15 02:38:09,829 - INFO - Metrics saved to longbenchresult/longbench__2wikimqa_e__Qwen--Qwen3-8B__streaming_llm__0.50/metrics.json
2025-12-15 02:38:09,829 - INFO - Metrics:
18.16
2025-12-15 02:38:09,830 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=2wikimqa_e, ratio=0.5) on GPU 2


========================================
LongBench Task: gov_report_e
========================================
----------------------------------------
Task: gov_report_e | Compression Ratio: 0.1
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 02:38:16,281 - INFO - Set deterministic seeds to 42
2025-12-15 02:38:16,281 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.1,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 02:38:16,281 - INFO - Starting evaluation run...
2025-12-15 02:38:16,281 - INFO - Output directory set to: longbenchresult
2025-12-15 02:38:16,281 - INFO - Set StreamingLLMPress compression_ratio to 0.1
2025-12-15 02:38:16,281 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 02:38:16,281 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 02:38:16,281 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.89it/s]
Device set to use cuda:0
2025-12-15 02:38:27,751 - INFO - Model pipeline loaded.
2025-12-15 02:38:27,751 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report_e)
2025-12-15 02:38:30,921 - INFO - Dataset loaded with 300 entries.
2025-12-15 02:38:30,922 - INFO - Dataset processed with 300 entries.
2025-12-15 02:38:30,962 - INFO - Starting inference...
Running Inference:   0%|          | 0/300 [00:00<?, ?it/s]Running Inference:   0%|          | 1/300 [00:23<1:56:15, 23.33s/it]Running Inference:   1%|          | 2/300 [00:50<2:06:20, 25.44s/it]Running Inference:   1%|          | 3/300 [01:12<1:59:46, 24.20s/it]Running Inference:   1%|▏         | 4/300 [01:33<1:52:18, 22.76s/it]Running Inference:   2%|▏         | 5/300 [01:56<1:53:00, 22.99s/it]Running Inference:   2%|▏         | 6/300 [02:20<1:53:49, 23.23s/it]Running Inference:   2%|▏         | 7/300 [02:43<1:52:23, 23.01s/it]Running Inference:   3%|▎         | 8/300 [03:06<1:53:13, 23.26s/it]Running Inference:   3%|▎         | 9/300 [03:29<1:51:52, 23.07s/it]Running Inference:   3%|▎         | 10/300 [03:52<1:51:01, 22.97s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Running Inference:   4%|▎         | 11/300 [04:15<1:50:50, 23.01s/it]Running Inference:   4%|▍         | 12/300 [04:19<1:23:09, 17.33s/it]Running Inference:   4%|▍         | 13/300 [04:43<1:32:04, 19.25s/it]Running Inference:   5%|▍         | 14/300 [05:06<1:36:33, 20.26s/it]Running Inference:   5%|▌         | 15/300 [05:28<1:39:34, 20.96s/it]Running Inference:   5%|▌         | 16/300 [05:45<1:32:48, 19.61s/it]Running Inference:   6%|▌         | 17/300 [06:07<1:36:38, 20.49s/it]Running Inference:   6%|▌         | 18/300 [06:25<1:32:43, 19.73s/it]Running Inference:   6%|▋         | 19/300 [06:48<1:36:58, 20.71s/it]Running Inference:   7%|▋         | 20/300 [07:14<1:43:16, 22.13s/it]Running Inference:   7%|▋         | 21/300 [07:36<1:43:37, 22.29s/it]Running Inference:   7%|▋         | 22/300 [08:00<1:45:47, 22.83s/it]Running Inference:   8%|▊         | 23/300 [08:25<1:48:05, 23.41s/it]Running Inference:   8%|▊         | 24/300 [08:48<1:47:03, 23.27s/it]Running Inference:   8%|▊         | 25/300 [08:57<1:26:49, 18.94s/it]Running Inference:   9%|▊         | 26/300 [09:20<1:32:32, 20.26s/it]Running Inference:   9%|▉         | 27/300 [09:22<1:06:40, 14.65s/it]Running Inference:   9%|▉         | 28/300 [09:45<1:17:48, 17.16s/it]Running Inference:  10%|▉         | 29/300 [09:49<59:26, 13.16s/it]  Running Inference:  10%|█         | 30/300 [10:01<57:56, 12.88s/it]Running Inference:  10%|█         | 31/300 [10:22<1:09:28, 15.50s/it]Running Inference:  11%|█         | 32/300 [10:45<1:19:13, 17.74s/it]Running Inference:  11%|█         | 33/300 [11:04<1:20:43, 18.14s/it]Running Inference:  11%|█▏        | 34/300 [11:28<1:27:04, 19.64s/it]Running Inference:  12%|█▏        | 35/300 [11:50<1:30:47, 20.56s/it]Running Inference:  12%|█▏        | 36/300 [12:10<1:28:53, 20.20s/it]Running Inference:  12%|█▏        | 37/300 [12:33<1:32:53, 21.19s/it]Running Inference:  13%|█▎        | 38/300 [12:57<1:35:56, 21.97s/it]Running Inference:  13%|█▎        | 39/300 [13:21<1:38:01, 22.54s/it]Running Inference:  13%|█▎        | 40/300 [13:44<1:38:54, 22.82s/it]Running Inference:  14%|█▎        | 41/300 [14:07<1:38:54, 22.91s/it]Running Inference:  14%|█▍        | 42/300 [14:31<1:38:48, 22.98s/it]Running Inference:  14%|█▍        | 43/300 [14:53<1:38:12, 22.93s/it]Running Inference:  15%|█▍        | 44/300 [15:16<1:37:34, 22.87s/it]Running Inference:  15%|█▌        | 45/300 [15:40<1:38:04, 23.08s/it]Running Inference:  15%|█▌        | 46/300 [16:03<1:37:46, 23.10s/it]Running Inference:  16%|█▌        | 47/300 [16:26<1:37:20, 23.09s/it]Running Inference:  16%|█▌        | 48/300 [16:50<1:37:40, 23.25s/it]Running Inference:  16%|█▋        | 49/300 [17:12<1:36:24, 23.05s/it]Running Inference:  17%|█▋        | 50/300 [17:26<1:24:54, 20.38s/it]Running Inference:  17%|█▋        | 51/300 [17:49<1:27:32, 21.09s/it]Running Inference:  17%|█▋        | 52/300 [18:12<1:29:55, 21.76s/it]Running Inference:  18%|█▊        | 53/300 [18:35<1:30:35, 22.01s/it]Running Inference:  18%|█▊        | 54/300 [18:58<1:31:06, 22.22s/it]Running Inference:  18%|█▊        | 55/300 [19:22<1:33:13, 22.83s/it]Running Inference:  19%|█▊        | 56/300 [19:45<1:32:46, 22.81s/it]Running Inference:  19%|█▉        | 57/300 [20:08<1:32:36, 22.87s/it]Running Inference:  19%|█▉        | 58/300 [20:23<1:22:35, 20.48s/it]Running Inference:  20%|█▉        | 59/300 [20:44<1:22:48, 20.62s/it]Running Inference:  20%|██        | 60/300 [21:07<1:25:20, 21.33s/it]Running Inference:  20%|██        | 61/300 [21:29<1:26:34, 21.74s/it]Running Inference:  21%|██        | 62/300 [21:51<1:26:32, 21.82s/it]Running Inference:  21%|██        | 63/300 [22:08<1:20:48, 20.46s/it]Running Inference:  21%|██▏       | 64/300 [22:32<1:23:38, 21.27s/it]Running Inference:  22%|██▏       | 65/300 [22:50<1:19:27, 20.29s/it]Running Inference:  22%|██▏       | 66/300 [23:09<1:17:40, 19.92s/it]Running Inference:  22%|██▏       | 67/300 [23:32<1:21:48, 21.07s/it]Running Inference:  23%|██▎       | 68/300 [23:55<1:22:51, 21.43s/it]Running Inference:  23%|██▎       | 69/300 [24:19<1:25:16, 22.15s/it]Running Inference:  23%|██▎       | 70/300 [24:47<1:31:41, 23.92s/it]Running Inference:  24%|██▎       | 71/300 [25:09<1:30:02, 23.59s/it]Running Inference:  24%|██▍       | 72/300 [25:33<1:29:35, 23.58s/it]Running Inference:  24%|██▍       | 73/300 [25:56<1:28:45, 23.46s/it]Running Inference:  25%|██▍       | 74/300 [26:20<1:28:40, 23.54s/it]Running Inference:  25%|██▌       | 75/300 [26:43<1:27:16, 23.27s/it]Running Inference:  25%|██▌       | 76/300 [27:05<1:26:23, 23.14s/it]Running Inference:  26%|██▌       | 77/300 [27:30<1:27:16, 23.48s/it]Running Inference:  26%|██▌       | 78/300 [27:46<1:18:53, 21.32s/it]Running Inference:  26%|██▋       | 79/300 [27:57<1:07:09, 18.23s/it]Running Inference:  27%|██▋       | 80/300 [28:20<1:12:38, 19.81s/it]Running Inference:  27%|██▋       | 81/300 [28:45<1:17:00, 21.10s/it]Running Inference:  27%|██▋       | 82/300 [29:07<1:18:20, 21.56s/it]Running Inference:  28%|██▊       | 83/300 [29:30<1:19:17, 21.92s/it]Running Inference:  28%|██▊       | 84/300 [29:53<1:19:47, 22.16s/it]Running Inference:  28%|██▊       | 85/300 [30:16<1:20:32, 22.48s/it]Running Inference:  29%|██▊       | 86/300 [30:40<1:21:41, 22.90s/it]Running Inference:  29%|██▉       | 87/300 [31:06<1:25:20, 24.04s/it]Running Inference:  29%|██▉       | 88/300 [31:30<1:24:09, 23.82s/it]Running Inference:  30%|██▉       | 89/300 [31:53<1:22:38, 23.50s/it]Running Inference:  30%|███       | 90/300 [32:11<1:17:13, 22.06s/it]Running Inference:  30%|███       | 91/300 [32:34<1:17:52, 22.35s/it]Running Inference:  31%|███       | 92/300 [32:57<1:17:49, 22.45s/it]Running Inference:  31%|███       | 93/300 [33:20<1:18:27, 22.74s/it]Running Inference:  31%|███▏      | 94/300 [33:43<1:18:11, 22.77s/it]Running Inference:  32%|███▏      | 95/300 [34:06<1:17:34, 22.70s/it]Running Inference:  32%|███▏      | 96/300 [34:29<1:17:26, 22.78s/it]Running Inference:  32%|███▏      | 97/300 [34:52<1:17:10, 22.81s/it]Running Inference:  33%|███▎      | 98/300 [35:15<1:16:57, 22.86s/it]Running Inference:  33%|███▎      | 99/300 [35:22<1:01:24, 18.33s/it]Running Inference:  33%|███▎      | 100/300 [35:47<1:07:40, 20.30s/it]Running Inference:  34%|███▎      | 101/300 [36:03<1:02:50, 18.95s/it]Running Inference:  34%|███▍      | 102/300 [36:26<1:06:20, 20.10s/it]Running Inference:  34%|███▍      | 103/300 [36:50<1:09:50, 21.27s/it]Running Inference:  35%|███▍      | 104/300 [37:13<1:11:08, 21.78s/it]Running Inference:  35%|███▌      | 105/300 [37:34<1:10:26, 21.67s/it]Running Inference:  35%|███▌      | 106/300 [37:57<1:11:07, 22.00s/it]Running Inference:  36%|███▌      | 107/300 [38:20<1:11:56, 22.37s/it]Running Inference:  36%|███▌      | 108/300 [38:43<1:11:51, 22.46s/it]Running Inference:  36%|███▋      | 109/300 [39:06<1:12:32, 22.79s/it]Running Inference:  37%|███▋      | 110/300 [39:26<1:09:21, 21.90s/it]Running Inference:  37%|███▋      | 111/300 [39:49<1:10:01, 22.23s/it]Running Inference:  37%|███▋      | 112/300 [40:15<1:13:12, 23.37s/it]Running Inference:  38%|███▊      | 113/300 [40:38<1:12:31, 23.27s/it]Running Inference:  38%|███▊      | 114/300 [40:41<52:49, 17.04s/it]  Running Inference:  38%|███▊      | 115/300 [41:04<57:54, 18.78s/it]Running Inference:  39%|███▊      | 116/300 [41:05<41:59, 13.69s/it]Running Inference:  39%|███▉      | 117/300 [41:24<45:52, 15.04s/it]Running Inference:  39%|███▉      | 118/300 [41:46<52:32, 17.32s/it]Running Inference:  40%|███▉      | 119/300 [42:09<57:05, 18.93s/it]Running Inference:  40%|████      | 120/300 [42:32<1:00:04, 20.03s/it]Running Inference:  40%|████      | 121/300 [42:55<1:02:50, 21.06s/it]Running Inference:  41%|████      | 122/300 [43:18<1:03:58, 21.56s/it]Running Inference:  41%|████      | 123/300 [43:41<1:05:19, 22.14s/it]Running Inference:  41%|████▏     | 124/300 [44:04<1:05:35, 22.36s/it]Running Inference:  42%|████▏     | 125/300 [44:26<1:04:58, 22.28s/it]Running Inference:  42%|████▏     | 126/300 [44:50<1:05:44, 22.67s/it]Running Inference:  42%|████▏     | 127/300 [45:17<1:09:39, 24.16s/it]Running Inference:  43%|████▎     | 128/300 [45:40<1:07:59, 23.72s/it]Running Inference:  43%|████▎     | 129/300 [46:03<1:06:44, 23.42s/it]Running Inference:  43%|████▎     | 130/300 [46:24<1:04:22, 22.72s/it]Running Inference:  44%|████▎     | 131/300 [46:47<1:04:22, 22.85s/it]Running Inference:  44%|████▍     | 132/300 [47:06<1:00:17, 21.53s/it]Running Inference:  44%|████▍     | 133/300 [47:29<1:01:17, 22.02s/it]Running Inference:  45%|████▍     | 134/300 [47:53<1:02:29, 22.59s/it]Running Inference:  45%|████▌     | 135/300 [48:10<57:51, 21.04s/it]  Running Inference:  45%|████▌     | 136/300 [48:29<55:29, 20.30s/it]Running Inference:  46%|████▌     | 137/300 [48:36<44:50, 16.50s/it]Running Inference:  46%|████▌     | 138/300 [48:59<49:55, 18.49s/it]Running Inference:  46%|████▋     | 139/300 [49:03<37:24, 13.94s/it]Running Inference:  47%|████▋     | 140/300 [49:26<44:53, 16.83s/it]Running Inference:  47%|████▋     | 141/300 [49:44<44:58, 16.97s/it]Running Inference:  47%|████▋     | 142/300 [50:07<49:28, 18.79s/it]Running Inference:  48%|████▊     | 143/300 [50:29<52:08, 19.93s/it]Running Inference:  48%|████▊     | 144/300 [50:52<53:52, 20.72s/it]Running Inference:  48%|████▊     | 145/300 [51:14<54:58, 21.28s/it]Running Inference:  49%|████▊     | 146/300 [51:37<55:33, 21.65s/it]Running Inference:  49%|████▉     | 147/300 [51:39<40:05, 15.72s/it]Running Inference:  49%|████▉     | 148/300 [52:03<46:14, 18.26s/it]Running Inference:  50%|████▉     | 149/300 [52:26<49:19, 19.60s/it]Running Inference:  50%|█████     | 150/300 [52:48<51:15, 20.50s/it]Running Inference:  50%|█████     | 151/300 [53:08<50:12, 20.22s/it]Running Inference:  51%|█████     | 152/300 [53:30<51:36, 20.92s/it]Running Inference:  51%|█████     | 153/300 [53:47<47:48, 19.51s/it]Running Inference:  51%|█████▏    | 154/300 [54:09<49:54, 20.51s/it]Running Inference:  52%|█████▏    | 155/300 [54:33<51:58, 21.50s/it]Running Inference:  52%|█████▏    | 156/300 [54:55<51:59, 21.66s/it]Running Inference:  52%|█████▏    | 157/300 [55:18<52:41, 22.11s/it]Running Inference:  53%|█████▎    | 158/300 [55:42<53:11, 22.48s/it]Running Inference:  53%|█████▎    | 159/300 [56:05<53:04, 22.58s/it]Running Inference:  53%|█████▎    | 160/300 [56:27<52:42, 22.59s/it]Running Inference:  54%|█████▎    | 161/300 [56:50<52:30, 22.67s/it]Running Inference:  54%|█████▍    | 162/300 [57:08<48:34, 21.12s/it]Running Inference:  54%|█████▍    | 163/300 [57:30<49:17, 21.59s/it]Running Inference:  55%|█████▍    | 164/300 [57:53<49:56, 22.03s/it]Running Inference:  55%|█████▌    | 165/300 [58:14<48:46, 21.68s/it]Running Inference:  55%|█████▌    | 166/300 [58:37<49:13, 22.04s/it]Running Inference:  56%|█████▌    | 167/300 [59:00<49:12, 22.20s/it]Running Inference:  56%|█████▌    | 168/300 [59:21<48:32, 22.06s/it]Running Inference:  56%|█████▋    | 169/300 [59:44<48:36, 22.26s/it]Running Inference:  57%|█████▋    | 170/300 [1:00:07<48:41, 22.48s/it]Running Inference:  57%|█████▋    | 171/300 [1:00:30<48:45, 22.68s/it]Running Inference:  57%|█████▋    | 172/300 [1:00:46<43:44, 20.51s/it]Running Inference:  58%|█████▊    | 173/300 [1:01:08<44:43, 21.13s/it]Running Inference:  58%|█████▊    | 174/300 [1:01:32<45:43, 21.77s/it]Running Inference:  58%|█████▊    | 175/300 [1:01:55<46:14, 22.20s/it]Running Inference:  59%|█████▊    | 176/300 [1:02:17<45:49, 22.17s/it]Running Inference:  59%|█████▉    | 177/300 [1:02:40<45:47, 22.33s/it]Running Inference:  59%|█████▉    | 178/300 [1:02:58<42:58, 21.13s/it]Running Inference:  60%|█████▉    | 179/300 [1:03:22<44:14, 21.94s/it]Running Inference:  60%|██████    | 180/300 [1:03:44<44:16, 22.14s/it]Running Inference:  60%|██████    | 181/300 [1:04:07<44:13, 22.30s/it]Running Inference:  61%|██████    | 182/300 [1:04:30<44:01, 22.39s/it]Running Inference:  61%|██████    | 183/300 [1:04:52<43:50, 22.48s/it]Running Inference:  61%|██████▏   | 184/300 [1:04:54<31:35, 16.34s/it]Running Inference:  62%|██████▏   | 185/300 [1:05:20<36:29, 19.04s/it]Running Inference:  62%|██████▏   | 186/300 [1:05:43<38:26, 20.23s/it]Running Inference:  62%|██████▏   | 187/300 [1:06:05<39:25, 20.93s/it]Running Inference:  63%|██████▎   | 188/300 [1:06:29<40:29, 21.70s/it]Running Inference:  63%|██████▎   | 189/300 [1:06:48<38:46, 20.96s/it]Running Inference:  63%|██████▎   | 190/300 [1:07:11<39:20, 21.46s/it]Running Inference:  64%|██████▎   | 191/300 [1:07:34<40:08, 22.10s/it]Running Inference:  64%|██████▍   | 192/300 [1:07:57<40:20, 22.41s/it]Running Inference:  64%|██████▍   | 193/300 [1:08:25<42:53, 24.05s/it]Running Inference:  65%|██████▍   | 194/300 [1:08:42<38:27, 21.77s/it]Running Inference:  65%|██████▌   | 195/300 [1:09:07<39:56, 22.82s/it]Running Inference:  65%|██████▌   | 196/300 [1:09:24<36:20, 20.96s/it]Running Inference:  66%|██████▌   | 197/300 [1:09:46<36:51, 21.47s/it]Running Inference:  66%|██████▌   | 198/300 [1:10:09<37:21, 21.98s/it]Running Inference:  66%|██████▋   | 199/300 [1:10:34<38:35, 22.92s/it]Running Inference:  67%|██████▋   | 200/300 [1:10:57<38:03, 22.83s/it]Running Inference:  67%|██████▋   | 201/300 [1:11:20<37:46, 22.89s/it]Running Inference:  67%|██████▋   | 202/300 [1:11:43<37:20, 22.87s/it]Running Inference:  68%|██████▊   | 203/300 [1:12:06<37:01, 22.90s/it]Running Inference:  68%|██████▊   | 204/300 [1:12:29<36:55, 23.08s/it]Running Inference:  68%|██████▊   | 205/300 [1:12:53<36:48, 23.25s/it]Running Inference:  69%|██████▊   | 206/300 [1:13:17<36:38, 23.39s/it]Running Inference:  69%|██████▉   | 207/300 [1:13:38<35:16, 22.76s/it]Running Inference:  69%|██████▉   | 208/300 [1:14:01<35:05, 22.88s/it]Running Inference:  70%|██████▉   | 209/300 [1:14:25<35:17, 23.27s/it]Running Inference:  70%|███████   | 210/300 [1:14:48<34:49, 23.21s/it]Running Inference:  70%|███████   | 211/300 [1:15:05<31:31, 21.25s/it]Running Inference:  71%|███████   | 212/300 [1:15:32<33:44, 23.01s/it]Running Inference:  71%|███████   | 213/300 [1:15:55<33:11, 22.89s/it]Running Inference:  71%|███████▏  | 214/300 [1:16:18<32:50, 22.92s/it]Running Inference:  72%|███████▏  | 215/300 [1:16:40<32:19, 22.82s/it]Running Inference:  72%|███████▏  | 216/300 [1:17:03<31:56, 22.82s/it]Running Inference:  72%|███████▏  | 217/300 [1:17:26<31:35, 22.84s/it]Running Inference:  73%|███████▎  | 218/300 [1:17:51<32:04, 23.47s/it]Running Inference:  73%|███████▎  | 219/300 [1:18:14<31:18, 23.19s/it]Running Inference:  73%|███████▎  | 220/300 [1:18:36<30:38, 22.98s/it]Running Inference:  74%|███████▎  | 221/300 [1:18:59<30:10, 22.92s/it]Running Inference:  74%|███████▍  | 222/300 [1:19:14<26:41, 20.53s/it]Running Inference:  74%|███████▍  | 223/300 [1:19:38<27:39, 21.56s/it]Running Inference:  75%|███████▍  | 224/300 [1:20:01<28:04, 22.17s/it]Running Inference:  75%|███████▌  | 225/300 [1:20:24<27:56, 22.35s/it]Running Inference:  75%|███████▌  | 226/300 [1:20:47<27:48, 22.55s/it]Running Inference:  76%|███████▌  | 227/300 [1:21:11<27:45, 22.82s/it]Running Inference:  76%|███████▌  | 228/300 [1:21:29<25:40, 21.39s/it]Running Inference:  76%|███████▋  | 229/300 [1:21:54<26:35, 22.47s/it]Running Inference:  77%|███████▋  | 230/300 [1:22:16<26:19, 22.56s/it]Running Inference:  77%|███████▋  | 231/300 [1:22:39<26:06, 22.70s/it]Running Inference:  77%|███████▋  | 232/300 [1:23:00<24:53, 21.96s/it]Running Inference:  78%|███████▊  | 233/300 [1:23:20<23:56, 21.44s/it]Running Inference:  78%|███████▊  | 234/300 [1:23:43<23:58, 21.80s/it]Running Inference:  78%|███████▊  | 235/300 [1:24:06<24:03, 22.21s/it]Running Inference:  79%|███████▊  | 236/300 [1:24:28<23:49, 22.34s/it]Running Inference:  79%|███████▉  | 237/300 [1:24:53<24:07, 22.97s/it]Running Inference:  79%|███████▉  | 238/300 [1:25:16<23:39, 22.89s/it]Running Inference:  80%|███████▉  | 239/300 [1:25:39<23:24, 23.02s/it]Running Inference:  80%|████████  | 240/300 [1:26:02<23:05, 23.09s/it]Running Inference:  80%|████████  | 241/300 [1:26:23<21:54, 22.28s/it]Running Inference:  81%|████████  | 242/300 [1:26:45<21:39, 22.41s/it]Running Inference:  81%|████████  | 243/300 [1:26:48<15:37, 16.44s/it]Running Inference:  81%|████████▏ | 244/300 [1:27:09<16:48, 18.01s/it]Running Inference:  82%|████████▏ | 245/300 [1:27:34<18:11, 19.85s/it]Running Inference:  82%|████████▏ | 246/300 [1:27:56<18:38, 20.72s/it]Running Inference:  82%|████████▏ | 247/300 [1:28:17<18:25, 20.86s/it]Running Inference:  83%|████████▎ | 248/300 [1:28:43<19:22, 22.35s/it]Running Inference:  83%|████████▎ | 249/300 [1:29:07<19:20, 22.76s/it]Running Inference:  83%|████████▎ | 250/300 [1:29:30<19:02, 22.85s/it]Running Inference:  84%|████████▎ | 251/300 [1:29:51<18:07, 22.20s/it]Running Inference:  84%|████████▍ | 252/300 [1:30:15<18:07, 22.66s/it]Running Inference:  84%|████████▍ | 253/300 [1:30:37<17:45, 22.67s/it]Running Inference:  85%|████████▍ | 254/300 [1:30:58<16:59, 22.16s/it]Running Inference:  85%|████████▌ | 255/300 [1:31:21<16:42, 22.29s/it]Running Inference:  85%|████████▌ | 256/300 [1:31:44<16:28, 22.47s/it]Running Inference:  86%|████████▌ | 257/300 [1:32:06<16:08, 22.53s/it]Running Inference:  86%|████████▌ | 258/300 [1:32:29<15:50, 22.62s/it]Running Inference:  86%|████████▋ | 259/300 [1:32:54<15:59, 23.40s/it]Running Inference:  87%|████████▋ | 260/300 [1:32:58<11:33, 17.33s/it]Running Inference:  87%|████████▋ | 261/300 [1:33:20<12:20, 18.98s/it]Running Inference:  87%|████████▋ | 262/300 [1:33:44<12:51, 20.31s/it]Running Inference:  88%|████████▊ | 263/300 [1:34:07<13:02, 21.15s/it]Running Inference:  88%|████████▊ | 264/300 [1:34:19<11:01, 18.39s/it]Running Inference:  88%|████████▊ | 265/300 [1:34:42<11:29, 19.71s/it]Running Inference:  89%|████████▊ | 266/300 [1:35:05<11:43, 20.68s/it]Running Inference:  89%|████████▉ | 267/300 [1:35:28<11:49, 21.49s/it]Running Inference:  89%|████████▉ | 268/300 [1:35:51<11:47, 22.10s/it]Running Inference:  90%|████████▉ | 269/300 [1:36:16<11:43, 22.68s/it]Running Inference:  90%|█████████ | 270/300 [1:36:33<10:31, 21.04s/it]Running Inference:  90%|█████████ | 271/300 [1:36:56<10:30, 21.74s/it]Running Inference:  91%|█████████ | 272/300 [1:37:22<10:43, 23.00s/it]Running Inference:  91%|█████████ | 273/300 [1:37:45<10:18, 22.91s/it]Running Inference:  91%|█████████▏| 274/300 [1:38:08<10:00, 23.11s/it]Running Inference:  92%|█████████▏| 275/300 [1:38:30<09:23, 22.53s/it]Running Inference:  92%|█████████▏| 276/300 [1:38:49<08:36, 21.51s/it]Running Inference:  92%|█████████▏| 277/300 [1:39:12<08:30, 22.21s/it]Running Inference:  93%|█████████▎| 278/300 [1:39:15<05:59, 16.34s/it]Running Inference:  93%|█████████▎| 279/300 [1:39:38<06:24, 18.32s/it]Running Inference:  93%|█████████▎| 280/300 [1:40:01<06:35, 19.79s/it]Running Inference:  94%|█████████▎| 281/300 [1:40:20<06:07, 19.36s/it]Running Inference:  94%|█████████▍| 282/300 [1:40:38<05:41, 18.98s/it]Running Inference:  94%|█████████▍| 283/300 [1:41:01<05:43, 20.23s/it]Running Inference:  95%|█████████▍| 284/300 [1:41:10<04:31, 16.96s/it]Running Inference:  95%|█████████▌| 285/300 [1:41:35<04:50, 19.36s/it]Running Inference:  95%|█████████▌| 286/300 [1:42:03<05:06, 21.87s/it]Running Inference:  96%|█████████▌| 287/300 [1:42:26<04:49, 22.25s/it]Running Inference:  96%|█████████▌| 288/300 [1:42:49<04:29, 22.43s/it]Running Inference:  96%|█████████▋| 289/300 [1:43:12<04:08, 22.55s/it]Running Inference:  97%|█████████▋| 290/300 [1:43:33<03:42, 22.25s/it]Running Inference:  97%|█████████▋| 291/300 [1:43:57<03:23, 22.65s/it]Running Inference:  97%|█████████▋| 292/300 [1:44:20<03:01, 22.68s/it]Running Inference:  98%|█████████▊| 293/300 [1:44:43<02:39, 22.81s/it]Running Inference:  98%|█████████▊| 294/300 [1:45:06<02:16, 22.82s/it]Running Inference:  98%|█████████▊| 295/300 [1:45:28<01:54, 22.82s/it]Running Inference:  99%|█████████▊| 296/300 [1:45:47<01:26, 21.60s/it]Running Inference:  99%|█████████▉| 297/300 [1:46:10<01:05, 21.89s/it]Running Inference:  99%|█████████▉| 298/300 [1:46:32<00:44, 22.11s/it]Running Inference: 100%|█████████▉| 299/300 [1:46:55<00:22, 22.40s/it]Running Inference: 100%|██████████| 300/300 [1:47:16<00:00, 21.80s/it]Running Inference: 100%|██████████| 300/300 [1:47:16<00:00, 21.45s/it]
2025-12-15 04:25:47,272 - INFO - Inference completed.
2025-12-15 04:25:47,319 - INFO - Results saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__streaming_llm__0.10/predictions.csv
2025-12-15 04:25:47,319 - INFO - Calculating metrics for dataset: longbench
2025-12-15 04:26:14,314 - INFO - Metrics saved to longbenchresult/longbench__gov_report_e__Qwen--Qwen3-8B__streaming_llm__0.10/metrics.json
2025-12-15 04:26:14,314 - INFO - Metrics:
15.14
2025-12-15 04:26:14,316 - INFO - Evaluation run completed successfully.
✓ Completed: streaming_llm (task=gov_report_e, ratio=0.1) on GPU 2

----------------------------------------
Task: gov_report_e | Compression Ratio: 0.2
----------------------------------------
/data/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
KVzipPress requires multiple forward passes for chunked context reconstruction, resulting in a computational overhead of 2–3 times the initial prefilling cost. This significantly increases the overall prefilling time compared to other compression methods, which is inherent to the KVzip algorithm design.
2025-12-15 04:26:21,603 - INFO - Set deterministic seeds to 42
2025-12-15 04:26:21,603 - INFO - Initialized EvaluationRunner with config:
{
  "dataset": "longbench",
  "data_dir": "gov_report_e",
  "model": "Qwen/Qwen3-8B",
  "device": "cuda:0",
  "press_name": "streaming_llm",
  "compression_ratio": 0.2,
  "key_channel_compression_ratio": null,
  "policy_checkpoint": null,
  "c_target": null,
  "min_tokens_per_head": 1.0,
  "fraction": 1.0,
  "max_new_tokens": null,
  "max_context_length": null,
  "compress_questions": false,
  "needle_depth": null,
  "compression_interval": null,
  "target_size": null,
  "hidden_states_buffer_size": null,
  "output_dir": "./longbenchresult",
  "log_level": "INFO",
  "model_kwargs": {
    "attn_implementation": null,
    "dtype": "auto"
  },
  "press_init_command": null,
  "seed": 42
}
2025-12-15 04:26:21,603 - INFO - Starting evaluation run...
2025-12-15 04:26:21,603 - INFO - Output directory set to: longbenchresult
2025-12-15 04:26:21,603 - INFO - Set StreamingLLMPress compression_ratio to 0.2
2025-12-15 04:26:21,603 - INFO - KV Press 'streaming_llm' setup.
2025-12-15 04:26:21,603 - INFO - Flash Attention 2 detected, setting attn_implementation to 'flash_attention_2'.
2025-12-15 04:26:21,603 - INFO - Loading model pipeline for: Qwen/Qwen3-8B on device: cuda:0 with model_kwargs: {'attn_implementation': 'flash_attention_2', 'dtype': 'auto'}
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 105.34it/s]
Device set to use cuda:0
2025-12-15 04:26:33,243 - INFO - Model pipeline loaded.
2025-12-15 04:26:33,243 - INFO - Loading dataset: Xnhyacinth/LongBench (data_dir: gov_report_e)
Using the latest cached version of the dataset since Xnhyacinth/LongBench couldn't be found on the Hugging Face Hub
Traceback (most recent call last):
  File "/data/kvpress-main1/evaluation/evaluate.py", line 685, in <module>
    Fire(CliEntryPoint)
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 568, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/kvpress-main1/evaluation/evaluate.py", line 681, in __call__
    runner.run_evaluation()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 631, in run_evaluation
    self._load_and_prepare_dataset()
  File "/data/kvpress-main1/evaluation/evaluate.py", line 411, in _load_and_prepare_dataset
    df = load_dataset(DATASET_REGISTRY[dataset_name], data_dir=data_dir, split="test").to_pandas()
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2606, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/load.py", line 2314, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 140, in __init__
    config_name, version, hash = _find_hash_in_cache(
                                 ^^^^^^^^^^^^^^^^^^^^
  File "/data/anaconda3/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 65, in _find_hash_in_cache
    raise ValueError(
ValueError: Couldn't find cache for Xnhyacinth/LongBench for config 'default-data_dir=gov_report_e'
Available configs in the cache: ['2wikimqa', '2wikimqa_e', 'default-0065b3a180de55c0', 'default-098a7eb91a184203', 'default-0b7076831c003298', 'default-1d60c6d61b7edecf', 'default-24f61eabf2a87b99', 'default-24fadcfe8fa8f3a5', 'default-4e16e465b7b19fe5', 'default-515182422f5c01c8', 'default-531e246ba9fed12a', 'default-5984d2c88b13ebba', 'default-5e4ea58ffb882d49', 'default-65a1875e7b162d76', 'default-7a8b9ba40e031066', 'default-8d4d4887aac16609', 'default-988b39b640b83efb', 'default-9d6e0826941c237c', 'default-a17dcc93f20cb815', 'default-a9c51f35339386df', 'default-afe1d98c024b9964', 'default-b1151d9592e7a1a3', 'default-dce78539c6759811', 'default-ddd7dcf150b1db88', 'default-f744668059827d3f', 'dureader', 'gov_report', 'gov_report_e', 'hotpotqa', 'hotpotqa_e', 'lcc', 'lcc_e', 'lsht', 'multi_news', 'multi_news_e', 'multifieldqa_en', 'multifieldqa_en_e', 'multifieldqa_zh', 'musique', 'narrativeqa', 'passage_count', 'passage_count_e', 'passage_retrieval_en', 'passage_retrieval_en_e', 'passage_retrieval_zh', 'qasper', 'qasper_e', 'qmsum', 'repobench-p', 'repobench-p_e', 'samsum', 'samsum_e', 'trec', 'trec_e', 'triviaqa', 'triviaqa_e', 'vcsum']
